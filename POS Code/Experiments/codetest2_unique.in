utf-8 def status ( self , query = , agent = None ) : \n 
result = [ ] \n 
try : \n 
qry = \n 
~~~ qry += . format ( query ) . encode ( ) \n 
~~ else : \n 
~~~ qry += query . encode ( ) \n 
~~ sock = socket . socket ( self . family , socket . SOCK_STREAM ) \n 
sock . connect ( ( self . host , self . port ) ) \n 
sock . send ( qry ) \n 
sock . shutdown ( socket . SHUT_WR ) \n 
fileobj = sock . makefile ( ) \n 
sock . close ( ) \n 
for line in fileobj . readlines ( ) : \n 
~~~ line = line . rstrip ( ) \n 
if not line : \n 
~~~ continue \n 
~~ try : \n 
~~~ if not PY3 : \n 
~~~ line = line . decode ( , ) \n 
~~~ pass \n 
~~ result . append ( line ) \n 
~~ fileobj . close ( ) \n 
self . _logger ( , . format ( result ) ) \n 
return result \n 
~~ except socket . error : \n 
~~~ self . _logger ( , , self . host , self . port ) \n 
raise BUIserverException ( . format ( self . host , self . port ) ) \n 
res = { } \n 
if agent : \n 
~~~ if not name or name not in self . running [ agent ] : \n 
~~~ return res \n 
~~ ~~ else : \n 
~~~ if not name or name not in self . running : \n 
~~ ~~ filemap = self . status ( . format ( name ) ) \n 
if not filemap : \n 
~~ for line in filemap : \n 
\n 
~~~ reg = re . search ( . format ( name ) , line ) \n 
if reg and reg . group ( 2 ) == and int ( reg . group ( 1 ) ) == 2 : \n 
~~~ count = 0 \n 
for val in reg . group ( 3 ) . split ( ) : \n 
~~~ if val and count > 0 and count < 15 : \n 
~~~ try : \n 
~~~ vals = map ( int , val . split ( ) ) \n 
if vals [ 0 ] > 0 or vals [ 1 ] > 0 or vals [ 2 ] or vals [ 3 ] > 0 : \n 
~~~ res [ self . counters [ count ] ] = vals \n 
~~ ~~ except ( ValueError , IndexError ) : \n 
~~~ count += 1 \n 
continue \n 
~~ ~~ elif val : \n 
~~~ if self . counters [ count ] == : \n 
~~~ res [ self . counters [ count ] ] = val \n 
~~~ res [ self . counters [ count ] ] = int ( val ) \n 
~~ except __HOLE__ : \n 
~~ ~~ ~~ count += 1 \n 
~~ ~~ ~~ if not in res : \n 
~~~ res [ ] = 0 \n 
~~ if res . viewkeys ( ) & { , , } : \n 
~~~ diff = time . time ( ) - int ( res [ ] ) \n 
byteswant = int ( res [ ] ) \n 
bytesgot = int ( res [ ] ) \n 
bytespersec = bytesgot / diff \n 
bytesleft = byteswant - bytesgot \n 
res [ ] = bytespersec \n 
if bytespersec > 0 : \n 
~~~ timeleft = int ( bytesleft / bytespersec ) \n 
res [ ] = timeleft \n 
~~~ res [ ] = - 1 \n 
~~ ~~ except : \n 
~~ ~~ try : \n 
~~~ res [ ] = round ( float ( res [ ] ) / float ( res [ ] ) * 100 ) \n 
~~ except Exception : \n 
~~ return res \n 
~~ def get_tree ( self , name = None , backup = None , root = None , agent = None ) : \n 
res = [ ] \n 
if not name or not backup : \n 
~~ if not root : \n 
~~~ top = \n 
~~~ top = root . decode ( , ) \n 
~~~ top = root \n 
~~ ~~ filemap = self . status ( . format ( name , backup , top ) ) \n 
useful = False \n 
for line in filemap : \n 
~~~ if not useful and re . match ( , line ) : \n 
~~~ useful = True \n 
~~ if useful and re . match ( , line ) : \n 
~~~ useful = False \n 
~~ if useful : \n 
~~~ tree = { } \n 
match = re . search ( , line ) \n 
if match : \n 
~~~ if re . match ( , match . group ( 1 ) ) : \n 
~~~ tree [ ] = \n 
~~ spl = re . split ( , line , 7 ) \n 
tree [ ] = spl [ 0 ] \n 
tree [ ] = spl [ 1 ] \n 
tree [ ] = spl [ 2 ] \n 
tree [ ] = spl [ 3 ] \n 
tree [ ] = . format ( _hr ( spl [ 4 ] ) ) \n 
tree [ ] = . format ( spl [ 5 ] , spl [ 6 ] ) \n 
tree [ ] = spl [ 7 ] \n 
tree [ ] = top \n 
res . append ( tree ) \n 
~~ ~~ ~~ return res \n 
~~ def clean ( self , value ) : \n 
~~~ if not value : \n 
~~~ return 0 \n 
~~ result = BitHandler ( 0 , [ k for k , v in self . choices ] ) \n 
for k in value : \n 
~~~ setattr ( result , str ( k ) , True ) \n 
~~~ raise ValidationError ( % ( k , ) ) \n 
~~ ~~ return int ( result ) \n 
~~ def makedirs ( * args , ** kwargs ) : \n 
~~~ os . makedirs ( * args , ** kwargs ) \n 
~~ except __HOLE__ as ex : \n 
~~~ if ex . errno != errno . EEXIST : \n 
~~~ raise \n 
~~ ~~ ~~ def determine_encoding ( path , default = None ) : \n 
byte_order_marks = ( \n 
( , ( codecs . BOM_UTF8 , ) ) , \n 
( , ( codecs . BOM_UTF16_LE , codecs . BOM_UTF16_BE ) ) , \n 
( , ( codecs . BOM_UTF32_LE , codecs . BOM_UTF32_BE ) ) , \n 
) \n 
~~~ with open ( path , ) as infile : \n 
~~~ raw = infile . read ( 4 ) \n 
~~ ~~ except __HOLE__ : \n 
~~~ return default \n 
~~ for encoding , boms in byte_order_marks : \n 
~~~ if any ( raw . startswith ( bom ) for bom in boms ) : \n 
~~~ return encoding \n 
~~ ~~ return default \n 
~~ def _get_port_range ( ) : \n 
~~~ config_range = CONF . serial_console . port_range \n 
~~~ start , stop = map ( int , config_range . split ( ) ) \n 
if start >= stop : \n 
~~~ raise ValueError \n 
~~~ default_port_range = nova . conf . serial_console . DEFAULT_PORT_RANGE \n 
{ : config_range , \n 
: default_port_range } ) \n 
start , stop = map ( int , default_port_range . split ( ) ) \n 
~~ return start , stop \n 
~~ def check ( file_staged_for_commit , options ) : \n 
~~~ basename = os . path . basename ( file_staged_for_commit . path ) \n 
if not fnmatch . fnmatch ( basename , options . json_files ) : \n 
~~~ return True \n 
~~~ json . loads ( file_staged_for_commit . contents ) \n 
~~~ return False \n 
~~ ~~ def gen_cyrillic ( length = 10 ) : \n 
_is_positive_int ( length ) \n 
random . seed ( ) \n 
codepoints = [ random . randint ( 0x0400 , 0x04FF ) for _ in range ( length ) ] \n 
~~~ output = . join ( unichr ( codepoint ) for codepoint in codepoints ) \n 
~~~ output = . join ( chr ( codepoint ) for codepoint in codepoints ) \n 
~~ return _make_unicode ( output ) \n 
~~ def rmtree ( dname ) : \n 
~~~ def paranoia_ok ( dname ) : \n 
~~~ if in dname or in dname : \n 
~~ return in dname or in dname \n 
~~ if paranoia_ok ( dname ) : \n 
~~~ shutil . rmtree ( dname ) \n 
~~ ~~ ~~ def get_func_name ( func , resolv_alias = True , win_characters = True ) : \n 
if hasattr ( func , ) : \n 
~~~ module = func . __module__ \n 
~~~ module = inspect . getmodule ( func ) \n 
~~~ if hasattr ( func , ) : \n 
~~~ module = func . __class__ . __module__ \n 
~~~ module = \n 
~~ ~~ ~~ if module is None : \n 
~~ if module == : \n 
~~~ filename = os . path . abspath ( inspect . getsourcefile ( func ) ) \n 
~~ except : \n 
~~~ filename = None \n 
~~ if filename is not None : \n 
~~~ parts = filename . split ( os . sep ) \n 
if parts [ - 1 ] . startswith ( ) : \n 
~~~ parts [ - 1 ] = \n 
~~ filename = . join ( parts ) \n 
if filename . endswith ( ) : \n 
~~~ filename = filename [ : - 3 ] \n 
~~ module = module + + filename \n 
~~ ~~ module = module . split ( ) \n 
~~~ name = func . func_name \n 
~~ elif hasattr ( func , ) : \n 
~~~ name = func . __name__ \n 
~~~ name = \n 
~~ if resolv_alias : \n 
~~~ if hasattr ( func , ) and name in func . func_globals : \n 
~~~ if not func . func_globals [ name ] is func : \n 
~~~ name = % name \n 
~~ ~~ ~~ if inspect . ismethod ( func ) : \n 
~~~ klass = func . im_class \n 
module . append ( klass . __name__ ) \n 
~~ ~~ if os . name == and win_characters : \n 
~~~ name = _clean_win_chars ( name ) \n 
module = [ _clean_win_chars ( s ) for s in module ] \n 
~~ return module , name \n 
~~ def getfullargspec ( func ) : \n 
~~~ return inspect . getfullargspec ( func ) \n 
~~~ arg_spec = inspect . getargspec ( func ) \n 
import collections \n 
tuple_fields = ( \n 
tuple_type = collections . namedtuple ( , tuple_fields ) \n 
return tuple_type ( args = arg_spec . args , \n 
varargs = arg_spec . varargs , \n 
varkw = arg_spec . keywords , \n 
defaults = arg_spec . defaults , \n 
kwonlyargs = [ ] , \n 
kwonlydefaults = None , \n 
annotations = { } ) \n 
~~ ~~ def filter_args ( func , ignore_lst , args = ( ) , kwargs = dict ( ) ) : \n 
args = list ( args ) \n 
if isinstance ( ignore_lst , _basestring ) : \n 
~~~ raise ValueError ( \n 
% ( ignore_lst , type ( ignore_lst ) ) ) \n 
~~ if ( not inspect . ismethod ( func ) and not inspect . isfunction ( func ) ) : \n 
~~~ if ignore_lst : \n 
~~~ warnings . warn ( \n 
% func , stacklevel = 2 ) \n 
~~ return { : args , : kwargs } \n 
~~ arg_spec = getfullargspec ( func ) \n 
arg_names = arg_spec . args + arg_spec . kwonlyargs \n 
arg_defaults = arg_spec . defaults or ( ) \n 
arg_defaults = arg_defaults + tuple ( arg_spec . kwonlydefaults [ k ] \n 
for k in arg_spec . kwonlyargs ) \n 
arg_varargs = arg_spec . varargs \n 
arg_varkw = arg_spec . varkw \n 
if inspect . ismethod ( func ) : \n 
~~~ args = [ func . __self__ , ] + args \n 
~~ _ , name = get_func_name ( func , resolv_alias = False ) \n 
arg_dict = dict ( ) \n 
arg_position = - 1 \n 
for arg_position , arg_name in enumerate ( arg_names ) : \n 
~~~ if arg_position < len ( args ) : \n 
~~~ if arg_name not in arg_spec . kwonlyargs : \n 
~~~ arg_dict [ arg_name ] = args [ arg_position ] \n 
% ( arg_name , \n 
_signature_str ( name , arg_spec ) , \n 
_function_called_str ( name , args , kwargs ) ) \n 
~~~ position = arg_position - len ( arg_names ) \n 
if arg_name in kwargs : \n 
~~~ arg_dict [ arg_name ] = kwargs . pop ( arg_name ) \n 
~~~ arg_dict [ arg_name ] = arg_defaults [ position ] \n 
~~ except ( __HOLE__ , KeyError ) : \n 
% ( _signature_str ( name , arg_spec ) , \n 
~~ ~~ ~~ ~~ varkwargs = dict ( ) \n 
for arg_name , arg_value in sorted ( kwargs . items ( ) ) : \n 
~~~ if arg_name in arg_dict : \n 
~~~ arg_dict [ arg_name ] = arg_value \n 
~~ elif arg_varkw is not None : \n 
~~~ varkwargs [ arg_name ] = arg_value \n 
~~ ~~ if arg_varkw is not None : \n 
~~~ arg_dict [ ] = varkwargs \n 
~~ if arg_varargs is not None : \n 
~~~ varargs = args [ arg_position + 1 : ] \n 
arg_dict [ ] = varargs \n 
~~ for item in ignore_lst : \n 
~~~ if item in arg_dict : \n 
~~~ arg_dict . pop ( item ) \n 
% ( item , \n 
_signature_str ( name , arg_spec ) ) \n 
~~ ~~ return arg_dict \n 
~~ def _safe_link ( src , dst ) : \n 
~~~ os . unlink ( dst ) \n 
~~ os . symlink ( src , dst ) \n 
~~ def main ( args ) : \n 
~~~ parser , resolver_options_builder = configure_clp ( ) \n 
~~~ separator = args . index ( ) \n 
args , cmdline = args [ : separator ] , args [ separator + 1 : ] \n 
~~~ args , cmdline = args , [ ] \n 
~~ options , reqs = parser . parse_args ( args = args ) \n 
if options . pex_root : \n 
~~~ ENV . set ( , options . pex_root ) \n 
~~ options . cache_dir = make_relative_to_root ( options . cache_dir ) \n 
options . interpreter_cache_dir = make_relative_to_root ( options . interpreter_cache_dir ) \n 
with ENV . patch ( PEX_VERBOSE = str ( options . verbosity ) ) : \n 
~~~ with TRACER . timed ( ) : \n 
~~~ pex_builder = build_pex ( reqs , options , resolver_options_builder ) \n 
~~ if options . pex_name is not None : \n 
~~~ log ( % options . pex_name , v = options . verbosity ) \n 
tmp_name = options . pex_name + \n 
safe_delete ( tmp_name ) \n 
pex_builder . build ( tmp_name ) \n 
os . rename ( tmp_name , options . pex_name ) \n 
return 0 \n 
~~ if options . platform != Platform . current ( ) : \n 
~~~ log ( ) \n 
~~ pex_builder . freeze ( ) \n 
log ( % ( pex_builder . path ( ) , cmdline ) , v = options . verbosity ) \n 
pex = PEX ( pex_builder . path ( ) , interpreter = pex_builder . interpreter ) \n 
sys . exit ( pex . run ( args = list ( cmdline ) ) ) \n 
~~ ~~ def read ( self , callback , grpos_range , frames = None ) : \n 
~~~ if grpos_range : \n 
~~~ end_grpos = self . _grpos + grpos_range \n 
pos = self . _pos \n 
grpos = self . _grpos \n 
while 1 : \n 
~~~ idx = pos - self . _s . data_offset \n 
if idx < 0 : \n 
~~~ pos -= idx \n 
idx = 0 \n 
~~~ f = self . _s . data [ idx ] \n 
~~~ break \n 
~~ grpos = f [ 0 ] \n 
if grpos >= end_grpos : \n 
~~~ grpos = end_grpos \n 
break \n 
~~ callback ( * f ) \n 
pos += 1 \n 
~~ self . _pos = pos \n 
self . _grpos = grpos \n 
~~ elif frames : \n 
~~~ pos = self . _pos \n 
frames += idx \n 
~~ if frames < 1 : \n 
~~ except IndexError : \n 
callback ( * f ) \n 
frames -= 1 \n 
~~ return None , defer . succeed ( None ) \n 
~~ def test_keyfile ( self ) : \n 
~~~ logging . debug ( ) \n 
logging . debug ( ) \n 
prefix = os . path . expanduser ( os . path . join ( , ) ) \n 
key_file = os . path . join ( prefix , ) \n 
id_file = os . path . join ( prefix , ) \n 
for name in ( key_file , id_file ) : \n 
~~~ if os . path . exists ( name ) : \n 
~~~ saved = name + \n 
if os . path . exists ( saved ) : \n 
~~~ os . remove ( saved ) \n 
~~ os . rename ( name , saved ) \n 
~~~ user = % ( getpass . getuser ( ) , socket . gethostname ( ) ) \n 
if user in _KEY_CACHE : \n 
~~~ del _KEY_CACHE [ user ] \n 
~~ key_pair = get_key_pair ( user , logging . getLogger ( ) , ignore_ssh = True ) \n 
if sys . platform != : \n 
~~~ os . chmod ( key_file , 0 644 ) \n 
del _KEY_CACHE [ user ] \n 
key_pair = get_key_pair ( user , logging . getLogger ( ) , ignore_ssh = True ) \n 
~~ del _KEY_CACHE [ user ] \n 
if sys . platform != or HAVE_PYWIN32 : \n 
~~~ self . assertTrue ( is_private ( key_file ) ) \n 
if sys . platform == : \n 
~~~ public_file = os . environ [ ] \n 
~~~ public_file = \n 
~~ self . assertFalse ( is_private ( public_file ) ) \n 
~~~ del threading . current_thread ( ) . credentials \n 
~~ ~~ finally : \n 
~~~ for name in ( key_file , id_file ) : \n 
~~~ os . remove ( name ) \n 
~~ os . rename ( saved , name ) \n 
~~ ~~ ~~ ~~ def test_authorized_keys ( self ) : \n 
hostname = socket . gethostname ( ) \n 
with open ( , ) as out : \n 
~~~ out . write ( good_key_data ) \n 
~~ if sys . platform != or HAVE_PYWIN32 : \n 
~~~ make_private ( ) \n 
~~~ keys = read_authorized_keys ( , logging . getLogger ( ) ) \n 
for name , key in keys . items ( ) : \n 
~~~ logging . debug ( , name , key ) \n 
~~ self . assertEqual ( sorted ( keys . keys ( ) ) , \n 
[ + hostname , \n 
+ hostname , \n 
+ hostname ] ) \n 
~~ finally : \n 
~~~ os . remove ( ) \n 
~~ key_file = \n 
~~~ write_authorized_keys ( keys , key_file ) \n 
new_keys = read_authorized_keys ( key_file ) \n 
self . assertEqual ( len ( keys ) , len ( new_keys ) ) \n 
for user in sorted ( keys . keys ( ) ) : \n 
~~~ pubkey = keys [ user ] \n 
~~~ new_pubkey = new_keys [ user ] \n 
~~ except KeyError : \n 
~~~ self . fail ( , user ) \n 
~~ self . assertEqual ( new_pubkey . n , pubkey . n ) \n 
self . assertEqual ( new_pubkey . e , pubkey . e ) \n 
~~ ~~ ~~ finally : \n 
~~~ if os . path . exists ( key_file ) : \n 
~~~ os . remove ( key_file ) \n 
~~~ keys = read_authorized_keys ( logger = logging . getLogger ( ) ) \n 
assert_raises ( self , code , globals ( ) , locals ( ) , RuntimeError , \n 
~~~ with open ( , ) as out : \n 
~~ os . chmod ( , 0 666 ) \n 
~~ ~~ with open ( , ) as out : \n 
~~~ out . write ( bad_key_data ) \n 
~~ ~~ def get_imports_info ( imports , pypi_server = "https://pypi.python.org/pypi/" , proxy = None ) : \n 
~~~ result = [ ] \n 
for item in imports : \n 
~~~ response = requests . get ( "{0}{1}/json" . format ( pypi_server , item ) , proxies = proxy ) \n 
if response . status_code == 200 : \n 
~~~ if hasattr ( response . content , ) : \n 
~~~ data = json2package ( response . content . decode ( ) ) \n 
~~~ data = json2package ( response . content ) \n 
~~ ~~ elif response . status_code >= 300 : \n 
~~~ raise HTTPError ( status_code = response . status_code , \n 
reason = response . reason ) \n 
~~~ logging . debug ( \n 
, item ) \n 
~~ result . append ( { : item , : data . latest_release_id } ) \n 
~~ return result \n 
~~~ args = docopt ( __doc__ , version = __version__ ) \n 
log_level = logging . DEBUG if args [ ] else logging . INFO \n 
logging . basicConfig ( level = log_level , format = ) \n 
~~~ init ( args ) \n 
~~~ sys . exit ( 0 ) \n 
~~ ~~ def get_field_model ( self , model ) : \n 
~~~ if not isinstance ( model , basestring ) : \n 
~~~ return model \n 
~~~ return apps . get_model ( model ) \n 
~~ except LookupError : \n 
~~~ app_name , class_name = model . split ( ) \n 
module = importlib . import_module ( % app_name ) \n 
return getattr ( module , class_name ) \n 
~~ except ( ImportError , AttributeError , __HOLE__ ) : \n 
~~ ~~ ~~ def compile_message ( self , message_template , context ) : \n 
~~~ msg = six . text_type ( message_template ) . format ( ** context ) \n 
~~ except __HOLE__ as e : \n 
~~~ raise LoggerError ( \n 
str ( e ) , . join ( context . keys ( ) ) ) ) \n 
~~ return msg \n 
~~ def setUp ( self ) : \n 
~~~ import feedparser \n 
~~~ raise SkipTest ( ) \n 
~~ super ( TestFeeds , self ) . setUp ( ) \n 
create_basic_categories ( self ) \n 
create_and_place_more_publishables ( self ) \n 
list_all_publishables_in_category_by_hour ( self ) \n 
self . _feeder = RSSTopCategoryListings ( ) \n 
~~ @ staticmethod \n 
~~~ def _normalize_name ( name ) : \n 
~~~ return HTTPHeaders . _normalized_headers [ name ] \n 
~~~ if HTTPHeaders . _NORMALIZED_HEADER_RE . match ( name ) : \n 
~~~ normalized = name \n 
~~~ normalized = "-" . join ( \n 
[ w . capitalize ( ) for w in name . split ( "-" ) ] ) \n 
~~ HTTPHeaders . _normalized_headers [ name ] = normalized \n 
return normalized \n 
~~ ~~ ~~ def setup_self ( self ) : \n 
~~~ self . path = add_slash ( self . _environ . get ( , ) ) \n 
self . method = self . _environ . get ( , ) . upper ( ) \n 
self . query = self . _environ . get ( , ) \n 
self . content_length = 0 \n 
self . headers = HTTPHeaders ( ) \n 
if self . _environ . get ( "CONTENT_TYPE" ) : \n 
~~~ self . headers [ "Content-Type" ] = self . _environ [ "CONTENT_TYPE" ] \n 
~~ if self . _environ . get ( "CONTENT_LENGTH" ) : \n 
~~~ self . headers [ "Content-Length" ] = self . _environ [ "CONTENT_LENGTH" ] \n 
~~ for key in self . _environ : \n 
~~~ if key . startswith ( "HTTP_" ) : \n 
~~~ self . headers [ key [ 5 : ] . replace ( "_" , "-" ) ] = self . _environ [ key ] \n 
~~~ self . content_length = int ( self . _environ . get ( , ) ) \n 
~~ self . GET = self . build_get_dict ( ) \n 
~~ def run_itty ( server = , host = , port = 8080 , config = None , \n 
cookie_secret = None ) : \n 
if not server in WSGI_ADAPTERS : \n 
~~ if config is not None : \n 
~~~ config_options = __import__ ( config ) \n 
host = getattr ( config_options , , host ) \n 
port = getattr ( config_options , , port ) \n 
server = getattr ( config_options , , server ) \n 
~~ if server != : \n 
~~~ print % server \n 
print % ( host , port ) \n 
print \n 
~~ global COOKIE_SECRET \n 
COOKIE_SECRET = cookie_secret or base64 . b64encode ( os . urandom ( 32 ) ) \n 
~~~ WSGI_ADAPTERS [ server ] ( host , port ) \n 
~~~ print \n 
~~ ~~ def convert_md_to_rst ( source , destination = None , backup_dir = None ) : \n 
~~~ import pypandoc \n 
import pypandoc \n 
~~ destination = destination or ( os . path . splitext ( source ) [ 0 ] + ) \n 
backup_dir = backup_dir or os . path . join ( os . path . dirname ( destination ) , \n 
bak_name = ( os . path . basename ( destination ) + \n 
time . strftime ( ) ) \n 
bak_path = os . path . join ( backup_dir , bak_name ) \n 
if os . path . isfile ( destination ) : \n 
~~~ if not os . path . isdir ( os . path . dirname ( bak_path ) ) : \n 
~~~ os . mkdir ( os . path . dirname ( bak_path ) ) \n 
~~ os . rename ( destination , bak_path ) \n 
~~~ pypandoc . convert ( \n 
source , \n 
, \n 
format = , \n 
outputfile = destination \n 
~~~ if os . path . isfile ( destination ) : \n 
~~~ os . remove ( destination ) \n 
~~ if os . path . isfile ( bak_path ) : \n 
~~~ os . rename ( bak_path , destination ) \n 
~~ raise \n 
~~ ~~ def _parse ( self , data ) : \n 
h = StringIO ( data ) \n 
result = { } \n 
for line in h . readlines ( ) : \n 
~~~ key , value = self . _parse_line ( line ) \n 
result [ key ] = value \n 
~~ ~~ return result \n 
~~ def _parse_line ( self , line ) : \n 
~~~ key , value = map ( str . strip , line . split ( ) ) \n 
~~ except ValueError : \n 
~~~ raise ValueError ( % line ) \n 
~~ if not key : \n 
~~~ raise ValueError ( ) \n 
~~~ value = int ( value ) \n 
~~ except ( __HOLE__ , ValueError ) : \n 
~~ return key , value \n 
~~ def get_environment ( environmentConfig ) : \n 
~~~ environment_config = open ( environmentConfig , ) \n 
print ( + environmentConfig + ) \n 
~~ except IOError : \n 
~~~ mssg = str ( ) . join ( [ , environmentConfig ] ) \n 
raise Exception ( mssg ) \n 
~~ dom = xml . dom . minidom . parse ( environment_config ) \n 
~~~ cacheLocationElements = dom . getElementsByTagName ( ) \n 
cacheLocation_wmts = None \n 
cacheLocation_twms = None \n 
cacheBasename_wmts = None \n 
cacheBasename_twms = None \n 
for cacheLocation in cacheLocationElements : \n 
~~~ if str ( cacheLocation . attributes [ ] . value ) . lower ( ) == "wmts" : \n 
~~~ cacheLocation_wmts = cacheLocation . firstChild . nodeValue . strip ( ) \n 
cacheBasename_wmts = cacheLocation . attributes [ ] . value \n 
~~ elif str ( cacheLocation . attributes [ ] . value ) . lower ( ) == "twms" : \n 
~~~ cacheLocation_twms = cacheLocation . firstChild . nodeValue . strip ( ) \n 
cacheBasename_twms = cacheLocation . attributes [ ] . value \n 
~~ ~~ except KeyError : \n 
cacheBasename_wmts = "cache_all_wmts" \n 
cacheLocation_twms = cacheLocation . firstChild . nodeValue . strip ( ) \n 
cacheBasename_twms = "cache_all_twms" \n 
~~ ~~ ~~ except IndexError : \n 
~~~ raise Exception ( + environmentConfig ) \n 
~~~ getTileService = get_dom_tag_value ( dom , ) \n 
~~~ getTileService = None \n 
~~ getCapabilitiesElements = dom . getElementsByTagName ( ) \n 
wmts_getCapabilities = None \n 
twms_getCapabilities = None \n 
for getCapabilities in getCapabilitiesElements : \n 
~~~ if str ( getCapabilities . attributes [ ] . value ) . lower ( ) == "wmts" : \n 
~~~ wmts_getCapabilities = getCapabilities . firstChild . nodeValue . strip ( ) \n 
~~ elif str ( getCapabilities . attributes [ ] . value ) . lower ( ) == "twms" : \n 
~~~ twms_getCapabilities = getCapabilities . firstChild . nodeValue . strip ( ) \n 
~~~ raise Exception ( ) \n 
~~ ~~ serviceUrlElements = dom . getElementsByTagName ( ) \n 
wmtsServiceUrl = None \n 
twmsServiceUrl = None \n 
for serviceUrl in serviceUrlElements : \n 
~~~ if str ( serviceUrl . attributes [ ] . value ) . lower ( ) == "wmts" : \n 
~~~ wmtsServiceUrl = serviceUrl . firstChild . nodeValue . strip ( ) \n 
~~ elif str ( serviceUrl . attributes [ ] . value ) . lower ( ) == "twms" : \n 
~~~ twmsServiceUrl = serviceUrl . firstChild . nodeValue . strip ( ) \n 
~~ ~~ stagingLocationElements = dom . getElementsByTagName ( ) \n 
wmtsStagingLocation = None \n 
twmsStagingLocation = None \n 
for stagingLocation in stagingLocationElements : \n 
~~~ if str ( stagingLocation . attributes [ ] . value ) . lower ( ) == "wmts" : \n 
~~~ wmtsStagingLocation = stagingLocation . firstChild . nodeValue . strip ( ) \n 
~~ elif str ( stagingLocation . attributes [ ] . value ) . lower ( ) == "twms" : \n 
~~~ twmsStagingLocation = stagingLocation . firstChild . nodeValue . strip ( ) \n 
~~ ~~ if twmsStagingLocation != None : \n 
~~~ add_trailing_slash ( twmsStagingLocation ) \n 
if not os . path . exists ( twmsStagingLocation ) : \n 
~~~ os . makedirs ( twmsStagingLocation ) \n 
~~ ~~ if wmtsStagingLocation != None : \n 
~~~ add_trailing_slash ( wmtsStagingLocation ) \n 
if not os . path . exists ( wmtsStagingLocation ) : \n 
~~~ os . makedirs ( wmtsStagingLocation ) \n 
~~~ legendLocation = add_trailing_slash ( get_dom_tag_value ( dom , ) ) \n 
~~~ legendLocation = None \n 
~~~ legendUrl = add_trailing_slash ( get_dom_tag_value ( dom , ) ) \n 
~~~ legendUrl = None \n 
~~~ colormapLocations = dom . getElementsByTagName ( ) \n 
for location in colormapLocations : \n 
~~~ if not in location . attributes . keys ( ) : \n 
~~~ if len ( colormapLocations ) > 1 : \n 
~~~ location . attributes [ ] = \n 
~~ ~~ ~~ ~~ except KeyError : \n 
~~~ colormapLocations = None \n 
~~~ colormapUrls = dom . getElementsByTagName ( ) \n 
for url in colormapUrls : \n 
~~~ if not in url . attributes . keys ( ) : \n 
~~~ if len ( colormapUrls ) > 1 : \n 
~~~ url . attributes [ ] = \n 
~~~ colormapUrls = None \n 
~~ if create_mapfile is True : \n 
~~~ mapfileStagingLocation = dom . getElementsByTagName ( ) [ 0 ] . firstChild . nodeValue \n 
~~~ log_sig_exit ( , , sigevent_url ) \n 
~~~ os . makedirs ( mapfileStagingLocation ) \n 
~~ except OSError : \n 
~~~ if not os . path . exists ( mapfileStagingLocation ) : \n 
~~~ log_sig_exit ( , + mapfileStagingLocation + , sigevent_url ) \n 
~~ pass \n 
~~~ mapfileLocationElement = dom . getElementsByTagName ( ) [ 0 ] \n 
mapfileLocation = mapfileLocationElement . firstChild . nodeValue \n 
~~~ mapfileLocationBasename = mapfileLocationElement . attributes [ ] . value \n 
mapfileLocationBasename = None \n 
~~~ mapfileConfigLocation = get_dom_tag_value ( dom , ) \n 
~~~ mapfileConfigLocation = \n 
log_sig_err ( , sigevent_url ) \n 
~~~ mapfileConfigBasename = dom . getElementsByTagName ( ) [ 0 ] . attributes [ ] . value \n 
~~~ mapfileStagingLocation = None \n 
mapfileLocation = None \n 
mapfileConfigLocation = None \n 
mapfileConfigBasename = None \n 
~~ return Environment ( add_trailing_slash ( cacheLocation_wmts ) , \n 
add_trailing_slash ( cacheLocation_twms ) , \n 
cacheBasename_wmts , cacheBasename_twms , \n 
add_trailing_slash ( wmts_getCapabilities ) , \n 
add_trailing_slash ( twms_getCapabilities ) , \n 
add_trailing_slash ( getTileService ) , \n 
add_trailing_slash ( wmtsServiceUrl ) , \n 
add_trailing_slash ( twmsServiceUrl ) , \n 
wmtsStagingLocation , twmsStagingLocation , \n 
legendLocation , legendUrl , \n 
colormapLocations , colormapUrls , \n 
mapfileStagingLocation , mapfileLocation , \n 
mapfileLocationBasename , mapfileConfigLocation , \n 
mapfileConfigBasename ) \n 
~~ def get_archive ( archive_root , archive_configuration ) : \n 
~~~ archive_config = open ( archive_configuration , ) \n 
print ( + archive_configuration ) \n 
~~~ mssg = str ( ) . join ( [ , archive_configuration ] ) \n 
log_sig_exit ( , mssg , sigevent_url ) \n 
~~ location = "" \n 
dom = xml . dom . minidom . parse ( archive_config ) \n 
archiveElements = dom . getElementsByTagName ( ) \n 
for archiveElement in archiveElements : \n 
~~~ if str ( archiveElement . attributes [ ] . value ) . lower ( ) == archive_root . lower ( ) : \n 
~~~ location = archiveElement . getElementsByTagName ( ) [ 0 ] . firstChild . data . strip ( ) \n 
~~ ~~ if location == "" : \n 
~~ return location \n 
~~ def get_projection ( projectionId , projectionConfig , lcdir , tilematrixset_configuration ) : \n 
~~~ projection_config = open ( projectionConfig , ) \n 
print ( + projectionConfig + ) \n 
~~~ mssg = str ( ) . join ( [ , projectionConfig ] ) \n 
~~ dom = xml . dom . minidom . parse ( projection_config ) \n 
projection = None \n 
projectionTags = dom . getElementsByTagName ( ) \n 
for projectionElement in projectionTags : \n 
~~~ if projectionElement . attributes [ ] . value == projectionId : \n 
~~~ wkt = projectionElement . getElementsByTagName ( ) [ 0 ] . firstChild . data . strip ( ) \n 
~~~ wgsbbox = projectionElement . getElementsByTagName ( ) [ 0 ] . toxml ( ) . replace ( "WGS84BoundingBox" , "ows:WGS84BoundingBox" ) \n 
~~~ wgsbbox = "" \n 
~~~ boundbox = "" \n 
~~ bbox = str ( wgsbbox + boundbox ) . replace ( "LowerCorner" , "ows:LowerCorner" ) . replace ( "UpperCorner" , "ows:UpperCorner" ) \n 
tilematrixsets = { } \n 
~~~ tilematrixsetconfig = open ( tilematrixset_configuration , ) \n 
print ( + tilematrixset_configuration + ) \n 
~~~ mssg = str ( ) . join ( [ , tilematrixset_configuration ] ) \n 
~~ tms_dom = xml . dom . minidom . parse ( tilematrixsetconfig ) \n 
tms_projections = tms_dom . getElementsByTagName ( ) \n 
tms_xml = "" \n 
for tms_projection in tms_projections : \n 
~~~ if tms_projection . attributes [ ] . value == projectionId : \n 
tileMatrixSetElements = tms_projection . getElementsByTagName ( ) \n 
for tilematrixset in tileMatrixSetElements : \n 
~~~ scale_denominators = tilematrixset . getElementsByTagName ( "ScaleDenominator" ) \n 
if scale_denominators . length > 1 : \n 
~~~ scale = int ( round ( float ( scale_denominators [ 0 ] . firstChild . nodeValue . strip ( ) ) / float ( scale_denominators [ 1 ] . firstChild . nodeValue . strip ( ) ) ) ) \n 
tilematrixsets [ tilematrixset . getElementsByTagName ( ) [ 0 ] . firstChild . nodeValue . strip ( ) ] = TileMatrixSetMeta ( tilematrixset . getElementsByTagName ( "TileMatrix" ) . length , scale ) \n 
~~ ~~ ~~ except KeyError , e : \n 
~~ ~~ projection = Projection ( projectionId , wkt , bbox , tilematrixsets , tms_xml , lowercorner , uppercorner ) \n 
~~ ~~ if projection == None : \n 
~~ return projection \n 
~~ def detect_time ( time , archiveLocation , fileNamePrefix , year , has_zdb ) : \n 
times = [ ] \n 
time = time . upper ( ) \n 
detect = "DETECT" \n 
period = "P1D" \n 
archiveLocation = add_trailing_slash ( archiveLocation ) \n 
subdaily = False \n 
if not os . path . isdir ( archiveLocation ) : \n 
log_sig_err ( message , sigevent_url ) \n 
return times \n 
~~ if ( time == detect or time == or time . startswith ( detect + ) ) and has_zdb == False : \n 
~~~ dates = [ ] \n 
for dirname , dirnames , filenames in os . walk ( archiveLocation , followlinks = True ) : \n 
~~~ for subdirname in dirnames : \n 
~~~ print "Searching:" , os . path . join ( dirname , subdirname ) \n 
~~ for filename in filenames : \n 
~~~ if str ( filename ) . startswith ( fileNamePrefix ) and len ( filename ) == ( len ( fileNamePrefix ) + len ( "YYYYJJJ" ) + 5 ) : \n 
~~~ filetime = filename [ - 12 : - 5 ] \n 
filedate = datetime . strptime ( filetime , "%Y%j" ) \n 
dates . append ( filedate ) \n 
~~~ print "Skipping" , filename \n 
~~ ~~ elif str ( filename ) . startswith ( fileNamePrefix ) and len ( filename ) == ( len ( fileNamePrefix ) + len ( "YYYYJJJHHMMSS" ) + 5 ) : \n 
~~~ filetime = filename [ - 18 : - 5 ] \n 
filedate = datetime . strptime ( filetime , "%Y%j%H%M%S" ) \n 
subdaily = True \n 
period = "PT24H" \n 
~~~ print "Ignoring" , filename \n 
~~ ~~ ~~ dates = sorted ( list ( set ( dates ) ) ) \n 
if time . startswith ( detect + ) : \n 
~~~ period = time . split ( ) [ 1 ] \n 
~~~ if subdaily == False : \n 
~~~ diff1 = abs ( ( dates [ 0 ] - dates [ 1 ] ) . days ) \n 
diff2 = abs ( ( dates [ 1 ] - dates [ 2 ] ) . days ) \n 
diff3 = abs ( ( dates [ 2 ] - dates [ 3 ] ) . days ) \n 
if diff1 == diff2 == diff3 : \n 
~~~ period = "P" + str ( diff1 ) + "D" \n 
~~ elif 31 in [ diff1 , diff2 , diff3 ] : \n 
~~~ period = "P1M" \n 
~~ if 365 in [ diff1 , diff2 , diff3 ] : \n 
~~~ period = "P1Y" \n 
~~~ diff1 = abs ( ( dates [ 0 ] - dates [ 1 ] ) ) \n 
diff2 = abs ( ( dates [ 1 ] - dates [ 2 ] ) ) \n 
diff3 = abs ( ( dates [ 2 ] - dates [ 3 ] ) ) \n 
~~~ if diff1 . seconds % 3600 == 0 : \n 
~~~ period = "PT" + str ( diff1 . seconds / 3600 ) + "H" \n 
~~ elif diff1 . seconds % 60 == 0 : \n 
~~~ period = "PT" + str ( diff1 . seconds / 60 ) + "M" \n 
~~~ period = "PT" + str ( diff1 . seconds ) + "S" \n 
log_sig_warn ( message , sigevent_url ) \n 
~~~ period_value = int ( period [ 1 : - 1 ] ) \n 
~~~ period_value = int ( period [ 2 : - 1 ] ) \n 
~~ ~~ except ValueError : \n 
~~ if len ( dates ) == 0 : \n 
~~~ startdate = min ( dates ) \n 
for i , d in enumerate ( dates ) : \n 
~~~ if period [ - 1 ] == "W" : \n 
~~~ next_day = d + timedelta ( weeks = period_value ) \n 
~~ elif period [ - 1 ] == "M" and subdaily == False : \n 
~~~ next_day = d + relativedelta ( months = period_value ) \n 
~~ elif period [ - 1 ] == "Y" : \n 
~~~ next_day = d + relativedelta ( years = period_value ) \n 
~~ elif period [ - 1 ] == "H" : \n 
~~~ next_day = d + relativedelta ( hours = period_value ) \n 
~~ elif period [ - 1 ] == "M" and subdaily == True : \n 
~~~ next_day = d + relativedelta ( minutes = period_value ) \n 
~~ elif period [ - 1 ] == "S" : \n 
~~~ next_day = d + relativedelta ( seconds = period_value ) \n 
~~~ next_day = d + timedelta ( days = period_value ) \n 
~~~ if dates [ i + 1 ] == next_day : \n 
start = datetime . strftime ( startdate , "%Y-%m-%d" ) \n 
end = datetime . strftime ( enddate , "%Y-%m-%d" ) \n 
start = datetime . strftime ( startdate , "%Y-%m-%dT%H:%M:%SZ" ) \n 
end = datetime . strftime ( enddate , "%Y-%m-%dT%H:%M:%SZ" ) \n 
~~ times . append ( start + + end + + period ) \n 
enddate = startdate \n 
~~ ~~ except IndexError : \n 
~~~ start = datetime . strftime ( startdate , "%Y-%m-%d" ) \n 
~~~ start = datetime . strftime ( startdate , "%Y-%m-%dT%H:%M:%SZ" ) \n 
~~ ~~ ~~ else : \n 
~~~ intervals = time . split ( ) \n 
~~~ start = detect \n 
~~~ start = \n 
~~ has_period = False \n 
for interval in list ( intervals ) : \n 
~~~ if len ( interval ) > 0 : \n 
~~~ if interval [ 0 ] == : \n 
~~~ has_period = True \n 
period = interval \n 
intervals . remove ( interval ) \n 
~~~ intervals . remove ( interval ) \n 
~~ ~~ if has_period == False : \n 
if has_zdb == False : \n 
~~ log_sig_warn ( message , sigevent_url ) \n 
if len ( intervals ) == 2 : \n 
~~~ start = intervals [ 0 ] \n 
end = intervals [ 1 ] \n 
~~~ if start == detect : \n 
~~~ end = intervals [ 0 ] \n 
end = detect \n 
~~ ~~ if start == detect or end == detect : \n 
~~~ newest_year = \n 
oldest_year = \n 
~~~ years = [ ] \n 
for subdirname in os . walk ( archiveLocation , followlinks = True ) . next ( ) [ 1 ] : \n 
~~~ if subdirname != : \n 
~~~ years . append ( subdirname ) \n 
~~ ~~ years = sorted ( years ) \n 
for idx in range ( 0 , len ( years ) ) : \n 
~~~ if len ( os . listdir ( archiveLocation + + years [ idx ] ) ) > 0 : \n 
~~~ if years [ idx ] . isdigit ( ) == True : \n 
~~~ oldest_year = years [ idx ] \n 
break ; \n 
~~ ~~ ~~ for idx in reversed ( range ( 0 , len ( years ) ) ) : \n 
~~~ newest_year = years [ idx ] \n 
~~ ~~ ~~ ~~ if ( newest_year == or oldest_year == ) and year == True : \n 
log_sig_warn ( mssg , sigevent_url ) \n 
~~ elif year == True : \n 
~~ ~~ if start == detect : \n 
for dirname , dirnames , filenames in os . walk ( archiveLocation + + oldest_year , followlinks = True ) : \n 
~~~ for filename in filenames : \n 
~~ ~~ ~~ if len ( dates ) == 0 : \n 
~~ startdate = min ( dates ) \n 
if has_zdb == True : \n 
~~~ zdb = archiveLocation + + oldest_year + + fileNamePrefix + datetime . strftime ( startdate , "%Y%j" ) + \n 
startdate = datetime . strptime ( str ( read_zkey ( zdb , ) ) , "%Y%m%d%H%M%S" ) \n 
~~ ~~ if subdaily == False : \n 
~~ ~~ if end == detect : \n 
for dirname , dirnames , filenames in os . walk ( archiveLocation + + newest_year , followlinks = True ) : \n 
~~ ~~ ~~ enddate = max ( dates ) \n 
~~~ zdb = archiveLocation + + oldest_year + + fileNamePrefix + datetime . strftime ( enddate , "%Y%j" ) + \n 
enddate = datetime . strptime ( str ( read_zkey ( zdb , ) ) , "%Y%m%d%H%M%S" ) \n 
~~~ end = datetime . strftime ( enddate , "%Y-%m-%d" ) \n 
~~~ end = datetime . strftime ( enddate , "%Y-%m-%dT%H:%M:%SZ" ) \n 
~~ ~~ if has_zdb == True and has_period == False : \n 
~~~ time = start + + end \n 
~~~ time = start + + end + + period \n 
~~ print str ( time ) \n 
times . append ( time ) \n 
~~ return times \n 
~~ def generate_legend ( colormap , output , legend_url , orientation ) : \n 
if os . path . isfile ( output ) == False : \n 
cmd = + colormap + + output + + orientation \n 
~~~ run_command ( cmd , sigevent_url ) \n 
~~ except Exception , e : \n 
~~~ colormap_file = urllib . urlopen ( colormap ) \n 
last_modified = colormap_file . info ( ) . getheader ( "Last-Modified" ) \n 
colormap_file . close ( ) \n 
legend_time = datetime . fromtimestamp ( os . path . getmtime ( output ) ) \n 
if colormap_time > legend_time : \n 
run_command ( cmd , sigevent_url ) \n 
~~ ~~ except Exception , e : \n 
~~~ svg = open ( output , ) \n 
~~~ mssg = str ( ) . join ( [ , output ] ) \n 
log_sig_err ( mssg , sigevent_url ) \n 
~~ dom = xml . dom . minidom . parse ( svg ) \n 
svgElement = dom . getElementsByTagName ( ) [ 0 ] \n 
height = float ( svgElement . attributes [ ] . value . replace ( , ) ) * pt \n 
width = float ( svgElement . attributes [ ] . value . replace ( , ) ) * pt \n 
svg . close ( ) \n 
if orientation == : \n 
~~ return legend_url_template \n 
~~ def check_layer ( self ) : \n 
self . geom_field = False \n 
self . fields = { } \n 
ogr_fields = self . layer . fields \n 
ogr_field_types = self . layer . field_types \n 
def check_ogr_fld ( ogr_map_fld ) : \n 
~~~ idx = ogr_fields . index ( ogr_map_fld ) \n 
~~ return idx \n 
~~ for field_name , ogr_name in self . mapping . items ( ) : \n 
~~~ model_field = self . model . _meta . get_field ( field_name ) \n 
~~ except models . fields . FieldDoesNotExist : \n 
~~ fld_name = model_field . __class__ . __name__ \n 
if isinstance ( model_field , GeometryField ) : \n 
~~~ if self . geom_field : \n 
~~~ raise LayerMapError ( ) \n 
~~ coord_dim = model_field . dim \n 
~~~ if coord_dim == 3 : \n 
~~~ gtype = OGRGeomType ( ogr_name + ) \n 
~~~ gtype = OGRGeomType ( ogr_name ) \n 
~~ ~~ except OGRException : \n 
~~ ltype = self . layer . geom_type \n 
if not ( ltype . name . startswith ( gtype . name ) or self . make_multi ( ltype , model_field ) ) : \n 
~~~ raise LayerMapError ( \n 
% \n 
( fld_name , ( coord_dim == 3 and ) or , ltype ) ) \n 
~~ self . geom_field = field_name \n 
self . coord_dim = coord_dim \n 
fields_val = model_field \n 
~~ elif isinstance ( model_field , models . ForeignKey ) : \n 
~~~ if isinstance ( ogr_name , dict ) : \n 
~~~ rel_model = model_field . rel . to \n 
for rel_name , ogr_field in ogr_name . items ( ) : \n 
~~~ idx = check_ogr_fld ( ogr_field ) \n 
~~~ rel_field = rel_model . _meta . get_field ( rel_name ) \n 
( rel_name , rel_model . __class__ . __name__ ) ) \n 
~~ ~~ fields_val = rel_model \n 
~~~ raise TypeError ( ) \n 
~~~ if not model_field . __class__ in self . FIELD_TYPES : \n 
~~ idx = check_ogr_fld ( ogr_name ) \n 
ogr_field = ogr_field_types [ idx ] \n 
if not issubclass ( ogr_field , self . FIELD_TYPES [ model_field . __class__ ] ) : \n 
( ogr_field , ogr_field . __name__ , fld_name ) ) \n 
~~ fields_val = model_field \n 
~~ self . fields [ field_name ] = fields_val \n 
~~ ~~ def verify_fk ( self , feat , rel_model , rel_mapping ) : \n 
fk_kwargs = { } \n 
for field_name , ogr_name in rel_mapping . items ( ) : \n 
~~~ fk_kwargs [ field_name ] = self . verify_ogr_field ( feat [ ogr_name ] , rel_model . _meta . get_field ( field_name ) ) \n 
~~~ return rel_model . objects . get ( ** fk_kwargs ) \n 
~~~ raise MissingForeignKey ( % ( rel_model . __name__ , fk_kwargs ) ) \n 
~~ ~~ def save ( self , verbose = False , fid_range = False , step = False , \n 
progress = False , silent = False , stream = sys . stdout , strict = False ) : \n 
default_range = self . check_fid_range ( fid_range ) \n 
if progress : \n 
~~~ if progress is True or not isinstance ( progress , int ) : \n 
~~~ progress_interval = 1000 \n 
~~~ progress_interval = progress \n 
~~ ~~ @ self . transaction_decorator \n 
def _save ( feat_range = default_range , num_feat = 0 , num_saved = 0 ) : \n 
~~~ if feat_range : \n 
~~~ layer_iter = self . layer [ feat_range ] \n 
~~~ layer_iter = self . layer \n 
~~ for feat in layer_iter : \n 
~~~ num_feat += 1 \n 
~~~ kwargs = self . feature_kwargs ( feat ) \n 
~~ except LayerMapError , msg : \n 
~~~ if strict : raise \n 
elif not silent : \n 
~~~ stream . write ( % ( feat . fid , msg ) ) \n 
~~~ is_update = False \n 
if self . unique : \n 
~~~ u_kwargs = self . unique_kwargs ( kwargs ) \n 
m = self . model . objects . using ( self . using ) . get ( ** u_kwargs ) \n 
is_update = True \n 
geom = getattr ( m , self . geom_field ) . ogr \n 
new = OGRGeometry ( kwargs [ self . geom_field ] ) \n 
for g in new : geom . add ( g ) \n 
setattr ( m , self . geom_field , geom . wkt ) \n 
~~ except ObjectDoesNotExist : \n 
~~~ m = self . model ( ** kwargs ) \n 
~~~ m . save ( using = self . using ) \n 
num_saved += 1 \n 
if verbose : stream . write ( % ( is_update and or , m ) ) \n 
~~ except Exception , msg : \n 
~~~ if self . transaction_mode == : \n 
~~~ transaction . rollback_unless_managed ( ) \n 
~~ if strict : \n 
~~~ if not silent : \n 
~~~ stream . write ( % feat . fid ) \n 
stream . write ( % kwargs ) \n 
~~ elif not silent : \n 
~~~ stream . write ( % ( kwargs , msg ) ) \n 
~~ ~~ ~~ if progress and num_feat % progress_interval == 0 : \n 
~~~ stream . write ( % ( num_feat , num_saved ) ) \n 
~~ ~~ return num_saved , num_feat \n 
~~ nfeat = self . layer . num_feat \n 
if step and isinstance ( step , int ) and step < nfeat : \n 
~~~ if default_range : \n 
~~ beg , num_feat , num_saved = ( 0 , 0 , 0 ) \n 
indices = range ( step , nfeat , step ) \n 
n_i = len ( indices ) \n 
for i , end in enumerate ( indices ) : \n 
~~~ if i + 1 == n_i : step_slice = slice ( beg , None ) \n 
else : step_slice = slice ( beg , end ) \n 
~~~ num_feat , num_saved = _save ( step_slice , num_feat , num_saved ) \n 
beg = end \n 
~~~ stream . write ( % ( * 20 , step_slice ) ) \n 
raise \n 
~~~ _save ( ) \n 
~~ ~~ def set_status ( self , status_code , reason = None ) : \n 
self . _status_code = status_code \n 
if reason is not None : \n 
~~~ self . _reason = escape . native_str ( reason ) \n 
~~~ self . _reason = httputil . responses [ status_code ] \n 
~~ ~~ ~~ def decode_argument ( self , value , name = None ) : \n 
~~~ return _unicode ( value ) \n 
( name or "url" , value [ : 40 ] ) ) \n 
~~ ~~ def get_browser_locale ( self , default = "en_US" ) : \n 
if "Accept-Language" in self . request . headers : \n 
~~~ languages = self . request . headers [ "Accept-Language" ] . split ( "," ) \n 
locales = [ ] \n 
for language in languages : \n 
~~~ parts = language . strip ( ) . split ( ";" ) \n 
if len ( parts ) > 1 and parts [ 1 ] . startswith ( "q=" ) : \n 
~~~ score = float ( parts [ 1 ] [ 2 : ] ) \n 
~~ except ( __HOLE__ , TypeError ) : \n 
~~~ score = 0.0 \n 
~~~ score = 1.0 \n 
~~ locales . append ( ( parts [ 0 ] , score ) ) \n 
~~ if locales : \n 
~~~ locales . sort ( key = lambda pair : pair [ 1 ] , reverse = True ) \n 
codes = [ l [ 0 ] for l in locales ] \n 
return locale . get ( * codes ) \n 
~~ ~~ return locale . get ( default ) \n 
~~ def _decode_xsrf_token ( self , cookie ) : \n 
m = _signed_value_version_re . match ( utf8 ( cookie ) ) \n 
if m : \n 
~~~ version = int ( m . group ( 1 ) ) \n 
if version == 2 : \n 
~~~ _ , mask , masked_token , timestamp = cookie . split ( "|" ) \n 
mask = binascii . a2b_hex ( utf8 ( mask ) ) \n 
token = _websocket_mask ( \n 
mask , binascii . a2b_hex ( utf8 ( masked_token ) ) ) \n 
timestamp = int ( timestamp ) \n 
return version , token , timestamp \n 
~~~ return None , None , None \n 
~~~ version = 1 \n 
~~~ token = binascii . a2b_hex ( utf8 ( cookie ) ) \n 
~~ except ( binascii . Error , __HOLE__ ) : \n 
~~~ token = utf8 ( cookie ) \n 
~~ timestamp = int ( time . time ( ) ) \n 
return ( version , token , timestamp ) \n 
~~ ~~ def _load_ui_modules ( self , modules ) : \n 
~~~ if isinstance ( modules , types . ModuleType ) : \n 
~~~ self . _load_ui_modules ( dict ( ( n , getattr ( modules , n ) ) \n 
for n in dir ( modules ) ) ) \n 
~~ elif isinstance ( modules , list ) : \n 
~~~ for m in modules : \n 
~~~ self . _load_ui_modules ( m ) \n 
~~~ assert isinstance ( modules , dict ) \n 
for name , cls in modules . items ( ) : \n 
~~~ if issubclass ( cls , UIModule ) : \n 
~~~ self . ui_modules [ name ] = cls \n 
~~ ~~ ~~ ~~ def __getattr__ ( self , key ) : \n 
~~~ return self [ key ] \n 
~~~ raise AttributeError ( str ( e ) ) \n 
~~ ~~ def decode_signed_value ( secret , name , value , max_age_days = 31 , clock = None , min_version = None ) : \n 
~~~ if clock is None : \n 
~~~ clock = time . time \n 
~~ if min_version is None : \n 
~~~ min_version = DEFAULT_SIGNED_VALUE_MIN_VERSION \n 
~~ if min_version > 2 : \n 
~~ if not value : \n 
~~~ return None \n 
~~ value = utf8 ( value ) \n 
m = _signed_value_version_re . match ( value ) \n 
if m is None : \n 
if version > 999 : \n 
~~ ~~ if version < min_version : \n 
~~ if version == 1 : \n 
~~~ return _decode_signed_value_v1 ( secret , name , value , max_age_days , clock ) \n 
~~ elif version == 2 : \n 
~~~ return _decode_signed_value_v2 ( secret , name , value , max_age_days , clock ) \n 
~~ ~~ def _decode_signed_value_v2 ( secret , name , value , max_age_days , clock ) : \n 
~~~ def _consume_field ( s ) : \n 
~~~ length , _ , rest = s . partition ( ) \n 
n = int ( length ) \n 
field_value = rest [ : n ] \n 
if rest [ n : n + 1 ] != : \n 
~~ rest = rest [ n + 1 : ] \n 
return field_value , rest \n 
~~~ key_version , rest = _consume_field ( rest ) \n 
timestamp , rest = _consume_field ( rest ) \n 
name_field , rest = _consume_field ( rest ) \n 
value_field , rest = _consume_field ( rest ) \n 
~~ passed_sig = rest \n 
signed_string = value [ : - len ( passed_sig ) ] \n 
expected_sig = _create_signature_v2 ( secret , signed_string ) \n 
if not _time_independent_equals ( passed_sig , expected_sig ) : \n 
~~ if name_field != utf8 ( name ) : \n 
~~ timestamp = int ( timestamp ) \n 
if timestamp < clock ( ) - max_age_days * 86400 : \n 
~~~ return base64 . b64decode ( value_field ) \n 
~~ ~~ def __init__ ( self , * args , ** kwargs ) : \n 
~~~ super ( CachedFilesMixin , self ) . __init__ ( * args , ** kwargs ) \n 
~~~ self . cache = get_cache ( ) \n 
~~ except ( InvalidCacheBackendError , __HOLE__ ) : \n 
~~~ self . cache = default_cache \n 
~~ self . _patterns = SortedDict ( ) \n 
for extension , patterns in self . patterns : \n 
~~~ for pattern in patterns : \n 
~~~ compiled = re . compile ( pattern ) \n 
self . _patterns . setdefault ( extension , [ ] ) . append ( compiled ) \n 
~~ ~~ ~~ def hashed_name ( self , name , content = None ) : \n 
~~~ parsed_name = urlsplit ( unquote ( name ) ) \n 
clean_name = parsed_name . path \n 
if content is None : \n 
~~~ if not self . exists ( clean_name ) : \n 
( clean_name , self ) ) \n 
~~~ content = self . open ( clean_name ) \n 
~~~ return name \n 
~~ ~~ path , filename = os . path . split ( clean_name ) \n 
root , ext = os . path . splitext ( filename ) \n 
md5 = md5_constructor ( ) \n 
for chunk in content . chunks ( ) : \n 
~~~ md5 . update ( chunk ) \n 
~~ md5sum = md5 . hexdigest ( ) [ : 12 ] \n 
hashed_name = os . path . join ( path , u"%s.%s%s" % \n 
( root , md5sum , ext ) ) \n 
unparsed_name = list ( parsed_name ) \n 
unparsed_name [ 2 ] = hashed_name \n 
if in name and not unparsed_name [ 3 ] : \n 
~~~ unparsed_name [ 2 ] += \n 
~~ return urlunsplit ( unparsed_name ) \n 
~~ def _set_host_maintenance ( self , context , host_name , mode = True ) : \n 
"%(mode)s." ) , \n 
{ : host_name , : mode } ) \n 
~~~ result = self . api . set_host_maintenance ( context , host_name , mode ) \n 
~~~ common . raise_feature_not_supported ( ) \n 
~~ except exception . HostNotFound as e : \n 
~~~ raise webob . exc . HTTPNotFound ( explanation = e . format_message ( ) ) \n 
~~ except exception . ComputeServiceUnavailable as e : \n 
~~~ raise webob . exc . HTTPBadRequest ( explanation = e . format_message ( ) ) \n 
~~ if result not in ( "on_maintenance" , "off_maintenance" ) : \n 
~~~ raise webob . exc . HTTPBadRequest ( explanation = result ) \n 
~~ def _set_enabled_status ( self , context , host_name , enabled ) : \n 
if enabled : \n 
~~~ result = self . api . set_host_enabled ( context , host_name = host_name , \n 
enabled = enabled ) \n 
~~ if result not in ( "enabled" , "disabled" ) : \n 
~~ def _host_power_action ( self , req , host_name , action ) : \n 
context = req . environ [ ] \n 
authorize ( context ) \n 
~~~ result = self . api . host_power_action ( context , host_name = host_name , \n 
action = action ) \n 
~~ return { "host" : host_name , "power_action" : result } \n 
~~ def test_404 ( webapp ) : \n 
~~~ urlopen ( "%s/foo" % webapp . server . http . base ) \n 
~~~ assert e . code == 404 \n 
~~~ assert False \n 
~~ ~~ def test_forbidden ( webapp ) : \n 
~~~ urlopen ( "%s/test_forbidden" % webapp . server . http . base ) \n 
~~~ assert e . code == 403 \n 
assert e . msg == "Forbidden" \n 
~~ ~~ def test_notfound ( webapp ) : \n 
~~~ urlopen ( "%s/test_notfound" % webapp . server . http . base ) \n 
~~ ~~ def get_command ( self , ctx , name ) : \n 
~~~ if sys . version_info [ 0 ] == 2 : \n 
~~~ name = name . encode ( , ) \n 
~~ mod = __import__ ( + name , None , None , [ ] ) \n 
~~~ return \n 
~~ return mod . cli \n 
~~ @ cached_property \n 
~~~ def json ( self ) : \n 
if __debug__ : \n 
~~~ _assert_have_json ( ) \n 
~~ if self . mimetype in [ , ] : \n 
~~~ request_charset = self . mimetype_params . get ( ) \n 
~~~ if request_charset is not None : \n 
~~~ return json . loads ( self . data , encoding = request_charset ) \n 
~~ return json . loads ( self . data ) \n 
~~ except __HOLE__ as E : \n 
~~~ return self . on_json_loading_failed ( E ) \n 
~~ ~~ ~~ ~~ def get_random_object ( model ) : \n 
~~~ return _get_model ( model ) . _default_manager . order_by ( ) [ 0 ] \n 
~~ ~~ def updateAverageRating ( review_key ) : \n 
def _tx ( ) : \n 
~~~ review = review_key . get ( ) \n 
product = review . product_key . get ( ) \n 
if not review . rating_added : \n 
~~~ review . rating_added = True \n 
product . num_reviews += 1 \n 
product . avg_rating = ( product . avg_rating + \n 
( review . rating - product . avg_rating ) / float ( product . num_reviews ) ) \n 
product . needs_review_reindex = True \n 
ndb . put_multi ( [ product , review ] ) \n 
if not config . BATCH_RATINGS_UPDATE : \n 
~~~ defer ( \n 
models . Product . updateProdDocWithNewRating , \n 
product . key . id ( ) , _transactional = True ) \n 
~~ ~~ return ( product , review ) \n 
~~~ ndb . transaction ( _tx , xg = True ) \n 
~~~ logging . exception ( \n 
+ ) \n 
~~ ~~ def load_keypair ( ) : \n 
cfgdir = util . get_config_dir ( ) \n 
privname = os . path . join ( cfgdir , ) \n 
~~~ st = os . stat ( privname ) \n 
~~ if not stat . S_ISREG ( st . st_mode ) : \n 
~~ pubname = privname + \n 
~~~ st = os . stat ( pubname ) \n 
~~~ st = None \n 
~~ if st is None : \n 
~~ elif not stat . S_ISREG ( st . st_mode ) : \n 
~~ with file ( pubname ) as fin : \n 
~~~ pubkey = fin . read ( ) \n 
~~ keyparts = pubkey . strip ( ) . split ( ) \n 
pubkeys = env . api . get_pubkeys ( ) \n 
for pubkey in pubkeys : \n 
~~~ if pubkey [ ] == keyparts [ 2 ] : \n 
~~~ env . public_key = pubkey \n 
env . private_key_file = privname \n 
return pubkey \n 
~~ ~~ ~~ def getContext ( self ) : \n 
~~~ from twisted . internet import ssl \n 
~~~ self . ctx = None \n 
~~~ self . ctx = ssl . ClientContextFactory ( ) \n 
self . ctx . method = ssl . SSL . TLSv1_METHOD \n 
~~ ~~ def test_initgroupsInC ( self ) : \n 
calls = [ ] \n 
util . setgroups = calls . append \n 
~~~ util . initgroups ( os . getuid ( ) , os . getgid ( ) ) \n 
~~ self . assertFalse ( calls ) \n 
~~ def recvloop ( self ) : \n 
~~~ \n 
self . dorecvloop = True \n 
while self . dorecvloop : \n 
~~~ rlist = [ ] \n 
with self . servers_lock : \n 
~~~ for name in self . servers : \n 
~~~ ( h , p , sock ) = self . servers [ name ] \n 
if sock is not None : \n 
~~~ rlist . append ( sock . fileno ( ) ) \n 
~~ ~~ ~~ r , w , x = select . select ( rlist , [ ] , [ ] , 1.0 ) \n 
for sockfd in r : \n 
~~~ ( h , p , sock , name ) = self . getserverbysock ( sockfd ) \n 
~~ rcvlen = self . recv ( sock , h ) \n 
if rcvlen == 0 : \n 
~~~ if self . verbose : \n 
~~ self . servers [ name ] = ( h , p , None ) \n 
~~ ~~ ~~ ~~ def addnettunnel ( self , n ) : \n 
~~~ net = self . session . obj ( n ) \n 
~~ if isinstance ( net , EmaneNet ) : \n 
~~ if isinstance ( net , CtrlNet ) : \n 
~~~ if hasattr ( net , ) : \n 
~~~ if net . serverintf is not None : \n 
~~ ~~ ~~ servers = self . getserversbynode ( n ) \n 
if len ( servers ) < 2 : \n 
~~ hosts = [ ] \n 
for server in servers : \n 
~~~ ( host , port , sock ) = self . getserver ( server ) \n 
if host is None : \n 
~~ hosts . append ( host ) \n 
~~ if len ( hosts ) == 0 : \n 
~~~ self . session . _handlerslock . acquire ( ) \n 
for h in self . session . _handlers : \n 
~~~ if h . client_address != "" : \n 
~~~ hosts . append ( h . client_address [ 0 ] ) \n 
~~ ~~ self . session . _handlerslock . release ( ) \n 
~~ r = [ ] \n 
for host in hosts : \n 
~~~ if self . myip : \n 
~~~ myip = self . myip \n 
~~~ myip = host \n 
~~ key = self . tunnelkey ( n , IPAddr . toint ( myip ) ) \n 
if key in self . tunnels . keys ( ) : \n 
gt = GreTap ( node = None , name = None , session = self . session , \n 
remoteip = host , key = key ) \n 
self . tunnels [ key ] = gt \n 
r . append ( gt ) \n 
net . attach ( gt ) \n 
~~ return r \n 
~~ def deltunnel ( self , n1num , n2num ) : \n 
key = self . tunnelkey ( n1num , n2num ) \n 
~~~ gt = self . tunnels . pop ( key ) \n 
~~~ gt = None \n 
~~ if gt : \n 
~~~ self . session . delobj ( gt . objid ) \n 
del gt \n 
~~ ~~ def handlenodemsg ( self , msg ) : \n 
serverlist = [ ] \n 
handle_locally = False \n 
serverfiletxt = None \n 
n = msg . tlvdata [ coreapi . CORE_TLV_NODE_NUMBER ] \n 
nodetype = msg . gettlv ( coreapi . CORE_TLV_NODE_TYPE ) \n 
if nodetype is not None : \n 
~~~ nodecls = coreapi . node_class ( nodetype ) \n 
return ( False , serverlist ) \n 
~~ if nodecls is None : \n 
~~ if issubclass ( nodecls , PyCoreNet ) and nodetype != coreapi . CORE_NODE_WLAN : \n 
~~~ serverlist = self . getserverlist ( ) \n 
handle_locally = True \n 
self . addnet ( n ) \n 
for server in serverlist : \n 
~~~ self . addnodemap ( server , n ) \n 
~~ return ( handle_locally , serverlist ) \n 
~~ if issubclass ( nodecls , PyCoreNet ) and nodetype == coreapi . CORE_NODE_WLAN : \n 
~~~ if msg . gettlv ( coreapi . CORE_TLV_NODE_EMUSRV ) is not None : \n 
~~~ self . incrbootcount ( ) \n 
~~ ~~ elif issubclass ( nodecls , PyCoreNode ) : \n 
~~~ name = msg . gettlv ( coreapi . CORE_TLV_NODE_NAME ) \n 
if name : \n 
~~ if issubclass ( nodecls , PhysicalNode ) : \n 
~~~ self . addphys ( n ) \n 
~~ ~~ ~~ server = msg . gettlv ( coreapi . CORE_TLV_NODE_EMUSRV ) \n 
if server is not None : \n 
if server not in serverlist : \n 
~~~ serverlist . append ( server ) \n 
~~ if serverfiletxt and self . session . master : \n 
~~~ self . writenodeserver ( serverfiletxt , server ) \n 
~~ ~~ if n in self . phys : \n 
~~~ self . session . mobility . physnodeupdateposition ( msg ) \n 
~~ def forwardmsg ( self , msg , serverlist , handle_locally ) : \n 
~~ if host is None and port is None : \n 
~~~ handle_locally = True \n 
~~~ if sock is None : \n 
~~~ sock . send ( msg . rawmsg ) \n 
~~ ~~ ~~ return handle_locally \n 
~~ def writenodeserver ( self , nodestr , server ) : \n 
( host , port , sock ) = self . getserver ( server ) \n 
name = nodestr . split ( ) [ 1 ] \n 
dirname = os . path . join ( self . session . sessiondir , name + ".conf" ) \n 
filename = os . path . join ( dirname , "server" ) \n 
~~~ os . makedirs ( dirname ) \n 
~~~ f = open ( filename , "w" ) \n 
f . write ( "%s\\n%s\\n" % ( serverstr , nodestr ) ) \n 
f . close ( ) \n 
return True \n 
self . session . warn ( msg ) \n 
return False \n 
~~ ~~ def bubble_sort ( items ) : \n 
~~~ num_items = len ( items ) \n 
if num_items < 2 : \n 
~~~ return items \n 
~~ while num_items > 0 : \n 
~~~ for k in range ( num_items ) : \n 
~~~ if items [ k ] > items [ k + 1 ] : \n 
~~~ copy = items [ k ] \n 
copy_next = items [ k + 1 ] \n 
items [ k ] = copy_next \n 
items [ k + 1 ] = copy \n 
~~ elif items [ k ] == items [ k + 1 ] : \n 
~~ ~~ num_items -= 1 \n 
~~ return items \n 
~~ def __run_tx ( self ) : \n 
~~~ buf = \n 
first_time = True \n 
while True : \n 
~~~ if self . __owner . _shutdown : \n 
~~ if self . __reconnect : \n 
~~ if len ( buf ) == 0 : \n 
~~~ data , addr = None , None \n 
if not self . __reconnect : \n 
~~~ self . __owner . _wi_available . acquire ( ) \n 
while len ( self . __owner . _wi ) == 0 and not self . __reconnect : \n 
~~~ self . __owner . _wi_available . wait ( ) \n 
if self . __owner . _shutdown : \n 
~~~ os . close ( self . fileno ( ) ) \n 
self . __owner . _wi_available . release ( ) \n 
return \n 
~~ ~~ if len ( self . __owner . _wi ) > 0 : \n 
~~~ data , addr , laddress = self . __owner . _wi . pop ( 0 ) \n 
~~ self . __owner . _wi_available . release ( ) \n 
~~~ if not first_time : \n 
~~~ time . sleep ( 0.1 ) \n 
~~~ self . connect ( jid = iksemel . JID ( ) , tls = False , port = 22223 ) \n 
first_time = False \n 
pollobj = poll ( ) \n 
pollobj . register ( self . fileno ( ) , POLLOUT ) \n 
~~ except iksemel . StreamError : \n 
~~~ traceback . print_exc ( file = sys . stdout ) \n 
sys . stdout . flush ( ) \n 
~~ self . __reconnect = False \n 
self . __reconnect_count += 1 \n 
~~ if data == None : \n 
~~ dst_addr , dst_port = addr \n 
laddress [ 0 ] , laddress [ 1 ] , \n 
base64 . b64encode ( data ) ) \n 
~~ if self . __owner . _shutdown : \n 
~~ pollret = dict ( pollobj . poll ( ) ) \n 
if pollret . get ( self . fileno ( ) , 0 ) & POLLOUT == 0 : \n 
~~~ sent = os . write ( self . fileno ( ) , buf ) \n 
buf = buf [ sent : ] \n 
~~~ self . __reconnect = True \n 
~~ ~~ ~~ def test_avg_std ( self ) : \n 
~~~ g = random . Random ( ) \n 
N = 5000 \n 
xx = [ i / float ( N ) for i in xrange ( 1 , N ) ] \n 
dists = [ \n 
( g . uniform , ( 1.0 , 10.0 ) , ( 10.0 + 1.0 ) / 2 , ( 10.0 - 1.0 ) ** 2 / 12 ) , \n 
( g . expovariate , ( 1.5 , ) , 1 / 1.5 , 1 / 1.5 ** 2 ) , \n 
( g . paretovariate , ( 5.0 , ) , 5.0 / ( 5.0 - 1 ) , \n 
5.0 / ( ( 5.0 - 1 ) ** 2 * ( 5.0 - 2 ) ) ) , \n 
( g . weibullvariate , ( 1.0 , 3.0 ) , gamma ( 1 + 1 / 3.0 ) , \n 
gamma ( 1 + 2 / 3.0 ) - gamma ( 1 + 1 / 3.0 ) ** 2 ) ] \n 
if hasattr ( g , ) : \n 
~~~ dists . append ( ( g . triangular , ( 0.0 , 1.0 , 1.0 / 3.0 ) , 4.0 / 9.0 , 7.0 / 9.0 / 18.0 ) ) \n 
~~ for variate , args , mu , sigmasqrd in dists : \n 
~~~ x = xx [ : ] \n 
g . random = getattr ( x , ) \n 
y = [ ] \n 
for i in xrange ( len ( x ) ) : \n 
~~~ y . append ( variate ( * args ) ) \n 
~~ ~~ s1 = s2 = 0 \n 
for e in y : \n 
~~~ s1 += e \n 
s2 += ( e - mu ) ** 2 \n 
~~ N = len ( y ) \n 
self . assertAlmostEqual ( s1 / N , mu , 2 ) \n 
self . assertAlmostEqual ( s2 / ( N - 1 ) , sigmasqrd , 2 ) \n 
~~ ~~ def __unicode__ ( self ) : \n 
~~~ return unicode ( self . _make_body ( ) , self . requested_encoding , "strict" ) \n 
~~ except __HOLE__ as err : \n 
~~~ raise EncodingError ( str ( err ) ) \n 
~~ ~~ def _make_status ( self ) : \n 
~~~ line = self . _responseline \n 
~~~ [ version , status , reason ] = line . split ( None , 2 ) \n 
~~~ [ version , status ] = line . split ( None , 1 ) \n 
reason = "" \n 
~~~ raise BadStatusLineError ( line ) \n 
~~~ status = int ( status ) \n 
if status < 100 or status > 999 : \n 
~~~ raise BadStatusLine ( line ) \n 
~~~ version = float ( version . split ( "/" ) [ 1 ] ) \n 
~~ except ( IndexError , ValueError ) : \n 
~~~ version = 0.9 \n 
~~ reason = reason . strip ( ) \n 
reason = reason or httputils . STATUSCODES . get ( status , "" ) \n 
self . _responseline = None \n 
return ResponseLine ( version , status , reason ) \n 
~~ def consume_msg ( self , body , msg ) : \n 
~~~ callback = getattr ( self . receiver , body [ "method" ] ) \n 
~~~ callback ( ** body [ "args" ] ) \n 
~~ except errors . CannotFindTask as e : \n 
~~~ logger . warn ( str ( e ) ) \n 
msg . ack ( ) \n 
~~~ logger . error ( traceback . format_exc ( ) ) \n 
msg . requeue ( ) \n 
~~~ db . commit ( ) \n 
~~~ db . remove ( ) \n 
~~ ~~ def run ( ) : \n 
~~~ logger = logs . prepare_submodule_logger ( , \n 
settings . RPC_CONSUMER_LOG_PATH ) \n 
with Connection ( rpc . conn_str ) as conn : \n 
~~~ RPCConsumer ( conn , NailgunReceiver ) . run ( ) \n 
~~ except ( __HOLE__ , SystemExit ) : \n 
~~ ~~ ~~ def require_oauth ( self , realm = None , require_resource_owner = True , \n 
require_verifier = False , require_realm = False ) : \n 
def decorator ( f ) : \n 
~~~ @ wraps ( f ) \n 
def verify_request ( * args , ** kwargs ) : \n 
~~~ if request . form : \n 
~~~ body = request . form . to_dict ( ) \n 
~~~ body = request . data . decode ( "utf-8" ) \n 
~~ verify_result = self . verify_request ( request . url . decode ( "utf-8" ) , \n 
http_method = request . method . decode ( "utf-8" ) , \n 
body = body , \n 
headers = request . headers , \n 
require_resource_owner = require_resource_owner , \n 
require_verifier = require_verifier , \n 
require_realm = require_realm or bool ( realm ) , \n 
required_realm = realm ) \n 
valid , oauth_request = verify_result \n 
if valid : \n 
~~~ request . oauth = self . collect_request_parameters ( request ) \n 
token = { } \n 
if require_verifier : \n 
~~~ token [ ] = request . oauth . resource_owner_key \n 
~~ self . save_timestamp_and_nonce ( request . oauth . client_key , \n 
request . oauth . timestamp , request . oauth . nonce , \n 
** token ) \n 
return f ( * args , ** kwargs ) \n 
~~~ raise Unauthorized ( ) \n 
~~ ~~ except __HOLE__ as err : \n 
~~~ raise BadRequest ( err . message ) \n 
~~ ~~ return verify_request \n 
~~ return decorator \n 
~~ def __init__ ( self , options = None ) : \n 
~~~ self . document = getattr ( options , ) \n 
~~~ self . document . collection \n 
~~ except AttributeError : \n 
~~ self . fields = getattr ( options , , None ) \n 
self . exclude = getattr ( options , , None ) \n 
~~ def __new__ ( cls , name , bases , attrs ) : \n 
~~~ formfield_callback = attrs . pop ( , None ) \n 
~~~ parents = [ b for b in bases if issubclass ( b , DocumentForm ) ] \n 
~~~ parents = None \n 
~~ declared_fields = get_declared_fields ( bases , attrs , False ) \n 
new_class = super ( DocumentFormMetaclass , cls ) . __new__ ( cls , name , bases , \n 
attrs ) \n 
if not parents : \n 
~~~ return new_class \n 
~~ opts = new_class . _meta = DocumentFormOptions ( \n 
getattr ( new_class , , None ) \n 
if opts . document : \n 
~~~ fields = fields_for_document ( opts . document , opts . fields , \n 
opts . exclude , formfield_callback ) \n 
fields . update ( declared_fields ) \n 
~~~ fields = declared_fields \n 
~~ new_class . declared_fields = declared_fields \n 
new_class . base_fields = fields \n 
return new_class \n 
~~ def __init__ ( self , data = None , files = None , auto_id = , prefix = None , \n 
initial = None , error_class = ErrorList , label_suffix = , \n 
empty_permitted = False , instance = None , \n 
collection = None ) : \n 
~~~ opts = self . _meta \n 
if instance is None : \n 
~~~ if collection is None : \n 
"DocumentForm" ) \n 
~~ self . instance = opts . document ( collection = collection ) \n 
object_data = { } \n 
~~~ self . instance = instance \n 
~~~ self . instance . collection \n 
~~ object_data = document_to_dict ( instance , opts . fields , opts . exclude ) \n 
~~ if initial is not None : \n 
~~~ object_data . update ( initial ) \n 
~~ super ( BaseDocumentForm , self ) . __init__ ( \n 
data , files , auto_id , prefix , object_data , \n 
error_class , label_suffix , empty_permitted \n 
~~ def documentform_factory ( document , form = DocumentForm , \n 
fields = None , exclude = None , \n 
formfield_callback = None ) : \n 
~~~ document . collection \n 
~~ attrs = { : document } \n 
if fields is not None : \n 
~~~ attrs [ ] = fields \n 
~~ if exclude is not None : \n 
~~~ attrs [ ] = exclude \n 
~~ parent = ( object , ) \n 
if hasattr ( form , ) : \n 
~~~ parent = ( form . Meta , object ) \n 
~~ Meta = type ( , parent , attrs ) \n 
class_name = % document . __name__ \n 
form_class_attrs = { \n 
: Meta , \n 
: formfield_callback \n 
} \n 
return DocumentFormMetaclass ( class_name , ( form , ) , form_class_attrs ) \n 
~~ def update ( self ) : \n 
~~~ self . _state = self . wemo . get_state ( True ) \n 
~~~ _LOGGER . warning ( , self . name ) \n 
~~ ~~ @ extensions . expected_errors ( 400 ) \n 
~~~ def show ( self , req , id ) : \n 
~~~ context = req . environ [ ] \n 
~~~ if in id : \n 
~~~ before_date = datetime . datetime . strptime ( str ( id ) , \n 
raise webob . exc . HTTPBadRequest ( explanation = msg ) \n 
~~ task_log = self . _get_audit_task_logs ( context , \n 
before = before_date ) \n 
return { : task_log } \n 
~~ ~~ def _run_autocomplete ( self ) : \n 
~~~ util = ManagementUtility ( argv = sys . argv ) \n 
~~~ util . autocomplete ( ) \n 
~~ return self . output . getvalue ( ) . strip ( ) . split ( ) \n 
~~ def train ( config , level_name = None , timestamp = None , time_budget = None , \n 
verbose_logging = None , debug = None ) : \n 
train_obj = serial . load_train_file ( config ) \n 
~~~ iter ( train_obj ) \n 
iterable = True \n 
~~~ iterable = False \n 
~~ restore_defaults ( ) \n 
root_logger = logging . getLogger ( ) \n 
if verbose_logging : \n 
"%(message)s" ) \n 
handler = CustomStreamHandler ( formatter = formatter ) \n 
~~~ if timestamp : \n 
~~~ prefix = \n 
~~ formatter = CustomFormatter ( prefix = prefix , only_from = ) \n 
~~ root_logger . addHandler ( handler ) \n 
if debug : \n 
~~~ root_logger . setLevel ( logging . DEBUG ) \n 
~~~ root_logger . setLevel ( logging . INFO ) \n 
~~ if iterable : \n 
~~~ for number , subobj in enumerate ( iter ( train_obj ) ) : \n 
~~~ phase_variable = \n 
phase_value = % ( number + 1 ) \n 
os . environ [ phase_variable ] = phase_value \n 
subobj . main_loop ( time_budget = time_budget ) \n 
del subobj \n 
gc . collect ( ) \n 
~~~ train_obj . main_loop ( time_budget = time_budget ) \n 
~~ ~~ def _import_c_scanstring ( ) : \n 
~~~ from simplejson . _speedups import scanstring \n 
return scanstring \n 
~~ ~~ def py_scanstring ( s , end , encoding = None , strict = True , \n 
_b = BACKSLASH , _m = STRINGCHUNK . match ) : \n 
if encoding is None : \n 
~~~ encoding = DEFAULT_ENCODING \n 
~~ chunks = [ ] \n 
_append = chunks . append \n 
begin = end - 1 \n 
~~~ chunk = _m ( s , end ) \n 
if chunk is None : \n 
~~~ raise JSONDecodeError ( \n 
~~ end = chunk . end ( ) \n 
content , terminator = chunk . groups ( ) \n 
if content : \n 
~~~ if not isinstance ( content , unicode ) : \n 
~~~ content = unicode ( content , encoding ) \n 
~~ _append ( content ) \n 
~~ if terminator == \'"\' : \n 
~~ elif terminator != : \n 
~~~ if strict : \n 
raise JSONDecodeError ( msg , s , end ) \n 
~~~ _append ( terminator ) \n 
~~~ esc = s [ end ] \n 
~~ if esc != : \n 
~~~ char = _b [ esc ] \n 
~~ end += 1 \n 
~~~ esc = s [ end + 1 : end + 5 ] \n 
next_end = end + 5 \n 
if len ( esc ) != 4 : \n 
~~ uni = int ( esc , 16 ) \n 
if 0xd800 <= uni <= 0xdbff and sys . maxunicode > 65535 : \n 
if not s [ end + 5 : end + 7 ] == : \n 
~~~ raise JSONDecodeError ( msg , s , end ) \n 
~~ esc2 = s [ end + 7 : end + 11 ] \n 
if len ( esc2 ) != 4 : \n 
~~ uni2 = int ( esc2 , 16 ) \n 
uni = 0x10000 + ( ( ( uni - 0xd800 ) << 10 ) | ( uni2 - 0xdc00 ) ) \n 
next_end += 6 \n 
~~ char = unichr ( uni ) \n 
end = next_end \n 
~~ _append ( char ) \n 
~~ return . join ( chunks ) , end \n 
~~ def JSONObject ( ( s , end ) , encoding , strict , scan_once , object_hook , \n 
object_pairs_hook , memo = None , \n 
_w = WHITESPACE . match , _ws = WHITESPACE_STR ) : \n 
~~~ if memo is None : \n 
~~~ memo = { } \n 
~~ memo_get = memo . setdefault \n 
pairs = [ ] \n 
nextchar = s [ end : end + 1 ] \n 
if nextchar != \'"\' : \n 
~~~ if nextchar in _ws : \n 
~~~ end = _w ( s , end ) . end ( ) \n 
~~ if nextchar == : \n 
~~~ if object_pairs_hook is not None : \n 
~~~ result = object_pairs_hook ( pairs ) \n 
return result , end + 1 \n 
~~ pairs = { } \n 
if object_hook is not None : \n 
~~~ pairs = object_hook ( pairs ) \n 
~~ return pairs , end + 1 \n 
~~ elif nextchar != \'"\' : \n 
~~ ~~ end += 1 \n 
~~~ key , end = scanstring ( s , end , encoding , strict ) \n 
key = memo_get ( key , key ) \n 
if s [ end : end + 1 ] != : \n 
~~~ if s [ end ] in _ws : \n 
~~~ end += 1 \n 
if s [ end ] in _ws : \n 
~~~ end = _w ( s , end + 1 ) . end ( ) \n 
~~~ value , end = scan_once ( s , end ) \n 
~~ pairs . append ( ( key , value ) ) \n 
~~~ nextchar = s [ end ] \n 
if nextchar in _ws : \n 
nextchar = s [ end ] \n 
~~~ nextchar = \n 
if nextchar == : \n 
~~ elif nextchar != : \n 
~~ ~~ if object_pairs_hook is not None : \n 
return result , end \n 
~~ pairs = dict ( pairs ) \n 
~~ return pairs , end \n 
~~ def JSONArray ( ( s , end ) , scan_once , _w = WHITESPACE . match , _ws = WHITESPACE_STR ) : \n 
~~~ values = [ ] \n 
~~~ return values , end + 1 \n 
~~ _append = values . append \n 
~~ except StopIteration : \n 
~~ _append ( value ) \n 
~~ ~~ ~~ except __HOLE__ : \n 
~~ ~~ return values , end \n 
~~ def raw_decode ( self , s , idx = 0 ) : \n 
~~~ obj , end = self . scan_once ( s , idx ) \n 
~~ return obj , end \n 
~~ def execute ( self , sql , params = None ) : \n 
~~~ return self . cursor . execute ( sql , params ) \n 
~~ except __HOLE__ , e : \n 
~~~ if params is None : \n 
~~~ return self . cursor . execute ( sql , ) \n 
~~~ raise ValueError ( e ) \n 
~~ ~~ ~~ def _get_cache_class ( self , import_path = None ) : \n 
~~~ dot = import_path . rindex ( ) \n 
~~ module , classname = import_path [ : dot ] , import_path [ dot + 1 : ] \n 
~~~ mod = import_module ( module ) \n 
~~~ return getattr ( mod , classname ) \n 
~~ ~~ def verify ( self , * args , ** kwargs ) : \n 
~~~ if "code" in kwargs : \n 
~~~ code = kwargs [ "code" ] \n 
~~~ entry = self . codes [ code ] \n 
~~~ return self . FAILED_AUTHN \n 
~~ username = entry [ "username" ] \n 
now = time . time ( ) \n 
if now - entry [ "time" ] > self . code_ttl : \n 
return username , True \n 
~~~ result = self . first_factor . verify ( * args , ** kwargs ) \n 
if result == self . FAILED_AUTHN : \n 
~~ username , _ = result \n 
~~~ receiver = self . user_db [ username ] [ "email" ] \n 
~~~ self . FAILED_AUTHN \n 
~~ code = hashlib . md5 ( str ( time . time ( ) ) ) . hexdigest ( ) \n 
self . codes [ code ] = { "username" : username , "time" : time . time ( ) } \n 
self . _send_mail ( code , receiver ) \n 
template = self . template_env . get_template ( self . template ) \n 
response = Response ( template . render ( mail = receiver , \n 
action = self . url_endpoint , \n 
state = json . dumps ( \n 
kwargs [ "state" ] ) ) ) \n 
return response , False \n 
~~ ~~ def get_chromosome_priority ( chrom , chrom_dict = { } ) : \n 
priority = 0 \n 
chrom = str ( chrom ) . lstrip ( ) \n 
if chrom_dict : \n 
~~~ priority = chrom_dict . get ( chrom , 0 ) \n 
~~~ if int ( chrom ) < 23 : \n 
~~~ priority = int ( chrom ) \n 
~~~ if chrom == : \n 
~~~ priority = 23 \n 
~~ elif chrom == : \n 
~~~ priority = 24 \n 
~~~ priority = 25 \n 
~~~ priority = 26 \n 
~~ ~~ ~~ return str ( priority ) \n 
~~~ from sqlalchemy import create_engine \n 
self . _create_sql_engine = create_engine \n 
~~~ self . _SQLALCHEMY_INSTALLED = False \n 
~~~ self . _SQLALCHEMY_INSTALLED = True \n 
~~ super ( TestCompression , self ) . setUp ( ) \n 
data = { \n 
: np . arange ( 1000 , dtype = np . float64 ) , \n 
: np . arange ( 1000 , dtype = np . int32 ) , \n 
: list ( 100 * ) , \n 
: date_range ( datetime . datetime ( 2015 , 4 , 1 ) , periods = 1000 ) , \n 
: [ datetime . timedelta ( days = x ) for x in range ( 1000 ) ] , \n 
self . frame = { \n 
: DataFrame ( dict ( ( k , data [ k ] ) for k in [ , ] ) ) , \n 
: DataFrame ( data ) , \n 
~~ def compare ( self , vf , version ) : \n 
~~~ if LooseVersion ( version ) < : \n 
~~~ data = read_msgpack ( vf , encoding = ) \n 
~~~ data = read_msgpack ( vf ) \n 
~~ self . check_min_structure ( data ) \n 
for typ , dv in data . items ( ) : \n 
~~~ assert typ in self . all_data , ( \n 
. format ( typ ) ) \n 
for dt , result in dv . items ( ) : \n 
~~~ expected = self . data [ typ ] [ dt ] \n 
~~ comparator = getattr ( \n 
self , "compare_{typ}_{dt}" . format ( typ = typ , dt = dt ) , None ) \n 
if comparator is not None : \n 
~~~ comparator ( result , expected , typ , version ) \n 
~~~ check_arbitrary ( result , expected ) \n 
~~ ~~ ~~ return data \n 
~~ def read_msgpacks ( self , version ) : \n 
~~~ pth = tm . get_data_path ( . format ( str ( version ) ) ) \n 
n = 0 \n 
for f in os . listdir ( pth ) : \n 
~~~ if ( compat . PY3 and \n 
version . startswith ( ) and \n 
f . split ( ) [ - 4 ] [ - 1 ] == ) : \n 
~~ vf = os . path . join ( pth , f ) \n 
~~~ self . compare ( vf , version ) \n 
~~ n += 1 \n 
~~ assert n > 0 , \n 
~~ def create_keypair ( conn ) : \n 
~~~ keypair = conn . compute . find_keypair ( KEYPAIR_NAME ) \n 
if not keypair : \n 
keypair = conn . compute . create_keypair ( name = KEYPAIR_NAME ) \n 
print ( keypair ) \n 
~~~ os . mkdir ( SSH_DIR ) \n 
~~~ if e . errno != errno . EEXIST : \n 
~~~ raise e \n 
~~ ~~ with open ( PRIVATE_KEYPAIR_FILE , ) as f : \n 
~~~ f . write ( "%s" % keypair . private_key ) \n 
~~ os . chmod ( PRIVATE_KEYPAIR_FILE , 0o400 ) \n 
~~ return keypair \n 
~~ @ classmethod \n 
~~~ def _check_geo_field ( cls , opts , lookup ) : \n 
from django . contrib . gis . db . models . fields import GeometryField \n 
field_list = lookup . split ( LOOKUP_SEP ) \n 
field_list . reverse ( ) \n 
fld_name = field_list . pop ( ) \n 
~~~ geo_fld = opts . get_field ( fld_name ) \n 
while len ( field_list ) : \n 
~~~ opts = geo_fld . remote_field . model . _meta \n 
geo_fld = opts . get_field ( field_list . pop ( ) ) \n 
~~ ~~ except ( FieldDoesNotExist , __HOLE__ ) : \n 
~~ if isinstance ( geo_fld , GeometryField ) : \n 
~~~ return geo_fld \n 
~~ ~~ ~~ def _get_phylogenetic_kwargs ( counts , ** kwargs ) : \n 
~~~ otu_ids = kwargs . pop ( ) \n 
"metrics." ) \n 
~~~ tree = kwargs . pop ( ) \n 
~~ return otu_ids , tree , kwargs \n 
~~ def isIterable ( obj ) : \n 
if isinstance ( obj , basestring ) : return False \n 
elif isinstance ( obj , ProxyUnicode ) : return False \n 
~~~ iter ( obj ) \n 
~~ except __HOLE__ : return False \n 
else : return True \n 
~~ def reorder ( x , indexList = [ ] , indexDict = { } ) : \n 
x = list ( x ) \n 
num = len ( x ) \n 
popCount = 0 \n 
indexValDict = { } \n 
for i , index in enumerate ( indexList ) : \n 
~~~ if index is not None : \n 
~~~ val = x . pop ( index - popCount ) \n 
assert index not in indexDict , indexDict \n 
indexValDict [ i ] = val \n 
popCount += 1 \n 
~~ ~~ for k , v in indexDict . items ( ) : \n 
~~~ indexValDict [ v ] = x . pop ( k - popCount ) \n 
~~ newlist = [ ] \n 
for i in range ( num ) : \n 
~~~ val = indexValDict [ i ] \n 
~~~ val = x . pop ( 0 ) \n 
~~ newlist . append ( val ) \n 
~~ return newlist \n 
~~ def getCascadingDictItem ( dict , keys , default = { } ) : \n 
~~~ currentDict = dict \n 
for key in keys [ : - 1 ] : \n 
~~~ if isMapping ( currentDict ) and key not in currentDict : \n 
~~~ currentDict [ key ] = { } \n 
~~ currentDict = currentDict [ key ] \n 
~~~ return currentDict [ keys [ - 1 ] ] \n 
~~ ~~ def izip_longest ( * args , ** kwds ) : \n 
~~~ fillvalue = kwds . get ( ) \n 
def sentinel ( counter = ( [ fillvalue ] * ( len ( args ) - 1 ) ) . pop ) : \n 
~~ fillers = itertools . repeat ( fillvalue ) \n 
iters = [ itertools . chain ( it , sentinel ( ) , fillers ) for it in args ] \n 
~~~ for tup in itertools . izip ( * iters ) : \n 
~~~ yield tup \n 
~~ ~~ @ app . route ( , defaults = { : } ) \n 
@ app . route ( ) \n 
def page_handler ( path ) : \n 
~~~ if not path . endswith ( "/" ) : \n 
~~~ return redirect ( path + "/" ) \n 
~~ elif not path . startswith ( "/" ) : \n 
~~~ path = "/" + path \n 
~~ domain = request . host . split ( ":" ) [ 0 ] \n 
site_nickname = app . config [ "SITE_CONFIGURATION" ] . get ( domain , None ) \n 
if site_nickname is None : \n 
~~~ site_nickname = domain \n 
~~ template_directory = os . path . join ( app . root_path , "templates/" , site_nickname ) \n 
~~~ template_name = app . config [ "URLS" ] [ site_nickname ] [ path ] [ 0 ] \n 
~~~ filepath = path [ 1 : - 1 ] \n 
paths = [ os . path . join ( filepath + "index" ) ] \n 
if filepath : \n 
~~~ paths . append ( filepath ) \n 
~~ filenames = [ ] \n 
for filename in paths : \n 
~~~ filenames . append ( filename + ".htm" ) \n 
filenames . append ( filename + ".html" ) \n 
~~~ if os . path . exists ( os . path . join ( template_directory , filename ) ) : \n 
~~~ template_name = filename \n 
~~~ abort ( 404 ) \n 
~~ ~~ app . jinja_loader = FileSystemLoader ( template_directory ) \n 
media_url = "/media/%s/" % site_nickname \n 
return render_template ( template_name , ** { "MEDIA_URL" : media_url , "url" : reverse_url ( site_nickname ) , "debug" : app . config . get ( "DEBUG" , False ) } ) \n 
~~ def _profile_default ( self ) : \n 
~~~ if BaseIPythonApplication . initialized ( ) : \n 
~~~ return BaseIPythonApplication . instance ( ) . profile \n 
~~ except ( __HOLE__ , MultipleInstanceError ) : \n 
~~ ~~ def __init__ ( self , url_or_file = None , profile = None , profile_dir = None , ipython_dir = None , \n 
context = None , debug = False , exec_key = None , \n 
sshserver = None , sshkey = None , password = None , paramiko = None , \n 
timeout = 10 , ** extra_args \n 
) : \n 
~~~ if profile : \n 
~~~ super ( Client , self ) . __init__ ( debug = debug , profile = profile ) \n 
~~~ super ( Client , self ) . __init__ ( debug = debug ) \n 
~~ if context is None : \n 
~~~ context = zmq . Context . instance ( ) \n 
~~ self . _context = context \n 
self . _setup_profile_dir ( self . profile , profile_dir , ipython_dir ) \n 
if self . _cd is not None : \n 
~~~ if url_or_file is None : \n 
~~~ url_or_file = pjoin ( self . _cd . security_dir , ) \n 
~~~ util . validate_url ( url_or_file ) \n 
~~~ if not os . path . exists ( url_or_file ) : \n 
~~~ if self . _cd : \n 
~~~ url_or_file = os . path . join ( self . _cd . security_dir , url_or_file ) \n 
~~ with open ( url_or_file ) as f : \n 
~~~ cfg = json . loads ( f . read ( ) ) \n 
~~~ cfg = { : url_or_file } \n 
~~ if sshserver : \n 
~~~ cfg [ ] = sshserver \n 
~~ if exec_key : \n 
~~~ cfg [ ] = exec_key \n 
~~ exec_key = cfg [ ] \n 
location = cfg . setdefault ( , None ) \n 
cfg [ ] = util . disambiguate_url ( cfg [ ] , location ) \n 
url = cfg [ ] \n 
proto , addr , port = util . split_url ( url ) \n 
if location is not None and addr == : \n 
~~~ if location not in LOCAL_IPS and not sshserver : \n 
~~~ sshserver = cfg [ ] \n 
~~ if location not in LOCAL_IPS and not sshserver : \n 
RuntimeWarning ) \n 
~~ ~~ elif not sshserver : \n 
~~ self . _config = cfg \n 
self . _ssh = bool ( sshserver or sshkey or password ) \n 
if self . _ssh and sshserver is None : \n 
~~~ sshserver = url . split ( ) [ 1 ] . split ( ) [ 0 ] \n 
~~ if self . _ssh and password is None : \n 
~~~ if tunnel . try_passwordless_ssh ( sshserver , sshkey , paramiko ) : \n 
~~~ password = False \n 
~~ ~~ ssh_kwargs = dict ( keyfile = sshkey , password = password , paramiko = paramiko ) \n 
if exec_key is not None : \n 
~~~ if os . path . isfile ( exec_key ) : \n 
~~~ extra_args [ ] = exec_key \n 
~~~ exec_key = util . asbytes ( exec_key ) \n 
extra_args [ ] = exec_key \n 
~~ ~~ self . session = Session ( ** extra_args ) \n 
self . _query_socket = self . _context . socket ( zmq . XREQ ) \n 
self . _query_socket . setsockopt ( zmq . IDENTITY , util . asbytes ( self . session . session ) ) \n 
if self . _ssh : \n 
~~~ tunnel . tunnel_connection ( self . _query_socket , url , sshserver , ** ssh_kwargs ) \n 
~~~ self . _query_socket . connect ( url ) \n 
~~ self . session . debug = self . debug \n 
self . _notification_handlers = { : self . _register_engine , \n 
: self . _unregister_engine , \n 
: lambda msg : self . close ( ) , \n 
self . _queue_handlers = { : self . _handle_execute_reply , \n 
: self . _handle_apply_reply } \n 
self . _connect ( sshserver , ssh_kwargs , timeout ) \n 
~~ def validate ( self , event , event_params ) : \n 
~~~ def is_integer ( value ) : \n 
~~~ int ( value ) \n 
~~ ~~ result = EventHandler . validate ( self , event , event_params ) \n 
result &= is_integer ( event_params . get ( , ) ) \n 
~~ def make_directory ( directory ) : \n 
~~~ os . mkdir ( directory ) \n 
~~~ def run_something ( msg ) : \n 
~~~ tag = msg . delivery_info [ ] \n 
make_directory ( LOG_PATH ) \n 
def ack ( ) : \n 
~~~ channel . basic . ack ( tag ) \n 
~~ def reject ( requeue = True ) : \n 
~~~ channel . basic . reject ( tag , requeue = requeue ) \n 
~~ def publish_result ( body ) : \n 
~~~ headers = { \n 
: str ( datetime . datetime . utcnow ( ) ) , \n 
: str ( socket . getfqdn ( ) ) , \n 
body . update ( headers ) \n 
msg = Message ( json . dumps ( body ) ) \n 
channel . basic . publish ( msg , , ) \n 
~~ def valid_job ( data ) : \n 
~~~ valid = True \n 
for key in [ , , ] : \n 
~~~ if not data . get ( key , None ) : \n 
~~~ logger . debug ( . format ( key ) ) \n 
valid = False \n 
~~ ~~ return valid \n 
~~ data = json . loads ( str ( msg . body ) ) \n 
if not valid_job ( data ) : \n 
~~~ return reject ( requeue = False ) \n 
~~ cmd = data . get ( ) \n 
logger . info ( . format ( \n 
data . get ( ) , data . get ( ) , cmd \n 
) ) \n 
publish_result ( { \n 
: data . get ( ) , \n 
: , \n 
} ) \n 
start = time . time ( ) \n 
process = None \n 
~~~ process = subprocess . Popen ( \n 
cmd , \n 
shell = True , \n 
stdout = subprocess . PIPE , \n 
stderr = subprocess . STDOUT , \n 
~~~ logger . exception ( . format ( \n 
data . get ( ) , data . get ( ) \n 
end = time . time ( ) \n 
: end - start , \n 
return reject ( requeue = False ) \n 
~~ fd = process . stdout . fileno ( ) \n 
fl = fcntl . fcntl ( fd , fcntl . F_GETFL ) \n 
fcntl . fcntl ( fd , fcntl . F_SETFL , fl | os . O_NONBLOCK ) \n 
filename = . format ( LOG_PATH , data . get ( , ) ) \n 
handler = logging . handlers . WatchedFileHandler ( filename ) \n 
log_to_stdout = bool ( os . getenv ( , False ) ) \n 
~~~ nextline = process . stdout . readline ( ) \n 
~~~ nextline = \n 
~~ if nextline == and process . poll ( ) is not None : \n 
~~ if nextline == : \n 
~~~ message = nextline . rstrip ( ) \n 
message = unicodedammit ( message ) \n 
~~ if message : \n 
~~~ for m in message . splitlines ( ) : \n 
~~~ log_record = logging . makeLogRecord ( { \n 
: message , \n 
handler . emit ( log_record ) \n 
if log_to_stdout : \n 
~~~ logger . info ( . format ( \n 
data . get ( ) , data . get ( ) , log_record . getMessage ( ) \n 
~~ ~~ ~~ time . sleep ( 0.00001 ) \n 
~~ handler . close ( ) \n 
: process . returncode , \n 
data . get ( ) , data . get ( ) , process . returncode \n 
ack ( ) \n 
~~ return run_something \n 
~~ def parse_discovery_service_request ( self , url = "" , query = "" ) : \n 
~~~ if url : \n 
~~~ part = urlparse ( url ) \n 
dsr = parse_qs ( part [ 4 ] ) \n 
~~ elif query : \n 
~~~ dsr = parse_qs ( query ) \n 
~~~ dsr = { } \n 
~~ for key in [ "isPassive" , "return" , "returnIDParam" , "policy" ] : \n 
~~~ assert len ( dsr [ key ] ) == 1 \n 
dsr [ key ] = dsr [ key ] [ 0 ] \n 
~~ ~~ if "return" in dsr : \n 
~~~ part = urlparse ( dsr [ "return" ] ) \n 
if part . query : \n 
~~~ qp = parse_qs ( part . query ) \n 
if "returnIDParam" in dsr : \n 
~~~ assert dsr [ "returnIDParam" ] not in qp . keys ( ) \n 
~~~ assert "entityID" not in qp . keys ( ) \n 
~~ if "policy" not in dsr : \n 
~~~ dsr [ "policy" ] = IDPDISC_POLICY \n 
~~~ assert dsr [ "isPassive" ] in [ "true" , "false" ] \n 
~~ if "isPassive" in dsr and dsr [ "isPassive" ] == "true" : \n 
~~~ dsr [ "isPassive" ] = True \n 
~~~ dsr [ "isPassive" ] = False \n 
~~ if not "returnIDParam" in dsr : \n 
~~~ dsr [ "returnIDParam" ] = "entityID" \n 
~~ return dsr \n 
~~ def verify_return ( self , entity_id , return_url ) : \n 
~~~ for endp in self . metadata . discovery_response ( entity_id ) : \n 
~~~ assert return_url . startswith ( endp [ "location" ] ) \n 
~~ ~~ return False \n 
~~ def start ( self , response ) : \n 
assert not self . status . time_started \n 
~~~ total_size = int ( response . headers [ ] ) \n 
~~ except ( KeyError , __HOLE__ , TypeError ) : \n 
~~~ total_size = None \n 
~~ if self . _output_file : \n 
~~~ if self . _resume and response . status_code == PARTIAL_CONTENT : \n 
~~~ total_size = parse_content_range ( \n 
response . headers . get ( ) , \n 
self . _resumed_from \n 
~~~ self . _resumed_from = 0 \n 
~~~ self . _output_file . seek ( 0 ) \n 
self . _output_file . truncate ( ) \n 
~~~ fn = None \n 
if in response . headers : \n 
~~~ fn = filename_from_content_disposition ( \n 
response . headers [ ] ) \n 
~~ if not fn : \n 
~~~ fn = filename_from_url ( \n 
url = response . url , \n 
content_type = response . headers . get ( ) , \n 
~~ self . _output_file = open ( get_unique_filename ( fn ) , mode = ) \n 
~~ self . status . started ( \n 
resumed_from = self . _resumed_from , \n 
total_size = total_size \n 
stream = RawStream ( \n 
msg = HTTPResponse ( response ) , \n 
with_headers = False , \n 
with_body = True , \n 
on_body_chunk_downloaded = self . chunk_downloaded , \n 
chunk_size = 1024 * 8 \n 
self . _progress_reporter . output . write ( \n 
( humanize_bytes ( total_size ) + \n 
if total_size is not None \n 
else ) , \n 
self . _output_file . name \n 
self . _progress_reporter . start ( ) \n 
return stream , self . _output_file \n 
~~ def from_json_or_fail ( schema ) : \n 
~~~ schema = json . loads ( schema ) if schema else { } \n 
~~~ if isinstance ( schema , dict ) : \n 
~~ ~~ return schema \n 
~~ def main ( dev = False , _db = None ) : \n 
~~~ _db = _db or db \n 
init_app ( routes = False ) \n 
count = 0 \n 
skipped = 0 \n 
scripts_utils . add_file_logger ( logger , __file__ ) \n 
prepare_nodes ( ) \n 
ensure_schemas ( ) \n 
node_documents = _db [ ] . find ( { : True } ) \n 
for node in node_documents : \n 
~~~ registered_schemas = [ ] \n 
registered_meta = { } \n 
schemas = node [ ] \n 
if not schemas : \n 
~~~ logger . info ( . format ( node [ ] ) ) \n 
~~ for schema_id , schema in schemas . iteritems ( ) : \n 
~~~ name = _id_to_name ( from_mongo ( schema_id ) ) \n 
schema = from_json_or_fail ( schema ) \n 
~~~ meta_schema = MetaSchema . find ( \n 
Q ( , , name ) \n 
) . sort ( ) [ 0 ] \n 
~~~ logger . error ( . format ( name , node [ ] ) ) \n 
skipped += 1 \n 
if dev : \n 
~~~ registered_meta [ meta_schema . _id ] = { \n 
key : { \n 
: value \n 
for key , value in schema . items ( ) \n 
registered_schemas . append ( meta_schema . _id ) \n 
~~ ~~ db [ ] . update ( \n 
{ : node [ ] } , \n 
{ : { \n 
: registered_meta , \n 
: registered_schemas \n 
} } \n 
count = count + 1 \n 
~~ logger . info ( . format ( count , skipped ) ) \n 
~~ def load ( self ) : \n 
module_name , obj_name = self . location . split ( ) \n 
~~~ module = importlib . import_module ( module_name ) \n 
~~ except ImportError : \n 
~~~ raise Exception ( . format ( \n 
self . category , module_name ) ) \n 
~~~ obj = getattr ( module , obj_name ) \n 
self . category , self . name , module_name , obj_name ) ) \n 
~~ return obj \n 
~~ def add_plugins ( self , cfg_parser , section ) : \n 
if section in cfg_parser : \n 
~~~ for name , location in cfg_parser [ section ] . items ( ) : \n 
~~~ self [ name ] = PluginEntry ( section , name , location ) \n 
~~~ import pkg_resources \n 
group = "doit.{}" . format ( section ) \n 
for point in pkg_resources . iter_entry_points ( group = group ) : \n 
~~~ name = point . name \n 
location = "{}:{}" . format ( point . module_name , point . attrs [ 0 ] ) \n 
self [ name ] = PluginEntry ( section , name , location ) \n 
~~ ~~ def __init__ ( self ) : \n 
settings = current . deployment_settings \n 
log_level = settings . get_log_level ( ) \n 
if log_level is None : \n 
~~~ self . critical = self . error = self . warning = self . info = self . debug = self . ignore \n 
self . log_level = 100 \n 
~~~ level = getattr ( logging , log_level . upper ( ) ) \n 
~~ self . log_level = level \n 
self . critical = self . _critical if level <= logging . CRITICAL else self . ignore \n 
self . error = self . _error if level <= logging . ERROR else self . ignore \n 
self . warning = self . _warning if level <= logging . WARNING else self . ignore \n 
self . info = self . _info if level <= logging . INFO else self . ignore \n 
self . debug = self . _debug if level <= logging . DEBUG else self . ignore \n 
~~ self . configure_logger ( ) \n 
~~ def test_iter_while_modifying_values ( self ) : \n 
~~~ if not hasattr ( self . f , ) : \n 
~~ di = iter ( self . d ) \n 
~~~ key = di . next ( ) \n 
self . d [ key ] = + key \n 
~~ ~~ fi = iter ( self . f ) \n 
~~~ key = fi . next ( ) \n 
self . f [ key ] = + key \n 
~~ ~~ self . test_mapping_iteration_methods ( ) \n 
~~ def test_iteritems_while_modifying_values ( self ) : \n 
~~ di = self . d . iteritems ( ) \n 
~~~ k , v = di . next ( ) \n 
self . d [ k ] = + v \n 
~~ ~~ fi = self . f . iteritems ( ) \n 
~~~ k , v = fi . next ( ) \n 
self . f [ k ] = + v \n 
~~ def test__no_deadlock_first ( self , debug = 0 ) : \n 
~~~ sys . stdout . flush ( ) \n 
if debug : print "A" \n 
k , v = self . f . first ( ) \n 
if debug : print "B" , k \n 
if debug : print "C" \n 
if hasattr ( self . f , ) : \n 
~~~ if debug : print "D" \n 
i = self . f . iteritems ( ) \n 
k , v = i . next ( ) \n 
if debug : print "E" \n 
if debug : print "F" \n 
~~~ k , v = i . next ( ) \n 
~~ ~~ if debug : print "F2" \n 
i = iter ( self . f ) \n 
if debug : print "G" \n 
while i : \n 
~~~ if debug : print "H" \n 
k = i . next ( ) \n 
if debug : print "I" \n 
self . f [ k ] = "deadlocks-r-us" \n 
if debug : print "J" \n 
~~~ i = None \n 
~~ ~~ if debug : print "K" \n 
~~ self . assert_ ( self . f . first ( ) [ 0 ] in self . d ) \n 
k = self . f . next ( ) [ 0 ] \n 
self . assert_ ( k in self . d ) \n 
~~ def get_backend_instance ( name ) : \n 
~~~ manager = DriverManager ( namespace = BACKENDS_NAMESPACE , name = name , \n 
invoke_on_load = False ) \n 
~~ except RuntimeError : \n 
~~~ message = % ( name ) \n 
LOG . exception ( message ) \n 
raise ValueError ( message ) \n 
~~ backend_kwargs = cfg . CONF . auth . backend_kwargs \n 
if backend_kwargs : \n 
~~~ kwargs = json . loads ( backend_kwargs ) \n 
~~~ raise ValueError ( % ( str ( e ) ) ) \n 
~~~ kwargs = { } \n 
~~ cls = manager . driver \n 
cls_instance = cls ( ** kwargs ) \n 
return cls_instance \n 
~~ def _create_learning_image ( domain , b ) : \n 
~~~ dem = domain . get_dem ( ) . image \n 
outputBands . addBands ( dem ) \n 
~~~ skyboxSensor = domain . skybox \n 
~~~ skyboxSensor = domain . skybox_nir \n 
~~ rgbBands = skyboxSensor . Red . addBands ( skyboxSensor . Green ) . addBands ( skyboxSensor . Blue ) \n 
grayBand = rgbBands . select ( ) . add ( rgbBands . select ( ) ) . add ( rgbBands . select ( ) ) . divide ( ee . Image ( 3.0 ) ) . uint16 ( ) \n 
edges = grayBand . convolve ( ee . Kernel . laplacian8 ( normalize = True ) ) . abs ( ) \n 
texture = edges . convolve ( ee . Kernel . square ( 3 , ) ) . select ( [ ] , [ ] ) \n 
texture2Raw = grayBand . glcmTexture ( ) \n 
bandList = safe_get_info ( texture2Raw ) [ ] \n 
bandName = [ x [ ] for x in bandList if in x [ ] ] \n 
texture2 = texture2Raw . select ( bandName ) . convolve ( ee . Kernel . square ( 5 , ) ) \n 
skyboxBands = rgbBands . addBands ( texture2 ) \n 
outputBands = outputBands . addBands ( skyboxBands ) \n 
~~~ outputBands = outputBands . addBands ( domain . skybox_nir . NIR ) \n 
~~ return outputBands \n 
~~ def loadTestModules ( testFiles , rootDir , rootModule ) : \n 
~~~ modules = set ( ) \n 
for f in testFiles : \n 
~~~ with DirectoryScope . DirectoryScope ( os . path . split ( f ) [ 0 ] ) : \n 
~~~ moduleName = fileNameToModuleName ( f , rootDir , rootModule ) \n 
logging . info ( , moduleName ) \n 
__import__ ( moduleName ) \n 
modules . add ( sys . modules [ moduleName ] ) \n 
traceback . print_exc ( ) \n 
~~ ~~ return modules \n 
~~ def parse ( self , stream , media_type = None , parser_context = None ) : \n 
assert yaml , \n 
parser_context = parser_context or { } \n 
encoding = parser_context . get ( , settings . DEFAULT_CHARSET ) \n 
~~~ data = stream . read ( ) . decode ( encoding ) \n 
return yaml . safe_load ( data ) \n 
~~ except ( __HOLE__ , yaml . parser . ParserError ) as exc : \n 
~~~ raise ParseError ( % six . text_type ( exc ) ) \n 
~~ ~~ def _get_value_for_key ( key , obj , default ) : \n 
~~~ if is_indexable_but_not_string ( obj ) : \n 
~~~ return obj [ key ] \n 
~~ except ( IndexError , __HOLE__ , KeyError ) : \n 
~~ ~~ return getattr ( obj , key , default ) \n 
~~ def format ( self , value ) : \n 
~~~ return six . text_type ( value ) \n 
~~ except __HOLE__ as ve : \n 
~~~ raise MarshallingException ( ve ) \n 
~~ ~~ def format ( self , value ) : \n 
~~~ if value is None : \n 
~~~ return self . default \n 
~~ return int ( value ) \n 
~~ ~~ def output ( self , key , obj ) : \n 
~~~ data = to_marshallable_type ( obj ) \n 
return self . src_str . format ( ** data ) \n 
~~ except ( __HOLE__ , IndexError ) as error : \n 
~~~ raise MarshallingException ( error ) \n 
endpoint = self . endpoint if self . endpoint is not None else request . endpoint \n 
o = urlparse ( url_for ( endpoint , _external = self . absolute , ** data ) ) \n 
if self . absolute : \n 
~~~ scheme = self . scheme if self . scheme is not None else o . scheme \n 
return urlunparse ( ( scheme , o . netloc , o . path , "" , "" , "" ) ) \n 
~~ return urlunparse ( ( "" , "" , o . path , "" , "" , "" ) ) \n 
~~ except __HOLE__ as te : \n 
~~~ raise MarshallingException ( te ) \n 
~~~ return float ( value ) \n 
~~~ if self . dt_format == : \n 
~~~ return _rfc822 ( value ) \n 
~~ elif self . dt_format == : \n 
~~~ return _iso8601 ( value ) \n 
~~~ raise MarshallingException ( \n 
% self . dt_format \n 
~~ ~~ except __HOLE__ as ae : \n 
~~~ raise MarshallingException ( ae ) \n 
~~ ~~ def __call__ ( self , * a , ** kw ) : \n 
~~~ app = DumbApplication ( * a , ** kw ) \n 
app . status_codes = self . status_codes \n 
~~~ app . default_status_code = self . status_codes [ - 1 ] \n 
~~~ app . default_status_code = 200 \n 
~~ app . body = self . body \n 
return app \n 
~~ def handle_request ( self , req ) : \n 
~~~ self . call_count += 1 \n 
req . path_info_pop ( ) \n 
if isinstance ( self . body , list ) : \n 
~~~ body = self . body . pop ( 0 ) \n 
~~~ body = \n 
~~~ body = self . body \n 
~~ resp = swob . Response ( request = req , body = body , \n 
conditional_response = True ) \n 
~~~ resp . status_int = self . status_codes . pop ( 0 ) \n 
~~~ resp . status_int = self . default_status_code \n 
~~ resp . app_iter = iter ( body ) \n 
return resp \n 
~~ def connectionTaskFailed ( self , connection_task , reason , fprint = None ) : \n 
fprint = fprint or connection_task . micro_status_entry . fingerprint \n 
~~~ for request in self . _pending_request_dict [ fprint ] : \n 
~~~ request . errback ( reason ) \n 
~~ del self . _pending_request_dict [ fprint ] \n 
logging . debug ( msg ) \n 
~~ ~~ def removeConnection ( self , connection ) : \n 
fprint = connection . micro_status_entry . fingerprint \n 
~~~ del self . _connection_dict [ connection . micro_status_entry . fingerprint ] \n 
"connection." . format ( fprint ) ) \n 
~~ ~~ def __init__ ( self , reactor , proc , name , fileno , forceReadHack = False ) : \n 
abstract . FileDescriptor . __init__ ( self , reactor ) \n 
fdesc . setNonBlocking ( fileno ) \n 
self . proc = proc \n 
self . name = name \n 
self . fd = fileno \n 
if not stat . S_ISFIFO ( os . fstat ( self . fileno ( ) ) . st_mode ) : \n 
~~~ self . enableReadHack = False \n 
~~ elif forceReadHack : \n 
~~~ self . enableReadHack = True \n 
~~~ os . read ( self . fileno ( ) , 0 ) \n 
~~ ~~ if self . enableReadHack : \n 
~~~ self . startReading ( ) \n 
~~ ~~ def reapProcess ( self ) : \n 
~~~ pid , status = os . waitpid ( self . pid , os . WNOHANG ) \n 
~~~ if e . errno == errno . ECHILD : \n 
~~~ pid = None \n 
~~ ~~ ~~ except : \n 
~~~ log . msg ( % self . pid ) \n 
log . err ( ) \n 
pid = None \n 
~~ if pid : \n 
~~~ self . processEnded ( status ) \n 
unregisterReapProcessHandler ( pid , self ) \n 
~~ ~~ def _setupChild ( self , masterfd , slavefd ) : \n 
os . close ( masterfd ) \n 
if hasattr ( termios , ) : \n 
~~~ fd = os . open ( "/dev/tty" , os . O_RDWR | os . O_NOCTTY ) \n 
~~~ fcntl . ioctl ( fd , termios . TIOCNOTTY , ) \n 
~~ os . close ( fd ) \n 
~~ ~~ os . setsid ( ) \n 
~~~ fcntl . ioctl ( slavefd , termios . TIOCSCTTY , ) \n 
~~ for fd in range ( 3 ) : \n 
~~~ if fd != slavefd : \n 
~~~ os . close ( fd ) \n 
for fd in _listOpenFDs ( ) : \n 
~~~ if fd > 2 : \n 
~~ ~~ ~~ self . _resetSignalDisposition ( ) \n 
~~ @ internationalizeDocstring \n 
~~~ def password ( self , irc , msg , args , nick , password ) : \n 
if not password : \n 
~~~ self . registryValue ( ) . remove ( nick ) \n 
irc . replySuccess ( ) \n 
~~~ irc . error ( _ ( ) ) \n 
~~~ self . registryValue ( ) . add ( nick ) \n 
config . registerNick ( nick , password ) \n 
~~ ~~ ~~ def discard ( self , key ) : \n 
~~~ self . remove ( key ) \n 
~~ ~~ def get ( self , key , default = None ) : \n 
~~~ return self . __getitem__ ( key ) \n 
~~ ~~ def itervalues ( self ) : \n 
for key in self . iterkeys ( ) : \n 
~~~ value = self [ key ] \n 
~~ yield value \n 
~~ ~~ def iteritems ( self ) : \n 
~~ yield ( key , value ) \n 
~~ ~~ def pop ( self , key , default = None ) : \n 
~~~ result = self [ key ] \n 
~~ self . discard ( key ) \n 
~~ def update ( self , arg = None ) : \n 
if hasattr ( arg , ) : \n 
~~~ source = arg . iteritems ( ) \n 
~~ elif hasattr ( arg , ) : \n 
~~~ source = arg . items ( ) \n 
~~~ source = arg \n 
~~ bad_key = False \n 
for key , message in source : \n 
~~~ self [ key ] = message \n 
~~~ bad_key = True \n 
~~ ~~ if bad_key : \n 
~~~ raise KeyError ( ) \n 
~~ ~~ def add ( self , message ) : \n 
tmp_file = self . _create_tmp ( ) \n 
~~~ self . _dump_message ( message , tmp_file ) \n 
~~~ _sync_close ( tmp_file ) \n 
~~ if isinstance ( message , MaildirMessage ) : \n 
~~~ subdir = message . get_subdir ( ) \n 
suffix = self . colon + message . get_info ( ) \n 
if suffix == self . colon : \n 
~~~ suffix = \n 
~~~ subdir = \n 
suffix = \n 
~~ uniq = os . path . basename ( tmp_file . name ) . split ( self . colon ) [ 0 ] \n 
dest = os . path . join ( self . _path , subdir , uniq + suffix ) \n 
~~~ if hasattr ( os , ) : \n 
~~~ os . link ( tmp_file . name , dest ) \n 
os . remove ( tmp_file . name ) \n 
~~~ os . rename ( tmp_file . name , dest ) \n 
~~ ~~ except __HOLE__ , e : \n 
~~~ os . remove ( tmp_file . name ) \n 
if e . errno == errno . EEXIST : \n 
~~~ raise ExternalClashError ( \n 
% dest ) \n 
~~ ~~ if isinstance ( message , MaildirMessage ) : \n 
~~~ os . utime ( dest , ( os . path . getatime ( dest ) , message . get_date ( ) ) ) \n 
~~ return uniq \n 
~~ def discard ( self , key ) : \n 
~~~ if e . errno != errno . ENOENT : \n 
~~ ~~ ~~ def iterkeys ( self ) : \n 
self . _refresh ( ) \n 
for key in self . _toc : \n 
~~~ self . _lookup ( key ) \n 
~~ yield key \n 
~~ ~~ def _create_tmp ( self ) : \n 
if in hostname : \n 
~~~ hostname = hostname . replace ( , ) \n 
~~ if in hostname : \n 
~~ uniq = "%s.M%sP%sQ%s.%s" % ( int ( now ) , int ( now % 1 * 1e6 ) , os . getpid ( ) , \n 
Maildir . _count , hostname ) \n 
path = os . path . join ( self . _path , , uniq ) \n 
~~~ os . stat ( path ) \n 
~~~ if e . errno == errno . ENOENT : \n 
~~~ Maildir . _count += 1 \n 
~~~ return _create_carefully ( path ) \n 
~~ except OSError , e : \n 
~~ ~~ raise ExternalClashError ( % \n 
path ) \n 
~~ def _lookup ( self , key ) : \n 
~~~ if os . path . exists ( os . path . join ( self . _path , self . _toc [ key ] ) ) : \n 
~~~ return self . _toc [ key ] \n 
~~ self . _refresh ( ) \n 
~~~ raise KeyError ( % key ) \n 
~~ ~~ def next ( self ) : \n 
if not hasattr ( self , ) : \n 
~~~ self . _onetime_keys = self . iterkeys ( ) \n 
~~ while True : \n 
~~~ return self [ self . _onetime_keys . next ( ) ] \n 
~~ ~~ ~~ def __init__ ( self , path , factory = None , create = True ) : \n 
Mailbox . __init__ ( self , path , factory , create ) \n 
~~~ f = open ( self . _path , ) \n 
~~~ if create : \n 
~~~ raise NoSuchMailboxError ( self . _path ) \n 
~~ ~~ elif e . errno == errno . EACCES : \n 
~~ ~~ self . _file = f \n 
self . _toc = None \n 
self . _next_key = 0 \n 
self . _locked = False \n 
~~ def flush ( self ) : \n 
if not self . _pending : \n 
~~ self . _lookup ( ) \n 
new_file = _create_temporary ( self . _path ) \n 
~~~ new_toc = { } \n 
self . _pre_mailbox_hook ( new_file ) \n 
for key in sorted ( self . _toc . keys ( ) ) : \n 
~~~ start , stop = self . _toc [ key ] \n 
self . _file . seek ( start ) \n 
self . _pre_message_hook ( new_file ) \n 
new_start = new_file . tell ( ) \n 
~~~ buffer = self . _file . read ( min ( 4096 , \n 
stop - self . _file . tell ( ) ) ) \n 
if buffer == : \n 
~~ new_file . write ( buffer ) \n 
~~ new_toc [ key ] = ( new_start , new_file . tell ( ) ) \n 
self . _post_message_hook ( new_file ) \n 
~~~ new_file . close ( ) \n 
os . remove ( new_file . name ) \n 
~~ _sync_close ( new_file ) \n 
self . _file . close ( ) \n 
~~~ os . rename ( new_file . name , self . _path ) \n 
~~~ if e . errno == errno . EEXIST or ( os . name == and e . errno == errno . EACCES ) : \n 
~~~ os . remove ( self . _path ) \n 
os . rename ( new_file . name , self . _path ) \n 
~~ ~~ self . _file = open ( self . _path , ) \n 
self . _toc = new_toc \n 
self . _pending = False \n 
if self . _locked : \n 
~~~ _lock_file ( self . _file , dotlock = False ) \n 
~~ ~~ def _lookup ( self , key = None ) : \n 
if self . _toc is None : \n 
~~~ self . _generate_toc ( ) \n 
~~ if key is not None : \n 
~~ ~~ ~~ def remove ( self , key ) : \n 
path = os . path . join ( self . _path , str ( key ) ) \n 
~~~ f = open ( path , ) \n 
~~~ if self . _locked : \n 
~~~ _lock_file ( f ) \n 
~~~ f . close ( ) \n 
os . remove ( os . path . join ( self . _path , str ( key ) ) ) \n 
~~~ _unlock_file ( f ) \n 
~~ ~~ def __setitem__ ( self , key , message ) : \n 
~~~ os . close ( os . open ( path , os . O_WRONLY | os . O_TRUNC ) ) \n 
self . _dump_message ( message , f ) \n 
if isinstance ( message , MHMessage ) : \n 
~~~ self . _dump_sequences ( message , key ) \n 
~~~ _sync_close ( f ) \n 
~~ ~~ def get_message ( self , key ) : \n 
~~~ f = open ( os . path . join ( self . _path , str ( key ) ) , ) \n 
~~~ msg = MHMessage ( f ) \n 
~~ for name , key_list in self . get_sequences ( ) : \n 
~~~ if key in key_list : \n 
~~~ msg . add_sequence ( name ) \n 
~~ ~~ return msg \n 
~~ def get_string ( self , key ) : \n 
~~~ return f . read ( ) \n 
~~ ~~ def get_file ( self , key ) : \n 
~~ ~~ return _ProxyFile ( f ) \n 
~~ def get_sequences ( self ) : \n 
results = { } \n 
f = open ( os . path . join ( self . _path , ) , ) \n 
~~~ all_keys = set ( self . keys ( ) ) \n 
for line in f : \n 
~~~ name , contents = line . split ( ) \n 
keys = set ( ) \n 
for spec in contents . split ( ) : \n 
~~~ if spec . isdigit ( ) : \n 
~~~ keys . add ( int ( spec ) ) \n 
~~~ start , stop = ( int ( x ) for x in spec . split ( ) ) \n 
keys . update ( range ( start , stop + 1 ) ) \n 
~~ ~~ results [ name ] = [ key for key in sorted ( keys ) if key in all_keys ] \n 
if len ( results [ name ] ) == 0 : \n 
~~~ del results [ name ] \n 
~~~ raise FormatError ( % \n 
line . rstrip ( ) ) \n 
~~ return results \n 
~~ def set_date ( self , date ) : \n 
~~~ self . _date = float ( date ) \n 
~~ ~~ def set_flags ( self , flags ) : \n 
flags = set ( flags ) \n 
status_flags , xstatus_flags = , \n 
for flag in ( , ) : \n 
~~~ if flag in flags : \n 
~~~ status_flags += flag \n 
flags . remove ( flag ) \n 
~~ ~~ for flag in ( , , ) : \n 
~~~ xstatus_flags += flag \n 
~~ ~~ xstatus_flags += . join ( sorted ( flags ) ) \n 
~~~ self . replace_header ( , status_flags ) \n 
~~~ self . add_header ( , status_flags ) \n 
~~~ self . replace_header ( , xstatus_flags ) \n 
~~~ self . add_header ( , xstatus_flags ) \n 
~~ ~~ def _explain_to ( self , message ) : \n 
if isinstance ( message , MaildirMessage ) : \n 
~~~ flags = set ( self . get_flags ( ) ) \n 
if in flags : \n 
~~~ message . set_subdir ( ) \n 
~~ if in flags : \n 
~~~ message . add_flag ( ) \n 
~~ del message [ ] \n 
del message [ ] \n 
maybe_date = . join ( self . get_from ( ) . split ( ) [ - 5 : ] ) \n 
~~~ message . set_date ( calendar . timegm ( time . strptime ( maybe_date , \n 
) ) ) \n 
~~ except ( __HOLE__ , OverflowError ) : \n 
~~ ~~ elif isinstance ( message , _mboxMMDFMessage ) : \n 
~~~ message . set_flags ( self . get_flags ( ) ) \n 
message . set_from ( self . get_from ( ) ) \n 
~~ elif isinstance ( message , MHMessage ) : \n 
if not in flags : \n 
~~~ message . add_sequence ( ) \n 
~~ elif isinstance ( message , BabylMessage ) : \n 
~~~ message . add_label ( ) \n 
~~ elif isinstance ( message , Message ) : \n 
~~~ raise TypeError ( % \n 
type ( message ) ) \n 
~~ ~~ def remove_sequence ( self , sequence ) : \n 
~~~ self . _sequences . remove ( sequence ) \n 
~~ ~~ def remove_label ( self , label ) : \n 
~~~ self . _labels . remove ( label ) \n 
~~ ~~ def _lock_file ( f , dotlock = True ) : \n 
dotlock_done = False \n 
~~~ if fcntl : \n 
~~~ fcntl . lockf ( f , fcntl . LOCK_EX | fcntl . LOCK_NB ) \n 
~~ except IOError , e : \n 
~~~ if e . errno in ( errno . EAGAIN , errno . EACCES ) : \n 
~~~ raise ExternalClashError ( % \n 
f . name ) \n 
~~ ~~ ~~ if dotlock : \n 
~~~ pre_lock = _create_temporary ( f . name + ) \n 
pre_lock . close ( ) \n 
~~~ if e . errno == errno . EACCES : \n 
~~~ os . link ( pre_lock . name , f . name + ) \n 
dotlock_done = True \n 
os . unlink ( pre_lock . name ) \n 
~~~ os . rename ( pre_lock . name , f . name + ) \n 
~~~ os . remove ( pre_lock . name ) \n 
raise ExternalClashError ( % \n 
~~ ~~ ~~ ~~ except : \n 
~~~ fcntl . lockf ( f , fcntl . LOCK_UN ) \n 
~~ if dotlock_done : \n 
~~~ os . remove ( f . name + ) \n 
~~~ if not self . boxes : \n 
~~ fn = self . boxes . pop ( ) \n 
fp = open ( os . path . join ( self . dirname , fn ) ) \n 
msg = self . factory ( fp ) \n 
~~~ msg . _mh_msgno = fn \n 
~~ except ( AttributeError , __HOLE__ ) : \n 
~~ def parse_title_page ( lines ) : \n 
it = iter ( lines ) \n 
~~~ line = it . next ( ) \n 
~~~ key_match = title_page_key_re . match ( line ) \n 
if not key_match : \n 
~~ key , value = key_match . groups ( ) \n 
if value : \n 
~~~ result . setdefault ( key , [ ] ) . append ( value ) \n 
line = it . next ( ) \n 
~~~ for line in it : \n 
~~~ value_match = title_page_value_re . match ( line ) \n 
if not value_match : \n 
~~ result . setdefault ( key , [ ] ) . append ( value_match . group ( 1 ) ) \n 
~~ ~~ ~~ ~~ except __HOLE__ : \n 
~~ def _try_get_item ( x ) : \n 
~~~ return x . item ( ) \n 
~~~ return x \n 
~~ ~~ def __new__ ( cls , data = None , dtype = None , copy = False , name = None , \n 
fastpath = False , tupleize_cols = True , ** kwargs ) : \n 
~~~ if name is None and hasattr ( data , ) : \n 
~~~ name = data . name \n 
~~ if fastpath : \n 
~~~ return cls . _simple_new ( data , name ) \n 
~~ from . range import RangeIndex \n 
if isinstance ( data , RangeIndex ) : \n 
~~~ return RangeIndex ( start = data , copy = copy , dtype = dtype , name = name ) \n 
~~ elif isinstance ( data , range ) : \n 
~~~ return RangeIndex . from_range ( data , copy = copy , dtype = dtype , \n 
name = name ) \n 
~~ if is_categorical_dtype ( data ) or is_categorical_dtype ( dtype ) : \n 
~~~ from . category import CategoricalIndex \n 
return CategoricalIndex ( data , copy = copy , name = name , ** kwargs ) \n 
~~ elif isinstance ( data , ( np . ndarray , Index , ABCSeries ) ) : \n 
~~~ if ( issubclass ( data . dtype . type , np . datetime64 ) or \n 
is_datetimetz ( data ) ) : \n 
~~~ from pandas . tseries . index import DatetimeIndex \n 
result = DatetimeIndex ( data , copy = copy , name = name , ** kwargs ) \n 
if dtype is not None and _o_dtype == dtype : \n 
~~~ return Index ( result . to_pydatetime ( ) , dtype = _o_dtype ) \n 
~~~ return result \n 
~~ ~~ elif issubclass ( data . dtype . type , np . timedelta64 ) : \n 
~~~ from pandas . tseries . tdi import TimedeltaIndex \n 
result = TimedeltaIndex ( data , copy = copy , name = name , ** kwargs ) \n 
~~~ return Index ( result . to_pytimedelta ( ) , dtype = _o_dtype ) \n 
~~ ~~ if dtype is not None : \n 
~~~ if is_integer_dtype ( dtype ) : \n 
~~~ inferred = lib . infer_dtype ( data ) \n 
if inferred == : \n 
~~~ data = np . array ( data , copy = copy , dtype = dtype ) \n 
~~ elif inferred in [ , ] : \n 
~~~ from . numeric import Int64Index , Float64Index \n 
~~~ res = data . astype ( ) \n 
if ( res == data ) . all ( ) : \n 
~~~ return Int64Index ( res , copy = copy , \n 
~~ ~~ except ( TypeError , ValueError ) : \n 
~~ return Float64Index ( data , copy = copy , dtype = dtype , \n 
~~ elif inferred == : \n 
~~~ data = data . astype ( dtype ) \n 
~~ ~~ elif is_float_dtype ( dtype ) : \n 
~~~ data = np . array ( data , dtype = dtype , copy = copy ) \n 
~~ ~~ from pandas . tseries . period import PeriodIndex \n 
if isinstance ( data , PeriodIndex ) : \n 
~~~ return PeriodIndex ( data , copy = copy , name = name , ** kwargs ) \n 
~~ if issubclass ( data . dtype . type , np . integer ) : \n 
~~~ from . numeric import Int64Index \n 
return Int64Index ( data , copy = copy , dtype = dtype , name = name ) \n 
~~ elif issubclass ( data . dtype . type , np . floating ) : \n 
~~~ from . numeric import Float64Index \n 
return Float64Index ( data , copy = copy , dtype = dtype , name = name ) \n 
~~ elif issubclass ( data . dtype . type , np . bool ) or is_bool_dtype ( data ) : \n 
~~~ subarr = data . astype ( ) \n 
~~~ subarr = com . _asarray_tuplesafe ( data , dtype = object ) \n 
~~ if copy : \n 
~~~ subarr = subarr . copy ( ) \n 
~~ if dtype is None : \n 
~~~ inferred = lib . infer_dtype ( subarr ) \n 
return Int64Index ( subarr . astype ( ) , copy = copy , \n 
return Float64Index ( subarr , copy = copy , name = name ) \n 
~~ elif inferred != : \n 
~~~ if ( inferred . startswith ( ) or \n 
tslib . is_timestamp_array ( subarr ) ) : \n 
~~~ if ( lib . is_datetime_with_singletz_array ( subarr ) or \n 
in kwargs ) : \n 
return DatetimeIndex ( subarr , copy = copy , name = name , \n 
** kwargs ) \n 
~~ ~~ elif ( inferred . startswith ( ) or \n 
lib . is_timedelta_array ( subarr ) ) : \n 
return TimedeltaIndex ( subarr , copy = copy , name = name , \n 
~~~ return PeriodIndex ( subarr , name = name , ** kwargs ) \n 
~~ ~~ ~~ return cls . _simple_new ( subarr , name ) \n 
~~ elif hasattr ( data , ) : \n 
~~~ return Index ( np . asarray ( data ) , dtype = dtype , copy = copy , name = name , \n 
~~ elif data is None or lib . isscalar ( data ) : \n 
~~~ cls . _scalar_data_error ( data ) \n 
~~~ if ( tupleize_cols and isinstance ( data , list ) and data and \n 
isinstance ( data [ 0 ] , tuple ) ) : \n 
~~~ if all ( isinstance ( e , tuple ) for e in data ) : \n 
~~~ if compat . PY3 : \n 
~~~ sorted ( data ) \n 
~~ from . multi import MultiIndex \n 
return MultiIndex . from_tuples ( \n 
data , names = name or kwargs . get ( ) ) \n 
~~ ~~ ~~ subarr = com . _asarray_tuplesafe ( data , dtype = object ) \n 
return Index ( subarr , dtype = dtype , copy = copy , name = name , ** kwargs ) \n 
~~ ~~ def _shallow_copy_with_infer ( self , values = None , ** kwargs ) : \n 
if values is None : \n 
~~~ values = self . values \n 
~~ attributes = self . _get_attributes_dict ( ) \n 
attributes . update ( kwargs ) \n 
attributes [ ] = False \n 
if self . _infer_as_myclass : \n 
~~~ return self . _constructor ( values , ** attributes ) \n 
~~ ~~ return Index ( values , ** attributes ) \n 
~~ def _convert_slice_indexer ( self , key , kind = None ) : \n 
assert kind in [ , , , , None ] \n 
if not isinstance ( key , slice ) : \n 
~~~ return key \n 
~~ if kind == : \n 
~~~ return slice ( self . _validate_indexer ( , key . start , kind ) , \n 
self . _validate_indexer ( , key . stop , kind ) , \n 
self . _validate_indexer ( , key . step , kind ) ) \n 
~~ start , stop , step = key . start , key . stop , key . step \n 
def is_int ( v ) : \n 
~~~ return v is None or is_integer ( v ) \n 
~~ is_null_slicer = start is None and stop is None \n 
is_index_slice = is_int ( start ) and is_int ( stop ) \n 
is_positional = is_index_slice and not self . is_integer ( ) \n 
if kind == : \n 
if self . is_integer ( ) or is_index_slice : \n 
~~~ if is_positional and self . is_mixed ( ) : \n 
~~~ if start is not None : \n 
~~ if stop is not None : \n 
~~ is_positional = False \n 
~~~ if self . inferred_type == : \n 
~~ ~~ if is_null_slicer : \n 
~~~ indexer = key \n 
~~ elif is_positional : \n 
~~~ indexer = self . slice_indexer ( start , stop , step , kind = kind ) \n 
~~~ if is_index_slice : \n 
~~~ if self . is_integer ( ) : \n 
~~ ~~ ~~ return indexer \n 
~~ def __contains__ ( self , key ) : \n 
~~~ hash ( key ) \n 
~~~ return key in self . _engine \n 
~~ ~~ def putmask ( self , mask , value ) : \n 
values = self . values . copy ( ) \n 
~~~ np . putmask ( values , mask , self . _convert_for_op ( value ) ) \n 
return self . _shallow_copy ( values ) \n 
~~ except ( ValueError , __HOLE__ ) : \n 
~~~ return self . astype ( object ) . putmask ( mask , value ) \n 
~~ ~~ def asof ( self , label ) : \n 
~~~ loc = self . get_loc ( label , method = ) \n 
~~~ return _get_na_value ( self . dtype ) \n 
~~~ if isinstance ( loc , slice ) : \n 
~~~ loc = loc . indices ( len ( self ) ) [ - 1 ] \n 
~~ return self [ loc ] \n 
~~ ~~ def union ( self , other ) : \n 
self . _assert_can_do_setop ( other ) \n 
other = _ensure_index ( other ) \n 
if len ( other ) == 0 or self . equals ( other ) : \n 
~~~ return self \n 
~~ if len ( self ) == 0 : \n 
~~~ return other \n 
~~ if not com . is_dtype_equal ( self . dtype , other . dtype ) : \n 
~~~ this = self . astype ( ) \n 
other = other . astype ( ) \n 
return this . union ( other ) \n 
~~ if self . is_monotonic and other . is_monotonic : \n 
~~~ result = self . _outer_indexer ( self . values , other . _values ) [ 0 ] \n 
~~~ result = list ( self . values ) \n 
value_set = set ( self . values ) \n 
result . extend ( [ x for x in other . _values if x not in value_set ] ) \n 
~~~ indexer = self . get_indexer ( other ) \n 
indexer , = ( indexer == - 1 ) . nonzero ( ) \n 
if len ( indexer ) > 0 : \n 
~~~ other_diff = algos . take_nd ( other . _values , indexer , \n 
allow_fill = False ) \n 
result = _concat . _concat_compat ( ( self . values , other_diff ) ) \n 
~~~ self . values [ 0 ] < other_diff [ 0 ] \n 
~~ except TypeError as e : \n 
stacklevel = 3 ) \n 
~~~ types = frozenset ( ( self . inferred_type , \n 
other . inferred_type ) ) \n 
if not types & _unsortable_types : \n 
~~~ result . sort ( ) \n 
~~~ result = self . values \n 
~~~ result = np . sort ( result ) \n 
~~ ~~ ~~ return self . _wrap_union_result ( other , result ) \n 
~~ def intersection ( self , other ) : \n 
if self . equals ( other ) : \n 
return this . intersection ( other ) \n 
~~~ result = self . _inner_indexer ( self . values , other . _values ) [ 0 ] \n 
return self . _wrap_union_result ( other , result ) \n 
~~~ indexer = Index ( self . values ) . get_indexer ( other . _values ) \n 
indexer = indexer . take ( ( indexer != - 1 ) . nonzero ( ) [ 0 ] ) \n 
~~~ indexer = Index ( self . values ) . get_indexer_non_unique ( \n 
other . _values ) [ 0 ] . unique ( ) \n 
indexer = indexer [ indexer != - 1 ] \n 
~~ taken = self . take ( indexer ) \n 
if self . name != other . name : \n 
~~~ taken . name = None \n 
~~ return taken \n 
~~ def get_loc ( self , key , method = None , tolerance = None ) : \n 
if method is None : \n 
~~~ if tolerance is not None : \n 
~~ key = _values_from_object ( key ) \n 
~~~ return self . _engine . get_loc ( key ) \n 
~~~ return self . _engine . get_loc ( self . _maybe_cast_indexer ( key ) ) \n 
~~ ~~ indexer = self . get_indexer ( [ key ] , method = method , tolerance = tolerance ) \n 
if indexer . ndim > 1 or indexer . size > 1 : \n 
~~ loc = indexer . item ( ) \n 
if loc == - 1 : \n 
~~~ raise KeyError ( key ) \n 
~~ return loc \n 
~~ def get_value ( self , series , key ) : \n 
s = getattr ( series , , None ) \n 
if isinstance ( s , Index ) and lib . isscalar ( key ) : \n 
~~~ return s [ key ] \n 
~~ ~~ s = _values_from_object ( series ) \n 
k = _values_from_object ( key ) \n 
k = self . _convert_scalar_indexer ( k , kind = ) \n 
~~~ return self . _engine . get_value ( s , k , \n 
tz = getattr ( series . dtype , , None ) ) \n 
~~ except KeyError as e1 : \n 
~~~ if len ( self ) > 0 and self . inferred_type in [ , ] : \n 
~~~ return tslib . get_value_box ( s , key ) \n 
~~ except TypeError : \n 
~~~ if is_iterator ( key ) : \n 
~~~ raise InvalidIndexError ( key ) \n 
~~~ raise e1 \n 
~~ ~~ except TypeError : \n 
~~~ raise IndexError ( key ) \n 
~~ raise InvalidIndexError ( key ) \n 
~~ ~~ def join ( self , other , how = , level = None , return_indexers = False ) : \n 
from . multi import MultiIndex \n 
self_is_mi = isinstance ( self , MultiIndex ) \n 
other_is_mi = isinstance ( other , MultiIndex ) \n 
if level is None and ( self_is_mi or other_is_mi ) : \n 
~~~ if self . names == other . names : \n 
~~~ return self . _join_multi ( other , how = how , \n 
return_indexers = return_indexers ) \n 
~~ ~~ if level is not None and ( self_is_mi or other_is_mi ) : \n 
~~~ return self . _join_level ( other , level , how = how , \n 
~~ other = _ensure_index ( other ) \n 
if len ( other ) == 0 and how in ( , ) : \n 
~~~ join_index = self . _shallow_copy ( ) \n 
if return_indexers : \n 
~~~ rindexer = np . repeat ( - 1 , len ( join_index ) ) \n 
return join_index , None , rindexer \n 
~~~ return join_index \n 
~~ ~~ if len ( self ) == 0 and how in ( , ) : \n 
~~~ join_index = other . _shallow_copy ( ) \n 
~~~ lindexer = np . repeat ( - 1 , len ( join_index ) ) \n 
return join_index , lindexer , None \n 
~~ ~~ if self . _join_precedence < other . _join_precedence : \n 
~~~ how = { : , : } . get ( how , how ) \n 
result = other . join ( self , how = how , level = level , \n 
~~~ x , y , z = result \n 
result = x , z , y \n 
return this . join ( other , how = how , return_indexers = return_indexers ) \n 
~~ _validate_join_method ( how ) \n 
if not self . is_unique and not other . is_unique : \n 
~~~ return self . _join_non_unique ( other , how = how , \n 
~~ elif not self . is_unique or not other . is_unique : \n 
~~~ if self . is_monotonic and other . is_monotonic : \n 
~~~ return self . _join_monotonic ( other , how = how , \n 
~~ ~~ elif self . is_monotonic and other . is_monotonic : \n 
~~ ~~ if how == : \n 
~~~ join_index = self \n 
~~ elif how == : \n 
~~~ join_index = other \n 
~~~ join_index = self . intersection ( other ) \n 
~~~ join_index = self . union ( other ) \n 
~~ if return_indexers : \n 
~~~ if join_index is self : \n 
~~~ lindexer = None \n 
~~~ lindexer = self . get_indexer ( join_index ) \n 
~~ if join_index is other : \n 
~~~ rindexer = None \n 
~~~ rindexer = other . get_indexer ( join_index ) \n 
~~ return join_index , lindexer , rindexer \n 
~~ ~~ def _maybe_cast_indexer ( self , key ) : \n 
if is_float ( key ) and not self . is_floating ( ) : \n 
~~~ ckey = int ( key ) \n 
if ckey == key : \n 
~~~ key = ckey \n 
~~ ~~ except ( ValueError , __HOLE__ ) : \n 
~~ ~~ return key \n 
~~ def get_slice_bound ( self , label , side , kind ) : \n 
assert kind in [ , , , None ] \n 
if side not in ( , ) : \n 
( side , ) ) \n 
~~ original_label = label \n 
label = self . _maybe_cast_slice_bound ( label , side , kind ) \n 
~~~ slc = self . get_loc ( label ) \n 
~~~ return self . _searchsorted_monotonic ( label , side ) \n 
~~~ raise err \n 
~~ ~~ if isinstance ( slc , np . ndarray ) : \n 
~~~ if is_bool_dtype ( slc ) : \n 
~~~ slc = lib . maybe_booleans_to_slice ( slc . view ( ) ) \n 
~~~ slc = lib . maybe_indices_to_slice ( slc . astype ( ) , len ( self ) ) \n 
~~ if isinstance ( slc , np . ndarray ) : \n 
~~ ~~ if isinstance ( slc , slice ) : \n 
~~~ if side == : \n 
~~~ return slc . start \n 
~~~ return slc . stop \n 
~~~ return slc + 1 \n 
~~~ return slc \n 
~~ ~~ ~~ @ classmethod \n 
~~~ def _add_comparison_methods ( cls ) : \n 
def _make_compare ( op ) : \n 
~~~ def _evaluate_compare ( self , other ) : \n 
~~~ if isinstance ( other , ( np . ndarray , Index , ABCSeries ) ) : \n 
~~~ if other . ndim > 0 and len ( self ) != len ( other ) : \n 
~~ ~~ if needs_i8_conversion ( self ) and needs_i8_conversion ( other ) : \n 
~~~ return self . _evaluate_compare ( other , op ) \n 
~~ func = getattr ( self . values , op ) \n 
result = func ( np . asarray ( other ) ) \n 
if is_bool_dtype ( result ) : \n 
~~~ return Index ( result ) \n 
~~ ~~ return _evaluate_compare \n 
~~ cls . __eq__ = _make_compare ( ) \n 
cls . __ne__ = _make_compare ( ) \n 
cls . __lt__ = _make_compare ( ) \n 
cls . __gt__ = _make_compare ( ) \n 
cls . __le__ = _make_compare ( ) \n 
cls . __ge__ = _make_compare ( ) \n 
~~ ~~ def _ensure_has_len ( seq ) : \n 
~~~ len ( seq ) \n 
~~~ return list ( seq ) \n 
~~~ return seq \n 
~~ ~~ def _json_convert ( obj ) : \n 
~~~ return SON ( ( ( k , _json_convert ( v ) ) for k , v in iteritems ( obj ) ) ) \n 
~~ elif hasattr ( obj , ) and not isinstance ( obj , ( text_type , bytes ) ) : \n 
~~~ return list ( ( _json_convert ( v ) for v in obj ) ) \n 
~~~ return default ( obj ) \n 
~~~ return obj \n 
~~ ~~ @ register . simple_tag \n 
def staticbundle ( bundle , mimetype = None , ** attrs ) : \n 
config = getattr ( settings , , { } ) \n 
if settings . DEBUG and in config and bundle in config [ ] : \n 
~~~ cache_root = os . path . join ( settings . STATIC_ROOT , config . get ( ) or DEFAULT_CACHE_DIR ) \n 
bundle_opts = config [ ] [ bundle ] \n 
root = os . path . join ( cache_root , bundle_opts . get ( , ) ) \n 
changed = set ( ) \n 
src_list = bundle_opts [ ] \n 
is_mapping = isinstance ( src_list , dict ) \n 
for src in src_list : \n 
~~~ src_path = os . path . join ( bundle_opts . get ( , ) , src ) \n 
abs_src = os . path . join ( settings . STATIC_ROOT , src_path ) \n 
if is_mapping and not os . path . exists ( os . path . join ( root , src_list [ src ] ) ) : \n 
~~~ changed . add ( src ) \n 
~~ if abs_src is not None : \n 
~~~ cached_mtime = BUNDLE_CACHE . get ( abs_src ) \n 
current_mtime = os . stat ( abs_src ) . st_mtime \n 
if cached_mtime is None : \n 
~~~ BUNDLE_CACHE [ abs_src ] = cached_mtime = current_mtime \n 
~~~ cached_mtime = 0 \n 
~~~ cached_mtime = BUNDLE_CACHE [ abs_src ] \n 
~~ if current_mtime != cached_mtime : \n 
~~~ changed . add ( src_path ) \n 
BUNDLE_CACHE [ abs_src ] = current_mtime \n 
~~ ~~ elif settings . TEMPLATE_DEBUG : \n 
~~~ raise template . TemplateSyntaxError ( \n 
~~ ~~ if changed : \n 
~~~ logger . info ( , bundle , . join ( changed ) ) \n 
call_command ( , bundle ) \n 
~~ ~~ src_list = [ bundle ] \n 
output = [ ] \n 
for src_path in src_list : \n 
~~~ url = staticfiles_storage . url ( src_path ) \n 
path = urlparse . urlparse ( url ) . path \n 
if path . endswith ( ) : \n 
~~~ mimetype = \n 
~~ elif path . endswith ( ) : \n 
~~ output . append ( TEMPLATES [ mimetype ] % dict ( \n 
url = url , \n 
mimetype = mimetype , \n 
attrs = . join ( \'%s="%s"\' % ( k , escape ( v ) ) for k , v in attrs . iteritems ( ) ) , \n 
~~ return . join ( output ) \n 
~~ def process_options ( self , args , opts ) : \n 
~~~ ScrapyCommand . process_options ( self , args , opts ) \n 
~~~ opts . spargs = arglist_to_dict ( opts . spargs ) \n 
~~ if opts . output : \n 
~~~ if opts . output == : \n 
~~~ self . settings . set ( , , priority = ) \n 
~~~ self . settings . set ( , opts . output , priority = ) \n 
~~ feed_exporters = without_none_values ( \n 
self . settings . getwithbase ( ) ) \n 
valid_output_formats = feed_exporters . keys ( ) \n 
if not opts . output_format : \n 
~~~ opts . output_format = os . path . splitext ( opts . output ) [ 1 ] . replace ( "." , "" ) \n 
~~ if opts . output_format not in valid_output_formats : \n 
tuple ( valid_output_formats ) ) ) \n 
~~ self . settings . set ( , opts . output_format , priority = ) \n 
~~ ~~ def songs_by_user ( filename ) : \n 
~~~ listens = [ ] \n 
old_user = 1 \n 
for line in open ( filename ) : \n 
~~~ user , song , count = [ int ( x ) for x in line . split ( ) ] \n 
~~ if old_user != user : \n 
~~~ yield listens \n 
listens = [ ] \n 
old_user = user \n 
~~ listens . append ( ( song , count ) ) \n 
~~ yield listens \n 
~~ def get_files ( remote_path , local_path ) : \n 
~~~ path_with_host_name = os . path . join ( local_path , env . host ) \n 
if not os . path . exists ( path_with_host_name ) : \n 
~~~ os . makedirs ( path_with_host_name ) \n 
~~ _LOGGER . debug ( + path_with_host_name ) \n 
~~~ get ( remote_path , path_with_host_name , True ) \n 
~~~ warn ( + remote_path + + env . host ) \n 
~~ ~~ @ transaction . atomic \n 
~~~ def form_valid ( self , form ) : \n 
~~~ saved_objects = [ ] \n 
args = [ form . cleaned_data [ field [ ] ] for field in self . fields ] \n 
for data in zip ( * args ) : \n 
~~~ for field in self . _get_ancestors_pointers ( self . obj ) : \n 
~~~ setattr ( self . obj , field , None ) \n 
~~ self . obj . id = self . obj . pk = None \n 
for field in self . clear_fields : \n 
~~~ setattr ( self . obj , field [ ] , field [ ] ) \n 
~~ for i , field in enumerate ( self . fields ) : \n 
~~~ setattr ( self . obj , field [ ] , data [ i ] ) \n 
~~~ self . obj . clean ( ) \n 
~~ except __HOLE__ as exc : \n 
~~~ for error in exc : \n 
~~~ form . add_error ( error [ 0 ] , error [ 1 ] ) \n 
~~ return self . form_invalid ( form ) \n 
~~ self . obj . save ( ) \n 
self . obj . get_absolute_url ( ) , \n 
str ( self . obj ) \n 
~~ messages . success ( \n 
self . request , \n 
mark_safe ( \n 
_ ( ) % { \n 
: len ( saved_objects ) , \n 
return HttpResponseRedirect ( self . get_url_name ( ) ) \n 
~~ ~~ def dunder_get ( _dict , key ) : \n 
parts = key . split ( , 1 ) \n 
~~~ result = _dict [ parts [ 0 ] ] \n 
~~~ return result if len ( parts ) == 1 else dunder_get ( result , parts [ 1 ] ) \n 
~~ ~~ def run ( self ) : \n 
~~~ self . log = logging . getLogger ( ) \n 
for x in self . log . handlers : \n 
~~~ self . log . removeHandler ( x ) \n 
~~ configure_logging ( \n 
level = self . log_level , \n 
format = \n 
% self . name , \n 
filename = self . log_filename , \n 
self . log . debug ( "Starting" ) \n 
signal . signal ( signal . SIGTERM , signal . SIG_DFL ) \n 
backend = get_backend ( ) \n 
time_item_last_processed = datetime . datetime . utcnow ( ) \n 
for item_count in itertools . count ( ) : \n 
~~~ if not self . running . value : \n 
~~ if self . idle_time_reached ( time_item_last_processed ) : \n 
~~ if item_count > 1000 : \n 
~~~ item_processed = self . process ( backend ) \n 
if item_processed : \n 
~~~ time_item_last_processed = datetime . datetime . utcnow ( ) \n 
~~~ sys . exit ( 1 ) \n 
~~ ~~ self . log . info ( "Exiting" ) \n 
~~ def process ( self , backend ) : \n 
self . tell_master ( None , False ) \n 
job = backend . dequeue ( self . queue , 15 ) \n 
if job is None : \n 
~~ self . tell_master ( \n 
job . timeout , \n 
job . sigkill_on_stop , \n 
if job . run ( ) and self . touch_filename : \n 
~~~ with open ( self . touch_filename , ) : \n 
~~~ os . utime ( self . touch_filename , None ) \n 
~~ ~~ for x in connections : \n 
~~~ transaction . abort ( x ) \n 
~~ connections [ x ] . close ( ) \n 
~~ return True \n 
~~ def _is_vlan_id ( text ) : \n 
~~~ number = int ( text ) \n 
~~ return 1 <= number <= 4093 \n 
~~ def time_period_str ( value ) : \n 
if isinstance ( value , int ) : \n 
~~~ raise vol . Invalid ( ) \n 
~~ elif not isinstance ( value , str ) : \n 
~~~ raise vol . Invalid ( TIME_PERIOD_ERROR . format ( value ) ) \n 
~~ negative_offset = False \n 
if value . startswith ( ) : \n 
~~~ negative_offset = True \n 
value = value [ 1 : ] \n 
~~ elif value . startswith ( ) : \n 
~~~ value = value [ 1 : ] \n 
~~~ parsed = [ int ( x ) for x in value . split ( ) ] \n 
~~ if len ( parsed ) == 2 : \n 
~~~ hour , minute = parsed \n 
second = 0 \n 
~~ elif len ( parsed ) == 3 : \n 
~~~ hour , minute , second = parsed \n 
~~ offset = timedelta ( hours = hour , minutes = minute , seconds = second ) \n 
if negative_offset : \n 
~~~ offset *= - 1 \n 
~~ return offset \n 
~~ def do_receive_and_parse ( self ) : \n 
~~~ content_length = int ( self . headers . getheader ( ) ) \n 
request_body = self . rfile . read ( content_length ) \n 
request_array = request_body . split ( ) \n 
if len ( request_array ) < 2 : \n 
~~~ self . _logger . debug ( , request_body ) \n 
~~ bytes_to_send = request_array [ 0 ] \n 
~~~ bytes_to_send = int ( bytes_to_send ) \n 
~~~ self . _logger . debug ( , bytes_to_send ) \n 
~~ chunked_mode = False \n 
mode_parameter = request_array [ 1 ] \n 
if mode_parameter == : \n 
~~~ self . _logger . debug ( ) \n 
chunked_mode = True \n 
~~ elif mode_parameter != : \n 
~~~ self . _logger . debug ( , mode_parameter ) \n 
~~ self . do_receive ( bytes_to_send , chunked_mode , False ) \n 
~~ def __eq__ ( self , other ) : \n 
~~~ if self . weekday != other . weekday or self . n != other . n : \n 
~~ def _iter_cached ( self ) : \n 
~~~ i = 0 \n 
gen = self . _cache_gen \n 
cache = self . _cache \n 
acquire = self . _cache_lock . acquire \n 
release = self . _cache_lock . release \n 
while gen : \n 
~~~ if i == len ( cache ) : \n 
~~~ acquire ( ) \n 
if self . _cache_complete : \n 
~~~ for j in range ( 10 ) : \n 
~~~ cache . append ( gen . next ( ) ) \n 
~~~ self . _cache_gen = gen = None \n 
self . _cache_complete = True \n 
~~ release ( ) \n 
~~ yield cache [ i ] \n 
i += 1 \n 
~~ while i < self . _len : \n 
~~~ yield cache [ i ] \n 
~~ ~~ def __getitem__ ( self , item ) : \n 
~~~ if self . _cache_complete : \n 
~~~ return self . _cache [ item ] \n 
~~ elif isinstance ( item , slice ) : \n 
~~~ if item . step and item . step < 0 : \n 
~~~ return list ( iter ( self ) ) [ item ] \n 
~~~ return list ( itertools . islice ( self , \n 
item . start or 0 , \n 
item . stop or sys . maxint , \n 
item . step or 1 ) ) \n 
~~ ~~ elif item >= 0 : \n 
~~~ gen = iter ( self ) \n 
~~~ for i in range ( item + 1 ) : \n 
~~~ res = gen . next ( ) \n 
~~~ raise IndexError \n 
~~ ~~ def _iter ( self ) : \n 
~~~ year , month , day , hour , minute , second , weekday , yearday , _ = self . _dtstart . timetuple ( ) \n 
freq = self . _freq \n 
interval = self . _interval \n 
wkst = self . _wkst \n 
until = self . _until \n 
bymonth = self . _bymonth \n 
byweekno = self . _byweekno \n 
byyearday = self . _byyearday \n 
byweekday = self . _byweekday \n 
byeaster = self . _byeaster \n 
bymonthday = self . _bymonthday \n 
bynmonthday = self . _bynmonthday \n 
bysetpos = self . _bysetpos \n 
byhour = self . _byhour \n 
byminute = self . _byminute \n 
bysecond = self . _bysecond \n 
ii = _iterinfo ( self ) \n 
ii . rebuild ( year , month ) \n 
getdayset = { YEARLY : ii . ydayset , \n 
MONTHLY : ii . mdayset , \n 
WEEKLY : ii . wdayset , \n 
DAILY : ii . ddayset , \n 
HOURLY : ii . ddayset , \n 
MINUTELY : ii . ddayset , \n 
SECONDLY : ii . ddayset } [ freq ] \n 
if freq < HOURLY : \n 
~~~ timeset = self . _timeset \n 
~~~ gettimeset = { HOURLY : ii . htimeset , \n 
MINUTELY : ii . mtimeset , \n 
SECONDLY : ii . stimeset } [ freq ] \n 
if ( ( freq >= HOURLY and \n 
self . _byhour and hour not in self . _byhour ) or \n 
( freq >= MINUTELY and \n 
self . _byminute and minute not in self . _byminute ) or \n 
( freq >= SECONDLY and \n 
self . _bysecond and second not in self . _bysecond ) ) : \n 
~~~ timeset = ( ) \n 
~~~ timeset = gettimeset ( hour , minute , second ) \n 
~~ ~~ total = 0 \n 
count = self . _count \n 
~~~ dayset , start , end = getdayset ( year , month , day ) \n 
filtered = False \n 
for i in dayset [ start : end ] : \n 
~~~ if ( ( bymonth and ii . mmask [ i ] not in bymonth ) or \n 
( byweekno and not ii . wnomask [ i ] ) or \n 
( byweekday and ii . wdaymask [ i ] not in byweekday ) or \n 
( ii . nwdaymask and not ii . nwdaymask [ i ] ) or \n 
( byeaster and not ii . eastermask [ i ] ) or \n 
( ( bymonthday or bynmonthday ) and \n 
ii . mdaymask [ i ] not in bymonthday and \n 
ii . nmdaymask [ i ] not in bynmonthday ) or \n 
( byyearday and \n 
( ( i < ii . yearlen and i + 1 not in byyearday \n 
and - ii . yearlen + i not in byyearday ) or \n 
( i >= ii . yearlen and i + 1 - ii . yearlen not in byyearday \n 
and - ii . nextyearlen + i - ii . yearlen \n 
not in byyearday ) ) ) ) : \n 
~~~ dayset [ i ] = None \n 
filtered = True \n 
~~ ~~ if bysetpos and timeset : \n 
~~~ poslist = [ ] \n 
for pos in bysetpos : \n 
~~~ if pos < 0 : \n 
~~~ daypos , timepos = divmod ( pos , len ( timeset ) ) \n 
~~~ daypos , timepos = divmod ( pos - 1 , len ( timeset ) ) \n 
~~~ i = [ x for x in dayset [ start : end ] \n 
if x is not None ] [ daypos ] \n 
time = timeset [ timepos ] \n 
~~~ date = datetime . date . fromordinal ( ii . yearordinal + i ) \n 
res = datetime . datetime . combine ( date , time ) \n 
if res not in poslist : \n 
~~~ poslist . append ( res ) \n 
~~ ~~ ~~ poslist . sort ( ) \n 
for res in poslist : \n 
~~~ if until and res > until : \n 
~~~ self . _len = total \n 
~~ elif res >= self . _dtstart : \n 
~~~ total += 1 \n 
yield res \n 
if count : \n 
~~~ count -= 1 \n 
if not count : \n 
~~ ~~ ~~ ~~ ~~ else : \n 
~~~ for i in dayset [ start : end ] : \n 
~~~ if i is not None : \n 
for time in timeset : \n 
~~~ res = datetime . datetime . combine ( date , time ) \n 
if until and res > until : \n 
~~ ~~ ~~ ~~ ~~ ~~ ~~ fixday = False \n 
if freq == YEARLY : \n 
~~~ year += interval \n 
if year > datetime . MAXYEAR : \n 
~~ ii . rebuild ( year , month ) \n 
~~ elif freq == MONTHLY : \n 
~~~ month += interval \n 
if month > 12 : \n 
~~~ div , mod = divmod ( month , 12 ) \n 
month = mod \n 
year += div \n 
if month == 0 : \n 
~~~ month = 12 \n 
year -= 1 \n 
~~ if year > datetime . MAXYEAR : \n 
~~ ~~ ii . rebuild ( year , month ) \n 
~~ elif freq == WEEKLY : \n 
~~~ if wkst > weekday : \n 
~~~ day += - ( weekday + 1 + ( 6 - wkst ) ) + self . _interval * 7 \n 
~~~ day += - ( weekday - wkst ) + self . _interval * 7 \n 
~~ weekday = wkst \n 
fixday = True \n 
~~ elif freq == DAILY : \n 
~~~ day += interval \n 
~~ elif freq == HOURLY : \n 
~~~ if filtered : \n 
~~~ hour += ( ( 23 - hour ) // interval ) * interval \n 
~~~ hour += interval \n 
div , mod = divmod ( hour , 24 ) \n 
if div : \n 
~~~ hour = mod \n 
day += div \n 
~~ if not byhour or hour in byhour : \n 
~~ ~~ timeset = gettimeset ( hour , minute , second ) \n 
~~ elif freq == MINUTELY : \n 
~~~ minute += ( ( 1439 - ( hour * 60 + minute ) ) // interval ) * interval \n 
~~~ minute += interval \n 
div , mod = divmod ( minute , 60 ) \n 
~~~ minute = mod \n 
hour += div \n 
~~ ~~ if ( ( not byhour or hour in byhour ) and \n 
( not byminute or minute in byminute ) ) : \n 
~~ elif freq == SECONDLY : \n 
~~~ second += ( ( ( 86399 - ( hour * 3600 + minute * 60 + second ) ) \n 
// interval ) * interval ) \n 
~~~ second += self . _interval \n 
div , mod = divmod ( second , 60 ) \n 
~~~ second = mod \n 
minute += div \n 
~~ ~~ ~~ if ( ( not byhour or hour in byhour ) and \n 
( not byminute or minute in byminute ) and \n 
( not bysecond or second in bysecond ) ) : \n 
~~ if fixday and day > 28 : \n 
~~~ daysinmonth = calendar . monthrange ( year , month ) [ 1 ] \n 
if day > daysinmonth : \n 
~~~ while day > daysinmonth : \n 
~~~ day -= daysinmonth \n 
month += 1 \n 
if month == 13 : \n 
~~~ month = 1 \n 
year += 1 \n 
~~ ~~ daysinmonth = calendar . monthrange ( year , month ) [ 1 ] \n 
~~ ~~ ~~ ~~ def __init__ ( self , genlist , gen ) : \n 
~~~ self . dt = gen ( ) \n 
genlist . append ( self ) \n 
~~ self . genlist = genlist \n 
self . gen = gen \n 
~~ def next ( self ) : \n 
~~~ self . dt = self . gen ( ) \n 
~~~ self . genlist . remove ( self ) \n 
~~ ~~ def _handle_UNTIL ( self , rrkwargs , name , value , ** kwargs ) : \n 
~~~ global parser \n 
if not parser : \n 
~~~ from dateutil import parser \n 
~~~ rrkwargs [ "until" ] = parser . parse ( value , \n 
ignoretz = kwargs . get ( "ignoretz" ) , \n 
tzinfos = kwargs . get ( "tzinfos" ) ) \n 
~~ ~~ def _parse_rfc_rrule ( self , line , \n 
dtstart = None , \n 
cache = False , \n 
ignoretz = False , \n 
tzinfos = None ) : \n 
~~~ if line . find ( ) != - 1 : \n 
~~~ name , value = line . split ( ) \n 
if name != "RRULE" : \n 
~~~ value = line \n 
~~ rrkwargs = { } \n 
for pair in value . split ( ) : \n 
~~~ name , value = pair . split ( ) \n 
name = name . upper ( ) \n 
value = value . upper ( ) \n 
~~~ getattr ( self , "_handle_" + name ) ( rrkwargs , name , value , \n 
ignoretz = ignoretz , \n 
tzinfos = tzinfos ) \n 
~~ except ( KeyError , __HOLE__ ) : \n 
~~ ~~ return rrule ( dtstart = dtstart , cache = cache , ** rrkwargs ) \n 
~~ @ require_superuser \n 
def callcenter_test ( request ) : \n 
~~~ user_id = request . GET . get ( "user_id" ) \n 
date_param = request . GET . get ( "date" ) \n 
enable_caching = request . GET . get ( ) \n 
doc_id = request . GET . get ( ) \n 
if not user_id and not doc_id : \n 
~~~ return render ( request , "hqadmin/callcenter_test.html" , { "enable_caching" : enable_caching } ) \n 
~~ error = None \n 
user = None \n 
user_case = None \n 
domain = None \n 
if user_id : \n 
~~~ user = CommCareUser . get ( user_id ) \n 
domain = user . project \n 
~~ except ResourceNotFound : \n 
~~ ~~ elif doc_id : \n 
~~~ doc = CommCareUser . get_db ( ) . get ( doc_id ) \n 
domain = Domain . get_by_name ( doc [ ] ) \n 
doc_type = doc . get ( , None ) \n 
if doc_type == : \n 
~~~ case_type = domain . call_center_config . case_type \n 
user_case = get_case_by_domain_hq_user_id ( doc [ ] , doc [ ] , case_type ) \n 
~~ elif doc_type == : \n 
~~~ if doc . get ( ) : \n 
~~~ user_case = CommCareCase . wrap ( doc ) \n 
~~~ error = \n 
~~ ~~ ~~ except ResourceNotFound : \n 
~~~ query_date = dateutil . parser . parse ( date_param ) \n 
query_date = date . today ( ) \n 
~~ def view_data ( case_id , indicators ) : \n 
~~~ new_dict = SortedDict ( ) \n 
key_list = sorted ( indicators . keys ( ) ) \n 
for key in key_list : \n 
~~~ new_dict [ key ] = indicators [ key ] \n 
~~ return { \n 
: new_dict , \n 
: CommCareCase . get ( case_id ) , \n 
~~ if user or user_case : \n 
~~~ custom_cache = None if enable_caching else cache . caches [ ] \n 
cci = CallCenterIndicators ( \n 
domain . name , \n 
domain . default_timezone , \n 
domain . call_center_config . case_type , \n 
user , \n 
custom_cache = custom_cache , \n 
override_date = query_date , \n 
override_cases = [ user_case ] if user_case else None \n 
data = { case_id : view_data ( case_id , values ) for case_id , values in cci . get_data ( ) . items ( ) } \n 
~~~ data = { } \n 
~~ context = { \n 
"error" : error , \n 
"mobile_user" : user , \n 
"date" : json_format_date ( query_date ) , \n 
"enable_caching" : enable_caching , \n 
"data" : data , \n 
"doc_id" : doc_id \n 
return render ( request , "hqadmin/callcenter_test.html" , context ) \n 
~~ def get ( self , request , * args , ** kwargs ) : \n 
~~~ from django . core . exceptions import ValidationError \n 
if in request . GET : \n 
~~~ year , month = request . GET [ ] . split ( ) \n 
year , month = int ( year ) , int ( month ) \n 
return _malt_csv_response ( month , year ) \n 
~~~ messages . error ( \n 
request , \n 
~~ ~~ return super ( DownloadMALTView , self ) . get ( request , * args , ** kwargs ) \n 
~~~ def setupClass ( cls ) : \n 
~~~ global scipy \n 
global assert_equal \n 
global assert_almost_equal \n 
~~~ import scipy . linalg \n 
~~ ~~ ~~ def update_editor ( self ) : \n 
user_value = self . _get_user_value ( ) \n 
~~~ unequal = bool ( user_value != self . value ) \n 
~~~ unequal = True \n 
~~ if unequal : \n 
~~~ self . _no_update = True \n 
self . control . SetValue ( self . str_value ) \n 
self . _no_update = False \n 
~~ if self . _error is not None : \n 
~~~ self . _error = None \n 
self . ui . errors -= 1 \n 
self . set_error_state ( False ) \n 
#--------------------------------------------------------------------------- \n 
~~ ~~ def _get_user_value ( self ) : \n 
value = self . control . GetValue ( ) \n 
~~~ value = self . evaluate ( value ) \n 
~~~ ret = self . factory . mapping . get ( value , value ) \n 
~~~ ret = value \n 
~~ return ret \n 
~~ def _install ( path , remove = False , prefix = sys . prefix ) : \n 
~~~ if abspath ( prefix ) == abspath ( sys . prefix ) : \n 
~~~ env_name = None \n 
~~~ env_name = basename ( prefix ) \n 
~~ data = json . load ( open ( path ) ) \n 
~~~ menu_name = data [ ] \n 
~~~ menu_name = % sys . version_info [ : 2 ] \n 
~~ shortcuts = data [ ] \n 
m = Menu ( menu_name ) \n 
if remove : \n 
~~~ for sc in shortcuts : \n 
~~~ ShortCut ( m , sc , target_prefix = prefix , \n 
env_name = env_name ) . remove ( ) \n 
~~ m . remove ( ) \n 
~~~ m . create ( ) \n 
for sc in shortcuts : \n 
env_name = env_name ) . create ( ) \n 
~~ ~~ ~~ def __init__ ( self , * args , ** kwargs ) : \n 
~~~ super ( FormsetCell , self ) . __init__ ( * args , ** kwargs ) \n 
~~~ self . field = ( self . row . form or { } ) [ self . column . name ] \n 
~~~ self . field = None \n 
~~~ if self . field . errors : \n 
~~~ self . attrs [ ] = ( self . attrs . get ( , ) + \n 
self . attrs [ ] = . join ( \n 
unicode ( error ) for error in self . field . errors ) \n 
~~ ~~ ~~ def addToHeap ( self , uuid , type ) : \n 
~~~ if not uuid in self . unscheduled_items : \n 
~~~ if self . service_mapping and type in self . service_mapping : \n 
~~~ LOGGER . info ( \n 
% ( type , self . service_mapping [ type ] ) ) \n 
type = self . service_mapping [ type ] \n 
~~~ uuid_bytes = UUID ( uuid ) . bytes \n 
~~~ LOGGER . error ( \n 
% uuid ) \n 
~~ if type in self . functions and in self . functions [ type ] : \n 
~~~ interval = int ( self . functions [ type ] [ ] ) \n 
~~~ LOGGER . error ( % type ) \n 
~~ enqueue_time = int ( time . time ( ) + interval ) \n 
LOGGER . debug ( \n 
% ( uuid , enqueue_time , interval ) ) \n 
heappush ( self . heap , ( enqueue_time , ( uuid_bytes , interval ) ) ) \n 
~~~ LOGGER . info ( % uuid ) \n 
self . unscheduled_items . remove ( uuid ) \n 
~~ ~~ @ inlineCallbacks \n 
def main ( reactor , tor_binary ) : \n 
~~~ config = txtorcon . TorConfig ( ) \n 
config . ORPort = 0 \n 
config . SOCKSPort = 0 \n 
config . Tor2WebMode = 1 \n 
~~~ yield txtorcon . launch_tor ( \n 
config , \n 
reactor , \n 
tor_binary = tor_binary , \n 
stdout = sys . stdout \n 
reactor . callLater ( 5 , lambda : reactor . stop ( ) ) \n 
~~ def rm_file ( ) : \n 
~~~ os . unlink ( FILENAME ) \n 
~~ ~~ @ action ( ) \n 
~~~ def push_content ( self ) : \n 
~~~ logger . debug ( ) \n 
conf = config . getConfig ( ) \n 
post_target = \n 
logger . debug ( + post_target ) \n 
poster = conf . get ( , getpass . getuser ( ) ) \n 
logger . debug ( + poster ) \n 
filename = conf . require ( ) . name \n 
if filename == : \n 
~~~ content = conf . require ( ) . read ( ) \n 
~~~ logger . warn ( ) \n 
sys . exit ( 1 ) \n 
~~ lines = content . count ( ) \n 
bytes = len ( content ) \n 
logger . info ( % ( lines , bytes ) ) \n 
lang = conf . get ( , _get_language ( filename ) ) \n 
logger . debug ( + lang ) \n 
post_data = { \n 
: poster , \n 
: lang , \n 
: content , \n 
~~~ resp = self . req . post ( post_target , data = post_data , allow_redirects = False ) \n 
~~ except exceptions . RequestException as e : \n 
~~~ logger . info ( + e . __class__ . __name__ ) \n 
logger . error ( ) \n 
raise exception . ServerException ( e ) \n 
~~ logger . debug ( ) \n 
logger . info ( % ( resp . status_code , resp . reason ) ) \n 
if resp . status_code == 302 : \n 
~~~ pastepad = resp . headers [ ] \n 
logger . debug ( + pastepad ) \n 
pattern = re . compile ( ) \n 
res = pattern . match ( pastepad ) \n 
if not res : \n 
~~~ raise exception . ServerException ( + pastepad ) \n 
~~ paste_id = res . group ( pas \n 
te_id ) \n 
logger . info ( + paste_id ) \n 
print + str ( paste_id ) \n 
print + pastepad \n 
~~ if resp . status_code == 200 : \n 
~~~ data = resp . content \n 
err_stop_flag = \n 
msg = self . html2text ( self . fetch_between ( resp . content , err_start_flag , err_stop_flag ) ) \n 
raise exception . ServerException ( + msg ) \n 
~~ raise exception . ServerException ( % ( resp . status_code , resp . reason ) ) \n 
~~ ~~ def get_region ( latitude , longitude ) : \n 
url = __G_API_GEOCODE_URL__ + __G_API_GEOCODE_OUTPUT__ [ 0 ] + + str ( latitude ) + + str ( longitude ) + \n 
r = misc . load_json_url ( url ) \n 
country_l , country_s , region_l , region_s = , , , \n 
country_found = False \n 
region_found = False \n 
~~~ address_list = r [ \n 
__G_API_RESULTS_ARRAY__ \n 
] [ \n 
__G_API_ADDRESS_ITEM__ \n 
__G_API_ADDRESS_ARRAY__ \n 
] \n 
for a in address_list : \n 
~~~ for t in a [ __G_API_TYPES_ARRAY__ ] : \n 
~~~ if t == __G_API_ADDRESS_COUNTRY_AREA__ : \n 
~~~ country_l = a [ __G_API_LONG_NAME__ ] \n 
country_s = a [ __G_API_SHORT_NAME__ ] \n 
country_found = True \n 
~~ if t == __G_API_ADDRESS_REGION_AREA__ : \n 
~~~ region_l = a [ __G_API_LONG_NAME__ ] \n 
region_s = a [ __G_API_SHORT_NAME__ ] \n 
region_found = True \n 
~~ ~~ ~~ ~~ except __HOLE__ as ex : \n 
~~~ logger . exception ( \n 
+ \n 
+ str ( latitude ) + + str ( longitude ) + + \n 
+ str ( ex ) , \n 
ex \n 
~~ if not country_found and not region_found : \n 
COUNTRY_LONG_NAME : country_l , \n 
COUNTRY_SHORT_NAME : country_s , \n 
REGION_LONG_NAME : region_l , \n 
REGION_SHORT_NAME : region_s \n 
~~ @ sensitive_post_parameters ( ) \n 
@ never_cache \n 
def password_reset_confirm ( request , uidb36 = None , token = None , \n 
template_name = , \n 
token_generator = default_token_generator , \n 
set_password_form = SetPasswordForm , \n 
post_reset_redirect = None , \n 
current_app = None , extra_context = None ) : \n 
UserModel = get_user_model ( ) \n 
if post_reset_redirect is None : \n 
~~~ post_reset_redirect = reverse ( ) \n 
~~~ uid_int = base36_to_int ( uidb36 ) \n 
user = UserModel . _default_manager . get ( pk = uid_int ) \n 
~~ except ( __HOLE__ , OverflowError , UserModel . DoesNotExist ) : \n 
~~~ user = None \n 
~~ if user is not None and token_generator . check_token ( user , token ) : \n 
~~~ validlink = True \n 
if request . method == : \n 
~~~ form = set_password_form ( user , request . POST ) \n 
if form . is_valid ( ) : \n 
~~~ form . save ( ) \n 
return HttpResponseRedirect ( post_reset_redirect ) \n 
~~~ form = set_password_form ( None ) \n 
~~~ validlink = False \n 
form = None \n 
: form , \n 
: validlink , \n 
if extra_context is not None : \n 
~~~ context . update ( extra_context ) \n 
~~ return TemplateResponse ( request , template_name , context , \n 
current_app = current_app ) \n 
~~ @ task \n 
def commit ( ) : \n 
message = \n 
yes_ans = ( , ) \n 
with settings ( warn_only = True ) : \n 
res = local ( , capture = True ) \n 
if int ( res . strip ( ) ) : \n 
~~~ test_res = local ( , capture = True ) \n 
~~~ _ans = raw_input ( \n 
. format ( test_res . strip ( ) ) ) \n 
if _ans . lower ( ) in yes_ans : \n 
~~ ~~ except ( KeyboardInterrupt , __HOLE__ ) : \n 
~~ ~~ res = local ( , \n 
capture = True ) \n 
~~ ~~ except ( KeyboardInterrupt , SystemExit ) : \n 
~~~ print ( ) \n 
~~ ~~ ~~ def __call__ ( self , env , start_response ) : \n 
env [ ] = time . time ( ) \n 
if not in env : \n 
~~~ self . logger . warning ( \n 
return self . app ( env , start_response ) \n 
~~~ ( version , account , container , obj ) = split_path ( env [ ] , 2 , 4 , True ) \n 
~~~ return self . app ( env , start_response ) \n 
~~ if env [ ] not in ( , ) : \n 
~~ if env . get ( ) and not config_true_value ( env . get ( , ) ) : \n 
~~ if not container : \n 
~~ context = _StaticWebContext ( self , version , account , container , obj ) \n 
if obj : \n 
~~~ return context . handle_object ( env , start_response ) \n 
~~ return context . handle_container ( env , start_response ) \n 
~~ @ property \n 
~~~ def release_year ( self ) : \n 
default_date = datetime . datetime ( datetime . MINYEAR , 1 , 1 ) \n 
default_date = default_date . replace ( tzinfo = None ) \n 
date = self . _getter ( , ) \n 
~~~ parsed_date = dateutil . parser . parse ( date , default = default_date ) \n 
~~ parsed_date = parsed_date . replace ( tzinfo = None ) \n 
if parsed_date != default_date : \n 
~~~ return parsed_date . year \n 
~~ return None \n 
~~ ~~ @ property \n 
~~~ def track_number ( self ) : \n 
~~~ _number = int ( self . _getter ( ) ) \n 
~~ except ( TypeError , __HOLE__ ) : \n 
~~~ _number = None \n 
~~ return _number \n 
~~ ~~ @ staticmethod \n 
~~~ def test_links ( ) : \n 
~~~ tempfile = __file__ + \n 
~~~ os . link ( __file__ , tempfile ) \n 
~~~ del os . link \n 
~~~ if os . path . exists ( tempfile ) : \n 
~~~ os . remove ( tempfile ) \n 
~~ ~~ ~~ ~~ ~~ def convert_image ( target , image_size ) : \n 
~~~ _ , extension = os . path . splitext ( os . path . basename ( target ) ) \n 
if image_size and not all ( image_size ) : \n 
~~~ return target \n 
~~ invalid_extensions = ( \n 
~~~ image = Image . open ( target ) \n 
~~ image_format = image . format \n 
image_file_name = target \n 
if image_size is not None and image . size != image_size : \n 
~~~ image = image . resize ( image_size , Image . ANTIALIAS ) \n 
~~ ~~ if extension . lower ( ) in invalid_extensions : \n 
~~~ image_format = \n 
image_file_name = replace_ext ( target , ) \n 
~~~ image . save ( image_file_name , image_format ) \n 
~~ return image_file_name \n 
~~ def _convert_expr ( self , lj , expr ) : \n 
~~~ if len ( expr ) == 2 : \n 
~~~ tmp_exprs = expr [ 0 ] \n 
final_exprs = expr [ 1 ] \n 
if len ( final_exprs ) != 1 and self . signature . ret_type == ctypes . c_double : \n 
~~ for name , e in tmp_exprs : \n 
~~~ val = lj . _print ( e ) \n 
lj . _add_tmp_var ( name , val ) \n 
~~~ final_exprs = [ expr ] \n 
~~ vals = [ lj . _print ( e ) for e in final_exprs ] \n 
return vals \n 
~~~ super ( ZAIDField , self ) . clean ( value ) \n 
if value in EMPTY_VALUES : \n 
~~ value = value . strip ( ) . replace ( , ) . replace ( , ) \n 
match = re . match ( id_re , value ) \n 
if not match : \n 
~~~ raise ValidationError ( self . error_messages [ ] ) \n 
~~ g = match . groupdict ( ) \n 
~~~ d = date ( int ( g [ ] ) + 2000 , int ( g [ ] ) , int ( g [ ] ) ) \n 
~~ if not luhn ( value ) : \n 
~~ return value \n 
~~ def dumps ( self ) : \n 
~~~ return self . dumps_simple ( ) \n 
~~ functions = [ ] \n 
for f , kwargs in self . functions : \n 
~~~ for key in kwargs : \n 
~~~ kwargs [ key ] = kwargs [ key ] . to_dict ( ) \n 
~~ ~~ functions . append ( { \n 
: self . to_path ( f ) , \n 
: kwargs \n 
~~ def handler ( obj ) : \n 
~~~ json_handler ( obj ) \n 
~~~ if isinstance ( obj , SerializableFunction ) : \n 
~~~ return { : , : obj . dumps ( ) } \n 
~~ elif isfunction ( obj ) : \n 
~~~ return { : , : SerializableFunction ( obj ) . dumps ( ) } \n 
~~ ~~ ~~ return json . dumps ( functions , default = handler ) \n 
~~ def to_python ( self , value ) : \n 
~~~ return SerializableFunction ( ) \n 
~~~ return SerializableFunction . loads ( value ) \n 
~~~ return SerializableFunction ( to_function ( value ) ) \n 
~~ ~~ def _get_security_group ( self , context , id ) : \n 
~~~ id = int ( id ) \n 
security_group = db . security_group_get ( context , id ) \n 
raise exc . HTTPBadRequest ( explanation = msg ) \n 
~~ except exception . NotFound as exp : \n 
~~~ raise exc . HTTPNotFound ( explanation = unicode ( exp ) ) \n 
~~ return security_group \n 
~~ def _validate_security_group_property ( self , value , typ ) : \n 
~~~ val = value . strip ( ) \n 
~~ if not val : \n 
~~ if len ( val ) > 255 : \n 
~~ ~~ def create ( self , req , body ) : \n 
if not body : \n 
~~~ raise exc . HTTPUnprocessableEntity ( ) \n 
~~ if not in body : \n 
~~ self . compute_api . ensure_default_security_group ( context ) \n 
sg_rule = body [ ] \n 
parent_group_id = sg_rule . get ( , None ) \n 
~~~ parent_group_id = int ( parent_group_id ) \n 
security_group = db . security_group_get ( context , parent_group_id ) \n 
return exc . HTTPBadRequest ( explanation = msg ) \n 
return exc . HTTPNotFound ( explanation = msg ) \n 
LOG . audit ( msg , security_group [ ] , context = context ) \n 
~~~ values = self . _rule_args_to_dict ( context , \n 
to_port = sg_rule . get ( ) , \n 
from_port = sg_rule . get ( ) , \n 
parent_group_id = sg_rule . get ( ) , \n 
ip_protocol = sg_rule . get ( ) , \n 
cidr = sg_rule . get ( ) , \n 
group_id = sg_rule . get ( ) ) \n 
~~ except Exception as exp : \n 
~~~ raise exc . HTTPBadRequest ( explanation = unicode ( exp ) ) \n 
~~ if values is None : \n 
~~ values [ ] = security_group . id \n 
if self . _security_group_rule_exists ( security_group , values ) : \n 
~~~ msg = _ ( ) % parent_group_id \n 
~~ security_group_rule = db . security_group_rule_create ( context , values ) \n 
self . compute_api . trigger_security_group_rules_refresh ( context , \n 
security_group_id = security_group [ ] ) \n 
return { "security_group_rule" : self . _format_security_group_rule ( \n 
context , \n 
security_group_rule ) } \n 
~~ def _rule_args_to_dict ( self , context , to_port = None , from_port = None , \n 
parent_group_id = None , ip_protocol = None , \n 
cidr = None , group_id = None ) : \n 
~~~ values = { } \n 
if group_id : \n 
group_id = int ( group_id ) \n 
raise exception . InvalidInput ( reason = msg ) \n 
~~ if parent_group_id == group_id : \n 
~~ values [ ] = group_id \n 
db . security_group_get ( context , group_id ) \n 
~~ elif cidr : \n 
~~~ cidr = urllib . unquote ( cidr ) . decode ( ) \n 
netaddr . IPNetwork ( cidr ) \n 
~~~ raise exception . InvalidCidr ( cidr = cidr ) \n 
~~ values [ ] = cidr \n 
~~~ values [ ] = \n 
~~ if ip_protocol and from_port and to_port : \n 
~~~ from_port = int ( from_port ) \n 
to_port = int ( to_port ) \n 
~~~ raise exception . InvalidPortRange ( from_port = from_port , \n 
to_port = to_port ) \n 
~~ ip_protocol = str ( ip_protocol ) \n 
if ip_protocol . upper ( ) not in [ , , ] : \n 
~~~ raise exception . InvalidIpProtocol ( protocol = ip_protocol ) \n 
~~ if ( ( min ( from_port , to_port ) < - 1 ) or \n 
( max ( from_port , to_port ) > 65535 ) ) : \n 
~~ values [ ] = ip_protocol \n 
values [ ] = from_port \n 
values [ ] = to_port \n 
~~~ if in values : \n 
~~ ~~ return values \n 
~~ def delete ( self , req , id ) : \n 
self . compute_api . ensure_default_security_group ( context ) \n 
rule = db . security_group_rule_get ( context , id ) \n 
~~ group_id = rule . parent_group_id \n 
security_group = db . security_group_get ( context , group_id ) \n 
db . security_group_rule_destroy ( context , rule [ ] ) \n 
return exc . HTTPAccepted ( ) \n 
~~ def _addSecurityGroup ( self , input_dict , req , instance_id ) : \n 
~~~ body = input_dict [ ] \n 
group_name = body [ ] \n 
instance_id = int ( instance_id ) \n 
~~ if not group_name or group_name . strip ( ) == : \n 
~~~ self . compute_api . add_security_group ( context , instance_id , \n 
group_name ) \n 
~~ except exception . SecurityGroupNotFound as exp : \n 
~~~ return exc . HTTPNotFound ( explanation = unicode ( exp ) ) \n 
~~ except exception . InstanceNotFound as exp : \n 
~~ except exception . Invalid as exp : \n 
~~~ return exc . HTTPBadRequest ( explanation = unicode ( exp ) ) \n 
~~ return exc . HTTPAccepted ( ) \n 
~~ def _removeSecurityGroup ( self , input_dict , req , instance_id ) : \n 
~~~ self . compute_api . remove_security_group ( context , instance_id , \n 
~~ def main ( ) : \n 
~~~ parser = argparse . ArgumentParser ( description = ) \n 
parser . add_argument ( , action = "store" , type = str , help = ) \n 
parser . add_argument ( , action = "store" , type = int , help = ) \n 
parser . add_argument ( , action = "store" , type = int , default = 0 , help = ) \n 
result = parser . parse_args ( ) \n 
with closing ( ocpcaproj . OCPCAProjectsDB ( ) ) as projdb : \n 
~~~ proj = projdb . loadToken ( result . token ) \n 
~~ with closing ( ocpcadb . OCPCADB ( proj ) ) as db : \n 
~~~ ch = proj . getChannelObj ( result . channel ) \n 
[ [ ximagesz , yimagesz , zimagesz ] , ( starttime , endtime ) ] = proj . datasetcfg . imageSize ( result . resolution ) \n 
[ xcubedim , ycubedim , zcubedim ] = cubedim = proj . datasetcfg . getCubeDims ( ) [ result . resolution ] \n 
[ xoffset , yoffset , zoffset ] = proj . datasetcfg . getOffset ( ) [ result . resolution ] \n 
for slice_number in range ( zoffset , zimagesz + 1 , zcubedim ) : \n 
~~~ slab = np . zeros ( [ zcubedim , yimagesz , ximagesz ] , dtype = np . uint8 ) \n 
for b in range ( zcubedim ) : \n 
~~~ if ( slice_number + b <= zimagesz ) : \n 
~~~ file_name = "{}slice_{}.jpg" . format ( result . path , ( slice_number + b ) ) \n 
slab [ b , : , : ] = np . asarray ( Image . open ( file_name , ) ) \n 
~~~ print e \n 
slab [ b , : , : ] = np . zeros ( ( yimagesz , ximagesz ) , dtype = np . uint8 ) \n 
~~ ~~ ~~ for y in range ( 0 , yimagesz + 1 , ycubedim ) : \n 
~~~ for x in range ( 0 , ximagesz + 1 , xcubedim ) : \n 
~~~ zidx = ocplib . XYZMorton ( [ x / xcubedim , y / ycubedim , ( slice_number - zoffset ) / zcubedim ] ) \n 
cube = Cube . getCube ( cubedim , ch . getChannelType ( ) , ch . getDataType ( ) ) \n 
cube . zeros ( ) \n 
xmin , ymin = x , y \n 
xmax = min ( ximagesz , x + xcubedim ) \n 
ymax = min ( yimagesz , y + ycubedim ) \n 
zmin = 0 \n 
zmax = min ( slice_number + zcubedim , zimagesz + 1 ) \n 
cube . data [ 0 : zmax - zmin , 0 : ymax - ymin , 0 : xmax - xmin ] = slab [ zmin : zmax , ymin : ymax , xmin : xmax ] \n 
if cube . isNotZeros ( ) : \n 
~~~ db . putCube ( ch , zidx , result . resolution , cube , update = True ) \n 
~~ ~~ ~~ slab = None \n 
~~ ~~ ~~ def _split_proto_line ( line , allowed ) : \n 
~~~ fields = [ None ] \n 
~~~ fields = line . rstrip ( ) . split ( , 1 ) \n 
~~ command = fields [ 0 ] \n 
if allowed is not None and command not in allowed : \n 
~~~ raise UnexpectedCommandError ( command ) \n 
~~~ if len ( fields ) == 1 and command in ( , None ) : \n 
~~~ return ( command , None ) \n 
~~ elif len ( fields ) == 2 and command in ( , ) : \n 
~~~ hex_to_sha ( fields [ 1 ] ) \n 
return tuple ( fields ) \n 
~~ ~~ except ( TypeError , __HOLE__ ) , e : \n 
~~~ raise GitProtocolError ( e ) \n 
~~ raise GitProtocolError ( % line ) \n 
~~ def _apply_pack ( self , refs ) : \n 
~~~ f , commit = self . repo . object_store . add_thin_pack ( ) \n 
all_exceptions = ( IOError , OSError , ChecksumMismatch , ApplyDeltaError , \n 
AssertionError , socket . error , zlib . error , \n 
ObjectFormatException ) \n 
status = [ ] \n 
~~~ PackStreamCopier ( self . proto . read , self . proto . recv , f ) . verify ( ) \n 
p = commit ( ) \n 
if not p : \n 
~~~ raise IOError ( ) \n 
~~ p . check ( ) \n 
status . append ( ( , ) ) \n 
~~ except all_exceptions , e : \n 
~~~ status . append ( ( , str ( e ) . replace ( , ) ) ) \n 
~~ for oldsha , sha , ref in refs : \n 
~~~ ref_status = \n 
~~~ if sha == ZERO_SHA : \n 
~~~ if not in self . capabilities ( ) : \n 
~~~ raise GitProtocolError ( \n 
~~~ del self . repo . refs [ ref ] \n 
~~ except all_exceptions : \n 
~~~ self . repo . refs [ ref ] = sha \n 
~~ ~~ ~~ except __HOLE__ , e : \n 
~~ status . append ( ( ref , ref_status ) ) \n 
~~ return status \n 
~~ def fix_path ( ) : \n 
~~~ current_folder = os . path . abspath ( os . path . dirname ( __file__ ) ) \n 
lib_path = os . path . join ( current_folder , "libs" ) \n 
djangae_path = os . path . abspath ( os . path . join ( current_folder , os . pardir ) ) \n 
if lib_path not in sys . path : \n 
~~~ sys . path . insert ( 0 , lib_path ) \n 
~~ if djangae_path not in sys . path : \n 
~~~ sys . path . insert ( 0 , djangae_path ) \n 
~~ base_django_path = os . path . join ( current_folder , "submodules" , "django" ) \n 
django_path = os . path . join ( base_django_path , "django" ) \n 
django_tests_path = os . path . join ( base_django_path , "tests" ) \n 
if base_django_path not in sys . path : \n 
~~~ sys . path . insert ( 0 , base_django_path ) \n 
~~ if django_path not in sys . path : \n 
~~~ sys . path . insert ( 0 , django_path ) \n 
~~ if django_tests_path not in sys . path : \n 
~~~ sys . path . insert ( 0 , django_tests_path ) \n 
~~ os . environ [ ] = current_folder \n 
os . environ [ ] = \n 
~~~ import wrapper_util \n 
~~~ appengine_path = os . path . join ( lib_path , "google_appengine" ) \n 
sys . path . insert ( 0 , appengine_path ) \n 
simplejson_path = os . path . join ( appengine_path , "lib" , "simplejson" ) \n 
sys . path . insert ( 0 , simplejson_path ) \n 
~~ ~~ @ patch ( ) \n 
~~~ @ inlineCallbacks \n 
def test_runner_no_run ( self , fakereactor ) : \n 
~~~ fakereactor . connectTCP = Mock ( side_effect = raise_error ) \n 
runner = ApplicationRunner ( , ) \n 
~~~ yield runner . run ( raise_error , start_reactor = False ) \n 
~~ self . assertEqual ( fakereactor . run . call_count , 0 ) \n 
self . assertEqual ( fakereactor . stop . call_count , 0 ) \n 
~~ ~~ def clean_content ( self ) : \n 
~~~ content = self . cleaned_data . get ( , ) \n 
~~~ origin = StringOrigin ( content ) \n 
~~~ from django . template . debug import DebugLexer , DebugParser \n 
~~~ from django . template import Template \n 
~~~ template = Template ( template_string = origin ) \n 
~~ except Exception as e : \n 
~~~ self . exc_info = { \n 
: e . args , \n 
: e . token . lineno , \n 
: origin . name , \n 
raise forms . ValidationError ( ) \n 
~~~ template . compile_nodelist ( ) \n 
~~~ e . template_debug = template . get_exception_info ( e , e . token ) \n 
self . exc_info = e . template_debug \n 
~~~ lexer = DebugLexer ( content , origin ) \n 
~~~ parser = DebugParser ( lexer . tokenize ( ) ) \n 
parser . parse ( ) \n 
~~~ self . exc_info = sys . exc_info ( ) \n 
if not hasattr ( self . exc_info [ 1 ] , ) : \n 
~~~ self . exc_info [ 1 ] . django_template_source = origin , ( 0 , 0 ) \n 
~~ raise forms . ValidationError ( ) \n 
~~ ~~ ~~ return content \n 
~~ def clean ( self ) : \n 
~~~ if not self . errors : \n 
~~~ ModelClass = self . content_type . model_class ( ) \n 
~~~ current_instance = ModelClass . objects . get ( pk = self . instance_pk ) \n 
~~~ raise forms . ValidationError ( e ) \n 
~~ if not hasattr ( current_instance , self . field_name ) : \n 
~~~ raise forms . ValidationError ( . format ( \n 
ModelClass . __name__ , self . field_name ) ) \n 
~~ setattr ( current_instance , self . field_name , self . cleaned_data [ ] ) \n 
current_instance . full_clean ( ) \n 
~~ return self . cleaned_data \n 
~~ @ csrf_exempt \n 
def ajax_upload ( request , folder_id = None ) : \n 
mimetype = "application/json" if request . is_ajax ( ) else "text/html" \n 
content_type_key = if LTE_DJANGO_1_4 else \n 
response_params = { content_type_key : mimetype } \n 
folder = None \n 
if folder_id : \n 
~~~ folder = Folder . objects . get ( pk = folder_id ) \n 
~~ except Folder . DoesNotExist : \n 
~~~ return HttpResponse ( json . dumps ( { : NO_FOLDER_ERROR } ) , \n 
** response_params ) \n 
~~ ~~ if folder and not folder . has_add_children_permission ( request ) : \n 
~~~ return HttpResponse ( \n 
json . dumps ( { : NO_PERMISSIONS_FOR_FOLDER } ) , \n 
~~~ if len ( request . FILES ) == 1 : \n 
~~~ upload , filename , is_raw = handle_request_files_upload ( request ) \n 
~~~ upload , filename , is_raw = handle_upload ( request ) \n 
~~ for filer_class in filer_settings . FILER_FILE_MODELS : \n 
~~~ FileSubClass = load_object ( filer_class ) \n 
if FileSubClass . matches_file_type ( filename , upload , request ) : \n 
~~~ FileForm = modelform_factory ( \n 
model = FileSubClass , \n 
fields = ( , , ) \n 
~~ ~~ uploadform = FileForm ( { : filename , \n 
: request . user . pk } , \n 
{ : upload } ) \n 
if uploadform . is_valid ( ) : \n 
~~~ file_obj = uploadform . save ( commit = False ) \n 
file_obj . is_public = filer_settings . FILER_IS_PUBLIC_DEFAULT \n 
file_obj . folder = folder \n 
file_obj . save ( ) \n 
if not file_obj . icons : \n 
~~~ file_obj . delete ( ) \n 
return HttpResponse ( \n 
json . dumps ( \n 
{ : } ) , \n 
status = 500 , \n 
~~ thumbnail = None \n 
for size in ( [ ] + \n 
filer_settings . FILER_ADMIN_ICON_SIZES [ 1 : : - 1 ] ) : \n 
~~~ thumbnail = file_obj . icons [ size ] \n 
~~ ~~ json_response = { \n 
: thumbnail , \n 
: str ( file_obj ) , \n 
: file_obj . pk , \n 
if type ( file_obj ) == Image : \n 
~~~ thumbnail_180_options = { \n 
: ( 180 , 180 ) , \n 
: True , \n 
thumbnail_180 = file_obj . file . get_thumbnail ( \n 
thumbnail_180_options ) \n 
json_response [ ] = thumbnail_180 . url \n 
json_response [ ] = file_obj . url \n 
~~ return HttpResponse ( json . dumps ( json_response ) , \n 
~~~ form_errors = . join ( [ % ( \n 
field , \n 
. join ( errors ) ) for field , errors in list ( \n 
uploadform . errors . items ( ) ) \n 
] ) \n 
raise UploadException ( \n 
form_errors , ) ) \n 
~~ ~~ except UploadException as e : \n 
~~~ return HttpResponse ( json . dumps ( { : str ( e ) } ) , \n 
~~ ~~ def main ( ) : \n 
~~~ input_dir = sys . argv [ 1 ] \n 
output_dir = sys . argv [ 2 ] \n 
~~~ batch_counter = int ( sys . argv [ 3 ] ) \n 
~~~ batch_counter = 7 \n 
~~ batch_size = 10000 \n 
names = [ d for d in os . listdir ( input_dir ) if d . endswith ( ) ] \n 
names = natsorted ( names ) \n 
if batch_counter > 7 : \n 
~~~ omit_batches = batch_counter - 7 \n 
omit_images = omit_batches * batch_size \n 
names = names [ omit_images : ] \n 
~~ current_batch = get_empty_batch ( ) \n 
counter = 0 \n 
for n in names : \n 
~~~ image = Image . open ( os . path . join ( input_dir , n ) ) \n 
~~~ image = process ( image ) \n 
~~ image = image . reshape ( - 1 , 1 ) \n 
current_batch = np . hstack ( ( current_batch , image ) ) \n 
if current_batch . shape [ 1 ] == batch_size : \n 
~~~ batch_path = get_batch_path ( output_dir , batch_counter ) \n 
write_batch ( batch_path , current_batch ) \n 
batch_counter += 1 \n 
current_batch = get_empty_batch ( ) \n 
~~ counter += 1 \n 
if counter % 1000 == 0 : \n 
~~~ print n \n 
~~ ~~ ~~ def FileEntryExistsByPathSpec ( self , path_spec ) : \n 
tar_info = None \n 
location = getattr ( path_spec , , None ) \n 
if ( location is None or \n 
not location . startswith ( self . LOCATION_ROOT ) ) : \n 
~~ if len ( location ) == 1 : \n 
~~~ tar_info = self . _tar_file . getmember ( location [ 1 : ] ) \n 
~~ return tar_info is not None \n 
~~ def GetFileEntryByPathSpec ( self , path_spec ) : \n 
~~~ return dfvfs . vfs . tar_file_entry . TARFileEntry ( \n 
self . _resolver_context , self , path_spec , is_root = True , \n 
is_virtual = True ) \n 
~~ if tar_info is None : \n 
~~ return dfvfs . vfs . tar_file_entry . TARFileEntry ( \n 
self . _resolver_context , self , path_spec , tar_info = tar_info ) \n 
~~ def read_headers ( data , target = None ) : \n 
if target is None : \n 
~~~ cast = True \n 
target = { } \n 
~~~ cast = False \n 
~~ data = data . rstrip ( CRLF ) \n 
key = None \n 
if data : \n 
~~~ for line in data . split ( CRLF ) : \n 
~~~ if not line : \n 
~~~ raise BadRequest ( % line ) \n 
~~ if key and line [ 0 ] in : \n 
~~~ val = line . strip ( ) \n 
mline = True \n 
~~~ mline = False \n 
~~~ key , val = line . split ( , 1 ) \n 
~~ key = key . strip ( ) . lower ( ) \n 
val = val . strip ( ) \n 
~~~ val = int ( val ) \n 
~~ ~~ if key in target : \n 
~~~ if mline : \n 
~~~ if isinstance ( target [ key ] , list ) : \n 
~~~ if target [ key ] : \n 
~~~ target [ key ] [ - 1 ] += + val \n 
~~~ target [ key ] . append ( val ) \n 
~~~ target [ key ] += + val \n 
~~ ~~ elif key in COMMA_HEADERS : \n 
~~~ target [ key ] = % ( target [ key ] , val ) \n 
~~ elif isinstance ( target [ key ] , list ) : \n 
~~~ target [ key ] = [ target [ key ] , val ] \n 
~~ continue \n 
~~ target [ key ] = val \n 
~~ ~~ if cast : \n 
~~~ target = HTTPHeaders ( _store = target ) \n 
~~ return target \n 
~~ def parse_date ( text ) : \n 
~~~ for fmt in DATE_FORMATS : \n 
~~~ return datetime ( * time . strptime ( text , fmt ) [ : 6 ] ) \n 
~~ def get_pagination_params ( request ) : \n 
params = { } \n 
for param in [ , ] : \n 
~~~ if not param in request . GET : \n 
~~~ params [ param ] = int ( request . GET [ param ] ) \n 
~~~ msg = _ ( ) % param \n 
~~ if params [ param ] < 0 : \n 
~~ ~~ return params \n 
~~ def limited ( items , request , max_limit = FLAGS . osapi_max_limit ) : \n 
~~~ offset = int ( request . GET . get ( , 0 ) ) \n 
~~~ msg = _ ( ) \n 
~~~ limit = int ( request . GET . get ( , max_limit ) ) \n 
~~ if limit < 0 : \n 
~~ if offset < 0 : \n 
~~ limit = min ( max_limit , limit or max_limit ) \n 
range_end = offset + limit \n 
return items [ offset : range_end ] \n 
~~ def get_version_from_href ( href ) : \n 
~~~ version = re . findall ( , href ) \n 
if not version : \n 
~~ version = re . findall ( , version [ 0 ] ) [ 0 ] \n 
~~~ version = \n 
~~ return version \n 
~~ def delete_session_data ( self , request ) : \n 
~~~ try : del request . session [ % ( SESSION_KEY , key ) ] \n 
except __HOLE__ : pass \n 
~~ ~~ def ler_num ( ) : \n 
~~~ num = input ( ) \n 
~~~ num = float ( num ) \n 
~~ return num \n 
~~ def server ( ) : \n 
~~~ global kafka , producer , consumer \n 
if platform . system ( ) == : \n 
~~~ win32api . SetConsoleCtrlHandler ( handler , 1 ) \n 
~~~ signal . signal ( signal . SIGINT , onexit ) \n 
~~~ kafka = KafkaClient ( kafkaHost , timeout = None ) \n 
consumer = SimpleConsumer ( kafka , kafkaGroup , kafkaTopic , partitions = [ 0 , 1 , 2 ] ) \n 
for message in consumer : \n 
~~~ print ( message ) \n 
~~ ~~ except Exception as e : \n 
~~~ print ( . format ( e ) ) \n 
print ( ) \n 
print ( traceback . format_exc ( ) ) \n 
~~~ onexit ( ) \n 
~~ ~~ def blast ( request , pk ) : \n 
~~~ b = prepare_blasts ( \n 
Blast . objects . filter ( pk = pk ) , request . user \n 
) [ 0 ] \n 
~~~ raise Http404 \n 
~~ return render ( request , , { \n 
: b , \n 
: True \n 
~~ def process_comment ( self , req ) : \n 
~~~ url = req . params [ ] \n 
name = req . params [ ] \n 
homepage = req . params [ ] \n 
comments = req . params [ ] \n 
~~~ resp = exc . HTTPBadRequest ( % e ) \n 
~~ data = self . get_data ( url ) \n 
data . append ( dict ( \n 
name = name , \n 
homepage = homepage , \n 
comments = comments , \n 
time = time . gmtime ( ) ) ) \n 
self . save_data ( url , data ) \n 
resp = exc . HTTPSeeOther ( location = url + ) \n 
~~ def test_log_throws_errors_when_needed ( self ) : \n 
~~~ vcs = self . get_vcs ( ) \n 
~~~ vcs . log ( parent = , branch = ) . next ( ) \n 
self . fail ( ) \n 
~~ ~~ def to_unicode ( text ) : \n 
~~~ return unicode ( text , "utf-8" , errors = "ignore" ) \n 
~~ return text \n 
~~ def filter ( args ) : \n 
p = OptionParser ( filter . __doc__ ) \n 
p . add_option ( "--minsize" , default = 0 , type = "int" , \n 
p . add_option ( "--maxsize" , default = 1000000000 , type = "int" , \n 
p . add_option ( "--minaccn" , type = "int" , \n 
p . set_outfile ( ) \n 
opts , args = p . parse_args ( args ) \n 
if len ( args ) != 1 : \n 
~~~ sys . exit ( not p . print_help ( ) ) \n 
~~ bedfile , = args \n 
fp = must_open ( bedfile ) \n 
fw = must_open ( opts . outfile , "w" ) \n 
minsize , maxsize = opts . minsize , opts . maxsize \n 
minaccn = opts . minaccn \n 
minscore = opts . minscore \n 
total = [ ] \n 
keep = [ ] \n 
for row in fp : \n 
~~~ b = BedLine ( row ) \n 
~~~ print >> fw , row . strip ( ) \n 
~~ span = b . span \n 
total . append ( span ) \n 
if not minsize <= span <= maxsize : \n 
~~ if minaccn and int ( b . accn ) < minaccn : \n 
~~ if minscore and int ( b . score ) < minscore : \n 
~~ print >> fw , b \n 
keep . append ( span ) \n 
~~ def intersectBed_wao ( abedfile , bbedfile , minOverlap = 0 ) : \n 
~~~ abed = Bed ( abedfile ) \n 
bbed = Bed ( bbedfile ) \n 
acols = abed [ 0 ] . nargs \n 
bcols = bbed [ 0 ] . nargs \n 
fp = popen ( cmd ) \n 
~~~ atoms = row . split ( ) \n 
aline = "\\t" . join ( atoms [ : acols ] ) \n 
bline = "\\t" . join ( atoms [ acols : acols + bcols ] ) \n 
c = int ( atoms [ - 1 ] ) \n 
if c < minOverlap : \n 
~~ a = BedLine ( aline ) \n 
~~~ b = BedLine ( bline ) \n 
~~~ b = None \n 
~~ yield a , b \n 
~~ ~~ def wait_for_startup ( self ) : \n 
~~~ listening = False \n 
while not listening : \n 
~~~ socket . create_connection ( ( self . _http_ip ( ) , self . _http_port ( ) ) , \n 
1.0 ) \n 
~~~ listening = True \n 
~~ ~~ ~~ def phonetisaurus_installed ( ) : \n 
~~~ g2p . PhonetisaurusG2P ( ** g2p . PhonetisaurusG2P . get_config ( ) ) \n 
~~ ~~ def get_client ( project_id = None , credentials = None , \n 
service_url = None , service_account = None , \n 
private_key = None , private_key_file = None , \n 
json_key = None , json_key_file = None , \n 
readonly = True , swallow_results = True ) : \n 
if not credentials : \n 
~~~ assert ( service_account and ( private_key or private_key_file ) ) or ( \n 
json_key or json_key_file ) , \n 
~~ if not project_id : \n 
~~~ assert json_key or json_key_file , \n 
~~ if service_url is None : \n 
~~~ service_url = DISCOVERY_URI \n 
~~ scope = BIGQUERY_SCOPE_READ_ONLY if readonly else BIGQUERY_SCOPE \n 
if private_key_file : \n 
~~~ credentials = _credentials ( ) . from_p12_keyfile ( service_account , \n 
private_key_file , \n 
scopes = scope ) \n 
~~ if private_key : \n 
~~~ if isinstance ( private_key , basestring ) : \n 
~~~ private_key = private_key . decode ( ) \n 
~~ credentials = _credentials ( ) . from_p12_keyfile_buffer ( \n 
service_account , \n 
StringIO ( private_key ) , \n 
~~ if json_key_file : \n 
~~~ with open ( json_key_file , ) as key_file : \n 
~~~ json_key = json . load ( key_file ) \n 
~~ ~~ if json_key : \n 
~~~ credentials = _credentials ( ) . from_json_keyfile_dict ( json_key , \n 
if not project_id : \n 
~~~ project_id = json_key [ ] \n 
~~ ~~ bq_service = _get_bq_service ( credentials = credentials , \n 
service_url = service_url ) \n 
return BigQueryClient ( bq_service , project_id , swallow_results ) \n 
~~ def _log_rate_limit ( self , future ) : \n 
~~~ r = future . result ( ) \n 
~~~ r = e . response \n 
if r is None : \n 
~~ ~~ limit_s = r . headers . get ( , ) \n 
remaining_s = r . headers . get ( , ) \n 
if not remaining_s or not limit_s : \n 
~~~ if r . code < 300 : \n 
json . dumps ( dict ( r . headers ) , indent = 1 ) \n 
~~ return \n 
~~ remaining = int ( remaining_s ) \n 
limit = int ( limit_s ) \n 
if remaining == 0 : \n 
~~~ jsondata = response_text ( r ) \n 
data = json . loads ( jsondata ) \n 
~~ if 10 * remaining > limit : \n 
~~~ log = app_log . debug \n 
~~~ log = app_log . warn \n 
~~ def make_empty_files ( files ) : \n 
for f in files : \n 
~~~ path = os . path . dirname ( f ) \n 
if path and not os . path . exists ( path ) : \n 
~~~ os . makedirs ( path ) \n 
~~~ logging . error ( , path , e ) \n 
~~~ with open ( f , ) : \n 
~~~ os . utime ( f , None ) \n 
~~ ~~ except IOError as e : \n 
~~~ logging . error ( , f , e ) \n 
~~ ~~ ~~ def __eq__ ( self , other ) : \n 
~~~ return other and self . created_at == other . created_at and self . id == other . id and self . text == other . text and self . user == other . user and self . in_reply_to_screen_name == other . in_reply_to_screen_name and self . in_reply_to_user_id == other . in_reply_to_user_id and self . in_reply_to_status_id == other . in_reply_to_status_id and self . truncated == other . truncated and self . favorited == other . favorited and self . source == other . source \n 
~~ ~~ def __eq__ ( self , other ) : \n 
~~~ return other and self . id == other . id and self . name == other . name and self . screen_name == other . screen_name and self . location == other . location and self . description == other . description and self . profile_image_url == other . profile_image_url and self . profile_background_tile == other . profile_background_tile and self . profile_background_image_url == other . profile_background_image_url and self . profile_sidebar_fill_color == other . profile_sidebar_fill_color and self . profile_background_color == other . profile_background_color and self . profile_link_color == other . profile_link_color and self . profile_text_color == other . profile_text_color and self . protected == other . protected and self . utc_offset == other . utc_offset and self . time_zone == other . time_zone and self . url == other . url and self . statuses_count == other . statuses_count and self . followers_count == other . followers_count and self . favourites_count == other . favourites_count and self . friends_count == other . friends_count and self . status == other . status \n 
~~~ return other and self . id == other . id and self . created_at == other . created_at and self . sender == other . sender and self . sender_id == other . sender_id and self . sender_screen_name == other . sender_screen_name and self . recipient_id == other . recipient_id and self . recipient_screen_name == other . recipient_screen_name and self . text == other . text \n 
~~ ~~ def GetFriendsTimeline ( self , \n 
user = None , \n 
count = None , \n 
since = None , \n 
since_id = None ) : \n 
if user : \n 
~~~ url = TWITTER_API_ROOT + % user \n 
~~ elif not user and not self . _username : \n 
~~~ url = TWITTER_API_ROOT + \n 
~~ parameters = { } \n 
if count is not None : \n 
~~~ if int ( count ) > 200 : \n 
~~ parameters [ ] = count \n 
~~ if since : \n 
~~~ parameters [ ] = since \n 
~~ if since_id : \n 
~~~ parameters [ ] = since_id \n 
~~ json = self . _FetchUrl ( url , parameters = parameters ) \n 
data = simplejson . loads ( json ) \n 
self . _CheckForTwitterError ( data ) \n 
return [ Status . NewFromJsonDict ( x ) for x in data ] \n 
~~ def GetHomeTimeline ( self , \n 
~~ def _GetUsername ( self ) : \n 
~~~ return os . getenv ( ) or os . getenv ( ) or os . getenv ( ) or os . getlogin ( ) or \n 
~~ except ( IOError , __HOLE__ ) , e : \n 
~~ ~~ def _GetPath ( self , key ) : \n 
~~~ hashed_key = md5 ( key ) . hexdigest ( ) \n 
~~~ hashed_key = md5 . new ( key ) . hexdigest ( ) \n 
~~ return os . path . join ( self . _root_directory , \n 
self . _GetPrefix ( hashed_key ) , \n 
hashed_key ) \n 
~~ def _request ( self , path ) : \n 
~~~ response = request ( os . path . join ( self . server_url , path ) ) \n 
if response . status == 404 : \n 
~~ elif response . status != 200 : \n 
~~~ raise BootstrapSourceError ( % response . status ) \n 
~~~ return json . loads ( response . read ( ) ) \n 
~~~ raise BootstrapSourceError ( ) \n 
~~ ~~ @ classmethod \n 
~~~ def _get_or_create_host ( cls , hostname , create = True ) : \n 
~~~ return Host . get ( Host . hostname == hostname ) , False \n 
~~~ return Host . create ( hostname = hostname ) , True \n 
~~ return None , None \n 
~~~ def _get_or_create_cdn ( cls , cdn_id , create = True ) : \n 
~~~ return CDN . get ( CDN . id == cdn_id ) , False \n 
~~~ return CDN . create ( id = cdn_id ) , True \n 
~~ ~~ ~~ def setter ( self , widget , value ) : \n 
~~~ idx = _find_combo_data ( widget , value ) \n 
~~~ idx = - 1 \n 
~~ ~~ widget . setCurrentIndex ( idx ) \n 
~~ def getter ( self , widget ) : \n 
~~~ return float ( widget . text ( ) ) \n 
~~ ~~ def connect_current_combo ( client , prop , widget ) : \n 
def update_widget ( value ) : \n 
~~ def update_prop ( idx ) : \n 
~~~ if idx == - 1 : \n 
~~~ setattr ( client , prop , None ) \n 
~~~ setattr ( client , prop , widget . itemData ( idx ) ) \n 
~~ ~~ add_callback ( client , prop , update_widget ) \n 
widget . currentIndexChanged . connect ( update_prop ) \n 
update_widget ( getattr ( client , prop ) ) \n 
~~ def connect_float_edit ( client , prop , widget ) : \n 
v = QtGui . QDoubleValidator ( None ) \n 
v . setDecimals ( 4 ) \n 
widget . setValidator ( v ) \n 
def update_prop ( ) : \n 
~~~ val = widget . text ( ) \n 
~~~ setattr ( client , prop , float ( val ) ) \n 
~~~ setattr ( client , prop , 0 ) \n 
~~ ~~ def update_widget ( val ) : \n 
~~~ if val is None : \n 
~~~ val = 0. \n 
~~ widget . setText ( pretty_number ( val ) ) \n 
~~ add_callback ( client , prop , update_widget ) \n 
widget . editingFinished . connect ( update_prop ) \n 
~~ def _build_story ( self , all_rows ) : \n 
all_stories = [ ] \n 
for ( info , detail ) in all_rows : \n 
~~~ info_cells = info . findAll ( ) \n 
rank = int ( info_cells [ 0 ] . string [ : - 1 ] ) \n 
title = % info_cells [ 2 ] . find ( ) . string \n 
link = info_cells [ 2 ] . find ( ) . get ( ) \n 
is_self = False \n 
if link . find ( ) is - 1 : \n 
~~~ domain = info_cells [ 2 ] . findAll ( ) [ 1 ] . string [ 2 : - 1 ] \n 
~~~ link = % ( BASE_URL , link ) \n 
domain = BASE_URL \n 
is_self = True \n 
~~ detail_cell = detail . findAll ( ) [ 1 ] \n 
detail_concern = detail_cell . contents \n 
num_comments = - 1 \n 
if re . match ( , detail_concern [ 0 ] . string ) is not None : \n 
~~~ points = int ( re . match ( , detail_concern [ \n 
0 ] . string ) . groups ( ) [ 0 ] ) \n 
submitter = % detail_concern [ 2 ] . string \n 
submitter_profile = % ( BASE_URL , detail_concern [ \n 
2 ] . get ( ) ) \n 
published_time = . join ( detail_concern [ 3 ] . strip ( ) . split ( ) [ \n 
: 3 ] ) \n 
comment_tag = detail_concern [ 4 ] \n 
story_id = int ( re . match ( , comment_tag . get ( \n 
) ) . groups ( ) [ 0 ] ) \n 
comments_link = % ( BASE_URL , story_id ) \n 
comment_count = re . match ( , comment_tag . string ) \n 
~~~ num_comments = int ( comment_count . groups ( ) [ 0 ] ) \n 
~~~ num_comments = 0 \n 
~~~ points = 0 \n 
submitter = \n 
submitter_profile = \n 
published_time = % detail_concern [ 0 ] \n 
comment_tag = \n 
~~~ story_id = int ( re . match ( , link ) . groups ( ) [ 0 ] ) \n 
~~~ story_id = - 1 \n 
~~ comments_link = \n 
comment_count = - 1 \n 
~~ story = Story ( rank , story_id , title , link , domain , points , \n 
submitter , published_time , submitter_profile , \n 
num_comments , comments_link , is_self ) \n 
all_stories . append ( story ) \n 
~~ return all_stories \n 
~~ def _build_comments ( self , soup ) : \n 
comments = [ ] \n 
current_page = 1 \n 
~~~ if current_page == 1 : \n 
~~~ table = soup . findChildren ( ) [ 3 ] \n 
~~ elif current_page > 1 : \n 
~~~ table = soup . findChildren ( ) [ 2 ] \n 
~~ rows = table . findChildren ( [ ] ) \n 
rows = rows [ : len ( rows ) - 2 ] \n 
rows = [ row for i , row in enumerate ( rows ) if ( i % 2 == 0 ) ] \n 
if len ( rows ) > 1 : \n 
~~~ for row in rows : \n 
~~~ if not row . findChildren ( ) : \n 
~~ level = int ( row . findChildren ( ) [ 1 ] . find ( ) . get ( \n 
) ) // 40 \n 
spans = row . findChildren ( ) [ 3 ] . findAll ( ) \n 
~~~ user = spans [ 0 ] . contents [ 0 ] . string \n 
time_ago = spans [ 0 ] . contents [ 1 ] . string . strip ( \n 
) . rstrip ( ) \n 
~~~ comment_id = int ( re . match ( , \n 
spans [ 0 ] . contents [ \n 
2 ] . get ( \n 
~~~ comment_id = int ( re . match ( % \n 
BASE_URL , \n 
~~ body = spans [ 1 ] . text \n 
if body [ - 2 : ] == : \n 
~~~ body = body [ : - 5 ] \n 
~~~ pat = re . compile ( \n 
body_html = re . match ( pat , str ( spans [ 1 ] ) . replace ( \n 
, ) ) . groups ( ) [ 0 ] \n 
~~~ user = \n 
time_ago = \n 
comment_id = - 1 \n 
body = \n 
body_html = \n 
~~ comment = Comment ( comment_id , level , user , time_ago , \n 
body , body_html ) \n 
comments . append ( comment ) \n 
~~ ~~ next_page_url = self . _get_next_page ( soup , current_page ) \n 
if not next_page_url : \n 
~~ soup = get_soup ( page = next_page_url ) \n 
current_page += 1 \n 
~~ previous_comment = None \n 
return comments \n 
~~~ def fromid ( self , item_id ) : \n 
if not item_id : \n 
~~ soup = get_item_soup ( item_id ) \n 
story_id = item_id \n 
rank = - 1 \n 
info_table = soup . findChildren ( ) [ 2 ] \n 
info_rows = info_table . findChildren ( ) \n 
title_row = info_rows [ 0 ] . findChildren ( ) [ 1 ] \n 
title = title_row . find ( ) . text \n 
~~~ domain = title_row . find ( ) . string [ 2 : - 2 ] \n 
link = title_row . find ( ) . get ( ) \n 
~~~ domain = BASE_URL \n 
link = % ( BASE_URL , item_id ) \n 
~~ meta_row = info_rows [ 1 ] . findChildren ( ) [ 1 ] . contents \n 
points = int ( re . match ( , meta_row [ 0 ] . text ) . groups ( ) [ 0 ] ) \n 
submitter = meta_row [ 2 ] . text \n 
submitter_profile = % ( BASE_URL , meta_row [ 2 ] . get ( ) ) \n 
published_time = . join ( meta_row [ 3 ] . strip ( ) . split ( ) [ : 3 ] ) \n 
comments_link = % ( BASE_URL , item_id ) \n 
~~~ num_comments = int ( re . match ( , meta_row [ \n 
4 ] . text ) . groups ( ) [ 0 ] ) \n 
~~ story = Story ( rank , story_id , title , link , domain , points , submitter , \n 
published_time , submitter_profile , num_comments , \n 
comments_link , is_self ) \n 
return story \n 
~~ ~~ def _handle_uploaded_file ( self , field_name ) : \n 
~~~ file = self . files [ field_name ] \n 
filepath = self . _get_upload_name ( file . name ) \n 
~~ destination = open ( settings . MEDIA_ROOT + filepath , ) \n 
for chunk in file . chunks ( ) : \n 
~~~ destination . write ( chunk ) \n 
~~ destination . close ( ) \n 
return settings . MEDIA_URL + filepath \n 
~~ def python_reloader ( main_func , filepatterns , args , kwargs ) : \n 
~~~ if os . environ . get ( "RUN_MAIN" ) == "true" : \n 
~~~ _thread . start_new_thread ( main_func , args , kwargs ) \n 
~~~ reloader_thread ( filepatterns ) \n 
~~~ sys . exit ( restart_with_reloader ( ) ) \n 
~~ except KeyboardInterrupt : \n 
~~ ~~ ~~ def call_magic ( self , mtype , name , code , args ) : \n 
~~~ self . code = code \n 
old_args = args \n 
mtype = mtype . replace ( , ) \n 
func = getattr ( self , mtype + + name ) \n 
~~~ args , kwargs = _parse_args ( func , args , usage = self . get_help ( mtype , name ) ) \n 
~~~ self . kernel . Error ( str ( e ) ) \n 
return self \n 
~~ arg_spec = inspect . getargspec ( func ) \n 
fargs = arg_spec . args \n 
if fargs [ 0 ] == : \n 
~~~ fargs = fargs [ 1 : ] \n 
~~ fargs = [ f for f in fargs if not f in kwargs . keys ( ) ] \n 
if len ( args ) > len ( fargs ) and not arg_spec . varargs : \n 
~~~ extra = . join ( str ( s ) for s in ( args [ len ( fargs ) - 1 : ] ) ) \n 
args = args [ : len ( fargs ) - 1 ] + [ extra ] \n 
~~~ func ( * args , ** kwargs ) \n 
~~~ func ( old_args ) \n 
~~ ~~ except Exception as exc : \n 
name , mtype , str ( exc ) , args , kwargs ) \n 
self . kernel . Error ( msg ) \n 
self . kernel . Error ( traceback . format_exc ( ) ) \n 
self . kernel . Error ( self . get_help ( mtype , name ) ) \n 
return Magic ( self . kernel ) \n 
~~ return self \n 
~~ def parse_synopsis ( page , cleanup = None ) : \n 
~~~ soup = BeautifulSoup ( page ) \n 
~~~ result = soup . find ( , attrs = { : } ) . text \n 
if cleanup : \n 
~~~ result , _ = result . split ( cleanup ) \n 
~~~ print ( % e ) \n 
~~ ~~ def registerDeferred ( self , event , d ) : \n 
~~~ self . _evq [ event ] . schedule ( d ) \n 
~~ ~~ def _write_fields ( field_descriptors , out ) : \n 
out << \n 
for field in field_descriptors or [ ] : \n 
~~~ type_format = \n 
label_format = \n 
message_field = _MESSAGE_FIELD_MAP . get ( field . type_name ) \n 
if message_field : \n 
field_type = message_field \n 
field_type = messages . Field . lookup_field_type_by_variant ( field . variant ) \n 
~~ if field_type in ( messages . EnumField , messages . MessageField ) : \n 
~~~ type_format = % field . type_name \n 
~~ if field . label == descriptor . FieldDescriptor . Label . REQUIRED : \n 
~~~ label_format = \n 
~~ elif field . label == descriptor . FieldDescriptor . Label . REPEATED : \n 
~~ if field_type . DEFAULT_VARIANT != field . variant : \n 
~~~ variant_format = % field . variant \n 
~~~ variant_format = \n 
~~ if field . default_value : \n 
~~~ if field_type in [ messages . BytesField , \n 
messages . StringField , \n 
] : \n 
~~~ default_value = repr ( field . default_value ) \n 
~~ elif field_type is messages . EnumField : \n 
~~~ default_value = str ( int ( field . default_value ) ) \n 
~~~ default_value = field . default_value \n 
~~ default_format = % ( default_value , ) \n 
~~~ default_format = \n 
~~ out << % ( field . name , \n 
module , \n 
field_type . __name__ , \n 
type_format , \n 
field . number , \n 
label_format , \n 
variant_format , \n 
default_format ) \n 
~~ ~~ def _is_valid_json ( self , doc ) : \n 
~~~ js = json . loads ( doc ) \n 
return js \n 
~~ ~~ def _read_proc_stdout ( self ) : \n 
doc = \n 
js = None \n 
~~~ if not self . _proc_is_alive ( ) : \n 
~~ r , _ , _ = select ( [ self . proc . stdout ] , [ ] , [ ] , self . timeout ) \n 
if self . proc . stdout not in r : \n 
~~~ raise TimeoutError ( ) \n 
~~ doc += self . proc . stdout . readline ( ) . rstrip ( ) \n 
js = self . _is_valid_json ( doc ) \n 
if js and self . _is_ignored ( js ) : \n 
~~~ doc = \n 
~~ elif js : \n 
~~ ~~ except ( TimeoutError , __HOLE__ , Exception ) as e : \n 
~~~ self . _logger ( , str ( e ) ) \n 
self . _kill_burp ( ) \n 
~~ ~~ return js \n 
~~ def status ( self , query = , agent = None ) : \n 
if not query . endswith ( ) : \n 
~~~ query = . format ( query ) \n 
~~ if not self . _proc_is_alive ( ) : \n 
~~~ self . _spawn_burp ( ) \n 
~~ _ , w , _ = select ( [ ] , [ self . proc . stdin ] , [ ] , self . timeout ) \n 
if self . proc . stdin not in w : \n 
~~ self . proc . stdin . write ( query ) \n 
js = self . _read_proc_stdout ( ) \n 
if self . _is_warning ( js ) : \n 
~~~ self . _logger ( , js [ ] ) \n 
self . _logger ( , ) \n 
return None \n 
~~ self . _logger ( , . format ( js ) ) \n 
~~ except TimeoutError as e : \n 
~~~ msg = . format ( str ( e ) ) \n 
self . _logger ( , msg ) \n 
raise BUIserverException ( msg ) \n 
~~ except ( __HOLE__ , Exception ) as e : \n 
~~ ~~ def get_backup_logs ( self , number , client , forward = False , agent = None ) : \n 
ret = { } \n 
if not client or not number : \n 
~~~ return ret \n 
~~ query = self . status ( . format ( client , number ) ) \n 
if not query : \n 
~~~ logs = query [ ] [ 0 ] [ ] [ 0 ] [ ] [ ] \n 
~~~ self . _logger ( , ) \n 
return ret \n 
~~ if in logs : \n 
~~~ ret = self . _parse_backup_stats ( number , client , forward ) \n 
~~ ret [ ] = False \n 
if in ret and ret [ ] [ ] > 0 : \n 
~~~ ret [ ] = True \n 
~~ def _guess_backup_protocol ( self , number , client ) : \n 
query = self . status ( . format ( client , number ) ) \n 
~~~ log = query [ ] [ 0 ] [ ] [ 0 ] [ ] [ ] \n 
for line in log : \n 
~~~ if re . search ( , line ) : \n 
~~~ return 2 \n 
~~ ~~ ~~ except __HOLE__ as e : \n 
~~~ return 1 \n 
~~ return 1 \n 
~~ def _parse_backup_stats ( self , number , client , forward = False , agent = None ) : \n 
backup = { : , : int ( number ) } \n 
if forward : \n 
~~~ backup [ ] = client \n 
~~ translate = { \n 
counts = { \n 
single = [ , , , , , ] \n 
query = self . status ( . format ( client , number ) , agent = agent ) \n 
~~~ back = query [ ] [ 0 ] [ ] [ 0 ] \n 
~~ if not in back [ ] : \n 
~~ stats = None \n 
~~~ stats = json . loads ( . join ( back [ ] [ ] ) ) \n 
~~~ stats = back [ ] [ ] \n 
~~ if not stats : \n 
~~ if not in stats : \n 
~~~ return super ( Burp , self ) . _parse_backup_stats ( number , client , forward , stats , agent ) \n 
~~ counters = stats [ ] \n 
for counter in counters : \n 
~~~ name = counter [ ] \n 
if name in translate : \n 
~~~ name = translate [ name ] \n 
~~ if counter [ ] in single : \n 
~~~ backup [ name ] = counter [ ] \n 
~~~ backup [ name ] = { } \n 
for ( k , v ) in iteritems ( counts ) : \n 
~~~ if v in counter : \n 
~~~ backup [ name ] [ k ] = counter [ v ] \n 
~~~ backup [ name ] [ k ] = 0 \n 
~~ ~~ ~~ ~~ if in backup and in backup : \n 
~~~ backup [ ] = backup [ ] - backup [ ] \n 
~~ return backup \n 
~~ def get_counters ( self , name = None , agent = None ) : \n 
~~ ~~ query = self . status ( . format ( name ) ) \n 
~~~ client = query [ ] [ 0 ] \n 
~~ if not in client or client [ ] != : \n 
~~ backup = None \n 
for back in client [ ] : \n 
~~~ if in back and in back [ ] : \n 
~~~ backup = back \n 
~~ ~~ if not backup : \n 
~~ single = [ \n 
translate = { : } \n 
for counter in backup [ ] : \n 
~~ if counter [ ] not in single : \n 
~~~ ret [ name ] = [ counter [ ] , counter [ ] , counter [ ] , counter [ ] , counter [ ] ] \n 
~~~ ret [ name ] = counter [ ] \n 
~~ ~~ if not in ret : \n 
~~~ ret [ ] = 0 \n 
~~ if ret . viewkeys ( ) & { , , } : \n 
~~~ diff = time . time ( ) - int ( ret [ ] ) \n 
byteswant = int ( ret [ ] ) \n 
bytesgot = int ( ret [ ] ) \n 
ret [ ] = bytespersec \n 
if ( bytespersec > 0 ) : \n 
ret [ ] = timeleft \n 
~~~ ret [ ] = - 1 \n 
~~~ ret [ ] = round ( float ( ret [ ] ) / float ( ret [ ] ) * 100 ) \n 
~~ def is_backup_running ( self , name = None , agent = None ) : \n 
if not name : \n 
~~~ query = self . status ( . format ( name ) ) \n 
~~ except BUIserverException : \n 
~~ if not query : \n 
~~~ return query [ ] [ 0 ] [ ] in [ ] \n 
~~ return False \n 
~~ def get_client ( self , name = None , agent = None ) : \n 
ret = [ ] \n 
~~ query = self . status ( . format ( name ) ) \n 
~~~ backups = query [ ] [ 0 ] [ ] \n 
~~ except KeyError as e : \n 
~~ for backup in backups : \n 
~~~ back = { } \n 
if in backup and in backup [ ] : \n 
~~ back [ ] = backup [ ] \n 
~~~ back [ ] = True \n 
~~~ back [ ] = False \n 
log = self . get_backup_logs ( backup [ ] , name ) \n 
~~~ back [ ] = log [ ] \n 
~~~ back [ ] = 0 \n 
~~ back [ ] = log [ ] \n 
back [ ] = log [ ] \n 
ret . append ( back ) \n 
pass \n 
~~ ~~ ret . reverse ( ) \n 
~~ ~~ query = self . status ( . format ( name , backup , top ) ) \n 
~~~ backup = query [ ] [ 0 ] [ ] [ 0 ] \n 
~~ for entry in backup [ ] [ ] : \n 
if entry [ ] == : \n 
~~~ data [ ] = entry [ ] \n 
~~ data [ ] = self . _human_st_mode ( entry [ ] ) \n 
if re . match ( , data [ ] ) : \n 
~~~ data [ ] = \n 
~~ data [ ] = entry [ ] \n 
data [ ] = entry [ ] \n 
data [ ] = top \n 
data [ ] = . format ( _hr ( entry [ ] ) ) \n 
ret . append ( data ) \n 
~~ def get_client_labels ( self , client = None , agent = None ) : \n 
if not client : \n 
~~ query = self . status ( . format ( client ) ) \n 
~~~ return query [ ] [ 0 ] [ ] \n 
~~ ~~ def test_init_process ( worker ) : \n 
~~~ with mock . patch ( ) as m_asyncio : \n 
~~~ worker . init_process ( ) \n 
~~ assert m_asyncio . get_event_loop . return_value . close . called \n 
assert m_asyncio . new_event_loop . called \n 
assert m_asyncio . set_event_loop . called \n 
~~ ~~ def print_provider ( doc , provider , formatters , excludes = None , output = None ) : \n 
~~~ output = output or sys . stdout \n 
if excludes is None : \n 
~~~ excludes = [ ] \n 
~~ print ( file = output ) \n 
doc . get_provider_name ( provider ) ) , file = output ) \n 
print ( file = output ) \n 
for signature , example in formatters . items ( ) : \n 
~~~ if signature in excludes : \n 
~~~ lines = text_type ( example ) . expandtabs ( ) . splitlines ( ) \n 
~~~ lines = [ "<bytes>" ] \n 
~~ except UnicodeEncodeError : \n 
signature , example ) ) \n 
~~ margin = max ( 30 , doc . max_name_len + 1 ) \n 
remains = 150 - margin \n 
separator = \n 
for line in lines : \n 
~~~ for i in range ( 0 , ( len ( line ) // remains ) + 1 ) : \n 
fake = signature , \n 
separator = separator , \n 
example = line [ i * remains : ( i + 1 ) * remains ] , \n 
margin = margin \n 
) , file = output ) \n 
signature = separator = \n 
~~ ~~ ~~ ~~ def print_doc ( provider_or_field = None , \n 
args = None , lang = DEFAULT_LOCALE , output = None , includes = None ) : \n 
~~~ args = args or [ ] \n 
output = output or sys . stdout \n 
fake = Faker ( locale = lang , includes = includes ) \n 
from faker . providers import BaseProvider \n 
base_provider_formatters = [ f for f in dir ( BaseProvider ) ] \n 
if provider_or_field : \n 
~~~ if in provider_or_field : \n 
~~~ parts = provider_or_field . split ( ) \n 
locale = parts [ - 2 ] if parts [ - 2 ] in AVAILABLE_LOCALES else lang \n 
fake = Factory . create ( locale , providers = [ provider_or_field ] , includes = includes ) \n 
doc = documentor . Documentor ( fake ) \n 
doc . already_generated = base_provider_formatters \n 
print_provider ( \n 
doc , \n 
fake . get_providers ( ) [ 0 ] , \n 
doc . get_provider_formatters ( fake . get_providers ( ) [ 0 ] ) , \n 
output = output ) \n 
~~~ print ( fake . format ( provider_or_field , * args ) , end = , file = output ) \n 
provider_or_field , args ) ) \n 
~~~ doc = documentor . Documentor ( fake ) \n 
formatters = doc . get_formatters ( with_args = True , with_defaults = True ) \n 
for provider , fakers in formatters : \n 
~~~ print_provider ( doc , provider , fakers , output = output ) \n 
~~ for language in AVAILABLE_LOCALES : \n 
~~~ if language == lang : \n 
print ( . format ( language ) , file = output ) \n 
fake = Faker ( locale = language ) \n 
d = documentor . Documentor ( fake ) \n 
for p , fs in d . get_formatters ( with_args = True , with_defaults = True , \n 
locale = language , \n 
excludes = base_provider_formatters ) : \n 
~~~ print_provider ( d , p , fs , output = output ) \n 
~~ ~~ ~~ ~~ @ internationalizeDocstring \n 
~~~ def contributors ( self , irc , msg , args , cb , nick ) : \n 
def getShortName ( authorInfo ) : \n 
return % authorInfo . __dict__ \n 
~~ def buildContributorsString ( longList ) : \n 
L = [ getShortName ( n ) for n in longList ] \n 
return format ( , L ) \n 
~~ def sortAuthors ( ) : \n 
L = list ( module . __contributors__ . items ( ) ) \n 
def negativeSecondElement ( x ) : \n 
~~~ return - len ( x [ 1 ] ) \n 
~~ utils . sortBy ( negativeSecondElement , L ) \n 
return [ t [ 0 ] for t in L ] \n 
~~ def buildPeopleString ( module ) : \n 
head = _ ( ) % cb . name ( ) \n 
author = _ ( ) \n 
conjunction = _ ( ) \n 
contrib = _ ( ) \n 
hasAuthor = False \n 
hasContribs = False \n 
if hasattr ( module , ) : \n 
~~~ if module . __author__ != supybot . authors . unknown : \n 
~~~ author = _ ( ) % utils . web . mungeEmail ( str ( module . __author__ ) ) \n 
hasAuthor = True \n 
~~ ~~ if hasattr ( module , ) : \n 
~~~ contribs = sortAuthors ( ) \n 
if hasAuthor : \n 
~~~ contribs . remove ( module . __author__ ) \n 
~~ ~~ if contribs : \n 
~~~ contrib = format ( _ ( ) , \n 
buildContributorsString ( contribs ) , \n 
len ( contribs ) ) \n 
hasContribs = True \n 
~~ elif hasAuthor : \n 
~~~ contrib = _ ( ) \n 
~~ ~~ if hasContribs and not hasAuthor : \n 
~~~ conjunction = _ ( ) \n 
~~ return . join ( [ head , author , conjunction , contrib ] ) \n 
~~ def buildPersonString ( module ) : \n 
isAuthor = False \n 
authorInfo = None \n 
moduleContribs = module . __contributors__ . keys ( ) \n 
lnick = nick . lower ( ) \n 
for contrib in moduleContribs : \n 
~~~ if contrib . nick . lower ( ) == lnick : \n 
~~~ authorInfo = contrib \n 
~~ ~~ authorInfo = authorInfo or getattr ( supybot . authors , nick , None ) \n 
if not authorInfo : \n 
~~~ return _ ( \n 
) % nick \n 
~~ fullName = utils . web . mungeEmail ( str ( authorInfo ) ) \n 
contributions = [ ] \n 
~~~ if authorInfo not in module . __contributors__ : \n 
) % ( cb . name ( ) , nick ) \n 
~~ contributions = module . __contributors__ [ authorInfo ] \n 
~~ isAuthor = getattr ( module , , False ) == authorInfo \n 
( nonCommands , commands ) = utils . iter . partition ( lambda s : in s , \n 
contributions ) \n 
results = [ ] \n 
if commands : \n 
~~~ s = _ ( ) \n 
if len ( commands ) > 1 : \n 
~~~ s = utils . str . pluralize ( s ) \n 
~~ results . append ( format ( _ ( ) , commands , s ) ) \n 
~~ if nonCommands : \n 
~~~ results . append ( format ( _ ( ) , nonCommands ) ) \n 
~~ if results and isAuthor : \n 
~~~ return format ( \n 
_ ( ) , \n 
( fullName , cb . name ( ) , results ) ) \n 
~~ elif results and not isAuthor : \n 
~~~ return format ( _ ( ) , \n 
fullName , results , cb . name ( ) ) \n 
~~ elif isAuthor and not results : \n 
~~~ return _ ( ) % ( fullName , cb . name ( ) ) \n 
) % ( fullName , cb . name ( ) ) \n 
~~ ~~ module = cb . classModule \n 
if not nick : \n 
~~~ irc . reply ( buildPeopleString ( module ) ) \n 
~~~ nick = ircutils . toLower ( nick ) \n 
irc . reply ( buildPersonString ( module ) ) \n 
~~ ~~ ~~ def does_file_exist ( fname ) : \n 
~~~ f = open ( fname ) \n 
~~ ~~ def __iter__ ( self ) : \n 
~~~ ext = os . path . splitext ( self . logname ) [ 1 ] \n 
if ext == : \n 
~~~ d = zlib . decompressobj ( 16 + zlib . MAX_WBITS ) \n 
~~ if isinstance ( self . obj , types . GeneratorType ) : \n 
~~~ buf = next ( self . obj ) \n 
partial = \n 
while buf : \n 
~~~ if ext == : \n 
~~~ string = partial + d . decompress ( buf ) \n 
~~~ string = partial + buf \n 
~~ split = string . split ( ) \n 
for line in split [ : - 1 ] : \n 
~~~ yield line + \n 
~~ partial = split [ - 1 ] \n 
~~ ~~ if partial != : \n 
~~~ yield partial \n 
~~~ output = self . obj \n 
~~~ output = d . decompress ( output ) \n 
~~ split = output . split ( ) \n 
if partial != : \n 
~~ ~~ ~~ def run ( self ) : \n 
~~~ events = [ ] \n 
while not self . _stopping : \n 
~~~ asap = False \n 
~~~ events = self . poll ( TIMEOUT_PRECISION ) \n 
~~ except ( OSError , __HOLE__ ) as e : \n 
~~~ if errno_from_exception ( e ) in ( errno . EPIPE , errno . EINTR ) : \n 
~~~ asap = True \n 
logging . debug ( , e ) \n 
~~~ logging . error ( , e ) \n 
import traceback \n 
~~ ~~ for sock , fd , event in events : \n 
~~~ handler = self . _fdmap . get ( fd , None ) \n 
if handler is not None : \n 
~~~ handler = handler [ 1 ] \n 
~~~ handler . handle_event ( sock , fd , event ) \n 
~~ except ( OSError , IOError ) as e : \n 
~~~ shell . print_exception ( e ) \n 
~~ ~~ ~~ now = time . time ( ) \n 
if asap or now - self . _last_time >= TIMEOUT_PRECISION : \n 
~~~ for callback in self . _periodic_callbacks : \n 
~~~ callback ( ) \n 
~~ self . _last_time = now \n 
~~ ~~ ~~ def get_multiline ( self , f , m ) : \n 
~~~ content = [ ] \n 
next_line = \n 
while not re . search ( "^}" , next_line ) : \n 
~~~ content . append ( next_line ) \n 
~~~ next_line = next ( f ) \n 
~~~ next_line = None \n 
~~ ~~ content = "" . join ( content ) \n 
return content , next_line \n 
~~ def get_file ( fname , origin , untar = False ) : \n 
~~~ datadir_base = os . path . expanduser ( os . path . join ( , ) ) \n 
if not os . access ( datadir_base , os . W_OK ) : \n 
~~~ datadir_base = os . path . join ( , ) \n 
~~ datadir = os . path . join ( datadir_base , ) \n 
if not os . path . exists ( datadir ) : \n 
~~~ os . makedirs ( datadir ) \n 
~~ if untar : \n 
~~~ untar_fpath = os . path . join ( datadir , fname ) \n 
fpath = untar_fpath + \n 
~~~ fpath = os . path . join ( datadir , fname ) \n 
~~ if not os . path . exists ( fpath ) : \n 
~~~ print ( , origin ) \n 
global progbar \n 
progbar = None \n 
def dl_progress ( count , block_size , total_size ) : \n 
~~~ global progbar \n 
if progbar is None : \n 
~~~ progbar = Progbar ( total_size ) \n 
~~~ progbar . update ( count * block_size ) \n 
~~ ~~ error_msg = \n 
~~~ urlretrieve ( origin , fpath , dl_progress ) \n 
~~ except URLError as e : \n 
~~~ raise Exception ( error_msg . format ( origin , e . errno , e . reason ) ) \n 
~~ except HTTPError as e : \n 
~~~ raise Exception ( error_msg . format ( origin , e . code , e . msg ) ) \n 
~~ ~~ except ( Exception , KeyboardInterrupt ) as e : \n 
~~~ if os . path . exists ( fpath ) : \n 
~~~ os . remove ( fpath ) \n 
~~ progbar = None \n 
~~~ if not os . path . exists ( untar_fpath ) : \n 
tfile = tarfile . open ( fpath , ) \n 
~~~ tfile . extractall ( path = datadir ) \n 
~~ except ( Exception , __HOLE__ ) as e : \n 
~~~ if os . path . exists ( untar_fpath ) : \n 
~~~ if os . path . isfile ( untar_fpath ) : \n 
~~~ os . remove ( untar_fpath ) \n 
~~~ shutil . rmtree ( untar_fpath ) \n 
~~ ~~ raise \n 
~~ tfile . close ( ) \n 
~~ return untar_fpath \n 
~~ return fpath \n 
~~ def do_tags_for_model ( parser , token ) : \n 
bits = token . contents . split ( ) \n 
len_bits = len ( bits ) \n 
if not len_bits > 3 : \n 
~~~ raise TemplateSyntaxError ( _ ( ) % bits [ 0 ] ) \n 
~~ if bits [ 2 ] != : \n 
~~ if len_bits > 6 : \n 
~~~ for i in range ( 5 , len_bits ) : \n 
~~~ name , value = bits [ i ] . split ( ) \n 
if name == : \n 
~~~ kwargs [ str ( name ) ] = int ( value ) \n 
: bits [ 0 ] , \n 
: name , \n 
: value , \n 
~~~ kwargs [ ] [ name ] = value \n 
: bits [ i ] , \n 
~~ ~~ if bits [ 4 ] != : \n 
~~ if bits [ 5 ] != : \n 
~~ ~~ if len_bits == 4 : \n 
~~~ return TagsForModelNode ( bits [ 1 ] , bits [ 3 ] , counts = False ) \n 
~~~ return TagsForModelNode ( bits [ 1 ] , bits [ 3 ] , counts = True ) \n 
~~ ~~ def do_tag_cloud_for_model ( parser , token ) : \n 
~~ kwargs = { : { } } \n 
if len_bits > 5 : \n 
~~~ if bits [ 4 ] != : \n 
~~ for i in range ( 5 , len_bits ) : \n 
if name == or name == : \n 
~~ ~~ elif name == : \n 
~~~ if value in [ , ] : \n 
~~~ kwargs [ str ( name ) ] = { : LINEAR , : LOGARITHMIC } [ value ] \n 
~~ ~~ ~~ return TagCloudForModelNode ( bits [ 1 ] , bits [ 3 ] , ** kwargs ) \n 
~~ def do_tags_for_object ( parser , token ) : \n 
if len_bits != 4 and len_bits not in range ( 6 , 7 ) : \n 
~~ kwargs = { } \n 
~~~ kwargs [ str ( name ) ] = str ( value ) \n 
~~ ~~ ~~ return TagsForObjectNode ( bits [ 1 ] , bits [ 3 ] , ** kwargs ) \n 
~~ def do_related_objects_for_object ( parser , token ) : \n 
if len_bits != 6 and len_bits not in range ( 7 , 10 ) : \n 
~~ if bits [ 4 ] != : \n 
if len_bits > 6 : \n 
~~~ if bits [ 6 ] != : \n 
~~ for i in range ( 7 , len_bits ) : \n 
if name in [ , ] : \n 
~~ ~~ ~~ return RelatedObjectsForObjectNode ( bits [ 1 ] , bits [ 3 ] , bits [ 5 ] , ** kwargs ) \n 
~~ def do_is_supertaggable ( parser , token ) : \n 
~~~ tag_name , obj = token . split_contents ( ) \n 
~~ return IsSupertaggableNode ( obj ) \n 
~~ def items_for_result ( cl , result , form ) : \n 
first = True \n 
pk = cl . lookup_opts . pk . attname \n 
for field_name in cl . list_display : \n 
~~~ row_class = \n 
~~~ f , attr , value = lookup_field ( field_name , result , cl . model_admin ) \n 
~~ except ( __HOLE__ , ObjectDoesNotExist ) : \n 
~~~ result_repr = EMPTY_CHANGELIST_VALUE \n 
~~~ if f is None : \n 
~~~ if field_name == : \n 
~~ allow_tags = getattr ( attr , , False ) \n 
boolean = getattr ( attr , , False ) \n 
if boolean : \n 
~~~ allow_tags = True \n 
result_repr = _boolean_icon ( value ) \n 
~~~ result_repr = smart_unicode ( value ) \n 
~~ if not allow_tags : \n 
~~~ result_repr = escape ( result_repr ) \n 
~~~ result_repr = mark_safe ( result_repr ) \n 
~~~ if isinstance ( f . rel , models . ManyToOneRel ) : \n 
~~~ field_val = getattr ( result , f . name ) \n 
if field_val is None : \n 
~~~ result_repr = escape ( field_val ) \n 
~~~ result_repr = display_for_field ( value , f ) \n 
~~ if isinstance ( f , models . DateField ) or isinstance ( f , models . TimeField ) or isinstance ( f , models . ForeignKey ) : \n 
~~ ~~ ~~ if force_unicode ( result_repr ) == : \n 
~~~ result_repr = mark_safe ( ) \n 
~~ if ( first and not cl . list_display_links ) or field_name in cl . list_display_links : \n 
~~~ table_tag = { True : , False : } [ first ] \n 
~~~ result_repr += force_unicode ( form [ cl . model . _meta . pk . name ] ) \n 
~~ first = False \n 
if cl . to_field : \n 
~~~ attr = str ( cl . to_field ) \n 
~~~ attr = pk \n 
~~ value = result . serializable_value ( attr ) \n 
result_id = repr ( force_unicode ( value ) ) [ 1 : ] \n 
yield mark_safe ( % ( table_tag , row_class , conditional_escape ( result_repr ) , table_tag ) ) \n 
~~~ if ( form and field_name in form . fields and not ( \n 
field_name == cl . model . _meta . pk . name and \n 
form [ cl . model . _meta . pk . name ] . is_hidden ) ) : \n 
~~~ bf = form [ field_name ] \n 
result_repr = mark_safe ( force_unicode ( bf . errors ) + force_unicode ( bf ) ) \n 
~~~ result_repr = conditional_escape ( result_repr ) \n 
~~ yield mark_safe ( % ( row_class , result_repr ) ) \n 
~~ ~~ if form and not form [ cl . model . _meta . pk . name ] . is_hidden : \n 
~~~ yield mark_safe ( % force_unicode ( form [ cl . model . _meta . pk . name ] ) ) \n 
~~ ~~ def results ( cl ) : \n 
~~~ from django . contrib . admin . templatetags . admin_list import ResultList \n 
if cl . formset : \n 
~~~ for res , form in zip ( cl . result_list , cl . formset . forms ) : \n 
~~~ yield ResultList ( form , items_for_result ( cl , res , form ) ) \n 
~~~ for res in cl . result_list : \n 
~~~ yield ResultList ( None , items_for_result ( cl , res , None ) ) \n 
~~~ if cl . formset : \n 
~~~ yield list ( items_for_result ( cl , res , form ) ) \n 
~~~ yield list ( items_for_result ( cl , res , None ) ) \n 
~~ ~~ ~~ ~~ @ register . inclusion_tag ( "admin/supertagging/supertaggeditem/change_list_results.html" ) \n 
def supertaggeditem_result_list ( cl ) : \n 
from django . contrib . admin . templatetags . admin_list import result_headers \n 
~~~ from django . contrib . admin . templatetags . admin_list import result_hidden_fields \n 
~~~ result_hidden_fields = lambda x : [ ] \n 
~~ headers = list ( result_headers ( cl ) ) \n 
cl . list_editable = cl . model_admin . list_editable \n 
return { : cl , \n 
: list ( result_hidden_fields ( cl ) ) , \n 
: headers , \n 
: cl . get_query_string ( remove = [ ORDER_VAR ] ) , \n 
: list ( results ( cl ) ) } \n 
~~ def swiss_roll ( optimize = True , verbose = 1 , plot = True , N = 1000 , num_inducing = 25 , Q = 4 , sigma = .2 ) : \n 
~~~ import GPy \n 
from pods . datasets import swiss_roll_generated \n 
from GPy . models import BayesianGPLVM \n 
data = swiss_roll_generated ( num_samples = N , sigma = sigma ) \n 
Y = data [ ] \n 
Y -= Y . mean ( ) \n 
Y /= Y . std ( ) \n 
t = data [ ] \n 
c = data [ ] \n 
~~~ from sklearn . manifold . isomap import Isomap \n 
iso = Isomap ( ) . fit ( Y ) \n 
X = iso . embedding_ \n 
if Q > 2 : \n 
~~~ X = _np . hstack ( ( X , _np . random . randn ( N , Q - 2 ) ) ) \n 
~~~ X = _np . random . randn ( N , Q ) \n 
~~ if plot : \n 
~~~ import matplotlib . pyplot as plt \n 
ax = fig . add_subplot ( 121 , projection = ) \n 
ax . scatter ( * Y . T , c = c ) \n 
ax = fig . add_subplot ( 122 ) \n 
ax . scatter ( * X . T [ : 2 ] , c = c ) \n 
~~ var = .5 \n 
S = ( var * _np . ones_like ( X ) + _np . clip ( _np . random . randn ( N , Q ) * var ** 2 , \n 
- ( 1 - var ) , \n 
( 1 - var ) ) ) + .001 \n 
Z = _np . random . permutation ( X ) [ : num_inducing ] \n 
kernel = GPy . kern . RBF ( Q , ARD = True ) + GPy . kern . Bias ( Q , _np . exp ( - 2 ) ) + GPy . kern . White ( Q , _np . exp ( - \n 
m = BayesianGPLVM ( Y , Q , X = X , X_variance = S , num_inducing = num_inducing , Z = Z , kernel = kernel ) \n 
m . data_colors = c \n 
m . data_t = t \n 
if optimize : \n 
~~~ m . optimize ( , messages = verbose , max_iters = 2e3 ) \n 
~~~ fig = plt . figure ( ) \n 
ax = fig . add_subplot ( 111 ) \n 
s = m . input_sensitivity ( ) . argsort ( ) [ : : - 1 ] [ : 2 ] \n 
ax . scatter ( * m . X . mean . T [ s ] , c = c ) \n 
~~ return m \n 
~~ def bgplvm_oil ( optimize = True , verbose = 1 , plot = True , N = 200 , Q = 7 , num_inducing = 40 , max_iters = 1000 , ** k ~~~ import GPy \n 
from matplotlib import pyplot as plt \n 
import numpy as np \n 
_np . random . seed ( 0 ) \n 
~~~ import pods \n 
data = pods . datasets . oil ( ) \n 
~~~ data = GPy . util . datasets . oil ( ) \n 
m = GPy . models . BayesianGPLVM ( Y , Q , kernel = kernel , num_inducing = num_inducing , ** k ) \n 
m . data_labels = data [ ] [ : N ] . argmax ( axis = 1 ) \n 
~~~ m . optimize ( , messages = verbose , max_iters = max_iters , gtol = .05 ) \n 
~~~ fig , ( latent_axes , sense_axes ) = plt . subplots ( 1 , 2 ) \n 
m . plot_latent ( ax = latent_axes , labels = m . data_labels ) \n 
data_show = GPy . plotting . matplot_dep . visualize . vector_show ( ( m . Y [ 0 , : ] ) ) \n 
lvm_visualizer = GPy . plotting . matplot_dep . visualize . lvm_dimselect ( m . X . mean . values [ 0 : 1 , : ] , m , data_show , latent_axes = latent_axes , sense_axes = sense_axes , labels = m . data_labels ) \n 
raw_input ( ) \n 
plt . close ( fig ) \n 
~~ def stick_bgplvm ( model = None , optimize = True , verbose = True , plot = True ) : \n 
import GPy \n 
import pods \n 
data = pods . datasets . osu_run1 ( ) \n 
Q = 6 \n 
kernel = GPy . kern . RBF ( Q , lengthscale = np . repeat ( .5 , Q ) , ARD = True ) \n 
m = BayesianGPLVM ( data [ ] , Q , init = "PCA" , num_inducing = 20 , kernel = kernel ) \n 
m . data = data \n 
m . likelihood . variance = 0.001 \n 
~~~ if optimize : m . optimize ( , messages = verbose , max_iters = 5e3 , bfgs_factor = 10 ) \n 
plt . sca ( latent_axes ) \n 
m . plot_latent ( ax = latent_axes ) \n 
y = m . Y [ : 1 , : ] . copy ( ) \n 
data_show = GPy . plotting . matplot_dep . visualize . stick_show ( y , connect = data [ ] ) \n 
dim_select = GPy . plotting . matplot_dep . visualize . lvm_dimselect ( m . X . mean [ : 1 , : ] . copy ( ) , m , data_show fig . canvas . draw ( ) \n 
#fig.canvas.show() \n 
~~ def initiate_send ( self ) : \n 
~~~ while self . producer_fifo and self . connected : \n 
~~~ first = self . producer_fifo [ 0 ] \n 
if not first : \n 
~~~ del self . producer_fifo [ 0 ] \n 
if first is None : \n 
~~~ self . handle_close ( ) \n 
~~ ~~ obs = self . ac_out_buffer_size \n 
~~~ with catch_warnings ( ) : \n 
~~~ if py3kwarning : \n 
~~~ filterwarnings ( "ignore" , ".*buffer" , DeprecationWarning ) \n 
~~ data = buffer ( first , 0 , obs ) \n 
~~~ data = first . more ( ) \n 
~~~ self . producer_fifo . appendleft ( data ) \n 
~~~ num_sent = self . send ( data ) \n 
~~~ self . handle_error ( ) \n 
~~ if num_sent : \n 
~~~ if num_sent < len ( data ) or obs < len ( first ) : \n 
~~~ self . producer_fifo [ 0 ] = first [ num_sent : ] \n 
~~ ~~ return \n 
~~ ~~ def get_session ( self , session_name ) : \n 
if session_name not in self . _sessions : \n 
~~~ conf = self . config [ ] [ session_name ] \n 
~~~ raise KitError ( % ( session_name , ) ) \n 
~~ engine = create_engine ( \n 
conf . get ( , ) , ** conf . get ( , { } ) \n 
session = scoped_session ( \n 
sessionmaker ( bind = engine , ** conf . get ( , { } ) ) \n 
options = conf . get ( , { } ) \n 
options . setdefault ( , False ) \n 
options . setdefault ( , True ) \n 
self . _sessions [ session_name ] = ( session , options ) \n 
~~ return self . _sessions [ session_name ] [ 0 ] \n 
~~ def start ( self ) : \n 
~~~ self . instructions ( ) \n 
p_canvas = self . fitsimage . get_canvas ( ) \n 
~~~ obj = p_canvas . getObjectByTag ( self . layertag ) \n 
~~~ p_canvas . add ( self . canvas , tag = self . layertag ) \n 
~~ self . resume ( ) \n 
~~ def add_datapoints ( self , stats ) : \n 
if not stats : \n 
~~ matches = PATTERN . match ( stats ) \n 
if matches : \n 
~~~ for key in self . KEYS . keys ( ) : \n 
~~~ value = int ( matches . group ( key ) or 0 ) \n 
~~ except ( IndexError , __HOLE__ ) : \n 
~~~ value = 0 \n 
~~ if key in self . GAUGES : \n 
~~~ self . add_gauge_value ( self . KEYS [ key ] , \n 
self . TYPES [ key ] , \n 
value ) \n 
~~~ self . add_derive_value ( self . KEYS [ key ] , \n 
~~~ LOGGER . debug ( , stats ) \n 
~~ ~~ def get_git_describe ( ) : \n 
~~~ if not os . path . exists ( os . path . join ( os . path . dirname ( __file__ ) , ".git" ) ) : \n 
~~~ po = subprocess . Popen ( \n 
( "git" , "describe" , "--tags" , "--long" , "--always" ) , \n 
stdout = subprocess . PIPE , stderr = subprocess . PIPE ) \n 
~~~ raise CantInvokeGit ( e ) \n 
~~ stdout , stderr = po . communicate ( ) \n 
if po . returncode != 0 : \n 
~~ return stdout . decode ( ) . rstrip ( ) \n 
~~ def get_custom_settings ( self ) : \n 
~~~ from settings_test import * \n 
settings_test = dict ( locals ( ) ) \n 
del settings_test [ ] \n 
if in settings_test : \n 
~~~ del settings_test [ ] \n 
~~~ settings_test = { } \n 
INSTALLED_APPS = [ ] \n 
~~ return INSTALLED_APPS , settings_test \n 
~~ def _check_antecedents ( g1 , g2 , x ) : \n 
from sympy import re , Eq , Ne , cos , I , exp , sin , sign , unpolarify \n 
from sympy import arg as arg_ , unbranched_argument as arg \n 
# \n 
sigma , _ = _get_coeff_exp ( g1 . argument , x ) \n 
omega , _ = _get_coeff_exp ( g2 . argument , x ) \n 
s , t , u , v = S ( [ len ( g1 . bm ) , len ( g1 . an ) , len ( g1 . ap ) , len ( g1 . bq ) ] ) \n 
m , n , p , q = S ( [ len ( g2 . bm ) , len ( g2 . an ) , len ( g2 . ap ) , len ( g2 . bq ) ] ) \n 
bstar = s + t - ( u + v ) / 2 \n 
cstar = m + n - ( p + q ) / 2 \n 
rho = g1 . nu + ( u - v ) / 2 + 1 \n 
mu = g2 . nu + ( p - q ) / 2 + 1 \n 
phi = q - p - ( v - u ) \n 
eta = 1 - ( v - u ) - mu - rho \n 
psi = ( pi * ( q - m - n ) + abs ( arg ( omega ) ) ) / ( q - p ) \n 
theta = ( pi * ( v - s - t ) + abs ( arg ( sigma ) ) ) / ( v - u ) \n 
_debug ( ) \n 
_debug ( \n 
% ( sigma , s , t , u , v , bstar , rho ) ) \n 
% ( omega , m , n , p , q , cstar , mu ) ) \n 
_debug ( % ( phi , eta , psi , theta ) ) \n 
def _c1 ( ) : \n 
~~~ for g in [ g1 , g2 ] : \n 
~~~ for i in g . an : \n 
~~~ for j in g . bm : \n 
~~~ diff = i - j \n 
if diff . is_integer and diff . is_positive : \n 
~~ ~~ ~~ ~~ return True \n 
~~ c1 = _c1 ( ) \n 
c2 = And ( * [ re ( 1 + i + j ) > 0 for i in g1 . bm for j in g2 . bm ] ) \n 
c3 = And ( * [ re ( 1 + i + j ) < 1 + 1 for i in g1 . an for j in g2 . an ] ) \n 
c4 = And ( * [ ( p - q ) * re ( 1 + i - 1 ) - re ( mu ) > - S ( 3 ) / 2 for i in g1 . an ] ) \n 
c5 = And ( * [ ( p - q ) * re ( 1 + i ) - re ( mu ) > - S ( 3 ) / 2 for i in g1 . bm ] ) \n 
c6 = And ( * [ ( u - v ) * re ( 1 + i - 1 ) - re ( rho ) > - S ( 3 ) / 2 for i in g2 . an ] ) \n 
c7 = And ( * [ ( u - v ) * re ( 1 + i ) - re ( rho ) > - S ( 3 ) / 2 for i in g2 . bm ] ) \n 
c8 = ( abs ( phi ) + 2 * re ( ( rho - 1 ) * ( q - p ) + ( v - u ) * ( q - p ) + ( mu - \n 
1 ) * ( v - u ) ) > 0 ) \n 
c9 = ( abs ( phi ) - 2 * re ( ( rho - 1 ) * ( q - p ) + ( v - u ) * ( q - p ) + ( mu - \n 
c10 = ( abs ( arg ( sigma ) ) < bstar * pi ) \n 
c11 = Eq ( abs ( arg ( sigma ) ) , bstar * pi ) \n 
c12 = ( abs ( arg ( omega ) ) < cstar * pi ) \n 
c13 = Eq ( abs ( arg ( omega ) ) , cstar * pi ) \n 
z0 = exp ( - ( bstar + cstar ) * pi * I ) \n 
zos = unpolarify ( z0 * omega / sigma ) \n 
zso = unpolarify ( z0 * sigma / omega ) \n 
if zos == 1 / zso : \n 
~~~ c14 = And ( Eq ( phi , 0 ) , bstar + cstar <= 1 , \n 
Or ( Ne ( zos , 1 ) , re ( mu + rho + v - u ) < 1 , \n 
re ( mu + rho + q - p ) < 1 ) ) \n 
~~~ c14 = And ( Eq ( phi , 0 ) , bstar - 1 + cstar <= 0 , \n 
Or ( And ( Ne ( zos , 1 ) , abs ( arg_ ( 1 - zos ) ) < pi ) , \n 
And ( re ( mu + rho + v - u ) < 1 , Eq ( zos , 1 ) ) ) ) \n 
def _cond ( ) : \n 
tmp = abs ( arg_ ( 1 - zso ) ) \n 
return False if tmp is S . NaN else tmp < pi \n 
~~ c14_alt = And ( Eq ( phi , 0 ) , cstar - 1 + bstar <= 0 , \n 
Or ( And ( Ne ( zso , 1 ) , _cond ( ) ) , \n 
And ( re ( mu + rho + q - p ) < 1 , Eq ( zso , 1 ) ) ) ) \n 
c14 = Or ( c14 , c14_alt ) \n 
~~ \n 
~~~ lambda_c = ( q - p ) * abs ( omega ) ** ( 1 / ( q - p ) ) * cos ( psi ) + ( v - u ) * abs ( sigma ) ** ( 1 / ( v - u ) ) * cos ( theta ) \n 
if _eval_cond ( lambda_c > 0 ) != False : \n 
~~~ c15 = ( lambda_c > 0 ) \n 
~~~ def lambda_s0 ( c1 , c2 ) : \n 
~~~ return c1 * ( q - p ) * abs ( omega ) ** ( 1 / ( q - p ) ) * sin ( psi ) + c2 * ( v - u ) * abs ( sigma ) ** ( 1 / ( v - u ) ) * sin ( theta ) \n 
~~ lambda_s = Piecewise ( \n 
( ( lambda_s0 ( + 1 , + 1 ) * lambda_s0 ( - 1 , - 1 ) ) , \n 
And ( Eq ( arg ( sigma ) , 0 ) , Eq ( arg ( omega ) , 0 ) ) ) , \n 
( lambda_s0 ( sign ( arg ( omega ) ) , + 1 ) * lambda_s0 ( sign ( arg ( omega ) ) , - 1 ) , \n 
And ( Eq ( arg ( sigma ) , 0 ) , Ne ( arg ( omega ) , 0 ) ) ) , \n 
( lambda_s0 ( + 1 , sign ( arg ( sigma ) ) ) * lambda_s0 ( - 1 , sign ( arg ( sigma ) ) ) , \n 
And ( Ne ( arg ( sigma ) , 0 ) , Eq ( arg ( omega ) , 0 ) ) ) , \n 
( lambda_s0 ( sign ( arg ( omega ) ) , sign ( arg ( sigma ) ) ) , True ) ) \n 
tmp = [ lambda_c > 0 , \n 
And ( Eq ( lambda_c , 0 ) , Ne ( lambda_s , 0 ) , re ( eta ) > - 1 ) , \n 
And ( Eq ( lambda_c , 0 ) , Eq ( lambda_s , 0 ) , re ( eta ) > 0 ) ] \n 
c15 = Or ( * tmp ) \n 
~~~ c15 = False \n 
~~ for cond , i in [ ( c1 , 1 ) , ( c2 , 2 ) , ( c3 , 3 ) , ( c4 , 4 ) , ( c5 , 5 ) , ( c6 , 6 ) , \n 
( c7 , 7 ) , ( c8 , 8 ) , ( c9 , 9 ) , ( c10 , 10 ) , ( c11 , 11 ) , \n 
( c12 , 12 ) , ( c13 , 13 ) , ( c14 , 14 ) , ( c15 , 15 ) ] : \n 
~~~ _debug ( % i , cond ) \n 
~~ conds = [ ] \n 
def pr ( count ) : \n 
~~~ _debug ( % count , conds [ - 1 ] ) \n 
pr ( 1 ) \n 
pr ( 2 ) \n 
pr ( 3 ) \n 
conds += [ And ( Eq ( p , q ) , Eq ( u , v ) , Eq ( bstar , 0 ) , Eq ( cstar , 0 ) , \n 
sigma . is_positive is True , omega . is_positive is True , re ( mu ) < 1 , re ( rho ) < 1 , \n 
pr ( 4 ) \n 
sigma . is_positive is True , omega . is_positive is True , re ( mu + rho ) < 1 , \n 
pr ( 5 ) \n 
conds += [ And ( p > q , s . is_positive is True , bstar . is_positive is True , cstar >= 0 , \n 
pr ( 6 ) \n 
conds += [ And ( p < q , t . is_positive is True , bstar . is_positive is True , cstar >= 0 , \n 
pr ( 7 ) \n 
conds += [ And ( u > v , m . is_positive is True , cstar . is_positive is True , bstar >= 0 , \n 
pr ( 8 ) \n 
conds += [ And ( u < v , n . is_positive is True , cstar . is_positive is True , bstar >= 0 , \n 
pr ( 9 ) \n 
conds += [ And ( p > q , Eq ( u , v ) , Eq ( bstar , 0 ) , cstar >= 0 , sigma . is_positive is True , \n 
pr ( 10 ) \n 
conds += [ And ( p < q , Eq ( u , v ) , Eq ( bstar , 0 ) , cstar >= 0 , sigma . is_positive is True , \n 
pr ( 11 ) \n 
conds += [ And ( Eq ( p , q ) , u > v , bstar >= 0 , Eq ( cstar , 0 ) , omega . is_positive is True , \n 
pr ( 12 ) \n 
conds += [ And ( Eq ( p , q ) , u < v , bstar >= 0 , Eq ( cstar , 0 ) , omega . is_positive is True , \n 
pr ( 13 ) \n 
conds += [ And ( p < q , u > v , bstar >= 0 , cstar >= 0 , \n 
pr ( 14 ) \n 
conds += [ And ( p > q , u < v , bstar >= 0 , cstar >= 0 , \n 
pr ( 15 ) \n 
conds += [ And ( p > q , u > v , bstar >= 0 , cstar >= 0 , \n 
pr ( 16 ) \n 
conds += [ And ( p < q , u < v , bstar >= 0 , cstar >= 0 , \n 
pr ( 17 ) \n 
conds += [ And ( Eq ( t , 0 ) , s . is_positive is True , bstar . is_positive is True , phi . is_positive is True pr ( 18 ) \n 
conds += [ And ( Eq ( s , 0 ) , t . is_positive is True , bstar . is_positive is True , phi . is_negative is True pr ( 19 ) \n 
conds += [ And ( Eq ( n , 0 ) , m . is_positive is True , cstar . is_positive is True , phi . is_negative is True pr ( 20 ) \n 
conds += [ And ( Eq ( m , 0 ) , n . is_positive is True , cstar . is_positive is True , phi . is_positive is True pr ( 21 ) \n 
conds += [ And ( Eq ( s * t , 0 ) , bstar . is_positive is True , cstar . is_positive is True , \n 
pr ( 22 ) \n 
conds += [ And ( Eq ( m * n , 0 ) , bstar . is_positive is True , cstar . is_positive is True , \n 
pr ( 23 ) \n 
mt1_exists = _check_antecedents_1 ( g1 , x , helper = True ) \n 
mt2_exists = _check_antecedents_1 ( g2 , x , helper = True ) \n 
conds += [ And ( mt2_exists , Eq ( t , 0 ) , u < s , bstar . is_positive is True , c10 , c1 , c2 , c3 ) ] \n 
pr ( ) \n 
conds += [ And ( mt2_exists , Eq ( s , 0 ) , v < t , bstar . is_positive is True , c10 , c1 , c2 , c3 ) ] \n 
conds += [ And ( mt1_exists , Eq ( n , 0 ) , p < m , cstar . is_positive is True , c12 , c1 , c2 , c3 ) ] \n 
conds += [ And ( mt1_exists , Eq ( m , 0 ) , q < n , cstar . is_positive is True , c12 , c1 , c2 , c3 ) ] \n 
r = Or ( * conds ) \n 
if _eval_cond ( r ) != False : \n 
~~~ return r \n 
~~ conds += [ And ( m + n > p , Eq ( t , 0 ) , Eq ( phi , 0 ) , s . is_positive is True , bstar . is_positive is True , abs ( arg ( omega ) ) < ( m + n - p + 1 ) * pi , \n 
pr ( 24 ) \n 
conds += [ And ( m + n > q , Eq ( s , 0 ) , Eq ( phi , 0 ) , t . is_positive is True , bstar . is_positive is True , abs ( arg ( omega ) ) < ( m + n - q + 1 ) * pi , \n 
pr ( 25 ) \n 
conds += [ And ( Eq ( p , q - 1 ) , Eq ( t , 0 ) , Eq ( phi , 0 ) , s . is_positive is True , bstar . is_positive is True cstar >= 0 , cstar * pi < abs ( arg ( omega ) ) , \n 
pr ( 26 ) \n 
conds += [ And ( Eq ( p , q + 1 ) , Eq ( s , 0 ) , Eq ( phi , 0 ) , t . is_positive is True , bstar . is_positive is True cstar >= 0 , cstar * pi < abs ( arg ( omega ) ) , \n 
pr ( 27 ) \n 
conds += [ And ( p < q - 1 , Eq ( t , 0 ) , Eq ( phi , 0 ) , s . is_positive is True , bstar . is_positive is True , cstar >= 0 , cstar * pi < abs ( arg ( omega ) ) , \n 
abs ( arg ( omega ) ) < ( m + n - p + 1 ) * pi , \n 
pr ( 28 ) \n 
conds += [ And ( \n 
p > q + 1 , Eq ( s , 0 ) , Eq ( phi , 0 ) , t . is_positive is True , bstar . is_positive is True , cstar >= cstar * pi < abs ( arg ( omega ) ) , \n 
abs ( arg ( omega ) ) < ( m + n - q + 1 ) * pi , \n 
pr ( 29 ) \n 
conds += [ And ( Eq ( n , 0 ) , Eq ( phi , 0 ) , s + t > 0 , m . is_positive is True , cstar . is_positive is True , abs ( arg ( sigma ) ) < ( s + t - u + 1 ) * pi , \n 
pr ( 30 ) \n 
conds += [ And ( Eq ( m , 0 ) , Eq ( phi , 0 ) , s + t > v , n . is_positive is True , cstar . is_positive is True , abs ( arg ( sigma ) ) < ( s + t - v + 1 ) * pi , \n 
pr ( 31 ) \n 
conds += [ And ( Eq ( n , 0 ) , Eq ( phi , 0 ) , Eq ( u , v - 1 ) , m . is_positive is True , cstar . is_positive is True bstar >= 0 , bstar * pi < abs ( arg ( sigma ) ) , \n 
abs ( arg ( sigma ) ) < ( bstar + 1 ) * pi , \n 
pr ( 32 ) \n 
conds += [ And ( Eq ( m , 0 ) , Eq ( phi , 0 ) , Eq ( u , v + 1 ) , n . is_positive is True , cstar . is_positive is True bstar >= 0 , bstar * pi < abs ( arg ( sigma ) ) , \n 
pr ( 33 ) \n 
Eq ( n , 0 ) , Eq ( phi , 0 ) , u < v - 1 , m . is_positive is True , cstar . is_positive is True , bstar >= bstar * pi < abs ( arg ( sigma ) ) , \n 
abs ( arg ( sigma ) ) < ( s + t - u + 1 ) * pi , \n 
pr ( 34 ) \n 
Eq ( m , 0 ) , Eq ( phi , 0 ) , u > v + 1 , n . is_positive is True , cstar . is_positive is True , bstar >= bstar * pi < abs ( arg ( sigma ) ) , \n 
abs ( arg ( sigma ) ) < ( s + t - v + 1 ) * pi , \n 
pr ( 35 ) \n 
return Or ( * conds ) \n 
~~ def parse_arg ( s ) : \n 
~~~ return json . loads ( s ) \n 
~~~ return s \n 
~~ ~~ def decode_string ( x , f ) : \n 
~~~ colon = x . index ( , f ) \n 
~~~ n = int ( x [ f : colon ] ) \n 
~~ except ( OverflowError , __HOLE__ ) : \n 
~~~ n = long ( x [ f : colon ] ) \n 
~~ if x [ f ] == and colon != f + 1 : \n 
~~ colon += 1 \n 
return ( x [ colon : colon + n ] , colon + n ) \n 
~~ def bdecode ( x , sloppy = 0 ) : \n 
~~~ r , l = decode_func [ x [ 0 ] ] ( x , 0 ) \n 
~~ except ( __HOLE__ , KeyError , ValueError ) : \n 
~~ if not sloppy and l != len ( x ) : \n 
~~ def test_bdecode ( ) : \n 
~~~ bdecode ( ) \n 
assert 0 \n 
~~ assert bdecode ( ) == 4 L \n 
assert bdecode ( ) == 0 L \n 
assert bdecode ( ) == 123456789 L \n 
assert bdecode ( ) == - 10 L \n 
~~ assert bdecode ( ) == \n 
assert bdecode ( ) == \n 
~~ assert bdecode ( ) == [ ] \n 
~~ assert bdecode ( ) == [ , , ] \n 
~~ assert bdecode ( ) == [ 1 , 2 , 3 ] \n 
assert bdecode ( ) == [ , ] \n 
assert bdecode ( ) == [ [ , ] , [ 2 , 3 ] ] \n 
~~ assert bdecode ( ) == { } \n 
assert bdecode ( ) == { : 25 , : } \n 
assert bdecode ( ) == { : { : try : \n 
~~ ~~ def test_bencode ( ) : \n 
~~~ assert bencode ( 4 ) == \n 
assert bencode ( 0 ) == \n 
assert bencode ( - 10 ) == \n 
assert bencode ( 12345678901234567890 L ) == \n 
assert bencode ( ) == \n 
assert bencode ( [ ] ) == \n 
assert bencode ( [ 1 , 2 , 3 ] ) == \n 
assert bencode ( [ [ , ] , [ 2 , 3 ] ] ) == \n 
assert bencode ( { } ) == \n 
assert bencode ( { : 25 , : } ) == \n 
assert bencode ( { : { : , : 100000 } } ) == try : \n 
~~~ bencode ( { 1 : } ) \n 
~~ ~~ def load_path_attr ( path ) : \n 
~~~ i = path . rfind ( "." ) \n 
module , attr = path [ : i ] , path [ i + 1 : ] \n 
~~~ mod = importlib . import_module ( module ) \n 
~~ except ImportError as e : \n 
~~~ attr = getattr ( mod , attr ) \n 
~~ return attr \n 
~~ def _configure_frozen_scoop ( kwargs ) : \n 
def _delete_old_scoop_rev_data ( old_scoop_rev ) : \n 
~~~ if old_scoop_rev is not None : \n 
~~~ elements = shared . elements \n 
for key in elements : \n 
~~~ var_dict = elements [ key ] \n 
if old_scoop_rev in var_dict : \n 
~~~ del var_dict [ old_scoop_rev ] \n 
~~ ~~ logging . getLogger ( ) . debug ( \n 
% old_scoop_rev ) \n 
~~~ logging . getLogger ( ) . error ( \n 
~~ ~~ ~~ scoop_rev = kwargs . pop ( ) \n 
~~~ old_scoop_rev = _frozen_scoop_single_run . kwargs [ ] \n 
configured = old_scoop_rev == scoop_rev \n 
~~ except ( AttributeError , KeyError ) : \n 
~~~ old_scoop_rev = None \n 
configured = False \n 
~~ if not configured : \n 
~~~ _frozen_scoop_single_run . kwargs = shared . getConst ( scoop_rev , timeout = 424.2 ) \n 
frozen_kwargs = _frozen_scoop_single_run . kwargs \n 
frozen_kwargs [ ] = scoop_rev \n 
frozen_kwargs [ ] . v_full_copy = frozen_kwargs [ ] \n 
if not scoop . IS_ORIGIN : \n 
~~~ _configure_niceness ( frozen_kwargs ) \n 
_configure_logging ( frozen_kwargs , extract = False ) \n 
~~ _delete_old_scoop_rev_data ( old_scoop_rev ) \n 
logging . getLogger ( ) . info ( % str ( scoop . worker ) ) \n 
~~ ~~ def _scoop_single_run ( kwargs ) : \n 
~~~ is_origin = scoop . IS_ORIGIN \n 
~~~ is_origin = True \n 
~~ if not is_origin : \n 
~~~ _configure_niceness ( kwargs ) \n 
_configure_logging ( kwargs ) \n 
~~ return _single_run ( kwargs ) \n 
~~~ scoop . logger . exception ( ) \n 
~~ ~~ def _configure_niceness ( kwargs ) : \n 
niceness = kwargs [ ] \n 
if niceness is not None : \n 
~~~ current = os . nice ( 0 ) \n 
if niceness - current > 0 : \n 
~~~ os . nice ( niceness - current ) \n 
~~~ psutil . Process ( ) . nice ( niceness ) \n 
~~~ sys . stderr . write ( % repr ( exc ) ) \n 
~~ ~~ ~~ @ parse_config \n 
~~~ @ kwargs_api_change ( , ) \n 
@ kwargs_api_change ( , ) \n 
@ kwargs_api_change ( ) \n 
@ simple_logging_config \n 
def __init__ ( self , trajectory = , \n 
add_time = False , \n 
comment = , \n 
dynamic_imports = None , \n 
wildcard_functions = None , \n 
automatic_storing = True , \n 
log_config = pypetconstants . DEFAULT_LOGGING , \n 
log_stdout = False , \n 
report_progress = ( 5 , , logging . INFO ) , \n 
multiproc = False , \n 
ncores = 1 , \n 
use_scoop = False , \n 
use_pool = False , \n 
freeze_input = False , \n 
timeout = None , \n 
cpu_cap = 100.0 , \n 
memory_cap = 100.0 , \n 
swap_cap = 100.0 , \n 
niceness = None , \n 
wrap_mode = pypetconstants . WRAP_MODE_LOCK , \n 
queue_maxsize = - 1 , \n 
port = None , \n 
gc_interval = None , \n 
clean_up_runs = True , \n 
immediate_postproc = False , \n 
resumable = False , \n 
resume_folder = None , \n 
delete_resume = True , \n 
storage_service = HDF5StorageService , \n 
git_repository = None , \n 
git_message = , \n 
git_fail = False , \n 
sumatra_project = None , \n 
sumatra_reason = , \n 
sumatra_label = None , \n 
do_single_runs = True , \n 
graceful_exit = False , \n 
lazy_debug = False , \n 
** kwargs ) : \n 
~~~ if git_repository is not None and git is None : \n 
~~ if resumable and dill is None : \n 
~~ if load_project is None and sumatra_project is not None : \n 
~~ if sumatra_label is not None and in sumatra_label : \n 
~~ if wrap_mode == pypetconstants . WRAP_MODE_NETLOCK and zmq is None : \n 
~~ if ( use_pool or use_scoop ) and immediate_postproc : \n 
~~ if use_pool and use_scoop : \n 
~~ if use_scoop and scoop is None : \n 
~~ if ( wrap_mode not in ( pypetconstants . WRAP_MODE_NONE , \n 
pypetconstants . WRAP_MODE_LOCAL , \n 
pypetconstants . WRAP_MODE_LOCK , \n 
pypetconstants . WRAP_MODE_NETLOCK ) and \n 
resumable ) : \n 
~~ if resumable and not automatic_storing : \n 
~~ if use_scoop and wrap_mode not in ( pypetconstants . WRAP_MODE_LOCAL , \n 
pypetconstants . WRAP_MODE_NONE , \n 
pypetconstants . WRAP_MODE_NETLOCK , \n 
pypetconstants . WRAP_MODE_NETQUEUE ) : \n 
~~ if niceness is not None and not hasattr ( os , ) and psutil is None : \n 
~~ if freeze_input and not use_pool and not use_scoop : \n 
~~ if not isinstance ( memory_cap , tuple ) : \n 
~~~ memory_cap = ( memory_cap , 0.0 ) \n 
~~ if ( cpu_cap <= 0.0 or cpu_cap > 100.0 or \n 
memory_cap [ 0 ] <= 0.0 or memory_cap [ 0 ] > 100.0 or \n 
swap_cap <= 0.0 or swap_cap > 100.0 ) : \n 
~~ check_usage = cpu_cap < 100.0 or memory_cap [ 0 ] < 100.0 or swap_cap < 100.0 \n 
if check_usage and psutil is None : \n 
~~ if ncores == 0 and psutil is None : \n 
~~ if port is not None and wrap_mode not in ( pypetconstants . WRAP_MODE_NETLOCK , \n 
~~ if use_scoop and graceful_exit : \n 
~~ unused_kwargs = set ( kwargs . keys ( ) ) \n 
self . _logging_manager = LoggingManager ( log_config = log_config , \n 
log_stdout = log_stdout , \n 
report_progress = report_progress ) \n 
self . _logging_manager . check_log_config ( ) \n 
self . _logging_manager . add_null_handler ( ) \n 
self . _set_logger ( ) \n 
self . _map_arguments = False \n 
self . _graceful_exit = graceful_exit \n 
self . _start_timestamp = None \n 
self . _finish_timestamp = None \n 
self . _runtime = None \n 
self . _cpu_cap = cpu_cap \n 
self . _memory_cap = memory_cap \n 
if psutil is not None : \n 
~~~ self . _total_memory = psutil . virtual_memory ( ) . total / 1024.0 / 1024.0 \n 
self . _est_per_process = self . _memory_cap [ 1 ] / self . _total_memory * 100.0 \n 
~~ self . _swap_cap = swap_cap \n 
self . _check_usage = check_usage \n 
self . _last_cpu_check = 0.0 \n 
self . _last_cpu_usage = 0.0 \n 
if self . _check_usage : \n 
~~~ self . _estimate_cpu_utilization ( ) \n 
~~ self . _niceness = niceness \n 
self . _sumatra_project = sumatra_project \n 
self . _sumatra_reason = sumatra_reason \n 
self . _sumatra_label = sumatra_label \n 
self . _loaded_sumatatra_project = None \n 
self . _sumatra_record = None \n 
self . _runfunc = None \n 
self . _args = ( ) \n 
self . _kwargs = { } \n 
self . _postproc = None \n 
self . _postproc_args = ( ) \n 
self . _postproc_kwargs = { } \n 
self . _immediate_postproc = immediate_postproc \n 
self . _user_pipeline = False \n 
self . _git_repository = git_repository \n 
self . _git_message = git_message \n 
self . _git_fail = git_fail \n 
if isinstance ( trajectory , compat . base_type ) : \n 
~~~ self . _traj = Trajectory ( trajectory , \n 
add_time = add_time , \n 
dynamic_imports = dynamic_imports , \n 
wildcard_functions = wildcard_functions , \n 
comment = comment ) \n 
~~~ self . _traj = trajectory \n 
init_time = time . time ( ) \n 
formatted_time = format_time ( init_time ) \n 
self . _timestamp = init_time \n 
self . _time = formatted_time \n 
~~ if self . _git_repository is not None : \n 
~~~ new_commit , self . _hexsha = make_git_commit ( self , self . _git_repository , \n 
self . _git_message , self . _git_fail ) \n 
~~~ new_commit = False \n 
~~ if not new_commit : \n 
~~~ self . _hexsha = hashlib . sha1 ( compat . tobytes ( self . trajectory . v_name + \n 
str ( self . trajectory . v_timestamp ) + \n 
str ( self . timestamp ) + \n 
VERSION ) ) . hexdigest ( ) \n 
~~ short_hexsha = self . _hexsha [ 0 : 7 ] \n 
name = \n 
self . _traj . _environment_hexsha = self . _hexsha \n 
self . _traj . _environment_name = self . _name \n 
self . _logging_manager . extract_replacements ( self . _traj ) \n 
self . _logging_manager . remove_null_handler ( ) \n 
self . _logging_manager . make_logging_handlers_and_tools ( ) \n 
if self . _git_repository is not None : \n 
~~~ if new_commit : \n 
~~~ self . _logger . info ( % str ( self . _hexsha ) ) \n 
~~~ self . _logger . info ( % \n 
str ( self . _hexsha ) ) \n 
~~~ storage_service = HDF5StorageService \n 
~~ if self . _traj . v_storage_service is not None : \n 
~~~ self . _logger . info ( \n 
self . _storage_service = self . trajectory . v_storage_service \n 
~~~ self . _storage_service , unused_factory_kwargs = storage_factory ( storage_service , \n 
self . _traj , ** kwargs ) \n 
unused_kwargs = unused_kwargs - ( set ( kwargs . keys ( ) ) - unused_factory_kwargs ) \n 
~~ if lazy_debug and is_debug ( ) : \n 
~~~ self . _storage_service = LazyStorageService ( ) \n 
~~ self . _traj . v_storage_service = self . _storage_service \n 
self . _resumable = resumable \n 
if self . _resumable : \n 
~~~ if resume_folder is None : \n 
~~~ resume_folder = os . path . join ( os . getcwd ( ) , ) \n 
~~ resume_path = os . path . join ( resume_folder , self . _traj . v_name ) \n 
~~~ resume_path = None \n 
~~ self . _resume_folder = resume_folder \n 
self . _resume_path = resume_path \n 
self . _delete_resume = delete_resume \n 
self . _multiproc = multiproc \n 
if ncores == 0 : \n 
~~~ ncores = psutil . cpu_count ( ) \n 
self . _logger . info ( % ncores ) \n 
~~ self . _ncores = ncores \n 
if queue_maxsize == - 1 : \n 
~~~ queue_maxsize = 2 * ncores \n 
~~ self . _queue_maxsize = queue_maxsize \n 
if wrap_mode is None : \n 
~~~ wrap_mode = pypetconstants . WRAP_MODE_NONE \n 
~~ self . _wrap_mode = wrap_mode \n 
self . _use_pool = use_pool \n 
self . _use_scoop = use_scoop \n 
self . _freeze_input = freeze_input \n 
self . _gc_interval = gc_interval \n 
self . _do_single_runs = do_single_runs \n 
self . _automatic_storing = automatic_storing \n 
self . _clean_up_runs = clean_up_runs \n 
if ( wrap_mode == pypetconstants . WRAP_MODE_NETLOCK and \n 
not isinstance ( port , compat . base_type ) ) : \n 
~~~ url = port_to_tcp ( port ) \n 
self . _logger . info ( % url ) \n 
~~~ url = port \n 
~~ self . _url = url \n 
self . _timeout = timeout \n 
if lazy_debug and is_debug ( ) : \n 
~~~ self . _logger . warning ( ) \n 
~~ self . _current_idx = 0 \n 
self . _trajectory_name = self . _traj . v_name \n 
for kwarg in list ( unused_kwargs ) : \n 
~~~ val = kwargs [ kwarg ] \n 
self . _traj . f_set_properties ( ** { kwarg : val } ) \n 
self . _logger . info ( % ( kwarg , str ( val ) ) ) \n 
unused_kwargs . remove ( kwarg ) \n 
~~ ~~ if len ( unused_kwargs ) > 0 : \n 
% str ( unused_kwargs ) ) \n 
~~ self . _add_config ( ) \n 
self . _logger . info ( ) \n 
~~ ~~ def _add_config ( self ) : \n 
~~~ if self . _do_single_runs : \n 
~~~ config_name = % self . name \n 
self . _traj . f_add_config ( Parameter , config_name , self . _multiproc , \n 
comment = ) . f_lock ( ) \n 
if self . _multiproc : \n 
self . _traj . f_add_config ( Parameter , config_name , self . _use_pool , \n 
comment = \n 
) . f_lock ( ) \n 
config_name = % self . name \n 
self . _traj . f_add_config ( Parameter , config_name , self . _use_scoop , \n 
if self . _niceness is not None : \n 
self . _traj . f_add_config ( Parameter , config_name , self . _niceness , \n 
~~ if self . _use_pool : \n 
self . _traj . f_add_config ( Parameter , config_name , self . _freeze_input , \n 
~~ elif self . _use_scoop : \n 
self . _traj . f_add_config ( Parameter , config_name , self . _cpu_cap , \n 
self . _traj . f_add_config ( Parameter , config_name , self . _memory_cap , \n 
self . _traj . f_add_config ( Parameter , config_name , self . _swap_cap , \n 
self . _traj . f_add_config ( Parameter , config_name , self . _immediate_postproc , \n 
~~ config_name = % self . name \n 
self . _traj . f_add_config ( Parameter , config_name , self . _ncores , \n 
self . _traj . f_add_config ( Parameter , config_name , self . _wrap_mode , \n 
if ( self . _wrap_mode == pypetconstants . WRAP_MODE_QUEUE or \n 
self . _wrap_mode == pypetconstants . WRAP_MODE_PIPE ) : \n 
self . _traj . f_add_config ( Parameter , config_name , self . _queue_maxsize , \n 
~~ if self . _wrap_mode == pypetconstants . WRAP_MODE_NETLOCK : \n 
self . _traj . f_add_config ( Parameter , config_name , self . _url , \n 
~~ if self . _wrap_mode == pypetconstants . WRAP_MODE_NETLOCK or self . _use_scoop : \n 
timeout = self . _timeout \n 
if timeout is None : \n 
~~~ timeout = - 1.0 \n 
~~ self . _traj . f_add_config ( Parameter , config_name , timeout , \n 
~~ if ( self . _gc_interval and \n 
( self . _wrap_mode == pypetconstants . WRAP_MODE_LOCAL or \n 
self . _wrap_mode == pypetconstants . WRAP_MODE_QUEUE or \n 
self . _wrap_mode == pypetconstants . WRAP_MODE_PIPE ) ) : \n 
self . _traj . f_add_config ( Parameter , config_name , self . _gc_interval , \n 
~~ ~~ config_name = % self . _name \n 
self . _traj . f_add_config ( Parameter , config_name , self . _clean_up_runs , \n 
config_name = % self . _name \n 
self . _traj . f_add_config ( Parameter , config_name , self . _resumable , \n 
self . _traj . f_add_config ( Parameter , config_name , self . _graceful_exit , \n 
self . _traj . f_add_config ( Parameter , config_name , self . trajectory . v_name , \n 
self . _traj . f_add_config ( Parameter , config_name , self . trajectory . v_timestamp , \n 
self . _traj . f_add_config ( Parameter , config_name , self . timestamp , \n 
self . _traj . f_add_config ( Parameter , config_name , self . hexsha , \n 
if not self . _traj . f_contains ( + config_name ) : \n 
~~~ self . _traj . f_add_config ( Parameter , config_name , self . _automatic_storing , \n 
self . _traj . f_add_config ( Parameter , config_name , main . __file__ , \n 
~~ for package_name , version in pypetconstants . VERSIONS_TO_STORE . items ( ) : \n 
~~~ config_name = % ( self . name , package_name ) \n 
self . _traj . f_add_config ( Parameter , config_name , version , \n 
~~ self . _traj . config . environment . v_comment = \n 
~~ def _execute_multiprocessing ( self , start_run_idx , results ) : \n 
n = start_run_idx \n 
total_runs = len ( self . _traj ) \n 
expanded_by_postproc = False \n 
if ( self . _wrap_mode == pypetconstants . WRAP_MODE_NONE or \n 
self . _storage_service . multiproc_safe ) : \n 
~~~ self . _logger . info ( ) \n 
~~~ use_manager = ( self . _wrap_mode == pypetconstants . WRAP_MODE_QUEUE or \n 
self . _immediate_postproc ) \n 
self . _multiproc_wrapper = MultiprocContext ( self . _traj , \n 
self . _wrap_mode , \n 
full_copy = None , \n 
manager = None , \n 
use_manager = use_manager , \n 
lock = None , \n 
queue = None , \n 
queue_maxsize = self . _queue_maxsize , \n 
port = self . _url , \n 
timeout = self . _timeout , \n 
gc_interval = self . _gc_interval , \n 
log_config = self . _logging_manager . log_config , \n 
log_stdout = self . _logging_manager . log_stdout , \n 
graceful_exit = self . _graceful_exit ) \n 
self . _multiproc_wrapper . start ( ) \n 
~~~ if self . _use_pool : \n 
~~~ self . _logger . info ( % self . _ncores ) \n 
if self . _freeze_input : \n 
init_kwargs = self . _make_kwargs ( ) \n 
pool_full_copy = self . _traj . v_full_copy \n 
self . _traj . v_full_copy = True \n 
initializer = _configure_frozen_pool \n 
target = _frozen_pool_single_run \n 
~~~ pool_service = self . _traj . v_storage_service \n 
self . _traj . v_storage_service = None \n 
init_kwargs = dict ( logging_manager = self . _logging_manager , \n 
storage_service = pool_service , \n 
niceness = self . _niceness ) \n 
initializer = _configure_pool \n 
target = _pool_single_run \n 
~~~ iterator = self . _make_iterator ( start_run_idx ) \n 
mpool = multip . Pool ( self . _ncores , initializer = initializer , \n 
initargs = ( init_kwargs , ) ) \n 
pool_results = mpool . imap ( target , iterator ) \n 
self . _show_progress ( n - 1 , total_runs ) \n 
for result in pool_results : \n 
~~~ n = self . _check_result_and_store_references ( result , results , \n 
n , total_runs ) \n 
~~ mpool . close ( ) \n 
mpool . join ( ) \n 
~~~ if self . _freeze_input : \n 
~~~ self . _traj . v_full_copy = pool_full_copy \n 
~~~ self . _traj . v_storage_service = pool_service \n 
~~ ~~ self . _logger . info ( ) \n 
del mpool \n 
if hasattr ( _frozen_scoop_single_run , ) : \n 
~~~ self . _logger . warning ( \n 
~~ _frozen_scoop_single_run . kwargs = { } \n 
scoop_full_copy = self . _traj . v_full_copy \n 
scoop_rev = self . name + + str ( time . time ( ) ) . replace ( , ) \n 
shared . setConst ( ** { scoop_rev : init_kwargs } ) \n 
iterator = self . _make_iterator ( start_run_idx , \n 
copy_data = True , \n 
scoop_rev = scoop_rev ) \n 
target = _frozen_scoop_single_run \n 
~~~ iterator = self . _make_iterator ( start_run_idx , \n 
copy_data = True ) \n 
target = _scoop_single_run \n 
~~~ if scoop . IS_RUNNING : \n 
~~~ scoop_results = futures . map ( target , iterator , timeout = self . _timeout ) \n 
~~~ self . _logger . error ( \n 
scoop_results = map ( target , iterator ) \n 
~~ self . _show_progress ( n - 1 , total_runs ) \n 
for result in scoop_results : \n 
~~~ self . _traj . v_full_copy = scoop_full_copy \n 
~~~ if self . _immediate_postproc : \n 
~~~ maxsize = 0 \n 
~~~ maxsize = total_runs \n 
~~ start_result_length = len ( results ) \n 
result_queue = multip . Queue ( maxsize = maxsize ) \n 
iterator = self . _make_iterator ( start_run_idx , result_queue = result_queue ) \n 
self . _logger . info ( \n 
% self . _ncores ) \n 
( self . _cpu_cap , self . _memory_cap [ 0 ] , self . _swap_cap ) ) \n 
cpu_usage_func = lambda : self . _estimate_cpu_utilization ( ) \n 
memory_usage_func = lambda : self . _estimate_memory_utilization ( process_dict ) \n 
swap_usage_func = lambda : psutil . swap_memory ( ) . percent \n 
while len ( process_dict ) > 0 or keep_running : \n 
~~~ for pid in compat . listkeys ( process_dict ) : \n 
~~~ proc = process_dict [ pid ] \n 
if not proc . is_alive ( ) : \n 
~~~ proc . join ( ) \n 
del process_dict [ pid ] \n 
del proc \n 
~~ ~~ no_cap = True \n 
if self . _check_usage and self . _ncores > len ( process_dict ) > 0 : \n 
~~~ for cap_name , cap_function , threshold in ( \n 
( , cpu_usage_func , self . _cpu_cap ) , \n 
( , memory_usage_func , self . _memory_cap [ 0 ] ) , \n 
( , swap_usage_func , self . _swap_cap ) ) : \n 
~~~ cap_value = cap_function ( ) \n 
if cap_value > threshold : \n 
~~~ no_cap = False \n 
if signal_cap : \n 
~~~ if cap_name == : \n 
~~~ add_on_str = \n 
~~ self . _logger . warning ( \n 
( len ( process_dict ) , cap_name , \n 
cap_value , threshold , \n 
add_on_str ) ) \n 
signal_cap = False \n 
max_signals -= 1 \n 
if max_signals == 0 : \n 
~~ ~~ ~~ if len ( process_dict ) < self . _ncores and keep_running and no_cap : \n 
~~~ task = next ( iterator ) \n 
proc = multip . Process ( target = _process_single_run , \n 
args = ( task , ) ) \n 
proc . start ( ) \n 
process_dict [ proc . pid ] = proc \n 
~~~ keep_running = False \n 
if self . _postproc is not None and self . _immediate_postproc : \n 
~~~ if self . _wrap_mode == pypetconstants . WRAP_MODE_LOCAL : \n 
~~~ reference_service = self . _traj . _storage_service \n 
self . _traj . v_storage_service = self . _storage_service \n 
keep_running , start_run_idx , new_runs = self . _execute_postproc ( results ) \n 
~~~ self . _traj . _storage_service = reference_service \n 
~~ ~~ if keep_running : \n 
~~~ expanded_by_postproc = True \n 
% new_runs ) \n 
result_queue = result_queue ) \n 
~~ ~~ if not keep_running : \n 
~~~ self . _logger . debug ( \n 
~~~ time . sleep ( 0.001 ) \n 
~~ n = self . _get_results_from_queue ( result_queue , results , n , total_runs ) \n 
~~ self . _get_results_from_queue ( result_queue , results , n , total_runs ) \n 
result_queue . close ( ) \n 
result_queue . join_thread ( ) \n 
del result_queue \n 
result_sort ( results , start_result_length ) \n 
~~~ if self . _multiproc_wrapper is not None : \n 
~~~ self . _multiproc_wrapper . finalize ( ) \n 
self . _multiproc_wrapper = None \n 
~~ ~~ return expanded_by_postproc \n 
~~ def cli_vars_to_dict ( self , tplvars ) : \n 
d = { } \n 
failed = [ ] \n 
for v in tplvars : \n 
~~~ varname , varval = v . split ( ) \n 
~~~ failed . append ( v ) \n 
~~ d [ varname ] = varval \n 
~~ return d , failed \n 
~~ def interactive_mode ( self , tpl ) : \n 
~~~ tplvars = tpl . get_template_vars ( ) \n 
~~ d [ v ] = s \n 
~~ return d \n 
~~ def bayesdb_generator_cell_value ( bdb , generator_id , colno , rowid ) : \n 
~~~ table_name = core . bayesdb_generator_table ( bdb , generator_id ) \n 
qt = bql_quote_name ( table_name ) \n 
colname = core . bayesdb_generator_column_name ( bdb , generator_id , colno ) \n 
qcn = bql_quote_name ( colname ) \n 
sql = % ( qcn , qt ) \n 
cursor = bdb . sql_execute ( sql , ( rowid , ) ) \n 
~~~ row = cursor . next ( ) \n 
~~~ assert False , % ( rowid , ) \n 
~~~ return row [ 0 ] \n 
~~ ~~ def is_iterable ( ob ) : \n 
~~~ if isinstance ( ob , six . string_types ) : \n 
~~~ iter ( ob ) \n 
~~ ~~ def get_cardinality ( ob ) : \n 
~~~ return len ( ob ) \n 
~~ ~~ def test_threaded_import_lock_fork ( self ) : \n 
import_started = threading . Event ( ) \n 
partial_module = "partial" \n 
complete_module = "complete" \n 
def importer ( ) : \n 
~~~ imp . acquire_lock ( ) \n 
sys . modules [ fake_module_name ] = partial_module \n 
import_started . set ( ) \n 
sys . modules [ fake_module_name ] = complete_module \n 
imp . release_lock ( ) \n 
~~ t = threading . Thread ( target = importer ) \n 
t . start ( ) \n 
import_started . wait ( ) \n 
pid = os . fork ( ) \n 
~~~ if not pid : \n 
~~~ m = __import__ ( fake_module_name ) \n 
if m == complete_module : \n 
~~~ os . _exit ( 0 ) \n 
~~~ if verbose > 1 : \n 
~~ os . _exit ( 1 ) \n 
~~~ t . join ( ) \n 
self . wait_impl ( pid ) \n 
~~~ os . kill ( pid , signal . SIGKILL ) \n 
~~ ~~ ~~ def test_nested_import_lock_fork ( self ) : \n 
def fork_with_import_lock ( level ) : \n 
~~~ release = 0 \n 
in_child = False \n 
~~~ for i in range ( level ) : \n 
release += 1 \n 
~~ pid = os . fork ( ) \n 
in_child = not pid \n 
~~~ for i in range ( release ) : \n 
~~~ imp . release_lock ( ) \n 
~~~ if in_child : \n 
~~ if in_child : \n 
~~ self . wait_impl ( pid ) \n 
~~ for level in range ( 5 ) : \n 
~~~ fork_with_import_lock ( level ) \n 
~~ ~~ def _init_toolkit ( ) : \n 
def import_toolkit ( tk ) : \n 
~~~ be = % tk \n 
__import__ ( be + ) \n 
~~ return be \n 
~~ if ETSConfig . toolkit : \n 
~~~ be = import_toolkit ( ETSConfig . toolkit ) \n 
~~~ known_toolkits = ( , , ) \n 
for tk in known_toolkits : \n 
~~~ with provisional_toolkit ( tk ) : \n 
~~~ be = import_toolkit ( tk ) \n 
~~ break \n 
~~ except ImportError as exc : \n 
logger . info ( msg . format ( tk ) ) \n 
if logger . getEffectiveLevel ( ) <= logging . INFO : \n 
~~~ logger . exception ( exc ) \n 
~~~ be = import_toolkit ( ) \n 
import warnings \n 
warnings . warn ( msg . format ( toolkit_name ) , RuntimeWarning ) \n 
~~ ~~ ~~ global _toolkit_backend \n 
_toolkit_backend = be \n 
~~ def toolkit_object ( name ) : \n 
mname , oname = name . split ( ) \n 
be_mname = _toolkit_backend + mname \n 
class Unimplemented ( object ) : \n 
def __init__ ( self , * args , ** kwargs ) : \n 
~~ ~~ be_obj = Unimplemented \n 
~~~ __import__ ( be_mname ) \n 
~~~ be_obj = getattr ( sys . modules [ be_mname ] , oname ) \n 
~~ ~~ except ImportError as exc : \n 
~~~ if all ( part not in exc . args [ 0 ] for part in mname . split ( ) ) : \n 
~~ if in os . environ : \n 
~~~ import traceback \n 
frames = traceback . extract_tb ( sys . exc_traceback ) \n 
filename , lineno , function , text = frames [ - 1 ] \n 
if not _toolkit_backend in filename : \n 
~~ ~~ ~~ return be_obj \n 
~~ def get_ratelimitable_key_tuples ( self , req , account_name , \n 
container_name = None , obj_name = None , \n 
global_ratelimit = None ) : \n 
keys = [ ] \n 
if self . account_ratelimit and account_name and container_name and not obj_name and req . method in ( , ) : \n 
~~~ keys . append ( ( "ratelimit/%s" % account_name , \n 
self . account_ratelimit ) ) \n 
~~ if account_name and container_name and obj_name and req . method in ( , , , ) : \n 
~~~ container_size = self . get_container_size ( req . environ ) \n 
container_rate = get_maxrate ( \n 
self . container_ratelimits , container_size ) \n 
if container_rate : \n 
~~~ keys . append ( ( \n 
"ratelimit/%s/%s" % ( account_name , container_name ) , \n 
container_rate ) ) \n 
~~ ~~ if account_name and container_name and not obj_name and req . method == : \n 
self . container_listing_ratelimits , container_size ) \n 
"ratelimit_listing/%s/%s" % ( account_name , container_name ) , \n 
~~ ~~ if account_name and req . method in ( , , , ) : \n 
~~~ if global_ratelimit : \n 
~~~ global_ratelimit = float ( global_ratelimit ) \n 
if global_ratelimit > 0 : \n 
"ratelimit/global-write/%s" % account_name , \n 
global_ratelimit ) ) \n 
~~ ~~ ~~ return keys \n 
~~ def handle_ratelimit ( self , req , account_name , container_name , obj_name ) : \n 
if not self . memcache_client : \n 
~~~ account_info = get_account_info ( req . environ , self . app , \n 
swift_source = ) \n 
account_global_ratelimit = account_info . get ( , { } ) . get ( ) \n 
~~~ account_global_ratelimit = None \n 
~~ if account_name in self . ratelimit_whitelist or account_global_ratelimit == : \n 
~~ if account_name in self . ratelimit_blacklist or account_global_ratelimit == : \n 
~~~ self . logger . error ( _ ( ) , \n 
account_name ) \n 
eventlet . sleep ( self . BLACK_LIST_SLEEP ) \n 
return Response ( status = , \n 
body = , \n 
request = req ) \n 
~~ for key , max_rate in self . get_ratelimitable_key_tuples ( \n 
req , account_name , container_name = container_name , \n 
obj_name = obj_name , global_ratelimit = account_global_ratelimit ) : \n 
~~~ need_to_sleep = self . _get_sleep_time ( key , max_rate ) \n 
if self . log_sleep_time_seconds and need_to_sleep > self . log_sleep_time_seconds : \n 
"%(account)s/%(container)s/%(object)s" ) , \n 
{ : need_to_sleep , : account_name , \n 
: container_name , : obj_name } ) \n 
~~ if need_to_sleep > 0 : \n 
~~~ eventlet . sleep ( need_to_sleep ) \n 
~~ ~~ except MaxSleepTimeHitError as e : \n 
~~~ self . logger . error ( \n 
_ ( \n 
) , \n 
{ : req . method , : account_name , \n 
: container_name , : obj_name , : str ( e ) } ) \n 
error_resp = Response ( status = , \n 
body = , request = req ) \n 
return error_resp \n 
~~ ~~ return None \n 
~~ def __call__ ( self , env , start_response ) : \n 
req = Request ( env ) \n 
if self . memcache_client is None : \n 
~~~ self . memcache_client = cache_from_env ( env ) \n 
~~ if not self . memcache_client : \n 
_ ( ) ) \n 
~~~ version , account , container , obj = req . split_path ( 1 , 4 , True ) \n 
~~ ratelimit_resp = self . handle_ratelimit ( req , account , container , obj ) \n 
if ratelimit_resp is None : \n 
~~~ return ratelimit_resp ( env , start_response ) \n 
~~ ~~ def make_word_list ( ) : \n 
word_list = [ ] \n 
for i in w : \n 
~~~ d [ i . lower ( ) ] \n 
~~~ if i . lower ( ) == "\'s" : \n 
~~ elif i [ - 1 ] == "." : \n 
~~~ word_list . append ( ( i . lower ( ) , d [ i . lower ( ) ] [ 0 ] ) ) \n 
~~ ~~ ~~ return word_list \n 
~~ def sylcount ( s ) : \n 
~~~ d [ s ] \n 
~~~ if len ( d [ s ] ) <= 1 : \n 
~~~ sj = . join ( d [ s ] [ 0 ] ) \n 
sl = re . split ( , sj ) \n 
return len ( sl ) - 1 \n 
~~~ sj0 = . join ( d [ s ] [ 0 ] ) \n 
sl0 = re . split ( , sj0 ) \n 
sj1 = . join ( d [ s ] [ 1 ] ) \n 
sl1 = re . split ( , sj1 ) \n 
if len ( sl1 ) < len ( sl0 ) : \n 
~~~ return len ( sl1 ) - 1 \n 
~~~ return len ( sl0 ) - 1 \n 
~~ ~~ ~~ ~~ def load ( self , distro , component , persona = None , origins_patch = None ) : \n 
~~~ self . _apply_persona ( component , persona ) \n 
dir_opts = self . _get_dir_opts ( component ) \n 
distro_opts = distro . options \n 
origins_opts = { } \n 
if self . _origins_path : \n 
~~~ origins = _origins . load ( self . _origins_path , \n 
patch_file = origins_patch ) \n 
origins_opts = origins [ component ] \n 
~~ ~~ component_opts = [ ] \n 
for conf in ( , component ) : \n 
~~~ component_opts . append ( self . _base_loader . load ( conf ) ) \n 
~~ except exceptions . YamlConfigNotFoundException : \n 
~~ ~~ merged_opts = utils . merge_dicts ( \n 
dir_opts , \n 
distro_opts , \n 
origins_opts , \n 
* component_opts \n 
return merged_opts \n 
~~ def _load_option ( self , conf , opt ) : \n 
~~~ return self . _processed [ conf ] [ opt ] \n 
~~~ if ( conf , opt ) in self . _ref_stack : \n 
~~~ raise exceptions . YamlLoopException ( conf , opt , self . _ref_stack ) \n 
~~ self . _ref_stack . append ( ( conf , opt ) ) \n 
self . _cache ( conf ) \n 
~~~ raw_value = self . _cached [ conf ] [ opt ] \n 
~~~ cur_conf , cur_opt = self . _ref_stack [ - 1 ] \n 
~~~ cur_conf , cur_opt = None , None \n 
~~ raise exceptions . YamlOptionNotFoundException ( \n 
cur_conf , cur_opt , conf , opt \n 
~~ result = self . _process ( raw_value ) \n 
self . _processed . setdefault ( conf , { } ) [ opt ] = result \n 
self . _ref_stack . pop ( ) \n 
~~ ~~ @ user_passes_test ( is_in_prereg_group ) \n 
def prereg ( request ) : \n 
paginator = Paginator ( get_prereg_drafts ( user = request . user ) , 5 ) \n 
~~~ page_number = int ( request . GET . get ( ) ) \n 
~~~ page_number = 1 \n 
~~ page = paginator . page ( page_number ) \n 
~~~ drafts = [ serializers . serialize_draft_registration ( d , json_safe = False ) for d in page ] \n 
~~ except EmptyPage : \n 
~~~ drafts = [ ] \n 
~~ for draft in drafts : \n 
~~~ draft [ ] = DraftRegistrationForm ( draft ) \n 
: drafts , \n 
: page , \n 
: serializers . IMMEDIATE , \n 
return render ( request , , context ) \n 
~~ def _module_from_ast ( self , name , path ) : \n 
~~~ tree = self . _parse_and_transform_ast ( path ) \n 
package = . join ( name . split ( ) [ : - 1 ] ) \n 
module = imp . new_module ( name ) \n 
module . __file__ = path \n 
~~~ __import__ ( package ) \n 
module . __package__ = package \n 
~~ except ( ImportError , __HOLE__ ) : \n 
~~ self . _prepare_path_for_local_packages ( ) \n 
code = compile ( tree , path , ) \n 
exec ( code , module . __dict__ ) \n 
self . _restore_path ( ) \n 
return module \n 
~~ def describe_message ( message_definition ) : \n 
message_descriptor = MessageDescriptor ( ) \n 
message_descriptor . name = message_definition . definition_name ( ) . split ( ) [ - 1 ] \n 
fields = sorted ( message_definition . all_fields ( ) , \n 
key = lambda v : v . number ) \n 
if fields : \n 
~~~ message_descriptor . fields = [ describe_field ( field ) for field in fields ] \n 
~~~ nested_messages = message_definition . __messages__ \n 
~~~ message_descriptors = [ ] \n 
for name in nested_messages : \n 
~~~ value = getattr ( message_definition , name ) \n 
message_descriptors . append ( describe_message ( value ) ) \n 
~~ message_descriptor . message_types = message_descriptors \n 
~~~ nested_enums = message_definition . __enums__ \n 
~~~ enum_descriptors = [ ] \n 
for name in nested_enums : \n 
enum_descriptors . append ( describe_enum ( value ) ) \n 
~~ message_descriptor . enum_types = enum_descriptors \n 
~~ return message_descriptor \n 
~~ @ util . positional ( 1 ) \n 
def import_descriptor_loader ( definition_name , importer = __import__ ) : \n 
if definition_name . startswith ( ) : \n 
~~~ definition_name = definition_name [ 1 : ] \n 
~~ if not definition_name . startswith ( ) : \n 
~~~ leaf = definition_name . split ( ) [ - 1 ] \n 
if definition_name : \n 
~~~ module = importer ( definition_name , , , [ leaf ] ) \n 
~~~ return describe ( module ) \n 
~~ ~~ ~~ try : \n 
~~~ return describe ( messages . find_definition ( definition_name , \n 
importer = __import__ ) ) \n 
~~ except messages . DefinitionNotFoundError , err : \n 
~~~ split_name = definition_name . rsplit ( , 1 ) \n 
if len ( split_name ) > 1 : \n 
~~~ parent , child = split_name \n 
~~~ parent_definition = import_descriptor_loader ( parent , importer = importer ) \n 
~~ except messages . DefinitionNotFoundError : \n 
~~~ if isinstance ( parent_definition , FileDescriptor ) : \n 
~~~ search_list = parent_definition . service_types or [ ] \n 
~~ elif isinstance ( parent_definition , ServiceDescriptor ) : \n 
~~~ search_list = parent_definition . methods or [ ] \n 
~~ elif isinstance ( parent_definition , EnumDescriptor ) : \n 
~~~ search_list = parent_definition . values or [ ] \n 
~~ elif isinstance ( parent_definition , MessageDescriptor ) : \n 
~~~ search_list = parent_definition . fields or [ ] \n 
~~~ search_list = [ ] \n 
~~ for definition in search_list : \n 
~~~ if definition . name == child : \n 
~~~ return definition \n 
~~ ~~ ~~ ~~ raise err \n 
~~ ~~ def lookup_descriptor ( self , definition_name ) : \n 
~~~ return self . __descriptors [ definition_name ] \n 
~~ if self . __descriptor_loader : \n 
~~~ definition = self . __descriptor_loader ( definition_name ) \n 
self . __descriptors [ definition_name ] = definition \n 
return definition \n 
~~~ raise messages . DefinitionNotFoundError ( \n 
% definition_name ) \n 
~~ ~~ def moveComponentsFrom ( self , source ) : \n 
i = source . getComponentIterator ( ) \n 
~~~ c = i . next ( ) \n 
caption = None \n 
icon = None \n 
if issubclass ( source . __class__ , TabSheet ) : \n 
~~~ caption = source . getTabCaption ( c ) \n 
icon = source . getTabIcon ( c ) \n 
~~ source . removeComponent ( c ) \n 
self . addTab ( c , caption , icon ) \n 
~~ ~~ ~~ def paintContent ( self , target ) : \n 
if self . areTabsHidden ( ) : \n 
~~~ target . addAttribute ( , True ) \n 
~~ target . startTag ( ) \n 
orphaned = set ( self . _paintedTabs ) \n 
i = self . getComponentIterator ( ) \n 
~~~ component = i . next ( ) \n 
if component in orphaned : \n 
~~~ orphaned . remove ( component ) \n 
~~ tab = self . _tabs . get ( component ) \n 
target . startTag ( ) \n 
if not tab . isEnabled ( ) and tab . isVisible ( ) : \n 
~~ if not tab . isVisible ( ) : \n 
~~ if tab . isClosable ( ) : \n 
~~ icon = tab . getIcon ( ) \n 
if icon is not None : \n 
~~~ target . addAttribute ( , icon ) \n 
~~ caption = tab . getCaption ( ) \n 
if caption is not None and len ( caption ) > 0 : \n 
~~~ target . addAttribute ( , caption ) \n 
~~ description = tab . getDescription ( ) \n 
if description is not None : \n 
~~~ target . addAttribute ( , description ) \n 
~~ componentError = tab . getComponentError ( ) \n 
if componentError is not None : \n 
~~~ componentError . paint ( target ) \n 
~~ target . addAttribute ( , self . _keyMapper . key ( component ) ) \n 
if component == self . _selected : \n 
component . paint ( target ) \n 
self . _paintedTabs . add ( component ) \n 
~~ elif component in self . _paintedTabs : \n 
~~~ component . paint ( target ) \n 
~~~ component . requestRepaintRequests ( ) \n 
~~ target . endTag ( ) \n 
~~ ~~ target . endTag ( ) \n 
if self . _selected is not None : \n 
~~~ target . addVariable ( self , , \n 
self . _keyMapper . key ( self . _selected ) ) \n 
~~ for component2 in orphaned : \n 
~~~ self . _paintedTabs . remove ( component2 ) \n 
~~ ~~ def updateSelection ( self ) : \n 
originalSelection = self . _selected \n 
tab = self . _tabs . get ( component ) \n 
selectedTabInfo = None \n 
~~~ selectedTabInfo = self . _tabs . get ( self . _selected ) \n 
~~ if ( self . _selected is None \n 
or selectedTabInfo is None \n 
or not selectedTabInfo . isVisible ( ) \n 
or not selectedTabInfo . isEnabled ( ) ) : \n 
~~~ if tab . isEnabled ( ) and tab . isVisible ( ) : \n 
~~~ self . _selected = component \n 
~~~ self . _selected = None \n 
~~ ~~ return originalSelection != self . _selected \n 
~~ def getTabPosition ( self , tab ) : \n 
~~~ return self . _components . index ( tab . getComponent ( ) ) \n 
~~~ return - 1 \n 
~~ ~~ def _getActivity ( self , serviceRecord , dbcl , path ) : \n 
~~~ activityData = None \n 
~~~ f , metadata = dbcl . get_file_and_metadata ( path ) \n 
~~ except rest . ErrorResponse as e : \n 
~~~ self . _raiseDbException ( e ) \n 
~~ if not activityData : \n 
~~~ activityData = f . read ( ) \n 
~~~ if path . lower ( ) . endswith ( ".tcx" ) : \n 
~~~ act = TCXIO . Parse ( activityData ) \n 
~~~ act = GPXIO . Parse ( activityData ) \n 
~~ ~~ except __HOLE__ as e : \n 
~~ def rekey ( dikt ) : \n 
for k in dikt . keys ( ) : \n 
~~~ if isinstance ( k , str ) : \n 
~~~ ik = fk = None \n 
~~~ ik = int ( k ) \n 
~~~ fk = float ( k ) \n 
~~ ~~ if ik is not None : \n 
~~~ nk = ik \n 
~~~ nk = fk \n 
~~ if nk in dikt : \n 
~~ dikt [ nk ] = dikt . pop ( k ) \n 
~~ ~~ return dikt \n 
~~ @ contextmanager \n 
def run_kuyruk ( queue = , terminate = True ) : \n 
~~~ assert not_running ( ) \n 
args = [ \n 
sys . executable , , \n 
"--app" , "tests.tasks.kuyruk" , \n 
"worker" , , queue , \n 
environ = os . environ . copy ( ) \n 
environ [ ] = \n 
popen = What ( * args , preexec_fn = os . setsid , env = environ ) \n 
popen . timeout = TIMEOUT \n 
~~~ yield popen \n 
if terminate : \n 
~~~ popen . terminate ( ) \n 
~~ popen . expect_exit ( ) \n 
~~~ popen . kill ( ) \n 
popen . wait ( ) \n 
~~ except OSError as e : \n 
~~ ~~ logger . debug ( , popen . returncode ) \n 
~~~ wait_while ( lambda : get_pids ( ) ) \n 
~~~ print ( popen . get_output ( ) ) \n 
raise Exception \n 
~~ ~~ ~~ def sample ( self , population , k ) : \n 
n = len ( population ) \n 
if not 0 <= k <= n : \n 
~~ random = self . random \n 
_int = int \n 
result = [ None ] * k \n 
~~~ pool = list ( population ) \n 
~~~ j = _int ( random ( ) * ( n - i ) ) \n 
result [ i ] = pool [ j ] \n 
~~~ n > 0 and ( population [ 0 ] , population [ n // 2 ] , population [ n - 1 ] ) \n 
~~~ population = tuple ( population ) \n 
~~ selected = { } \n 
for i in xrange ( k ) : \n 
~~~ j = _int ( random ( ) * n ) \n 
while j in selected : \n 
~~ result [ i ] = selected [ j ] = population [ j ] \n 
~~ def locked_get ( self ) : \n 
credentials = None \n 
~~~ f = open ( self . _filename , ) \n 
content = f . read ( ) \n 
~~~ return credentials \n 
~~~ credentials = Credentials . new_from_json ( content ) \n 
credentials . set_store ( self ) \n 
~~ return credentials \n 
~~ def __init__ ( self ) : \n 
~~~ self . _docker_host = os . environ . get ( ) or None \n 
if self . _docker_host : \n 
~~~ self . _docker_host = self . _docker_host . replace ( , ) \n 
~~ cert_path = os . environ . get ( ) or None \n 
if cert_path : \n 
~~~ self . _ssl_context = ssl . create_default_context ( \n 
cafile = os . path . join ( cert_path , ) ) \n 
self . _ssl_context . load_cert_chain ( \n 
os . path . join ( cert_path , ) , \n 
os . path . join ( cert_path , ) ) \n 
self . _ssl_context . check_hostname = False \n 
~~~ self . test ( ) \n 
~~ except DockerException : \n 
~~ self . _libcuda_files = [ ] \n 
~~~ for lib in subprocess . check_output ( [ , ] ) . split ( ) : \n 
~~~ if not in lib or not in lib : \n 
~~ self . _libcuda_files . append ( lib . split ( ) [ - 1 ] ) \n 
~~ self . _nvidia_device_files = [ ] \n 
for filename in os . listdir ( ) : \n 
~~~ if filename . startswith ( ) : \n 
~~~ self . _nvidia_device_files . append ( os . path . join ( , filename ) ) \n 
~~ ~~ ~~ @ wrap_exception ( ) \n 
~~~ def download_image ( self , docker_image , loop_callback ) : \n 
~~~ logger . debug ( , docker_image ) \n 
with closing ( self . _create_connection ( ) ) as conn : \n 
~~~ conn . request ( , \n 
% docker_image ) \n 
create_image_response = conn . getresponse ( ) \n 
if create_image_response . status != 200 : \n 
~~~ raise DockerException ( create_image_response . read ( ) ) \n 
~~~ loop_callback ( ) \n 
response = None \n 
line = \n 
~~~ ch = create_image_response . read ( 1 ) \n 
if not ch : \n 
~~ line += ch \n 
~~~ response = json . loads ( line ) \n 
logger . debug ( line . strip ( ) ) \n 
~~ ~~ if not response : \n 
~~ if in response : \n 
~~~ raise DockerException ( response [ ] ) \n 
~~ ~~ ~~ ~~ ~~ def _parallel_execute ( datasources , options , outs_dir , pabot_args , suite_names ) : \n 
~~~ original_signal_handler = signal . signal ( signal . SIGINT , keyboard_interrupt ) \n 
pool = ThreadPool ( pabot_args [ ] ) \n 
result = pool . map_async ( execute_and_wait_with , \n 
( ( datasources , outs_dir , options , suite , pabot_args [ ] , pabot_args for suite in suite_names ) ) \n 
pool . close ( ) \n 
while not result . ready ( ) : \n 
~~~ keyboard_interrupt ( ) \n 
~~ ~~ signal . signal ( signal . SIGINT , original_signal_handler ) \n 
~~ def _connect ( self ) : \n 
~~~ return self . driver . connect ( \n 
* self . connect_args , ** self . connect_kw_args \n 
~~ ~~ def test_apilevel ( self ) : \n 
~~~ apilevel = self . driver . apilevel \n 
self . assertEqual ( apilevel , ) \n 
~~ ~~ def test_threadsafety ( self ) : \n 
~~~ threadsafety = self . driver . threadsafety \n 
self . assertTrue ( threadsafety in ( 0 , 1 , 2 , 3 ) ) \n 
~~ ~~ def test_paramstyle ( self ) : \n 
~~~ paramstyle = self . driver . paramstyle \n 
self . assertTrue ( paramstyle in ( \n 
, , , , \n 
~~ ~~ def _is_anaconda ( env ) : \n 
with quiet ( ) : \n 
~~~ conda = _conda_cmd ( env ) \n 
~~ with quiet ( ) : \n 
~~~ full_pip = None \n 
~~ ~~ in_anaconda_dir = full_pip and full_pip . succeeded and "/anaconda/" in full_pip \n 
return has_conda or in_anaconda_dir \n 
~~ def update ( self , ** kwargs ) : \n 
~~~ for key , item in kwargs . items ( ) : \n 
~~~ setattr ( self . document_instance , key , item ) \n 
~~ ~~ self . document_instance . save ( ) \n 
~~ def append ( self , node ) : \n 
if isinstance ( node , ( Stream , Element , basestring , int , float , long ) ) : \n 
~~~ self . children . append ( node ) \n 
~~ elif isinstance ( node , Fragment ) : \n 
~~~ self . children . extend ( node . children ) \n 
~~ elif node is not None : \n 
~~~ for child in node : \n 
~~~ self . append ( child ) \n 
~~ ~~ ~~ def convert_to_int ( value ) : \n 
~~~ new_value = int ( value ) \n 
~~~ new_value = 0 \n 
~~ return new_value \n 
~~ def value_exists ( value ) : \n 
~~~ value = float ( value ) \n 
~~ return bool ( value ) \n 
~~ def _url_for_fetch ( self , mapping ) : \n 
~~~ return mapping [ ] \n 
~~ ~~ def _build_metadata ( self , year , elections ) : \n 
~~~ meta = [ ] \n 
year_int = int ( year ) \n 
for election in elections : \n 
~~~ if election [ ] == True : \n 
~~~ results = [ x for x in self . _url_paths ( ) if x [ ] == election [ ] and x for result in results : \n 
~~~ ocd_id = \n 
generated_filename = self . _generates_special_filename ( result ) \n 
pre_processed_url = result [ ] \n 
meta . append ( { \n 
"generated_filename" : generated_filename , \n 
"raw_url" : election [ ] [ 0 ] , \n 
"pre_processed_url" : None , \n 
"ocd_id" : ocd_id , \n 
"name" : , \n 
"election" : election [ ] \n 
~~ ~~ elif in election [ ] [ 0 ] : \n 
generated_filename = self . _generate_county_filename ( election ) \n 
"raw_url" : j . report_url ( ) , \n 
subs = j . get_subjurisdictions ( ) \n 
for county in self . _jurisdictions ( ) : \n 
~~~ subj = [ s for s in subs if s . name . strip ( ) == county [ ] . strip ( ) ] [ 0 ] \n 
generated_filename = self . _generate_precinct_filename ( election , county ) \n 
"raw_url" : subj . report_url ( ) , \n 
"ocd_id" : county [ ] , \n 
"name" : county [ ] , \n 
~~ ~~ ~~ ~~ return meta \n 
~~ def upload_emoji ( request , user_profile ) : \n 
~~~ emoji_name = request . POST . get ( , None ) \n 
emoji_url = request . POST . get ( , None ) \n 
~~~ check_add_realm_emoji ( user_profile . realm , emoji_name , emoji_url ) \n 
~~~ return json_error ( e . message_dict ) \n 
~~ return json_success ( ) \n 
~~ def test_numpy_validators_loaded_if_numpy_present ( self ) : \n 
~~~ import numpy \n 
~~ import sys \n 
del sys . modules [ ] \n 
for k in list ( sys . modules ) : \n 
~~~ if k . startswith ( ) : \n 
~~~ del sys . modules [ k ] \n 
~~ ~~ from traits . trait_types import float_fast_validate \n 
import numpy \n 
self . assertIn ( numpy . floating , float_fast_validate ) \n 
~~ def get_cached_data ( self , file_path , preread_lines = None ) : \n 
~~~ if not file_path in self . cache : \n 
~~~ return None , None \n 
~~ entry = self . cache [ file_path ] \n 
~~~ current_hash = self . compute_hash ( file_path , preread_lines ) \n 
~~~ del self . cache [ file_path ] \n 
return None , None \n 
~~ if entry . stored_hash != current_hash : \n 
~~~ return None , current_hash \n 
~~ return entry . data , current_hash \n 
~~ def shell ( task , welcome_banner = None , prompt_text = None , prompt_color = None , prompt_symbol = ) : \n 
~~~ if prompt_color : \n 
~~~ prompt_text = Color . str ( prompt_text , foreground = prompt_color ) \n 
~~ prompt = . format ( prompt_text . strip ( ) + or , prompt_symbol ) \n 
input = six . moves . input \n 
if welcome_banner : \n 
~~~ print ( welcome_banner ) \n 
~~ print ( ) \n 
~~~ cmd = input ( prompt ) \n 
~~~ task . dispatch ( shlex . split ( cmd ) ) \n 
~~ ~~ ~~ def setuid ( user_id_or_name ) : \n 
~~~ new_uid = int ( user_id_or_name ) \n 
~~~ new_uid = pwd . getpwnam ( user_id_or_name ) . pw_uid \n 
~~ if new_uid != 0 : \n 
~~~ os . setuid ( new_uid ) \n 
~~~ msg = _ ( ) % new_uid \n 
LOG . critical ( msg ) \n 
raise exceptions . FailToDropPrivilegesExit ( msg ) \n 
~~ ~~ ~~ def setgid ( group_id_or_name ) : \n 
~~~ new_gid = int ( group_id_or_name ) \n 
~~ except ( TypeError , ValueError ) : \n 
~~~ new_gid = grp . getgrnam ( group_id_or_name ) . gr_gid \n 
~~ if new_gid != 0 : \n 
~~~ os . setgid ( new_gid ) \n 
~~~ msg = _ ( ) % new_gid \n 
~~ ~~ ~~ def drop_privileges ( user = None , group = None ) : \n 
if user is None and group is None : \n 
~~ if os . geteuid ( ) != 0 : \n 
~~ if group is not None : \n 
~~~ os . setgroups ( [ ] ) \n 
~~ setgid ( group ) \n 
~~ if user is not None : \n 
~~~ setuid ( user ) \n 
{ : os . getuid ( ) , : os . getgid ( ) } ) \n 
~~ def __init__ ( self , pidfile , procname , uuid = None ) : \n 
~~~ self . pidfile = pidfile \n 
self . procname = procname \n 
self . uuid = uuid \n 
~~~ self . fd = os . open ( pidfile , os . O_CREAT | os . O_RDWR ) \n 
fcntl . flock ( self . fd , fcntl . LOCK_EX | fcntl . LOCK_NB ) \n 
~~ ~~ def read ( self ) : \n 
~~~ pid = int ( os . read ( self . fd , 128 ) ) \n 
os . lseek ( self . fd , 0 , os . SEEK_SET ) \n 
return pid \n 
~~ ~~ def is_running ( self ) : \n 
~~~ pid = self . read ( ) \n 
if not pid : \n 
~~ cmdline = % pid \n 
~~~ with open ( cmdline , "r" ) as f : \n 
~~~ exec_out = f . readline ( ) \n 
~~ return self . procname in exec_out and ( not self . uuid or \n 
self . uuid in exec_out ) \n 
~~ ~~ def _fork ( self ) : \n 
~~~ pid = os . fork ( ) \n 
if pid > 0 : \n 
~~~ LOG . exception ( _LE ( ) ) \n 
~~ ~~ def rollback_checkpoints ( self , rollback = 1 ) : \n 
~~~ rollback = int ( rollback ) \n 
~~ if rollback < 0 : \n 
~~ backups = os . listdir ( self . config . backup_dir ) \n 
backups . sort ( ) \n 
if not backups : \n 
~~~ logger . warning ( \n 
~~ elif len ( backups ) < rollback : \n 
rollback , len ( backups ) ) \n 
~~ while rollback > 0 and backups : \n 
~~~ cp_dir = os . path . join ( self . config . backup_dir , backups . pop ( ) ) \n 
~~~ self . _recover_checkpoint ( cp_dir ) \n 
~~ except errors . ReverterError : \n 
raise errors . ReverterError ( \n 
~~ rollback -= 1 \n 
~~ ~~ def view_config_changes ( self , for_logging = False , num = None ) : \n 
backups = os . listdir ( self . config . backup_dir ) \n 
backups . sort ( reverse = True ) \n 
if num : \n 
~~~ backups = backups [ : num ] \n 
~~ if not backups : \n 
~~~ for bkup in backups : \n 
~~~ float ( bkup ) \n 
~~~ raise errors . ReverterError ( \n 
~~ output = [ ] \n 
for bkup in backups : \n 
~~~ output . append ( time . ctime ( float ( bkup ) ) ) \n 
cur_dir = os . path . join ( self . config . backup_dir , bkup ) \n 
with open ( os . path . join ( cur_dir , "CHANGES_SINCE" ) ) as changes_fd : \n 
~~~ output . append ( changes_fd . read ( ) ) \n 
with open ( os . path . join ( cur_dir , "FILEPATHS" ) ) as paths_fd : \n 
~~~ filepaths = paths_fd . read ( ) . splitlines ( ) \n 
for path in filepaths : \n 
~~ ~~ if os . path . isfile ( os . path . join ( cur_dir , "NEW_FILES" ) ) : \n 
~~~ with open ( os . path . join ( cur_dir , "NEW_FILES" ) ) as new_fd : \n 
filepaths = new_fd . read ( ) . splitlines ( ) \n 
~~ ~~ ~~ output . append ( os . linesep ) \n 
~~ if for_logging : \n 
~~~ return os . linesep . join ( output ) \n 
~~ zope . component . getUtility ( interfaces . IDisplay ) . notification ( \n 
os . linesep . join ( output ) , display_util . HEIGHT ) \n 
~~ def _add_to_checkpoint_dir ( self , cp_dir , save_files , save_notes ) : \n 
le_util . make_or_verify_dir ( \n 
cp_dir , constants . CONFIG_DIRS_MODE , os . geteuid ( ) , \n 
self . config . strict_permissions ) \n 
op_fd , existing_filepaths = self . _read_and_append ( \n 
os . path . join ( cp_dir , "FILEPATHS" ) ) \n 
idx = len ( existing_filepaths ) \n 
for filename in save_files : \n 
~~~ if filename not in existing_filepaths : \n 
~~~ shutil . copy2 ( filename , os . path . join ( \n 
cp_dir , os . path . basename ( filename ) + "_" + str ( idx ) ) ) \n 
op_fd . write ( filename + os . linesep ) \n 
~~~ op_fd . close ( ) \n 
logger . error ( \n 
filename , cp_dir ) \n 
"{1}" . format ( filename , cp_dir ) ) \n 
~~ idx += 1 \n 
~~ ~~ op_fd . close ( ) \n 
with open ( os . path . join ( cp_dir , "CHANGES_SINCE" ) , "a" ) as notes_fd : \n 
~~~ notes_fd . write ( save_notes ) \n 
~~ ~~ def _recover_checkpoint ( self , cp_dir ) : \n 
if os . path . isfile ( os . path . join ( cp_dir , "COMMANDS" ) ) : \n 
~~~ self . _run_undo_commands ( os . path . join ( cp_dir , "COMMANDS" ) ) \n 
~~ if os . path . isfile ( os . path . join ( cp_dir , "FILEPATHS" ) ) : \n 
~~~ with open ( os . path . join ( cp_dir , "FILEPATHS" ) ) as paths_fd : \n 
for idx , path in enumerate ( filepaths ) : \n 
~~~ shutil . copy2 ( os . path . join ( \n 
cp_dir , \n 
os . path . basename ( path ) + "_" + str ( idx ) ) , path ) \n 
~~ ~~ ~~ except ( __HOLE__ , OSError ) : \n 
~~ ~~ self . _remove_contained_files ( os . path . join ( cp_dir , "NEW_FILES" ) ) \n 
~~~ shutil . rmtree ( cp_dir ) \n 
~~ ~~ def register_file_creation ( self , temporary , * files ) : \n 
if not files : \n 
~~ cp_dir = self . _get_cp_dir ( temporary ) \n 
new_fd = None \n 
~~~ new_fd , ex_files = self . _read_and_append ( os . path . join ( cp_dir , "NEW_FILES" ) ) \n 
for path in files : \n 
~~~ if path not in ex_files : \n 
~~~ new_fd . write ( "{0}{1}" . format ( path , os . linesep ) ) \n 
~~~ if new_fd is not None : \n 
~~~ new_fd . close ( ) \n 
~~ ~~ ~~ def register_undo_command ( self , temporary , command ) : \n 
commands_fp = os . path . join ( self . _get_cp_dir ( temporary ) , "COMMANDS" ) \n 
command_file = None \n 
~~~ if os . path . isfile ( commands_fp ) : \n 
~~~ command_file = open ( commands_fp , "ab" ) \n 
~~~ command_file = open ( commands_fp , "wb" ) \n 
~~ csvwriter = csv . writer ( command_file ) \n 
csvwriter . writerow ( command ) \n 
~~ except ( IOError , __HOLE__ ) : \n 
~~~ if command_file is not None : \n 
~~~ command_file . close ( ) \n 
if not os . path . isfile ( file_list ) : \n 
~~~ with open ( file_list , "r" ) as list_fd : \n 
~~~ filepaths = list_fd . read ( ) . splitlines ( ) \n 
~~~ if os . path . lexists ( path ) : \n 
~~~ os . remove ( path ) \n 
os . linesep , path ) \n 
~~ ~~ ~~ ~~ except ( IOError , __HOLE__ ) : \n 
~~~ logger . fatal ( \n 
"{0}" . format ( file_list ) ) \n 
~~ def finalize_checkpoint ( self , title ) : \n 
if not os . path . isdir ( self . config . in_progress_dir ) : \n 
~~ changes_since_path = os . path . join ( self . config . in_progress_dir , "CHANGES_SINCE" ) \n 
changes_since_tmp_path = os . path . join ( self . config . in_progress_dir , "CHANGES_SINCE.tmp" ) \n 
if not os . path . exists ( changes_since_path ) : \n 
with open ( self . config . changes_since_path ) as f : \n 
~~ ~~ ~~ NGES_SINCE \n 
~~~ with open ( changes_since_tmp_path , "w" ) as changes_tmp : \n 
with open ( changes_since_path , "r" ) as changes_orig : \n 
~~~ changes_tmp . write ( changes_orig . read ( ) ) \n 
~~ ~~ shutil . move ( changes_since_tmp_path , changes_since_path ) \n 
~~ self . _timestamp_progress_dir ( ) \n 
~~ def _timestamp_progress_dir ( self ) : \n 
for _ in xrange ( 2 ) : \n 
~~~ timestamp = self . _checkpoint_timestamp ( ) \n 
final_dir = os . path . join ( self . config . backup_dir , timestamp ) \n 
~~~ os . rename ( self . config . in_progress_dir , final_dir ) \n 
~~ ~~ logger . error ( \n 
self . config . in_progress_dir , final_dir ) \n 
~~ def testConstructor_noCases ( self ) : \n 
~~~ self . makeTest ( [ ] ) \n 
~~~ self . fail ( ) \n 
~~ ~~ def testConstructor_validTestCase ( self ) : \n 
~~~ self . makeTest ( [ \n 
{ \n 
} , \n 
~~ ~~ @ register . inclusion_tag ( ) \n 
def box_scratchpad ( user ) : \n 
~~~ scratchpad = Scratchpad . objects . latest ( ) \n 
~~~ scratchpad = [ ] \n 
~~ return { : scratchpad } \n 
~~ def link_GL ( name , restype , argtypes , requires = None , suggestions = None ) : \n 
~~~ func = getattr ( gl_lib , name ) \n 
func . restype = restype \n 
func . argtypes = argtypes \n 
decorate_function ( func , name ) \n 
return func \n 
~~~ if _have_getprocaddress : \n 
~~~ bname = cast ( pointer ( create_string_buffer ( asbytes ( name ) ) ) , POINTER ( c_ubyte ) ) \n 
addr = glXGetProcAddressARB ( bname ) \n 
if addr : \n 
~~~ ftype = CFUNCTYPE ( * ( ( restype , ) + tuple ( argtypes ) ) ) \n 
func = cast ( addr , ftype ) \n 
~~ ~~ ~~ return missing_function ( name , requires , suggestions ) \n 
~~ def link_GLU ( name , restype , argtypes , requires = None , suggestions = None ) : \n 
~~~ func = getattr ( glu_lib , name ) \n 
~~~ return missing_function ( name , requires , suggestions ) \n 
~~ ~~ def nonblocking_readlines ( f ) : \n 
fd = f . fileno ( ) \n 
if not platform . system ( ) == : \n 
~~~ fl = fcntl . fcntl ( fd , fcntl . F_GETFL ) \n 
~~ enc = locale . getpreferredencoding ( False ) \n 
buf = bytearray ( ) \n 
~~~ if not platform . system ( ) == : \n 
~~~ block = os . read ( fd , 8192 ) \n 
~~~ block = gevent . os . tp_read ( fd , 8192 ) \n 
~~ ~~ except ( BlockingIOError , __HOLE__ ) : \n 
~~~ yield "" \n 
~~ if not block : \n 
~~~ if buf : \n 
~~~ yield buf . decode ( enc ) \n 
~~ buf . extend ( block ) \n 
~~~ r = buf . find ( ) \n 
n = buf . find ( ) \n 
if r == - 1 and n == - 1 : break \n 
if r == - 1 or r > n : \n 
~~~ yield buf [ : ( n + 1 ) ] . decode ( enc ) \n 
buf = buf [ ( n + 1 ) : ] \n 
~~ elif n == - 1 or n > r : \n 
~~~ yield buf [ : r ] . decode ( enc ) + \n 
if n == r + 1 : \n 
~~~ buf = buf [ ( r + 2 ) : ] \n 
~~~ buf = buf [ ( r + 1 ) : ] \n 
~~ ~~ ~~ ~~ ~~ def sizeof_fmt ( size , suffix = ) : \n 
~~~ size = int ( size ) \n 
~~ if size <= 0 : \n 
~~~ return % suffix \n 
~~ size_name = ( , , , , , , , , ) \n 
i = int ( math . floor ( math . log ( size , 1024 ) ) ) \n 
if i >= len ( size_name ) : \n 
~~~ i = len ( size_name ) - 1 \n 
~~ p = math . pow ( 1024 , i ) \n 
s = size / p \n 
s = round ( s , 2 - int ( math . floor ( math . log10 ( s ) ) ) ) \n 
if s . is_integer ( ) : \n 
~~~ s = int ( s ) \n 
~~ if s > 0 : \n 
~~~ return % ( s , size_name [ i ] , suffix ) \n 
~~ ~~ def parse_version ( * args ) : \n 
v = None \n 
if len ( args ) == 1 : \n 
~~~ a = args [ 0 ] \n 
if isinstance ( a , tuple ) : \n 
~~~ v = . join ( str ( x ) for x in a ) \n 
~~~ v = str ( a ) \n 
~~~ v = . join ( str ( a ) for a in args ) \n 
~~ if v . startswith ( ) : \n 
~~~ v = v [ 1 : ] \n 
~~~ return pkg_resources . SetuptoolsVersion ( v ) \n 
~~~ return pkg_resources . parse_version ( v ) \n 
~~ ~~ def __getattribute__ ( self , key ) : \n 
~~~ return super ( AttrDictWrapper , self ) . __getattribute__ ( key ) \n 
~~~ return self . __dict__ [ "_mapping" ] . __getattribute__ ( key ) \n 
~~~ obj = self . __dict__ [ "_mapping" ] . __getitem__ ( key ) \n 
if hasattr ( obj , "keys" ) : \n 
~~ ~~ ~~ ~~ def __delattr__ ( self , key ) : \n 
~~~ self . __dict__ [ "_mapping" ] . __delitem__ ( key ) \n 
~~~ object . __delattr__ ( self , key ) \n 
~~ ~~ def __getitem__ ( self , name ) : \n 
~~~ return super ( AttrDictDefault , self ) . __getitem__ ( name ) \n 
~~~ return self . __dict__ [ "_default" ] \n 
~~ ~~ def get_object ( self , index , constructor , ** kwargs ) : \n 
~~~ obj = self [ index ] \n 
~~~ obj = constructor ( ** kwargs ) \n 
self [ index ] = obj \n 
~~ def pop ( self , key , * args ) : \n 
~~~ item = dict . __getitem__ ( self , key ) \n 
self . __delitem__ ( key ) \n 
~~~ if args : \n 
~~~ item = args [ 0 ] \n 
~~ ~~ return item \n 
~~ def __getitem__ ( self , key ) : \n 
~~~ list_ = dict . __getitem__ ( self , key ) \n 
~~~ return list_ [ - 1 ] \n 
~~~ return [ ] \n 
~~~ val = self [ key ] \n 
~~ if val == [ ] : \n 
~~ return val \n 
~~ def getlist ( self , key ) : \n 
~~~ return dict . __getitem__ ( self , key ) \n 
~~ ~~ def update ( self , * args , ** kwargs ) : \n 
if len ( args ) > 1 : \n 
~~ if args : \n 
~~~ other_dict = args [ 0 ] \n 
if isinstance ( other_dict , MultiValueDict ) : \n 
~~~ for key , value_list in other_dict . lists ( ) : \n 
~~~ self . setlistdefault ( key , [ ] ) . extend ( value_list ) \n 
~~~ for key , value in other_dict . items ( ) : \n 
~~~ self . setlistdefault ( key , [ ] ) . append ( value ) \n 
~~ ~~ def create_routes ( self , data , ** kwargs ) : \n 
~~~ for row in data : \n 
~~~ id = int ( row [ ] ) \n 
~~~ print % row \n 
~~~ type = int ( row [ ] ) \n 
~~~ type = 0 \n 
~~~ Route ( id = id , \n 
name = self . allcaps ( row [ ] ) , \n 
description = self . allcaps ( row [ ] ) , \n 
type = type , \n 
color = row [ ] ) . save ( ) \n 
~~ ~~ ~~ def create_calendar ( self , data , ** kwargs ) : \n 
~~ begin , end = None , None \n 
if row [ ] : \n 
~~~ begin = parse_date ( row [ ] ) \n 
~~ if row [ ] : \n 
~~~ end = parse_date ( row [ ] ) \n 
~~ defaults = { \n 
: begin , \n 
: end \n 
for dow in [ , , , , , , ~~~ defaults [ dow ] = str ( row [ dow ] ) == \n 
~~~ Schedule ( id = id , ** defaults ) . save ( ) \n 
~~ ~~ ~~ def create_shapes ( self , data , ** kwargs ) : \n 
~~~ shapes = { } \n 
for row in data : \n 
~~ if id not in shapes : \n 
~~~ shapes [ id ] = [ ] \n 
~~~ lat = float ( row [ ] ) \n 
lng = float ( row [ ] ) \n 
~~ shapes [ id ] . append ( ( lat , lng ) ) \n 
~~ for id , points in shapes . iteritems ( ) : \n 
~~~ Shape ( id = id , points = points ) . save ( ) \n 
~~ ~~ ~~ def create_stops ( self , data , ** kwargs ) : \n 
~~~ self . stop_locations = { } \n 
~~ self . stop_locations [ id ] = ( lat , lng ) \n 
defaults = { \n 
: row [ ] , \n 
: self . allcaps ( row [ ] ) , \n 
: ( lat , lng ) \n 
~~~ Stop ( id = id , ** defaults ) . save ( ) \n 
~~ ~~ ~~ def create_stop_times ( self , data , ** kwargs ) : \n 
~~~ if count % 1000 == 0 : \n 
~~~ print % count \n 
~~ count += 1 \n 
~~~ trip_id = int ( row [ ] ) \n 
stop_id = int ( row [ ] ) \n 
~~~ route_id = self . trip_mapping [ trip_id ] [ ] \n 
schedule_id = self . trip_mapping [ trip_id ] [ ] \n 
direction = self . trip_mapping [ trip_id ] [ ] \n 
headsign = self . trip_mapping [ trip_id ] [ ] \n 
block = self . trip_mapping [ trip_id ] [ ] \n 
~~~ seq = int ( row [ ] ) \n 
~~~ seq = 0 \n 
~~~ dist = float ( row [ ] ) \n 
~~~ dist = 0.0 \n 
~~ parts = row [ ] . split ( ) \n 
time = ( int ( parts [ 0 ] ) * 3600 ) + ( int ( parts [ 1 ] ) * 60 ) + int ( parts [ 2 ] ) \n 
~~~ ScheduledStop ( route_id = route_id , \n 
schedule_id = schedule_id , \n 
trip_id = trip_id , \n 
stop_id = stop_id , \n 
arrival = time , \n 
location = self . stop_locations [ stop_id ] , \n 
shape_distance = dist , \n 
pickup_type = row [ ] , \n 
dropoff_type = row [ ] , \n 
direction = direction , \n 
headsign = headsign , \n 
sequence = seq ) . save ( ) \n 
~~ ~~ ~~ def create_trips ( self , data , ** kwargs ) : \n 
~~~ self . trip_mapping = { } \n 
~~~ if count % 500 == 0 : \n 
~~~ route_id = int ( row [ ] ) \n 
svc_id = int ( row [ ] ) \n 
trip_id = int ( row [ ] ) \n 
shape_id = int ( row [ ] ) \n 
self . trip_mapping [ trip_id ] = { \n 
: route_id , \n 
: svc_id , \n 
~~~ route = Route . objects . get ( id = route_id ) \n 
~~ except Route . DoesNotExist : \n 
~~~ print % route_id \n 
~~~ schedule = Schedule . objects . get ( id = svc_id ) \n 
~~ except Schedule . DoesNotExist : \n 
~~~ print % svc_id \n 
schedule = None \n 
~~~ shape = Shape . objects . get ( id = shape_id ) \n 
~~ except Shape . DoesNotExist : \n 
~~~ print % shape_id \n 
shape = None \n 
: route , \n 
: schedule , \n 
: shape , \n 
: row [ ] \n 
~~~ Trip ( id = trip_id , ** defaults ) . save ( ) \n 
~~ ~~ ~~ def paginate_queryset ( self , queryset , request_data ) : \n 
paginator = self . get_paginator ( queryset ) \n 
page_kwarg = self . page_kwarg \n 
page = request_data . get ( page_kwarg , 1 ) \n 
~~~ page_number = int ( page ) \n 
~~~ if page == : \n 
~~~ page_number = paginator . num_pages \n 
~~~ raise InvalidPage ( _ ( \n 
~~ ~~ return paginator , paginator . page ( page_number ) \n 
~~ def bulk_fetch_results ( self , paginated_results ) : \n 
objects = [ ] \n 
models_pks = loaded_objects = { } \n 
for result in paginated_results : \n 
~~~ models_pks . setdefault ( result . model , [ ] ) . append ( result . pk ) \n 
~~ search_backend_alias = self . results . query . backend . connection_alias \n 
for model in models_pks : \n 
~~~ ui = connections [ search_backend_alias ] . get_unified_index ( ) \n 
index = ui . get_index ( model ) \n 
queryset = index . read_queryset ( using = search_backend_alias ) \n 
loaded_objects [ model ] = queryset . in_bulk ( models_pks [ model ] ) \n 
~~ for result in paginated_results : \n 
~~~ model_objects = loaded_objects . get ( result . model , { } ) \n 
~~~ result . _object = model_objects [ int ( result . pk ) ] \n 
~~~ objects . append ( result . _object ) \n 
~~ ~~ return objects \n 
~~ def run_install_command ( graphical , cmd , args , as_root = True ) : \n 
~~~ if isinstance ( args , basestring ) : \n 
~~~ cmd += % shell_escape ( args ) \n 
~~ elif isinstance ( args , list ) : \n 
~~~ for package in args : \n 
~~~ if not isinstance ( package , basestring ) : \n 
~~ cmd += % shell_escape ( package ) \n 
args ) \n 
if as_root and systemType != : \n 
~~~ if graphical : \n 
~~~ sucmd , escape = guess_graphical_sudo ( ) \n 
~~~ if get_executable_path ( ) : \n 
~~ elif systemType != : \n 
~~~ sucmd , escape = , False \n 
~~ ~~ if escape : \n 
~~~ cmd = sucmd % shell_escape ( cmd ) \n 
~~~ cmd = sucmd % cmd \n 
p = subprocess . Popen ( cmd , stdout = subprocess . PIPE , \n 
shell = True ) \n 
lines = [ ] \n 
~~~ for line in iter ( p . stdout . readline , ) : \n 
~~~ print line , \n 
lines . append ( line ) \n 
~~ result = p . wait ( ) \n 
if result != 0 : \n 
~~ def linux_debian_install ( package_name ) : \n 
~~~ qt = qt_available ( ) \n 
~~~ import apt \n 
import apt_pkg \n 
~~~ qt = False \n 
~~ hide_splash_if_necessary ( ) \n 
if qt : \n 
~~~ cmd = shell_escape ( vistrails_root_directory ( ) + \n 
~~~ cmd = % ( \n 
if executable_is_in_path ( ) \n 
else ) \n 
~~ return run_install_command ( qt , cmd , package_name ) \n 
~~ def test_affinity_fail ( ) : \n 
~~~ arch = get_preferred_affinity ( \n 
[ , , ] , \n 
[ "ppc64" , "armel" ] , \n 
valid_arches \n 
~~ ~~ def page ( title = None , pageid = None , auto_suggest = True , redirect = True , preload = False ) : \n 
if title is not None : \n 
~~~ if auto_suggest : \n 
~~~ results , suggestion = search ( title , results = 1 , suggestion = True ) \n 
~~~ title = suggestion or results [ 0 ] \n 
~~~ raise PageError ( title ) \n 
~~ ~~ return WikipediaPage ( title , redirect = redirect , preload = preload ) \n 
~~ elif pageid is not None : \n 
~~~ return WikipediaPage ( pageid = pageid , preload = preload ) \n 
~~ ~~ def section ( self , section_title ) : \n 
~~~ index = self . content . index ( section ) + len ( section ) \n 
~~~ next_index = self . content . index ( "==" , index ) \n 
~~~ next_index = len ( self . content ) \n 
~~ return self . content [ index : next_index ] . lstrip ( "=" ) . strip ( ) \n 
~~ def module_exists ( module_name ) : \n 
~~~ __import__ ( module_name ) \n 
~~ ~~ def install_jrnl ( config_path = ) : \n 
~~~ def autocomplete ( text , state ) : \n 
~~~ expansions = glob . glob ( os . path . expanduser ( os . path . expandvars ( text ) ) + ) \n 
expansions = [ e + "/" if os . path . isdir ( e ) else e for e in expansions ] \n 
expansions . append ( None ) \n 
return expansions [ state ] \n 
~~ readline . set_completer_delims ( ) \n 
readline . set_completer ( autocomplete ) \n 
path_query = \n 
journal_path = util . py23_input ( path_query ) . strip ( ) or os . path . expanduser ( ) \n 
default_config [ ] [ ] = os . path . expanduser ( os . path . expandvars ( journal_path ) ) \n 
if module_exists ( "Crypto" ) : \n 
if password : \n 
~~~ default_config [ ] = True \n 
~~~ util . set_keychain ( "default" , password ) \n 
~~~ util . set_keychain ( "default" , None ) \n 
~~~ password = None \n 
~~ path = os . path . split ( default_config [ ] [ ] ) [ 0 ] try : \n 
~~ if not os . path . isdir ( path ) : ~~~ open ( default_config [ ] [ ] , ) . close ( ) \n 
~~ with open ( config_path , ) as f : \n 
~~~ json . dump ( default_config , f , indent = 2 ) \n 
~~ config = default_config \n 
~~~ config [ ] = password \n 
~~ return config \n 
~~ def _fifo ( self , prefix ) : \n 
~~~ if os . access ( self . cf [ "%s.mkfifo.path" % prefix ] , os . F_OK | os . R_OK | os . W_OK ) is False : \n 
~~~ os . unlink ( self . cf [ "%s.mkfifo.path" % prefix ] ) \n 
self . logger . info ( % self . cf [ "%s.mkfifo.path" % prefix ] ) \n 
~~~ create_fifo ( self . cf [ "%s.mkfifo.path" % prefix ] , \n 
self . cf [ "%s.mkfifo.user.name" % prefix ] , \n 
self . cf [ "%s.mkfifo.group.name" % prefix ] , \n 
self . cf [ "%s.mkfifo.perms" % prefix ] , \n 
~~ except __HOLE__ , oe : \n 
~~~ self . logger . error ( ) \n 
raise oe \n 
~~ self . logger . info ( % self . cf [ "%s.mkfifo.path" % prefix ] ) \n 
~~ ~~ def load_tweets ( filename ) : \n 
~~~ archive = open ( filename , "r" ) \n 
~~~ return { } \n 
~~ tweets = { } \n 
for line in archive . readlines ( ) : \n 
tweets [ int ( tid ) ] = text . decode ( "utf-8" ) \n 
~~ ~~ archive . close ( ) \n 
return tweets \n 
~~ def save_tweets ( filename , tweets ) : \n 
if len ( tweets ) == 0 : \n 
~~~ archive = open ( filename , "w" ) \n 
~~ for k in sorted ( tweets . keys ( ) ) : \n 
~~ except Exception as ex : \n 
max_id = None \n 
fail = Fail ( ) \n 
~~~ portion = statuses_portion ( twitter , screen_name , max_id , mentions , favorites , received_dms ~~ except TwitterError as e : \n 
~~~ if e . e . code == 401 : \n 
% e . e . code ) \n 
~~ elif e . e . code == 429 : \n 
rls = twitter . application . rate_limit_status ( ) \n 
reset = rls . rate_limit_reset \n 
reset = _time . asctime ( _time . localtime ( reset ) ) \n 
delay = int ( rls . rate_limit_reset \n 
reset , delay ) ) \n 
fail . wait ( delay ) \n 
~~ elif e . e . code == 404 : \n 
~~ elif e . e . code == 502 : \n 
~~ fail . wait ( 3 ) \n 
~~ except urllib2 . URLError as e : \n 
fail . wait ( 3 ) \n 
~~ except httplib . error as e : \n 
~~~ new = - len ( tweets ) \n 
tweets . update ( portion ) \n 
new += len ( tweets ) \n 
% ( screen_name if screen_name else "home" , new ) ) \n 
if new < 190 : \n 
~~ ~~ ~~ def main ( args = sys . argv [ 1 : ] ) : \n 
~~~ options = { \n 
: False , \n 
: "." , \n 
: "" , \n 
: None , \n 
~~~ parse_args ( args , options ) \n 
~~ except GetoptError as e : \n 
raise SystemExit ( 1 ) \n 
~~ if not options [ ] and not ( options [ ] or \n 
options [ ] or \n 
options [ ] ) : \n 
~~~ print ( __doc__ ) \n 
~~ if options [ ] : \n 
~~~ oauth_filename = ( os . environ . get ( , \n 
os . environ . get ( , ) ) \n 
+ os . sep \n 
if not os . path . exists ( oauth_filename ) : \n 
~~~ oauth_dance ( "Twitter-Archiver" , CONSUMER_KEY , CONSUMER_SECRET , \n 
oauth_filename ) \n 
~~ oauth_token , oauth_token_secret = read_token_file ( oauth_filename ) \n 
auth = OAuth ( oauth_token , oauth_token_secret , CONSUMER_KEY , \n 
CONSUMER_SECRET ) \n 
~~~ auth = NoAuth ( ) \n 
~~ twitter = Twitter ( auth = auth , api_version = , domain = ) \n 
if options [ ] : \n 
~~~ rate_limit_status ( twitter ) \n 
~~ global format_text \n 
if options [ ] or options [ ] : \n 
~~~ if options [ ] : \n 
~~~ hosts = parse_host_list ( options [ ] ) \n 
~~~ hosts = None \n 
~~ format_text = functools . partial ( expand_format_text , hosts ) \n 
~~~ format_text = direct_format_text \n 
~~ if options [ ] or options [ ] : \n 
~~~ if isinstance ( auth , NoAuth ) : \n 
~~~ filename = options [ ] + os . sep + options [ ] \n 
~~ elif options [ ] : \n 
~~~ tweets = load_tweets ( filename ) \n 
% str ( e ) ) \n 
~~~ statuses ( twitter , "" , tweets , options [ ] , options [ ] , isoformat = options ~~ except KeyboardInterrupt : \n 
~~~ err ( ) \n 
err ( "Interrupted" ) \n 
~~ save_tweets ( filename , tweets ) \n 
~~ ~~ if options [ ] : \n 
~~ filename = options [ ] + os . sep + options [ ] \n 
dms = { } \n 
~~~ dms = load_tweets ( filename ) \n 
~~~ statuses ( twitter , "" , dms , received_dms = True , isoformat = options [ ] ) \n 
statuses ( twitter , "" , dms , received_dms = False , isoformat = options [ ] ) \n 
~~ save_tweets ( filename , dms ) \n 
~~ users = options [ ] \n 
if len ( users ) == 1 and users [ 0 ] == "-" : \n 
~~~ users = [ line . strip ( ) for line in sys . stdin . readlines ( ) ] \n 
~~ total , total_new = 0 , 0 \n 
for user in users : \n 
~~~ filename = options [ ] + os . sep + user \n 
~~~ filename = filename + "-favorites" \n 
tweets = { } \n 
~~ new = 0 \n 
before = len ( tweets ) \n 
~~~ statuses ( twitter , user , tweets , options [ ] , options [ ] , isoformat = options ~~ except __HOLE__ : \n 
total += len ( tweets ) \n 
new = len ( tweets ) - before \n 
total_new += new \n 
% ( total , total_new , len ( users ) ) ) \n 
~~ def tearDown ( self ) : \n 
~~~ super ( IntegrationTestCase , self ) . tearDown ( ) \n 
for pid , process in self . processes . items ( ) : \n 
~~~ process . kill ( ) \n 
~~ if self . print_stdout_stderr_on_teardown : \n 
~~~ stdout = process . stdout . read ( ) \n 
~~~ stdout = None \n 
~~~ stderr = process . stderr . read ( ) \n 
~~~ stderr = None \n 
print ( % ( stdout ) ) \n 
print ( % ( stderr ) ) \n 
~~ ~~ ~~ def hmmSegmentation ( wavFileName , hmmModelName , PLOT = False , gtFileName = "" ) : \n 
~~~ fo = open ( hmmModelName , "rb" ) \n 
~~~ hmm = cPickle . load ( fo ) \n 
classesAll = cPickle . load ( fo ) \n 
mtWin = cPickle . load ( fo ) \n 
mtStep = cPickle . load ( fo ) \n 
~~~ fo . close ( ) \n 
~~ fo . close ( ) \n 
if os . path . isfile ( gtFileName ) : \n 
~~~ [ segStart , segEnd , segLabels ] = readSegmentGT ( gtFileName ) \n 
flagsGT , classNamesGT = segs2flags ( segStart , segEnd , segLabels , mtStep ) \n 
flagsGTNew = [ ] \n 
~~~ if classNamesGT [ flagsGT [ j ] ] in classesAll : \n 
~~~ flagsGTNew . append ( classesAll . index ( classNamesGT [ flagsGT [ j ] ] ) ) \n 
~~~ flagsGTNew . append ( - 1 ) \n 
~~ ~~ flagsIndGT = numpy . array ( flagsGTNew ) \n 
~~~ flagsIndGT = numpy . array ( [ ] ) \n 
~~ acc = plotSegmentationResults ( flagsInd , flagsIndGT , classesAll , mtStep , not PLOT ) \n 
if acc >= 0 : \n 
~~ return flagsInd , classesAll , acc \n 
~~ def prompt_password ( self , host ) : \n 
~~~ self . value = self . _getpass ( \n 
% ( self . key , host ) ) \n 
~~ except ( EOFError , __HOLE__ ) : \n 
~~~ sys . stderr . write ( ) \n 
sys . exit ( 0 ) \n 
~~ ~~ def parse_items ( items , data = None , headers = None , files = None , params = None ) : \n 
if headers is None : \n 
~~~ headers = CaseInsensitiveDict ( ) \n 
~~ if data is None : \n 
~~~ data = OrderedDict ( ) \n 
~~ if files is None : \n 
~~~ files = OrderedDict ( ) \n 
~~ if params is None : \n 
~~~ params = ParamDict ( ) \n 
~~ for item in items : \n 
~~~ value = item . value \n 
key = item . key \n 
if item . sep == SEP_HEADERS : \n 
~~~ target = headers \n 
~~ elif item . sep == SEP_QUERY : \n 
~~~ target = params \n 
~~ elif item . sep == SEP_FILES : \n 
~~~ with open ( os . path . expanduser ( value ) , ) as f : \n 
~~~ value = ( os . path . basename ( value ) , \n 
BytesIO ( f . read ( ) ) ) \n 
~~~ raise ParseError ( \n 
~~ target = files \n 
~~ elif item . sep in [ SEP_DATA , SEP_DATA_RAW_JSON ] : \n 
~~~ if item . sep == SEP_DATA_RAW_JSON : \n 
~~~ value = json . loads ( item . value ) \n 
~~ ~~ target = data \n 
~~~ raise TypeError ( item ) \n 
~~ target [ key ] = value \n 
~~ return headers , data , files , params \n 
~~ def _parse_content ( self , text ) : \n 
~~~ return json . loads ( text ) \n 
~~~ raise exc . UnexpectedlyNotJSON ( \n 
~~ ~~ @ lib . api_call \n 
~~~ def read_loop ( self ) : \n 
~~~ t0 = time . time ( ) \n 
self . print_all ( ) \n 
~~ ~~ print "Done." \n 
~~ ~~ def choose_form ( self , number = None , id = None , name = None , xpath = None ) : \n 
if id is not None : \n 
~~~ self . _lxml_form = self . select ( \'//form[@id="%s"]\' % id ) . node ( ) \n 
~~ ~~ elif name is not None : \n 
~~~ self . _lxml_form = self . select ( \n 
\'//form[@name="%s"]\' % name ) . node ( ) \n 
~~~ raise DataNotFound ( % name ) \n 
~~ ~~ elif number is not None : \n 
~~~ self . _lxml_form = self . tree . forms [ number ] \n 
~~~ raise DataNotFound ( % number ) \n 
~~ ~~ elif xpath is not None : \n 
~~~ self . _lxml_form = self . select ( xpath ) . node ( ) \n 
~~~ raise DataNotFound ( \n 
% xpath ) \n 
~~~ raise GrabMisuseError ( \n 
~~ ~~ def save ( self , path , create_dirs = False ) : \n 
path_dir , path_fname = os . path . split ( path ) \n 
if not os . path . exists ( path_dir ) : \n 
~~~ os . makedirs ( path_dir ) \n 
~~ ~~ with open ( path , ) as out : \n 
~~~ out . write ( self . _bytes_body ) \n 
~~ ~~ def save_hash ( self , location , basedir , ext = None ) : \n 
if isinstance ( location , six . text_type ) : \n 
~~~ location = location . encode ( ) \n 
~~ rel_path = hashed_path ( location , ext = ext ) \n 
path = os . path . join ( basedir , rel_path ) \n 
if not os . path . exists ( path ) : \n 
~~~ path_dir , path_fname = os . path . split ( path ) \n 
~~ with open ( path , ) as out : \n 
~~ ~~ return rel_path \n 
~~ def post ( self , * args ) : \n 
~~~ if not self . api_key . can_create_user ( ) : \n 
~~~ raise HTTPError ( 403 ) \n 
~~~ username , password , user_type , key , extra = self . fetch_user ( ) \n 
~~~ raise HTTPError ( 400 ) \n 
~~~ self . auth_mgr . create_user ( username , password , user_type = user_type , \n 
key = key , extra = extra ) \n 
~~ except UserConflict : \n 
~~~ raise HTTPError ( 409 ) \n 
~~ self . write ( { "ok" : True } ) \n 
~~ def fetch_user ( self , update = False ) : \n 
~~~ obj = json . loads ( self . request . body . decode ( ) ) \n 
if not update : \n 
~~~ username = obj . pop ( ) \n 
~~ ~~ elif "username" in obj : \n 
~~~ del obj [ ] \n 
~~~ password = obj . pop ( ) \n 
~~ user_type = obj . pop ( , 1 ) \n 
if user_type == 0 and not self . api_key . is_admin ( ) : \n 
~~ key = obj . pop ( , None ) \n 
~~~ return username , password , user_type , key , obj \n 
~~ return password , user_type , key , obj \n 
~~ def put ( self , * args ) : \n 
~~~ if not self . api_key . can_create_user ( ) and self . key_username != args [ 0 ] : \n 
~~~ password , user_type , key , extra = self . fetch_user ( update = True ) \n 
~~~ self . auth_mgr . update_user ( args [ 0 ] , password , user_type = user_type , \n 
contents = f . read ( ) \n 
if sniff_limit is None : \n 
~~~ kwargs [ ] = sniff_dialect ( contents ) \n 
~~ elif sniff_limit > 0 : \n 
~~~ kwargs [ ] = sniff_dialect ( contents [ : sniff_limit ] ) \n 
~~ f = six . StringIO ( contents ) \n 
rows = agate . csv . reader ( f , ** kwargs ) \n 
~~~ if no_header_row : \n 
~~~ row = next ( rows ) \n 
rows = itertools . chain ( [ row ] , rows ) \n 
headers = make_default_headers ( len ( row ) ) \n 
~~~ headers = next ( rows ) \n 
~~ ~~ except StopIteration : \n 
~~~ headers = [ ] \n 
~~ if no_header_row or column_ids : \n 
~~~ column_ids = parse_column_identifiers ( column_ids , headers , column_offset ) \n 
headers = [ headers [ c ] for c in column_ids ] \n 
~~~ column_ids = range ( len ( headers ) ) \n 
~~ data_columns = [ [ ] for c in headers ] \n 
width = len ( data_columns ) \n 
for i , row in enumerate ( rows ) : \n 
~~~ j = 0 \n 
for j , d in enumerate ( row ) : \n 
~~~ data_columns [ j ] . append ( row [ column_ids [ j ] ] . strip ( ) ) \n 
~~ ~~ j += 1 \n 
while j < width : \n 
~~~ data_columns [ j ] . append ( None ) \n 
j += 1 \n 
~~ ~~ columns = [ ] \n 
for i , c in enumerate ( data_columns ) : \n 
~~~ columns . append ( Column ( column_ids [ i ] , headers [ i ] , c , blanks_as_nulls = blanks_as_nulls , infer_types \n 
~~ return Table ( columns , name = name ) \n 
~~ ~~ def init_django ( ) : \n 
global django , management , create_test_db , destroy_test_db \n 
global setup_test_environment , teardown_test_environment \n 
if not django : \n 
~~ from django . core import management \n 
project_dir = management . setup_environ ( settings ) \n 
sys . path . insert ( 0 , project_dir ) \n 
~~~ from django . test . utils import create_test_db , destroy_test_db \n 
~~~ from django . db import connection \n 
create_test_db = connection . creation . create_test_db \n 
destroy_test_db = connection . creation . destroy_test_db \n 
~~ from django . test . utils import setup_test_environment , teardown_test_environment \n 
~~~ import PIL \n 
~~ BaseTestCase . setUp ( self ) \n 
~~ def check_package ( self , package , package_dir ) : \n 
~~~ return self . packages_checked [ package ] \n 
~~ init_py = _build_py . check_package ( self , package , package_dir ) \n 
self . packages_checked [ package ] = init_py \n 
if not init_py or not self . distribution . namespace_packages : \n 
~~~ return init_py \n 
~~ for pkg in self . distribution . namespace_packages : \n 
~~~ if pkg == package or pkg . startswith ( package + ) : \n 
~~ f = open ( init_py , ) \n 
if not in f . read ( ) : \n 
~~~ from distutils import log \n 
log . warn ( \n 
"details.)\\n" , package \n 
~~ f . close ( ) \n 
return init_py \n 
~~ def __get_raw_model ( self , model_id ) : \n 
~~~ return deepcopy ( self . _db [ ] [ model_id ] ) \n 
~~~ raise backend_exceptions . ModelNotFound ( model_id ) \n 
~~ ~~ def __get_raw_records ( self , model_id ) : \n 
~~~ return self . _db [ ] [ model_id ] . values ( ) \n 
~~ ~~ def __get_raw_record ( self , model_id , record_id ) : \n 
~~~ return deepcopy ( self . _db [ ] [ model_id ] [ record_id ] ) \n 
~~~ raise backend_exceptions . RecordNotFound ( \n 
% ( model_id , record_id ) \n 
~~ ~~ def get_token ( self , credentials_id ) : \n 
~~~ return str ( self . _db [ ] [ credentials_id ] ) \n 
~~~ raise backend_exceptions . CredentialsNotFound ( credentials_id ) \n 
~~ ~~ def get_credentials_key ( self , credentials_id ) : \n 
~~ ~~ def _toaiff ( filename , temps ) : \n 
~~~ if filename [ - 2 : ] == : \n 
~~~ ( fd , fname ) = tempfile . mkstemp ( ) \n 
os . close ( fd ) \n 
temps . append ( fname ) \n 
sts = uncompress . copy ( filename , fname ) \n 
if sts : \n 
~~~ raise error , filename + \n 
~~~ fname = filename \n 
~~~ ftype = sndhdr . whathdr ( fname ) \n 
if ftype : \n 
~~~ ftype = ftype [ 0 ] \n 
~~ ~~ except __HOLE__ , msg : \n 
~~~ if type ( msg ) == type ( ( ) ) and len ( msg ) == 2 and type ( msg [ 0 ] ) == type ( 0 ) and type ( msg [ 1 ] ) == type ( ) : \n 
~~~ msg = msg [ 1 ] \n 
~~ if type ( msg ) != type ( ) : \n 
~~~ msg = repr ( msg ) \n 
~~ raise error , filename + + msg \n 
~~ if ftype == : \n 
~~~ return fname \n 
~~ if ftype is None or not ftype in table : \n 
~~~ raise error , % ( filename , ftype ) \n 
~~ ( fd , temp ) = tempfile . mkstemp ( ) \n 
temps . append ( temp ) \n 
sts = table [ ftype ] . copy ( fname , temp ) \n 
~~ return temp \n 
~~ def reload_playlist ( self ) : \n 
~~~ if self . closed : \n 
~~ self . reader . buffer . wait_free ( ) \n 
res = self . session . http . get ( self . stream . url , \n 
exception = StreamError , \n 
** self . reader . request_params ) \n 
~~~ playlist = hls_playlist . load ( res . text , res . url ) \n 
~~~ raise StreamError ( err ) \n 
~~ if playlist . is_master : \n 
~~ if playlist . iframes_only : \n 
~~ media_sequence = playlist . media_sequence or 0 \n 
sequences = [ Sequence ( media_sequence + i , s ) \n 
for i , s in enumerate ( playlist . segments ) ] \n 
if sequences : \n 
~~~ self . process_sequences ( playlist , sequences ) \n 
~~~ def parse_variant_playlist ( cls , session_ , url , name_key = "name" , \n 
name_prefix = "" , check_streams = False , \n 
** request_params ) : \n 
name_key = request_params . pop ( "namekey" , name_key ) \n 
name_prefix = request_params . pop ( "nameprefix" , name_prefix ) \n 
res = session_ . http . get ( url , exception = IOError , ** request_params ) \n 
~~~ parser = hls_playlist . load ( res . text , base_uri = res . url ) \n 
~~ streams = { } \n 
for playlist in filter ( lambda p : not p . is_iframe , parser . playlists ) : \n 
~~~ names = dict ( name = None , pixels = None , bitrate = None ) \n 
for media in playlist . media : \n 
~~~ if media . type == "VIDEO" and media . name : \n 
~~~ names [ "name" ] = media . name \n 
~~ ~~ if playlist . stream_info . resolution : \n 
~~~ width , height = playlist . stream_info . resolution \n 
names [ "pixels" ] = "{0}p" . format ( height ) \n 
~~ if playlist . stream_info . bandwidth : \n 
~~~ bw = playlist . stream_info . bandwidth \n 
if bw >= 1000 : \n 
~~~ names [ "bitrate" ] = "{0}k" . format ( int ( bw / 1000.0 ) ) \n 
~~~ names [ "bitrate" ] = "{0}k" . format ( bw / 1000.0 ) \n 
~~ ~~ stream_name = ( names . get ( name_key ) or names . get ( "name" ) or \n 
names . get ( "pixels" ) or names . get ( "bitrate" ) ) \n 
if not stream_name or stream_name in streams : \n 
~~ if check_streams : \n 
~~~ session_ . http . get ( playlist . uri , ** request_params ) \n 
~~ ~~ stream = HLSStream ( session_ , playlist . uri , ** request_params ) \n 
streams [ name_prefix + stream_name ] = stream \n 
~~ return streams \n 
~~ ~~ @ internationalizeDocstring \n 
~~~ def chr ( self , irc , msg , args , i ) : \n 
~~~ irc . reply ( chr ( i ) ) \n 
~~ ~~ ~~ @ internationalizeDocstring \n 
~~~ def decode ( self , irc , msg , args , encoding , text ) : \n 
if encoding in : \n 
~~~ encoding += \n 
~~ if encoding . endswith ( ) and encoding != : \n 
~~~ text = codecs . getdecoder ( ) ( text . encode ( ) ) [ 0 ] \n 
~~~ decoder = codecs . getdecoder ( encoding ) \n 
~~~ irc . errorInvalid ( _ ( ) , encoding ) \n 
~~ if minisix . PY3 and not isinstance ( text , bytes ) : \n 
~~~ text = text . encode ( ) \n 
~~~ text = decoder ( text ) [ 0 ] \n 
~~ except binascii . Error : \n 
~~~ irc . errorInvalid ( _ ( ) , \n 
s = _ ( \n 
~~ if minisix . PY2 and isinstance ( text , unicode ) : \n 
~~ elif minisix . PY3 and isinstance ( text , bytes ) : \n 
~~~ text = text . decode ( ) \n 
~~ ~~ irc . reply ( text ) \n 
~~ ~~ def label_for_value ( self , value ) : \n 
~~~ key = self . rel . get_related_field ( ) . name \n 
~~~ obj = self . rel . to . _default_manager . using ( self . db ) . get ( \n 
** { key : value } ) \n 
label = [ % escape ( \n 
shorten_string ( six . text_type ( obj ) ) ) ] \n 
image = admin_thumbnail ( obj ) \n 
if image : \n 
~~~ label . append ( \n 
% image ) \n 
~~ return . join ( label ) \n 
~~ except ( __HOLE__ , self . rel . to . DoesNotExist ) : \n 
~~ ~~ def get_value_from_conf ( conf_file , key ) : \n 
~~~ if not os . path . exists ( conf_file ) : \n 
~~~ return "" \n 
~~ if not os . path . isfile ( conf_file ) : \n 
~~~ with open ( conf_file , ) as f : \n 
~~~ while True : \n 
~~~ data = f . readline ( ) \n 
if not data : \n 
~~ if len ( data . split ( ) ) < 2 : \n 
~~ if data . strip ( ) [ 0 ] == "#" : \n 
~~ if data . split ( ) [ 0 ] . strip ( ) == key : \n 
~~~ return str ( data . split ( , 1 ) [ 1 ] . strip ( ) ) \n 
~~ ~~ def rollback_cmd ( self , args ) : \n 
~~~ app_name , deployment_name = self . parse_app_deployment_name ( args . name ) \n 
~~ except ParseAppDeploymentName : \n 
~~~ raise InputErrorException ( ) \n 
~~ if not deployment_name : \n 
~~~ deployment_name = \n 
~~ logEntries = [ ] \n 
~~~ logEntries = self . api . read_log ( \n 
app_name , \n 
deployment_name , \n 
last_time = None ) \n 
~~ except GoneError : \n 
~~ deployments = [ e [ ] . split ( ) [ 1 ] . strip ( ) for e in logEntries \n 
if in e [ ] ] \n 
~~~ current_deployment = deployments . pop ( ) \n 
~~ previous_deployment = None \n 
while not previous_deployment : \n 
~~~ deployment_version = deployments . pop ( ) \n 
if deployment_version != current_deployment : \n 
~~~ previous_deployment = deployment_version \n 
~~~ self . api . update_deployment ( \n 
version = previous_deployment , \n 
deployment_name = deployment_name ) \n 
~~ except ForbiddenError : \n 
~~ ~~ def _details ( self , app_or_deployment_name ) : \n 
~~~ app_name , deployment_name = self . parse_app_deployment_name ( app_or_deployment_name ) \n 
if deployment_name : \n 
~~~ deployment = self . api . read_deployment ( \n 
deployment_name ) \n 
~~~ app_users = self . api . read_app_users ( app_name ) \n 
~~ except ( UnauthorizedError , ForbiddenError , NotImplementedError ) : \n 
~~~ deployment [ ] = [ \n 
dict ( au , app = True ) \n 
for au in app_users \n 
] + deployment [ ] \n 
~~ ~~ except GoneError : \n 
~~~ return app_name , deployment_name , deployment \n 
~~~ app = self . api . read_app ( app_name ) \n 
if len ( app [ ] ) : \n 
~~~ for deployment in app [ ] : \n 
~~~ appname , depname = self . parse_app_deployment_name ( deployment [ ] ) \n 
depusers = self . api . read_deployment_users ( appname , depname ) \n 
app [ ] . extend ( \n 
dict ( du , deployment = depname ) \n 
for du in depusers \n 
~~ ~~ ~~ except GoneError : \n 
~~~ return app_name , deployment_name , app \n 
~~ ~~ ~~ def redeploy ( self , deployment ) : \n 
~~~ app_name , deployment_name = self . parse_app_deployment_name ( deployment [ ] ) \n 
self . api . update_deployment ( \n 
version = - 1 , \n 
deployment_name = deployment_name , \n 
min_boxes = deployment [ ] , \n 
max_boxes = deployment [ ] , \n 
stack = deployment [ ] [ ] ) \n 
~~ except ( __HOLE__ , GoneError , BadRequestError ) : \n 
~~ ~~ def _get_config_vars ( self , app_name , deployment_name ) : \n 
~~~ addon = self . api . read_addon ( app_name , deployment_name , CONFIG_ADDON ) \n 
return addon [ ] [ ] \n 
~~ except ( __HOLE__ , GoneError ) : \n 
~~ ~~ def addUser ( self , args ) : \n 
app_name , deployment_name = self . parse_app_deployment_name ( args . name ) \n 
if self . settings . prefix_project_name : \n 
~~~ if len ( args . email . split ( ) ) != 2 : \n 
~~~ prefix = self . api . read_users ( ) [ 0 ] [ ] . split ( ) [ 0 ] \n 
args . email = . format ( prefix , args . email ) \n 
~~~ if deployment_name : \n 
~~~ self . api . create_deployment_user ( app_name , deployment_name , args . email , args . role ) \n 
~~~ self . api . create_app_user ( app_name , args . email , args . role ) \n 
~~ ~~ except ConflictDuplicateError : \n 
~~ def removeUser ( self , args ) : \n 
if in args . username : \n 
~~~ users = self . api . read_deployment_users ( app_name , deployment_name ) \n 
~~~ users = self . api . read_app ( app_name ) [ ] \n 
~~~ username = [ user [ ] for user in users \n 
if user [ ] == args . username ] [ 0 ] \n 
~~~ username = args . username \n 
~~~ self . api . delete_deployment_user ( app_name , deployment_name , \n 
username ) \n 
~~~ self . api . delete_app_user ( app_name , username ) \n 
~~ except NotImplementedError : \n 
~~ def push ( self , args ) : \n 
if not check_installed_rcs ( ) and not check_installed_rcs ( ) : \n 
~~ if args . deploy and args . ship : \n 
~~ app_name , deployment_name = self . parse_app_deployment_name ( args . name ) \n 
deployment , push_deployment_name = self . _get_or_create_deployment ( app_name , deployment_name , \n 
cmd = self . _push_cmd ( deployment , push_deployment_name , args . source ) \n 
~~~ check_call ( cmd ) \n 
~~ except CalledProcessError , e : \n 
~~~ print str ( e ) \n 
~~ if args . deploy : \n 
self . redeploy ( deployment ) \n 
~~ if args . ship : \n 
~~~ self . log_from_now ( app_name , deployment_name , ) \n 
self . _open ( self . _get_deployment_url ( deployment ) ) \n 
~~ ~~ ~~ @ attr ( ) \n 
@ raises ( TransportException ) \n 
def test_httplib2_transport_exception ( ) : \n 
~~~ import httplib2 \n 
~~ from refreshbooks . transports . use_httplib2 import Transport \n 
Transport ( , dict ) ( "foo" ) \n 
~~ @ attr ( ) \n 
def test_httplib2 ( ) : \n 
assert len ( Transport ( , dict ) ( "foo" ) ) > 0 \n 
def test_requests_transport_exception ( ) : \n 
~~~ import requests \n 
~~ from refreshbooks . transports . use_requests import Transport \n 
def test_requests ( ) : \n 
~~ def application_id ( ) : \n 
~~~ from google . appengine . api import app_identity \n 
~~~ result = app_identity . get_application_id ( ) \n 
~~~ result = None \n 
~~ if not result : \n 
~~~ from google . appengine . api import appinfo \n 
info = appinfo . LoadSingleAppInfo ( open ( os . path . join ( find_project_root ( ) , "app.yaml" ) ) ) \n 
result = "dev~" + info . application \n 
os . environ [ ] = result \n 
result = app_identity . get_application_id ( ) \n 
~~ def appengine_on_path ( ) : \n 
~~~ from google . appengine . api import apiproxy_stub_map \n 
~~~ def _deserialize ( obj_dict ) : \n 
~~~ if not isinstance ( obj_dict , dict ) : \n 
~~~ return obj_dict \n 
~~ if not in obj_dict : \n 
~~~ model_cls = model . get_model ( obj_dict [ ] ) \n 
~~ if not issubclass ( model_cls , model . Model ) : \n 
~~ return model_cls . load ( obj_dict [ ] ) \n 
~~ ~~ def build_tuple ( op , args ) : \n 
~~~ return term ( op , args ) \n 
~~~ raise EarlyGoalError ( ) \n 
~~ ~~ def buildo ( op , args , obj ) : \n 
if not isvar ( obj ) : \n 
~~~ oop , oargs = op_args ( obj ) \n 
return lall ( ( eq , op , oop ) , ( eq , args , oargs ) ) \n 
~~~ return eq ( obj , build ( op , args ) ) \n 
~~ ~~ raise EarlyGoalError ( ) \n 
~~ def build ( op , args ) : \n 
~~ ~~ def op_args ( x ) : \n 
if isvar ( x ) : \n 
~~~ return operator ( x ) , arguments ( x ) \n 
~~ ~~ def eq_assoccomm ( u , v ) : \n 
~~~ uop , uargs = op_args ( u ) \n 
vop , vargs = op_args ( v ) \n 
~~~ return ( eq , u , v ) \n 
~~ if uop and not vop and not isvar ( v ) : \n 
~~~ return fail \n 
~~ if vop and not uop and not isvar ( u ) : \n 
~~ if uop and vop and not uop == vop : \n 
~~ if uop and not ( uop , ) in associative . facts : \n 
~~ if vop and not ( vop , ) in associative . facts : \n 
~~ if uop and vop : \n 
~~~ u , v = ( u , v ) if len ( uargs ) >= len ( vargs ) else ( v , u ) \n 
~~~ n = None \n 
~~ if vop and not uop : \n 
~~~ u , v = v , u \n 
~~ w = var ( ) \n 
return ( lall , ( eq_assoc , u , w , eq_assoccomm , n ) , \n 
( eq_comm , v , w , eq_assoccomm ) ) \n 
~~~ if self . started : \n 
~~ self . init ( ) \n 
env . start ( True , environ = { : self . isLocal } ) \n 
self . scheduler . start ( ) \n 
self . started = True \n 
atexit . register ( self . stop ) \n 
def handler ( signm , frame ) : \n 
self . scheduler . shutdown ( ) \n 
~~~ signal . signal ( signal . SIGTERM , handler ) \n 
signal . signal ( signal . SIGHUP , handler ) \n 
signal . signal ( signal . SIGABRT , handler ) \n 
signal . signal ( signal . SIGQUIT , handler ) \n 
~~ except : pass \n 
~~~ from rfoo . utils import rconsole \n 
rconsole . spawn_server ( locals ( ) , 0 ) \n 
~~ ~~ def fetch ( self , url , headers , body ) : \n 
~~~ current_url = url \n 
~~~ parsed = urlparse . urlparse ( current_url ) \n 
path = parsed [ 2 ] [ 1 : ] \n 
~~~ data = discoverdata . generateSample ( path , self . base_url ) \n 
~~~ return fetchers . HTTPResponse ( status = 404 , \n 
final_url = current_url , \n 
headers = { } , \n 
body = ) \n 
~~ response = mkResponse ( data ) \n 
if response . status in [ 301 , 302 , 303 , 307 ] : \n 
~~~ current_url = response . headers [ ] \n 
~~~ response . final_url = current_url \n 
return response \n 
~~ ~~ ~~ def shortDescription ( self ) : \n 
~~~ n = self . input_url \n 
~~~ n = self . input_name \n 
n , \n 
self . __class__ . __module__ ) \n 
~~ def testHelp ( self ) : \n 
reload ( config ) \n 
with mock . patch ( , [ , ] ) : \n 
~~~ config . init ( ) \n 
~~ ~~ ~~ def testDirectoryNoCreation ( self ) : \n 
with mock . patch ( , [ , , self . tmpdir + "abc" , "--config_no_create_dir" ~~~ try : \n 
~~ ~~ ~~ def testInvalidConfigValue ( self ) : \n 
~~~ config . get_value ( "testing" ) \n 
~~ ~~ def testInvalidConfigurationFileLoad ( self ) : \n 
config . set_value ( "server_address" , "ipc://wat" ) \n 
with open ( os . path . join ( config . _cdirs [ "config" ] , config . CONFIG_FILENAME ) , "w" ) as f : \n 
~~ reload ( config ) \n 
with mock . patch ( , [ , , self . tmpdir ] ) : \n 
~~ ~~ ~~ def testInvalidConfigurationKeyLoad ( self ) : \n 
~~~ json . dump ( { "server_addres" : "tcp://127.0.0.1:9389" } , f ) \n 
~~ ~~ ~~ def goto ( self , goto ) : \n 
~~~ what , which , where = self . parse_goto ( goto ) \n 
if what == "section" : \n 
~~~ self . move_to_section ( which , where ) \n 
~~ elif what == "item" : \n 
~~~ self . move_to_item ( which , where ) \n 
~~ elif what == "file" : \n 
~~~ self . move_to_file ( which , where ) \n 
~~ elif what == "stash" : \n 
~~~ self . move_to_stash ( which , where ) \n 
~~ elif what == "point" : \n 
~~~ point = int ( which ) \n 
self . move_to_point ( point ) \n 
~~ ~~ ~~ def parse_goto ( self , goto ) : \n 
~~~ what , which , where = None , None , None \n 
parts = goto . split ( ) \n 
what = parts [ 0 ] \n 
if len ( parts ) > 1 : \n 
~~~ which = int ( parts [ 1 ] ) \n 
~~~ which = parts [ 1 ] \n 
~~ ~~ if len ( parts ) > 2 : \n 
~~~ where = int ( parts [ 2 ] ) \n 
~~~ where = parts [ 2 ] \n 
~~ ~~ return ( what , which , where ) \n 
~~ def parse_next_message ( self ) : \n 
~~~ parsed_message = None \n 
remainder = self . message_buffer \n 
message = "" \n 
if self . SERIALIZED_COMMAND_TERMINATOR in self . message_buffer : \n 
~~~ message , _ , remainder = self . message_buffer . partition ( \n 
self . SERIALIZED_COMMAND_TERMINATOR ) \n 
~~~ parsed_message = JsonFormatter . deserialize ( message ) \n 
if not isinstance ( parsed_message , dict ) : \n 
~~ ~~ self . message_buffer = remainder \n 
return parsed_message \n 
~~ def _listen_for_dweets_from_response ( response ) : \n 
streambuffer = \n 
for byte in response . iter_content ( ) : \n 
~~~ if byte : \n 
~~~ streambuffer += byte . decode ( ) \n 
~~~ dweet = json . loads ( streambuffer . splitlines ( ) [ 1 ] ) \n 
~~ if isstr ( dweet ) : \n 
~~~ yield json . loads ( dweet ) \n 
~~ streambuffer = \n 
~~ ~~ ~~ def getDescription ( self , longname ) : \n 
if longname in self . descriptions : \n 
~~~ return self . descriptions [ longname ] \n 
~~~ descr = self . flagNameToDefinition [ longname ] [ 1 ] \n 
~~~ descr = self . paramNameToDefinition [ longname ] [ 2 ] \n 
~~~ descr = None \n 
~~ ~~ if descr is not None : \n 
~~~ return descr \n 
obj = getattr ( self . options , % longMangled , None ) \n 
if obj is not None : \n 
~~~ descr = descrFromDoc ( obj ) \n 
if descr is not None : \n 
~~ def get_relations ( cursor , table_name ) : \n 
relations = { } \n 
for row in cursor . fetchall ( ) : \n 
~~~ relations [ int ( row [ 0 ] [ 1 : - 1 ] ) - 1 ] = ( int ( row [ 1 ] [ 1 : - 1 ] ) - 1 , row [ 2 ] ) \n 
~~ ~~ return relations \n 
~~ def info ( name ) : \n 
~~~ data = pwd . getpwnam ( name ) \n 
~~~ return _format_info ( data ) \n 
~~ ~~ def is_toolbelt_installed ( \n 
default_command = [ "heroku" , "--version" ] , \n 
default_test_string = "heroku-toolbelt" \n 
~~~ p = subprocess . Popen ( \n 
default_command , \n 
stderr = subprocess . PIPE \n 
~~ version_info = p . stdout . readlines ( ) \n 
p . kill ( ) \n 
if len ( version_info ) : \n 
~~~ return version_info [ 0 ] . startswith ( default_test_string ) \n 
~~ def update_editor ( self ) : \n 
font = self . factory . to_qt4_font ( self ) \n 
self . _bold = font . bold ( ) \n 
self . _italic = font . italic ( ) \n 
self . _facename . setCurrentFont ( font ) \n 
~~~ idx = PointSizes . index ( str ( font . pointSize ( ) ) ) \n 
~~~ idx = PointSizes . index ( ) \n 
~~ self . _point_size . setCurrentIndex ( idx ) \n 
~~ def parse_token_response ( body , scope = None ) : \n 
~~~ params = json . loads ( body ) \n 
~~~ params = dict ( urlparse . parse_qsl ( body ) ) \n 
for key in ( , ) : \n 
~~~ params [ key ] = int ( params [ key ] ) \n 
~~ ~~ ~~ if in params : \n 
~~~ params [ ] = scope_to_list ( params [ ] ) \n 
~~ if in params : \n 
~~~ params [ ] = params . pop ( ) \n 
~~~ params [ ] = time . time ( ) + int ( params [ ] ) \n 
~~ params = OAuth2Token ( params , old_scope = scope ) \n 
validate_token_parameters ( params ) \n 
return params \n 
~~ def _to_node ( self , compute ) : \n 
~~~ state = self . NODE_STATE_MAP [ compute . findtext ( ) . upper ( ) ] \n 
~~~ state = NodeState . UNKNOWN \n 
~~ return Node ( id = compute . findtext ( ) , \n 
name = compute . findtext ( ) , \n 
state = state , \n 
public_ips = self . _extract_networks ( compute ) , \n 
private_ips = [ ] , \n 
driver = self . connection . driver , \n 
image = self . _extract_images ( compute ) ) \n 
image = self . _extract_images ( compute ) , \n 
size = self . _extract_size ( compute ) , \n 
extra = { : self . _extract_context ( compute ) } ) \n 
~~ def _extract_size ( self , compute ) : \n 
instance_type = compute . find ( ) \n 
~~~ return next ( ( node_size for node_size in self . list_sizes ( ) \n 
if node_size . name == instance_type . text ) ) \n 
~~ ~~ def get_ipdir ( ) : \n 
~~~ ipdir = sys . argv [ 1 ] \n 
~~~ ipdir = \n 
~~ ipdir = os . path . abspath ( ipdir ) \n 
cd ( ipdir ) \n 
if not os . path . isdir ( ) and os . path . isfile ( ) : \n 
~~~ raise SystemExit ( % ipdir ) \n 
~~ return ipdir \n 
~~ def wake ( self ) : \n 
~~~ self . writer . write ( b"x" ) \n 
~~ ~~ def consume ( self ) : \n 
~~~ result = self . reader . read ( ) \n 
if not result : \n 
~~ ~~ def _read_status ( self ) : \n 
~~~ line = str ( self . fp . readline ( _MAXLINE + 1 ) , "iso-8859-1" ) \n 
if len ( line ) > _MAXLINE : \n 
~~ if self . debuglevel > 0 : \n 
~~~ print ( "reply:" , repr ( line ) ) \n 
~~ if not line : \n 
~~~ version , status , reason = line . split ( None , 2 ) \n 
~~~ version , status = line . split ( None , 1 ) \n 
~~~ version = "" \n 
~~ ~~ if not version . startswith ( "HTTP/" ) : \n 
~~~ self . _close_conn ( ) \n 
raise BadStatusLine ( line ) \n 
~~ return version , status , reason \n 
~~ def begin ( self ) : \n 
~~~ if self . headers is not None : \n 
~~~ version , status , reason = self . _read_status ( ) \n 
if status != CONTINUE : \n 
~~~ skip = self . fp . readline ( _MAXLINE + 1 ) \n 
if len ( skip ) > _MAXLINE : \n 
~~ skip = skip . strip ( ) \n 
if not skip : \n 
~~~ print ( "header:" , skip ) \n 
~~ ~~ ~~ self . code = self . status = status \n 
self . reason = reason . strip ( ) \n 
if version in ( "HTTP/1.0" , "HTTP/0.9" ) : \n 
~~~ self . version = 10 \n 
~~ elif version . startswith ( "HTTP/1." ) : \n 
~~~ raise UnknownProtocol ( version ) \n 
~~ self . headers = self . msg = parse_headers ( self . fp ) \n 
if self . debuglevel > 0 : \n 
~~~ for hdr in self . headers : \n 
~~ ~~ tr_enc = self . headers . get ( "transfer-encoding" ) \n 
if tr_enc and tr_enc . lower ( ) == "chunked" : \n 
~~~ self . chunked = True \n 
self . chunk_left = None \n 
~~~ self . chunked = False \n 
~~ self . will_close = self . _check_close ( ) \n 
self . length = None \n 
length = self . headers . get ( "content-length" ) \n 
tr_enc = self . headers . get ( "transfer-encoding" ) \n 
if length and not self . chunked : \n 
~~~ self . length = int ( length ) \n 
~~~ self . length = None \n 
~~ if ( status == NO_CONTENT or status == NOT_MODIFIED or \n 
self . _method == "HEAD" ) : \n 
~~~ self . length = 0 \n 
~~ if ( not self . will_close and \n 
not self . chunked and \n 
self . length is None ) : \n 
~~~ self . will_close = True \n 
~~ ~~ def _read_next_chunk_size ( self ) : \n 
~~~ line = self . fp . readline ( _MAXLINE + 1 ) \n 
~~ i = line . find ( b";" ) \n 
if i >= 0 : \n 
~~~ return int ( line , 16 ) \n 
~~ ~~ def _readall_chunked ( self ) : \n 
~~~ assert self . chunked != _UNKNOWN \n 
chunk_left = self . chunk_left \n 
value = [ ] \n 
~~~ if chunk_left is None : \n 
~~~ chunk_left = self . _read_next_chunk_size ( ) \n 
if chunk_left == 0 : \n 
~~~ raise IncompleteRead ( bytes ( ) . join ( value ) ) \n 
~~ ~~ value . append ( self . _safe_read ( chunk_left ) ) \n 
chunk_left = None \n 
~~ self . _read_and_discard_trailer ( ) \n 
self . _close_conn ( ) \n 
return bytes ( ) . join ( value ) \n 
~~ def _readinto_chunked ( self , b ) : \n 
total_bytes = 0 \n 
mvb = memoryview ( b ) \n 
~~~ raise IncompleteRead ( bytes ( b [ 0 : total_bytes ] ) ) \n 
~~ ~~ if len ( mvb ) < chunk_left : \n 
~~~ n = self . _safe_readinto ( mvb ) \n 
self . chunk_left = chunk_left - n \n 
return total_bytes + n \n 
~~ elif len ( mvb ) == chunk_left : \n 
~~~ temp_mvb = mvb [ 0 : chunk_left ] \n 
n = self . _safe_readinto ( temp_mvb ) \n 
mvb = mvb [ n : ] \n 
total_bytes += n \n 
return total_bytes \n 
~~ def _set_hostport ( self , host , port ) : \n 
~~~ if port is None : \n 
~~~ i = host . rfind ( ) \n 
if i > j : \n 
~~~ port = int ( host [ i + 1 : ] ) \n 
~~~ port = self . default_port \n 
~~ ~~ host = host [ : i ] \n 
~~ if host and host [ 0 ] == and host [ - 1 ] == : \n 
~~~ host = host [ 1 : - 1 ] \n 
~~ ~~ self . host = host \n 
self . port = port \n 
~~ def send ( self , data ) : \n 
if self . sock is None : \n 
~~~ if self . auto_open : \n 
~~~ self . connect ( ) \n 
~~~ raise NotConnected ( ) \n 
~~ ~~ if self . debuglevel > 0 : \n 
~~~ print ( "send:" , repr ( data ) ) \n 
~~ blocksize = 8192 \n 
if hasattr ( data , "read" ) and not isinstance ( data , array ) : \n 
~~~ if self . debuglevel > 0 : \n 
~~ encode = False \n 
~~~ mode = data . mode \n 
~~~ if "b" not in mode : \n 
~~~ encode = True \n 
~~ ~~ ~~ while 1 : \n 
~~~ datablock = data . read ( blocksize ) \n 
if not datablock : \n 
~~ if encode : \n 
~~~ datablock = datablock . encode ( "iso-8859-1" ) \n 
~~ self . sock . sendall ( datablock ) \n 
~~~ self . sock . sendall ( data ) \n 
~~~ if isinstance ( data , collections . Iterable ) : \n 
~~~ for d in data : \n 
~~~ self . sock . sendall ( d ) \n 
~~ ~~ ~~ def _set_content_length ( self , body ) : \n 
~~~ thelen = None \n 
~~~ thelen = str ( len ( body ) ) \n 
~~ except TypeError as te : \n 
~~~ thelen = str ( os . fstat ( body . fileno ( ) ) . st_size ) \n 
~~ ~~ if thelen is not None : \n 
~~~ self . putheader ( , thelen ) \n 
~~ ~~ def migrate_app ( migrations , target_name = None , merge = False , fake = False , db_dry_run = False , yes = False , ~~~ app_label = migrations . app_label ( ) \n 
verbosity = int ( verbosity ) \n 
pre_migrate . send ( None , app = app_label , verbosity = verbosity , interactive = verbosity , db = database ) \n 
if not migrations : \n 
~~ Migrations . calculate_dependencies ( ) \n 
applied_all = MigrationHistory . objects . filter ( applied__isnull = False ) . order_by ( ) . using ( database applied = applied_all . filter ( app_name = app_label ) . using ( database ) \n 
south . db . db = south . db . dbs [ database ] \n 
Migrations . invalidate_all_modules ( ) \n 
south . db . db . debug = ( verbosity > 1 ) \n 
if target_name == : \n 
~~~ if applied . count ( ) > 1 : \n 
~~~ previous_migration = applied [ applied . count ( ) - 2 ] \n 
if verbosity : \n 
~~~ print ( % ( previous_migration . migration , previous_migration ~~ target_name = previous_migration . migration \n 
~~~ if verbosity : \n 
~~ target_name = \n 
~~ ~~ elif target_name == : \n 
~~~ first_unapplied_migration = get_unapplied_migrations ( migrations , applied ) . next ( ) \n 
target_name = first_unapplied_migration . name ( ) \n 
~~~ target_name = None \n 
~~ ~~ applied_all = check_migration_histories ( applied_all , delete_ghosts , ignore_ghosts ) \n 
target = migrations . guess_migration ( target_name ) \n 
~~~ if target_name not in ( , None ) and target . name ( ) != target_name : \n 
target . name ( ) ) ) \n 
~~ direction , problems , workplan = get_direction ( target , applied_all , migrations , \n 
verbosity , interactive ) \n 
if problems and not ( merge or skip ) : \n 
~~~ raise exceptions . InconsistentMigrationHistory ( problems ) \n 
~~ migrator = get_migrator ( direction , db_dry_run , fake , load_initial_data ) \n 
if migrator : \n 
~~~ migrator . print_title ( target ) \n 
success = migrator . migrate_many ( target , workplan , database ) \n 
if success : \n 
~~~ post_migrate . send ( None , app = app_label , verbosity = verbosity , interactive = verbosity , db = database ~~ ~~ else : \n 
~~ if load_initial_data : \n 
~~~ migrator = LoadInitialDataMigrator ( migrator = Forwards ( verbosity = verbosity ) ) \n 
migrator . load_initial_data ( target , db = database ) \n 
~~ post_migrate . send ( None , app = app_label , verbosity = verbosity , interactive = verbosity , db = database ~~ ~~ def run ( root_coro ) : \n 
threads = { root_coro : ValueEvent ( None ) } \n 
delegators = { } \n 
joiners = collections . defaultdict ( list ) \n 
def complete_thread ( coro , return_value ) : \n 
del threads [ coro ] \n 
if coro in delegators : \n 
~~~ threads [ delegators [ coro ] ] = ValueEvent ( return_value ) \n 
del delegators [ coro ] \n 
~~ if coro in joiners : \n 
~~~ for parent in joiners [ coro ] : \n 
~~~ threads [ parent ] = ValueEvent ( None ) \n 
~~ del joiners [ coro ] \n 
~~ ~~ def advance_thread ( coro , value , is_exc = False ) : \n 
~~~ if is_exc : \n 
~~~ next_event = coro . throw ( * value ) \n 
~~~ next_event = coro . send ( value ) \n 
~~~ complete_thread ( coro , None ) \n 
~~~ del threads [ coro ] \n 
raise ThreadException ( coro , sys . exc_info ( ) ) \n 
~~~ if isinstance ( next_event , types . GeneratorType ) : \n 
~~~ next_event = DelegationEvent ( next_event ) \n 
~~ threads [ coro ] = next_event \n 
~~ ~~ def kill_thread ( coro ) : \n 
coros = [ coro ] \n 
while isinstance ( threads [ coro ] , Delegated ) : \n 
~~~ coro = threads [ coro ] . child \n 
coros . append ( coro ) \n 
~~ for coro in reversed ( coros ) : \n 
~~ ~~ exit_te = None \n 
while threads : \n 
~~~ have_ready = False \n 
for coro , event in list ( threads . items ( ) ) : \n 
~~~ if isinstance ( event , SpawnEvent ) : \n 
advance_thread ( coro , None ) \n 
have_ready = True \n 
~~ elif isinstance ( event , ValueEvent ) : \n 
~~~ advance_thread ( coro , event . value ) \n 
~~ elif isinstance ( event , ExceptionEvent ) : \n 
~~~ advance_thread ( coro , event . exc_info , True ) \n 
~~ elif isinstance ( event , DelegationEvent ) : \n 
delegators [ event . spawned ] = coro \n 
~~ elif isinstance ( event , ReturnEvent ) : \n 
~~~ complete_thread ( coro , event . value ) \n 
~~ elif isinstance ( event , JoinEvent ) : \n 
joiners [ event . child ] . append ( coro ) \n 
~~ elif isinstance ( event , KillEvent ) : \n 
~~~ threads [ coro ] = ValueEvent ( None ) \n 
kill_thread ( event . child ) \n 
~~ ~~ if not have_ready : \n 
~~ ~~ event2coro = dict ( ( v , k ) for k , v in threads . items ( ) ) \n 
for event in _event_select ( threads . values ( ) ) : \n 
~~~ value = event . fire ( ) \n 
~~ except socket . error as exc : \n 
~~~ if isinstance ( exc . args , tuple ) and exc . args [ 0 ] == errno . EPIPE : \n 
~~~ traceback . print_exc ( ) \n 
~~ threads [ event2coro [ event ] ] = ReturnEvent ( None ) \n 
~~~ advance_thread ( event2coro [ event ] , value ) \n 
~~ ~~ ~~ except ThreadException as te : \n 
~~~ event = ExceptionEvent ( te . exc_info ) \n 
if te . coro in delegators : \n 
~~~ threads [ delegators [ te . coro ] ] = event \n 
del delegators [ te . coro ] \n 
~~~ exit_te = te \n 
~~~ threads = { root_coro : ExceptionEvent ( sys . exc_info ( ) ) } \n 
~~ ~~ for coro in threads : \n 
~~~ coro . close ( ) \n 
~~ if exit_te : \n 
~~~ exit_te . reraise ( ) \n 
~~ ~~ def server ( host , port , func ) : \n 
def handler ( conn ) : \n 
~~~ yield func ( conn ) \n 
~~~ conn . close ( ) \n 
~~ ~~ listener = Listener ( host , port ) \n 
~~~ conn = yield listener . accept ( ) \n 
yield spawn ( handler ( conn ) ) \n 
~~~ listener . close ( ) \n 
~~ ~~ def __init__ ( self , host = , port = 11211 , ** kwargs ) : \n 
~~~ import pylibmc \n 
self . _make_connection = lambda : pylibmc . Client ( [ % ( host , port ) ] , behaviors = { "tcp_nodelay" ~~ def status ( self ) : \n 
~~~ p = subprocess . Popen ( [ self . supervisorctl_path , "status" ] , \n 
stdout = subprocess . PIPE , stderr = subprocess . STDOUT ) \n 
out = p . communicate ( ) [ 0 ] \n 
for l in out . split ( ) : \n 
~~~ svc , status , info = whitespace_re . split ( l . strip ( ) , 2 ) \n 
service , process_name = svc . split ( ) \n 
~~ if service == self . resource . service_name : \n 
~~~ return status . strip ( ) == "RUNNING" \n 
~~ def test_encode_latin1_long ( self ) : \n 
sm = binascii . a2b_hex ( self . latin1_long_sm ) \n 
pdu = self . buildSubmitSmTest ( sm ) \n 
self . assertTrue ( isinstance ( pdu . nextPdu , SubmitSM ) ) \n 
self . assertTrue ( pdu . params [ ] > 0 ) \n 
partedSmPdu = pdu \n 
assembledSm = \n 
lastSeqNum = 0 \n 
~~~ assembledSm += partedSmPdu . params [ ] \n 
self . assertTrue ( partedSmPdu . params [ ] == pdu . params [ ] ) \n 
self . assertTrue ( partedSmPdu . params [ ] == pdu . params [ self . assertTrue ( partedSmPdu . params [ ] > lastSeqNum ) \n 
lastSeqNum = partedSmPdu . params [ ] \n 
~~~ partedSmPdu = partedSmPdu . nextPdu \n 
~~ ~~ self . assertEquals ( assembledSm , sm ) \n 
self . assertEquals ( lastSeqNum , pdu . params [ ] ) \n 
~~ def check ( self , agentConfig ) : \n 
~~~ self . logger . debug ( ) \n 
plugin_directory = agentConfig . get ( , None ) \n 
if plugin_directory : \n 
~~~ self . logger . info ( \n 
, plugin_directory ) \n 
if not os . access ( plugin_directory , os . R_OK ) : \n 
~~ if self . plugins is None : \n 
~~~ self . logger . debug ( \n 
sys . path . append ( plugin_directory ) \n 
self . plugins = [ ] \n 
plugins = [ ] \n 
for root , dirs , files in os . walk ( plugin_directory ) : \n 
~~~ for name in files : \n 
~~~ self . logger . debug ( , name ) \n 
name = name . split ( , 1 ) \n 
~~~ if name [ 1 ] == : \n 
+ name [ 0 ] + + name [ 1 ] + \n 
plugins . append ( name [ 0 ] ) \n 
~~ ~~ ~~ for plugin_name in plugins : \n 
~~~ self . logger . debug ( , plugin_name ) \n 
plugin_path = os . path . join ( \n 
plugin_directory , % plugin_name ) \n 
if not os . access ( plugin_path , os . R_OK ) : \n 
, plugin_path ) \n 
~~~ import imp \n 
imported_plugin = imp . load_source ( plugin_name , plugin_path ) \n 
self . logger . debug ( , plugin_name ) \n 
plugin_class = getattr ( imported_plugin , plugin_name , None ) \n 
if plugin_class is None : \n 
, plugin_name , plugin_path ) \n 
~~~ plugin_obj = plugin_class ( \n 
agentConfig , self . logger , self . raw_config ) \n 
agentConfig , self . logger ) \n 
~~~ plugin_obj = plugin_class ( ) \n 
~~ ~~ self . logger . debug ( , plugin_name ) \n 
self . plugins . append ( plugin_obj ) \n 
, plugin_name , \n 
traceback . format_exc ( ) ) \n 
~~ ~~ ~~ if self . plugins is not None : \n 
output = { } \n 
for plugin in self . plugins : \n 
, plugin . __class__ . __name__ ) \n 
~~~ value = plugin . run ( ) \n 
~~~ output [ plugin . __class__ . __name__ ] = value \n 
self . logger . debug ( \n 
plugin . __class__ . __name__ , \n 
output [ plugin . __class__ . __name__ ] ) \n 
self . logger . info ( \n 
plugin . __class__ . __name__ ) \n 
~~ ~~ except Exception : \n 
, traceback . format_exc ( ) ) \n 
~~ ~~ self . logger . debug ( ) \n 
return output \n 
~~ ~~ def registered_modeladmin ( model_cls ) : \n 
~~~ model_admin_cls = type ( admin . site . _registry [ model_cls ] ) \n 
model_admin_cls . _model_cls = model_cls \n 
return model_admin_cls \n 
~~~ raise ImproperlyConfigured ( \n 
~~ ~~ def add_bidirectional_m2m ( form_cls ) : \n 
~~~ class BidirectionalM2MForm ( form_cls ) : \n 
~~~ def _get_bidirectional_m2m_fields ( self ) : \n 
~~~ return super ( BidirectionalM2MForm , self ) . _get_bidirectional_m2m_fields ( ) \n 
~~~ super ( BidirectionalM2MForm , self ) . __init__ ( * args , ** kwargs ) \n 
if self . instance . pk is not None : \n 
~~~ for m2m_field , related_manager in self . _get_bidirectional_m2m_fields ( ) : \n 
~~~ self . fields [ m2m_field ] . initial = getattr ( \n 
self . instance , related_manager ) . all ( ) \n 
~~ ~~ ~~ def save ( self , commit = True ) : \n 
instance = super ( BidirectionalM2MForm , self ) . save ( commit = False ) \n 
force_save = self . instance . pk is None \n 
if force_save : \n 
~~~ instance . save ( ) \n 
~~ for m2m_field , related_manager in self . _get_bidirectional_m2m_fields ( ) : \n 
~~~ setattr ( self . instance , related_manager , self . cleaned_data [ m2m_field ] ) \n 
~~ if commit : \n 
~~~ if not force_save : \n 
~~ self . save_m2m ( ) \n 
~~ return instance \n 
~~ ~~ return BidirectionalM2MForm \n 
~~ def wrap_exceptions ( callable ) : \n 
def wrapper ( self , * args , ** kwargs ) : \n 
~~~ return callable ( self , * args , ** kwargs ) \n 
~~~ err = sys . exc_info ( ) [ 1 ] \n 
if err . errno == errno . ESRCH : \n 
~~~ raise NoSuchProcess ( self . pid , self . _process_name ) \n 
~~ if err . errno in ( errno . EPERM , errno . EACCES ) : \n 
~~~ raise AccessDenied ( self . pid , self . _process_name ) \n 
~~ ~~ return wrapper \n 
~~ @ wrap_exceptions \n 
~~~ def get_process_terminal ( self ) : \n 
~~~ tty_nr = _psutil_osx . get_process_tty_nr ( self . pid ) \n 
~~~ return _TERMINAL_MAP [ tty_nr ] \n 
~~ ~~ ~~ def check_own_version ( log ) : \n 
~~~ if not DEV_MODE : \n 
~~~ own_dir = get_own_dir ( __file__ ) \n 
version_path = os . path . join ( own_dir , ) \n 
restart_script_path = os . path . join ( own_dir , ) \n 
~~~ gevent . sleep ( DAEMON_VERSION_CHECKER_PERIOD ) \n 
with open ( version_path ) as f : \n 
~~~ current_version = f . read ( ) . strip ( ) \n 
~~ if current_version != VERSION : \n 
~~~ log . info ( Popen ( restart_script_path , close_fds = True ) \n 
gevent . sleep ( 10 ) \n 
log . error ( ) \n 
Events . shutdown_required . set ( ) \n 
~~~ log . exception ( ) \n 
~~ ~~ ~~ def getProb ( meshType = , rxTypes = , nSrc = 1 ) : \n 
~~~ cs = 5. \n 
ncx = 20 \n 
ncy = 6 \n 
npad = 20 \n 
hx = [ ( cs , ncx ) , ( cs , npad , 1.3 ) ] \n 
hy = [ ( cs , npad , - 1.3 ) , ( cs , ncy ) , ( cs , npad , 1.3 ) ] \n 
mesh = Mesh . CylMesh ( [ hx , 1 , hy ] , ) \n 
active = mesh . vectorCCz < 0. \n 
activeMap = Maps . InjectActiveCells ( mesh , active , np . log ( 1e-8 ) , nC = mesh . nCz ) \n 
mapping = Maps . ExpMap ( mesh ) * Maps . SurjectVertical1D ( mesh ) * activeMap \n 
rxOffset = 40. \n 
srcs = [ ] \n 
for ii in range ( nSrc ) : \n 
~~~ rxs = [ EM . TDEM . RxTDEM ( np . array ( [ [ rxOffset , 0. , 0. ] ] ) , np . logspace ( - 4 , - 3 , 20 + ii ) , rxType ) for srcs += [ EM . TDEM . SrcTDEM_VMD_MVP ( rxs , np . array ( [ 0. , 0. , 0. ] ) ) ] \n 
~~ survey = EM . TDEM . SurveyTDEM ( srcs ) \n 
prb = EM . TDEM . ProblemTDEM_b ( mesh , mapping = mapping ) \n 
prb . timeSteps = [ ( 1e-05 , 10 ) , ( 5e-05 , 10 ) , ( 2.5e-4 , 10 ) ] \n 
~~~ from pymatsolver import MumpsSolver \n 
prb . Solver = MumpsSolver \n 
~~~ prb . Solver = SolverLU \n 
~~ sigma = np . ones ( mesh . nCz ) * 1e-8 \n 
sigma [ mesh . vectorCCz < 0 ] = 1e-1 \n 
sigma = np . log ( sigma [ active ] ) \n 
prb . pair ( survey ) \n 
return prb , mesh , sigma \n 
~~ def parseOptions ( self , options = None ) : \n 
if options is None : \n 
~~~ options = sys . argv [ 1 : ] \n 
~~~ opts , args = getopt . getopt ( options , self . shortOpt , self . longOpt ) \n 
~~ except getopt . error as e : \n 
~~~ raise usage . UsageError ( str ( e ) ) \n 
~~ for opt , arg in opts : \n 
~~~ if opt [ 1 ] == : \n 
~~~ opt = opt [ 2 : ] \n 
~~~ opt = opt [ 1 : ] \n 
~~ optMangled = opt \n 
if optMangled not in self . synonyms : \n 
~~~ optMangled = opt . replace ( "-" , "_" ) \n 
~~ ~~ optMangled = self . synonyms [ optMangled ] \n 
if isinstance ( self . _dispatch [ optMangled ] , usage . CoerceParameter ) : \n 
~~~ self . _dispatch [ optMangled ] . dispatch ( optMangled , arg ) \n 
~~~ self . _dispatch [ optMangled ] ( optMangled , arg ) \n 
~~ ~~ if ( getattr ( self , , None ) \n 
and ( args or self . defaultSubCommand is not None ) ) : \n 
~~~ if not args : \n 
~~~ args = [ self . defaultSubCommand ] \n 
~~ sub , rest = args [ 0 ] , args [ 1 : ] \n 
for ( cmd , short , parser , _ ) in self . subCommands : \n 
~~~ if sub == cmd or sub == short : \n 
~~~ self . subCommand = cmd \n 
self . subOptions = parser ( self . terminal ) \n 
self . subOptions . parent = self \n 
self . subOptions . parseOptions ( rest ) \n 
~~~ self . parseArgs ( * args ) \n 
~~ ~~ self . postOptions ( ) \n 
~~ def _errorHandle ( func ) : \n 
~~~ def call ( self , * args , ** kwargs ) : \n 
~~~ func ( self , * args , ** kwargs ) \n 
~~ ~~ return call \n 
~~ @ _errorHandle \n 
~~~ def callToRosProxy ( self , command , parameter ) : \n 
def perform_action ( ( url , key ) ) : \n 
~~~ self . _connected_rosapi_nodes [ parameter ] = ( url , key ) \n 
argList = [ ( , self . _username ) , ( , command ) , \n 
( , key ) ] \n 
~~~ f = urlopen ( . format ( url , urlencode ( argList ) ) ) \n 
response = json . loads ( f . read ( ) ) \n 
self . terminal . write ( str ( response [ ] ) ) \n 
~~~ msg = e . read ( ) \n 
if msg : \n 
~~~ msg = . format ( msg ) \n 
~~ self . terminal . write ( \n 
. format ( e . getcode ( ) , e . msg , msg ) ) \n 
~~~ url , key = self . _connected_rosapi_nodes [ parameter ] \n 
perform_action ( ( url , key ) ) \n 
~~~ d = self . _user [ ] . callRemote ( , \n 
parameter ) \n 
d . addCallback ( perform_action ) \n 
"{0}" . format ( err ) ) ) \n 
~~ ~~ ~~ def base64_decode_helper ( s ) : \n 
~~~ return base64 . b64decode ( s ) \n 
~~~ for i in range ( 1 , 5 ) : \n 
~~~ s_padded = base64 . b64decode ( s + * i ) \n 
return s_padded \n 
~~ ~~ def build_url ( self , values = None ) : \n 
~~~ if values and len ( self . variables ) > len ( values ) : \n 
for _is_dynamic , var in self . _trace : \n 
~~~ if _is_dynamic : \n 
~~~ url_parts . append ( str ( values [ var ] ) ) \n 
~~~ url_parts . append ( var ) \n 
~~ ~~ return str ( . join ( url_parts ) ) \n 
~~ def _format_metric_name ( self , m_name , cfunc ) : \n 
~~~ aggr = CFUNC_TO_AGGR [ cfunc ] \n 
~~~ aggr = cfunc . lower ( ) \n 
~~~ m_name = CACTI_TO_DD [ m_name ] \n 
if aggr != : \n 
~~~ m_name += % ( aggr ) \n 
~~ return m_name \n 
~~~ return "cacti.%s.%s" % ( m_name . lower ( ) , aggr ) \n 
~~ ~~ def get_code_num ( self ) : \n 
~~~ return self . code [ 1 ] \n 
~~~ return self . code \n 
~~ ~~ def ingest ( self ) : \n 
~~~ proj = projdb . loadToken ( self . token ) \n 
~~~ ch = proj . getChannelObj ( self . channel ) \n 
[ [ ximagesz , yimagesz , zimagesz ] , ( starttime , endtime ) ] = proj . datasetcfg . imageSize ( self . resolution [ xcubedim , ycubedim , zcubedim ] = cubedim = proj . datasetcfg . getCubeDims ( ) [ self . resolution ] \n 
[ xoffset , yoffset , zoffset ] = proj . datasetcfg . getOffset ( ) [ self . resolution ] \n 
for resolution in range ( 0 , 1 , 1 ) : \n 
~~~ numxtiles = ximagesz / self . tilesz [ 0 ] \n 
numytiles = yimagesz / self . tilesz [ 1 ] \n 
for slice_number in range ( 0 , zimagesz , zcubedim ) : \n 
~~~ slab = np . zeros ( [ zcubedim , yimagesz , ximagesz ] , dtype = np . uint32 ) \n 
~~~ for ytile in range ( numytiles ) : \n 
~~~ for xtile in range ( numxtiles ) : \n 
~~~ if slice_number + b <= zimagesz : \n 
imgdata = np . asarray ( Image . open ( filename , ) . convert ( ) ) \n 
imgdata = np . left_shift ( imgdata [ : , : , 3 ] , 24 , dtype = np . uint32 ) | np . left_shift ( imgdata slab [ b , ytile * self . tilesz [ 1 ] : ( ytile + 1 ) * self . tilesz [ 1 ] , xtile * self . tilesz [ 0 ] : ( xtile ~~ except __HOLE__ , e : \n 
slab [ b , ytile * self . tilesz [ 1 ] : ( ytile + 1 ) * self . tilesz [ 1 ] , xtile * self . tilesz [ 0 ] : ( xtile \n 
~~ ~~ ~~ ~~ ~~ for y in range ( 0 , yimagesz + 1 , ycubedim ) : \n 
~~~ zidx = ocplib . XYZMorton ( [ x / xcubedim , y / ycubedim , ( slice_number ) / zcubedim ] ) \n 
~~~ db . putCube ( ch , zidx , self . resolution , cube , update = True ) \n 
~~ ~~ ~~ ~~ ~~ ~~ ~~ def dispatch_request ( self , seq , handlername , args ) : \n 
~~~ res = getattr ( self , handlername ) ( * args ) \n 
~~~ self . send_exception ( seq , sys . exc_info ( ) ) \n 
~~~ self . send_result ( seq , res ) \n 
~~ ~~ def pre_init ( self ) : \n 
~~~ mod = runpy . run_module ( self . async ) \n 
self . async = mod [ ] ( ) \n 
self . add_service ( self . async ) \n 
~~ except ( __HOLE__ , ImportError ) as e : \n 
~~~ if self . async not in self . async_available : \n 
+ "\\n\\t" . join ( self . async_available ) ) \n 
~~ elif self . async . endswith ( "gevent" ) : \n 
~~~ helptext = "" \n 
~~ raise RuntimeError ( \n 
helptext ) ) \n 
~~ ~~ def _detect_toolkit ( ) : \n 
~~~ global _toolkit \n 
if in sys . argv : \n 
~~~ _toolkit = TOOLKIT_CURSES \n 
~~ if in sys . argv : \n 
~~~ _toolkit = TOOLKIT_CHROME \n 
~~ if os . getenv ( ) : \n 
~~~ can_have_gui = True \n 
~~ elif sys . platform == : \n 
~~~ can_have_gui = False \n 
~~ if can_have_gui : \n 
~~~ import pygtk \n 
pygtk . require ( ) \n 
_toolkit = TOOLKIT_GTK \n 
~~ ~~ if can_have_gui : \n 
~~~ import wx \n 
_toolkit = TOOLKIT_WX \n 
~~~ if sys . platform == : \n 
~~ ~~ ~~ if in sys . argv or not can_have_gui : \n 
~~ _toolkit = None \n 
~~ def _get_distinct_values ( self , field , silk_request ) : \n 
~~~ if silk_request : \n 
~~~ query_set = Profile . objects . filter ( request = silk_request ) \n 
~~~ query_set = Profile . objects . all ( ) \n 
~~ function_names = [ x [ field ] for x in query_set . values ( field ) . distinct ( ) ] \n 
~~~ function_names . remove ( ) \n 
~~ function_names = [ ] + function_names \n 
return function_names \n 
~~ def _drive ( iterable , finished ) : \n 
~~~ next = iterable . next ( ) \n 
~~~ finished . callback ( ) \n 
~~~ finished . errback ( ) \n 
~~~ deferred , returner = next \n 
def cb ( result ) : \n 
returner ( result ) \n 
_drive ( iterable , finished ) \n 
~~ def eb ( failure ) : \n 
finished . errback ( failure ) \n 
~~ deferred . addCallback ( cb ) . addErrback ( eb ) \n 
~~ ~~ def scrape ( iterator , location , limit ) : \n 
for photo in iterator : \n 
~~~ filepath = "{0}/{1}/{2}.jpg" . format ( location , \n 
photo . flickrid % 100 , \n 
photo . flickrid ) \n 
if os . path . exists ( filepath ) : \n 
photo . format ) ) \n 
image = photo . download ( ) \n 
~~~ image . save ( filepath ) \n 
~~~ os . makedirs ( os . path . dirname ( filepath ) ) \n 
image . save ( filepath ) \n 
~~ limit -= 1 \n 
if limit == 0 : \n 
~~ ~~ ~~ def get_async ( apply_async , num_workers , dsk , result , cache = None , \n 
queue = None , get_id = default_get_id , raise_on_exception = False , \n 
rerun_exceptions_locally = None , callbacks = None , ** kwargs ) : \n 
assert queue \n 
if callbacks is None : \n 
~~~ callbacks = _globals [ ] \n 
~~ start_cbs , start_state_cbs , pretask_cbs , posttask_cbs , finish_cbs = unpack_callbacks ( callbacks ) \n 
if isinstance ( result , list ) : \n 
~~~ result_flat = set ( flatten ( result ) ) \n 
~~~ result_flat = set ( [ result ] ) \n 
~~ results = set ( result_flat ) \n 
dsk = dsk . copy ( ) \n 
for f in start_cbs : \n 
~~~ f ( dsk ) \n 
~~ dsk = cull ( dsk , list ( results ) ) \n 
keyorder = order ( dsk ) \n 
state = start_state_from_dask ( dsk , cache = cache , sortkey = keyorder . get ) \n 
for f in start_state_cbs : \n 
~~~ f ( dsk , state ) \n 
~~ if rerun_exceptions_locally is None : \n 
~~~ rerun_exceptions_locally = _globals . get ( , False ) \n 
~~ if state [ ] and not state [ ] : \n 
~~ def fire_task ( ) : \n 
key = state [ ] . pop ( ) \n 
state [ ] . add ( key ) \n 
for f in pretask_cbs : \n 
~~~ f ( key , dsk , state ) \n 
~~ data = dict ( ( dep , state [ ] [ dep ] ) \n 
for dep in get_dependencies ( dsk , key ) ) \n 
apply_async ( execute_task , args = [ key , dsk [ key ] , data , queue , \n 
get_id , raise_on_exception ] ) \n 
~~ while state [ ] and len ( state [ ] ) < num_workers : \n 
~~~ fire_task ( ) \n 
~~ while state [ ] or state [ ] or state [ ] : \n 
~~~ key , res , tb , worker_id = queue . get ( ) \n 
~~~ for f in finish_cbs : \n 
~~~ f ( dsk , state , True ) \n 
~~ if isinstance ( res , Exception ) : \n 
~~ if rerun_exceptions_locally : \n 
~~~ data = dict ( ( dep , state [ ] [ dep ] ) \n 
task = dsk [ key ] \n 
~~~ raise ( remote_exception ( res , tb ) ) \n 
~~ ~~ state [ ] [ key ] = res \n 
finish_task ( dsk , key , state , results , keyorder . get ) \n 
for f in posttask_cbs : \n 
~~~ f ( key , res , dsk , state , worker_id ) \n 
~~ ~~ while state [ ] or not queue . empty ( ) : \n 
~~ for f in finish_cbs : \n 
~~~ f ( dsk , state , False ) \n 
~~ return nested_get ( result , state [ ] ) \n 
~~ def __getattr__ ( self , key ) : \n 
~~~ return object . __getattribute__ ( self , key ) \n 
~~~ return getattr ( self . exception , key ) \n 
~~ ~~ def remote_exception ( exc , tb ) : \n 
if type ( exc ) in exceptions : \n 
~~~ typ = exceptions [ type ( exc ) ] \n 
return typ ( exc , tb ) \n 
~~~ typ = type ( exc . __class__ . __name__ , \n 
( RemoteException , type ( exc ) ) , \n 
{ : type ( exc ) } ) \n 
exceptions [ type ( exc ) ] = typ \n 
~~~ return exc \n 
~~ ~~ ~~ def next_glyph ( font , index ) : \n 
~~~ next = font . glyphOrder [ index + 1 ] \n 
~~~ next = font . glyphOrder [ 0 ] \n 
~~ return next \n 
~~ def previous_glyph ( font , index ) : \n 
~~~ prev = font . glyphOrder [ index - 1 ] \n 
~~~ prev = font . glyphOrder [ - 1 ] \n 
~~ return prev \n 
~~ def next_glyph ( self ) : \n 
~~~ next = next_glyph ( self . font , self . glyph_index ) \n 
~~~ self . glyph_window . setGlyphByName ( next ) \n 
~~~ self . glyph_window = CurrentGlyphWindow ( ) \n 
self . glyph_window . setGlyphByName ( next ) \n 
~~ self . update ( ) \n 
~~ def previous_glyph ( self ) : \n 
~~~ prev = previous_glyph ( self . font , self . glyph_index ) \n 
~~~ self . glyph_window . setGlyphByName ( prev ) \n 
self . glyph_window . setGlyphByName ( prev ) \n 
~~ def layer_down ( self ) : \n 
~~~ self . glyph_window . layerDown ( ) \n 
self . glyph_window . layerDown ( ) \n 
~~ def layer_up ( self ) : \n 
~~~ self . glyph_window . layerUp ( ) \n 
self . glyph_window . layerUp ( ) \n 
~~ def _up_right_callback ( self , sender ) : \n 
~~~ if len ( self . all_fonts ) > 1 : \n 
~~~ f = CurrentFont ( ) \n 
i = self . all_fonts . index ( f ) \n 
~~~ next_i = i + 1 \n 
next_font = self . all_fonts [ next_i ] \n 
~~~ next_i = 0 \n 
~~ g_current = CurrentGlyph ( ) \n 
if g_current is not None : \n 
~~~ if next_font . has_key ( g_current . name ) : \n 
~~~ next_glyph = next_font [ g_current . name ] \n 
~~~ next_glyph = next_font [ next_font . glyphOrder [ 0 ] ] \n 
~~ G = OpenGlyphWindow ( next_glyph , newWindow = False ) \n 
self . update ( ) \n 
~~ ~~ ~~ def _down_left_callback ( self , sender ) : \n 
~~~ prev_i = i - 1 \n 
prev_font = self . all_fonts [ prev_i ] \n 
~~~ prev_i = - 1 \n 
~~~ if prev_font . has_key ( g_current . name ) : \n 
~~~ prev_glyph = prev_font [ g_current . name ] \n 
~~~ prev_glyph = prev_font [ prev_font . glyphOrder [ 0 ] ] \n 
~~ G = OpenGlyphWindow ( prev_glyph , newWindow = False ) \n 
~~ ~~ ~~ def toposort_flatten ( data , sort = True ) : \n 
for d in toposort ( data ) : \n 
~~~ result . extend ( ( sorted if sort else list ) ( d ) ) \n 
~~~ result . extend ( list ( d ) ) \n 
~~ def _spawn_nt ( cmd , \n 
search_path = 1 , \n 
verbose = 0 , \n 
dry_run = 0 ) : \n 
~~~ executable = cmd [ 0 ] \n 
cmd = _nt_quote_args ( cmd ) \n 
if search_path : \n 
~~~ executable = find_executable ( executable ) or executable \n 
~~ log . info ( string . join ( [ executable ] + cmd [ 1 : ] , ) ) \n 
if not dry_run : \n 
~~~ rc = os . spawnv ( os . P_WAIT , executable , cmd ) \n 
~~ except __HOLE__ , exc : \n 
~~ if rc != 0 : \n 
~~ ~~ ~~ def _spawn_os2 ( cmd , \n 
~~ ~~ ~~ def _spawn_posix ( cmd , \n 
~~~ log . info ( string . join ( cmd , ) ) \n 
if dry_run : \n 
~~ exec_fn = search_path and os . execvp or os . execv \n 
~~~ exec_fn ( cmd [ 0 ] , cmd ) \n 
( cmd [ 0 ] , e . strerror ) ) \n 
os . _exit ( 1 ) \n 
~~~ while 1 : \n 
~~~ ( pid , status ) = os . waitpid ( pid , 0 ) \n 
~~~ import errno \n 
if exc . errno == errno . EINTR : \n 
~~ if os . WIFSIGNALED ( status ) : \n 
~~ elif os . WIFEXITED ( status ) : \n 
~~~ exit_status = os . WEXITSTATUS ( status ) \n 
if exit_status == 0 : \n 
~~ ~~ elif os . WIFSTOPPED ( status ) : \n 
~~ ~~ ~~ ~~ def _spawn_java ( cmd , \n 
cmd = . join ( _nt_quote_args ( cmd ) ) \n 
log . info ( cmd ) \n 
~~~ rc = os . system ( cmd ) >> 8 \n 
~~ ~~ ~~ def testGetattr ( self ) : \n 
~~~ func = getattr ( self , "getName" ) \n 
self . assertEqual ( func ( ) , "AttributeTest" , \n 
self . assertEqual ( getattr ( Foo , ) , \n 
self . assertEqual ( 1 , getattr ( Foo , "notthere" , 1 ) ) \n 
foo = Foo ( 1 ) \n 
self . assertEqual ( foo . v , getattr ( foo , "v" ) ) \n 
self . assertEqual ( getattr ( foo , "v" ) , getattr ( foo , "v" ) ) \n 
self . assertEqual ( getattr ( 1 , , 2 ) , 2 ) \n 
self . assertEqual ( getattr ( None , , 2 ) , 2 ) \n 
~~~ self . assertEqual ( 1 , getattr ( foo , "vv" ) ) \n 
~~~ self . assertEqual ( e . __class__ . __name__ , ) \n 
~~ def testDelAttr ( self ) : \n 
~~~ foo = Foo ( 1 ) \n 
self . assertEqual ( hasattr ( foo , "v" ) , True ) \n 
delattr ( foo , "v" ) \n 
self . assertEqual ( hasattr ( foo , "v" ) , False ) \n 
self . assertEqual ( hasattr ( foo , "getV" ) , True ) \n 
~~~ delattr ( foo , "getV" ) \n 
~~ except AttributeError , e : \n 
~~ class Foo1 ( Foo ) : \n 
~~ foo1 = Foo1 ( 1 ) \n 
~~~ delattr ( foo , "delete_me" ) \n 
~~~ self . assertTrue ( True ) \n 
~~ self . assertEqual ( hasattr ( foo , "delete_me" ) , True ) \n 
~~~ delattr ( Foo1 , "delete_me" ) \n 
delattr ( Foo , "delete_me" ) \n 
~~ def testAttributMapping ( self ) : \n 
~~~ f = Foo ( 1 ) \n 
self . assertEqual ( Foo . name , ) \n 
self . assertEqual ( f . name , ) \n 
name , prototype , apply , constructor = f . call ( ) \n 
self . assertEqual ( name , ) \n 
self . assertEqual ( prototype , ) \n 
self . assertEqual ( apply , ) \n 
self . assertEqual ( constructor , ) \n 
self . assertEqual ( Foo . label , ) \n 
self . assertEqual ( f . label , ) \n 
self . assertEqual ( f . do ( ) , ) \n 
self . assertEqual ( getattr ( f , ) ( ) , ) \n 
setattr ( Foo , , 1 ) \n 
self . assertEqual ( getattr ( f , ) , 1 ) \n 
~~~ self . assertEqual ( f . typeof , 1 ) \n 
~~ self . assertTrue ( hasattr ( Foo , ) ) \n 
delattr ( Foo , ) \n 
self . assertFalse ( hasattr ( Foo , ) ) \n 
setattr ( Foo , , 2 ) \n 
self . assertTrue ( hasattr ( Foo , ) ) \n 
del Foo . typeof \n 
o = object ( ) \n 
self . assertFalse ( getattr ( o , , False ) ) \n 
~~ def validate_models ( ) : \n 
import logging \n 
from django . core . management . validation import get_validation_errors \n 
~~~ from cStringIO import StringIO \n 
~~~ from StringIO import StringIO \n 
s = StringIO ( ) \n 
num_errors = get_validation_errors ( s , None ) \n 
if num_errors : \n 
~~~ s . seek ( 0 ) \n 
error_text = s . read ( ) \n 
~~ ~~ def run_only_if_pymongo_is_available ( func ) : \n 
~~~ import pymongo \n 
~~~ pymongo = None \n 
~~ pred = lambda : pymongo is not None \n 
return run_only ( func , pred ) \n 
~~ def compact_word_vectors ( self , vocab , filename = None , array = None , \n 
use_spacy = True ) : \n 
if use_spacy : \n 
~~~ import spacy . en \n 
nlp = spacy . en . English ( ) \n 
if filename : \n 
~~~ nlp . vocab . load_vectors ( filename ) \n 
~~ n_dim = nlp . vocab . vectors_length \n 
~~~ from gensim . models . word2vec import Word2Vec \n 
model = Word2Vec . load_word2vec_format ( filename , binary = True ) \n 
n_dim = model . syn0 . shape [ 1 ] \n 
~~ n_words = len ( self . compact_to_loose ) \n 
data = np . random . randn ( ( n_words , n_dim ) , dtype = ) \n 
if not use_spacy : \n 
~~~ data -= model . syn0 . mean ( ) \n 
data /= data . std ( ) \n 
data *= model . syn0 . std ( ) \n 
~~ if array is not None : \n 
~~~ data = array \n 
n_words = data . shape [ 0 ] \n 
~~ s , f = 0 , 0 \n 
for compact , loose in self . compact_to_loose . items ( ) : \n 
~~~ word = vocab . get ( loose , None ) \n 
if compact >= n_words : \n 
~~ if word is None : \n 
~~ word = word . strip ( ) \n 
~~~ token = nlp . vocab [ unicode ( word ) ] \n 
if not token . has_vector : \n 
~~~ token = nlp . vocab [ unicode ( word . replace ( , ) ) ] \n 
~~ if not token . has_vector : \n 
~~~ f += 1 \n 
if f > 30 : \n 
~~~ print token . orth_ \n 
~~ vector = token . vector \n 
~~~ vector = None \n 
~~~ vector = model [ word ] \n 
~~ if vector is None : \n 
~~~ vector = model [ word . replace ( , ) ] \n 
~~ ~~ if vector is None : \n 
~~~ choices = model . vocab . keys ( ) \n 
~~~ choice = difflib . get_close_matches ( word , choices ) [ 0 ] \n 
vector = model [ choice ] \n 
print word , , choice \n 
~~ ~~ s += 1 \n 
data [ compact , : ] = vector [ : ] \n 
~~ return data , s , f \n 
~~ def __getattr__ ( self , name ) : \n 
~~~ return self . _dynamic_properties [ name ] ( ) \n 
raise AttributeError ( msg . format ( type ( self ) . __name__ , name ) ) \n 
~~ ~~ def process_view ( self , request , view_func , view_args , view_kwargs ) : \n 
from django . shortcuts import render_to_response \n 
from django . template import RequestContext \n 
~~~ from django . http . response import HttpResponseBase as RBase \n 
~~ res = view_func ( request , * view_args , ** view_kwargs ) \n 
if isinstance ( res , basestring ) : \n 
~~~ res = res , { } \n 
~~ if isinstance ( res , RBase ) : \n 
~~ if isinstance ( res , tuple ) : \n 
~~~ template_name , context = res \n 
res = render_to_response ( template_name , context , \n 
RequestContext ( request ) ) \n 
~~ def _import_django ( self ) : \n 
self . surl = surl \n 
from django . http import HttpResponse , Http404 , HttpResponseRedirect \n 
self . HttpResponse = HttpResponse \n 
self . Http404 , self . HttpResponseRedirect = Http404 , HttpResponseRedirect \n 
from django . shortcuts import ( get_object_or_404 , get_list_or_404 , \n 
render_to_response , render , redirect ) \n 
self . get_object_or_404 = get_object_or_404 \n 
self . get_list_or_404 = get_list_or_404 \n 
self . render_to_response = render_to_response \n 
self . render , self . redirect = render , redirect \n 
self . RequestContext = RequestContext \n 
from django import forms \n 
self . forms = forms \n 
~~~ from django . core . wsgi import get_wsgi_application \n 
self . wsgi_application = get_wsgi_application ( ) \n 
~~~ import django . core . handlers . wsgi \n 
self . wsgi_application = django . core . handlers . wsgi . WSGIHandler ( ) \n 
~~~ from whitenoise . django import DjangoWhiteNoise \n 
self . wsgi_application = DjangoWhiteNoise ( self . wsgi_application ) \n 
~~~ from django . conf . urls . defaults import patterns , url \n 
~~ self . patterns , self . url = patterns , url \n 
~~ def dotslash ( self , pth ) : \n 
if hasattr ( self , "APP_DIR" ) : \n 
~~~ return self . _get_app_dir ( pth = pth ) \n 
~~~ import speaklater \n 
~~~ return speaklater . make_lazy_string ( self . _get_app_dir , pth ) \n 
~~ ~~ ~~ def get_secret_key ( self ) : \n 
jsonf , secret = self . dotslash ( "secret.json" ) , os . environ . get ( "secret" ) \n 
if not secret : \n 
~~~ with open ( jsonf , "r" ) as f : \n 
~~~ secret = loads ( f . read ( ) ) . get ( "secret" ) \n 
~~ ~~ except ( __HOLE__ , IndexError ) : \n 
~~~ with open ( jsonf , "w" ) as f : \n 
~~~ secret = uuid4 ( ) . hex \n 
f . write ( dumps ( { "secret" : secret } , indent = 4 ) + "\\n" ) \n 
~~ ~~ ~~ return secret \n 
~~ def _configure_django ( self , ** kw ) : \n 
from django . conf import settings , global_settings \n 
self . settings = settings \n 
if settings . configured : \n 
~~ self . APP_DIR , app_filename = os . path . split ( \n 
os . path . realpath ( inspect . stack ( ) [ 2 ] [ 1 ] ) \n 
DEBUG = kw . get ( "DEBUG" , False ) \n 
md = { } \n 
dp = { } \n 
for k , v in kw . items ( ) : \n 
~~~ if isinstance ( v , E ) : \n 
~~~ md [ k ] = v . value \n 
setattr ( global_esettings , k , v . value ) \n 
~~ if isinstance ( v , DSetting ) : \n 
~~~ dp [ k ] = v \n 
~~ ~~ for k , v in md . items ( ) : \n 
~~~ kw [ k ] = v \n 
~~ for k , v in dp . items ( ) : \n 
~~~ if DEBUG : \n 
~~~ if v . dvalue is not NotSet : \n 
~~~ kw [ k ] = v . dvalue \n 
~~~ if v . pvalue is not NotSet : \n 
~~~ kw [ k ] = v . pvalue \n 
~~ ~~ ~~ del md \n 
del dp \n 
def do_dp ( key ) : \n 
~~~ if key not in kw : \n 
~~ old = kw [ key ] \n 
kw [ key ] = [ ] \n 
for value in old : \n 
~~~ if value . startswith ( "prod:" ) : \n 
~~ kw [ key ] . append ( value . replace ( "debug:" , "" ) ) \n 
~~~ if value . startswith ( "debug:" ) : \n 
~~ kw [ key ] . append ( value . replace ( "prod:" , "" ) ) \n 
~~ ~~ ~~ s_found = 0 \n 
sd = { } \n 
~~~ if isinstance ( v , MirrorSetting ) : \n 
~~~ s_found += 1 \n 
if s_found > MirrorSetting . count : \n 
~~~ raise ImproperlyConfiguredError \n 
~~ sd [ k ] = v \n 
~~ ~~ if MirrorSetting . count > s_found : \n 
~~ for k , v in sd . items ( ) : \n 
~~~ kw [ k ] = kw [ v . name ] \n 
~~ del sd \n 
del s_found \n 
do_dp ( "MIDDLEWARE_CLASSES" ) \n 
do_dp ( "INSTALLED_APPS" ) \n 
do_dp ( "TEMPLATE_CONTEXT_PROCESSORS" ) \n 
if "debug" in kw : \n 
~~~ db = kw . pop ( "debug" ) \n 
if DEBUG : \n 
~~~ kw . update ( db ) \n 
~~ ~~ if "regexers" in kw : \n 
~~~ self . update_regexers ( kw . pop ( "regexers" ) ) \n 
~~ self . mounts = kw . pop ( "mounts" , { } ) \n 
if not kw . get ( "dont_configure" , False ) : \n 
~~~ kw [ "ROOT_URLCONF" ] = kw . get ( "ROOT_URLCONF" , "importd.urlconf" ) \n 
if "TEMPLATE_DIRS" not in kw : \n 
~~~ kw [ "TEMPLATE_DIRS" ] = ( self . dotslash ( "templates" ) , ) \n 
~~ if "STATIC_URL" not in kw : \n 
~~~ kw [ "STATIC_URL" ] = "/static/" \n 
~~ if "STATIC_ROOT" not in kw : \n 
~~~ kw [ "STATIC_ROOT" ] = self . dotslash ( "staticfiles" ) \n 
~~ if "STATICFILES_DIRS" not in kw : \n 
~~~ kw [ "STATICFILES_DIRS" ] = [ self . dotslash ( "static" ) ] \n 
~~ if "MEDIA_URL" not in kw : \n 
~~~ kw [ "MEDIA_URL" ] = "/static/media/" \n 
~~ if "lr" in kw : \n 
~~~ self . lr = kw . pop ( "lr" ) \n 
~~ if "db" in kw : \n 
~~~ if isinstance ( kw [ "db" ] , basestring ) : \n 
~~~ kw [ "DATABASES" ] = { \n 
"default" : dj_database_url . parse ( kw . pop ( "db" ) ) \n 
~~~ db = kw . pop ( "db" ) \n 
default = dj_database_url . parse ( db [ 0 ] ) \n 
default . update ( db [ 1 ] ) \n 
kw [ "DATABASES" ] = dict ( default = default ) \n 
~~ ~~ if "DATABASES" not in kw : \n 
"default" : { \n 
: "django.db.backends.sqlite3" , \n 
: self . dotslash ( "db.sqlite" ) \n 
~~ self . smart_return = False \n 
if kw . pop ( "SMART_RETURN" , True ) : \n 
~~~ self . smart_return = True \n 
if "MIDDLEWARE_CLASSES" not in kw : \n 
~~~ kw [ "MIDDLEWARE_CLASSES" ] = ( \n 
global_settings . MIDDLEWARE_CLASSES \n 
~~ kw [ "MIDDLEWARE_CLASSES" ] = list ( kw [ "MIDDLEWARE_CLASSES" ] ) \n 
kw [ "MIDDLEWARE_CLASSES" ] . append ( \n 
"importd.SmartReturnMiddleware" \n 
~~ installed = list ( kw . setdefault ( "INSTALLED_APPS" , [ ] ) ) \n 
admin_url = kw . pop ( "admin" , "^admin/" ) \n 
if admin_url : \n 
~~~ if "django.contrib.auth" not in installed : \n 
~~~ installed . append ( "django.contrib.auth" ) \n 
~~ if "django.contrib.contenttypes" not in installed : \n 
~~~ installed . append ( "django.contrib.contenttypes" ) \n 
~~ if "django.contrib.auth" not in installed : \n 
~~ if "django.contrib.messages" not in installed : \n 
~~~ installed . append ( "django.contrib.messages" ) \n 
~~ if "django.contrib.sessions" not in installed : \n 
~~~ installed . append ( "django.contrib.sessions" ) \n 
last_position = len ( kw [ "MIDDLEWARE_CLASSES" ] ) \n 
kw [ "MIDDLEWARE_CLASSES" ] = list ( kw [ "MIDDLEWARE_CLASSES" ] ) \n 
kw [ "MIDDLEWARE_CLASSES" ] . insert ( \n 
last_position , \n 
"django.contrib.sessions.middleware.SessionMiddleware" \n 
~~ if "django.contrib.admin" not in installed : \n 
~~~ installed . append ( "django.contrib.admin" ) \n 
"django.contrib.auth.middleware" \n 
".AuthenticationMiddleware" \n 
~~ if "django.contrib.humanize" not in installed : \n 
~~~ installed . append ( "django.contrib.humanize" ) \n 
~~ if "django.contrib.staticfiles" not in installed : \n 
~~~ installed . append ( "django.contrib.staticfiles" ) \n 
~~ if "debug_toolbar" not in installed and debug_toolbar : \n 
~~~ installed . append ( "debug_toolbar" ) \n 
if not in kw : \n 
~~~ kw [ ] = ( , ) \n 
~~ kw [ ] . insert ( \n 
1 , \n 
kw [ ] = ( \n 
~~ ~~ kw [ ] = installed \n 
if "DEBUG" not in kw : \n 
~~~ kw [ "DEBUG" ] = kw [ "TEMPLATE_DEBUG" ] = True \n 
~~ if "APP_DIR" not in kw : \n 
~~~ kw [ "APP_DIR" ] = self . APP_DIR \n 
~~ if "SECRET_KEY" not in kw : \n 
~~~ kw [ "SECRET_KEY" ] = self . get_secret_key ( ) \n 
~~ if "ADMINS" not in kw : \n 
~~~ kw [ "ADMINS" ] = kw [ "MANAGERS" ] = ( ( getuser ( ) , "" ) , ) \n 
~~ autoimport = kw . pop ( "autoimport" , True ) \n 
kw [ "SETTINGS_MODULE" ] = kw . get ( "SETTINGS_MODULE" , "importd" ) \n 
settings . configure ( ** kw ) \n 
if hasattr ( django , "setup" ) : \n 
~~~ django . setup ( ) \n 
~~ self . _import_django ( ) \n 
from django . contrib . staticfiles . urls import staticfiles_urlpatterns \n 
urlpatterns = self . get_urlpatterns ( ) \n 
urlpatterns += staticfiles_urlpatterns ( ) \n 
if autoimport : \n 
~~~ for app in settings . INSTALLED_APPS : \n 
~~~ self . _import_app_module ( "{}.admin" , app ) \n 
self . _import_app_module ( "{}.models" , app ) \n 
~~ ~~ if admin_url : \n 
~~~ from django . contrib import admin \n 
~~~ from django . conf . urls import include \n 
~~ admin . autodiscover ( ) \n 
self . add_view ( admin_url , include ( admin . site . urls ) ) \n 
~~ if autoimport : \n 
~~~ self . _import_app_module ( "{}.forms" , app ) \n 
self . _import_app_module ( "{}.views" , app ) \n 
self . _import_app_module ( "{}.signals" , app ) \n 
~~ ~~ ~~ self . blueprints = kw . pop ( "blueprints" , { } ) \n 
for namespace , meta in self . blueprints . items ( ) : \n 
~~~ if isinstance ( meta , basestring ) : \n 
~~~ meta = { "blueprint" : meta } \n 
~~ mod_path , bp_name = meta [ "blueprint" ] . rsplit ( "." , 1 ) \n 
mod = importlib . import_module ( mod_path ) \n 
bp = getattr ( mod , bp_name ) \n 
self . register_blueprint ( \n 
bp , url_prefix = meta . get ( "url_prefix" , namespace + "/" ) , \n 
namespace = namespace , app_name = meta . get ( "app_name" , "" ) ) \n 
~~ self . _configured = True \n 
~~ def _import_app_module ( self , fmt , app ) : \n 
~~~ exc_type , exc_value , exc_traceback = sys . exc_info ( ) \n 
traceback . print_exception ( exc_type , exc_value , exc_traceback ) \n 
raise SystemExit ( - 1 ) \n 
~~ ~~ def _apply_blueprint ( self , bp ) : \n 
~~ url = self . surl ( bp . url_prefix , include ( bp . patterns , \n 
namespace = bp . namespace , \n 
app_name = bp . app_name ) ) \n 
urlpatterns . append ( url ) \n 
django . core . urlresolvers . clear_url_caches ( ) \n 
~~ def tile_raster_images ( X , img_shape , \n 
tile_shape = None , tile_spacing = ( 1 , 1 ) , \n 
scale_rows_to_unit_interval = True , \n 
output_pixel_vals = True , \n 
min_dynamic_range = 1e-4 , \n 
if len ( img_shape ) == 3 and img_shape [ 2 ] == 3 : \n 
~~~ if scale_rows_to_unit_interval : \n 
~~ return tile_raster_images ( \n 
( X [ : , 0 : : 3 ] , X [ : , 1 : : 3 ] , X [ : , 2 : : 3 ] , None ) , \n 
img_shape = img_shape [ : 2 ] , \n 
tile_shape = tile_shape , \n 
tile_spacing = tile_spacing , \n 
scale_rows_to_unit_interval = scale_rows_to_unit_interval , \n 
output_pixel_vals = output_pixel_vals , \n 
min_dynamic_range = min_dynamic_range ) \n 
~~ if isinstance ( X , tuple ) : \n 
~~~ n_images_in_x = X [ 0 ] . shape [ 0 ] \n 
~~~ n_images_in_x = X . shape [ 0 ] \n 
~~ if tile_shape is None : \n 
~~~ tile_shape = most_square_shape ( n_images_in_x ) \n 
~~ assert len ( img_shape ) == 2 \n 
assert len ( tile_shape ) == 2 \n 
assert len ( tile_spacing ) == 2 \n 
out_shape = [ ( ishp + tsp ) * tshp - tsp for ishp , tshp , tsp \n 
in zip ( img_shape , tile_shape , tile_spacing ) ] \n 
if isinstance ( X , tuple ) : \n 
~~~ raise NotImplementedError ( ) \n 
~~ assert len ( X ) == 4 \n 
if output_pixel_vals : \n 
~~~ out_array = numpy . zeros ( ( out_shape [ 0 ] , out_shape [ 1 ] , 4 ) , dtype = ) \n 
~~~ out_array = numpy . zeros ( ( out_shape [ 0 ] , out_shape [ 1 ] , 4 ) , dtype = X . dtype ) \n 
~~ if output_pixel_vals : \n 
~~~ channel_defaults = [ 0 , 0 , 0 , 255 ] \n 
~~~ channel_defaults = [ 0. , 0. , 0. , 1. ] \n 
~~ for i in xrange ( 4 ) : \n 
~~~ if X [ i ] is None : \n 
~~~ out_array [ : , : , i ] = numpy . zeros ( out_shape , \n 
dtype = if output_pixel_vals else out_array . dtype \n 
) + channel_defaults [ i ] \n 
~~~ out_array [ : , : , i ] = tile_raster_images ( X [ i ] , img_shape , tile_shape , tile_spacing , scale_rows_to_unit_interval ~~ ~~ return out_array \n 
~~~ H , W = img_shape \n 
Hs , Ws = tile_spacing \n 
out_scaling = 1 \n 
if output_pixel_vals and str ( X . dtype ) . startswith ( ) : \n 
~~~ out_scaling = 255 \n 
~~ out_array = numpy . zeros ( out_shape , dtype = if output_pixel_vals else X . dtype ) \n 
for tile_row in xrange ( tile_shape [ 0 ] ) : \n 
~~~ for tile_col in xrange ( tile_shape [ 1 ] ) : \n 
~~~ if tile_row * tile_shape [ 1 ] + tile_col < X . shape [ 0 ] : \n 
~~~ this_img = scale_to_unit_interval ( \n 
X [ tile_row * tile_shape [ 1 ] + tile_col ] . reshape ( img_shape ) , \n 
eps = min_dynamic_range ) \n 
% ( \n 
X [ tile_row * tile_shape [ 1 ] + tile_col ] . shape \n 
, img_shape \n 
~~~ this_img = X [ tile_row * tile_shape [ 1 ] + tile_col ] . reshape ( img_shape ) \n 
~~ out_array [ \n 
tile_row * ( H + Hs ) : tile_row * ( H + Hs ) + H , \n 
tile_col * ( W + Ws ) : tile_col * ( W + Ws ) + W \n 
] = this_img * out_scaling \n 
~~ ~~ ~~ return out_array \n 
~~ ~~ def _close_result ( self ) : \n 
~~~ if self . result is not None : \n 
~~~ self . result_iter . next ( ) \n 
~~~ pass #done \n 
~~ self . result . close ( ) \n 
~~ self . description = None \n 
self . result = None \n 
self . result_iter = None \n 
self . lastrowid = None \n 
self . rowcount = - 1 \n 
~~ def fetchone ( self ) : \n 
~~~ return self . result_iter . next ( ) \n 
~~ except TaskletExit : \n 
~~~ def create ( \n 
name = "Launcher" , \n 
bundle = [ ] , \n 
platforms = [ "mac" , "win" ] , \n 
outdir = "dist.platforms" , \n 
ignorelibs = [ "*video*" ] \n 
import jycessing . Runner as Runner \n 
import jycessing . launcher . StandaloneSketch as StandaloneSketch \n 
import sys \n 
if not isinstance ( Runner . sketch , StandaloneSketch ) : \n 
~~ if "--internal" in sys . argv : return \n 
import jycessing . launcher . LaunchHelper as LaunchHelper \n 
import java . lang . System as System \n 
import java . nio . file . Paths as Paths \n 
import os , shutil , zipfile , inspect , stat , glob , errno \n 
main = System . getProperty ( "python.main" ) \n 
mainroot = System . getProperty ( "python.main.root" ) \n 
outdir = mainroot + "/" + outdir \n 
try : shutil . rmtree ( outdir ) \n 
except : pass \n 
def copyeverything ( src , dst ) : \n 
import shutil , errno \n 
~~~ shutil . copytree ( src , dst ) \n 
~~~ if exc . errno == errno . ENOTDIR : \n 
~~~ shutil . copy ( src , dst ) \n 
~~ else : raise \n 
~~ ~~ def copyjars ( root ) : \n 
sketch = Runner . sketch \n 
_mainjar = sketch . getMainJarFile ( ) \n 
mainjar , mainjarname = _mainjar . getAbsolutePath ( ) , _mainjar . getName ( ) \n 
shutil . copyfile ( mainjar , root + "/" + mainjarname ) \n 
libraries = sketch . getLibraryDirectories ( ) \n 
for lib in libraries : \n 
~~~ shutil . copytree ( lib . getPath ( ) , root + "/libraries" , ignore = shutil . ignore_patterns ( * ignorelibs \n 
~~ ~~ def copydata ( runtimedir ) : \n 
try : os . mkdir ( runtimedir ) \n 
for data in bundle : \n 
~~~ for f in list ( glob . iglob ( mainroot + "/" + data ) ) : \n 
~~~ copyeverything ( f , runtimedir + "/" + f . replace ( mainroot , "" ) ) \n 
~~ ~~ shutil . copyfile ( main , runtimedir + "/sketch.py" ) \n 
~~ os . mkdir ( outdir ) \n 
for platform in platforms : \n 
~~~ pdir = outdir + "/" + platform \n 
tmpfile = pdir + ".zip" \n 
os . mkdir ( pdir ) \n 
LaunchHelper . copyResourceTo ( "launcher." + platform + ".zip" , Paths . get ( tmpfile ) ) \n 
z = zipfile . ZipFile ( tmpfile , "r" ) \n 
z . extractall ( pdir ) \n 
z . close ( ) \n 
~~~ os . remove ( tmpfile ) \n 
~~ ~~ if "mac" in platforms : \n 
~~~ root = outdir + "/mac/Processing.app/Contents/" \n 
mode = os . stat ( root + "/MacOS/JavaAppLauncher" ) . st_mode \n 
os . chmod ( root + "/MacOS/JavaAppLauncher" , mode | stat . S_IXUSR ) \n 
copyjars ( root + "Java" ) \n 
copydata ( root + "/Runtime" ) \n 
os . rename ( outdir + "/mac/Processing.app" , outdir + "/mac/" + name + ".app/" ) \n 
~~ if "win" in platforms : \n 
~~~ root = outdir + "/win/" \n 
copyjars ( root ) \n 
copydata ( root + "/runtime" ) \n 
os . mkdir ( root + "/jre/" ) \n 
JREREADME = open ( root + "/jre/README.txt" , "w" ) \n 
JREREADME . close ( ) \n 
os . remove ( root + "/launcherc.exe" ) \n 
os . rename ( root + "/launcher.exe" , root + "/" + name . lower ( ) + ".exe" ) \n 
os . rename ( root + "/launcher.ini" , root + "/" + name . lower ( ) + ".ini" ) \n 
~~ System . exit ( 0 ) \n 
~~~ return getattr ( self , name ) \n 
~~~ raise KeyError ( name ) \n 
~~ ~~ def read_file ( file_name ) : \n 
~~~ logger . info ( ) \n 
parsed_data = [ ] \n 
~~~ with open ( file_name ) as f : \n 
~~~ for line in reversed ( f . readlines ( ) ) : \n 
~~~ processed_line = raw_line_parser ( line . rstrip ( ) ) \n 
if processed_line is not None : \n 
~~~ parsed_data . append ( processed_line ) \n 
~~ ~~ ~~ ~~ except __HOLE__ as e : \n 
~~~ logger . error ( . format ( e ) ) \n 
exit ( 1 ) \n 
~~ return parsed_data \n 
~~ def process_formdata ( self , valuelist ) : \n 
~~~ from colour import Color \n 
if valuelist : \n 
~~~ if valuelist [ 0 ] == or valuelist [ 0 ] == : \n 
~~~ self . data = None \n 
~~~ self . data = Color ( valuelist [ 0 ] ) \n 
raise ValueError ( self . gettext ( self . error_msg ) ) \n 
~~ ~~ ~~ ~~ def assertMethod ( self , method , request = - 1 ) : \n 
~~~ request = self . server . requests [ request ] \n 
~~~ raise AssertionError ( ) \n 
~~ self . assertEqual ( method . upper ( ) , request . method ) \n 
~~ def assertPath ( self , path , request = - 1 ) : \n 
~~ self . assertEqual ( path , request . path ) \n 
~~ def assertData ( self , key , value , request = - 1 ) : \n 
~~ self . assertIn ( value , request . data . getvalue ( key , [ ] ) ) \n 
~~ def find ( elem , path ) : \n 
~~~ return findall ( elem , path ) . next ( ) \n 
## \n 
~~ ~~ def findall ( elem , path ) : \n 
~~~ selector = _cache [ path ] \n 
~~~ if len ( _cache ) > 100 : \n 
~~~ _cache . clear ( ) \n 
~~ if path [ : 1 ] == "/" : \n 
~~ stream = iter ( xpath_tokenizer ( path ) ) \n 
next = stream . next ; token = next ( ) \n 
selector = [ ] \n 
~~~ selector . append ( ops [ token [ 0 ] ] ( next , token ) ) \n 
~~~ token = next ( ) \n 
if token [ 0 ] == "/" : \n 
~~ ~~ _cache [ path ] = selector \n 
~~ result = [ elem ] \n 
context = _SelectorContext ( elem ) \n 
for select in selector : \n 
~~~ result = select ( context , result ) \n 
~~ def findtext ( elem , path , default = None ) : \n 
~~~ elem = findall ( elem , path ) . next ( ) \n 
return elem . text \n 
~~ ~~ def __init__ ( self , question_set , * args , ** kwargs ) : \n 
related_object = kwargs . get ( , None ) \n 
~~~ del kwargs [ ] \n 
~~ answer_set = kwargs . get ( , None ) \n 
~~ super ( self . __class__ , self ) . __init__ ( * args , ** kwargs ) \n 
for question in question_set . questions . order_by ( ) : \n 
~~~ name = "answer_for_%i" % question . id \n 
field = self . _get_field ( question ) \n 
if field : \n 
~~~ self . fields [ name ] = field \n 
~~ ~~ if related_object : \n 
~~~ ct = ContentType . objects . get_for_model ( related_object ) \n 
related = u"%i:%s" % ( ct . id , related_object . pk ) \n 
self . fields [ ] = forms . CharField ( widget = forms . HiddenInput , initial = related ) \n 
if answer_set : \n 
~~~ self . data [ ] = related \n 
~~ ~~ if answer_set : \n 
~~~ self . fields [ ] = forms . CharField ( widget = forms . HiddenInput , initial = answer_set self . data [ ] = answer_set . pk \n 
self . is_bound = True \n 
for question , answer in answer_set . q_and_a ( ) : \n 
self . data [ name ] = answer . text \n 
~~ ~~ self . fields [ ] = forms . CharField ( widget = forms . HiddenInput , initial = question_set if answer_set : \n 
~~~ self . data [ ] = question_set . slug \n 
~~ ~~ def compute ( self ) : \n 
~~~ url = URL ( drivername = self . get_input ( ) , \n 
username = self . force_get_input ( , None ) , \n 
password = self . force_get_input ( , None ) , \n 
host = self . force_get_input ( , None ) , \n 
port = self . force_get_input ( , None ) , \n 
database = self . get_input ( ) ) \n 
~~~ engine = create_engine ( url ) \n 
~~~ driver = url . drivername \n 
installed = False \n 
if driver == : \n 
~~~ raise ModuleError ( self , \n 
~~ elif ( driver == or \n 
~~~ installed = install ( { \n 
: } ) \n 
~~ elif driver == : \n 
~~ elif driver == or driver == : \n 
debug . format_exception ( e ) ) \n 
~~ if not installed : \n 
~~ ~~ except SQLAlchemyError : \n 
~~~ raise ModuleError ( \n 
self , \n 
~~ self . set_output ( , engine . connect ( ) ) \n 
~~ def test_query_sqlite3 ( self ) : \n 
import os \n 
import sqlite3 \n 
import tempfile \n 
import urllib2 \n 
from vistrails . tests . utils import execute , intercept_results \n 
identifier = \n 
test_db_fd , test_db = tempfile . mkstemp ( suffix = ) \n 
os . close ( test_db_fd ) \n 
~~~ conn = sqlite3 . connect ( test_db ) \n 
cur = conn . cursor ( ) \n 
cur . execute ( ) \n 
cur . executemany ( , \n 
[ { : , : , : 25 } , \n 
{ : , : , : 21 } ] ) \n 
conn . commit ( ) \n 
conn . close ( ) \n 
source = ( ) \n 
with intercept_results ( DBConnection , , SQLSource , ) as ( connection , ~~~ self . assertFalse ( execute ( [ \n 
( , identifier , [ \n 
( , [ ( , ) ] ) , \n 
( , [ ( , test_db ) ] ) , \n 
] ) , \n 
( , [ ( , urllib2 . quote ( source ) ) ] ) , \n 
] , \n 
[ \n 
( 0 , , 1 , ) , \n 
add_port_specs = [ \n 
( 1 , , , \n 
] ) ) \n 
~~ self . assertEqual ( len ( connection ) , 1 ) \n 
connection [ 0 ] . close ( ) \n 
self . assertEqual ( len ( table ) , 1 ) \n 
self . assertIsNone ( table [ 0 ] ) \n 
table , = table \n 
self . assertEqual ( table . names , [ , , ] ) \n 
self . assertEqual ( ( table . rows , table . columns ) , ( 2 , 3 ) ) \n 
self . assertEqual ( set ( table . get_column ( 1 ) ) , \n 
set ( [ , ] ) ) \n 
~~~ os . remove ( test_db ) \n 
~~ ~~ ~~ def pop ( self ) : \n 
~~~ self . _Stack . pop ( ) \n 
~~ ~~ def top ( self ) : \n 
~~~ return self . _Stack [ - 1 ] \n 
~~ ~~ def get ( self , key ) : \n 
~~~ value = self . cache . pop ( key ) \n 
self . cache [ key ] = value \n 
return value \n 
~~ ~~ def set ( self , key , value ) : \n 
~~~ self . cache . pop ( key ) \n 
~~~ if len ( self . cache ) >= self . capacity : \n 
~~~ self . cache . popitem ( last = False ) \n 
~~ ~~ self . cache [ key ] = value \n 
~~ def check_pid ( pid ) : \n 
~~~ os . kill ( pid , 0 ) \n 
~~~ if err . errno == errno . ESRCH : \n 
~~ elif err . errno == errno . EPERM : \n 
~~ ~~ @ webapi_check_login_required \n 
~~~ def get_queryset ( self , request , is_list = False , local_site = None , \n 
* args , ** kwargs ) : \n 
queryset = self . model . objects . filter ( local_site = local_site ) \n 
if is_list : \n 
~~~ if in request . GET : \n 
~~~ for repo_id in request . GET . get ( ) . split ( ) : \n 
~~~ queryset = queryset . filter ( repository = repo_id ) \n 
~~ ~~ ~~ if in request . GET : \n 
~~~ for username in request . GET . get ( ) . split ( ) : \n 
~~~ queryset = queryset . filter ( people__username = username ) \n 
~~ ~~ if in request . GET : \n 
~~~ for name in request . GET . get ( ) . split ( ) : \n 
~~~ queryset = queryset . filter ( groups__name = name ) \n 
~~ ~~ ~~ return queryset \n 
~~ ~~ @ webapi_check_local_site \n 
~~~ @ webapi_login_required \n 
@ webapi_response_errors ( INVALID_FORM_DATA , NOT_LOGGED_IN , \n 
PERMISSION_DENIED ) \n 
@ webapi_request_fields ( \n 
optional = { \n 
: { \n 
: six . text_type , \n 
: \n 
def update ( self , request , local_site = None , * args , ** kwargs ) : \n 
~~~ default_reviewer = self . get_object ( request , local_site = local_site , \n 
* args , ** kwargs ) \n 
~~~ return DOES_NOT_EXIST \n 
~~ if not self . has_modify_permissions ( request , default_reviewer ) : \n 
~~~ return self . get_no_access_error ( request ) \n 
~~ return self . _create_or_update ( local_site , default_reviewer , ** kwargs ) \n 
~~ ~~ def _create_or_update ( self , local_site , default_reviewer = None , ** kwargs ) : \n 
~~~ invalid_fields = { } \n 
form_data = { } \n 
if in kwargs : \n 
~~~ group_names = kwargs [ ] . split ( ) \n 
group_ids = [ \n 
group [ ] \n 
for group in Group . objects . filter ( \n 
name__in = group_names , local_site = local_site ) . values ( ) \n 
if len ( group_ids ) != len ( group_names ) : \n 
~~~ invalid_fields [ ] = [ \n 
~~ form_data [ ] = group_ids \n 
~~ if in kwargs : \n 
~~~ repo_ids = [ ] \n 
~~~ repo_ids = [ \n 
int ( repo_id ) \n 
for repo_id in kwargs [ ] . split ( ) \n 
~~ if repo_ids : \n 
~~~ found_count = Repository . objects . filter ( \n 
pk__in = repo_ids , local_site = local_site ) . count ( ) \n 
if len ( repo_ids ) != found_count : \n 
~~ ~~ form_data [ ] = repo_ids \n 
~~~ usernames = kwargs [ ] . split ( ) \n 
user_ids = [ \n 
user [ ] \n 
for user in User . objects . filter ( \n 
username__in = usernames ) . values ( ) \n 
if len ( user_ids ) != len ( usernames ) : \n 
~~ form_data [ ] = user_ids \n 
~~ if invalid_fields : \n 
~~~ return INVALID_FORM_DATA , { \n 
: invalid_fields \n 
~~ for field in ( , ) : \n 
~~~ if field in kwargs : \n 
~~~ form_data [ field ] = kwargs [ field ] \n 
~~ ~~ if local_site : \n 
~~~ form_data [ ] = local_site . pk \n 
~~ form = DefaultReviewerForm ( form_data , instance = default_reviewer ) \n 
if not form . is_valid ( ) : \n 
~~~ field_errors = self . _get_form_errors ( form ) \n 
if in field_errors : \n 
~~~ field_errors [ ] = field_errors . pop ( ) \n 
~~ if in field_errors : \n 
~~ return INVALID_FORM_DATA , { \n 
: field_errors , \n 
~~ default_reviewer = form . save ( ) \n 
return 200 , { \n 
self . item_result_key : default_reviewer , \n 
~~ def list_ ( show_all = False , \n 
show_disabled = True , \n 
where = None , \n 
return_yaml = True ) : \n 
schedule = { } \n 
~~~ eventer = salt . utils . event . get_event ( , opts = __opts__ ) \n 
res = __salt__ [ ] ( { : , \n 
: where } , ) \n 
if res : \n 
~~~ event_ret = eventer . get_event ( tag = , wait = 30 ) if event_ret and event_ret [ ] : \n 
~~~ schedule = event_ret [ ] \n 
~~~ ret = { } \n 
ret [ ] = \n 
ret [ ] = True \n 
log . debug ( ) \n 
~~~ if job == : \n 
~~ if job . startswith ( ) and not show_all : \n 
~~~ del schedule [ job ] \n 
~~ if not in schedule [ job ] : \n 
~~~ schedule [ job ] [ ] = True \n 
~~ for item in pycopy . copy ( schedule [ job ] ) : \n 
~~~ if item not in SCHEDULE_CONF : \n 
~~~ del schedule [ job ] [ item ] \n 
~~ if schedule [ job ] [ item ] == : \n 
~~~ schedule [ job ] [ item ] = True \n 
~~~ schedule [ job ] [ item ] = False \n 
~~ ~~ if not show_disabled and not schedule [ job ] [ ] : \n 
~~ if in schedule [ job ] : \n 
~~~ if schedule [ job ] [ ] > 0 : \n 
~~~ schedule [ job ] [ ] = schedule [ job ] [ ] \n 
~~ elif in schedule [ job ] : \n 
~~~ del schedule [ job ] [ ] \n 
~~ del schedule [ job ] [ ] \n 
~~ ~~ if schedule : \n 
~~~ if return_yaml : \n 
~~~ tmp = { : schedule } \n 
yaml_out = yaml . safe_dump ( tmp , default_flow_style = False ) \n 
return yaml_out \n 
~~~ return schedule \n 
~~~ return { : { } } \n 
~~ ~~ def purge ( ** kwargs ) : \n 
ret = { : [ ] , \n 
: True } \n 
for name in list_ ( show_all = True , return_yaml = False ) : \n 
~~~ if name == : \n 
~~ if name . startswith ( ) : \n 
~~ if in kwargs and kwargs [ ] : \n 
ret [ ] . append ( . format ( name ) ) \n 
~~~ persist = True \n 
~~~ persist = kwargs [ ] \n 
res = __salt__ [ ] ( { : name , \n 
: persist } , ) \n 
~~~ event_ret = eventer . get_event ( tag = if event_ret and event_ret [ ] : \n 
~~~ _schedule_ret = event_ret [ ] \n 
if name not in _schedule_ret : \n 
~~~ ret [ ] . append ( . format ( name ret [ ] = True \n 
~~~ ret [ ] = \n 
~~ ~~ ~~ return ret \n 
~~ def delete ( name , ** kwargs ) : \n 
ret = { : . format ( name ) , \n 
: False } \n 
~~~ ret [ ] = . format ( name ) \n 
~~ if name in list_ ( show_all = True , where = , return_yaml = False ) : \n 
~~~ event_data = { : name , : , : persist } \n 
~~ elif name in list_ ( show_all = True , where = , return_yaml = False ) : \n 
~~~ event_data = { : name , : , : , : False } \n 
res = __salt__ [ ] ( event_data , ) \n 
~~~ event_ret = eventer . get_event ( tag = , wait if event_ret and event_ret [ ] : \n 
if name not in schedule : \n 
ret [ ] = . format ( name ) \n 
~~ ~~ return ret \n 
~~ def add ( name , ** kwargs ) : \n 
if name in list_ ( show_all = True , return_yaml = False ) : \n 
ret [ ] = False \n 
~~ if not name : \n 
~~ time_conflict = False \n 
for item in [ , , , ] : \n 
~~~ if item in kwargs and in kwargs : \n 
~~~ time_conflict = True \n 
~~ if item in kwargs and in kwargs : \n 
~~ ~~ if time_conflict : \n 
~~ if in kwargs and in kwargs : \n 
~~ persist = True \n 
~~ _new = build_schedule_item ( name , ** kwargs ) \n 
schedule_data = { } \n 
schedule_data [ name ] = _new \n 
if in kwargs and kwargs [ ] : \n 
: schedule_data , \n 
~~~ event_ret = eventer . get_event ( tag = , wait = if event_ret and event_ret [ ] : \n 
if name in schedule : \n 
~~ def enable_job ( name , ** kwargs ) : \n 
~~ if in __opts__ and __opts__ [ ] : \n 
if name in schedule and schedule [ name ] [ ] : \n 
~~~ ret [ ] = False \n 
~~ def disable_job ( name , ** kwargs ) : \n 
~~ elif name in list_ ( show_all = True , where = ) : \n 
if name in schedule and not schedule [ name ] [ ] : \n 
~~ def save ( ** kwargs ) : \n 
res = __salt__ [ ] ( { : } , ) \n 
~~~ event_ret = eventer . get_event ( tag = , wait = 30 ) \n 
if event_ret and event_ret [ ] : \n 
~~ def enable ( ** kwargs ) : \n 
if in schedule and schedule [ ] : \n 
~~ def disable ( ** kwargs ) : \n 
~~~ event_ret = eventer . get_event ( tag = , if event_ret and event_ret [ ] : \n 
if in schedule and not schedule [ ] : \n 
~~ def open_url ( self , location , data = None , headers = { } , method = None ) : \n 
~~~ if self . is_verbose : \n 
~~ self . last_location = location \n 
~~~ if data != None : \n 
~~~ data = urlencode ( { data : 1 } ) \n 
~~ req = ApiRequest ( location , data , headers , method = method ) \n 
self . url_response = urlopen ( req ) \n 
if data and self . url_response . geturl ( ) != location : \n 
~~~ redirection = % ( location , self . url_response . geturl ( ) ) \n 
self . last_http_error = inst \n 
self . last_status = inst . code \n 
self . last_message = inst . read ( ) \n 
~~ except URLError , inst : \n 
self . last_url_error = inst \n 
if isinstance ( inst . reason , tuple ) : \n 
~~~ self . last_status , self . last_message = inst . reason \n 
~~~ self . last_message = inst . reason \n 
self . last_status = inst . errno \n 
self . last_status = self . url_response . code \n 
self . _print ( % self . last_status ) \n 
self . last_body = self . url_response . read ( ) \n 
self . _print ( % self . last_body ) \n 
self . last_headers = self . url_response . headers \n 
self . _print ( % self . last_headers ) \n 
content_type = self . last_headers [ ] \n 
self . _print ( % content_type ) \n 
is_json_response = False \n 
if in content_type : \n 
~~~ is_json_response = True \n 
~~ if is_json_response : \n 
~~~ self . last_message = self . _loadstr ( self . last_body ) \n 
~~~ self . last_message = self . last_body \n 
~~ self . _print ( % self . last_message ) \n 
~~ ~~ def _loadstr ( self , string ) : \n 
~~~ if string == : \n 
~~~ data = None \n 
~~~ data = json . loads ( string ) \n 
~~ ~~ except __HOLE__ , exception : \n 
raise ValueError , msg \n 
~~ return data \n 
~~~ raw = self . _load_raw_from_source ( source ) \n 
~~~ name = getattr ( source , , ) \n 
raise ConfigError ( . format ( name , e ) ) \n 
~~ if not isinstance ( raw , dict ) : \n 
~~~ message = \n 
raise ConfigError ( message . format ( self . filepath ) ) \n 
~~ for k , v in raw . iteritems ( ) : \n 
~~~ if v is None : \n 
~~ if k == : \n 
~~~ if not isinstance ( v , dict ) : \n 
~~ self . config = v \n 
~~ elif k == : \n 
~~~ self . global_ = AgendaGlobalEntry ( ** v ) \n 
~~~ self . _collect_existing_ids ( v , ) \n 
for s in v : \n 
~~~ if not isinstance ( s , dict ) : \n 
~~ for s in v : \n 
~~~ self . _assign_id_if_needed ( s , ) \n 
self . sections . append ( AgendaSectionEntry ( self , ** s ) ) \n 
~~ ~~ elif k == : \n 
for w in v : \n 
~~~ self . workloads . append ( self . get_workload_entry ( w ) ) \n 
~~ ~~ ~~ def pil_resize ( maxwidth , path_in , path_out = None ) : \n 
path_out = path_out or temp_file_for ( path_in ) \n 
from PIL import Image \n 
log . debug ( , \n 
util . displayable_path ( path_in ) , util . displayable_path ( path_out ) ) \n 
~~~ im = Image . open ( util . syspath ( path_in ) ) \n 
size = maxwidth , maxwidth \n 
im . thumbnail ( size , Image . ANTIALIAS ) \n 
im . save ( path_out ) \n 
return path_out \n 
util . displayable_path ( path_in ) ) \n 
return path_in \n 
~~ ~~ def pil_getsize ( path_in ) : \n 
~~~ from PIL import Image \n 
return im . size \n 
util . displayable_path ( path_in ) , exc ) \n 
~~ ~~ def im_getsize ( path_in ) : \n 
~~~ cmd = [ , , , \n 
util . syspath ( path_in , prefix = False ) ] \n 
~~~ out = util . command_output ( cmd ) \n 
~~ except subprocess . CalledProcessError as exc : \n 
~~~ log . warn ( ) \n 
log . debug ( \n 
exc . returncode , cmd , exc . output . strip ( ) \n 
~~~ return tuple ( map ( int , out . split ( ) ) ) \n 
~~~ log . warn ( , out ) \n 
~~ ~~ def get_im_version ( ) : \n 
~~~ out = util . command_output ( [ , ] ) \n 
if in out . lower ( ) : \n 
match = re . search ( pattern , out ) \n 
~~~ return ( int ( match . group ( 1 ) ) , \n 
int ( match . group ( 2 ) ) , \n 
int ( match . group ( 3 ) ) ) \n 
~~ return ( 0 , ) \n 
~~ ~~ except ( subprocess . CalledProcessError , __HOLE__ ) : \n 
~~ ~~ def get_pil_version ( ) : \n 
~~~ __import__ ( , fromlist = [ str ( ) ] ) \n 
return ( 0 , ) \n 
args = parse_input ( ) \n 
args . lock = True \n 
args . question = [ ] \n 
args . all = False \n 
args . timeout = 0 \n 
args . verbose = False \n 
args . interactive = False \n 
~~~ assign = assignment . load_assignment ( args . config , args ) \n 
msgs = messages . Messages ( ) \n 
lock . protocol ( args , assign ) . run ( msgs ) \n 
~~ except ( ex . LoadingException , ex . SerializeException ) as e : \n 
~~~ log . warning ( , exc_info = True ) \n 
print ( + str ( e ) . strip ( ) ) \n 
~~ except ( __HOLE__ , EOFError ) : \n 
~~~ log . info ( ) \n 
~~~ assign . dump_tests ( ) \n 
~~ ~~ def _delete_test_db ( self ) : \n 
~~~ os . remove ( self . SQLITE_DB_NAME ) \n 
~~ ~~ def test_reserved_words ( self ) : \n 
rec1 = { : { : , : , \n 
: } } \n 
rec2 = { : { : , : , \n 
rec_position1 = self . client . record_create ( class_id1 , rec1 ) \n 
rec_position2 = self . client . record_create ( class_id1 , rec2 ) \n 
res = self . client . command ( sql_edge ) \n 
assert isinstance ( res [ 0 ] . _in , \n 
pyorient . OrientRecordLink ) \n 
assert res [ 0 ] . _in . get_hash ( ) == rec_position2 . _rid \n 
assert isinstance ( res [ 0 ] . _out , pyorient . OrientRecordLink ) \n 
assert res [ 0 ] . _out . get_hash ( ) == rec_position1 . _rid \n 
result = self . client . query ( \n 
assert result [ 0 ] . oRecordData [ ] . get ( ) == \n 
assert result [ 0 ] . rid . get_hash ( ) == rec_position1 . _rid \n 
assert result [ 0 ] . holiday == rec1 [ ] [ ] \n 
assert result [ 0 ] . version != 0 \n 
assert result [ 1 ] . rid . get ( ) == \n 
assert result [ 1 ] . rid . get_hash ( ) == rec_position2 . _rid \n 
assert result [ 1 ] . holiday == rec2 [ ] [ ] \n 
x = self . client . command ( \n 
assert x [ 0 ] . ciao == 1234 \n 
import re \n 
assert re . match ( , x [ 0 ] . _rid ) , ( \n 
print ( x [ 0 ] . _rid ) \n 
assert x [ 0 ] . rid == \n 
~~~ x [ 0 ] . _rid . get_hash ( ) \n 
assert False \n 
~~~ assert True \n 
assert x [ 0 ] . model == \n 
~~ @ extension ( EXT_ASM ) \n 
def asm_hook ( self , node ) : \n 
~~~ try : obj_ext = self . obj_ext \n 
except __HOLE__ : obj_ext = % self . idx \n 
task = self . create_task ( , node , node . change_ext ( obj_ext ) ) \n 
self . compiled_tasks . append ( task ) \n 
self . meths . append ( ) \n 
~~ def loop_template_list ( loop_positions , instance , instance_type , \n 
default_template , registery = { } ) : \n 
templates = [ ] \n 
local_loop_position = loop_positions [ 1 ] \n 
global_loop_position = loop_positions [ 0 ] \n 
instance_string = slugify ( str ( instance ) ) \n 
for key in [ % ( instance_type , instance_string ) , \n 
instance_string , \n 
instance_type , \n 
~~~ templates . append ( registery [ key ] [ global_loop_position ] ) \n 
~~ ~~ templates . append ( \n 
append_position ( default_template , global_loop_position , ) ) \n 
templates . append ( \n 
append_position ( default_template , local_loop_position , ) ) \n 
templates . append ( default_template ) \n 
return templates \n 
~~ def validates_host_edit ( obj ) : \n 
~~~ checker = Checker ( ) \n 
check = True \n 
_ = obj . _ \n 
checker . errors = [ ] \n 
if not is_param ( obj . input , ) : \n 
~~~ check = False \n 
checker . add_error ( _ ( ) ) \n 
~~~ check = checker . check_string ( \n 
obj . input . m_name , \n 
CHECK_EMPTY | CHECK_LENGTH | CHECK_ONLYSPACE , \n 
None , \n 
min = MACHINE_NAME_MIN_LENGTH , \n 
max = MACHINE_NAME_MAX_LENGTH , \n 
) and check \n 
~~ if not is_param ( obj . input , ) : \n 
~~~ if obj . input . m_connect_type == "karesansui" : \n 
~~~ if not is_param ( obj . input , ) : \n 
~~~ m_hostname_parts = obj . input . m_hostname . split ( ":" ) \n 
if len ( m_hostname_parts ) > 2 : \n 
checker . add_error ( _ ( ) % _ ( ) ) \n 
~~~ check = checker . check_domainname ( \n 
m_hostname_parts [ 0 ] , \n 
CHECK_EMPTY | CHECK_LENGTH | CHECK_VALID , \n 
min = FQDN_MIN_LENGTH , \n 
max = FQDN_MAX_LENGTH , \n 
~~~ check = checker . check_number ( \n 
m_hostname_parts [ 1 ] , \n 
CHECK_EMPTY | CHECK_VALID | CHECK_MIN | CHECK_MAX , \n 
PORT_MIN_NUMBER , \n 
PORT_MAX_NUMBER , \n 
~~ ~~ ~~ ~~ if obj . input . m_connect_type == "libvirt" : \n 
~~ if is_param ( obj . input , ) and obj . input . m_auth_user != "" : \n 
~~~ check = checker . check_username ( \n 
obj . input . m_auth_user , \n 
CHECK_LENGTH | CHECK_ONLYSPACE , \n 
min = USER_MIN_LENGTH , \n 
max = USER_MAX_LENGTH , \n 
~~ ~~ ~~ if is_param ( obj . input , ) : \n 
obj . input . note_title , \n 
min = NOTE_TITLE_MIN_LENGTH , \n 
max = NOTE_TITLE_MAX_LENGTH , \n 
~~ if is_param ( obj . input , ) : \n 
obj . input . note_value , \n 
CHECK_ONLYSPACE , \n 
~~~ for tag in comma_split ( obj . input . tags ) : \n 
tag , \n 
min = TAG_MIN_LENGTH , \n 
max = TAG_MAX_LENGTH , \n 
~~ ~~ obj . view . alert = checker . errors \n 
return check \n 
~~~ def modelXbrl ( self ) : \n 
~~~ return self . modelDocument . modelXbrl \n 
~~ ~~ ~~ @ property \n 
~~~ def localName ( self ) : \n 
~~~ return self . _localName \n 
~~~ self . setNamespaceLocalName ( ) \n 
return self . _localName \n 
~~~ def prefixedName ( self ) : \n 
~~~ return self . _prefixedName \n 
return self . _prefixedName \n 
~~~ def namespaceURI ( self ) : \n 
~~~ return self . _namespaceURI \n 
return self . _namespaceURI \n 
~~~ def qname ( self ) : \n 
~~~ return self . _elementQname \n 
~~~ self . _elementQname = QName ( self . prefix , self . namespaceURI , self . localName ) \n 
return self . _elementQname \n 
~~~ def elementQname ( self ) : \n 
~~~ self . _elementQname = qname ( self ) \n 
~~~ def parentQname ( self ) : \n 
~~~ return self . _parentQname \n 
~~~ parentObj = self . getparent ( ) \n 
self . _parentQname = parentObj . elementQname if parentObj is not None else None \n 
return self . _parentQname \n 
~~ ~~ ~~ def resize ( self , size = None ) : \n 
if not self . operation . israw ( ) : \n 
~~ size = size or tty . size ( self . operation . stdout ) \n 
if size is not None : \n 
~~~ rows , cols = size \n 
~~~ self . operation . resize ( height = rows , width = cols ) \n 
~~ ~~ ~~ def has_module ( module ) : \n 
~~~ __import__ ( module ) \n 
~~ ~~ def get_backup_log_configuration_dict ( environment , databaseinfra ) : \n 
~~~ from backup . models import LogConfiguration \n 
from django . core . exceptions import ObjectDoesNotExist \n 
~~~ log_configuration = LogConfiguration . objects . get ( environment = environment , \n 
engine_type = databaseinfra . engine . engine_type ~~ except __HOLE__ : \n 
: log_configuration . mount_point_path , \n 
: log_configuration . filer_path , \n 
: log_configuration . retention_days , \n 
: log_configuration . log_path , \n 
: log_configuration . backup_log_script , \n 
: log_configuration . config_backup_log_script , \n 
: log_configuration . clean_backup_log_script , \n 
: databaseinfra . name , \n 
: log_configuration . cron_minute , \n 
: log_configuration . cron_hour , \n 
~~ def dist ( ) : \n 
~~~ check_call ( [ , \n 
. format ( DIST_DIR ) , \n 
~~~ cleanup ( ) \n 
sys . exit ( e ) \n 
~~ ~~ def group_and_bridge ( kwargs ) : \n 
bridge = kwargs . pop ( "bridge" , None ) \n 
if bridge : \n 
~~~ group = bridge . get_group ( ** kwargs ) \n 
~~~ group = None \n 
~~ return group , bridge \n 
~~ def password_reset_from_key ( request , uidb36 , key , ** kwargs ) : \n 
~~~ form_class = kwargs . get ( "form_class" , ResetPasswordKeyForm ) \n 
template_name = kwargs . get ( \n 
"template_name" , "account/password_reset_from_key.html" ) \n 
token_generator = kwargs . get ( "token_generator" , default_token_generator ) \n 
group , bridge = group_and_bridge ( kwargs ) \n 
ctx = group_context ( group , bridge ) \n 
~~ user = get_object_or_404 ( User , id = uid_int ) \n 
if token_generator . check_token ( user , key ) : \n 
~~~ if request . method == "POST" : \n 
~~~ password_reset_key_form = form_class ( \n 
request . POST , user = user , temp_key = key ) \n 
if password_reset_key_form . is_valid ( ) : \n 
~~~ password_reset_key_form . save ( ) \n 
messages . add_message ( request , messages . SUCCESS , \n 
ugettext ( \n 
password_reset_key_form = None \n 
~~~ password_reset_key_form = form_class ( ) \n 
~~ ctx . update ( { \n 
"form" : password_reset_key_form , \n 
~~~ ctx . update ( { \n 
"token_fail" : True , \n 
~~ return render_to_response ( template_name , RequestContext ( request , ctx ) ) \n 
~~ def cleanup ( logger , * args ) : \n 
for obj in args : \n 
~~~ if obj is not None and hasattr ( obj , ) : \n 
~~~ obj . cleanup ( ) \n 
~~ ~~ ~~ ~~ def will_ttype ( self , option ) : \n 
options = self . protocol . protocol_flags . get ( ) \n 
if options and options . get ( ) or self . ttype_step > 3 : \n 
~~~ option = "" . join ( option ) . lstrip ( IS ) \n 
~~ if self . ttype_step == 0 : \n 
~~~ self . protocol . requestNegotiation ( TTYPE , SEND ) \n 
~~ elif self . ttype_step == 1 : \n 
~~~ clientname = option . upper ( ) \n 
xterm256 = False \n 
if clientname . startswith ( "MUDLET" ) : \n 
~~~ xterm256 = clientname . split ( "MUDLET" , 1 ) [ 1 ] . strip ( ) >= "1.1" \n 
~~~ xterm256 = ( clientname . startswith ( "XTERM" ) or \n 
clientname . endswith ( "-256COLOR" ) or \n 
~~ self . protocol . protocol_flags [ ] [ ] = True \n 
self . protocol . protocol_flags [ ] [ ] = xterm256 \n 
self . protocol . protocol_flags [ ] [ ] = clientname \n 
self . protocol . requestNegotiation ( TTYPE , SEND ) \n 
~~ elif self . ttype_step == 2 : \n 
~~~ term = option \n 
not term . endswith ( "-color" ) ) \n 
if xterm256 : \n 
~~~ self . protocol . protocol_flags [ ] [ ] = True \n 
~~ self . protocol . protocol_flags [ ] [ ] = term \n 
~~ elif self . ttype_step == 3 : \n 
~~~ if option . startswith ( "MTTS" ) : \n 
if option . isdigit ( ) : \n 
~~~ option = int ( option ) \n 
support = dict ( ( capability , True ) for bitval , capability in MTTS if option & bitval self . protocol . protocol_flags [ ] . update ( support ) \n 
~~~ self . protocol . protocol_flags [ ] [ option . upper ( ) ] = True \n 
~~ ~~ self . protocol . protocol_flags [ ] [ ] = True \n 
self . protocol . handshake_done ( ) \n 
~~ self . ttype_step += 1 \n 
~~ def get_parent_language_code ( parent_object ) : \n 
if parent_object is None : \n 
~~~ return parent_object . get_current_language ( ) \n 
~~~ return parent_object . language_code \n 
~~ def get_parent_active_language_choices ( parent_object , exclude_current = False ) : \n 
from . db import ContentItem \n 
qs = ContentItem . objects . parent ( parent_object , limit_parent_language = False ) . values_list ( , flat = True ) . distinct ( ) \n 
languages = set ( qs ) \n 
if exclude_current : \n 
~~~ parent_lang = get_parent_language_code ( parent_object ) \n 
~~~ languages . remove ( parent_lang ) \n 
~~ ~~ choices = [ ( lang , str ( get_language_title ( lang ) ) ) for lang in languages if lang ] \n 
choices . sort ( key = lambda tup : tup [ 1 ] ) \n 
return choices \n 
~~ def test_it_allows_an_rds_instance_with_iops ( self ) : \n 
~~~ long_number = long ( 2000 ) \n 
~~~ long_number = 2000 \n 
~~ rds_instance = rds . DBInstance ( \n 
AllocatedStorage = 200 , \n 
DBInstanceClass = , \n 
Engine = , \n 
MasterUsername = , \n 
MasterUserPassword = , \n 
StorageType = , \n 
Iops = long_number , \n 
rds_instance . JSONrepr ( ) \n 
~~ def testFunctionality ( self ) : \n 
np . random . seed ( 3141592 ) \n 
b01 = np . zeros ( ( 3 , 6 ) ) \n 
b02 = np . zeros ( ( 3 , 6 ) ) \n 
b01 [ 1 : 3 , 2 : 6 ] = [ [ 0.4 , - 0.2 , 0.3 , 0.0 ] , \n 
[ - 0.7 , 0.0 , 0.9 , 0.0 ] ] \n 
b02 [ 0 : 3 , 2 : 6 ] = [ [ 0.4 , 0.0 , 0.0 , 0.0 ] , \n 
[ 0.4 , 0.0 , 0.4 , 0.0 ] , \n 
[ 0.0 , 0.0 , 0.4 , 0.0 ] ] \n 
m0 = b01 . shape [ 0 ] \n 
cl = np . array ( [ 0 , 1 , 0 , 1 , 0 , 0 , 1 , 1 , 1 , 0 ] ) \n 
l = 200 \n 
t = len ( cl ) \n 
noisefunc = lambda : np . random . normal ( size = ( 1 , m0 ) ) ** 3 / 1e3 \n 
var = VAR ( 2 ) \n 
var . coef = b01 \n 
sources1 = var . simulate ( [ l , sum ( cl == 0 ) ] , noisefunc ) \n 
var . coef = b02 \n 
sources2 = var . simulate ( [ l , sum ( cl == 1 ) ] , noisefunc ) \n 
var . fit ( sources1 ) \n 
var . fit ( sources2 ) \n 
sources = np . zeros ( ( t , m0 , l ) ) \n 
sources [ cl == 0 , : , : ] = sources1 \n 
sources [ cl == 1 , : , : ] = sources2 \n 
mix = [ [ 0.5 , 1.0 , 0.5 , 0.2 , 0.0 , 0.0 , 0.0 ] , \n 
[ 0.0 , 0.2 , 0.5 , 1.0 , 0.5 , 0.2 , 0.0 ] , \n 
[ 0.0 , 0.0 , 0.0 , 0.2 , 0.5 , 1.0 , 0.5 ] ] \n 
data = datatools . dot_special ( np . transpose ( mix ) , sources ) \n 
for backend_name , backend_gen in scot . backend . items ( ) : \n 
~~~ np . random . seed ( 3141592 ) \n 
api = scot . Workspace ( { : 2 } , reducedim = 3 , backend = backend_gen ( ) ) \n 
api . set_data ( data ) \n 
api . do_ica ( ) \n 
self . assertEqual ( api . mixing_ . shape , ( 3 , 7 ) ) \n 
self . assertEqual ( api . unmixing_ . shape , ( 7 , 3 ) ) \n 
api . do_mvarica ( ) \n 
self . assertEqual ( api . get_connectivity ( ) . shape , ( 3 , 3 , 512 ) ) \n 
self . assertFalse ( np . any ( np . isnan ( api . activations_ ) ) ) \n 
self . assertFalse ( np . any ( np . isinf ( api . activations_ ) ) ) \n 
api . fit_var ( ) \n 
self . assertEqual ( api . get_tf_connectivity ( , 100 , 50 ) . shape , ( 3 , 3 , 512 , ( l - 100 ) // 50 ) ) \n 
self . assertTrue ( np . allclose ( tfc1 , tfc2 ) ) \n 
self . assertTrue ( np . allclose ( tfc3 , tfc4 ) ) \n 
self . assertTrue ( np . allclose ( tfc5 , tfc6 , rtol = 1e-05 , atol = 1e-06 ) ) \n 
api . set_data ( data , cl ) \n 
self . assertFalse ( np . any ( np . isnan ( api . data_ ) ) ) \n 
self . assertFalse ( np . any ( np . isinf ( api . data_ ) ) ) \n 
api . do_cspvarica ( ) \n 
for c in np . unique ( cl ) : \n 
~~~ api . set_used_labels ( [ c ] ) \n 
fc = api . get_connectivity ( ) \n 
self . assertEqual ( fc . shape , ( 3 , 3 , 512 ) ) \n 
tfc = api . get_tf_connectivity ( , 100 , 50 ) \n 
self . assertEqual ( tfc . shape , ( 3 , 3 , 512 , ( l - 100 ) // 50 ) ) \n 
~~ api . set_data ( data ) \n 
api . remove_sources ( [ 0 , 2 ] ) \n 
self . assertEqual ( api . get_connectivity ( ) . shape , ( 1 , 1 , 512 ) ) \n 
self . assertEqual ( api . get_tf_connectivity ( , 100 , 50 ) . shape , ( 1 , 1 , 512 , ( l - 100 ) // 50 ) ) \n 
~~~ api . optimize_var ( ) \n 
~~ api . fit_var ( ) \n 
~~ ~~ def convert_to_py2 ( ) : \n 
~~~ if source_dir == and not PY2_CONVERTED : \n 
~~~ subprocess . check_output ( [ , ] ) \n 
subprocess . check_output ( [ , ] ) \n 
~~ if not os . path . exists ( os . path . join ( source_dir , ) ) : \n 
~~~ raise ImportError ( \n 
~~~ converter = os . path . dirname ( os . path . realpath ( __file__ ) ) + \n 
subprocess . check_call ( [ converter ] ) \n 
global PY2_CONVERTED \n 
PY2_CONVERTED = True \n 
~~ ~~ ~~ def setFeatureContainer ( self , c ) : \n 
~~~ self . _grid . removeAllComponents ( ) \n 
features = c . getItemIds ( ) \n 
rootSet = CssLayout ( ) \n 
rootTitle = None \n 
highlightRow = CssLayout ( ) \n 
highlightRow . setStyleName ( ) \n 
sampleCount = 0 \n 
for f in features : \n 
~~~ if isinstance ( f , FeatureSet ) : \n 
~~~ if c . isRoot ( f ) : \n 
~~~ if rootTitle is not None : \n 
~~~ rootTitle . setValue ( ( + str ( sampleCount ) \n 
+ + rootTitle . getValue ( ) ) ) \n 
~~ desc = f . getDescription ( ) \n 
~~~ idx = desc . index ( "." ) \n 
~~ rootTitle = Label ( "<h2>" \n 
+ f . getName ( ) \n 
+ "</h2><span>" \n 
+ desc [ : idx + 1 ] \n 
+ "</span>" , Label . CONTENT_XHTML ) \n 
rootTitle . setSizeUndefined ( ) \n 
if f . getRelatedFeatures ( ) is not None : \n 
~~~ rootTitle . setValue ( \n 
+ len ( f . getRelatedFeatures ( ) ) \n 
+ rootTitle . getValue ( ) ) \n 
~~ rootSet = CssLayout ( ) \n 
rootSet . setStyleName ( ) \n 
rootTitle . setStyleName ( ) \n 
self . _grid . addComponent ( rootTitle ) \n 
self . _grid . addComponent ( rootSet ) \n 
~~~ sampleCount += 1 \n 
resId = + f . getIconName ( ) \n 
res = self . _app . getSampleIcon ( resId ) \n 
if rootSet . getParent ( ) is None : \n 
~~~ if rootTitle is None : \n 
~~~ parent = self . _app . _allFeatures . getParent ( f ) \n 
rootTitle = Label ( "<h2>" + parent . getName ( ) \n 
+ "</h2>" , Label . CONTENT_XHTML ) \n 
if parent . getDescription ( ) is not None : \n 
~~~ desc = Label ( parent . getDescription ( ) , \n 
Label . CONTENT_XHTML ) \n 
desc . setStyleName ( ) \n 
desc . setSizeUndefined ( ) \n 
self . _grid . addComponent ( desc ) \n 
~~ ~~ if sampleCount % 2 == 1 : \n 
~~~ highlightRow = CssLayout ( ) \n 
self . _grid . addComponent ( highlightRow ) \n 
~~ l = CssLayout ( ) \n 
l . setStyleName ( ) \n 
er = ExternalResource ( + f . getFragmentName ( ) ) \n 
sample = ActiveLink ( f . getName ( ) , er ) \n 
sample . setIcon ( res ) \n 
l . addComponent ( sample ) \n 
if ( f . getDescription ( ) is not None \n 
and f . getDescription ( ) != ) : \n 
~~~ d = f . getDescription ( ) \n 
desc = Label ( d [ : d . index ( "." ) + 1 ] , Label . CONTENT_XHTML ) \n 
l . addComponent ( desc ) \n 
~~ highlightRow . addComponent ( l ) \n 
~~~ sample = ActiveLink ( f . getName ( ) , \n 
ExternalResource ( + f . getFragmentName ( ) ) ) \n 
sample . setStyleName ( BaseTheme . BUTTON_LINK ) \n 
sample . addStyleName ( ) \n 
~~~ desc = f . getDescription ( ) \n 
~~~ idx = desc . index ( ) \n 
~~ sample . setDescription ( desc [ : idx + 1 ] ) \n 
~~ sample . setIcon ( res ) \n 
rootSet . addComponent ( sample ) \n 
~~ ~~ ~~ if rootTitle is not None : \n 
~~~ rootTitle . setValue ( + str ( sampleCount ) + \n 
~~ ~~ def __init__ ( self , text , created_at = None , source = None ) : \n 
~~~ if text : \n 
~~~ self . text = text \n 
~~ if created_at is None : \n 
~~~ created_at = datetime . now ( tzlocal ( ) ) \n 
~~~ self . created_at = created_at . replace ( microsecond = 0 ) \n 
~~ self . source = source \n 
~~ def ParseApiConfigResponse ( self , body ) : \n 
~~~ response_obj = json . loads ( body ) \n 
~~ except ValueError , unused_err : \n 
~~~ logging . error ( , \n 
body ) \n 
~~~ self . _AddDiscoveryConfig ( ) \n 
for api_config_json in response_obj . get ( , [ ] ) : \n 
~~~ config = json . loads ( api_config_json ) \n 
~~ except __HOLE__ , unused_err : \n 
api_config_json ) \n 
~~~ lookup_key = config . get ( , ) , config . get ( , ) \n 
self . configs [ lookup_key ] = config \n 
~~ ~~ for config in self . configs . itervalues ( ) : \n 
~~~ version = config . get ( , ) \n 
sorted_methods = self . _GetSortedMethods ( config . get ( , { } ) ) \n 
~~ ~~ ~~ 
