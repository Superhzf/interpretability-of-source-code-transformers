Loading model: bert-base-uncased
Reading input corpus
Preparing output file
Extracting representations from model
Sentence         : "\n"
Original    (001): ['\\n']
Tokenized   (004): ['[CLS]', '\\', 'n', '[SEP]']
Filtered   (002): ['\\', 'n']
Detokenized (001): ['\\n']
Counter: 2
===================================================================
Hidden states:  (13, 1, 768)
# Extracted words:  1
Sentence         : "# \n"
Original    (002): ['#', '\\n']
Tokenized   (005): ['[CLS]', '#', '\\', 'n', '[SEP]']
Filtered   (003): ['#', '\\', 'n']
Detokenized (002): ['#', '\\n']
Counter: 3
===================================================================
Hidden states:  (13, 2, 768)
# Extracted words:  2
Sentence         : "template_name = \n"
Original    (003): ['template_name', '=', '\\n']
Tokenized   (008): ['[CLS]', 'template', '_', 'name', '=', '\\', 'n', '[SEP]']
Filtered   (006): ['template', '_', 'name', '=', '\\', 'n']
Detokenized (003): ['template_name', '=', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "slug = "policy_profile" \n"
Original    (004): ['slug', '=', '"policy_profile"', '\\n']
Tokenized   (011): ['[CLS]', 'slug', '=', '"', 'policy', '_', 'profile', '"', '\\', 'n', '[SEP]']
Filtered   (009): ['slug', '=', '"', 'policy', '_', 'profile', '"', '\\', 'n']
Detokenized (004): ['slug', '=', '"policy_profile"', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "preload = False \n"
Original    (004): ['preload', '=', 'False', '\\n']
Tokenized   (008): ['[CLS]', 'pre', '##load', '=', 'false', '\\', 'n', '[SEP]']
Filtered   (006): ['pre', '##load', '=', 'false', '\\', 'n']
Detokenized (004): ['pre##load', '=', 'false', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "tabs = ( NetworkProfileTab , PolicyProfileTab ) \n"
Original    (008): ['tabs', '=', '(', 'NetworkProfileTab', ',', 'PolicyProfileTab', ')', '\\n']
Tokenized   (020): ['[CLS]', 'tab', '##s', '=', '(', 'network', '##pro', '##fi', '##let', '##ab', ',', 'policy', '##pro', '##fi', '##let', '##ab', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['tab', '##s', '=', '(', 'network', '##pro', '##fi', '##let', '##ab', ',', 'policy', '##pro', '##fi', '##let', '##ab', ')', '\\', 'n']
Detokenized (008): ['tab##s', '=', '(', 'network##pro##fi##let##ab', ',', 'policy##pro##fi##let##ab', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "weak_store = WeakLocal ( ) \n"
Original    (006): ['weak_store', '=', 'WeakLocal', '(', ')', '\\n']
Tokenized   (013): ['[CLS]', 'weak', '_', 'store', '=', 'weak', '##lo', '##cal', '(', ')', '\\', 'n', '[SEP]']
Filtered   (011): ['weak', '_', 'store', '=', 'weak', '##lo', '##cal', '(', ')', '\\', 'n']
Detokenized (006): ['weak_store', '=', 'weak##lo##cal', '(', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "strong_store = corolocal . local \n"
Original    (006): ['strong_store', '=', 'corolocal', '.', 'local', '\\n']
Tokenized   (014): ['[CLS]', 'strong', '_', 'store', '=', 'co', '##rol', '##oca', '##l', '.', 'local', '\\', 'n', '[SEP]']
Filtered   (012): ['strong', '_', 'store', '=', 'co', '##rol', '##oca', '##l', '.', 'local', '\\', 'n']
Detokenized (006): ['strong_store', '=', 'co##rol##oca##l', '.', 'local', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "eventlet . monkey_patch ( ) \n"
Original    (006): ['eventlet', '.', 'monkey_patch', '(', ')', '\\n']
Tokenized   (012): ['[CLS]', 'event', '##let', '.', 'monkey', '_', 'patch', '(', ')', '\\', 'n', '[SEP]']
Filtered   (010): ['event', '##let', '.', 'monkey', '_', 'patch', '(', ')', '\\', 'n']
Detokenized (006): ['event##let', '.', 'monkey_patch', '(', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "CONF . register_opts ( impl_zmq . zmq_opts ) \n"
Original    (009): ['CONF', '.', 'register_opts', '(', 'impl_zmq', '.', 'zmq_opts', ')', '\\n']
Tokenized   (026): ['[CLS]', 'con', '##f', '.', 'register', '_', 'opt', '##s', '(', 'imp', '##l', '_', 'z', '##m', '##q', '.', 'z', '##m', '##q', '_', 'opt', '##s', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['con', '##f', '.', 'register', '_', 'opt', '##s', '(', 'imp', '##l', '_', 'z', '##m', '##q', '.', 'z', '##m', '##q', '_', 'opt', '##s', ')', '\\', 'n']
Detokenized (009): ['con##f', '.', 'register_opt##s', '(', 'imp##l_z##m##q', '.', 'z##m##q_opt##s', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "vpnservices_dict = { : self . api_vpnservices . list ( ) } \n"
Original    (013): ['vpnservices_dict', '=', '{', ':', 'self', '.', 'api_vpnservices', '.', 'list', '(', ')', '}', '\\n']
Tokenized   (029): ['[CLS]', 'vp', '##nse', '##r', '##vic', '##es', '_', 'di', '##ct', '=', '{', ':', 'self', '.', 'api', '_', 'vp', '##nse', '##r', '##vic', '##es', '.', 'list', '(', ')', '}', '\\', 'n', '[SEP]']
Filtered   (027): ['vp', '##nse', '##r', '##vic', '##es', '_', 'di', '##ct', '=', '{', ':', 'self', '.', 'api', '_', 'vp', '##nse', '##r', '##vic', '##es', '.', 'list', '(', ')', '}', '\\', 'n']
Detokenized (013): ['vp##nse##r##vic##es_di##ct', '=', '{', ':', 'self', '.', 'api_vp##nse##r##vic##es', '.', 'list', '(', ')', '}', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "vpnservice [ ] [ ] ) \n"
Original    (007): ['vpnservice', '[', ']', '[', ']', ')', '\\n']
Tokenized   (014): ['[CLS]', 'vp', '##nse', '##r', '##vic', '##e', '[', ']', '[', ']', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['vp', '##nse', '##r', '##vic', '##e', '[', ']', '[', ']', ')', '\\', 'n']
Detokenized (007): ['vp##nse##r##vic##e', '[', ']', '[', ']', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "form_data } ) . AndReturn ( ipsecsiteconnection ) \n"
Original    (009): ['form_data', '}', ')', '.', 'AndReturn', '(', 'ipsecsiteconnection', ')', '\\n']
Tokenized   (021): ['[CLS]', 'form', '_', 'data', '}', ')', '.', 'andre', '##turn', '(', 'ip', '##se', '##cs', '##ite', '##con', '##ne', '##ction', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['form', '_', 'data', '}', ')', '.', 'andre', '##turn', '(', 'ip', '##se', '##cs', '##ite', '##con', '##ne', '##ction', ')', '\\', 'n']
Detokenized (009): ['form_data', '}', ')', '.', 'andre##turn', '(', 'ip##se##cs##ite##con##ne##ction', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "ipsecsiteconnections_dict ) \n"
Original    (003): ['ipsecsiteconnections_dict', ')', '\\n']
Tokenized   (015): ['[CLS]', 'ip', '##se', '##cs', '##ite', '##con', '##ne', '##ctions', '_', 'di', '##ct', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['ip', '##se', '##cs', '##ite', '##con', '##ne', '##ctions', '_', 'di', '##ct', ')', '\\', 'n']
Detokenized (003): ['ip##se##cs##ite##con##ne##ctions_di##ct', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "ipsecsiteconnections [ ] ) : \n"
Original    (006): ['ipsecsiteconnections', '[', ']', ')', ':', '\\n']
Tokenized   (015): ['[CLS]', 'ip', '##se', '##cs', '##ite', '##con', '##ne', '##ctions', '[', ']', ')', ':', '\\', 'n', '[SEP]']
Filtered   (013): ['ip', '##se', '##cs', '##ite', '##con', '##ne', '##ctions', '[', ']', ')', ':', '\\', 'n']
Detokenized (006): ['ip##se##cs##ite##con##ne##ctions', '[', ']', ')', ':', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "neutronclient . show_ipsec_site_connection ( \n"
Original    (005): ['neutronclient', '.', 'show_ipsec_site_connection', '(', '\\n']
Tokenized   (018): ['[CLS]', 'neutron', '##cl', '##ient', '.', 'show', '_', 'ip', '##se', '##c', '_', 'site', '_', 'connection', '(', '\\', 'n', '[SEP]']
Filtered   (016): ['neutron', '##cl', '##ient', '.', 'show', '_', 'ip', '##se', '##c', '_', 'site', '_', 'connection', '(', '\\', 'n']
Detokenized (005): ['neutron##cl##ient', '.', 'show_ip##se##c_site_connection', '(', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "ret_val = api . vpn . ipsecsiteconnection_get ( self . request , \n"
Original    (013): ['ret_val', '=', 'api', '.', 'vpn', '.', 'ipsecsiteconnection_get', '(', 'self', '.', 'request', ',', '\\n']
Tokenized   (028): ['[CLS]', 're', '##t', '_', 'val', '=', 'api', '.', 'vp', '##n', '.', 'ip', '##se', '##cs', '##ite', '##con', '##ne', '##ction', '_', 'get', '(', 'self', '.', 'request', ',', '\\', 'n', '[SEP]']
Filtered   (026): ['re', '##t', '_', 'val', '=', 'api', '.', 'vp', '##n', '.', 'ip', '##se', '##cs', '##ite', '##con', '##ne', '##ction', '_', 'get', '(', 'self', '.', 'request', ',', '\\', 'n']
Detokenized (013): ['re##t_val', '=', 'api', '.', 'vp##n', '.', 'ip##se##cs##ite##con##ne##ction_get', '(', 'self', '.', 'request', ',', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "response_kwargs . setdefault ( "filename" , "usage.csv" ) \n"
Original    (009): ['response_kwargs', '.', 'setdefault', '(', '"filename"', ',', '"usage.csv"', ')', '\\n']
Tokenized   (027): ['[CLS]', 'response', '_', 'kw', '##ar', '##gs', '.', 'set', '##de', '##fa', '##ult', '(', '"', 'file', '##name', '"', ',', '"', 'usage', '.', 'cs', '##v', '"', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['response', '_', 'kw', '##ar', '##gs', '.', 'set', '##de', '##fa', '##ult', '(', '"', 'file', '##name', '"', ',', '"', 'usage', '.', 'cs', '##v', '"', ')', '\\', 'n']
Detokenized (009): ['response_kw##ar##gs', '.', 'set##de##fa##ult', '(', '"file##name"', ',', '"usage.cs##v"', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "BlendProbes = 1 \n"
Original    (004): ['BlendProbes', '=', '1', '\\n']
Tokenized   (009): ['[CLS]', 'blend', '##pro', '##bes', '=', '1', '\\', 'n', '[SEP]']
Filtered   (007): ['blend', '##pro', '##bes', '=', '1', '\\', 'n']
Detokenized (004): ['blend##pro##bes', '=', '1', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "lightmap_index = field ( "m_LightmapIndex" ) \n"
Original    (007): ['lightmap_index', '=', 'field', '(', '"m_LightmapIndex"', ')', '\\n']
Tokenized   (022): ['[CLS]', 'light', '##ma', '##p', '_', 'index', '=', 'field', '(', '"', 'm', '_', 'light', '##ma', '##pin', '##de', '##x', '"', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['light', '##ma', '##p', '_', 'index', '=', 'field', '(', '"', 'm', '_', 'light', '##ma', '##pin', '##de', '##x', '"', ')', '\\', 'n']
Detokenized (007): ['light##ma##p_index', '=', 'field', '(', '"m_light##ma##pin##de##x"', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "receive_shadows = field ( "m_ReceiveShadows" , bool ) \n"
Original    (009): ['receive_shadows', '=', 'field', '(', '"m_ReceiveShadows"', ',', 'bool', ')', '\\n']
Tokenized   (021): ['[CLS]', 'receive', '_', 'shadows', '=', 'field', '(', '"', 'm', '_', 'receives', '##had', '##ows', '"', ',', 'boo', '##l', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['receive', '_', 'shadows', '=', 'field', '(', '"', 'm', '_', 'receives', '##had', '##ows', '"', ',', 'boo', '##l', ')', '\\', 'n']
Detokenized (009): ['receive_shadows', '=', 'field', '(', '"m_receives##had##ows"', ',', 'boo##l', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "Config . parser . readfp ( sconf ) \n"
Original    (009): ['Config', '.', 'parser', '.', 'readfp', '(', 'sconf', ')', '\\n']
Tokenized   (018): ['[CLS]', 'con', '##fi', '##g', '.', 'par', '##ser', '.', 'read', '##fp', '(', 'sc', '##on', '##f', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['con', '##fi', '##g', '.', 'par', '##ser', '.', 'read', '##fp', '(', 'sc', '##on', '##f', ')', '\\', 'n']
Detokenized (009): ['con##fi##g', '.', 'par##ser', '.', 'read##fp', '(', 'sc##on##f', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "BBS_XMPP_CERT_FILE = BBS_ROOT + "xmpp.crt" \n"
Original    (006): ['BBS_XMPP_CERT_FILE', '=', 'BBS_ROOT', '+', '"xmpp.crt"', '\\n']
Tokenized   (029): ['[CLS]', 'bb', '##s', '_', 'x', '##mp', '##p', '_', 'ce', '##rt', '_', 'file', '=', 'bb', '##s', '_', 'root', '+', '"', 'x', '##mp', '##p', '.', 'cr', '##t', '"', '\\', 'n', '[SEP]']
Filtered   (027): ['bb', '##s', '_', 'x', '##mp', '##p', '_', 'ce', '##rt', '_', 'file', '=', 'bb', '##s', '_', 'root', '+', '"', 'x', '##mp', '##p', '.', 'cr', '##t', '"', '\\', 'n']
Detokenized (006): ['bb##s_x##mp##p_ce##rt_file', '=', 'bb##s_root', '+', '"x##mp##p.cr##t"', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "BOARDS_FILE = BBS_ROOT + \n"
Original    (005): ['BOARDS_FILE', '=', 'BBS_ROOT', '+', '\\n']
Tokenized   (013): ['[CLS]', 'boards', '_', 'file', '=', 'bb', '##s', '_', 'root', '+', '\\', 'n', '[SEP]']
Filtered   (011): ['boards', '_', 'file', '=', 'bb', '##s', '_', 'root', '+', '\\', 'n']
Detokenized (005): ['boards_file', '=', 'bb##s_root', '+', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "USHM_SIZE = MAXACTIVE + 10 \n"
Original    (006): ['USHM_SIZE', '=', 'MAXACTIVE', '+', '10', '\\n']
Tokenized   (013): ['[CLS]', 'us', '##hm', '_', 'size', '=', 'max', '##active', '+', '10', '\\', 'n', '[SEP]']
Filtered   (011): ['us', '##hm', '_', 'size', '=', 'max', '##active', '+', '10', '\\', 'n']
Detokenized (006): ['us##hm_size', '=', 'max##active', '+', '10', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "UTMP_HASHSIZE = USHM_SIZE * 4 \n"
Original    (006): ['UTMP_HASHSIZE', '=', 'USHM_SIZE', '*', '4', '\\n']
Tokenized   (017): ['[CLS]', 'ut', '##mp', '_', 'hash', '##si', '##ze', '=', 'us', '##hm', '_', 'size', '*', '4', '\\', 'n', '[SEP]']
Filtered   (015): ['ut', '##mp', '_', 'hash', '##si', '##ze', '=', 'us', '##hm', '_', 'size', '*', '4', '\\', 'n']
Detokenized (006): ['ut##mp_hash##si##ze', '=', 'us##hm_size', '*', '4', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "SESSION_TIMEOUT = datetime . timedelta ( 30 ) \n"
Original    (009): ['SESSION_TIMEOUT', '=', 'datetime', '.', 'timedelta', '(', '30', ')', '\\n']
Tokenized   (018): ['[CLS]', 'session', '_', 'time', '##out', '=', 'date', '##time', '.', 'timed', '##elt', '##a', '(', '30', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['session', '_', 'time', '##out', '=', 'date', '##time', '.', 'timed', '##elt', '##a', '(', '30', ')', '\\', 'n']
Detokenized (009): ['session_time##out', '=', 'date##time', '.', 'timed##elt##a', '(', '30', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "SESSION_TIMEOUT_SECONDS = 86400 * 30 \n"
Original    (006): ['SESSION_TIMEOUT_SECONDS', '=', '86400', '*', '30', '\\n']
Tokenized   (015): ['[CLS]', 'session', '_', 'time', '##out', '_', 'seconds', '=', '86', '##400', '*', '30', '\\', 'n', '[SEP]']
Filtered   (013): ['session', '_', 'time', '##out', '_', 'seconds', '=', '86', '##400', '*', '30', '\\', 'n']
Detokenized (006): ['session_time##out_seconds', '=', '86##400', '*', '30', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "MAX_ATTACHSIZE = 20 * 1024 * 1024 \n"
Original    (008): ['MAX_ATTACHSIZE', '=', '20', '*', '1024', '*', '1024', '\\n']
Tokenized   (017): ['[CLS]', 'max', '_', 'attach', '##si', '##ze', '=', '20', '*', '102', '##4', '*', '102', '##4', '\\', 'n', '[SEP]']
Filtered   (015): ['max', '_', 'attach', '##si', '##ze', '=', '20', '*', '102', '##4', '*', '102', '##4', '\\', 'n']
Detokenized (008): ['max_attach##si##ze', '=', '20', '*', '102##4', '*', '102##4', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "MAIL_SIZE_LIMIT = - 1 \n"
Original    (005): ['MAIL_SIZE_LIMIT', '=', '-', '1', '\\n']
Tokenized   (012): ['[CLS]', 'mail', '_', 'size', '_', 'limit', '=', '-', '1', '\\', 'n', '[SEP]']
Filtered   (010): ['mail', '_', 'size', '_', 'limit', '=', '-', '1', '\\', 'n']
Detokenized (005): ['mail_size_limit', '=', '-', '1', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "newparts = [ ] \n"
Original    (005): ['newparts', '=', '[', ']', '\\n']
Tokenized   (009): ['[CLS]', 'new', '##parts', '=', '[', ']', '\\', 'n', '[SEP]']
Filtered   (007): ['new', '##parts', '=', '[', ']', '\\', 'n']
Detokenized (005): ['new##parts', '=', '[', ']', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "firstitem = self . GetItem ( user , route + [ start ] , has_perm , need_perm ) \n"
Original    (019): ['firstitem', '=', 'self', '.', 'GetItem', '(', 'user', ',', 'route', '+', '[', 'start', ']', ',', 'has_perm', ',', 'need_perm', ')', '\\n']
Tokenized   (032): ['[CLS]', 'first', '##ite', '##m', '=', 'self', '.', 'get', '##ite', '##m', '(', 'user', ',', 'route', '+', '[', 'start', ']', ',', 'has', '_', 'per', '##m', ',', 'need', '_', 'per', '##m', ')', '\\', 'n', '[SEP]']
Filtered   (030): ['first', '##ite', '##m', '=', 'self', '.', 'get', '##ite', '##m', '(', 'user', ',', 'route', '+', '[', 'start', ']', ',', 'has', '_', 'per', '##m', ',', 'need', '_', 'per', '##m', ')', '\\', 'n']
Detokenized (019): ['first##ite##m', '=', 'self', '.', 'get##ite##m', '(', 'user', ',', 'route', '+', '[', 'start', ']', ',', 'has_per##m', ',', 'need_per##m', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "_id = start - 1 \n"
Original    (006): ['_id', '=', 'start', '-', '1', '\\n']
Tokenized   (010): ['[CLS]', '_', 'id', '=', 'start', '-', '1', '\\', 'n', '[SEP]']
Filtered   (008): ['_', 'id', '=', 'start', '-', '1', '\\', 'n']
Detokenized (006): ['_id', '=', 'start', '-', '1', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "linkfile = "%s/boards/xattach/%s" % ( Config . BBS_ROOT , filename ) \n"
Original    (012): ['linkfile', '=', '"%s/boards/xattach/%s"', '%', '(', 'Config', '.', 'BBS_ROOT', ',', 'filename', ')', '\\n']
Tokenized   (035): ['[CLS]', 'link', '##fi', '##le', '=', '"', '%', 's', '/', 'boards', '/', 'x', '##att', '##ach', '/', '%', 's', '"', '%', '(', 'con', '##fi', '##g', '.', 'bb', '##s', '_', 'root', ',', 'file', '##name', ')', '\\', 'n', '[SEP]']
Filtered   (033): ['link', '##fi', '##le', '=', '"', '%', 's', '/', 'boards', '/', 'x', '##att', '##ach', '/', '%', 's', '"', '%', '(', 'con', '##fi', '##g', '.', 'bb', '##s', '_', 'root', ',', 'file', '##name', ')', '\\', 'n']
Detokenized (012): ['link##fi##le', '=', '"%s/boards/x##att##ach/%s"', '%', '(', 'con##fi##g', '.', 'bb##s_root', ',', 'file##name', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "boardname = svc . get_str ( params , , ) \n"
Original    (011): ['boardname', '=', 'svc', '.', 'get_str', '(', 'params', ',', ',', ')', '\\n']
Tokenized   (020): ['[CLS]', 'board', '##name', '=', 'sv', '##c', '.', 'get', '_', 'st', '##r', '(', 'para', '##ms', ',', ',', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['board', '##name', '=', 'sv', '##c', '.', 'get', '_', 'st', '##r', '(', 'para', '##ms', ',', ',', ')', '\\', 'n']
Detokenized (011): ['board##name', '=', 'sv##c', '.', 'get_st##r', '(', 'para##ms', ',', ',', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "has_perm = user . IsDigestMgr ( ) \n"
Original    (008): ['has_perm', '=', 'user', '.', 'IsDigestMgr', '(', ')', '\\n']
Tokenized   (018): ['[CLS]', 'has', '_', 'per', '##m', '=', 'user', '.', 'is', '##di', '##ges', '##tm', '##gr', '(', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['has', '_', 'per', '##m', '=', 'user', '.', 'is', '##di', '##ges', '##tm', '##gr', '(', ')', '\\', 'n']
Detokenized (008): ['has_per##m', '=', 'user', '.', 'is##di##ges##tm##gr', '(', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "Digest . View ( svc , basenode , route , session , has_perm , start , count ) \n"
Original    (019): ['Digest', '.', 'View', '(', 'svc', ',', 'basenode', ',', 'route', ',', 'session', ',', 'has_perm', ',', 'start', ',', 'count', ')', '\\n']
Tokenized   (028): ['[CLS]', 'digest', '.', 'view', '(', 'sv', '##c', ',', 'base', '##no', '##de', ',', 'route', ',', 'session', ',', 'has', '_', 'per', '##m', ',', 'start', ',', 'count', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['digest', '.', 'view', '(', 'sv', '##c', ',', 'base', '##no', '##de', ',', 'route', ',', 'session', ',', 'has', '_', 'per', '##m', ',', 'start', ',', 'count', ')', '\\', 'n']
Detokenized (019): ['digest', '.', 'view', '(', 'sv##c', ',', 'base##no##de', ',', 'route', ',', 'session', ',', 'has_per##m', ',', 'start', ',', 'count', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "svc . writedata ( json . dumps ( result ) ) \n"
Original    (012): ['svc', '.', 'writedata', '(', 'json', '.', 'dumps', '(', 'result', ')', ')', '\\n']
Tokenized   (020): ['[CLS]', 'sv', '##c', '.', 'write', '##da', '##ta', '(', 'j', '##son', '.', 'dump', '##s', '(', 'result', ')', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['sv', '##c', '.', 'write', '##da', '##ta', '(', 'j', '##son', '.', 'dump', '##s', '(', 'result', ')', ')', '\\', 'n']
Detokenized (012): ['sv##c', '.', 'write##da##ta', '(', 'j##son', '.', 'dump##s', '(', 'result', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "postinfo = Post . Post ( item . realpath ( ) , None ) \n"
Original    (015): ['postinfo', '=', 'Post', '.', 'Post', '(', 'item', '.', 'realpath', '(', ')', ',', 'None', ')', '\\n']
Tokenized   (021): ['[CLS]', 'post', '##in', '##fo', '=', 'post', '.', 'post', '(', 'item', '.', 'real', '##path', '(', ')', ',', 'none', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['post', '##in', '##fo', '=', 'post', '.', 'post', '(', 'item', '.', 'real', '##path', '(', ')', ',', 'none', ')', '\\', 'n']
Detokenized (015): ['post##in##fo', '=', 'post', '.', 'post', '(', 'item', '.', 'real##path', '(', ')', ',', 'none', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "msg_count = msgbox . GetMsgCount ( all = False ) \n"
Original    (011): ['msg_count', '=', 'msgbox', '.', 'GetMsgCount', '(', 'all', '=', 'False', ')', '\\n']
Tokenized   (022): ['[CLS]', 'ms', '##g', '_', 'count', '=', 'ms', '##gb', '##ox', '.', 'get', '##ms', '##gc', '##ount', '(', 'all', '=', 'false', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['ms', '##g', '_', 'count', '=', 'ms', '##gb', '##ox', '.', 'get', '##ms', '##gc', '##ount', '(', 'all', '=', 'false', ')', '\\', 'n']
Detokenized (011): ['ms##g_count', '=', 'ms##gb##ox', '.', 'get##ms##gc##ount', '(', 'all', '=', 'false', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "xmpp_read = self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) \n"
Original    (017): ['xmpp_read', '=', 'self', '.', 'rosters', '.', 'get_xmpp_read', '(', 'self', '.', '_user', '.', 'GetUID', '(', ')', ')', '\\n']
Tokenized   (033): ['[CLS]', 'x', '##mp', '##p', '_', 'read', '=', 'self', '.', 'roster', '##s', '.', 'get', '_', 'x', '##mp', '##p', '_', 'read', '(', 'self', '.', '_', 'user', '.', 'get', '##uid', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['x', '##mp', '##p', '_', 'read', '=', 'self', '.', 'roster', '##s', '.', 'get', '_', 'x', '##mp', '##p', '_', 'read', '(', 'self', '.', '_', 'user', '.', 'get', '##uid', '(', ')', ')', '\\', 'n']
Detokenized (017): ['x##mp##p_read', '=', 'self', '.', 'roster##s', '.', 'get_x##mp##p_read', '(', 'self', '.', '_user', '.', 'get##uid', '(', ')', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "read_count = msg_count - msg_unread \n"
Original    (006): ['read_count', '=', 'msg_count', '-', 'msg_unread', '\\n']
Tokenized   (019): ['[CLS]', 'read', '_', 'count', '=', 'ms', '##g', '_', 'count', '-', 'ms', '##g', '_', 'un', '##rea', '##d', '\\', 'n', '[SEP]']
Filtered   (017): ['read', '_', 'count', '=', 'ms', '##g', '_', 'count', '-', 'ms', '##g', '_', 'un', '##rea', '##d', '\\', 'n']
Detokenized (006): ['read_count', '=', 'ms##g_count', '-', 'ms##g_un##rea##d', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "term_read = self . rosters . get_term_read ( self . get_uid ( ) ) \n"
Original    (015): ['term_read', '=', 'self', '.', 'rosters', '.', 'get_term_read', '(', 'self', '.', 'get_uid', '(', ')', ')', '\\n']
Tokenized   (028): ['[CLS]', 'term', '_', 'read', '=', 'self', '.', 'roster', '##s', '.', 'get', '_', 'term', '_', 'read', '(', 'self', '.', 'get', '_', 'ui', '##d', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['term', '_', 'read', '=', 'self', '.', 'roster', '##s', '.', 'get', '_', 'term', '_', 'read', '(', 'self', '.', 'get', '_', 'ui', '##d', '(', ')', ')', '\\', 'n']
Detokenized (015): ['term_read', '=', 'self', '.', 'roster##s', '.', 'get_term_read', '(', 'self', '.', 'get_ui##d', '(', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "new_unread [ msghead . topid ] = i \n"
Original    (009): ['new_unread', '[', 'msghead', '.', 'topid', ']', '=', 'i', '\\n']
Tokenized   (019): ['[CLS]', 'new', '_', 'un', '##rea', '##d', '[', 'ms', '##gh', '##ead', '.', 'top', '##id', ']', '=', 'i', '\\', 'n', '[SEP]']
Filtered   (017): ['new', '_', 'un', '##rea', '##d', '[', 'ms', '##gh', '##ead', '.', 'top', '##id', ']', '=', 'i', '\\', 'n']
Detokenized (009): ['new_un##rea##d', '[', 'ms##gh##ead', '.', 'top##id', ']', '=', 'i', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "to_steal = { } \n"
Original    (005): ['to_steal', '=', '{', '}', '\\n']
Tokenized   (010): ['[CLS]', 'to', '_', 'steal', '=', '{', '}', '\\', 'n', '[SEP]']
Filtered   (008): ['to', '_', 'steal', '=', '{', '}', '\\', 'n']
Detokenized (005): ['to_steal', '=', '{', '}', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "to_steal_begin = msg_count \n"
Original    (004): ['to_steal_begin', '=', 'msg_count', '\\n']
Tokenized   (014): ['[CLS]', 'to', '_', 'steal', '_', 'begin', '=', 'ms', '##g', '_', 'count', '\\', 'n', '[SEP]']
Filtered   (012): ['to', '_', 'steal', '_', 'begin', '=', 'ms', '##g', '_', 'count', '\\', 'n']
Detokenized (004): ['to_steal_begin', '=', 'ms##g_count', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "pass \n"
Original    (002): ['pass', '\\n']
Tokenized   (005): ['[CLS]', 'pass', '\\', 'n', '[SEP]']
Filtered   (003): ['pass', '\\', 'n']
Detokenized (002): ['pass', '\\n']
Counter: 3
===================================================================
Hidden states:  (13, 2, 768)
# Extracted words:  2
Sentence         : "final_unread [ pid ] = ( new_unread [ pid ] , 1 ) \n"
Original    (014): ['final_unread', '[', 'pid', ']', '=', '(', 'new_unread', '[', 'pid', ']', ',', '1', ')', '\\n']
Tokenized   (027): ['[CLS]', 'final', '_', 'un', '##rea', '##d', '[', 'pi', '##d', ']', '=', '(', 'new', '_', 'un', '##rea', '##d', '[', 'pi', '##d', ']', ',', '1', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['final', '_', 'un', '##rea', '##d', '[', 'pi', '##d', ']', '=', '(', 'new', '_', 'un', '##rea', '##d', '[', 'pi', '##d', ']', ',', '1', ')', '\\', 'n']
Detokenized (014): ['final_un##rea##d', '[', 'pi##d', ']', '=', '(', 'new_un##rea##d', '[', 'pi##d', ']', ',', '1', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "msgtext = msgbox . LoadMsgText ( msghead ) \n"
Original    (009): ['msgtext', '=', 'msgbox', '.', 'LoadMsgText', '(', 'msghead', ')', '\\n']
Tokenized   (023): ['[CLS]', 'ms', '##gt', '##ex', '##t', '=', 'ms', '##gb', '##ox', '.', 'load', '##ms', '##gt', '##ex', '##t', '(', 'ms', '##gh', '##ead', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['ms', '##gt', '##ex', '##t', '=', 'ms', '##gb', '##ox', '.', 'load', '##ms', '##gt', '##ex', '##t', '(', 'ms', '##gh', '##ead', ')', '\\', 'n']
Detokenized (009): ['ms##gt##ex##t', '=', 'ms##gb##ox', '.', 'load##ms##gt##ex##t', '(', 'ms##gh##ead', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "roster = self . rosters . get ( self ) \n"
Original    (011): ['roster', '=', 'self', '.', 'rosters', '.', 'get', '(', 'self', ')', '\\n']
Tokenized   (015): ['[CLS]', 'roster', '=', 'self', '.', 'roster', '##s', '.', 'get', '(', 'self', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['roster', '=', 'self', '.', 'roster', '##s', '.', 'get', '(', 'self', ')', '\\', 'n']
Detokenized (011): ['roster', '=', 'self', '.', 'roster##s', '.', 'get', '(', 'self', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "PYTHON_VERSION = sys . version_info [ : 3 ] \n"
Original    (010): ['PYTHON_VERSION', '=', 'sys', '.', 'version_info', '[', ':', '3', ']', '\\n']
Tokenized   (018): ['[CLS]', 'python', '_', 'version', '=', 'sy', '##s', '.', 'version', '_', 'info', '[', ':', '3', ']', '\\', 'n', '[SEP]']
Filtered   (016): ['python', '_', 'version', '=', 'sy', '##s', '.', 'version', '_', 'info', '[', ':', '3', ']', '\\', 'n']
Detokenized (010): ['python_version', '=', 'sy##s', '.', 'version_info', '[', ':', '3', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "PY2 = ( PYTHON_VERSION [ 0 ] == 2 ) \n"
Original    (011): ['PY2', '=', '(', 'PYTHON_VERSION', '[', '0', ']', '==', '2', ')', '\\n']
Tokenized   (019): ['[CLS]', 'p', '##y', '##2', '=', '(', 'python', '_', 'version', '[', '0', ']', '=', '=', '2', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['p', '##y', '##2', '=', '(', 'python', '_', 'version', '[', '0', ']', '=', '=', '2', ')', '\\', 'n']
Detokenized (011): ['p##y##2', '=', '(', 'python_version', '[', '0', ']', '==', '2', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "sp_desc , \n"
Original    (003): ['sp_desc', ',', '\\n']
Tokenized   (009): ['[CLS]', 'sp', '_', 'des', '##c', ',', '\\', 'n', '[SEP]']
Filtered   (007): ['sp', '_', 'des', '##c', ',', '\\', 'n']
Detokenized (003): ['sp_des##c', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "con = hpov . connection ( args . host ) \n"
Original    (011): ['con', '=', 'hpov', '.', 'connection', '(', 'args', '.', 'host', ')', '\\n']
Tokenized   (016): ['[CLS]', 'con', '=', 'hp', '##ov', '.', 'connection', '(', 'ar', '##gs', '.', 'host', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['con', '=', 'hp', '##ov', '.', 'connection', '(', 'ar', '##gs', '.', 'host', ')', '\\', 'n']
Detokenized (011): ['con', '=', 'hp##ov', '.', 'connection', '(', 'ar##gs', '.', 'host', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "acceptEULA ( con ) \n"
Original    (005): ['acceptEULA', '(', 'con', ')', '\\n']
Tokenized   (010): ['[CLS]', 'accept', '##eu', '##la', '(', 'con', ')', '\\', 'n', '[SEP]']
Filtered   (008): ['accept', '##eu', '##la', '(', 'con', ')', '\\', 'n']
Detokenized (005): ['accept##eu##la', '(', 'con', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "fw_settings = profile . make_firmware_dict ( sts , args . baseline ) \n"
Original    (013): ['fw_settings', '=', 'profile', '.', 'make_firmware_dict', '(', 'sts', ',', 'args', '.', 'baseline', ')', '\\n']
Tokenized   (026): ['[CLS]', 'f', '##w', '_', 'settings', '=', 'profile', '.', 'make', '_', 'firm', '##ware', '_', 'di', '##ct', '(', 'sts', ',', 'ar', '##gs', '.', 'baseline', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['f', '##w', '_', 'settings', '=', 'profile', '.', 'make', '_', 'firm', '##ware', '_', 'di', '##ct', '(', 'sts', ',', 'ar', '##gs', '.', 'baseline', ')', '\\', 'n']
Detokenized (013): ['f##w_settings', '=', 'profile', '.', 'make_firm##ware_di##ct', '(', 'sts', ',', 'ar##gs', '.', 'baseline', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "boot , bootmode = profile . make_boot_settings_dict ( srv , sht , args . disable_manage_boot , \n"
Original    (017): ['boot', ',', 'bootmode', '=', 'profile', '.', 'make_boot_settings_dict', '(', 'srv', ',', 'sht', ',', 'args', '.', 'disable_manage_boot', ',', '\\n']
Tokenized   (037): ['[CLS]', 'boot', ',', 'boot', '##mo', '##de', '=', 'profile', '.', 'make', '_', 'boot', '_', 'settings', '_', 'di', '##ct', '(', 'sr', '##v', ',', 'sh', '##t', ',', 'ar', '##gs', '.', 'di', '##sable', '_', 'manage', '_', 'boot', ',', '\\', 'n', '[SEP]']
Filtered   (035): ['boot', ',', 'boot', '##mo', '##de', '=', 'profile', '.', 'make', '_', 'boot', '_', 'settings', '_', 'di', '##ct', '(', 'sr', '##v', ',', 'sh', '##t', ',', 'ar', '##gs', '.', 'di', '##sable', '_', 'manage', '_', 'boot', ',', '\\', 'n']
Detokenized (017): ['boot', ',', 'boot##mo##de', '=', 'profile', '.', 'make_boot_settings_di##ct', '(', 'sr##v', ',', 'sh##t', ',', 'ar##gs', '.', 'di##sable_manage_boot', ',', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "define_profile_template ( srv , \n"
Original    (005): ['define_profile_template', '(', 'srv', ',', '\\n']
Tokenized   (013): ['[CLS]', 'define', '_', 'profile', '_', 'template', '(', 'sr', '##v', ',', '\\', 'n', '[SEP]']
Filtered   (011): ['define', '_', 'profile', '_', 'template', '(', 'sr', '##v', ',', '\\', 'n']
Detokenized (005): ['define_profile_template', '(', 'sr##v', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "sht [ ] , \n"
Original    (005): ['sht', '[', ']', ',', '\\n']
Tokenized   (009): ['[CLS]', 'sh', '##t', '[', ']', ',', '\\', 'n', '[SEP]']
Filtered   (007): ['sh', '##t', '[', ']', ',', '\\', 'n']
Detokenized (005): ['sh##t', '[', ']', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "credential = { : args . domain . upper ( ) , : args . user , : args . passwd } \n"
Original    (023): ['credential', '=', '{', ':', 'args', '.', 'domain', '.', 'upper', '(', ')', ',', ':', 'args', '.', 'user', ',', ':', 'args', '.', 'passwd', '}', '\\n']
Tokenized   (032): ['[CLS]', 'cr', '##ede', '##ntial', '=', '{', ':', 'ar', '##gs', '.', 'domain', '.', 'upper', '(', ')', ',', ':', 'ar', '##gs', '.', 'user', ',', ':', 'ar', '##gs', '.', 'pass', '##wd', '}', '\\', 'n', '[SEP]']
Filtered   (030): ['cr', '##ede', '##ntial', '=', '{', ':', 'ar', '##gs', '.', 'domain', '.', 'upper', '(', ')', ',', ':', 'ar', '##gs', '.', 'user', ',', ':', 'ar', '##gs', '.', 'pass', '##wd', '}', '\\', 'n']
Detokenized (023): ['cr##ede##ntial', '=', '{', ':', 'ar##gs', '.', 'domain', '.', 'upper', '(', ')', ',', ':', 'ar##gs', '.', 'user', ',', ':', 'ar##gs', '.', 'pass##wd', '}', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "get_address_pools ( con , srv , args . types ) \n"
Original    (011): ['get_address_pools', '(', 'con', ',', 'srv', ',', 'args', '.', 'types', ')', '\\n']
Tokenized   (020): ['[CLS]', 'get', '_', 'address', '_', 'pools', '(', 'con', ',', 'sr', '##v', ',', 'ar', '##gs', '.', 'types', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['get', '_', 'address', '_', 'pools', '(', 'con', ',', 'sr', '##v', ',', 'ar', '##gs', '.', 'types', ')', '\\', 'n']
Detokenized (011): ['get_address_pools', '(', 'con', ',', 'sr##v', ',', 'ar##gs', '.', 'types', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "enclosure_group = None , server_profile = None ) : \n"
Original    (010): ['enclosure_group', '=', 'None', ',', 'server_profile', '=', 'None', ')', ':', '\\n']
Tokenized   (017): ['[CLS]', 'enclosure', '_', 'group', '=', 'none', ',', 'server', '_', 'profile', '=', 'none', ')', ':', '\\', 'n', '[SEP]']
Filtered   (015): ['enclosure', '_', 'group', '=', 'none', ',', 'server', '_', 'profile', '=', 'none', ')', ':', '\\', 'n']
Detokenized (010): ['enclosure_group', '=', 'none', ',', 'server_profile', '=', 'none', ')', ':', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "biosSettings = None , \n"
Original    (005): ['biosSettings', '=', 'None', ',', '\\n']
Tokenized   (011): ['[CLS]', 'bio', '##sse', '##tting', '##s', '=', 'none', ',', '\\', 'n', '[SEP]']
Filtered   (009): ['bio', '##sse', '##tting', '##s', '=', 'none', ',', '\\', 'n']
Detokenized (005): ['bio##sse##tting##s', '=', 'none', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "macType = , \n"
Original    (004): ['macType', '=', ',', '\\n']
Tokenized   (008): ['[CLS]', 'mac', '##type', '=', ',', '\\', 'n', '[SEP]']
Filtered   (006): ['mac', '##type', '=', ',', '\\', 'n']
Detokenized (004): ['mac##type', '=', ',', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "localStorageSettingsV3 , macType , name , \n"
Original    (007): ['localStorageSettingsV3', ',', 'macType', ',', 'name', ',', '\\n']
Tokenized   (018): ['[CLS]', 'locals', '##tor', '##ages', '##etti', '##ng', '##s', '##v', '##3', ',', 'mac', '##type', ',', 'name', ',', '\\', 'n', '[SEP]']
Filtered   (016): ['locals', '##tor', '##ages', '##etti', '##ng', '##s', '##v', '##3', ',', 'mac', '##type', ',', 'name', ',', '\\', 'n']
Detokenized (007): ['locals##tor##ages##etti##ng##s##v##3', ',', 'mac##type', ',', 'name', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "sanStorageV3 , serialNumber , \n"
Original    (005): ['sanStorageV3', ',', 'serialNumber', ',', '\\n']
Tokenized   (014): ['[CLS]', 'sans', '##tor', '##age', '##v', '##3', ',', 'serial', '##num', '##ber', ',', '\\', 'n', '[SEP]']
Filtered   (012): ['sans', '##tor', '##age', '##v', '##3', ',', 'serial', '##num', '##ber', ',', '\\', 'n']
Detokenized (005): ['sans##tor##age##v##3', ',', 'serial##num##ber', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "serverProfileTemplateUri , uuid , wwnType ) \n"
Original    (007): ['serverProfileTemplateUri', ',', 'uuid', ',', 'wwnType', ')', '\\n']
Tokenized   (019): ['[CLS]', 'server', '##pro', '##fi', '##lete', '##mp', '##late', '##uri', ',', 'u', '##uid', ',', 'w', '##wn', '##type', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['server', '##pro', '##fi', '##lete', '##mp', '##late', '##uri', ',', 'u', '##uid', ',', 'w', '##wn', '##type', ')', '\\', 'n']
Detokenized (007): ['server##pro##fi##lete##mp##late##uri', ',', 'u##uid', ',', 'w##wn##type', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "profile_template = self . _con . get ( entity [ ] ) \n"
Original    (013): ['profile_template', '=', 'self', '.', '_con', '.', 'get', '(', 'entity', '[', ']', ')', '\\n']
Tokenized   (019): ['[CLS]', 'profile', '_', 'template', '=', 'self', '.', '_', 'con', '.', 'get', '(', 'entity', '[', ']', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['profile', '_', 'template', '=', 'self', '.', '_', 'con', '.', 'get', '(', 'entity', '[', ']', ')', '\\', 'n']
Detokenized (013): ['profile_template', '=', 'self', '.', '_con', '.', 'get', '(', 'entity', '[', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "powerMode = ) : \n"
Original    (005): ['powerMode', '=', ')', ':', '\\n']
Tokenized   (010): ['[CLS]', 'power', '##mo', '##de', '=', ')', ':', '\\', 'n', '[SEP]']
Filtered   (008): ['power', '##mo', '##de', '=', ')', ':', '\\', 'n']
Detokenized (005): ['power##mo##de', '=', ')', ':', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "egroup = make_EnclosureGroupV200 ( associatedLIGs , name , powerMode ) \n"
Original    (011): ['egroup', '=', 'make_EnclosureGroupV200', '(', 'associatedLIGs', ',', 'name', ',', 'powerMode', ')', '\\n']
Tokenized   (024): ['[CLS]', 'e', '##group', '=', 'make', '_', 'enclosure', '##group', '##v', '##200', '(', 'associated', '##li', '##gs', ',', 'name', ',', 'power', '##mo', '##de', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['e', '##group', '=', 'make', '_', 'enclosure', '##group', '##v', '##200', '(', 'associated', '##li', '##gs', ',', 'name', ',', 'power', '##mo', '##de', ')', '\\', 'n']
Detokenized (011): ['e##group', '=', 'make_enclosure##group##v##200', '(', 'associated##li##gs', ',', 'name', ',', 'power##mo##de', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "allocatorBody = { : count } \n"
Original    (007): ['allocatorBody', '=', '{', ':', 'count', '}', '\\n']
Tokenized   (013): ['[CLS]', 'all', '##oca', '##tor', '##body', '=', '{', ':', 'count', '}', '\\', 'n', '[SEP]']
Filtered   (011): ['all', '##oca', '##tor', '##body', '=', '{', ':', 'count', '}', '\\', 'n']
Detokenized (007): ['all##oca##tor##body', '=', '{', ':', 'count', '}', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "prange [ ] = False \n"
Original    (006): ['prange', '[', ']', '=', 'False', '\\n']
Tokenized   (010): ['[CLS]', 'pr', '##ange', '[', ']', '=', 'false', '\\', 'n', '[SEP]']
Filtered   (008): ['pr', '##ange', '[', ']', '=', 'false', '\\', 'n']
Detokenized (006): ['pr##ange', '[', ']', '=', 'false', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "tempstr = "hp-rest-classes-bios-" + romfamily + "-" + biosversion \n"
Original    (010): ['tempstr', '=', '"hp-rest-classes-bios-"', '+', 'romfamily', '+', '"-"', '+', 'biosversion', '\\n']
Tokenized   (031): ['[CLS]', 'temps', '##tr', '=', '"', 'hp', '-', 'rest', '-', 'classes', '-', 'bio', '##s', '-', '"', '+', 'rom', '##fa', '##mi', '##ly', '+', '"', '-', '"', '+', 'bio', '##s', '##version', '\\', 'n', '[SEP]']
Filtered   (029): ['temps', '##tr', '=', '"', 'hp', '-', 'rest', '-', 'classes', '-', 'bio', '##s', '-', '"', '+', 'rom', '##fa', '##mi', '##ly', '+', '"', '-', '"', '+', 'bio', '##s', '##version', '\\', 'n']
Detokenized (010): ['temps##tr', '=', '"hp-rest-classes-bio##s-"', '+', 'rom##fa##mi##ly', '+', '"-"', '+', 'bio##s##version', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "monolith = None ) : \n"
Original    (006): ['monolith', '=', 'None', ')', ':', '\\n']
Tokenized   (011): ['[CLS]', 'mono', '##lit', '##h', '=', 'none', ')', ':', '\\', 'n', '[SEP]']
Filtered   (009): ['mono', '##lit', '##h', '=', 'none', ')', ':', '\\', 'n']
Detokenized (006): ['mono##lit##h', '=', 'none', ')', ':', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "pathjoinstr ) ) : \n"
Original    (005): ['pathjoinstr', ')', ')', ':', '\\n']
Tokenized   (011): ['[CLS]', 'path', '##jo', '##ins', '##tr', ')', ')', ':', '\\', 'n', '[SEP]']
Filtered   (009): ['path', '##jo', '##ins', '##tr', ')', ')', ':', '\\', 'n']
Detokenized (005): ['path##jo##ins##tr', ')', ')', ':', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "newclass . set_root ( root ) \n"
Original    (007): ['newclass', '.', 'set_root', '(', 'root', ')', '\\n']
Tokenized   (013): ['[CLS]', 'new', '##class', '.', 'set', '_', 'root', '(', 'root', ')', '\\', 'n', '[SEP]']
Filtered   (011): ['new', '##class', '.', 'set', '_', 'root', '(', 'root', ')', '\\', 'n']
Detokenized (007): ['new##class', '.', 'set_root', '(', 'root', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "folderentries = data [ "links" ] \n"
Original    (007): ['folderentries', '=', 'data', '[', '"links"', ']', '\\n']
Tokenized   (014): ['[CLS]', 'folder', '##ent', '##ries', '=', 'data', '[', '"', 'links', '"', ']', '\\', 'n', '[SEP]']
Filtered   (012): ['folder', '##ent', '##ries', '=', 'data', '[', '"', 'links', '"', ']', '\\', 'n']
Detokenized (007): ['folder##ent##ries', '=', 'data', '[', '"links"', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "datareturn . append ( self . load_file ( fqpath , root = root , biossection = True , registries = True , datareturn = True ) ) \n"
Original    (028): ['datareturn', '.', 'append', '(', 'self', '.', 'load_file', '(', 'fqpath', ',', 'root', '=', 'root', ',', 'biossection', '=', 'True', ',', 'registries', '=', 'True', ',', 'datareturn', '=', 'True', ')', ')', '\\n']
Tokenized   (043): ['[CLS]', 'data', '##ret', '##urn', '.', 'app', '##end', '(', 'self', '.', 'load', '_', 'file', '(', 'f', '##q', '##path', ',', 'root', '=', 'root', ',', 'bio', '##sse', '##ction', '=', 'true', ',', 'regis', '##tries', '=', 'true', ',', 'data', '##ret', '##urn', '=', 'true', ')', ')', '\\', 'n', '[SEP]']
Filtered   (041): ['data', '##ret', '##urn', '.', 'app', '##end', '(', 'self', '.', 'load', '_', 'file', '(', 'f', '##q', '##path', ',', 'root', '=', 'root', ',', 'bio', '##sse', '##ction', '=', 'true', ',', 'regis', '##tries', '=', 'true', ',', 'data', '##ret', '##urn', '=', 'true', ')', ')', '\\', 'n']
Detokenized (028): ['data##ret##urn', '.', 'app##end', '(', 'self', '.', 'load_file', '(', 'f##q##path', ',', 'root', '=', 'root', ',', 'bio##sse##ction', '=', 'true', ',', 'regis##tries', '=', 'true', ',', 'data##ret##urn', '=', 'true', ')', ')', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 28, 768)
# Extracted words:  28
Sentence         : "currdict = currdict , monolith = monolith , \n"
Original    (009): ['currdict', '=', 'currdict', ',', 'monolith', '=', 'monolith', ',', '\\n']
Tokenized   (020): ['[CLS]', 'cu', '##rr', '##dict', '=', 'cu', '##rr', '##dict', ',', 'mono', '##lit', '##h', '=', 'mono', '##lit', '##h', ',', '\\', 'n', '[SEP]']
Filtered   (018): ['cu', '##rr', '##dict', '=', 'cu', '##rr', '##dict', ',', 'mono', '##lit', '##h', '=', 'mono', '##lit', '##h', ',', '\\', 'n']
Detokenized (009): ['cu##rr##dict', '=', 'cu##rr##dict', ',', 'mono##lit##h', '=', 'mono##lit##h', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "newarg = newarg , checkall = checkall ) \n"
Original    (009): ['newarg', '=', 'newarg', ',', 'checkall', '=', 'checkall', ')', '\\n']
Tokenized   (018): ['[CLS]', 'new', '##ar', '##g', '=', 'new', '##ar', '##g', ',', 'check', '##all', '=', 'check', '##all', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['new', '##ar', '##g', '=', 'new', '##ar', '##g', ',', 'check', '##all', '=', 'check', '##all', ')', '\\', 'n']
Detokenized (009): ['new##ar##g', '=', 'new##ar##g', ',', 'check##all', '=', 'check##all', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "attrreg = self . find_bios_registry ( regname = regname ) \n"
Original    (011): ['attrreg', '=', 'self', '.', 'find_bios_registry', '(', 'regname', '=', 'regname', ')', '\\n']
Tokenized   (024): ['[CLS]', 'at', '##tr', '##re', '##g', '=', 'self', '.', 'find', '_', 'bio', '##s', '_', 'registry', '(', 'reg', '##name', '=', 'reg', '##name', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['at', '##tr', '##re', '##g', '=', 'self', '.', 'find', '_', 'bio', '##s', '_', 'registry', '(', 'reg', '##name', '=', 'reg', '##name', ')', '\\', 'n']
Detokenized (011): ['at##tr##re##g', '=', 'self', '.', 'find_bio##s_registry', '(', 'reg##name', '=', 'reg##name', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "schlink = schlink [ len ( schlink ) - 2 ] \n"
Original    (012): ['schlink', '=', 'schlink', '[', 'len', '(', 'schlink', ')', '-', '2', ']', '\\n']
Tokenized   (021): ['[CLS]', 'sc', '##hli', '##nk', '=', 'sc', '##hli', '##nk', '[', 'len', '(', 'sc', '##hli', '##nk', ')', '-', '2', ']', '\\', 'n', '[SEP]']
Filtered   (019): ['sc', '##hli', '##nk', '=', 'sc', '##hli', '##nk', '[', 'len', '(', 'sc', '##hli', '##nk', ')', '-', '2', ']', '\\', 'n']
Detokenized (012): ['sc##hli##nk', '=', 'sc##hli##nk', '[', 'len', '(', 'sc##hli##nk', ')', '-', '2', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "schname . lower ( ) ) : \n"
Original    (008): ['schname', '.', 'lower', '(', ')', ')', ':', '\\n']
Tokenized   (013): ['[CLS]', 'sc', '##hn', '##ame', '.', 'lower', '(', ')', ')', ':', '\\', 'n', '[SEP]']
Filtered   (011): ['sc', '##hn', '##ame', '.', 'lower', '(', ')', ')', ':', '\\', 'n']
Detokenized (008): ['sc##hn##ame', '.', 'lower', '(', ')', ')', ':', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "xref = os . path . normpath ( currloc . Uri . extref ) . lstrip ( os . path . sep ) \n"
Original    (024): ['xref', '=', 'os', '.', 'path', '.', 'normpath', '(', 'currloc', '.', 'Uri', '.', 'extref', ')', '.', 'lstrip', '(', 'os', '.', 'path', '.', 'sep', ')', '\\n']
Tokenized   (038): ['[CLS]', 'x', '##re', '##f', '=', 'os', '.', 'path', '.', 'norm', '##path', '(', 'cu', '##rr', '##lo', '##c', '.', 'ur', '##i', '.', 'ex', '##tre', '##f', ')', '.', 'l', '##st', '##rip', '(', 'os', '.', 'path', '.', 'sep', ')', '\\', 'n', '[SEP]']
Filtered   (036): ['x', '##re', '##f', '=', 'os', '.', 'path', '.', 'norm', '##path', '(', 'cu', '##rr', '##lo', '##c', '.', 'ur', '##i', '.', 'ex', '##tre', '##f', ')', '.', 'l', '##st', '##rip', '(', 'os', '.', 'path', '.', 'sep', ')', '\\', 'n']
Detokenized (024): ['x##re##f', '=', 'os', '.', 'path', '.', 'norm##path', '(', 'cu##rr##lo##c', '.', 'ur##i', '.', 'ex##tre##f', ')', '.', 'l##st##rip', '(', 'os', '.', 'path', '.', 'sep', ')', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "fqpath = os . path . join ( root , xref ) \n"
Original    (013): ['fqpath', '=', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'xref', ')', '\\n']
Tokenized   (020): ['[CLS]', 'f', '##q', '##path', '=', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'x', '##re', '##f', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['f', '##q', '##path', '=', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'x', '##re', '##f', ')', '\\', 'n']
Detokenized (013): ['f##q##path', '=', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'x##re##f', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "langcode = list ( locale . getdefaultlocale ( ) ) \n"
Original    (011): ['langcode', '=', 'list', '(', 'locale', '.', 'getdefaultlocale', '(', ')', ')', '\\n']
Tokenized   (022): ['[CLS]', 'lang', '##code', '=', 'list', '(', 'local', '##e', '.', 'get', '##de', '##fa', '##ult', '##lo', '##cal', '##e', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['lang', '##code', '=', 'list', '(', 'local', '##e', '.', 'get', '##de', '##fa', '##ult', '##lo', '##cal', '##e', '(', ')', ')', '\\', 'n']
Detokenized (011): ['lang##code', '=', 'list', '(', 'local##e', '.', 'get##de##fa##ult##lo##cal##e', '(', ')', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "locationlanguage = locationlanguage . replace ( "-" , "_" ) \n"
Original    (011): ['locationlanguage', '=', 'locationlanguage', '.', 'replace', '(', '"-"', ',', '"_"', ')', '\\n']
Tokenized   (024): ['[CLS]', 'location', '##lang', '##ua', '##ge', '=', 'location', '##lang', '##ua', '##ge', '.', 'replace', '(', '"', '-', '"', ',', '"', '_', '"', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['location', '##lang', '##ua', '##ge', '=', 'location', '##lang', '##ua', '##ge', '.', 'replace', '(', '"', '-', '"', ',', '"', '_', '"', ')', '\\', 'n']
Detokenized (011): ['location##lang##ua##ge', '=', 'location##lang##ua##ge', '.', 'replace', '(', '"-"', ',', '"_"', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "currtype = currtype . split ( ) [ 0 ] + \n"
Original    (012): ['currtype', '=', 'currtype', '.', 'split', '(', ')', '[', '0', ']', '+', '\\n']
Tokenized   (019): ['[CLS]', 'cu', '##rr', '##type', '=', 'cu', '##rr', '##type', '.', 'split', '(', ')', '[', '0', ']', '+', '\\', 'n', '[SEP]']
Filtered   (017): ['cu', '##rr', '##type', '=', 'cu', '##rr', '##type', '.', 'split', '(', ')', '[', '0', ']', '+', '\\', 'n']
Detokenized (012): ['cu##rr##type', '=', 'cu##rr##type', '.', 'split', '(', ')', '[', '0', ']', '+', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "insttype = instance . resp . dict [ "title" ] . split ( ) [ : 1 ] \n"
Original    (019): ['insttype', '=', 'instance', '.', 'resp', '.', 'dict', '[', '"title"', ']', '.', 'split', '(', ')', '[', ':', '1', ']', '\\n']
Tokenized   (028): ['[CLS]', 'ins', '##tty', '##pe', '=', 'instance', '.', 'res', '##p', '.', 'di', '##ct', '[', '"', 'title', '"', ']', '.', 'split', '(', ')', '[', ':', '1', ']', '\\', 'n', '[SEP]']
Filtered   (026): ['ins', '##tty', '##pe', '=', 'instance', '.', 'res', '##p', '.', 'di', '##ct', '[', '"', 'title', '"', ']', '.', 'split', '(', ')', '[', ':', '1', ']', '\\', 'n']
Detokenized (019): ['ins##tty##pe', '=', 'instance', '.', 'res##p', '.', 'di##ct', '[', '"title"', ']', '.', 'split', '(', ')', '[', ':', '1', ']', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "nextarg = newarg [ newarg . index ( arg ) + 1 ] \n"
Original    (014): ['nextarg', '=', 'newarg', '[', 'newarg', '.', 'index', '(', 'arg', ')', '+', '1', ']', '\\n']
Tokenized   (024): ['[CLS]', 'next', '##ar', '##g', '=', 'new', '##ar', '##g', '[', 'new', '##ar', '##g', '.', 'index', '(', 'ar', '##g', ')', '+', '1', ']', '\\', 'n', '[SEP]']
Filtered   (022): ['next', '##ar', '##g', '=', 'new', '##ar', '##g', '[', 'new', '##ar', '##g', '.', 'index', '(', 'ar', '##g', ')', '+', '1', ']', '\\', 'n']
Detokenized (014): ['next##ar##g', '=', 'new##ar##g', '[', 'new##ar##g', '.', 'index', '(', 'ar##g', ')', '+', '1', ']', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "regcopy [ nextarg ] = patterninfo \n"
Original    (007): ['regcopy', '[', 'nextarg', ']', '=', 'patterninfo', '\\n']
Tokenized   (016): ['[CLS]', 'reg', '##co', '##py', '[', 'next', '##ar', '##g', ']', '=', 'pattern', '##in', '##fo', '\\', 'n', '[SEP]']
Filtered   (014): ['reg', '##co', '##py', '[', 'next', '##ar', '##g', ']', '=', 'pattern', '##in', '##fo', '\\', 'n']
Detokenized (007): ['reg##co##py', '[', 'next##ar##g', ']', '=', 'pattern##in##fo', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "validictory . validate ( tdict , jsonsch ) \n"
Original    (009): ['validictory', '.', 'validate', '(', 'tdict', ',', 'jsonsch', ')', '\\n']
Tokenized   (019): ['[CLS]', 'valid', '##ic', '##tory', '.', 'valid', '##ate', '(', 'td', '##ic', '##t', ',', 'j', '##sons', '##ch', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['valid', '##ic', '##tory', '.', 'valid', '##ate', '(', 'td', '##ic', '##t', ',', 'j', '##sons', '##ch', ')', '\\', 'n']
Detokenized (009): ['valid##ic##tory', '.', 'valid##ate', '(', 'td##ic##t', ',', 'j##sons##ch', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "wrapper . subsequent_indent = * 4 \n"
Original    (007): ['wrapper', '.', 'subsequent_indent', '=', '*', '4', '\\n']
Tokenized   (014): ['[CLS]', 'wrap', '##per', '.', 'subsequent', '_', 'ind', '##ent', '=', '*', '4', '\\', 'n', '[SEP]']
Filtered   (012): ['wrap', '##per', '.', 'subsequent', '_', 'ind', '##ent', '=', '*', '4', '\\', 'n']
Detokenized (007): ['wrap##per', '.', 'subsequent_ind##ent', '=', '*', '4', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "RegistryValidationError ( \n"
Original    (003): ['RegistryValidationError', '(', '\\n']
Tokenized   (011): ['[CLS]', 'registry', '##val', '##ida', '##tion', '##er', '##ror', '(', '\\', 'n', '[SEP]']
Filtered   (009): ['registry', '##val', '##ida', '##tion', '##er', '##ror', '(', '\\', 'n']
Detokenized (003): ['registry##val##ida##tion##er##ror', '(', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "regentry = self \n"
Original    (004): ['regentry', '=', 'self', '\\n']
Tokenized   (008): ['[CLS]', 'regent', '##ry', '=', 'self', '\\', 'n', '[SEP]']
Filtered   (006): ['regent', '##ry', '=', 'self', '\\', 'n']
Detokenized (004): ['regent##ry', '=', 'self', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : ""\'%(ValueExpression)s\'" % ( self ) , regentry = self ) ) \n"
Original    (012): ['"\\\'%(ValueExpression)s\\\'"', '%', '(', 'self', ')', ',', 'regentry', '=', 'self', ')', ')', '\\n']
Tokenized   (029): ['[CLS]', '"', '\\', "'", '%', '(', 'value', '##ex', '##press', '##ion', ')', 's', '\\', "'", '"', '%', '(', 'self', ')', ',', 'regent', '##ry', '=', 'self', ')', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['"', '\\', "'", '%', '(', 'value', '##ex', '##press', '##ion', ')', 's', '\\', "'", '"', '%', '(', 'self', ')', ',', 'regent', '##ry', '=', 'self', ')', ')', '\\', 'n']
Detokenized (012): ['"\\\'%(value##ex##press##ion)s\\\'"', '%', '(', 'self', ')', ',', 'regent##ry', '=', 'self', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "intval = int ( newval ) \n"
Original    (007): ['intval', '=', 'int', '(', 'newval', ')', '\\n']
Tokenized   (012): ['[CLS]', 'int', '##val', '=', 'int', '(', 'new', '##val', ')', '\\', 'n', '[SEP]']
Filtered   (010): ['int', '##val', '=', 'int', '(', 'new', '##val', ')', '\\', 'n']
Detokenized (007): ['int##val', '=', 'int', '(', 'new##val', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "MICROS_TRANSLATIONS = ( \n"
Original    (004): ['MICROS_TRANSLATIONS', '=', '(', '\\n']
Tokenized   (010): ['[CLS]', 'micro', '##s', '_', 'translations', '=', '(', '\\', 'n', '[SEP]']
Filtered   (008): ['micro', '##s', '_', 'translations', '=', '(', '\\', 'n']
Detokenized (004): ['micro##s_translations', '=', '(', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "MICROS_TRANSLATION_HASH = dict ( ( alt , v ) for k , v in MICROS_TRANSLATIONS for alt in k ) \n"
Original    (021): ['MICROS_TRANSLATION_HASH', '=', 'dict', '(', '(', 'alt', ',', 'v', ')', 'for', 'k', ',', 'v', 'in', 'MICROS_TRANSLATIONS', 'for', 'alt', 'in', 'k', ')', '\\n']
Tokenized   (033): ['[CLS]', 'micro', '##s', '_', 'translation', '_', 'hash', '=', 'di', '##ct', '(', '(', 'alt', ',', 'v', ')', 'for', 'k', ',', 'v', 'in', 'micro', '##s', '_', 'translations', 'for', 'alt', 'in', 'k', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['micro', '##s', '_', 'translation', '_', 'hash', '=', 'di', '##ct', '(', '(', 'alt', ',', 'v', ')', 'for', 'k', ',', 'v', 'in', 'micro', '##s', '_', 'translations', 'for', 'alt', 'in', 'k', ')', '\\', 'n']
Detokenized (021): ['micro##s_translation_hash', '=', 'di##ct', '(', '(', 'alt', ',', 'v', ')', 'for', 'k', ',', 'v', 'in', 'micro##s_translations', 'for', 'alt', 'in', 'k', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "epoch_milliseconds = epoch_millis = milliseconds = millis = ms \n"
Original    (010): ['epoch_milliseconds', '=', 'epoch_millis', '=', 'milliseconds', '=', 'millis', '=', 'ms', '\\n']
Tokenized   (025): ['[CLS]', 'epoch', '_', 'mill', '##ise', '##con', '##ds', '=', 'epoch', '_', 'mill', '##is', '=', 'mill', '##ise', '##con', '##ds', '=', 'mill', '##is', '=', 'ms', '\\', 'n', '[SEP]']
Filtered   (023): ['epoch', '_', 'mill', '##ise', '##con', '##ds', '=', 'epoch', '_', 'mill', '##is', '=', 'mill', '##ise', '##con', '##ds', '=', 'mill', '##is', '=', 'ms', '\\', 'n']
Detokenized (010): ['epoch_mill##ise##con##ds', '=', 'epoch_mill##is', '=', 'mill##ise##con##ds', '=', 'mill##is', '=', 'ms', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "epoch_microseconds = epoch_micros = microseconds = micros \n"
Original    (008): ['epoch_microseconds', '=', 'epoch_micros', '=', 'microseconds', '=', 'micros', '\\n']
Tokenized   (023): ['[CLS]', 'epoch', '_', 'micro', '##se', '##con', '##ds', '=', 'epoch', '_', 'micro', '##s', '=', 'micro', '##se', '##con', '##ds', '=', 'micro', '##s', '\\', 'n', '[SEP]']
Filtered   (021): ['epoch', '_', 'micro', '##se', '##con', '##ds', '=', 'epoch', '_', 'micro', '##s', '=', 'micro', '##se', '##con', '##ds', '=', 'micro', '##s', '\\', 'n']
Detokenized (008): ['epoch_micro##se##con##ds', '=', 'epoch_micro##s', '=', 'micro##se##con##ds', '=', 'micro##s', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "micros = u".%06d" % dt . microsecond if dt . microsecond else \n"
Original    (013): ['micros', '=', 'u".%06d"', '%', 'dt', '.', 'microsecond', 'if', 'dt', '.', 'microsecond', 'else', '\\n']
Tokenized   (029): ['[CLS]', 'micro', '##s', '=', 'u', '"', '.', '%', '06', '##d', '"', '%', 'dt', '.', 'micro', '##se', '##con', '##d', 'if', 'dt', '.', 'micro', '##se', '##con', '##d', 'else', '\\', 'n', '[SEP]']
Filtered   (027): ['micro', '##s', '=', 'u', '"', '.', '%', '06', '##d', '"', '%', 'dt', '.', 'micro', '##se', '##con', '##d', 'if', 'dt', '.', 'micro', '##se', '##con', '##d', 'else', '\\', 'n']
Detokenized (013): ['micro##s', '=', 'u".%06##d"', '%', 'dt', '.', 'micro##se##con##d', 'if', 'dt', '.', 'micro##se##con##d', 'else', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "datastore_owner_uuid = request . REQUEST [ "datastore_owner__uuid" ] \n"
Original    (009): ['datastore_owner_uuid', '=', 'request', '.', 'REQUEST', '[', '"datastore_owner__uuid"', ']', '\\n']
Tokenized   (029): ['[CLS]', 'data', '##stor', '##e', '_', 'owner', '_', 'u', '##uid', '=', 'request', '.', 'request', '[', '"', 'data', '##stor', '##e', '_', 'owner', '_', '_', 'u', '##uid', '"', ']', '\\', 'n', '[SEP]']
Filtered   (027): ['data', '##stor', '##e', '_', 'owner', '_', 'u', '##uid', '=', 'request', '.', 'request', '[', '"', 'data', '##stor', '##e', '_', 'owner', '_', '_', 'u', '##uid', '"', ']', '\\', 'n']
Detokenized (009): ['data##stor##e_owner_u##uid', '=', 'request', '.', 'request', '[', '"data##stor##e_owner__u##uid"', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "datastore_owner , ds_owner_created = Profile . objects . get_or_create ( uuid = datastore_owner_uuid ) \n"
Original    (015): ['datastore_owner', ',', 'ds_owner_created', '=', 'Profile', '.', 'objects', '.', 'get_or_create', '(', 'uuid', '=', 'datastore_owner_uuid', ')', '\\n']
Tokenized   (038): ['[CLS]', 'data', '##stor', '##e', '_', 'owner', ',', 'ds', '_', 'owner', '_', 'created', '=', 'profile', '.', 'objects', '.', 'get', '_', 'or', '_', 'create', '(', 'u', '##uid', '=', 'data', '##stor', '##e', '_', 'owner', '_', 'u', '##uid', ')', '\\', 'n', '[SEP]']
Filtered   (036): ['data', '##stor', '##e', '_', 'owner', ',', 'ds', '_', 'owner', '_', 'created', '=', 'profile', '.', 'objects', '.', 'get', '_', 'or', '_', 'create', '(', 'u', '##uid', '=', 'data', '##stor', '##e', '_', 'owner', '_', 'u', '##uid', ')', '\\', 'n']
Detokenized (015): ['data##stor##e_owner', ',', 'ds_owner_created', '=', 'profile', '.', 'objects', '.', 'get_or_create', '(', 'u##uid', '=', 'data##stor##e_owner_u##uid', ')', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "port = db . Column ( db . Integer , nullable = False ) \n"
Original    (015): ['port', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'nullable', '=', 'False', ')', '\\n']
Tokenized   (019): ['[CLS]', 'port', '=', 'db', '.', 'column', '(', 'db', '.', 'integer', ',', 'null', '##able', '=', 'false', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['port', '=', 'db', '.', 'column', '(', 'db', '.', 'integer', ',', 'null', '##able', '=', 'false', ')', '\\', 'n']
Detokenized (015): ['port', '=', 'db', '.', 'column', '(', 'db', '.', 'integer', ',', 'null##able', '=', 'false', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "eru_container_id = db . Column ( db . String ( 64 ) , index = True ) \n"
Original    (018): ['eru_container_id', '=', 'db', '.', 'Column', '(', 'db', '.', 'String', '(', '64', ')', ',', 'index', '=', 'True', ')', '\\n']
Tokenized   (026): ['[CLS]', 'er', '##u', '_', 'container', '_', 'id', '=', 'db', '.', 'column', '(', 'db', '.', 'string', '(', '64', ')', ',', 'index', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['er', '##u', '_', 'container', '_', 'id', '=', 'db', '.', 'column', '(', 'db', '.', 'string', '(', '64', ')', ',', 'index', '=', 'true', ')', '\\', 'n']
Detokenized (018): ['er##u_container_id', '=', 'db', '.', 'column', '(', 'db', '.', 'string', '(', '64', ')', ',', 'index', '=', 'true', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "suppress_alert = db . Column ( db . Integer , nullable = False , default = 1 ) \n"
Original    (019): ['suppress_alert', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'nullable', '=', 'False', ',', 'default', '=', '1', ')', '\\n']
Tokenized   (025): ['[CLS]', 'suppress', '_', 'alert', '=', 'db', '.', 'column', '(', 'db', '.', 'integer', ',', 'null', '##able', '=', 'false', ',', 'default', '=', '1', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['suppress', '_', 'alert', '=', 'db', '.', 'column', '(', 'db', '.', 'integer', ',', 'null', '##able', '=', 'false', ',', 'default', '=', '1', ')', '\\', 'n']
Detokenized (019): ['suppress_alert', '=', 'db', '.', 'column', '(', 'db', '.', 'integer', ',', 'null##able', '=', 'false', ',', 'default', '=', '1', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "__table_args__ = ( db . Index ( , , , unique = True ) , ) \n"
Original    (017): ['__table_args__', '=', '(', 'db', '.', 'Index', '(', ',', ',', ',', 'unique', '=', 'True', ')', ',', ')', '\\n']
Tokenized   (027): ['[CLS]', '_', '_', 'table', '_', 'ar', '##gs', '_', '_', '=', '(', 'db', '.', 'index', '(', ',', ',', ',', 'unique', '=', 'true', ')', ',', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['_', '_', 'table', '_', 'ar', '##gs', '_', '_', '=', '(', 'db', '.', 'index', '(', ',', ',', ',', 'unique', '=', 'true', ')', ',', ')', '\\', 'n']
Detokenized (017): ['__table_ar##gs__', '=', '(', 'db', '.', 'index', '(', ',', ',', ',', 'unique', '=', 'true', ')', ',', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "cluster_id = cluster_id ) \n"
Original    (005): ['cluster_id', '=', 'cluster_id', ')', '\\n']
Tokenized   (012): ['[CLS]', 'cluster', '_', 'id', '=', 'cluster', '_', 'id', ')', '\\', 'n', '[SEP]']
Filtered   (010): ['cluster', '_', 'id', '=', 'cluster', '_', 'id', ')', '\\', 'n']
Detokenized (005): ['cluster_id', '=', 'cluster_id', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "Proxy . id . desc ( ) ) . offset ( offset ) . limit ( limit ) . all ( ) \n"
Original    (023): ['Proxy', '.', 'id', '.', 'desc', '(', ')', ')', '.', 'offset', '(', 'offset', ')', '.', 'limit', '(', 'limit', ')', '.', 'all', '(', ')', '\\n']
Tokenized   (027): ['[CLS]', 'proxy', '.', 'id', '.', 'des', '##c', '(', ')', ')', '.', 'offset', '(', 'offset', ')', '.', 'limit', '(', 'limit', ')', '.', 'all', '(', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['proxy', '.', 'id', '.', 'des', '##c', '(', ')', ')', '.', 'offset', '(', 'offset', ')', '.', 'limit', '(', 'limit', ')', '.', 'all', '(', ')', '\\', 'n']
Detokenized (023): ['proxy', '.', 'id', '.', 'des##c', '(', ')', ')', '.', 'offset', '(', 'offset', ')', '.', 'limit', '(', 'limit', ')', '.', 'all', '(', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "iou_as_issuer . issue_funds ( amount_issued , ) \n"
Original    (008): ['iou_as_issuer', '.', 'issue_funds', '(', 'amount_issued', ',', ')', '\\n']
Tokenized   (021): ['[CLS]', 'io', '##u', '_', 'as', '_', 'issue', '##r', '.', 'issue', '_', 'funds', '(', 'amount', '_', 'issued', ',', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['io', '##u', '_', 'as', '_', 'issue', '##r', '.', 'issue', '_', 'funds', '(', 'amount', '_', 'issued', ',', ')', '\\', 'n']
Detokenized (008): ['io##u_as_issue##r', '.', 'issue_funds', '(', 'amount_issued', ',', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "github_info_json = urllib2 . urlopen ( latest ) . read ( ) \n"
Original    (013): ['github_info_json', '=', 'urllib2', '.', 'urlopen', '(', 'latest', ')', '.', 'read', '(', ')', '\\n']
Tokenized   (028): ['[CLS]', 'gi', '##th', '##ub', '_', 'info', '_', 'j', '##son', '=', 'ur', '##lli', '##b', '##2', '.', 'ur', '##lo', '##pen', '(', 'latest', ')', '.', 'read', '(', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['gi', '##th', '##ub', '_', 'info', '_', 'j', '##son', '=', 'ur', '##lli', '##b', '##2', '.', 'ur', '##lo', '##pen', '(', 'latest', ')', '.', 'read', '(', ')', '\\', 'n']
Detokenized (013): ['gi##th##ub_info_j##son', '=', 'ur##lli##b##2', '.', 'ur##lo##pen', '(', 'latest', ')', '.', 'read', '(', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "drawing_tool . set_window_title ( update_notifier , watching_player = twitch_username , updates_queued = len ( new_states_queue ) , read_delay = opt . read_delay ) \n"
Original    (024): ['drawing_tool', '.', 'set_window_title', '(', 'update_notifier', ',', 'watching_player', '=', 'twitch_username', ',', 'updates_queued', '=', 'len', '(', 'new_states_queue', ')', ',', 'read_delay', '=', 'opt', '.', 'read_delay', ')', '\\n']
Tokenized   (052): ['[CLS]', 'drawing', '_', 'tool', '.', 'set', '_', 'window', '_', 'title', '(', 'update', '_', 'not', '##ifier', ',', 'watching', '_', 'player', '=', 'twitch', '_', 'user', '##name', ',', 'updates', '_', 'queue', '##d', '=', 'len', '(', 'new', '_', 'states', '_', 'queue', ')', ',', 'read', '_', 'delay', '=', 'opt', '.', 'read', '_', 'delay', ')', '\\', 'n', '[SEP]']
Filtered   (050): ['drawing', '_', 'tool', '.', 'set', '_', 'window', '_', 'title', '(', 'update', '_', 'not', '##ifier', ',', 'watching', '_', 'player', '=', 'twitch', '_', 'user', '##name', ',', 'updates', '_', 'queue', '##d', '=', 'len', '(', 'new', '_', 'states', '_', 'queue', ')', ',', 'read', '_', 'delay', '=', 'opt', '.', 'read', '_', 'delay', ')', '\\', 'n']
Detokenized (024): ['drawing_tool', '.', 'set_window_title', '(', 'update_not##ifier', ',', 'watching_player', '=', 'twitch_user##name', ',', 'updates_queue##d', '=', 'len', '(', 'new_states_queue', ')', ',', 'read_delay', '=', 'opt', '.', 'read_delay', ')', '\\n']
Counter: 50
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "put_url = opt . trackerserver_url + "/tracker/api/update/" + opt . trackerserver_authkey \n"
Original    (012): ['put_url', '=', 'opt', '.', 'trackerserver_url', '+', '"/tracker/api/update/"', '+', 'opt', '.', 'trackerserver_authkey', '\\n']
Tokenized   (037): ['[CLS]', 'put', '_', 'ur', '##l', '=', 'opt', '.', 'tracker', '##ser', '##ver', '_', 'ur', '##l', '+', '"', '/', 'tracker', '/', 'api', '/', 'update', '/', '"', '+', 'opt', '.', 'tracker', '##ser', '##ver', '_', 'au', '##th', '##key', '\\', 'n', '[SEP]']
Filtered   (035): ['put', '_', 'ur', '##l', '=', 'opt', '.', 'tracker', '##ser', '##ver', '_', 'ur', '##l', '+', '"', '/', 'tracker', '/', 'api', '/', 'update', '/', '"', '+', 'opt', '.', 'tracker', '##ser', '##ver', '_', 'au', '##th', '##key', '\\', 'n']
Detokenized (012): ['put_ur##l', '=', 'opt', '.', 'tracker##ser##ver_ur##l', '+', '"/tracker/api/update/"', '+', 'opt', '.', 'tracker##ser##ver_au##th##key', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "json_string = json . dumps ( state , cls = TrackerStateEncoder , sort_keys = True ) \n"
Original    (017): ['json_string', '=', 'json', '.', 'dumps', '(', 'state', ',', 'cls', '=', 'TrackerStateEncoder', ',', 'sort_keys', '=', 'True', ')', '\\n']
Tokenized   (032): ['[CLS]', 'j', '##son', '_', 'string', '=', 'j', '##son', '.', 'dump', '##s', '(', 'state', ',', 'cl', '##s', '=', 'tracker', '##sta', '##tee', '##nco', '##der', ',', 'sort', '_', 'keys', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (030): ['j', '##son', '_', 'string', '=', 'j', '##son', '.', 'dump', '##s', '(', 'state', ',', 'cl', '##s', '=', 'tracker', '##sta', '##tee', '##nco', '##der', ',', 'sort', '_', 'keys', '=', 'true', ')', '\\', 'n']
Detokenized (017): ['j##son_string', '=', 'j##son', '.', 'dump##s', '(', 'state', ',', 'cl##s', '=', 'tracker##sta##tee##nco##der', ',', 'sort_keys', '=', 'true', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "new_states_queue . pop ( 0 ) \n"
Original    (007): ['new_states_queue', '.', 'pop', '(', '0', ')', '\\n']
Tokenized   (014): ['[CLS]', 'new', '_', 'states', '_', 'queue', '.', 'pop', '(', '0', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['new', '_', 'states', '_', 'queue', '.', 'pop', '(', '0', ')', '\\', 'n']
Detokenized (007): ['new_states_queue', '.', 'pop', '(', '0', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "framecount += 1 \n"
Original    (004): ['framecount', '+=', '1', '\\n']
Tokenized   (010): ['[CLS]', 'frame', '##co', '##unt', '+', '=', '1', '\\', 'n', '[SEP]']
Filtered   (008): ['frame', '##co', '##unt', '+', '=', '1', '\\', 'n']
Detokenized (004): ['frame##co##unt', '+=', '1', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "decay = decay , \n"
Original    (005): ['decay', '=', 'decay', ',', '\\n']
Tokenized   (008): ['[CLS]', 'decay', '=', 'decay', ',', '\\', 'n', '[SEP]']
Filtered   (006): ['decay', '=', 'decay', ',', '\\', 'n']
Detokenized (005): ['decay', '=', 'decay', ',', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "expected_kwargs = { , } \n"
Original    (006): ['expected_kwargs', '=', '{', ',', '}', '\\n']
Tokenized   (013): ['[CLS]', 'expected', '_', 'kw', '##ar', '##gs', '=', '{', ',', '}', '\\', 'n', '[SEP]']
Filtered   (011): ['expected', '_', 'kw', '##ar', '##gs', '=', '{', ',', '}', '\\', 'n']
Detokenized (006): ['expected_kw##ar##gs', '=', '{', ',', '}', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "outputs [ ] = in_shapes [ ] \n"
Original    (008): ['outputs', '[', ']', '=', 'in_shapes', '[', ']', '\\n']
Tokenized   (013): ['[CLS]', 'outputs', '[', ']', '=', 'in', '_', 'shapes', '[', ']', '\\', 'n', '[SEP]']
Filtered   (011): ['outputs', '[', ']', '=', 'in', '_', 'shapes', '[', ']', '\\', 'n']
Detokenized (008): ['outputs', '[', ']', '=', 'in_shapes', '[', ']', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "buf = BufferStructure ( self . in_shapes [ ] . feature_shape [ - 1 ] ) \n"
Original    (017): ['buf', '=', 'BufferStructure', '(', 'self', '.', 'in_shapes', '[', ']', '.', 'feature_shape', '[', '-', '1', ']', ')', '\\n']
Tokenized   (028): ['[CLS]', 'bu', '##f', '=', 'buffer', '##st', '##ru', '##cture', '(', 'self', '.', 'in', '_', 'shapes', '[', ']', '.', 'feature', '_', 'shape', '[', '-', '1', ']', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['bu', '##f', '=', 'buffer', '##st', '##ru', '##cture', '(', 'self', '.', 'in', '_', 'shapes', '[', ']', '.', 'feature', '_', 'shape', '[', '-', '1', ']', ')', '\\', 'n']
Detokenized (017): ['bu##f', '=', 'buffer##st##ru##cture', '(', 'self', '.', 'in_shapes', '[', ']', '.', 'feature_shape', '[', '-', '1', ']', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "internals [ ] = self . in_shapes [ ] \n"
Original    (010): ['internals', '[', ']', '=', 'self', '.', 'in_shapes', '[', ']', '\\n']
Tokenized   (016): ['[CLS]', 'internal', '##s', '[', ']', '=', 'self', '.', 'in', '_', 'shapes', '[', ']', '\\', 'n', '[SEP]']
Filtered   (014): ['internal', '##s', '[', ']', '=', 'self', '.', 'in', '_', 'shapes', '[', ']', '\\', 'n']
Detokenized (010): ['internal##s', '[', ']', '=', 'self', '.', 'in_shapes', '[', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sigma_b , centered , x_hat = buffers . internals \n"
Original    (010): ['sigma_b', ',', 'centered', ',', 'x_hat', '=', 'buffers', '.', 'internals', '\\n']
Tokenized   (019): ['[CLS]', 'sigma', '_', 'b', ',', 'centered', ',', 'x', '_', 'hat', '=', 'buffer', '##s', '.', 'internal', '##s', '\\', 'n', '[SEP]']
Filtered   (017): ['sigma', '_', 'b', ',', 'centered', ',', 'x', '_', 'hat', '=', 'buffer', '##s', '.', 'internal', '##s', '\\', 'n']
Detokenized (010): ['sigma_b', ',', 'centered', ',', 'x_hat', '=', 'buffer##s', '.', 'internal##s', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "dgamma = buffers . gradients . gamma \n"
Original    (008): ['dgamma', '=', 'buffers', '.', 'gradients', '.', 'gamma', '\\n']
Tokenized   (015): ['[CLS]', 'd', '##gam', '##ma', '=', 'buffer', '##s', '.', 'gradient', '##s', '.', 'gamma', '\\', 'n', '[SEP]']
Filtered   (013): ['d', '##gam', '##ma', '=', 'buffer', '##s', '.', 'gradient', '##s', '.', 'gamma', '\\', 'n']
Detokenized (008): ['d##gam##ma', '=', 'buffer##s', '.', 'gradient##s', '.', 'gamma', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "outdeltas = flatten_all_but_last ( buffers . output_deltas . default ) \n"
Original    (011): ['outdeltas', '=', 'flatten_all_but_last', '(', 'buffers', '.', 'output_deltas', '.', 'default', ')', '\\n']
Tokenized   (027): ['[CLS]', 'out', '##del', '##tas', '=', 'flat', '##ten', '_', 'all', '_', 'but', '_', 'last', '(', 'buffer', '##s', '.', 'output', '_', 'delta', '##s', '.', 'default', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['out', '##del', '##tas', '=', 'flat', '##ten', '_', 'all', '_', 'but', '_', 'last', '(', 'buffer', '##s', '.', 'output', '_', 'delta', '##s', '.', 'default', ')', '\\', 'n']
Detokenized (011): ['out##del##tas', '=', 'flat##ten_all_but_last', '(', 'buffer##s', '.', 'output_delta##s', '.', 'default', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "_h . add_tt ( term4 , indeltas , indeltas ) \n"
Original    (011): ['_h', '.', 'add_tt', '(', 'term4', ',', 'indeltas', ',', 'indeltas', ')', '\\n']
Tokenized   (022): ['[CLS]', '_', 'h', '.', 'add', '_', 'tt', '(', 'term', '##4', ',', 'ind', '##elt', '##as', ',', 'ind', '##elt', '##as', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['_', 'h', '.', 'add', '_', 'tt', '(', 'term', '##4', ',', 'ind', '##elt', '##as', ',', 'ind', '##elt', '##as', ')', '\\', 'n']
Detokenized (011): ['_h', '.', 'add_tt', '(', 'term##4', ',', 'ind##elt##as', ',', 'ind##elt##as', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "targets_name = , mask_name = ) : \n"
Original    (008): ['targets_name', '=', ',', 'mask_name', '=', ')', ':', '\\n']
Tokenized   (015): ['[CLS]', 'targets', '_', 'name', '=', ',', 'mask', '_', 'name', '=', ')', ':', '\\', 'n', '[SEP]']
Filtered   (013): ['targets', '_', 'name', '=', ',', 'mask', '_', 'name', '=', ')', ':', '\\', 'n']
Detokenized (008): ['targets_name', '=', ',', 'mask_name', '=', ')', ':', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "mask_name = , name = None ) : \n"
Original    (009): ['mask_name', '=', ',', 'name', '=', 'None', ')', ':', '\\n']
Tokenized   (014): ['[CLS]', 'mask', '_', 'name', '=', ',', 'name', '=', 'none', ')', ':', '\\', 'n', '[SEP]']
Filtered   (012): ['mask', '_', 'name', '=', ',', 'name', '=', 'none', ')', ':', '\\', 'n']
Detokenized (009): ['mask_name', '=', ',', 'name', '=', 'none', ')', ':', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "true_labels ) . astype ( np . float ) \n"
Original    (010): ['true_labels', ')', '.', 'astype', '(', 'np', '.', 'float', ')', '\\n']
Tokenized   (016): ['[CLS]', 'true', '_', 'labels', ')', '.', 'as', '##type', '(', 'np', '.', 'float', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['true', '_', 'labels', ')', '.', 'as', '##type', '(', 'np', '.', 'float', ')', '\\', 'n']
Detokenized (010): ['true_labels', ')', '.', 'as##type', '(', 'np', '.', 'float', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "epochs = [ 0 ] * 4 + [ 1 ] * 4 + [ 2 ] * 4 \n"
Original    (020): ['epochs', '=', '[', '0', ']', '*', '4', '+', '[', '1', ']', '*', '4', '+', '[', '2', ']', '*', '4', '\\n']
Tokenized   (024): ['[CLS]', 'epoch', '##s', '=', '[', '0', ']', '*', '4', '+', '[', '1', ']', '*', '4', '+', '[', '2', ']', '*', '4', '\\', 'n', '[SEP]']
Filtered   (022): ['epoch', '##s', '=', '[', '0', ']', '*', '4', '+', '[', '1', ']', '*', '4', '+', '[', '2', ']', '*', '4', '\\', 'n']
Detokenized (020): ['epoch##s', '=', '[', '0', ']', '*', '4', '+', '[', '1', ']', '*', '4', '+', '[', '2', ']', '*', '4', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "on_rtd = os . environ . get ( , None ) == \n"
Original    (013): ['on_rtd', '=', 'os', '.', 'environ', '.', 'get', '(', ',', 'None', ')', '==', '\\n']
Tokenized   (022): ['[CLS]', 'on', '_', 'rt', '##d', '=', 'os', '.', 'en', '##vir', '##on', '.', 'get', '(', ',', 'none', ')', '=', '=', '\\', 'n', '[SEP]']
Filtered   (020): ['on', '_', 'rt', '##d', '=', 'os', '.', 'en', '##vir', '##on', '.', 'get', '(', ',', 'none', ')', '=', '=', '\\', 'n']
Detokenized (013): ['on_rt##d', '=', 'os', '.', 'en##vir##on', '.', 'get', '(', ',', 'none', ')', '==', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "html_theme_path = [ sphinx_rtd_theme . get_html_theme_path ( ) ] \n"
Original    (010): ['html_theme_path', '=', '[', 'sphinx_rtd_theme', '.', 'get_html_theme_path', '(', ')', ']', '\\n']
Tokenized   (028): ['[CLS]', 'html', '_', 'theme', '_', 'path', '=', '[', 'sphinx', '_', 'rt', '##d', '_', 'theme', '.', 'get', '_', 'html', '_', 'theme', '_', 'path', '(', ')', ']', '\\', 'n', '[SEP]']
Filtered   (026): ['html', '_', 'theme', '_', 'path', '=', '[', 'sphinx', '_', 'rt', '##d', '_', 'theme', '.', 'get', '_', 'html', '_', 'theme', '_', 'path', '(', ')', ']', '\\', 'n']
Detokenized (010): ['html_theme_path', '=', '[', 'sphinx_rt##d_theme', '.', 'get_html_theme_path', '(', ')', ']', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "latex_elements = { \n"
Original    (004): ['latex_elements', '=', '{', '\\n']
Tokenized   (010): ['[CLS]', 'late', '##x', '_', 'elements', '=', '{', '\\', 'n', '[SEP]']
Filtered   (008): ['late', '##x', '_', 'elements', '=', '{', '\\', 'n']
Detokenized (004): ['late##x_elements', '=', '{', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "latex_documents = [ \n"
Original    (004): ['latex_documents', '=', '[', '\\n']
Tokenized   (010): ['[CLS]', 'late', '##x', '_', 'documents', '=', '[', '\\', 'n', '[SEP]']
Filtered   (008): ['late', '##x', '_', 'documents', '=', '[', '\\', 'n']
Detokenized (004): ['late##x_documents', '=', '[', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "ignored_fallbacks = ( ) ) : \n"
Original    (007): ['ignored_fallbacks', '=', '(', ')', ')', ':', '\\n']
Tokenized   (013): ['[CLS]', 'ignored', '_', 'fall', '##backs', '=', '(', ')', ')', ':', '\\', 'n', '[SEP]']
Filtered   (011): ['ignored', '_', 'fall', '##backs', '=', '(', ')', ')', ':', '\\', 'n']
Detokenized (007): ['ignored_fall##backs', '=', '(', ')', ')', ':', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""b" : 2.0 , \n"
Original    (005): ['"b"', ':', '2.0', ',', '\\n']
Tokenized   (012): ['[CLS]', '"', 'b', '"', ':', '2', '.', '0', ',', '\\', 'n', '[SEP]']
Filtered   (010): ['"', 'b', '"', ':', '2', '.', '0', ',', '\\', 'n']
Detokenized (005): ['"b"', ':', '2.0', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""c" : True , \n"
Original    (005): ['"c"', ':', 'True', ',', '\\n']
Tokenized   (010): ['[CLS]', '"', 'c', '"', ':', 'true', ',', '\\', 'n', '[SEP]']
Filtered   (008): ['"', 'c', '"', ':', 'true', ',', '\\', 'n']
Detokenized (005): ['"c"', ':', 'true', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""d" : , \n"
Original    (004): ['"d"', ':', ',', '\\n']
Tokenized   (009): ['[CLS]', '"', 'd', '"', ':', ',', '\\', 'n', '[SEP]']
Filtered   (007): ['"', 'd', '"', ':', ',', '\\', 'n']
Detokenized (004): ['"d"', ':', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : ""e" : [ 1 , 2 , 3 ] , \n"
Original    (011): ['"e"', ':', '[', '1', ',', '2', ',', '3', ']', ',', '\\n']
Tokenized   (016): ['[CLS]', '"', 'e', '"', ':', '[', '1', ',', '2', ',', '3', ']', ',', '\\', 'n', '[SEP]']
Filtered   (014): ['"', 'e', '"', ':', '[', '1', ',', '2', ',', '3', ']', ',', '\\', 'n']
Detokenized (011): ['"e"', ':', '[', '1', ',', '2', ',', '3', ']', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""f" : { : , : } , \n"
Original    (009): ['"f"', ':', '{', ':', ',', ':', '}', ',', '\\n']
Tokenized   (014): ['[CLS]', '"', 'f', '"', ':', '{', ':', ',', ':', '}', ',', '\\', 'n', '[SEP]']
Filtered   (012): ['"', 'f', '"', ':', '{', ':', ',', ':', '}', ',', '\\', 'n']
Detokenized (009): ['"f"', ':', '{', ':', ',', ':', '}', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ""answer" : 42 \n"
Original    (004): ['"answer"', ':', '42', '\\n']
Tokenized   (009): ['[CLS]', '"', 'answer', '"', ':', '42', '\\', 'n', '[SEP]']
Filtered   (007): ['"', 'answer', '"', ':', '42', '\\', 'n']
Detokenized (004): ['"answer"', ':', '42', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "p_error = self . kp * current_error \n"
Original    (008): ['p_error', '=', 'self', '.', 'kp', '*', 'current_error', '\\n']
Tokenized   (016): ['[CLS]', 'p', '_', 'error', '=', 'self', '.', 'k', '##p', '*', 'current', '_', 'error', '\\', 'n', '[SEP]']
Filtered   (014): ['p', '_', 'error', '=', 'self', '.', 'k', '##p', '*', 'current', '_', 'error', '\\', 'n']
Detokenized (008): ['p_error', '=', 'self', '.', 'k##p', '*', 'current_error', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "d_error = self . kd * ( current_error - self . previous_error ) / timestep \n"
Original    (016): ['d_error', '=', 'self', '.', 'kd', '*', '(', 'current_error', '-', 'self', '.', 'previous_error', ')', '/', 'timestep', '\\n']
Tokenized   (028): ['[CLS]', 'd', '_', 'error', '=', 'self', '.', 'k', '##d', '*', '(', 'current', '_', 'error', '-', 'self', '.', 'previous', '_', 'error', ')', '/', 'times', '##te', '##p', '\\', 'n', '[SEP]']
Filtered   (026): ['d', '_', 'error', '=', 'self', '.', 'k', '##d', '*', '(', 'current', '_', 'error', '-', 'self', '.', 'previous', '_', 'error', ')', '/', 'times', '##te', '##p', '\\', 'n']
Detokenized (016): ['d_error', '=', 'self', '.', 'k##d', '*', '(', 'current_error', '-', 'self', '.', 'previous_error', ')', '/', 'times##te##p', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "current_error + self . previous_error ) / 2 + self . integral_error \n"
Original    (013): ['current_error', '+', 'self', '.', 'previous_error', ')', '/', '2', '+', 'self', '.', 'integral_error', '\\n']
Tokenized   (022): ['[CLS]', 'current', '_', 'error', '+', 'self', '.', 'previous', '_', 'error', ')', '/', '2', '+', 'self', '.', 'integral', '_', 'error', '\\', 'n', '[SEP]']
Filtered   (020): ['current', '_', 'error', '+', 'self', '.', 'previous', '_', 'error', ')', '/', '2', '+', 'self', '.', 'integral', '_', 'error', '\\', 'n']
Detokenized (013): ['current_error', '+', 'self', '.', 'previous_error', ')', '/', '2', '+', 'self', '.', 'integral_error', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "i_error = self . ki * self . integral_error \n"
Original    (010): ['i_error', '=', 'self', '.', 'ki', '*', 'self', '.', 'integral_error', '\\n']
Tokenized   (017): ['[CLS]', 'i', '_', 'error', '=', 'self', '.', 'ki', '*', 'self', '.', 'integral', '_', 'error', '\\', 'n', '[SEP]']
Filtered   (015): ['i', '_', 'error', '=', 'self', '.', 'ki', '*', 'self', '.', 'integral', '_', 'error', '\\', 'n']
Detokenized (010): ['i_error', '=', 'self', '.', 'ki', '*', 'self', '.', 'integral_error', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "total_error = p_error + d_error + i_error \n"
Original    (008): ['total_error', '=', 'p_error', '+', 'd_error', '+', 'i_error', '\\n']
Tokenized   (019): ['[CLS]', 'total', '_', 'error', '=', 'p', '_', 'error', '+', 'd', '_', 'error', '+', 'i', '_', 'error', '\\', 'n', '[SEP]']
Filtered   (017): ['total', '_', 'error', '=', 'p', '_', 'error', '+', 'd', '_', 'error', '+', 'i', '_', 'error', '\\', 'n']
Detokenized (008): ['total_error', '=', 'p_error', '+', 'd_error', '+', 'i_error', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "cmd_match_names = cmd . Cmd . completenames ( self , text , * ignored ) \n"
Original    (016): ['cmd_match_names', '=', 'cmd', '.', 'Cmd', '.', 'completenames', '(', 'self', ',', 'text', ',', '*', 'ignored', ')', '\\n']
Tokenized   (028): ['[CLS]', 'cm', '##d', '_', 'match', '_', 'names', '=', 'cm', '##d', '.', 'cm', '##d', '.', 'complete', '##name', '##s', '(', 'self', ',', 'text', ',', '*', 'ignored', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['cm', '##d', '_', 'match', '_', 'names', '=', 'cm', '##d', '.', 'cm', '##d', '.', 'complete', '##name', '##s', '(', 'self', ',', 'text', ',', '*', 'ignored', ')', '\\', 'n']
Detokenized (016): ['cm##d_match_names', '=', 'cm##d', '.', 'cm##d', '.', 'complete##name##s', '(', 'self', ',', 'text', ',', '*', 'ignored', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "obj_names = self . ctrl_client . objects . keys ( ) \n"
Original    (012): ['obj_names', '=', 'self', '.', 'ctrl_client', '.', 'objects', '.', 'keys', '(', ')', '\\n']
Tokenized   (021): ['[CLS]', 'ob', '##j', '_', 'names', '=', 'self', '.', 'ct', '##rl', '_', 'client', '.', 'objects', '.', 'keys', '(', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['ob', '##j', '_', 'names', '=', 'self', '.', 'ct', '##rl', '_', 'client', '.', 'objects', '.', 'keys', '(', ')', '\\', 'n']
Detokenized (012): ['ob##j_names', '=', 'self', '.', 'ct##rl_client', '.', 'objects', '.', 'keys', '(', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "api_match_names = [ x for x in obj_names if x . startswith ( text ) ] \n"
Original    (017): ['api_match_names', '=', '[', 'x', 'for', 'x', 'in', 'obj_names', 'if', 'x', '.', 'startswith', '(', 'text', ')', ']', '\\n']
Tokenized   (028): ['[CLS]', 'api', '_', 'match', '_', 'names', '=', '[', 'x', 'for', 'x', 'in', 'ob', '##j', '_', 'names', 'if', 'x', '.', 'starts', '##with', '(', 'text', ')', ']', '\\', 'n', '[SEP]']
Filtered   (026): ['api', '_', 'match', '_', 'names', '=', '[', 'x', 'for', 'x', 'in', 'ob', '##j', '_', 'names', 'if', 'x', '.', 'starts', '##with', '(', 'text', ')', ']', '\\', 'n']
Detokenized (017): ['api_match_names', '=', '[', 'x', 'for', 'x', 'in', 'ob##j_names', 'if', 'x', '.', 'starts##with', '(', 'text', ')', ']', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "reply_time = self . ctrl_client . ping ( ) \n"
Original    (010): ['reply_time', '=', 'self', '.', 'ctrl_client', '.', 'ping', '(', ')', '\\n']
Tokenized   (018): ['[CLS]', 'reply', '_', 'time', '=', 'self', '.', 'ct', '##rl', '_', 'client', '.', 'ping', '(', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['reply', '_', 'time', '=', 'self', '.', 'ct', '##rl', '_', 'client', '.', 'ping', '(', ')', '\\', 'n']
Detokenized (010): ['reply_time', '=', 'self', '.', 'ct##rl_client', '.', 'ping', '(', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sub_addr = sys . argv [ 2 ] \n"
Original    (009): ['sub_addr', '=', 'sys', '.', 'argv', '[', '2', ']', '\\n']
Tokenized   (018): ['[CLS]', 'sub', '_', 'add', '##r', '=', 'sy', '##s', '.', 'ar', '##g', '##v', '[', '2', ']', '\\', 'n', '[SEP]']
Filtered   (016): ['sub', '_', 'add', '##r', '=', 'sy', '##s', '.', 'ar', '##g', '##v', '[', '2', ']', '\\', 'n']
Detokenized (009): ['sub_add##r', '=', 'sy##s', '.', 'ar##g##v', '[', '2', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "CLI ( ctrl_addr , sub_addr ) . cmdloop ( ) \n"
Original    (011): ['CLI', '(', 'ctrl_addr', ',', 'sub_addr', ')', '.', 'cmdloop', '(', ')', '\\n']
Tokenized   (024): ['[CLS]', 'cl', '##i', '(', 'ct', '##rl', '_', 'add', '##r', ',', 'sub', '_', 'add', '##r', ')', '.', 'cm', '##dl', '##oop', '(', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['cl', '##i', '(', 'ct', '##rl', '_', 'add', '##r', ',', 'sub', '_', 'add', '##r', ')', '.', 'cm', '##dl', '##oop', '(', ')', '\\', 'n']
Detokenized (011): ['cl##i', '(', 'ct##rl_add##r', ',', 'sub_add##r', ')', '.', 'cm##dl##oop', '(', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "duty = int ( cur_pwm [ "duty_ns" ] ) \n"
Original    (010): ['duty', '=', 'int', '(', 'cur_pwm', '[', '"duty_ns"', ']', ')', '\\n']
Tokenized   (022): ['[CLS]', 'duty', '=', 'int', '(', 'cu', '##r', '_', 'p', '##w', '##m', '[', '"', 'duty', '_', 'ns', '"', ']', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['duty', '=', 'int', '(', 'cu', '##r', '_', 'p', '##w', '##m', '[', '"', 'duty', '_', 'ns', '"', ']', ')', '\\', 'n']
Detokenized (010): ['duty', '=', 'int', '(', 'cu##r_p##w##m', '[', '"duty_ns"', ']', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "read_pos = int ( round ( ( ( duty - 580000 ) / 2320000. ) * 180 ) ) \n"
Original    (020): ['read_pos', '=', 'int', '(', 'round', '(', '(', '(', 'duty', '-', '580000', ')', '/', '2320000.', ')', '*', '180', ')', ')', '\\n']
Tokenized   (031): ['[CLS]', 'read', '_', 'po', '##s', '=', 'int', '(', 'round', '(', '(', '(', 'duty', '-', '580', '##00', '##0', ')', '/', '232', '##00', '##00', '.', ')', '*', '180', ')', ')', '\\', 'n', '[SEP]']
Filtered   (029): ['read', '_', 'po', '##s', '=', 'int', '(', 'round', '(', '(', '(', 'duty', '-', '580', '##00', '##0', ')', '/', '232', '##00', '##00', '.', ')', '*', '180', ')', ')', '\\', 'n']
Detokenized (020): ['read_po##s', '=', 'int', '(', 'round', '(', '(', '(', 'duty', '-', '580##00##0', ')', '/', '232##00##00.', ')', '*', '180', ')', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "create_login_url , create_logout_url \n"
Original    (004): ['create_login_url', ',', 'create_logout_url', '\\n']
Tokenized   (019): ['[CLS]', 'create', '_', 'log', '##in', '_', 'ur', '##l', ',', 'create', '_', 'logo', '##ut', '_', 'ur', '##l', '\\', 'n', '[SEP]']
Filtered   (017): ['create', '_', 'log', '##in', '_', 'ur', '##l', ',', 'create', '_', 'logo', '##ut', '_', 'ur', '##l', '\\', 'n']
Detokenized (004): ['create_log##in_ur##l', ',', 'create_logo##ut_ur##l', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "create_logout_url ( request . url ) \n"
Original    (007): ['create_logout_url', '(', 'request', '.', 'url', ')', '\\n']
Tokenized   (017): ['[CLS]', 'create', '_', 'logo', '##ut', '_', 'ur', '##l', '(', 'request', '.', 'ur', '##l', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['create', '_', 'logo', '##ut', '_', 'ur', '##l', '(', 'request', '.', 'ur', '##l', ')', '\\', 'n']
Detokenized (007): ['create_logo##ut_ur##l', '(', 'request', '.', 'ur##l', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "_value = _options_header_vkw ( _value , kw ) \n"
Original    (009): ['_value', '=', '_options_header_vkw', '(', '_value', ',', 'kw', ')', '\\n']
Tokenized   (021): ['[CLS]', '_', 'value', '=', '_', 'options', '_', 'header', '_', 'v', '##k', '##w', '(', '_', 'value', ',', 'kw', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['_', 'value', '=', '_', 'options', '_', 'header', '_', 'v', '##k', '##w', '(', '_', 'value', ',', 'kw', ')', '\\', 'n']
Detokenized (009): ['_value', '=', '_options_header_v##k##w', '(', '_value', ',', 'kw', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "value_type == value_subtype == ) or \n"
Original    (007): ['value_type', '==', 'value_subtype', '==', ')', 'or', '\\n']
Tokenized   (017): ['[CLS]', 'value', '_', 'type', '=', '=', 'value', '_', 'sub', '##type', '=', '=', ')', 'or', '\\', 'n', '[SEP]']
Filtered   (015): ['value', '_', 'type', '=', '=', 'value', '_', 'sub', '##type', '=', '=', ')', 'or', '\\', 'n']
Detokenized (007): ['value_type', '==', 'value_sub##type', '==', ')', 'or', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "value_subtype == or \n"
Original    (004): ['value_subtype', '==', 'or', '\\n']
Tokenized   (011): ['[CLS]', 'value', '_', 'sub', '##type', '=', '=', 'or', '\\', 'n', '[SEP]']
Filtered   (009): ['value', '_', 'sub', '##type', '=', '=', 'or', '\\', 'n']
Detokenized (004): ['value_sub##type', '==', 'or', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "item_subtype == value_subtype ) ) \n"
Original    (006): ['item_subtype', '==', 'value_subtype', ')', ')', '\\n']
Tokenized   (016): ['[CLS]', 'item', '_', 'sub', '##type', '=', '=', 'value', '_', 'sub', '##type', ')', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['item', '_', 'sub', '##type', '=', '=', 'value', '_', 'sub', '##type', ')', ')', '\\', 'n']
Detokenized (006): ['item_sub##type', '==', 'value_sub##type', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "no_cache = cache_property ( , , None ) \n"
Original    (009): ['no_cache', '=', 'cache_property', '(', ',', ',', 'None', ')', '\\n']
Tokenized   (016): ['[CLS]', 'no', '_', 'cache', '=', 'cache', '_', 'property', '(', ',', ',', 'none', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['no', '_', 'cache', '=', 'cache', '_', 'property', '(', ',', ',', 'none', ')', '\\', 'n']
Detokenized (009): ['no_cache', '=', 'cache_property', '(', ',', ',', 'none', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "no_store = cache_property ( , None , bool ) \n"
Original    (010): ['no_store', '=', 'cache_property', '(', ',', 'None', ',', 'bool', ')', '\\n']
Tokenized   (018): ['[CLS]', 'no', '_', 'store', '=', 'cache', '_', 'property', '(', ',', 'none', ',', 'boo', '##l', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['no', '_', 'store', '=', 'cache', '_', 'property', '(', ',', 'none', ',', 'boo', '##l', ')', '\\', 'n']
Detokenized (010): ['no_store', '=', 'cache_property', '(', ',', 'none', ',', 'boo##l', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "max_age = cache_property ( , - 1 , int ) \n"
Original    (011): ['max_age', '=', 'cache_property', '(', ',', '-', '1', ',', 'int', ')', '\\n']
Tokenized   (018): ['[CLS]', 'max', '_', 'age', '=', 'cache', '_', 'property', '(', ',', '-', '1', ',', 'int', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['max', '_', 'age', '=', 'cache', '_', 'property', '(', ',', '-', '1', ',', 'int', ')', '\\', 'n']
Detokenized (011): ['max_age', '=', 'cache_property', '(', ',', '-', '1', ',', 'int', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "no_transform = cache_property ( , None , None ) \n"
Original    (010): ['no_transform', '=', 'cache_property', '(', ',', 'None', ',', 'None', ')', '\\n']
Tokenized   (017): ['[CLS]', 'no', '_', 'transform', '=', 'cache', '_', 'property', '(', ',', 'none', ',', 'none', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['no', '_', 'transform', '=', 'cache', '_', 'property', '(', ',', 'none', ',', 'none', ')', '\\', 'n']
Detokenized (010): ['no_transform', '=', 'cache_property', '(', ',', 'none', ',', 'none', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "max_stale = cache_property ( , , int ) \n"
Original    (009): ['max_stale', '=', 'cache_property', '(', ',', ',', 'int', ')', '\\n']
Tokenized   (016): ['[CLS]', 'max', '_', 'stale', '=', 'cache', '_', 'property', '(', ',', ',', 'int', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['max', '_', 'stale', '=', 'cache', '_', 'property', '(', ',', ',', 'int', ')', '\\', 'n']
Detokenized (009): ['max_stale', '=', 'cache_property', '(', ',', ',', 'int', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "etag , weak = unquote_etag ( etag ) \n"
Original    (009): ['etag', ',', 'weak', '=', 'unquote_etag', '(', 'etag', ')', '\\n']
Tokenized   (019): ['[CLS]', 'eta', '##g', ',', 'weak', '=', 'un', '##qu', '##ote', '_', 'eta', '##g', '(', 'eta', '##g', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['eta', '##g', ',', 'weak', '=', 'un', '##qu', '##ote', '_', 'eta', '##g', '(', 'eta', '##g', ')', '\\', 'n']
Detokenized (009): ['eta##g', ',', 'weak', '=', 'un##qu##ote_eta##g', '(', 'eta##g', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "uri = property ( lambda x : x . get ( ) , doc = ) \n"
Original    (017): ['uri', '=', 'property', '(', 'lambda', 'x', ':', 'x', '.', 'get', '(', ')', ',', 'doc', '=', ')', '\\n']
Tokenized   (021): ['[CLS]', 'ur', '##i', '=', 'property', '(', 'lambda', 'x', ':', 'x', '.', 'get', '(', ')', ',', 'doc', '=', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['ur', '##i', '=', 'property', '(', 'lambda', 'x', ':', 'x', '.', 'get', '(', ')', ',', 'doc', '=', ')', '\\', 'n']
Detokenized (017): ['ur##i', '=', 'property', '(', 'lambda', 'x', ':', 'x', '.', 'get', '(', ')', ',', 'doc', '=', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "_require_quoting = frozenset ( [ , , , ] ) \n"
Original    (011): ['_require_quoting', '=', 'frozenset', '(', '[', ',', ',', ',', ']', ')', '\\n']
Tokenized   (018): ['[CLS]', '_', 'require', '_', 'quoting', '=', 'frozen', '##set', '(', '[', ',', ',', ',', ']', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['_', 'require', '_', 'quoting', '=', 'frozen', '##set', '(', '[', ',', ',', ',', ']', ')', '\\', 'n']
Detokenized (011): ['_require_quoting', '=', 'frozen##set', '(', '[', ',', ',', ',', ']', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "auth_type = d . pop ( , None ) or \n"
Original    (011): ['auth_type', '=', 'd', '.', 'pop', '(', ',', 'None', ')', 'or', '\\n']
Tokenized   (017): ['[CLS]', 'au', '##th', '_', 'type', '=', 'd', '.', 'pop', '(', ',', 'none', ')', 'or', '\\', 'n', '[SEP]']
Filtered   (015): ['au', '##th', '_', 'type', '=', 'd', '.', 'pop', '(', ',', 'none', ')', 'or', '\\', 'n']
Detokenized (011): ['au##th_type', '=', 'd', '.', 'pop', '(', ',', 'none', ')', 'or', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "allow_token = key not in self . _require_quoting ) ) \n"
Original    (011): ['allow_token', '=', 'key', 'not', 'in', 'self', '.', '_require_quoting', ')', ')', '\\n']
Tokenized   (019): ['[CLS]', 'allow', '_', 'token', '=', 'key', 'not', 'in', 'self', '.', '_', 'require', '_', 'quoting', ')', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['allow', '_', 'token', '=', 'key', 'not', 'in', 'self', '.', '_', 'require', '_', 'quoting', ')', ')', '\\', 'n']
Detokenized (011): ['allow_token', '=', 'key', 'not', 'in', 'self', '.', '_require_quoting', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "realm = auth_property ( , doc = ) \n"
Original    (009): ['realm', '=', 'auth_property', '(', ',', 'doc', '=', ')', '\\n']
Tokenized   (015): ['[CLS]', 'realm', '=', 'au', '##th', '_', 'property', '(', ',', 'doc', '=', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['realm', '=', 'au', '##th', '_', 'property', '(', ',', 'doc', '=', ')', '\\', 'n']
Detokenized (009): ['realm', '=', 'au##th_property', '(', ',', 'doc', '=', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "rshell , shell , clear_datastore , create_user , \n"
Original    (009): ['rshell', ',', 'shell', ',', 'clear_datastore', ',', 'create_user', ',', '\\n']
Tokenized   (019): ['[CLS]', 'rs', '##hell', ',', 'shell', ',', 'clear', '_', 'data', '##stor', '##e', ',', 'create', '_', 'user', ',', '\\', 'n', '[SEP]']
Filtered   (017): ['rs', '##hell', ',', 'shell', ',', 'clear', '_', 'data', '##stor', '##e', ',', 'create', '_', 'user', ',', '\\', 'n']
Detokenized (009): ['rs##hell', ',', 'shell', ',', 'clear_data##stor##e', ',', 'create_user', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "Rule ( , endpoint = , \n"
Original    (007): ['Rule', '(', ',', 'endpoint', '=', ',', '\\n']
Tokenized   (011): ['[CLS]', 'rule', '(', ',', 'end', '##point', '=', ',', '\\', 'n', '[SEP]']
Filtered   (009): ['rule', '(', ',', 'end', '##point', '=', ',', '\\', 'n']
Detokenized (007): ['rule', '(', ',', 'end##point', '=', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "data_field = db . StringProperty ( required = True , \n"
Original    (011): ['data_field', '=', 'db', '.', 'StringProperty', '(', 'required', '=', 'True', ',', '\\n']
Tokenized   (019): ['[CLS]', 'data', '_', 'field', '=', 'db', '.', 'string', '##pro', '##per', '##ty', '(', 'required', '=', 'true', ',', '\\', 'n', '[SEP]']
Filtered   (017): ['data', '_', 'field', '=', 'db', '.', 'string', '##pro', '##per', '##ty', '(', 'required', '=', 'true', ',', '\\', 'n']
Detokenized (011): ['data_field', '=', 'db', '.', 'string##pro##per##ty', '(', 'required', '=', 'true', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "dst_name = path . join ( dst_path , filename ) \n"
Original    (011): ['dst_name', '=', 'path', '.', 'join', '(', 'dst_path', ',', 'filename', ')', '\\n']
Tokenized   (021): ['[CLS]', 'ds', '##t', '_', 'name', '=', 'path', '.', 'join', '(', 'ds', '##t', '_', 'path', ',', 'file', '##name', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['ds', '##t', '_', 'name', '=', 'path', '.', 'join', '(', 'ds', '##t', '_', 'path', ',', 'file', '##name', ')', '\\', 'n']
Detokenized (011): ['ds##t_name', '=', 'path', '.', 'join', '(', 'ds##t_path', ',', 'file##name', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "modifiable_problem_fields = [ "description" ] \n"
Original    (006): ['modifiable_problem_fields', '=', '[', '"description"', ']', '\\n']
Tokenized   (017): ['[CLS]', 'mod', '##if', '##iable', '_', 'problem', '_', 'fields', '=', '[', '"', 'description', '"', ']', '\\', 'n', '[SEP]']
Filtered   (015): ['mod', '##if', '##iable', '_', 'problem', '_', 'fields', '=', '[', '"', 'description', '"', ']', '\\', 'n']
Detokenized (006): ['mod##if##iable_problem_fields', '=', '[', '"description"', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "problem = api . problem . get_problem ( pid = pid ) \n"
Original    (013): ['problem', '=', 'api', '.', 'problem', '.', 'get_problem', '(', 'pid', '=', 'pid', ')', '\\n']
Tokenized   (020): ['[CLS]', 'problem', '=', 'api', '.', 'problem', '.', 'get', '_', 'problem', '(', 'pi', '##d', '=', 'pi', '##d', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['problem', '=', 'api', '.', 'problem', '.', 'get', '_', 'problem', '(', 'pi', '##d', '=', 'pi', '##d', ')', '\\', 'n']
Detokenized (013): ['problem', '=', 'api', '.', 'problem', '.', 'get_problem', '(', 'pi##d', '=', 'pi##d', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "build = get_generator ( pid ) . generate ( random , pid , api . autogen_tools , n ) \n"
Original    (020): ['build', '=', 'get_generator', '(', 'pid', ')', '.', 'generate', '(', 'random', ',', 'pid', ',', 'api', '.', 'autogen_tools', ',', 'n', ')', '\\n']
Tokenized   (030): ['[CLS]', 'build', '=', 'get', '_', 'generator', '(', 'pi', '##d', ')', '.', 'generate', '(', 'random', ',', 'pi', '##d', ',', 'api', '.', 'auto', '##gen', '_', 'tools', ',', 'n', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['build', '=', 'get', '_', 'generator', '(', 'pi', '##d', ')', '.', 'generate', '(', 'random', ',', 'pi', '##d', ',', 'api', '.', 'auto', '##gen', '_', 'tools', ',', 'n', ')', '\\', 'n']
Detokenized (020): ['build', '=', 'get_generator', '(', 'pi##d', ')', '.', 'generate', '(', 'random', ',', 'pi##d', ',', 'api', '.', 'auto##gen_tools', ',', 'n', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "autogen_instance_path = get_instance_path ( pid , n = n ) \n"
Original    (011): ['autogen_instance_path', '=', 'get_instance_path', '(', 'pid', ',', 'n', '=', 'n', ')', '\\n']
Tokenized   (024): ['[CLS]', 'auto', '##gen', '_', 'instance', '_', 'path', '=', 'get', '_', 'instance', '_', 'path', '(', 'pi', '##d', ',', 'n', '=', 'n', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['auto', '##gen', '_', 'instance', '_', 'path', '=', 'get', '_', 'instance', '_', 'path', '(', 'pi', '##d', ',', 'n', '=', 'n', ')', '\\', 'n']
Detokenized (011): ['auto##gen_instance_path', '=', 'get_instance_path', '(', 'pi##d', ',', 'n', '=', 'n', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""resource_files" : { \n"
Original    (004): ['"resource_files"', ':', '{', '\\n']
Tokenized   (011): ['[CLS]', '"', 'resource', '_', 'files', '"', ':', '{', '\\', 'n', '[SEP]']
Filtered   (009): ['"', 'resource', '_', 'files', '"', ':', '{', '\\', 'n']
Detokenized (004): ['"resource_files"', ':', '{', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "instance_path = path . join ( path . dirname ( generator_path ) , "instances" , name , str ( n ) ) \n"
Original    (023): ['instance_path', '=', 'path', '.', 'join', '(', 'path', '.', 'dirname', '(', 'generator_path', ')', ',', '"instances"', ',', 'name', ',', 'str', '(', 'n', ')', ')', '\\n']
Tokenized   (034): ['[CLS]', 'instance', '_', 'path', '=', 'path', '.', 'join', '(', 'path', '.', 'dir', '##name', '(', 'generator', '_', 'path', ')', ',', '"', 'instances', '"', ',', 'name', ',', 'st', '##r', '(', 'n', ')', ')', '\\', 'n', '[SEP]']
Filtered   (032): ['instance', '_', 'path', '=', 'path', '.', 'join', '(', 'path', '.', 'dir', '##name', '(', 'generator', '_', 'path', ')', ',', '"', 'instances', '"', ',', 'name', ',', 'st', '##r', '(', 'n', ')', ')', '\\', 'n']
Detokenized (023): ['instance_path', '=', 'path', '.', 'join', '(', 'path', '.', 'dir##name', '(', 'generator_path', ')', ',', '"instances"', ',', 'name', ',', 'st##r', '(', 'n', ')', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : ""correct" : correct , \n"
Original    (005): ['"correct"', ':', 'correct', ',', '\\n']
Tokenized   (010): ['[CLS]', '"', 'correct', '"', ':', 'correct', ',', '\\', 'n', '[SEP]']
Filtered   (008): ['"', 'correct', '"', ':', 'correct', ',', '\\', 'n']
Detokenized (005): ['"correct"', ':', 'correct', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""points" : problem [ "score" ] , \n"
Original    (008): ['"points"', ':', 'problem', '[', '"score"', ']', ',', '\\n']
Tokenized   (015): ['[CLS]', '"', 'points', '"', ':', 'problem', '[', '"', 'score', '"', ']', ',', '\\', 'n', '[SEP]']
Filtered   (013): ['"', 'points', '"', ':', 'problem', '[', '"', 'score', '"', ']', ',', '\\', 'n']
Detokenized (008): ['"points"', ':', 'problem', '[', '"score"', ']', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : ""message" : message \n"
Original    (004): ['"message"', ':', 'message', '\\n']
Tokenized   (009): ['[CLS]', '"', 'message', '"', ':', 'message', '\\', 'n', '[SEP]']
Filtered   (007): ['"', 'message', '"', ':', 'message', '\\', 'n']
Detokenized (004): ['"message"', ':', 'message', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "k = str ( random . randint ( 0 , 1000 ) ) \n"
Original    (014): ['k', '=', 'str', '(', 'random', '.', 'randint', '(', '0', ',', '1000', ')', ')', '\\n']
Tokenized   (019): ['[CLS]', 'k', '=', 'st', '##r', '(', 'random', '.', 'rand', '##int', '(', '0', ',', '1000', ')', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['k', '=', 'st', '##r', '(', 'random', '.', 'rand', '##int', '(', '0', ',', '1000', ')', ')', '\\', 'n']
Detokenized (014): ['k', '=', 'st##r', '(', 'random', '.', 'rand##int', '(', '0', ',', '1000', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : ""public" : [ ( "/tmp/key" , "public_static" ) ] , \n"
Original    (011): ['"public"', ':', '[', '(', '"/tmp/key"', ',', '"public_static"', ')', ']', ',', '\\n']
Tokenized   (026): ['[CLS]', '"', 'public', '"', ':', '[', '(', '"', '/', 't', '##mp', '/', 'key', '"', ',', '"', 'public', '_', 'static', '"', ')', ']', ',', '\\', 'n', '[SEP]']
Filtered   (024): ['"', 'public', '"', ':', '[', '(', '"', '/', 't', '##mp', '/', 'key', '"', ',', '"', 'public', '_', 'static', '"', ')', ']', ',', '\\', 'n']
Detokenized (011): ['"public"', ':', '[', '(', '"/t##mp/key"', ',', '"public_static"', ')', ']', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""private" : [ ( "/tmp/key" , "private_static" ) ] \n"
Original    (010): ['"private"', ':', '[', '(', '"/tmp/key"', ',', '"private_static"', ')', ']', '\\n']
Tokenized   (025): ['[CLS]', '"', 'private', '"', ':', '[', '(', '"', '/', 't', '##mp', '/', 'key', '"', ',', '"', 'private', '_', 'static', '"', ')', ']', '\\', 'n', '[SEP]']
Filtered   (023): ['"', 'private', '"', ':', '[', '(', '"', '/', 't', '##mp', '/', 'key', '"', ',', '"', 'private', '_', 'static', '"', ')', ']', '\\', 'n']
Detokenized (010): ['"private"', ':', '[', '(', '"/t##mp/key"', ',', '"private_static"', ')', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "layout = eval ( scriptWindow . setLayout ( layout ) \n"
Original    (011): ['layout', '=', 'eval', '(', 'scriptWindow', '.', 'setLayout', '(', 'layout', ')', '\\n']
Tokenized   (019): ['[CLS]', 'layout', '=', 'eva', '##l', '(', 'script', '##wind', '##ow', '.', 'set', '##lay', '##out', '(', 'layout', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['layout', '=', 'eva', '##l', '(', 'script', '##wind', '##ow', '.', 'set', '##lay', '##out', '(', 'layout', ')', '\\', 'n']
Detokenized (011): ['layout', '=', 'eva##l', '(', 'script##wind##ow', '.', 'set##lay##out', '(', 'layout', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "scriptWindow . _Widget__qtWidget . resize ( 995 , 500 ) \n"
Original    (011): ['scriptWindow', '.', '_Widget__qtWidget', '.', 'resize', '(', '995', ',', '500', ')', '\\n']
Tokenized   (026): ['[CLS]', 'script', '##wind', '##ow', '.', '_', 'wi', '##dget', '_', '_', 'q', '##t', '##wi', '##dget', '.', 'res', '##ize', '(', '99', '##5', ',', '500', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['script', '##wind', '##ow', '.', '_', 'wi', '##dget', '_', '_', 'q', '##t', '##wi', '##dget', '.', 'res', '##ize', '(', '99', '##5', ',', '500', ')', '\\', 'n']
Detokenized (011): ['script##wind##ow', '.', '_wi##dget__q##t##wi##dget', '.', 'res##ize', '(', '99##5', ',', '500', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""inputSequence" , \n"
Original    (003): ['"inputSequence"', ',', '\\n']
Tokenized   (011): ['[CLS]', '"', 'inputs', '##e', '##que', '##nce', '"', ',', '\\', 'n', '[SEP]']
Filtered   (009): ['"', 'inputs', '##e', '##que', '##nce', '"', ',', '\\', 'n']
Detokenized (003): ['"inputs##e##que##nce"', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "defaultValue = "" , \n"
Original    (005): ['defaultValue', '=', '""', ',', '\\n']
Tokenized   (011): ['[CLS]', 'default', '##val', '##ue', '=', '"', '"', ',', '\\', 'n', '[SEP]']
Filtered   (009): ['default', '##val', '##ue', '=', '"', '"', ',', '\\', 'n']
Detokenized (005): ['default##val##ue', '=', '""', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "dc . write ( "-" + objectName , "bound" , b ) \n"
Original    (013): ['dc', '.', 'write', '(', '"-"', '+', 'objectName', ',', '"bound"', ',', 'b', ')', '\\n']
Tokenized   (021): ['[CLS]', 'dc', '.', 'write', '(', '"', '-', '"', '+', 'object', '##name', ',', '"', 'bound', '"', ',', 'b', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['dc', '.', 'write', '(', '"', '-', '"', '+', 'object', '##name', ',', '"', 'bound', '"', ',', 'b', ')', '\\', 'n']
Detokenized (013): ['dc', '.', 'write', '(', '"-"', '+', 'object##name', ',', '"bound"', ',', 'b', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "additionalTerminalPlugTypes = ( GafferScene . ScenePlug , ) \n"
Original    (009): ['additionalTerminalPlugTypes', '=', '(', 'GafferScene', '.', 'ScenePlug', ',', ')', '\\n']
Tokenized   (024): ['[CLS]', 'additional', '##ter', '##mina', '##lp', '##lu', '##gt', '##ype', '##s', '=', '(', 'ga', '##ffer', '##sc', '##ene', '.', 'scene', '##pl', '##ug', ',', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['additional', '##ter', '##mina', '##lp', '##lu', '##gt', '##ype', '##s', '=', '(', 'ga', '##ffer', '##sc', '##ene', '.', 'scene', '##pl', '##ug', ',', ')', '\\', 'n']
Detokenized (009): ['additional##ter##mina##lp##lu##gt##ype##s', '=', '(', 'ga##ffer##sc##ene', '.', 'scene##pl##ug', ',', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "replace = context . get ( "textWriter:replace" , IECore . StringVectorData ( ) ) \n"
Original    (015): ['replace', '=', 'context', '.', 'get', '(', '"textWriter:replace"', ',', 'IECore', '.', 'StringVectorData', '(', ')', ')', '\\n']
Tokenized   (028): ['[CLS]', 'replace', '=', 'context', '.', 'get', '(', '"', 'text', '##writer', ':', 'replace', '"', ',', 'iec', '##ore', '.', 'string', '##ve', '##ctor', '##da', '##ta', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['replace', '=', 'context', '.', 'get', '(', '"', 'text', '##writer', ':', 'replace', '"', ',', 'iec', '##ore', '.', 'string', '##ve', '##ctor', '##da', '##ta', '(', ')', ')', '\\', 'n']
Detokenized (015): ['replace', '=', 'context', '.', 'get', '(', '"text##writer:replace"', ',', 'iec##ore', '.', 'string##ve##ctor##da##ta', '(', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "inMetadata = r [ "out" ] [ "metadata" ] . getValue ( ) \n"
Original    (014): ['inMetadata', '=', 'r', '[', '"out"', ']', '[', '"metadata"', ']', '.', 'getValue', '(', ')', '\\n']
Tokenized   (026): ['[CLS]', 'in', '##met', '##ada', '##ta', '=', 'r', '[', '"', 'out', '"', ']', '[', '"', 'metadata', '"', ']', '.', 'get', '##val', '##ue', '(', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['in', '##met', '##ada', '##ta', '=', 'r', '[', '"', 'out', '"', ']', '[', '"', 'metadata', '"', ']', '.', 'get', '##val', '##ue', '(', ')', '\\', 'n']
Detokenized (014): ['in##met##ada##ta', '=', 'r', '[', '"out"', ']', '[', '"metadata"', ']', '.', 'get##val##ue', '(', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "negFileName = os . path . expandvars ( "$GAFFER_ROOT/python/GafferImageTest/images/checkerWithNegativeDataWindow.200x150.exr" \n"
Original    (010): ['negFileName', '=', 'os', '.', 'path', '.', 'expandvars', '(', '"$GAFFER_ROOT/python/GafferImageTest/images/checkerWithNegativeDataWindow.200x150.exr"', '\\n']
Tokenized   (052): ['[CLS]', 'ne', '##gf', '##ile', '##name', '=', 'os', '.', 'path', '.', 'expand', '##var', '##s', '(', '"', '$', 'ga', '##ffer', '_', 'root', '/', 'python', '/', 'ga', '##ffer', '##ima', '##get', '##est', '/', 'images', '/', 'check', '##er', '##with', '##ne', '##gative', '##da', '##ta', '##wind', '##ow', '.', '200', '##x', '##15', '##0', '.', 'ex', '##r', '"', '\\', 'n', '[SEP]']
Filtered   (050): ['ne', '##gf', '##ile', '##name', '=', 'os', '.', 'path', '.', 'expand', '##var', '##s', '(', '"', '$', 'ga', '##ffer', '_', 'root', '/', 'python', '/', 'ga', '##ffer', '##ima', '##get', '##est', '/', 'images', '/', 'check', '##er', '##with', '##ne', '##gative', '##da', '##ta', '##wind', '##ow', '.', '200', '##x', '##15', '##0', '.', 'ex', '##r', '"', '\\', 'n']
Detokenized (010): ['ne##gf##ile##name', '=', 'os', '.', 'path', '.', 'expand##var##s', '(', '"$ga##ffer_root/python/ga##ffer##ima##get##est/images/check##er##with##ne##gative##da##ta##wind##ow.200##x##15##0.ex##r"', '\\n']
Counter: 50
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "GafferImage . Display , \n"
Original    (005): ['GafferImage', '.', 'Display', ',', '\\n']
Tokenized   (011): ['[CLS]', 'ga', '##ffer', '##ima', '##ge', '.', 'display', ',', '\\', 'n', '[SEP]']
Filtered   (009): ['ga', '##ffer', '##ima', '##ge', '.', 'display', ',', '\\', 'n']
Detokenized (005): ['ga##ffer##ima##ge', '.', 'display', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""port" : [ \n"
Original    (004): ['"port"', ':', '[', '\\n']
Tokenized   (009): ['[CLS]', '"', 'port', '"', ':', '[', '\\', 'n', '[SEP]']
Filtered   (007): ['"', 'port', '"', ':', '[', '\\', 'n']
Detokenized (004): ['"port"', ':', '[', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "updateCountPlug . setValue ( updateCountPlug . getValue ( ) + 1 ) \n"
Original    (013): ['updateCountPlug', '.', 'setValue', '(', 'updateCountPlug', '.', 'getValue', '(', ')', '+', '1', ')', '\\n']
Tokenized   (028): ['[CLS]', 'update', '##co', '##unt', '##pl', '##ug', '.', 'set', '##val', '##ue', '(', 'update', '##co', '##unt', '##pl', '##ug', '.', 'get', '##val', '##ue', '(', ')', '+', '1', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['update', '##co', '##unt', '##pl', '##ug', '.', 'set', '##val', '##ue', '(', 'update', '##co', '##unt', '##pl', '##ug', '.', 'get', '##val', '##ue', '(', ')', '+', '1', ')', '\\', 'n']
Detokenized (013): ['update##co##unt##pl##ug', '.', 'set##val##ue', '(', 'update##co##unt##pl##ug', '.', 'get##val##ue', '(', ')', '+', '1', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "__import__ ( "IECore" ) . loadConfig ( "GAFFER_STARTUP_PATHS" , { } , subdirectory = "GafferImageUI" ) \n"
Original    (017): ['__import__', '(', '"IECore"', ')', '.', 'loadConfig', '(', '"GAFFER_STARTUP_PATHS"', ',', '{', '}', ',', 'subdirectory', '=', '"GafferImageUI"', ')', '\\n']
Tokenized   (047): ['[CLS]', '_', '_', 'import', '_', '_', '(', '"', 'iec', '##ore', '"', ')', '.', 'load', '##con', '##fi', '##g', '(', '"', 'ga', '##ffer', '_', 'startup', '_', 'paths', '"', ',', '{', '}', ',', 'sub', '##di', '##re', '##ctor', '##y', '=', '"', 'ga', '##ffer', '##ima', '##ge', '##ui', '"', ')', '\\', 'n', '[SEP]']
Filtered   (045): ['_', '_', 'import', '_', '_', '(', '"', 'iec', '##ore', '"', ')', '.', 'load', '##con', '##fi', '##g', '(', '"', 'ga', '##ffer', '_', 'startup', '_', 'paths', '"', ',', '{', '}', ',', 'sub', '##di', '##re', '##ctor', '##y', '=', '"', 'ga', '##ffer', '##ima', '##ge', '##ui', '"', ')', '\\', 'n']
Detokenized (017): ['__import__', '(', '"iec##ore"', ')', '.', 'load##con##fi##g', '(', '"ga##ffer_startup_paths"', ',', '{', '}', ',', 'sub##di##re##ctor##y', '=', '"ga##ffer##ima##ge##ui"', ')', '\\n']
Counter: 45
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "GafferRenderMan . RenderManShader . shaderLoader ( ) . clear ( ) \n"
Original    (012): ['GafferRenderMan', '.', 'RenderManShader', '.', 'shaderLoader', '(', ')', '.', 'clear', '(', ')', '\\n']
Tokenized   (025): ['[CLS]', 'ga', '##ffer', '##ren', '##der', '##man', '.', 'render', '##mans', '##had', '##er', '.', 'shade', '##rl', '##oa', '##der', '(', ')', '.', 'clear', '(', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['ga', '##ffer', '##ren', '##der', '##man', '.', 'render', '##mans', '##had', '##er', '.', 'shade', '##rl', '##oa', '##der', '(', ')', '.', 'clear', '(', ')', '\\', 'n']
Detokenized (012): ['ga##ffer##ren##der##man', '.', 'render##mans##had##er', '.', 'shade##rl##oa##der', '(', ')', '.', 'clear', '(', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n"
Original    (018): ['coshader', '=', 'self', '.', 'compileShader', '(', 'os', '.', 'path', '.', 'dirname', '(', '__file__', ')', '+', '"/shaders/coshader.sl"', ')', '\\n']
Tokenized   (041): ['[CLS]', 'co', '##sha', '##der', '=', 'self', '.', 'com', '##pile', '##sha', '##der', '(', 'os', '.', 'path', '.', 'dir', '##name', '(', '_', '_', 'file', '_', '_', ')', '+', '"', '/', 'shade', '##rs', '/', 'co', '##sha', '##der', '.', 'sl', '"', ')', '\\', 'n', '[SEP]']
Filtered   (039): ['co', '##sha', '##der', '=', 'self', '.', 'com', '##pile', '##sha', '##der', '(', 'os', '.', 'path', '.', 'dir', '##name', '(', '_', '_', 'file', '_', '_', ')', '+', '"', '/', 'shade', '##rs', '/', 'co', '##sha', '##der', '.', 'sl', '"', ')', '\\', 'n']
Detokenized (018): ['co##sha##der', '=', 'self', '.', 'com##pile##sha##der', '(', 'os', '.', 'path', '.', 'dir##name', '(', '__file__', ')', '+', '"/shade##rs/co##sha##der.sl"', ')', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "nn [ "outString" ] = Gaffer . StringPlug ( direction = Gaffer . Plug . Direction . Out ) \n"
Original    (020): ['nn', '[', '"outString"', ']', '=', 'Gaffer', '.', 'StringPlug', '(', 'direction', '=', 'Gaffer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\n']
Tokenized   (032): ['[CLS]', 'n', '##n', '[', '"', 'outs', '##tri', '##ng', '"', ']', '=', 'ga', '##ffer', '.', 'string', '##pl', '##ug', '(', 'direction', '=', 'ga', '##ffer', '.', 'plug', '.', 'direction', '.', 'out', ')', '\\', 'n', '[SEP]']
Filtered   (030): ['n', '##n', '[', '"', 'outs', '##tri', '##ng', '"', ']', '=', 'ga', '##ffer', '.', 'string', '##pl', '##ug', '(', 'direction', '=', 'ga', '##ffer', '.', 'plug', '.', 'direction', '.', 'out', ')', '\\', 'n']
Detokenized (020): ['n##n', '[', '"outs##tri##ng"', ']', '=', 'ga##ffer', '.', 'string##pl##ug', '(', 'direction', '=', 'ga##ffer', '.', 'plug', '.', 'direction', '.', 'out', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "shader2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version2.sl" , shaderName = "unversioned" \n"
Original    (021): ['shader2', '=', 'self', '.', 'compileShader', '(', 'os', '.', 'path', '.', 'dirname', '(', '__file__', ')', '+', '"/shaders/version2.sl"', ',', 'shaderName', '=', '"unversioned"', '\\n']
Tokenized   (049): ['[CLS]', 'shade', '##r', '##2', '=', 'self', '.', 'com', '##pile', '##sha', '##der', '(', 'os', '.', 'path', '.', 'dir', '##name', '(', '_', '_', 'file', '_', '_', ')', '+', '"', '/', 'shade', '##rs', '/', 'version', '##2', '.', 'sl', '"', ',', 'shade', '##rna', '##me', '=', '"', 'un', '##version', '##ed', '"', '\\', 'n', '[SEP]']
Filtered   (047): ['shade', '##r', '##2', '=', 'self', '.', 'com', '##pile', '##sha', '##der', '(', 'os', '.', 'path', '.', 'dir', '##name', '(', '_', '_', 'file', '_', '_', ')', '+', '"', '/', 'shade', '##rs', '/', 'version', '##2', '.', 'sl', '"', ',', 'shade', '##rna', '##me', '=', '"', 'un', '##version', '##ed', '"', '\\', 'n']
Detokenized (021): ['shade##r##2', '=', 'self', '.', 'com##pile##sha##der', '(', 'os', '.', 'path', '.', 'dir##name', '(', '__file__', ')', '+', '"/shade##rs/version##2.sl"', ',', 'shade##rna##me', '=', '"un##version##ed"', '\\n']
Counter: 47
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "assignment [ "shader" ] . setInput ( shaderNode [ "out" ] ) \n"
Original    (013): ['assignment', '[', '"shader"', ']', '.', 'setInput', '(', 'shaderNode', '[', '"out"', ']', ')', '\\n']
Tokenized   (025): ['[CLS]', 'assignment', '[', '"', 'shade', '##r', '"', ']', '.', 'set', '##in', '##put', '(', 'shade', '##rno', '##de', '[', '"', 'out', '"', ']', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['assignment', '[', '"', 'shade', '##r', '"', ']', '.', 'set', '##in', '##put', '(', 'shade', '##rno', '##de', '[', '"', 'out', '"', ']', ')', '\\', 'n']
Detokenized (013): ['assignment', '[', '"shade##r"', ']', '.', 'set##in##put', '(', 'shade##rno##de', '[', '"out"', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "dirtiedNames = [ x [ 0 ] . fullName ( ) for x in cs ] \n"
Original    (017): ['dirtiedNames', '=', '[', 'x', '[', '0', ']', '.', 'fullName', '(', ')', 'for', 'x', 'in', 'cs', ']', '\\n']
Tokenized   (024): ['[CLS]', 'dirt', '##ied', '##name', '##s', '=', '[', 'x', '[', '0', ']', '.', 'full', '##name', '(', ')', 'for', 'x', 'in', 'cs', ']', '\\', 'n', '[SEP]']
Filtered   (022): ['dirt', '##ied', '##name', '##s', '=', '[', 'x', '[', '0', ']', '.', 'full', '##name', '(', ')', 'for', 'x', 'in', 'cs', ']', '\\', 'n']
Detokenized (017): ['dirt##ied##name##s', '=', '[', 'x', '[', '0', ']', '.', 'full##name', '(', ')', 'for', 'x', 'in', 'cs', ']', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : ""dynamicFloatArray" : IECore . FloatVectorData ( [ ] ) , \n"
Original    (011): ['"dynamicFloatArray"', ':', 'IECore', '.', 'FloatVectorData', '(', '[', ']', ')', ',', '\\n']
Tokenized   (025): ['[CLS]', '"', 'dynamic', '##fl', '##oat', '##ar', '##ray', '"', ':', 'iec', '##ore', '.', 'float', '##ve', '##ctor', '##da', '##ta', '(', '[', ']', ')', ',', '\\', 'n', '[SEP]']
Filtered   (023): ['"', 'dynamic', '##fl', '##oat', '##ar', '##ray', '"', ':', 'iec', '##ore', '.', 'float', '##ve', '##ctor', '##da', '##ta', '(', '[', ']', ')', ',', '\\', 'n']
Detokenized (011): ['"dynamic##fl##oat##ar##ray"', ':', 'iec##ore', '.', 'float##ve##ctor##da##ta', '(', '[', ']', ')', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""fixedFloatArray" : IECore . FloatVectorData ( [ 1 , 2 , 3 , 4 ] ) , \n"
Original    (018): ['"fixedFloatArray"', ':', 'IECore', '.', 'FloatVectorData', '(', '[', '1', ',', '2', ',', '3', ',', '4', ']', ')', ',', '\\n']
Tokenized   (032): ['[CLS]', '"', 'fixed', '##fl', '##oat', '##ar', '##ray', '"', ':', 'iec', '##ore', '.', 'float', '##ve', '##ctor', '##da', '##ta', '(', '[', '1', ',', '2', ',', '3', ',', '4', ']', ')', ',', '\\', 'n', '[SEP]']
Filtered   (030): ['"', 'fixed', '##fl', '##oat', '##ar', '##ray', '"', ':', 'iec', '##ore', '.', 'float', '##ve', '##ctor', '##da', '##ta', '(', '[', '1', ',', '2', ',', '3', ',', '4', ']', ')', ',', '\\', 'n']
Detokenized (018): ['"fixed##fl##oat##ar##ray"', ':', 'iec##ore', '.', 'float##ve##ctor##da##ta', '(', '[', '1', ',', '2', ',', '3', ',', '4', ']', ')', ',', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : ""dynamicStringArray" : IECore . StringVectorData ( [ "dynamic" , "arrays" , "can" , "still" , "have" , "defaults" "fixedStringArray" : IECore . StringVectorData ( [ "hello" , "goodbye" ] ) , \n"
Original    (032): ['"dynamicStringArray"', ':', 'IECore', '.', 'StringVectorData', '(', '[', '"dynamic"', ',', '"arrays"', ',', '"can"', ',', '"still"', ',', '"have"', ',', '"defaults"', '"fixedStringArray"', ':', 'IECore', '.', 'StringVectorData', '(', '[', '"hello"', ',', '"goodbye"', ']', ')', ',', '\\n']
Tokenized   (074): ['[CLS]', '"', 'dynamics', '##tri', '##nga', '##rra', '##y', '"', ':', 'iec', '##ore', '.', 'string', '##ve', '##ctor', '##da', '##ta', '(', '[', '"', 'dynamic', '"', ',', '"', 'arrays', '"', ',', '"', 'can', '"', ',', '"', 'still', '"', ',', '"', 'have', '"', ',', '"', 'default', '##s', '"', '"', 'fixed', '##st', '##ring', '##ar', '##ray', '"', ':', 'iec', '##ore', '.', 'string', '##ve', '##ctor', '##da', '##ta', '(', '[', '"', 'hello', '"', ',', '"', 'goodbye', '"', ']', ')', ',', '\\', 'n', '[SEP]']
Filtered   (072): ['"', 'dynamics', '##tri', '##nga', '##rra', '##y', '"', ':', 'iec', '##ore', '.', 'string', '##ve', '##ctor', '##da', '##ta', '(', '[', '"', 'dynamic', '"', ',', '"', 'arrays', '"', ',', '"', 'can', '"', ',', '"', 'still', '"', ',', '"', 'have', '"', ',', '"', 'default', '##s', '"', '"', 'fixed', '##st', '##ring', '##ar', '##ray', '"', ':', 'iec', '##ore', '.', 'string', '##ve', '##ctor', '##da', '##ta', '(', '[', '"', 'hello', '"', ',', '"', 'goodbye', '"', ']', ')', ',', '\\', 'n']
Detokenized (032): ['"dynamics##tri##nga##rra##y"', ':', 'iec##ore', '.', 'string##ve##ctor##da##ta', '(', '[', '"dynamic"', ',', '"arrays"', ',', '"can"', ',', '"still"', ',', '"have"', ',', '"default##s"', '"fixed##st##ring##ar##ray"', ':', 'iec##ore', '.', 'string##ve##ctor##da##ta', '(', '[', '"hello"', ',', '"goodbye"', ']', ')', ',', '\\n']
Counter: 72
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : ""dynamicColorArray" : IECore . Color3fVectorData ( [ IECore . Color3f ( 1 ) , IECore . Color3f ( 2 ) ] ) , \n"
Original    (024): ['"dynamicColorArray"', ':', 'IECore', '.', 'Color3fVectorData', '(', '[', 'IECore', '.', 'Color3f', '(', '1', ')', ',', 'IECore', '.', 'Color3f', '(', '2', ')', ']', ')', ',', '\\n']
Tokenized   (045): ['[CLS]', '"', 'dynamic', '##color', '##ar', '##ray', '"', ':', 'iec', '##ore', '.', 'color', '##3', '##f', '##ve', '##ctor', '##da', '##ta', '(', '[', 'iec', '##ore', '.', 'color', '##3', '##f', '(', '1', ')', ',', 'iec', '##ore', '.', 'color', '##3', '##f', '(', '2', ')', ']', ')', ',', '\\', 'n', '[SEP]']
Filtered   (043): ['"', 'dynamic', '##color', '##ar', '##ray', '"', ':', 'iec', '##ore', '.', 'color', '##3', '##f', '##ve', '##ctor', '##da', '##ta', '(', '[', 'iec', '##ore', '.', 'color', '##3', '##f', '(', '1', ')', ',', 'iec', '##ore', '.', 'color', '##3', '##f', '(', '2', ')', ']', ')', ',', '\\', 'n']
Detokenized (024): ['"dynamic##color##ar##ray"', ':', 'iec##ore', '.', 'color##3##f##ve##ctor##da##ta', '(', '[', 'iec##ore', '.', 'color##3##f', '(', '1', ')', ',', 'iec##ore', '.', 'color##3##f', '(', '2', ')', ']', ')', ',', '\\n']
Counter: 43
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : ""dynamicVectorArray" : IECore . V3fVectorData ( [ ] , IECore . GeometricData . Interpretation . Vector ) , \n"
Original    (019): ['"dynamicVectorArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', ']', ',', 'IECore', '.', 'GeometricData', '.', 'Interpretation', '.', 'Vector', ')', ',', '\\n']
Tokenized   (038): ['[CLS]', '"', 'dynamic', '##ve', '##ctor', '##ar', '##ray', '"', ':', 'iec', '##ore', '.', 'v', '##3', '##f', '##ve', '##ctor', '##da', '##ta', '(', '[', ']', ',', 'iec', '##ore', '.', 'geometric', '##da', '##ta', '.', 'interpretation', '.', 'vector', ')', ',', '\\', 'n', '[SEP]']
Filtered   (036): ['"', 'dynamic', '##ve', '##ctor', '##ar', '##ray', '"', ':', 'iec', '##ore', '.', 'v', '##3', '##f', '##ve', '##ctor', '##da', '##ta', '(', '[', ']', ',', 'iec', '##ore', '.', 'geometric', '##da', '##ta', '.', 'interpretation', '.', 'vector', ')', ',', '\\', 'n']
Detokenized (019): ['"dynamic##ve##ctor##ar##ray"', ':', 'iec##ore', '.', 'v##3##f##ve##ctor##da##ta', '(', '[', ']', ',', 'iec##ore', '.', 'geometric##da##ta', '.', 'interpretation', '.', 'vector', ')', ',', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : ""fixedVectorArray" : IECore . V3fVectorData ( [ IECore . V3f ( x ) for x in range ( 1 , 6 ) ] , IECore . GeometricData "dynamicPointArray" : IECore . V3fVectorData ( [ ] , IECore . GeometricData . Interpretation . Point ) , \n"
Original    (046): ['"fixedVectorArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', 'IECore', '.', 'V3f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'IECore', '.', 'GeometricData', '"dynamicPointArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', ']', ',', 'IECore', '.', 'GeometricData', '.', 'Interpretation', '.', 'Point', ')', ',', '\\n']
Tokenized   (083): ['[CLS]', '"', 'fixed', '##ve', '##ctor', '##ar', '##ray', '"', ':', 'iec', '##ore', '.', 'v', '##3', '##f', '##ve', '##ctor', '##da', '##ta', '(', '[', 'iec', '##ore', '.', 'v', '##3', '##f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'iec', '##ore', '.', 'geometric', '##da', '##ta', '"', 'dynamic', '##point', '##ar', '##ray', '"', ':', 'iec', '##ore', '.', 'v', '##3', '##f', '##ve', '##ctor', '##da', '##ta', '(', '[', ']', ',', 'iec', '##ore', '.', 'geometric', '##da', '##ta', '.', 'interpretation', '.', 'point', ')', ',', '\\', 'n', '[SEP]']
Filtered   (081): ['"', 'fixed', '##ve', '##ctor', '##ar', '##ray', '"', ':', 'iec', '##ore', '.', 'v', '##3', '##f', '##ve', '##ctor', '##da', '##ta', '(', '[', 'iec', '##ore', '.', 'v', '##3', '##f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'iec', '##ore', '.', 'geometric', '##da', '##ta', '"', 'dynamic', '##point', '##ar', '##ray', '"', ':', 'iec', '##ore', '.', 'v', '##3', '##f', '##ve', '##ctor', '##da', '##ta', '(', '[', ']', ',', 'iec', '##ore', '.', 'geometric', '##da', '##ta', '.', 'interpretation', '.', 'point', ')', ',', '\\', 'n']
Detokenized (046): ['"fixed##ve##ctor##ar##ray"', ':', 'iec##ore', '.', 'v##3##f##ve##ctor##da##ta', '(', '[', 'iec##ore', '.', 'v##3##f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'iec##ore', '.', 'geometric##da##ta', '"dynamic##point##ar##ray"', ':', 'iec##ore', '.', 'v##3##f##ve##ctor##da##ta', '(', '[', ']', ',', 'iec##ore', '.', 'geometric##da##ta', '.', 'interpretation', '.', 'point', ')', ',', '\\n']
Counter: 81
===================================================================
Hidden states:  (13, 46, 768)
# Extracted words:  46
Sentence         : ""fixedNormalArray" : IECore . V3fVectorData ( [ IECore . V3f ( x ) for x in range ( 1 , 6 ) ] , IECore . GeometricData } \n"
Original    (029): ['"fixedNormalArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', 'IECore', '.', 'V3f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'IECore', '.', 'GeometricData', '}', '\\n']
Tokenized   (051): ['[CLS]', '"', 'fixed', '##nor', '##mal', '##ar', '##ray', '"', ':', 'iec', '##ore', '.', 'v', '##3', '##f', '##ve', '##ctor', '##da', '##ta', '(', '[', 'iec', '##ore', '.', 'v', '##3', '##f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'iec', '##ore', '.', 'geometric', '##da', '##ta', '}', '\\', 'n', '[SEP]']
Filtered   (049): ['"', 'fixed', '##nor', '##mal', '##ar', '##ray', '"', ':', 'iec', '##ore', '.', 'v', '##3', '##f', '##ve', '##ctor', '##da', '##ta', '(', '[', 'iec', '##ore', '.', 'v', '##3', '##f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'iec', '##ore', '.', 'geometric', '##da', '##ta', '}', '\\', 'n']
Detokenized (029): ['"fixed##nor##mal##ar##ray"', ':', 'iec##ore', '.', 'v##3##f##ve##ctor##da##ta', '(', '[', 'iec##ore', '.', 'v##3##f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'iec##ore', '.', 'geometric##da##ta', '}', '\\n']
Counter: 49
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "arrayShader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" n4 = GafferRenderMan . RenderManShader ( ) \n"
Original    (024): ['arrayShader', '=', 'self', '.', 'compileShader', '(', 'os', '.', 'path', '.', 'dirname', '(', '__file__', ')', '+', '"/shaders/coshaderArrayParameters.sl"', 'n4', '=', 'GafferRenderMan', '.', 'RenderManShader', '(', ')', '\\n']
Tokenized   (060): ['[CLS]', 'arrays', '##had', '##er', '=', 'self', '.', 'com', '##pile', '##sha', '##der', '(', 'os', '.', 'path', '.', 'dir', '##name', '(', '_', '_', 'file', '_', '_', ')', '+', '"', '/', 'shade', '##rs', '/', 'co', '##sha', '##der', '##ar', '##ray', '##para', '##meter', '##s', '.', 'sl', '"', 'n', '##4', '=', 'ga', '##ffer', '##ren', '##der', '##man', '.', 'render', '##mans', '##had', '##er', '(', ')', '\\', 'n', '[SEP]']
Filtered   (058): ['arrays', '##had', '##er', '=', 'self', '.', 'com', '##pile', '##sha', '##der', '(', 'os', '.', 'path', '.', 'dir', '##name', '(', '_', '_', 'file', '_', '_', ')', '+', '"', '/', 'shade', '##rs', '/', 'co', '##sha', '##der', '##ar', '##ray', '##para', '##meter', '##s', '.', 'sl', '"', 'n', '##4', '=', 'ga', '##ffer', '##ren', '##der', '##man', '.', 'render', '##mans', '##had', '##er', '(', ')', '\\', 'n']
Detokenized (024): ['arrays##had##er', '=', 'self', '.', 'com##pile##sha##der', '(', 'os', '.', 'path', '.', 'dir##name', '(', '__file__', ')', '+', '"/shade##rs/co##sha##der##ar##ray##para##meter##s.sl"', 'n##4', '=', 'ga##ffer##ren##der##man', '.', 'render##mans##had##er', '(', ')', '\\n']
Counter: 58
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "coshaderNode [ "enabled" ] . setValue ( False ) \n"
Original    (010): ['coshaderNode', '[', '"enabled"', ']', '.', 'setValue', '(', 'False', ')', '\\n']
Tokenized   (020): ['[CLS]', 'co', '##sha', '##dern', '##ode', '[', '"', 'enabled', '"', ']', '.', 'set', '##val', '##ue', '(', 'false', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['co', '##sha', '##dern', '##ode', '[', '"', 'enabled', '"', ']', '.', 'set', '##val', '##ue', '(', 'false', ')', '\\', 'n']
Detokenized (010): ['co##sha##dern##ode', '[', '"enabled"', ']', '.', 'set##val##ue', '(', 'false', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "floatValue = IECore . Splineff ( \n"
Original    (007): ['floatValue', '=', 'IECore', '.', 'Splineff', '(', '\\n']
Tokenized   (015): ['[CLS]', 'float', '##val', '##ue', '=', 'iec', '##ore', '.', 'sp', '##line', '##ff', '(', '\\', 'n', '[SEP]']
Filtered   (013): ['float', '##val', '##ue', '=', 'iec', '##ore', '.', 'sp', '##line', '##ff', '(', '\\', 'n']
Detokenized (007): ['float##val##ue', '=', 'iec##ore', '.', 'sp##line##ff', '(', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "S [ "parameters" ] [ "coshaderParameter" ] . setInput ( D2 [ "out" ] ) \n"
Original    (016): ['S', '[', '"parameters"', ']', '[', '"coshaderParameter"', ']', '.', 'setInput', '(', 'D2', '[', '"out"', ']', ')', '\\n']
Tokenized   (032): ['[CLS]', 's', '[', '"', 'parameters', '"', ']', '[', '"', 'co', '##sha', '##der', '##para', '##meter', '"', ']', '.', 'set', '##in', '##put', '(', 'd', '##2', '[', '"', 'out', '"', ']', ')', '\\', 'n', '[SEP]']
Filtered   (030): ['s', '[', '"', 'parameters', '"', ']', '[', '"', 'co', '##sha', '##der', '##para', '##meter', '"', ']', '.', 'set', '##in', '##put', '(', 'd', '##2', '[', '"', 'out', '"', ']', ')', '\\', 'n']
Detokenized (016): ['s', '[', '"parameters"', ']', '[', '"co##sha##der##para##meter"', ']', '.', 'set##in##put', '(', 'd##2', '[', '"out"', ']', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "coshaderNode0 [ "parameters" ] [ "floatParameter" ] . setValue ( 0 ) \n"
Original    (013): ['coshaderNode0', '[', '"parameters"', ']', '[', '"floatParameter"', ']', '.', 'setValue', '(', '0', ')', '\\n']
Tokenized   (028): ['[CLS]', 'co', '##sha', '##dern', '##ode', '##0', '[', '"', 'parameters', '"', ']', '[', '"', 'float', '##para', '##meter', '"', ']', '.', 'set', '##val', '##ue', '(', '0', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['co', '##sha', '##dern', '##ode', '##0', '[', '"', 'parameters', '"', ']', '[', '"', 'float', '##para', '##meter', '"', ']', '.', 'set', '##val', '##ue', '(', '0', ')', '\\', 'n']
Detokenized (013): ['co##sha##dern##ode##0', '[', '"parameters"', ']', '[', '"float##para##meter"', ']', '.', 'set##val##ue', '(', '0', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "sn1 = GafferRenderMan . RenderManShader ( "Shader1" ) \n"
Original    (009): ['sn1', '=', 'GafferRenderMan', '.', 'RenderManShader', '(', '"Shader1"', ')', '\\n']
Tokenized   (025): ['[CLS]', 's', '##n', '##1', '=', 'ga', '##ffer', '##ren', '##der', '##man', '.', 'render', '##mans', '##had', '##er', '(', '"', 'shade', '##r', '##1', '"', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['s', '##n', '##1', '=', 'ga', '##ffer', '##ren', '##der', '##man', '.', 'render', '##mans', '##had', '##er', '(', '"', 'shade', '##r', '##1', '"', ')', '\\', 'n']
Detokenized (009): ['s##n##1', '=', 'ga##ffer##ren##der##man', '.', 'render##mans##had##er', '(', '"shade##r##1"', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "script [ "assignment" ] [ "shader" ] . setInput ( script [ "shader" ] [ "out" ] ) \n"
Original    (019): ['script', '[', '"assignment"', ']', '[', '"shader"', ']', '.', 'setInput', '(', 'script', '[', '"shader"', ']', '[', '"out"', ']', ')', '\\n']
Tokenized   (034): ['[CLS]', 'script', '[', '"', 'assignment', '"', ']', '[', '"', 'shade', '##r', '"', ']', '.', 'set', '##in', '##put', '(', 'script', '[', '"', 'shade', '##r', '"', ']', '[', '"', 'out', '"', ']', ')', '\\', 'n', '[SEP]']
Filtered   (032): ['script', '[', '"', 'assignment', '"', ']', '[', '"', 'shade', '##r', '"', ']', '.', 'set', '##in', '##put', '(', 'script', '[', '"', 'shade', '##r', '"', ']', '[', '"', 'out', '"', ']', ')', '\\', 'n']
Detokenized (019): ['script', '[', '"assignment"', ']', '[', '"shade##r"', ']', '.', 'set##in##put', '(', 'script', '[', '"shade##r"', ']', '[', '"out"', ']', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "traverseConnection = Gaffer . ScopedConnection ( GafferSceneTest . connectTraverseSceneToPlugDirtiedSignal script [ "shader" ] . loadShader ( "matte" ) \n"
Original    (019): ['traverseConnection', '=', 'Gaffer', '.', 'ScopedConnection', '(', 'GafferSceneTest', '.', 'connectTraverseSceneToPlugDirtiedSignal', 'script', '[', '"shader"', ']', '.', 'loadShader', '(', '"matte"', ')', '\\n']
Tokenized   (054): ['[CLS]', 'traverse', '##con', '##ne', '##ction', '=', 'ga', '##ffer', '.', 'scope', '##dc', '##onne', '##ction', '(', 'ga', '##ffer', '##sc', '##ene', '##test', '.', 'connect', '##tra', '##verse', '##sc', '##ene', '##top', '##lu', '##g', '##di', '##rti', '##ed', '##si', '##gna', '##l', 'script', '[', '"', 'shade', '##r', '"', ']', '.', 'loads', '##had', '##er', '(', '"', 'matt', '##e', '"', ')', '\\', 'n', '[SEP]']
Filtered   (052): ['traverse', '##con', '##ne', '##ction', '=', 'ga', '##ffer', '.', 'scope', '##dc', '##onne', '##ction', '(', 'ga', '##ffer', '##sc', '##ene', '##test', '.', 'connect', '##tra', '##verse', '##sc', '##ene', '##top', '##lu', '##g', '##di', '##rti', '##ed', '##si', '##gna', '##l', 'script', '[', '"', 'shade', '##r', '"', ']', '.', 'loads', '##had', '##er', '(', '"', 'matt', '##e', '"', ')', '\\', 'n']
Detokenized (019): ['traverse##con##ne##ction', '=', 'ga##ffer', '.', 'scope##dc##onne##ction', '(', 'ga##ffer##sc##ene##test', '.', 'connect##tra##verse##sc##ene##top##lu##g##di##rti##ed##si##gna##l', 'script', '[', '"shade##r"', ']', '.', 'loads##had##er', '(', '"matt##e"', ')', '\\n']
Counter: 52
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "current = s [ "render" ] . hash ( c ) \n"
Original    (012): ['current', '=', 's', '[', '"render"', ']', '.', 'hash', '(', 'c', ')', '\\n']
Tokenized   (017): ['[CLS]', 'current', '=', 's', '[', '"', 'render', '"', ']', '.', 'hash', '(', 'c', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['current', '=', 's', '[', '"', 'render', '"', ']', '.', 'hash', '(', 'c', ')', '\\', 'n']
Detokenized (012): ['current', '=', 's', '[', '"render"', ']', '.', 'hash', '(', 'c', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : ""layout:section" , "Transform" , \n"
Original    (005): ['"layout:section"', ',', '"Transform"', ',', '\\n']
Tokenized   (014): ['[CLS]', '"', 'layout', ':', 'section', '"', ',', '"', 'transform', '"', ',', '\\', 'n', '[SEP]']
Filtered   (012): ['"', 'layout', ':', 'section', '"', ',', '"', 'transform', '"', ',', '\\', 'n']
Detokenized (005): ['"layout:section"', ',', '"transform"', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""toolbarLayout:index" , 2 , \n"
Original    (005): ['"toolbarLayout:index"', ',', '2', ',', '\\n']
Tokenized   (015): ['[CLS]', '"', 'tool', '##bar', '##lay', '##out', ':', 'index', '"', ',', '2', ',', '\\', 'n', '[SEP]']
Filtered   (013): ['"', 'tool', '##bar', '##lay', '##out', ':', 'index', '"', ',', '2', ',', '\\', 'n']
Detokenized (005): ['"tool##bar##lay##out:index"', ',', '2', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""toolbarLayout:divider" , True , \n"
Original    (005): ['"toolbarLayout:divider"', ',', 'True', ',', '\\n']
Tokenized   (016): ['[CLS]', '"', 'tool', '##bar', '##lay', '##out', ':', 'divide', '##r', '"', ',', 'true', ',', '\\', 'n', '[SEP]']
Filtered   (014): ['"', 'tool', '##bar', '##lay', '##out', ':', 'divide', '##r', '"', ',', 'true', ',', '\\', 'n']
Detokenized (005): ['"tool##bar##lay##out:divide##r"', ',', 'true', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "currentName = self . getPlug ( ) . getValue ( ) \n"
Original    (012): ['currentName', '=', 'self', '.', 'getPlug', '(', ')', '.', 'getValue', '(', ')', '\\n']
Tokenized   (020): ['[CLS]', 'current', '##name', '=', 'self', '.', 'get', '##pl', '##ug', '(', ')', '.', 'get', '##val', '##ue', '(', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['current', '##name', '=', 'self', '.', 'get', '##pl', '##ug', '(', ')', '.', 'get', '##val', '##ue', '(', ')', '\\', 'n']
Detokenized (012): ['current##name', '=', 'self', '.', 'get##pl##ug', '(', ')', '.', 'get##val##ue', '(', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "menuButton = GafferUI . MenuButton ( menu = menu , image = "grid.png" , hasFrame = False ) \n"
Original    (019): ['menuButton', '=', 'GafferUI', '.', 'MenuButton', '(', 'menu', '=', 'menu', ',', 'image', '=', '"grid.png"', ',', 'hasFrame', '=', 'False', ')', '\\n']
Tokenized   (034): ['[CLS]', 'menu', '##bu', '##tton', '=', 'ga', '##ffer', '##ui', '.', 'menu', '##bu', '##tton', '(', 'menu', '=', 'menu', ',', 'image', '=', '"', 'grid', '.', 'p', '##ng', '"', ',', 'has', '##frame', '=', 'false', ')', '\\', 'n', '[SEP]']
Filtered   (032): ['menu', '##bu', '##tton', '=', 'ga', '##ffer', '##ui', '.', 'menu', '##bu', '##tton', '(', 'menu', '=', 'menu', ',', 'image', '=', '"', 'grid', '.', 'p', '##ng', '"', ',', 'has', '##frame', '=', 'false', ')', '\\', 'n']
Detokenized (019): ['menu##bu##tton', '=', 'ga##ffer##ui', '.', 'menu##bu##tton', '(', 'menu', '=', 'menu', ',', 'image', '=', '"grid.p##ng"', ',', 'has##frame', '=', 'false', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "p3 = Gaffer . IntPlug ( "sum" , Gaffer . Plug . Direction . Out ) \n"
Original    (017): ['p3', '=', 'Gaffer', '.', 'IntPlug', '(', '"sum"', ',', 'Gaffer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\n']
Tokenized   (027): ['[CLS]', 'p', '##3', '=', 'ga', '##ffer', '.', 'int', '##pl', '##ug', '(', '"', 'sum', '"', ',', 'ga', '##ffer', '.', 'plug', '.', 'direction', '.', 'out', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['p', '##3', '=', 'ga', '##ffer', '.', 'int', '##pl', '##ug', '(', '"', 'sum', '"', ',', 'ga', '##ffer', '.', 'plug', '.', 'direction', '.', 'out', ')', '\\', 'n']
Detokenized (017): ['p##3', '=', 'ga##ffer', '.', 'int##pl##ug', '(', '"sum"', ',', 'ga##ffer', '.', 'plug', '.', 'direction', '.', 'out', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "childrenStrings = [ str ( c ) for c in children ] \n"
Original    (013): ['childrenStrings', '=', '[', 'str', '(', 'c', ')', 'for', 'c', 'in', 'children', ']', '\\n']
Tokenized   (020): ['[CLS]', 'children', '##st', '##ring', '##s', '=', '[', 'st', '##r', '(', 'c', ')', 'for', 'c', 'in', 'children', ']', '\\', 'n', '[SEP]']
Filtered   (018): ['children', '##st', '##ring', '##s', '=', '[', 'st', '##r', '(', 'c', ')', 'for', 'c', 'in', 'children', ']', '\\', 'n']
Detokenized (013): ['children##st##ring##s', '=', '[', 'st##r', '(', 'c', ')', 'for', 'c', 'in', 'children', ']', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "c2 = [ str ( p ) for p in path2 . children ( ) ] \n"
Original    (017): ['c2', '=', '[', 'str', '(', 'p', ')', 'for', 'p', 'in', 'path2', '.', 'children', '(', ')', ']', '\\n']
Tokenized   (022): ['[CLS]', 'c2', '=', '[', 'st', '##r', '(', 'p', ')', 'for', 'p', 'in', 'path', '##2', '.', 'children', '(', ')', ']', '\\', 'n', '[SEP]']
Filtered   (020): ['c2', '=', '[', 'st', '##r', '(', 'p', ')', 'for', 'p', 'in', 'path', '##2', '.', 'children', '(', ')', ']', '\\', 'n']
Detokenized (017): ['c2', '=', '[', 'st##r', '(', 'p', ')', 'for', 'p', 'in', 'path##2', '.', 'children', '(', ')', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "horizontalAlignment = GafferUI . Label . HorizontalAlignment . Right , \n"
Original    (011): ['horizontalAlignment', '=', 'GafferUI', '.', 'Label', '.', 'HorizontalAlignment', '.', 'Right', ',', '\\n']
Tokenized   (022): ['[CLS]', 'horizontal', '##ali', '##gn', '##ment', '=', 'ga', '##ffer', '##ui', '.', 'label', '.', 'horizontal', '##ali', '##gn', '##ment', '.', 'right', ',', '\\', 'n', '[SEP]']
Filtered   (020): ['horizontal', '##ali', '##gn', '##ment', '=', 'ga', '##ffer', '##ui', '.', 'label', '.', 'horizontal', '##ali', '##gn', '##ment', '.', 'right', ',', '\\', 'n']
Detokenized (011): ['horizontal##ali##gn##ment', '=', 'ga##ffer##ui', '.', 'label', '.', 'horizontal##ali##gn##ment', '.', 'right', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "nameWidget . textWidget ( ) . _qtWidget ( ) . setFixedWidth ( GafferUI . PlugWidget . labelWidth ( ) ) \n"
Original    (021): ['nameWidget', '.', 'textWidget', '(', ')', '.', '_qtWidget', '(', ')', '.', 'setFixedWidth', '(', 'GafferUI', '.', 'PlugWidget', '.', 'labelWidth', '(', ')', ')', '\\n']
Tokenized   (043): ['[CLS]', 'name', '##wi', '##dget', '.', 'text', '##wi', '##dget', '(', ')', '.', '_', 'q', '##t', '##wi', '##dget', '(', ')', '.', 'set', '##fixed', '##wi', '##dt', '##h', '(', 'ga', '##ffer', '##ui', '.', 'plug', '##wi', '##dget', '.', 'label', '##wi', '##dt', '##h', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (041): ['name', '##wi', '##dget', '.', 'text', '##wi', '##dget', '(', ')', '.', '_', 'q', '##t', '##wi', '##dget', '(', ')', '.', 'set', '##fixed', '##wi', '##dt', '##h', '(', 'ga', '##ffer', '##ui', '.', 'plug', '##wi', '##dget', '.', 'label', '##wi', '##dt', '##h', '(', ')', ')', '\\', 'n']
Detokenized (021): ['name##wi##dget', '.', 'text##wi##dget', '(', ')', '.', '_q##t##wi##dget', '(', ')', '.', 'set##fixed##wi##dt##h', '(', 'ga##ffer##ui', '.', 'plug##wi##dget', '.', 'label##wi##dt##h', '(', ')', ')', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "childPlug [ "enabled" ] , \n"
Original    (006): ['childPlug', '[', '"enabled"', ']', ',', '\\n']
Tokenized   (013): ['[CLS]', 'child', '##pl', '##ug', '[', '"', 'enabled', '"', ']', ',', '\\', 'n', '[SEP]']
Filtered   (011): ['child', '##pl', '##ug', '[', '"', 'enabled', '"', ']', ',', '\\', 'n']
Detokenized (006): ['child##pl##ug', '[', '"enabled"', ']', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "memberPlug = memberPlug if memberPlug is not None else plug . ancestor ( Gaffer . CompoundDataPlug . MemberPlug if memberPlug is None : \n"
Original    (024): ['memberPlug', '=', 'memberPlug', 'if', 'memberPlug', 'is', 'not', 'None', 'else', 'plug', '.', 'ancestor', '(', 'Gaffer', '.', 'CompoundDataPlug', '.', 'MemberPlug', 'if', 'memberPlug', 'is', 'None', ':', '\\n']
Tokenized   (042): ['[CLS]', 'member', '##pl', '##ug', '=', 'member', '##pl', '##ug', 'if', 'member', '##pl', '##ug', 'is', 'not', 'none', 'else', 'plug', '.', 'ancestor', '(', 'ga', '##ffer', '.', 'compound', '##da', '##ta', '##pl', '##ug', '.', 'member', '##pl', '##ug', 'if', 'member', '##pl', '##ug', 'is', 'none', ':', '\\', 'n', '[SEP]']
Filtered   (040): ['member', '##pl', '##ug', '=', 'member', '##pl', '##ug', 'if', 'member', '##pl', '##ug', 'is', 'not', 'none', 'else', 'plug', '.', 'ancestor', '(', 'ga', '##ffer', '.', 'compound', '##da', '##ta', '##pl', '##ug', '.', 'member', '##pl', '##ug', 'if', 'member', '##pl', '##ug', 'is', 'none', ':', '\\', 'n']
Detokenized (024): ['member##pl##ug', '=', 'member##pl##ug', 'if', 'member##pl##ug', 'is', 'not', 'none', 'else', 'plug', '.', 'ancestor', '(', 'ga##ffer', '.', 'compound##da##ta##pl##ug', '.', 'member##pl##ug', 'if', 'member##pl##ug', 'is', 'none', ':', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "menuDefinition . append ( "/Delete" , { "command" : IECore . curry ( __deletePlug , memberPlug ) , "active" \n"
Original    (020): ['menuDefinition', '.', 'append', '(', '"/Delete"', ',', '{', '"command"', ':', 'IECore', '.', 'curry', '(', '__deletePlug', ',', 'memberPlug', ')', ',', '"active"', '\\n']
Tokenized   (043): ['[CLS]', 'menu', '##de', '##fin', '##ition', '.', 'app', '##end', '(', '"', '/', 'del', '##ete', '"', ',', '{', '"', 'command', '"', ':', 'iec', '##ore', '.', 'curry', '(', '_', '_', 'del', '##ete', '##pl', '##ug', ',', 'member', '##pl', '##ug', ')', ',', '"', 'active', '"', '\\', 'n', '[SEP]']
Filtered   (041): ['menu', '##de', '##fin', '##ition', '.', 'app', '##end', '(', '"', '/', 'del', '##ete', '"', ',', '{', '"', 'command', '"', ':', 'iec', '##ore', '.', 'curry', '(', '_', '_', 'del', '##ete', '##pl', '##ug', ',', 'member', '##pl', '##ug', ')', ',', '"', 'active', '"', '\\', 'n']
Detokenized (020): ['menu##de##fin##ition', '.', 'app##end', '(', '"/del##ete"', ',', '{', '"command"', ':', 'iec##ore', '.', 'curry', '(', '__del##ete##pl##ug', ',', 'member##pl##ug', ')', ',', '"active"', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "includeSequences = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "fileSystemPathPlugValueWidget:includeSequences" \n"
Original    (016): ['includeSequences', '=', 'Gaffer', '.', 'Metadata', '.', 'plugValue', '(', 'self', '.', 'getPlug', '(', ')', ',', '"fileSystemPathPlugValueWidget:includeSequences"', '\\n']
Tokenized   (045): ['[CLS]', 'includes', '##e', '##que', '##nce', '##s', '=', 'ga', '##ffer', '.', 'metadata', '.', 'plug', '##val', '##ue', '(', 'self', '.', 'get', '##pl', '##ug', '(', ')', ',', '"', 'files', '##yst', '##em', '##path', '##pl', '##ug', '##val', '##ue', '##wi', '##dget', ':', 'includes', '##e', '##que', '##nce', '##s', '"', '\\', 'n', '[SEP]']
Filtered   (043): ['includes', '##e', '##que', '##nce', '##s', '=', 'ga', '##ffer', '.', 'metadata', '.', 'plug', '##val', '##ue', '(', 'self', '.', 'get', '##pl', '##ug', '(', ')', ',', '"', 'files', '##yst', '##em', '##path', '##pl', '##ug', '##val', '##ue', '##wi', '##dget', ':', 'includes', '##e', '##que', '##nce', '##s', '"', '\\', 'n']
Detokenized (016): ['includes##e##que##nce##s', '=', 'ga##ffer', '.', 'metadata', '.', 'plug##val##ue', '(', 'self', '.', 'get##pl##ug', '(', ')', ',', '"files##yst##em##path##pl##ug##val##ue##wi##dget:includes##e##que##nce##s"', '\\n']
Counter: 43
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "reuse = reuseUntil is not None \n"
Original    (007): ['reuse', '=', 'reuseUntil', 'is', 'not', 'None', '\\n']
Tokenized   (014): ['[CLS]', 're', '##use', '=', 're', '##use', '##unt', '##il', 'is', 'not', 'none', '\\', 'n', '[SEP]']
Filtered   (012): ['re', '##use', '=', 're', '##use', '##unt', '##il', 'is', 'not', 'none', '\\', 'n']
Detokenized (007): ['re##use', '=', 're##use##unt##il', 'is', 'not', 'none', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "_MultiLineStringMetadataWidget ( key = "description" ) \n"
Original    (007): ['_MultiLineStringMetadataWidget', '(', 'key', '=', '"description"', ')', '\\n']
Tokenized   (021): ['[CLS]', '_', 'multi', '##lines', '##tri', '##ng', '##met', '##ada', '##ta', '##wi', '##dget', '(', 'key', '=', '"', 'description', '"', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['_', 'multi', '##lines', '##tri', '##ng', '##met', '##ada', '##ta', '##wi', '##dget', '(', 'key', '=', '"', 'description', '"', ')', '\\', 'n']
Detokenized (007): ['_multi##lines##tri##ng##met##ada##ta##wi##dget', '(', 'key', '=', '"description"', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""active" : isinstance ( node , Gaffer . Box ) or nodeEditor . nodeUI ( ) . plugValueWidget ( node [ "user" ] ) } \n"
Original    (026): ['"active"', ':', 'isinstance', '(', 'node', ',', 'Gaffer', '.', 'Box', ')', 'or', 'nodeEditor', '.', 'nodeUI', '(', ')', '.', 'plugValueWidget', '(', 'node', '[', '"user"', ']', ')', '}', '\\n']
Tokenized   (043): ['[CLS]', '"', 'active', '"', ':', 'is', '##ins', '##tance', '(', 'node', ',', 'ga', '##ffer', '.', 'box', ')', 'or', 'node', '##ed', '##itor', '.', 'node', '##ui', '(', ')', '.', 'plug', '##val', '##ue', '##wi', '##dget', '(', 'node', '[', '"', 'user', '"', ']', ')', '}', '\\', 'n', '[SEP]']
Filtered   (041): ['"', 'active', '"', ':', 'is', '##ins', '##tance', '(', 'node', ',', 'ga', '##ffer', '.', 'box', ')', 'or', 'node', '##ed', '##itor', '.', 'node', '##ui', '(', ')', '.', 'plug', '##val', '##ue', '##wi', '##dget', '(', 'node', '[', '"', 'user', '"', ']', ')', '}', '\\', 'n']
Detokenized (026): ['"active"', ':', 'is##ins##tance', '(', 'node', ',', 'ga##ffer', '.', 'box', ')', 'or', 'node##ed##itor', '.', 'node##ui', '(', ')', '.', 'plug##val##ue##wi##dget', '(', 'node', '[', '"user"', ']', ')', '}', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "dialogue = GafferUI . ColorChooserDialogue ( color = color , useDisplayTransform = False ) \n"
Original    (015): ['dialogue', '=', 'GafferUI', '.', 'ColorChooserDialogue', '(', 'color', '=', 'color', ',', 'useDisplayTransform', '=', 'False', ')', '\\n']
Tokenized   (030): ['[CLS]', 'dialogue', '=', 'ga', '##ffer', '##ui', '.', 'color', '##cho', '##ose', '##rdial', '##og', '##ue', '(', 'color', '=', 'color', ',', 'used', '##is', '##play', '##tra', '##ns', '##form', '=', 'false', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['dialogue', '=', 'ga', '##ffer', '##ui', '.', 'color', '##cho', '##ose', '##rdial', '##og', '##ue', '(', 'color', '=', 'color', ',', 'used', '##is', '##play', '##tra', '##ns', '##form', '=', 'false', ')', '\\', 'n']
Detokenized (015): ['dialogue', '=', 'ga##ffer##ui', '.', 'color##cho##ose##rdial##og##ue', '(', 'color', '=', 'color', ',', 'used##is##play##tra##ns##form', '=', 'false', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "editor . plugEditor ( ) . reveal ( ) \n"
Original    (010): ['editor', '.', 'plugEditor', '(', ')', '.', 'reveal', '(', ')', '\\n']
Tokenized   (015): ['[CLS]', 'editor', '.', 'plug', '##ed', '##itor', '(', ')', '.', 'reveal', '(', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['editor', '.', 'plug', '##ed', '##itor', '(', ')', '.', 'reveal', '(', ')', '\\', 'n']
Detokenized (010): ['editor', '.', 'plug##ed##itor', '(', ')', '.', 'reveal', '(', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "_MetadataWidget . __init__ ( self , self . __menuButton , key , target , parenting = parenting ) \n"
Original    (019): ['_MetadataWidget', '.', '__init__', '(', 'self', ',', 'self', '.', '__menuButton', ',', 'key', ',', 'target', ',', 'parenting', '=', 'parenting', ')', '\\n']
Tokenized   (034): ['[CLS]', '_', 'metadata', '##wi', '##dget', '.', '_', '_', 'in', '##it', '_', '_', '(', 'self', ',', 'self', '.', '_', '_', 'menu', '##bu', '##tton', ',', 'key', ',', 'target', ',', 'parenting', '=', 'parenting', ')', '\\', 'n', '[SEP]']
Filtered   (032): ['_', 'metadata', '##wi', '##dget', '.', '_', '_', 'in', '##it', '_', '_', '(', 'self', ',', 'self', '.', '_', '_', 'menu', '##bu', '##tton', ',', 'key', ',', 'target', ',', 'parenting', '=', 'parenting', ')', '\\', 'n']
Detokenized (019): ['_metadata##wi##dget', '.', '__in##it__', '(', 'self', ',', 'self', '.', '__menu##bu##tton', ',', 'key', ',', 'target', ',', 'parenting', '=', 'parenting', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : ""checkBox" : value == self . __currentValue \n"
Original    (008): ['"checkBox"', ':', 'value', '==', 'self', '.', '__currentValue', '\\n']
Tokenized   (019): ['[CLS]', '"', 'check', '##box', '"', ':', 'value', '=', '=', 'self', '.', '_', '_', 'current', '##val', '##ue', '\\', 'n', '[SEP]']
Filtered   (017): ['"', 'check', '##box', '"', ':', 'value', '=', '=', 'self', '.', '_', '_', 'current', '##val', '##ue', '\\', 'n']
Detokenized (008): ['"check##box"', ':', 'value', '==', 'self', '.', '__current##val##ue', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "child . __parent = None \n"
Original    (006): ['child', '.', '__parent', '=', 'None', '\\n']
Tokenized   (011): ['[CLS]', 'child', '.', '_', '_', 'parent', '=', 'none', '\\', 'n', '[SEP]']
Filtered   (009): ['child', '.', '_', '_', 'parent', '=', 'none', '\\', 'n']
Detokenized (006): ['child', '.', '__parent', '=', 'none', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "columns = ( GafferUI . PathListingWidget . defaultNameColumn , ) , \n"
Original    (012): ['columns', '=', '(', 'GafferUI', '.', 'PathListingWidget', '.', 'defaultNameColumn', ',', ')', ',', '\\n']
Tokenized   (025): ['[CLS]', 'columns', '=', '(', 'ga', '##ffer', '##ui', '.', 'path', '##list', '##ing', '##wi', '##dget', '.', 'default', '##name', '##col', '##um', '##n', ',', ')', ',', '\\', 'n', '[SEP]']
Filtered   (023): ['columns', '=', '(', 'ga', '##ffer', '##ui', '.', 'path', '##list', '##ing', '##wi', '##dget', '.', 'default', '##name', '##col', '##um', '##n', ',', ')', ',', '\\', 'n']
Detokenized (012): ['columns', '=', '(', 'ga##ffer##ui', '.', 'path##list##ing##wi##dget', '.', 'default##name##col##um##n', ',', ')', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "definition = Gaffer . WeakMethod ( self . __addMenuDefinition ) \n"
Original    (011): ['definition', '=', 'Gaffer', '.', 'WeakMethod', '(', 'self', '.', '__addMenuDefinition', ')', '\\n']
Tokenized   (024): ['[CLS]', 'definition', '=', 'ga', '##ffer', '.', 'weak', '##met', '##ho', '##d', '(', 'self', '.', '_', '_', 'add', '##men', '##ude', '##fin', '##ition', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['definition', '=', 'ga', '##ffer', '.', 'weak', '##met', '##ho', '##d', '(', 'self', '.', '_', '_', 'add', '##men', '##ude', '##fin', '##ition', ')', '\\', 'n']
Detokenized (011): ['definition', '=', 'ga##ffer', '.', 'weak##met##ho##d', '(', 'self', '.', '__add##men##ude##fin##ition', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "newIndex = 0 if event . line . p0 . y < 1 else len ( newParent ) \n"
Original    (019): ['newIndex', '=', '0', 'if', 'event', '.', 'line', '.', 'p0', '.', 'y', '<', '1', 'else', 'len', '(', 'newParent', ')', '\\n']
Tokenized   (027): ['[CLS]', 'new', '##ind', '##ex', '=', '0', 'if', 'event', '.', 'line', '.', 'p', '##0', '.', 'y', '<', '1', 'else', 'len', '(', 'new', '##par', '##ent', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['new', '##ind', '##ex', '=', '0', 'if', 'event', '.', 'line', '.', 'p', '##0', '.', 'y', '<', '1', 'else', 'len', '(', 'new', '##par', '##ent', ')', '\\', 'n']
Detokenized (019): ['new##ind##ex', '=', '0', 'if', 'event', '.', 'line', '.', 'p##0', '.', 'y', '<', '1', 'else', 'len', '(', 'new##par##ent', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "newParent . insert ( newIndex , self . __dragItem ) \n"
Original    (011): ['newParent', '.', 'insert', '(', 'newIndex', ',', 'self', '.', '__dragItem', ')', '\\n']
Tokenized   (022): ['[CLS]', 'new', '##par', '##ent', '.', 'insert', '(', 'new', '##ind', '##ex', ',', 'self', '.', '_', '_', 'drag', '##ite', '##m', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['new', '##par', '##ent', '.', 'insert', '(', 'new', '##ind', '##ex', ',', 'self', '.', '_', '_', 'drag', '##ite', '##m', ')', '\\', 'n']
Detokenized (011): ['new##par##ent', '.', 'insert', '(', 'new##ind##ex', ',', 'self', '.', '__drag##ite##m', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "selection [ : ] = self . __dragItem . fullName ( ) . split ( "." ) \n"
Original    (018): ['selection', '[', ':', ']', '=', 'self', '.', '__dragItem', '.', 'fullName', '(', ')', '.', 'split', '(', '"."', ')', '\\n']
Tokenized   (028): ['[CLS]', 'selection', '[', ':', ']', '=', 'self', '.', '_', '_', 'drag', '##ite', '##m', '.', 'full', '##name', '(', ')', '.', 'split', '(', '"', '.', '"', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['selection', '[', ':', ']', '=', 'self', '.', '_', '_', 'drag', '##ite', '##m', '.', 'full', '##name', '(', ')', '.', 'split', '(', '"', '.', '"', ')', '\\', 'n']
Detokenized (018): ['selection', '[', ':', ']', '=', 'self', '.', '__drag##ite##m', '.', 'full##name', '(', ')', '.', 'split', '(', '"."', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "_registerMetadata ( plug , "nodule:type" , "" ) \n"
Original    (009): ['_registerMetadata', '(', 'plug', ',', '"nodule:type"', ',', '""', ')', '\\n']
Tokenized   (022): ['[CLS]', '_', 'register', '##met', '##ada', '##ta', '(', 'plug', ',', '"', 'nod', '##ule', ':', 'type', '"', ',', '"', '"', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['_', 'register', '##met', '##ada', '##ta', '(', 'plug', ',', '"', 'nod', '##ule', ':', 'type', '"', ',', '"', '"', ')', '\\', 'n']
Detokenized (009): ['_register##met##ada##ta', '(', 'plug', ',', '"nod##ule:type"', ',', '""', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "parentItem \n"
Original    (002): ['parentItem', '\\n']
Tokenized   (007): ['[CLS]', 'parent', '##ite', '##m', '\\', 'n', '[SEP]']
Filtered   (005): ['parent', '##ite', '##m', '\\', 'n']
Detokenized (002): ['parent##ite##m', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 2, 768)
# Extracted words:  2
Sentence         : "existingSectionNames = set ( c . name ( ) for c in rootItem if isinstance ( c , _SectionLayoutItem ) ) \n"
Original    (022): ['existingSectionNames', '=', 'set', '(', 'c', '.', 'name', '(', ')', 'for', 'c', 'in', 'rootItem', 'if', 'isinstance', '(', 'c', ',', '_SectionLayoutItem', ')', ')', '\\n']
Tokenized   (037): ['[CLS]', 'existing', '##section', '##name', '##s', '=', 'set', '(', 'c', '.', 'name', '(', ')', 'for', 'c', 'in', 'root', '##ite', '##m', 'if', 'is', '##ins', '##tance', '(', 'c', ',', '_', 'section', '##lay', '##out', '##ite', '##m', ')', ')', '\\', 'n', '[SEP]']
Filtered   (035): ['existing', '##section', '##name', '##s', '=', 'set', '(', 'c', '.', 'name', '(', ')', 'for', 'c', 'in', 'root', '##ite', '##m', 'if', 'is', '##ins', '##tance', '(', 'c', ',', '_', 'section', '##lay', '##out', '##ite', '##m', ')', ')', '\\', 'n']
Detokenized (022): ['existing##section##name##s', '=', 'set', '(', 'c', '.', 'name', '(', ')', 'for', 'c', 'in', 'root##ite##m', 'if', 'is##ins##tance', '(', 'c', ',', '_section##lay##out##ite##m', ')', ')', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "Gaffer . Metadata . plugValue ( self . getPlug ( ) , "preset:" + selectedPaths [ 0 ] [ 0 ] ) \n"
Original    (023): ['Gaffer', '.', 'Metadata', '.', 'plugValue', '(', 'self', '.', 'getPlug', '(', ')', ',', '"preset:"', '+', 'selectedPaths', '[', '0', ']', '[', '0', ']', ')', '\\n']
Tokenized   (037): ['[CLS]', 'ga', '##ffer', '.', 'metadata', '.', 'plug', '##val', '##ue', '(', 'self', '.', 'get', '##pl', '##ug', '(', ')', ',', '"', 'pre', '##set', ':', '"', '+', 'selected', '##path', '##s', '[', '0', ']', '[', '0', ']', ')', '\\', 'n', '[SEP]']
Filtered   (035): ['ga', '##ffer', '.', 'metadata', '.', 'plug', '##val', '##ue', '(', 'self', '.', 'get', '##pl', '##ug', '(', ')', ',', '"', 'pre', '##set', ':', '"', '+', 'selected', '##path', '##s', '[', '0', ']', '[', '0', ']', ')', '\\', 'n']
Detokenized (023): ['ga##ffer', '.', 'metadata', '.', 'plug##val##ue', '(', 'self', '.', 'get##pl##ug', '(', ')', ',', '"pre##set:"', '+', 'selected##path##s', '[', '0', ']', '[', '0', ']', ')', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "srcPath = self . __pathListing . getPath ( ) . copy ( ) . setFromString ( event . data [ 0 ] ) \n"
Original    (024): ['srcPath', '=', 'self', '.', '__pathListing', '.', 'getPath', '(', ')', '.', 'copy', '(', ')', '.', 'setFromString', '(', 'event', '.', 'data', '[', '0', ']', ')', '\\n']
Tokenized   (038): ['[CLS]', 'sr', '##cp', '##ath', '=', 'self', '.', '_', '_', 'path', '##list', '##ing', '.', 'get', '##path', '(', ')', '.', 'copy', '(', ')', '.', 'set', '##fr', '##oms', '##tri', '##ng', '(', 'event', '.', 'data', '[', '0', ']', ')', '\\', 'n', '[SEP]']
Filtered   (036): ['sr', '##cp', '##ath', '=', 'self', '.', '_', '_', 'path', '##list', '##ing', '.', 'get', '##path', '(', ')', '.', 'copy', '(', ')', '.', 'set', '##fr', '##oms', '##tri', '##ng', '(', 'event', '.', 'data', '[', '0', ']', ')', '\\', 'n']
Detokenized (024): ['sr##cp##ath', '=', 'self', '.', '__path##list##ing', '.', 'get##path', '(', ')', '.', 'copy', '(', ')', '.', 'set##fr##oms##tri##ng', '(', 'event', '.', 'data', '[', '0', ']', ')', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "srcIndex = d . keys ( ) . index ( srcPath [ 0 ] ) \n"
Original    (016): ['srcIndex', '=', 'd', '.', 'keys', '(', ')', '.', 'index', '(', 'srcPath', '[', '0', ']', ')', '\\n']
Tokenized   (024): ['[CLS]', 'sr', '##cin', '##de', '##x', '=', 'd', '.', 'keys', '(', ')', '.', 'index', '(', 'sr', '##cp', '##ath', '[', '0', ']', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['sr', '##cin', '##de', '##x', '=', 'd', '.', 'keys', '(', ')', '.', 'index', '(', 'sr', '##cp', '##ath', '[', '0', ']', ')', '\\', 'n']
Detokenized (016): ['sr##cin##de##x', '=', 'd', '.', 'keys', '(', ')', '.', 'index', '(', 'sr##cp##ath', '[', '0', ']', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "targetPath = self . __pathListing . pathAt ( event . line . p0 ) \n"
Original    (015): ['targetPath', '=', 'self', '.', '__pathListing', '.', 'pathAt', '(', 'event', '.', 'line', '.', 'p0', ')', '\\n']
Tokenized   (025): ['[CLS]', 'target', '##path', '=', 'self', '.', '_', '_', 'path', '##list', '##ing', '.', 'path', '##at', '(', 'event', '.', 'line', '.', 'p', '##0', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['target', '##path', '=', 'self', '.', '_', '_', 'path', '##list', '##ing', '.', 'path', '##at', '(', 'event', '.', 'line', '.', 'p', '##0', ')', '\\', 'n']
Detokenized (015): ['target##path', '=', 'self', '.', '__path##list##ing', '.', 'path##at', '(', 'event', '.', 'line', '.', 'p##0', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "item = items [ srcIndex ] \n"
Original    (007): ['item', '=', 'items', '[', 'srcIndex', ']', '\\n']
Tokenized   (013): ['[CLS]', 'item', '=', 'items', '[', 'sr', '##cin', '##de', '##x', ']', '\\', 'n', '[SEP]']
Filtered   (011): ['item', '=', 'items', '[', 'sr', '##cin', '##de', '##x', ']', '\\', 'n']
Detokenized (007): ['item', '=', 'items', '[', 'sr##cin##de##x', ']', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "selectedPreset = self . __pathListing . getSelectedPaths ( ) [ 0 ] [ 0 ] \n"
Original    (016): ['selectedPreset', '=', 'self', '.', '__pathListing', '.', 'getSelectedPaths', '(', ')', '[', '0', ']', '[', '0', ']', '\\n']
Tokenized   (029): ['[CLS]', 'selected', '##pres', '##et', '=', 'self', '.', '_', '_', 'path', '##list', '##ing', '.', 'gets', '##ele', '##cted', '##path', '##s', '(', ')', '[', '0', ']', '[', '0', ']', '\\', 'n', '[SEP]']
Filtered   (027): ['selected', '##pres', '##et', '=', 'self', '.', '_', '_', 'path', '##list', '##ing', '.', 'gets', '##ele', '##cted', '##path', '##s', '(', ')', '[', '0', ']', '[', '0', ']', '\\', 'n']
Detokenized (016): ['selected##pres##et', '=', 'self', '.', '__path##list##ing', '.', 'gets##ele##cted##path##s', '(', ')', '[', '0', ']', '[', '0', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "selectedIndex = [ p [ 0 ] for p in paths ] . index ( selectedPreset ) \n"
Original    (018): ['selectedIndex', '=', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'paths', ']', '.', 'index', '(', 'selectedPreset', ')', '\\n']
Tokenized   (025): ['[CLS]', 'selected', '##ind', '##ex', '=', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'paths', ']', '.', 'index', '(', 'selected', '##pres', '##et', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['selected', '##ind', '##ex', '=', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'paths', ']', '.', 'index', '(', 'selected', '##pres', '##et', ')', '\\', 'n']
Detokenized (018): ['selected##ind##ex', '=', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'paths', ']', '.', 'index', '(', 'selected##pres##et', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "preset = selectedPaths [ 0 ] [ 0 ] \n"
Original    (010): ['preset', '=', 'selectedPaths', '[', '0', ']', '[', '0', ']', '\\n']
Tokenized   (016): ['[CLS]', 'pre', '##set', '=', 'selected', '##path', '##s', '[', '0', ']', '[', '0', ']', '\\', 'n', '[SEP]']
Filtered   (014): ['pre', '##set', '=', 'selected', '##path', '##s', '[', '0', ']', '[', '0', ']', '\\', 'n']
Detokenized (010): ['pre##set', '=', 'selected##path##s', '[', '0', ']', '[', '0', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "scrolledContainer . setChild ( GafferUI . ListContainer ( spacing = 4 ) ) \n"
Original    (014): ['scrolledContainer', '.', 'setChild', '(', 'GafferUI', '.', 'ListContainer', '(', 'spacing', '=', '4', ')', ')', '\\n']
Tokenized   (028): ['[CLS]', 'scroll', '##ed', '##con', '##tain', '##er', '.', 'set', '##child', '(', 'ga', '##ffer', '##ui', '.', 'list', '##con', '##tain', '##er', '(', 'spa', '##cing', '=', '4', ')', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['scroll', '##ed', '##con', '##tain', '##er', '.', 'set', '##child', '(', 'ga', '##ffer', '##ui', '.', 'list', '##con', '##tain', '##er', '(', 'spa', '##cing', '=', '4', ')', ')', '\\', 'n']
Detokenized (014): ['scroll##ed##con##tain##er', '.', 'set##child', '(', 'ga##ffer##ui', '.', 'list##con##tain##er', '(', 'spa##cing', '=', '4', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __gadgetMenuDefinition ) ) \n"
Original    (016): ['menu', '=', 'GafferUI', '.', 'Menu', '(', 'Gaffer', '.', 'WeakMethod', '(', 'self', '.', '__gadgetMenuDefinition', ')', ')', '\\n']
Tokenized   (032): ['[CLS]', 'menu', '=', 'ga', '##ffer', '##ui', '.', 'menu', '(', 'ga', '##ffer', '.', 'weak', '##met', '##ho', '##d', '(', 'self', '.', '_', '_', 'ga', '##dget', '##men', '##ude', '##fin', '##ition', ')', ')', '\\', 'n', '[SEP]']
Filtered   (030): ['menu', '=', 'ga', '##ffer', '##ui', '.', 'menu', '(', 'ga', '##ffer', '.', 'weak', '##met', '##ho', '##d', '(', 'self', '.', '_', '_', 'ga', '##dget', '##men', '##ude', '##fin', '##ition', ')', ')', '\\', 'n']
Detokenized (016): ['menu', '=', 'ga##ffer##ui', '.', 'menu', '(', 'ga##ffer', '.', 'weak##met##ho##d', '(', 'self', '.', '__ga##dget##men##ude##fin##ition', ')', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : ""/" + g . label , \n"
Original    (007): ['"/"', '+', 'g', '.', 'label', ',', '\\n']
Tokenized   (012): ['[CLS]', '"', '/', '"', '+', 'g', '.', 'label', ',', '\\', 'n', '[SEP]']
Filtered   (010): ['"', '/', '"', '+', 'g', '.', 'label', ',', '\\', 'n']
Detokenized (007): ['"/"', '+', 'g', '.', 'label', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""command" : functools . partial ( Gaffer . WeakMethod ( self . __registerOrDeregisterMetadata ) , key = "checkBox" : metadata == g . metadata , \n"
Original    (026): ['"command"', ':', 'functools', '.', 'partial', '(', 'Gaffer', '.', 'WeakMethod', '(', 'self', '.', '__registerOrDeregisterMetadata', ')', ',', 'key', '=', '"checkBox"', ':', 'metadata', '==', 'g', '.', 'metadata', ',', '\\n']
Tokenized   (051): ['[CLS]', '"', 'command', '"', ':', 'fun', '##ct', '##ool', '##s', '.', 'partial', '(', 'ga', '##ffer', '.', 'weak', '##met', '##ho', '##d', '(', 'self', '.', '_', '_', 'register', '##ord', '##ere', '##gist', '##er', '##met', '##ada', '##ta', ')', ',', 'key', '=', '"', 'check', '##box', '"', ':', 'metadata', '=', '=', 'g', '.', 'metadata', ',', '\\', 'n', '[SEP]']
Filtered   (049): ['"', 'command', '"', ':', 'fun', '##ct', '##ool', '##s', '.', 'partial', '(', 'ga', '##ffer', '.', 'weak', '##met', '##ho', '##d', '(', 'self', '.', '_', '_', 'register', '##ord', '##ere', '##gist', '##er', '##met', '##ada', '##ta', ')', ',', 'key', '=', '"', 'check', '##box', '"', ':', 'metadata', '=', '=', 'g', '.', 'metadata', ',', '\\', 'n']
Detokenized (026): ['"command"', ':', 'fun##ct##ool##s', '.', 'partial', '(', 'ga##ffer', '.', 'weak##met##ho##d', '(', 'self', '.', '__register##ord##ere##gist##er##met##ada##ta', ')', ',', 'key', '=', '"check##box"', ':', 'metadata', '==', 'g', '.', 'metadata', ',', '\\n']
Counter: 49
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "__WidgetDefinition ( "None" , Gaffer . Plug , "" ) , \n"
Original    (012): ['__WidgetDefinition', '(', '"None"', ',', 'Gaffer', '.', 'Plug', ',', '""', ')', ',', '\\n']
Tokenized   (025): ['[CLS]', '_', '_', 'wi', '##dget', '##de', '##fin', '##ition', '(', '"', 'none', '"', ',', 'ga', '##ffer', '.', 'plug', ',', '"', '"', ')', ',', '\\', 'n', '[SEP]']
Filtered   (023): ['_', '_', 'wi', '##dget', '##de', '##fin', '##ition', '(', '"', 'none', '"', ',', 'ga', '##ffer', '.', 'plug', ',', '"', '"', ')', ',', '\\', 'n']
Detokenized (012): ['__wi##dget##de##fin##ition', '(', '"none"', ',', 'ga##ffer', '.', 'plug', ',', '""', ')', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "__MetadataDefinition = collections . namedtuple ( "MetadataDefinition" , ( "key" , "label" , "metadataWidgetType" __metadataDefinitions = ( \n"
Original    (018): ['__MetadataDefinition', '=', 'collections', '.', 'namedtuple', '(', '"MetadataDefinition"', ',', '(', '"key"', ',', '"label"', ',', '"metadataWidgetType"', '__metadataDefinitions', '=', '(', '\\n']
Tokenized   (048): ['[CLS]', '_', '_', 'metadata', '##de', '##fin', '##ition', '=', 'collections', '.', 'named', '##tu', '##ple', '(', '"', 'metadata', '##de', '##fin', '##ition', '"', ',', '(', '"', 'key', '"', ',', '"', 'label', '"', ',', '"', 'metadata', '##wi', '##dget', '##type', '"', '_', '_', 'metadata', '##de', '##fin', '##ition', '##s', '=', '(', '\\', 'n', '[SEP]']
Filtered   (046): ['_', '_', 'metadata', '##de', '##fin', '##ition', '=', 'collections', '.', 'named', '##tu', '##ple', '(', '"', 'metadata', '##de', '##fin', '##ition', '"', ',', '(', '"', 'key', '"', ',', '"', 'label', '"', ',', '"', 'metadata', '##wi', '##dget', '##type', '"', '_', '_', 'metadata', '##de', '##fin', '##ition', '##s', '=', '(', '\\', 'n']
Detokenized (018): ['__metadata##de##fin##ition', '=', 'collections', '.', 'named##tu##ple', '(', '"metadata##de##fin##ition"', ',', '(', '"key"', ',', '"label"', ',', '"metadata##wi##dget##type"', '__metadata##de##fin##ition##s', '=', '(', '\\n']
Counter: 46
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "newSectionPath [ - 1 ] = nameWidget . getText ( ) . replace ( "." , "" ) \n"
Original    (019): ['newSectionPath', '[', '-', '1', ']', '=', 'nameWidget', '.', 'getText', '(', ')', '.', 'replace', '(', '"."', ',', '""', ')', '\\n']
Tokenized   (030): ['[CLS]', 'news', '##ection', '##path', '[', '-', '1', ']', '=', 'name', '##wi', '##dget', '.', 'get', '##text', '(', ')', '.', 'replace', '(', '"', '.', '"', ',', '"', '"', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['news', '##ection', '##path', '[', '-', '1', ']', '=', 'name', '##wi', '##dget', '.', 'get', '##text', '(', ')', '.', 'replace', '(', '"', '.', '"', ',', '"', '"', ')', '\\', 'n']
Detokenized (019): ['news##ection##path', '[', '-', '1', ']', '=', 'name##wi##dget', '.', 'get##text', '(', ')', '.', 'replace', '(', '"."', ',', '""', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "_metadata ( self . getPlugParent ( ) , name ) \n"
Original    (011): ['_metadata', '(', 'self', '.', 'getPlugParent', '(', ')', ',', 'name', ')', '\\n']
Tokenized   (019): ['[CLS]', '_', 'metadata', '(', 'self', '.', 'get', '##pl', '##ug', '##par', '##ent', '(', ')', ',', 'name', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['_', 'metadata', '(', 'self', '.', 'get', '##pl', '##ug', '##par', '##ent', '(', ')', ',', 'name', ')', '\\', 'n']
Detokenized (011): ['_metadata', '(', 'self', '.', 'get##pl##ug##par##ent', '(', ')', ',', 'name', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "_deregisterMetadata ( self . getPlugParent ( ) , name ) \n"
Original    (011): ['_deregisterMetadata', '(', 'self', '.', 'getPlugParent', '(', ')', ',', 'name', ')', '\\n']
Tokenized   (024): ['[CLS]', '_', 'der', '##eg', '##ister', '##met', '##ada', '##ta', '(', 'self', '.', 'get', '##pl', '##ug', '##par', '##ent', '(', ')', ',', 'name', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['_', 'der', '##eg', '##ister', '##met', '##ada', '##ta', '(', 'self', '.', 'get', '##pl', '##ug', '##par', '##ent', '(', ')', ',', 'name', ')', '\\', 'n']
Detokenized (011): ['_der##eg##ister##met##ada##ta', '(', 'self', '.', 'get##pl##ug##par##ent', '(', ')', ',', 'name', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "wr2 = weakref . ref ( w . _qtWidget ( ) ) \n"
Original    (013): ['wr2', '=', 'weakref', '.', 'ref', '(', 'w', '.', '_qtWidget', '(', ')', ')', '\\n']
Tokenized   (023): ['[CLS]', 'wr', '##2', '=', 'weak', '##re', '##f', '.', 'ref', '(', 'w', '.', '_', 'q', '##t', '##wi', '##dget', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['wr', '##2', '=', 'weak', '##re', '##f', '.', 'ref', '(', 'w', '.', '_', 'q', '##t', '##wi', '##dget', '(', ')', ')', '\\', 'n']
Detokenized (013): ['wr##2', '=', 'weak##re##f', '.', 'ref', '(', 'w', '.', '_q##t##wi##dget', '(', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "WidgetTest . signalsEmitted = 0 \n"
Original    (006): ['WidgetTest', '.', 'signalsEmitted', '=', '0', '\\n']
Tokenized   (013): ['[CLS]', 'wi', '##dget', '##test', '.', 'signals', '##emi', '##tted', '=', '0', '\\', 'n', '[SEP]']
Filtered   (011): ['wi', '##dget', '##test', '.', 'signals', '##emi', '##tted', '=', '0', '\\', 'n']
Detokenized (006): ['wi##dget##test', '.', 'signals##emi##tted', '=', '0', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "QtGui . QApplication . instance ( ) . sendEvent ( w . _qtWidget ( ) , event ) \n"
Original    (019): ['QtGui', '.', 'QApplication', '.', 'instance', '(', ')', '.', 'sendEvent', '(', 'w', '.', '_qtWidget', '(', ')', ',', 'event', ')', '\\n']
Tokenized   (033): ['[CLS]', 'q', '##t', '##gui', '.', 'q', '##app', '##lica', '##tion', '.', 'instance', '(', ')', '.', 'send', '##eve', '##nt', '(', 'w', '.', '_', 'q', '##t', '##wi', '##dget', '(', ')', ',', 'event', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['q', '##t', '##gui', '.', 'q', '##app', '##lica', '##tion', '.', 'instance', '(', ')', '.', 'send', '##eve', '##nt', '(', 'w', '.', '_', 'q', '##t', '##wi', '##dget', '(', ')', ',', 'event', ')', '\\', 'n']
Detokenized (019): ['q##t##gui', '.', 'q##app##lica##tion', '.', 'instance', '(', ')', '.', 'send##eve##nt', '(', 'w', '.', '_q##t##wi##dget', '(', ')', ',', 'event', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "GafferUI . BoxUI . appendNodeEditorToolMenuDefinitions ( nodeEditor , node , menuDefinition ) \n"
Original    (013): ['GafferUI', '.', 'BoxUI', '.', 'appendNodeEditorToolMenuDefinitions', '(', 'nodeEditor', ',', 'node', ',', 'menuDefinition', ')', '\\n']
Tokenized   (035): ['[CLS]', 'ga', '##ffer', '##ui', '.', 'box', '##ui', '.', 'app', '##end', '##no', '##dee', '##dit', '##ort', '##ool', '##men', '##ude', '##fin', '##ition', '##s', '(', 'node', '##ed', '##itor', ',', 'node', ',', 'menu', '##de', '##fin', '##ition', ')', '\\', 'n', '[SEP]']
Filtered   (033): ['ga', '##ffer', '##ui', '.', 'box', '##ui', '.', 'app', '##end', '##no', '##dee', '##dit', '##ort', '##ool', '##men', '##ude', '##fin', '##ition', '##s', '(', 'node', '##ed', '##itor', ',', 'node', ',', 'menu', '##de', '##fin', '##ition', ')', '\\', 'n']
Detokenized (013): ['ga##ffer##ui', '.', 'box##ui', '.', 'app##end##no##dee##dit##ort##ool##men##ude##fin##ition##s', '(', 'node##ed##itor', ',', 'node', ',', 'menu##de##fin##ition', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "GafferSceneUI . FilteredSceneProcessorUI . appendNodeEditorToolMenuDefinitions ( nodeEditor , node , menuDefinition \n"
Original    (012): ['GafferSceneUI', '.', 'FilteredSceneProcessorUI', '.', 'appendNodeEditorToolMenuDefinitions', '(', 'nodeEditor', ',', 'node', ',', 'menuDefinition', '\\n']
Tokenized   (041): ['[CLS]', 'ga', '##ffer', '##sc', '##ene', '##ui', '.', 'filtered', '##sc', '##ene', '##pro', '##ces', '##sor', '##ui', '.', 'app', '##end', '##no', '##dee', '##dit', '##ort', '##ool', '##men', '##ude', '##fin', '##ition', '##s', '(', 'node', '##ed', '##itor', ',', 'node', ',', 'menu', '##de', '##fin', '##ition', '\\', 'n', '[SEP]']
Filtered   (039): ['ga', '##ffer', '##sc', '##ene', '##ui', '.', 'filtered', '##sc', '##ene', '##pro', '##ces', '##sor', '##ui', '.', 'app', '##end', '##no', '##dee', '##dit', '##ort', '##ool', '##men', '##ude', '##fin', '##ition', '##s', '(', 'node', '##ed', '##itor', ',', 'node', ',', 'menu', '##de', '##fin', '##ition', '\\', 'n']
Detokenized (012): ['ga##ffer##sc##ene##ui', '.', 'filtered##sc##ene##pro##ces##sor##ui', '.', 'app##end##no##dee##dit##ort##ool##men##ude##fin##ition##s', '(', 'node##ed##itor', ',', 'node', ',', 'menu##de##fin##ition', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "yappi . print_stats ( sort_type = yappi . SORTTYPE_TTOT , limit = 30 , thread_stats_on = False ) \n"
Original    (019): ['yappi', '.', 'print_stats', '(', 'sort_type', '=', 'yappi', '.', 'SORTTYPE_TTOT', ',', 'limit', '=', '30', ',', 'thread_stats_on', '=', 'False', ')', '\\n']
Tokenized   (038): ['[CLS]', 'ya', '##pp', '##i', '.', 'print', '_', 'stats', '(', 'sort', '_', 'type', '=', 'ya', '##pp', '##i', '.', 'sort', '##type', '_', 'tt', '##ot', ',', 'limit', '=', '30', ',', 'thread', '_', 'stats', '_', 'on', '=', 'false', ')', '\\', 'n', '[SEP]']
Filtered   (036): ['ya', '##pp', '##i', '.', 'print', '_', 'stats', '(', 'sort', '_', 'type', '=', 'ya', '##pp', '##i', '.', 'sort', '##type', '_', 'tt', '##ot', ',', 'limit', '=', '30', ',', 'thread', '_', 'stats', '_', 'on', '=', 'false', ')', '\\', 'n']
Detokenized (019): ['ya##pp##i', '.', 'print_stats', '(', 'sort_type', '=', 'ya##pp##i', '.', 'sort##type_tt##ot', ',', 'limit', '=', '30', ',', 'thread_stats_on', '=', 'false', ')', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "SAMPLE_EXTRACT_METRICS_PAGE = os . path . join ( datadir , "monthly_download" ) \n"
Original    (013): ['SAMPLE_EXTRACT_METRICS_PAGE', '=', 'os', '.', 'path', '.', 'join', '(', 'datadir', ',', '"monthly_download"', ')', '\\n']
Tokenized   (029): ['[CLS]', 'sample', '_', 'extract', '_', 'metric', '##s', '_', 'page', '=', 'os', '.', 'path', '.', 'join', '(', 'data', '##di', '##r', ',', '"', 'monthly', '_', 'download', '"', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['sample', '_', 'extract', '_', 'metric', '##s', '_', 'page', '=', 'os', '.', 'path', '.', 'join', '(', 'data', '##di', '##r', ',', '"', 'monthly', '_', 'download', '"', ')', '\\', 'n']
Detokenized (013): ['sample_extract_metric##s_page', '=', 'os', '.', 'path', '.', 'join', '(', 'data##di##r', ',', '"monthly_download"', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH = os . path . join ( datadir , "monthly_download_different_month" \n"
Original    (012): ['SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH', '=', 'os', '.', 'path', '.', 'join', '(', 'datadir', ',', '"monthly_download_different_month"', '\\n']
Tokenized   (036): ['[CLS]', 'sample', '_', 'extract', '_', 'metric', '##s', '_', 'page', '_', 'different', '_', 'month', '=', 'os', '.', 'path', '.', 'join', '(', 'data', '##di', '##r', ',', '"', 'monthly', '_', 'download', '_', 'different', '_', 'month', '"', '\\', 'n', '[SEP]']
Filtered   (034): ['sample', '_', 'extract', '_', 'metric', '##s', '_', 'page', '_', 'different', '_', 'month', '=', 'os', '.', 'path', '.', 'join', '(', 'data', '##di', '##r', ',', '"', 'monthly', '_', 'download', '_', 'different', '_', 'month', '"', '\\', 'n']
Detokenized (012): ['sample_extract_metric##s_page_different_month', '=', 'os', '.', 'path', '.', 'join', '(', 'data##di##r', ',', '"monthly_download_different_month"', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "testitem_aliases = ( "pmid" , TEST_PMID ) \n"
Original    (008): ['testitem_aliases', '=', '(', '"pmid"', ',', 'TEST_PMID', ')', '\\n']
Tokenized   (020): ['[CLS]', 'test', '##ite', '##m', '_', 'alias', '##es', '=', '(', '"', 'pmid', '"', ',', 'test', '_', 'pmid', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['test', '##ite', '##m', '_', 'alias', '##es', '=', '(', '"', 'pmid', '"', ',', 'test', '_', 'pmid', ')', '\\', 'n']
Detokenized (008): ['test##ite##m_alias##es', '=', '(', '"pmid"', ',', 'test_pmid', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "sample_data_dump = open ( SAMPLE_EXTRACT_METRICS_PAGE , "r" ) . read ( ) \n"
Original    (013): ['sample_data_dump', '=', 'open', '(', 'SAMPLE_EXTRACT_METRICS_PAGE', ',', '"r"', ')', '.', 'read', '(', ')', '\\n']
Tokenized   (029): ['[CLS]', 'sample', '_', 'data', '_', 'dump', '=', 'open', '(', 'sample', '_', 'extract', '_', 'metric', '##s', '_', 'page', ',', '"', 'r', '"', ')', '.', 'read', '(', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['sample', '_', 'data', '_', 'dump', '=', 'open', '(', 'sample', '_', 'extract', '_', 'metric', '##s', '_', 'page', ',', '"', 'r', '"', ')', '.', 'read', '(', ')', '\\', 'n']
Detokenized (013): ['sample_data_dump', '=', 'open', '(', 'sample_extract_metric##s_page', ',', '"r"', ')', '.', 'read', '(', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "sample_data_dump_different_month = open ( SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH , "r" ) . read \n"
Original    (011): ['sample_data_dump_different_month', '=', 'open', '(', 'SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH', ',', '"r"', ')', '.', 'read', '\\n']
Tokenized   (035): ['[CLS]', 'sample', '_', 'data', '_', 'dump', '_', 'different', '_', 'month', '=', 'open', '(', 'sample', '_', 'extract', '_', 'metric', '##s', '_', 'page', '_', 'different', '_', 'month', ',', '"', 'r', '"', ')', '.', 'read', '\\', 'n', '[SEP]']
Filtered   (033): ['sample', '_', 'data', '_', 'dump', '_', 'different', '_', 'month', '=', 'open', '(', 'sample', '_', 'extract', '_', 'metric', '##s', '_', 'page', '_', 'different', '_', 'month', ',', '"', 'r', '"', ')', '.', 'read', '\\', 'n']
Detokenized (011): ['sample_data_dump_different_month', '=', 'open', '(', 'sample_extract_metric##s_page_different_month', ',', '"r"', ')', '.', 'read', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""max_event_date" : "2012-01-31T07:34:01.126892" \n"
Original    (004): ['"max_event_date"', ':', '"2012-01-31T07:34:01.126892"', '\\n']
Tokenized   (031): ['[CLS]', '"', 'max', '_', 'event', '_', 'date', '"', ':', '"', '2012', '-', '01', '-', '31', '##t', '##0', '##7', ':', '34', ':', '01', '.', '126', '##8', '##9', '##2', '"', '\\', 'n', '[SEP]']
Filtered   (029): ['"', 'max', '_', 'event', '_', 'date', '"', ':', '"', '2012', '-', '01', '-', '31', '##t', '##0', '##7', ':', '34', ':', '01', '.', '126', '##8', '##9', '##2', '"', '\\', 'n']
Detokenized (004): ['"max_event_date"', ':', '"2012-01-31##t##0##7:34:01.126##8##9##2"', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : ""_id" : "abc123" , \n"
Original    (005): ['"_id"', ':', '"abc123"', ',', '\\n']
Tokenized   (015): ['[CLS]', '"', '_', 'id', '"', ':', '"', 'abc', '##12', '##3', '"', ',', '\\', 'n', '[SEP]']
Filtered   (013): ['"', '_', 'id', '"', ':', '"', 'abc', '##12', '##3', '"', ',', '\\', 'n']
Detokenized (005): ['"_id"', ':', '"abc##12##3"', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""raw" : "max_event_date" : "2012-10-31T07:34:01.126892" , \n"
Original    (007): ['"raw"', ':', '"max_event_date"', ':', '"2012-10-31T07:34:01.126892"', ',', '\\n']
Tokenized   (036): ['[CLS]', '"', 'raw', '"', ':', '"', 'max', '_', 'event', '_', 'date', '"', ':', '"', '2012', '-', '10', '-', '31', '##t', '##0', '##7', ':', '34', ':', '01', '.', '126', '##8', '##9', '##2', '"', ',', '\\', 'n', '[SEP]']
Filtered   (034): ['"', 'raw', '"', ':', '"', 'max', '_', 'event', '_', 'date', '"', ':', '"', '2012', '-', '10', '-', '31', '##t', '##0', '##7', ':', '34', ':', '01', '.', '126', '##8', '##9', '##2', '"', ',', '\\', 'n']
Detokenized (007): ['"raw"', ':', '"max_event_date"', ':', '"2012-10-31##t##0##7:34:01.126##8##9##2"', ',', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""23110252" \n"
Original    (002): ['"23110252"', '\\n']
Tokenized   (010): ['[CLS]', '"', '231', '##10', '##25', '##2', '"', '\\', 'n', '[SEP]']
Filtered   (008): ['"', '231', '##10', '##25', '##2', '"', '\\', 'n']
Detokenized (002): ['"231##10##25##2"', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 2, 768)
# Extracted words:  2
Sentence         : "cache_client = redis . from_url ( os . getenv ( "REDIS_URL" ) , REDIS_CACHE_DATABASE_NUMBER ) \n"
Original    (016): ['cache_client', '=', 'redis', '.', 'from_url', '(', 'os', '.', 'getenv', '(', '"REDIS_URL"', ')', ',', 'REDIS_CACHE_DATABASE_NUMBER', ')', '\\n']
Tokenized   (040): ['[CLS]', 'cache', '_', 'client', '=', 'red', '##is', '.', 'from', '_', 'ur', '##l', '(', 'os', '.', 'get', '##en', '##v', '(', '"', 'red', '##is', '_', 'ur', '##l', '"', ')', ',', 'red', '##is', '_', 'cache', '_', 'database', '_', 'number', ')', '\\', 'n', '[SEP]']
Filtered   (038): ['cache', '_', 'client', '=', 'red', '##is', '.', 'from', '_', 'ur', '##l', '(', 'os', '.', 'get', '##en', '##v', '(', '"', 'red', '##is', '_', 'ur', '##l', '"', ')', ',', 'red', '##is', '_', 'cache', '_', 'database', '_', 'number', ')', '\\', 'n']
Detokenized (016): ['cache_client', '=', 'red##is', '.', 'from_ur##l', '(', 'os', '.', 'get##en##v', '(', '"red##is_ur##l"', ')', ',', 'red##is_cache_database_number', ')', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "MAX_CACHE_SIZE_BYTES = 100 * 1000 * 1000 #100mb \n"
Original    (009): ['MAX_CACHE_SIZE_BYTES', '=', '100', '*', '1000', '*', '1000', '#100mb', '\\n']
Tokenized   (020): ['[CLS]', 'max', '_', 'cache', '_', 'size', '_', 'bytes', '=', '100', '*', '1000', '*', '1000', '#', '100', '##mb', '\\', 'n', '[SEP]']
Filtered   (018): ['max', '_', 'cache', '_', 'size', '_', 'bytes', '=', '100', '*', '1000', '*', '1000', '#', '100', '##mb', '\\', 'n']
Detokenized (009): ['max_cache_size_bytes', '=', '100', '*', '1000', '*', '1000', '#100##mb', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "set_response = mc . set ( hash_key , json . dumps ( data ) ) \n"
Original    (016): ['set_response', '=', 'mc', '.', 'set', '(', 'hash_key', ',', 'json', '.', 'dumps', '(', 'data', ')', ')', '\\n']
Tokenized   (025): ['[CLS]', 'set', '_', 'response', '=', 'mc', '.', 'set', '(', 'hash', '_', 'key', ',', 'j', '##son', '.', 'dump', '##s', '(', 'data', ')', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['set', '_', 'response', '=', 'mc', '.', 'set', '(', 'hash', '_', 'key', ',', 'j', '##son', '.', 'dump', '##s', '(', 'data', ')', ')', '\\', 'n']
Detokenized (016): ['set_response', '=', 'mc', '.', 'set', '(', 'hash_key', ',', 'j##son', '.', 'dump##s', '(', 'data', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "metrics_url_template = "http://alm.plos.org/api/v3/articles?ids=%s&source=citations,counter&api_key=" provenance_url_template = "http://dx.doi.org/%s" \n"
Original    (007): ['metrics_url_template', '=', '"http://alm.plos.org/api/v3/articles?ids=%s&source=citations,counter&api_key="', 'provenance_url_template', '=', '"http://dx.doi.org/%s"', '\\n']
Tokenized   (072): ['[CLS]', 'metric', '##s', '_', 'ur', '##l', '_', 'template', '=', '"', 'http', ':', '/', '/', 'al', '##m', '.', 'pl', '##os', '.', 'org', '/', 'api', '/', 'v', '##3', '/', 'articles', '?', 'id', '##s', '=', '%', 's', '&', 'source', '=', 'citations', ',', 'counter', '&', 'api', '_', 'key', '=', '"', 'proven', '##ance', '_', 'ur', '##l', '_', 'template', '=', '"', 'http', ':', '/', '/', 'd', '##x', '.', 'doi', '.', 'org', '/', '%', 's', '"', '\\', 'n', '[SEP]']
Filtered   (070): ['metric', '##s', '_', 'ur', '##l', '_', 'template', '=', '"', 'http', ':', '/', '/', 'al', '##m', '.', 'pl', '##os', '.', 'org', '/', 'api', '/', 'v', '##3', '/', 'articles', '?', 'id', '##s', '=', '%', 's', '&', 'source', '=', 'citations', ',', 'counter', '&', 'api', '_', 'key', '=', '"', 'proven', '##ance', '_', 'ur', '##l', '_', 'template', '=', '"', 'http', ':', '/', '/', 'd', '##x', '.', 'doi', '.', 'org', '/', '%', 's', '"', '\\', 'n']
Detokenized (007): ['metric##s_ur##l_template', '=', '"http://al##m.pl##os.org/api/v##3/articles?id##s=%s&source=citations,counter&api_key="', 'proven##ance_ur##l_template', '=', '"http://d##x.doi.org/%s"', '\\n']
Counter: 70
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "relevant = ( ( "doi" == namespace ) and ( "10.1371/" in nid ) ) \n"
Original    (016): ['relevant', '=', '(', '(', '"doi"', '==', 'namespace', ')', 'and', '(', '"10.1371/"', 'in', 'nid', ')', ')', '\\n']
Tokenized   (030): ['[CLS]', 'relevant', '=', '(', '(', '"', 'doi', '"', '=', '=', 'names', '##pace', ')', 'and', '(', '"', '10', '.', '137', '##1', '/', '"', 'in', 'ni', '##d', ')', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['relevant', '=', '(', '(', '"', 'doi', '"', '=', '=', 'names', '##pace', ')', 'and', '(', '"', '10', '.', '137', '##1', '/', '"', 'in', 'ni', '##d', ')', ')', '\\', 'n']
Detokenized (016): ['relevant', '=', '(', '(', '"doi"', '==', 'names##pace', ')', 'and', '(', '"10.137##1/"', 'in', 'ni##d', ')', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "this_article = json_response [ 0 ] [ "sources" ] [ 0 ] [ "metrics" ] \n"
Original    (016): ['this_article', '=', 'json_response', '[', '0', ']', '[', '"sources"', ']', '[', '0', ']', '[', '"metrics"', ']', '\\n']
Tokenized   (029): ['[CLS]', 'this', '_', 'article', '=', 'j', '##son', '_', 'response', '[', '0', ']', '[', '"', 'sources', '"', ']', '[', '0', ']', '[', '"', 'metric', '##s', '"', ']', '\\', 'n', '[SEP]']
Filtered   (027): ['this', '_', 'article', '=', 'j', '##son', '_', 'response', '[', '0', ']', '[', '"', 'sources', '"', ']', '[', '0', ']', '[', '"', 'metric', '##s', '"', ']', '\\', 'n']
Detokenized (016): ['this_article', '=', 'j##son_response', '[', '0', ']', '[', '"sources"', ']', '[', '0', ']', '[', '"metric##s"', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "redis_url = os . environ . get ( , "redis://127.0.0.1:6379/" ) \n"
Original    (012): ['redis_url', '=', 'os', '.', 'environ', '.', 'get', '(', ',', '"redis://127.0.0.1:6379/"', ')', '\\n']
Tokenized   (039): ['[CLS]', 'red', '##is', '_', 'ur', '##l', '=', 'os', '.', 'en', '##vir', '##on', '.', 'get', '(', ',', '"', 'red', '##is', ':', '/', '/', '127', '.', '0', '.', '0', '.', '1', ':', '63', '##7', '##9', '/', '"', ')', '\\', 'n', '[SEP]']
Filtered   (037): ['red', '##is', '_', 'ur', '##l', '=', 'os', '.', 'en', '##vir', '##on', '.', 'get', '(', ',', '"', 'red', '##is', ':', '/', '/', '127', '.', '0', '.', '0', '.', '1', ':', '63', '##7', '##9', '/', '"', ')', '\\', 'n']
Detokenized (012): ['red##is_ur##l', '=', 'os', '.', 'en##vir##on', '.', 'get', '(', ',', '"red##is://127.0.0.1:63##7##9/"', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "Queue ( , routing_key = ) \n"
Original    (007): ['Queue', '(', ',', 'routing_key', '=', ')', '\\n']
Tokenized   (012): ['[CLS]', 'queue', '(', ',', 'routing', '_', 'key', '=', ')', '\\', 'n', '[SEP]']
Filtered   (010): ['queue', '(', ',', 'routing', '_', 'key', '=', ')', '\\', 'n']
Detokenized (007): ['queue', '(', ',', 'routing_key', '=', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "CELERY_ACCEPT_CONTENT = [ , ] \n"
Original    (006): ['CELERY_ACCEPT_CONTENT', '=', '[', ',', ']', '\\n']
Tokenized   (015): ['[CLS]', 'ce', '##ler', '##y', '_', 'accept', '_', 'content', '=', '[', ',', ']', '\\', 'n', '[SEP]']
Filtered   (013): ['ce', '##ler', '##y', '_', 'accept', '_', 'content', '=', '[', ',', ']', '\\', 'n']
Detokenized (006): ['ce##ler##y_accept_content', '=', '[', ',', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "CELERY_IMPORTS = ( "core_tasks" , ) \n"
Original    (007): ['CELERY_IMPORTS', '=', '(', '"core_tasks"', ',', ')', '\\n']
Tokenized   (018): ['[CLS]', 'ce', '##ler', '##y', '_', 'imports', '=', '(', '"', 'core', '_', 'tasks', '"', ',', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['ce', '##ler', '##y', '_', 'imports', '=', '(', '"', 'core', '_', 'tasks', '"', ',', ')', '\\', 'n']
Detokenized (007): ['ce##ler##y_imports', '=', '(', '"core_tasks"', ',', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "sampledir = os . path . join ( os . path . split ( __file__ ) [ 0 ] , "../../../extras/sample_provider_pages/" ) \n"
Original    (023): ['sampledir', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'split', '(', '__file__', ')', '[', '0', ']', ',', '"../../../extras/sample_provider_pages/"', ')', '\\n']
Tokenized   (049): ['[CLS]', 'sampled', '##ir', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'split', '(', '_', '_', 'file', '_', '_', ')', '[', '0', ']', ',', '"', '.', '.', '/', '.', '.', '/', '.', '.', '/', 'extras', '/', 'sample', '_', 'provider', '_', 'pages', '/', '"', ')', '\\', 'n', '[SEP]']
Filtered   (047): ['sampled', '##ir', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'split', '(', '_', '_', 'file', '_', '_', ')', '[', '0', ']', ',', '"', '.', '.', '/', '.', '.', '/', '.', '.', '/', 'extras', '/', 'sample', '_', 'provider', '_', 'pages', '/', '"', ')', '\\', 'n']
Detokenized (023): ['sampled##ir', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'split', '(', '__file__', ')', '[', '0', ']', ',', '"../../../extras/sample_provider_pages/"', ')', '\\n']
Counter: 47
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "TEST_XML = open ( os . path . join ( sampledir , "facebook" , "metrics" ) ) . read ( ) \n"
Original    (022): ['TEST_XML', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'sampledir', ',', '"facebook"', ',', '"metrics"', ')', ')', '.', 'read', '(', ')', '\\n']
Tokenized   (033): ['[CLS]', 'test', '_', 'xml', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'sampled', '##ir', ',', '"', 'facebook', '"', ',', '"', 'metric', '##s', '"', ')', ')', '.', 'read', '(', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['test', '_', 'xml', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'sampled', '##ir', ',', '"', 'facebook', '"', ',', '"', 'metric', '##s', '"', ')', ')', '.', 'read', '(', ')', '\\', 'n']
Detokenized (022): ['test_xml', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'sampled##ir', ',', '"facebook"', ',', '"metric##s"', ')', ')', '.', 'read', '(', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "provider_names = [ provider . __class__ . __name__ for provider in providers ] \n"
Original    (014): ['provider_names', '=', '[', 'provider', '.', '__class__', '.', '__name__', 'for', 'provider', 'in', 'providers', ']', '\\n']
Tokenized   (027): ['[CLS]', 'provider', '_', 'names', '=', '[', 'provider', '.', '_', '_', 'class', '_', '_', '.', '_', '_', 'name', '_', '_', 'for', 'provider', 'in', 'providers', ']', '\\', 'n', '[SEP]']
Filtered   (025): ['provider', '_', 'names', '=', '[', 'provider', '.', '_', '_', 'class', '_', '_', '.', '_', '_', 'name', '_', '_', 'for', 'provider', 'in', 'providers', ']', '\\', 'n']
Detokenized (014): ['provider_names', '=', '[', 'provider', '.', '__class__', '.', '__name__', 'for', 'provider', 'in', 'providers', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "assert_equals ( md [ "pubmed" ] [ ] , ) \n"
Original    (011): ['assert_equals', '(', 'md', '[', '"pubmed"', ']', '[', ']', ',', ')', '\\n']
Tokenized   (019): ['[CLS]', 'assert', '_', 'equals', '(', 'md', '[', '"', 'pub', '##med', '"', ']', '[', ']', ',', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['assert', '_', 'equals', '(', 'md', '[', '"', 'pub', '##med', '"', ']', '[', ']', ',', ')', '\\', 'n']
Detokenized (011): ['assert_equals', '(', 'md', '[', '"pub##med"', ']', '[', ']', ',', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "tiid = db . Column ( db . Text , db . ForeignKey ( ) , primary_key = True ) \n"
Original    (021): ['tiid', '=', 'db', '.', 'Column', '(', 'db', '.', 'Text', ',', 'db', '.', 'ForeignKey', '(', ')', ',', 'primary_key', '=', 'True', ')', '\\n']
Tokenized   (028): ['[CLS]', 'ti', '##id', '=', 'db', '.', 'column', '(', 'db', '.', 'text', ',', 'db', '.', 'foreign', '##key', '(', ')', ',', 'primary', '_', 'key', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['ti', '##id', '=', 'db', '.', 'column', '(', 'db', '.', 'text', ',', 'db', '.', 'foreign', '##key', '(', ')', ',', 'primary', '_', 'key', '=', 'true', ')', '\\', 'n']
Detokenized (021): ['ti##id', '=', 'db', '.', 'column', '(', 'db', '.', 'text', ',', 'db', '.', 'foreign##key', '(', ')', ',', 'primary_key', '=', 'true', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "tweet_ids_with_response = [ tweet [ "id_str" ] for tweet in data ] \n"
Original    (013): ['tweet_ids_with_response', '=', '[', 'tweet', '[', '"id_str"', ']', 'for', 'tweet', 'in', 'data', ']', '\\n']
Tokenized   (034): ['[CLS]', 't', '##wee', '##t', '_', 'id', '##s', '_', 'with', '_', 'response', '=', '[', 't', '##wee', '##t', '[', '"', 'id', '_', 'st', '##r', '"', ']', 'for', 't', '##wee', '##t', 'in', 'data', ']', '\\', 'n', '[SEP]']
Filtered   (032): ['t', '##wee', '##t', '_', 'id', '##s', '_', 'with', '_', 'response', '=', '[', 't', '##wee', '##t', '[', '"', 'id', '_', 'st', '##r', '"', ']', 'for', 't', '##wee', '##t', 'in', 'data', ']', '\\', 'n']
Detokenized (013): ['t##wee##t_id##s_with_response', '=', '[', 't##wee##t', '[', '"id_st##r"', ']', 'for', 't##wee##t', 'in', 'data', ']', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "tweet_ids_without_response = [ tweet for tweet in tweet_ids if tweet not in tweet_ids_with_response flag_deleted_tweets ( tweet_ids_without_response ) \n"
Original    (018): ['tweet_ids_without_response', '=', '[', 'tweet', 'for', 'tweet', 'in', 'tweet_ids', 'if', 'tweet', 'not', 'in', 'tweet_ids_with_response', 'flag_deleted_tweets', '(', 'tweet_ids_without_response', ')', '\\n']
Tokenized   (065): ['[CLS]', 't', '##wee', '##t', '_', 'id', '##s', '_', 'without', '_', 'response', '=', '[', 't', '##wee', '##t', 'for', 't', '##wee', '##t', 'in', 't', '##wee', '##t', '_', 'id', '##s', 'if', 't', '##wee', '##t', 'not', 'in', 't', '##wee', '##t', '_', 'id', '##s', '_', 'with', '_', 'response', 'flag', '_', 'deleted', '_', 't', '##wee', '##ts', '(', 't', '##wee', '##t', '_', 'id', '##s', '_', 'without', '_', 'response', ')', '\\', 'n', '[SEP]']
Filtered   (063): ['t', '##wee', '##t', '_', 'id', '##s', '_', 'without', '_', 'response', '=', '[', 't', '##wee', '##t', 'for', 't', '##wee', '##t', 'in', 't', '##wee', '##t', '_', 'id', '##s', 'if', 't', '##wee', '##t', 'not', 'in', 't', '##wee', '##t', '_', 'id', '##s', '_', 'with', '_', 'response', 'flag', '_', 'deleted', '_', 't', '##wee', '##ts', '(', 't', '##wee', '##t', '_', 'id', '##s', '_', 'without', '_', 'response', ')', '\\', 'n']
Detokenized (018): ['t##wee##t_id##s_without_response', '=', '[', 't##wee##t', 'for', 't##wee##t', 'in', 't##wee##t_id##s', 'if', 't##wee##t', 'not', 'in', 't##wee##t_id##s_with_response', 'flag_deleted_t##wee##ts', '(', 't##wee##t_id##s_without_response', ')', '\\n']
Counter: 63
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "access_token = os . getenv ( "TWITTER_ACCESS_TOKEN" ) \n"
Original    (009): ['access_token', '=', 'os', '.', 'getenv', '(', '"TWITTER_ACCESS_TOKEN"', ')', '\\n']
Tokenized   (022): ['[CLS]', 'access', '_', 'token', '=', 'os', '.', 'get', '##en', '##v', '(', '"', 'twitter', '_', 'access', '_', 'token', '"', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['access', '_', 'token', '=', 'os', '.', 'get', '##en', '##v', '(', '"', 'twitter', '_', 'access', '_', 'token', '"', ')', '\\', 'n']
Detokenized (009): ['access_token', '=', 'os', '.', 'get##en##v', '(', '"twitter_access_token"', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "num = len ( tweets ) ) ) \n"
Original    (009): ['num', '=', 'len', '(', 'tweets', ')', ')', ')', '\\n']
Tokenized   (015): ['[CLS]', 'nu', '##m', '=', 'len', '(', 't', '##wee', '##ts', ')', ')', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['nu', '##m', '=', 'len', '(', 't', '##wee', '##ts', ')', ')', ')', '\\', 'n']
Detokenized (009): ['nu##m', '=', 'len', '(', 't##wee##ts', ')', ')', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "list_of_groups = [ tweets [ i : i + group_size ] for i in range ( 0 , len ( tweets ) , group_size ) ] \n"
Original    (027): ['list_of_groups', '=', '[', 'tweets', '[', 'i', ':', 'i', '+', 'group_size', ']', 'for', 'i', 'in', 'range', '(', '0', ',', 'len', '(', 'tweets', ')', ',', 'group_size', ')', ']', '\\n']
Tokenized   (042): ['[CLS]', 'list', '_', 'of', '_', 'groups', '=', '[', 't', '##wee', '##ts', '[', 'i', ':', 'i', '+', 'group', '_', 'size', ']', 'for', 'i', 'in', 'range', '(', '0', ',', 'len', '(', 't', '##wee', '##ts', ')', ',', 'group', '_', 'size', ')', ']', '\\', 'n', '[SEP]']
Filtered   (040): ['list', '_', 'of', '_', 'groups', '=', '[', 't', '##wee', '##ts', '[', 'i', ':', 'i', '+', 'group', '_', 'size', ']', 'for', 'i', 'in', 'range', '(', '0', ',', 'len', '(', 't', '##wee', '##ts', ')', ',', 'group', '_', 'size', ')', ']', '\\', 'n']
Detokenized (027): ['list_of_groups', '=', '[', 't##wee##ts', '[', 'i', ':', 'i', '+', 'group_size', ']', 'for', 'i', 'in', 'range', '(', '0', ',', 'len', '(', 't##wee##ts', ')', ',', 'group_size', ')', ']', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "handle_all_tweets ( response . data , tweet_subset ) \n"
Original    (009): ['handle_all_tweets', '(', 'response', '.', 'data', ',', 'tweet_subset', ')', '\\n']
Tokenized   (022): ['[CLS]', 'handle', '_', 'all', '_', 't', '##wee', '##ts', '(', 'response', '.', 'data', ',', 't', '##wee', '##t', '_', 'subset', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['handle', '_', 'all', '_', 't', '##wee', '##ts', '(', 'response', '.', 'data', ',', 't', '##wee', '##t', '_', 'subset', ')', '\\', 'n']
Detokenized (009): ['handle_all_t##wee##ts', '(', 'response', '.', 'data', ',', 't##wee##t_subset', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "tweets = Tweet . query . filter ( Tweet . profile_id == profile_id ) \n"
Original    (015): ['tweets', '=', 'Tweet', '.', 'query', '.', 'filter', '(', 'Tweet', '.', 'profile_id', '==', 'profile_id', ')', '\\n']
Tokenized   (029): ['[CLS]', 't', '##wee', '##ts', '=', 't', '##wee', '##t', '.', 'query', '.', 'filter', '(', 't', '##wee', '##t', '.', 'profile', '_', 'id', '=', '=', 'profile', '_', 'id', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['t', '##wee', '##ts', '=', 't', '##wee', '##t', '.', 'query', '.', 'filter', '(', 't', '##wee', '##t', '.', 'profile', '_', 'id', '=', '=', 'profile', '_', 'id', ')', '\\', 'n']
Detokenized (015): ['t##wee##ts', '=', 't##wee##t', '.', 'query', '.', 'filter', '(', 't##wee##t', '.', 'profile_id', '==', 'profile_id', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "tweet_dict = dict ( [ ( ( tweet . tweet_id , tweet . tiid ) , tweet ) for tweet in tweets ] ) \n"
Original    (025): ['tweet_dict', '=', 'dict', '(', '[', '(', '(', 'tweet', '.', 'tweet_id', ',', 'tweet', '.', 'tiid', ')', ',', 'tweet', ')', 'for', 'tweet', 'in', 'tweets', ']', ')', '\\n']
Tokenized   (049): ['[CLS]', 't', '##wee', '##t', '_', 'di', '##ct', '=', 'di', '##ct', '(', '[', '(', '(', 't', '##wee', '##t', '.', 't', '##wee', '##t', '_', 'id', ',', 't', '##wee', '##t', '.', 'ti', '##id', ')', ',', 't', '##wee', '##t', ')', 'for', 't', '##wee', '##t', 'in', 't', '##wee', '##ts', ']', ')', '\\', 'n', '[SEP]']
Filtered   (047): ['t', '##wee', '##t', '_', 'di', '##ct', '=', 'di', '##ct', '(', '[', '(', '(', 't', '##wee', '##t', '.', 't', '##wee', '##t', '_', 'id', ',', 't', '##wee', '##t', '.', 'ti', '##id', ')', ',', 't', '##wee', '##t', ')', 'for', 't', '##wee', '##t', 'in', 't', '##wee', '##ts', ']', ')', '\\', 'n']
Detokenized (025): ['t##wee##t_di##ct', '=', 'di##ct', '(', '[', '(', '(', 't##wee##t', '.', 't##wee##t_id', ',', 't##wee##t', '.', 'ti##id', ')', ',', 't##wee##t', ')', 'for', 't##wee##t', 'in', 't##wee##ts', ']', ')', '\\n']
Counter: 47
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "tweet . profile_id = profile_id \n"
Original    (006): ['tweet', '.', 'profile_id', '=', 'profile_id', '\\n']
Tokenized   (015): ['[CLS]', 't', '##wee', '##t', '.', 'profile', '_', 'id', '=', 'profile', '_', 'id', '\\', 'n', '[SEP]']
Filtered   (013): ['t', '##wee', '##t', '.', 'profile', '_', 'id', '=', 'profile', '_', 'id', '\\', 'n']
Detokenized (006): ['t##wee##t', '.', 'profile_id', '=', 'profile_id', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "tweet_ids = [ tweet . tweet_id for tweet in tweets_to_hydrate_from_twitter ] \n"
Original    (012): ['tweet_ids', '=', '[', 'tweet', '.', 'tweet_id', 'for', 'tweet', 'in', 'tweets_to_hydrate_from_twitter', ']', '\\n']
Tokenized   (039): ['[CLS]', 't', '##wee', '##t', '_', 'id', '##s', '=', '[', 't', '##wee', '##t', '.', 't', '##wee', '##t', '_', 'id', 'for', 't', '##wee', '##t', 'in', 't', '##wee', '##ts', '_', 'to', '_', 'hydra', '##te', '_', 'from', '_', 'twitter', ']', '\\', 'n', '[SEP]']
Filtered   (037): ['t', '##wee', '##t', '_', 'id', '##s', '=', '[', 't', '##wee', '##t', '.', 't', '##wee', '##t', '_', 'id', 'for', 't', '##wee', '##t', 'in', 't', '##wee', '##ts', '_', 'to', '_', 'hydra', '##te', '_', 'from', '_', 'twitter', ']', '\\', 'n']
Detokenized (012): ['t##wee##t_id##s', '=', '[', 't##wee##t', '.', 't##wee##t_id', 'for', 't##wee##t', 'in', 't##wee##ts_to_hydra##te_from_twitter', ']', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "display_url = url_info [ "display_url" ] \n"
Original    (007): ['display_url', '=', 'url_info', '[', '"display_url"', ']', '\\n']
Tokenized   (021): ['[CLS]', 'display', '_', 'ur', '##l', '=', 'ur', '##l', '_', 'info', '[', '"', 'display', '_', 'ur', '##l', '"', ']', '\\', 'n', '[SEP]']
Filtered   (019): ['display', '_', 'ur', '##l', '=', 'ur', '##l', '_', 'info', '[', '"', 'display', '_', 'ur', '##l', '"', ']', '\\', 'n']
Detokenized (007): ['display_ur##l', '=', 'ur##l_info', '[', '"display_ur##l"', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "tweet_id = self . tweet_id , \n"
Original    (007): ['tweet_id', '=', 'self', '.', 'tweet_id', ',', '\\n']
Tokenized   (018): ['[CLS]', 't', '##wee', '##t', '_', 'id', '=', 'self', '.', 't', '##wee', '##t', '_', 'id', ',', '\\', 'n', '[SEP]']
Filtered   (016): ['t', '##wee', '##t', '_', 'id', '=', 'self', '.', 't', '##wee', '##t', '_', 'id', ',', '\\', 'n']
Detokenized (007): ['t##wee##t_id', '=', 'self', '.', 't##wee##t_id', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "file_loc = os . path . dirname ( os . path . realpath ( __file__ ) ) \n"
Original    (018): ['file_loc', '=', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'realpath', '(', '__file__', ')', ')', '\\n']
Tokenized   (030): ['[CLS]', 'file', '_', 'lo', '##c', '=', 'os', '.', 'path', '.', 'dir', '##name', '(', 'os', '.', 'path', '.', 'real', '##path', '(', '_', '_', 'file', '_', '_', ')', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['file', '_', 'lo', '##c', '=', 'os', '.', 'path', '.', 'dir', '##name', '(', 'os', '.', 'path', '.', 'real', '##path', '(', '_', '_', 'file', '_', '_', ')', ')', '\\', 'n']
Detokenized (018): ['file_lo##c', '=', 'os', '.', 'path', '.', 'dir##name', '(', 'os', '.', 'path', '.', 'real##path', '(', '__file__', ')', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "urllib . urlretrieve ( url + fname , fname ) \n"
Original    (011): ['urllib', '.', 'urlretrieve', '(', 'url', '+', 'fname', ',', 'fname', ')', '\\n']
Tokenized   (023): ['[CLS]', 'ur', '##lli', '##b', '.', 'ur', '##lr', '##et', '##rie', '##ve', '(', 'ur', '##l', '+', 'f', '##name', ',', 'f', '##name', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['ur', '##lli', '##b', '.', 'ur', '##lr', '##et', '##rie', '##ve', '(', 'ur', '##l', '+', 'f', '##name', ',', 'f', '##name', ')', '\\', 'n']
Detokenized (011): ['ur##lli##b', '.', 'ur##lr##et##rie##ve', '(', 'ur##l', '+', 'f##name', ',', 'f##name', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "loaded = np . fromstring ( fd . read ( ) , dtype = np . uint8 ) \n"
Original    (019): ['loaded', '=', 'np', '.', 'fromstring', '(', 'fd', '.', 'read', '(', ')', ',', 'dtype', '=', 'np', '.', 'uint8', ')', '\\n']
Tokenized   (028): ['[CLS]', 'loaded', '=', 'np', '.', 'from', '##st', '##ring', '(', 'f', '##d', '.', 'read', '(', ')', ',', 'dt', '##ype', '=', 'np', '.', 'ui', '##nt', '##8', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['loaded', '=', 'np', '.', 'from', '##st', '##ring', '(', 'f', '##d', '.', 'read', '(', ')', ',', 'dt', '##ype', '=', 'np', '.', 'ui', '##nt', '##8', ')', '\\', 'n']
Detokenized (019): ['loaded', '=', 'np', '.', 'from##st##ring', '(', 'f##d', '.', 'read', '(', ')', ',', 'dt##ype', '=', 'np', '.', 'ui##nt##8', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "fd = gzip . open ( os . path . join ( data_dir , ) ) \n"
Original    (017): ['fd', '=', 'gzip', '.', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'data_dir', ',', ')', ')', '\\n']
Tokenized   (025): ['[CLS]', 'f', '##d', '=', 'g', '##zi', '##p', '.', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'data', '_', 'dir', ',', ')', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['f', '##d', '=', 'g', '##zi', '##p', '.', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'data', '_', 'dir', ',', ')', ')', '\\', 'n']
Detokenized (017): ['f##d', '=', 'g##zi##p', '.', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'data_dir', ',', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "trY = loaded [ 8 : ] . reshape ( ( 60000 ) ) \n"
Original    (015): ['trY', '=', 'loaded', '[', '8', ':', ']', '.', 'reshape', '(', '(', '60000', ')', ')', '\\n']
Tokenized   (021): ['[CLS]', 'try', '=', 'loaded', '[', '8', ':', ']', '.', 'res', '##ha', '##pe', '(', '(', '6000', '##0', ')', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['try', '=', 'loaded', '[', '8', ':', ']', '.', 'res', '##ha', '##pe', '(', '(', '6000', '##0', ')', ')', '\\', 'n']
Detokenized (015): ['try', '=', 'loaded', '[', '8', ':', ']', '.', 'res##ha##pe', '(', '(', '6000##0', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "trX = trX . reshape ( - 1 , 28 , 28 ) \n"
Original    (014): ['trX', '=', 'trX', '.', 'reshape', '(', '-', '1', ',', '28', ',', '28', ')', '\\n']
Tokenized   (021): ['[CLS]', 'tr', '##x', '=', 'tr', '##x', '.', 'res', '##ha', '##pe', '(', '-', '1', ',', '28', ',', '28', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['tr', '##x', '=', 'tr', '##x', '.', 'res', '##ha', '##pe', '(', '-', '1', ',', '28', ',', '28', ')', '\\', 'n']
Detokenized (014): ['tr##x', '=', 'tr##x', '.', 'res##ha##pe', '(', '-', '1', ',', '28', ',', '28', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "dirpath = os . path . join ( self . repo . path , "unused_directory" ) \n"
Original    (017): ['dirpath', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"unused_directory"', ')', '\\n']
Tokenized   (026): ['[CLS]', 'dir', '##path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'rep', '##o', '.', 'path', ',', '"', 'unused', '_', 'directory', '"', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['dir', '##path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'rep', '##o', '.', 'path', ',', '"', 'unused', '_', 'directory', '"', ')', '\\', 'n']
Detokenized (017): ['dir##path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'rep##o', '.', 'path', ',', '"unused_directory"', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "subpath = os . path . join ( self . repo . path , "a" , "b" , "c" ) \n"
Original    (021): ['subpath', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"a"', ',', '"b"', ',', '"c"', ')', '\\n']
Tokenized   (032): ['[CLS]', 'sub', '##path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'rep', '##o', '.', 'path', ',', '"', 'a', '"', ',', '"', 'b', '"', ',', '"', 'c', '"', ')', '\\', 'n', '[SEP]']
Filtered   (030): ['sub', '##path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'rep', '##o', '.', 'path', ',', '"', 'a', '"', ',', '"', 'b', '"', ',', '"', 'c', '"', ')', '\\', 'n']
Detokenized (021): ['sub##path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'rep##o', '.', 'path', ',', '"a"', ',', '"b"', ',', '"c"', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "handle = self . profile . username , \n"
Original    (009): ['handle', '=', 'self', '.', 'profile', '.', 'username', ',', '\\n']
Tokenized   (013): ['[CLS]', 'handle', '=', 'self', '.', 'profile', '.', 'user', '##name', ',', '\\', 'n', '[SEP]']
Filtered   (011): ['handle', '=', 'self', '.', 'profile', '.', 'user', '##name', ',', '\\', 'n']
Detokenized (009): ['handle', '=', 'self', '.', 'profile', '.', 'user##name', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "id_key = ) \n"
Original    (004): ['id_key', '=', ')', '\\n']
Tokenized   (009): ['[CLS]', 'id', '_', 'key', '=', ')', '\\', 'n', '[SEP]']
Filtered   (007): ['id', '_', 'key', '=', ')', '\\', 'n']
Detokenized (004): ['id_key', '=', ')', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "message_thread = model . MessageThread ( okc_id = self . thread . id , \n"
Original    (015): ['message_thread', '=', 'model', '.', 'MessageThread', '(', 'okc_id', '=', 'self', '.', 'thread', '.', 'id', ',', '\\n']
Tokenized   (026): ['[CLS]', 'message', '_', 'thread', '=', 'model', '.', 'message', '##th', '##rea', '##d', '(', 'ok', '##c', '_', 'id', '=', 'self', '.', 'thread', '.', 'id', ',', '\\', 'n', '[SEP]']
Filtered   (024): ['message', '_', 'thread', '=', 'model', '.', 'message', '##th', '##rea', '##d', '(', 'ok', '##c', '_', 'id', '=', 'self', '.', 'thread', '.', 'id', ',', '\\', 'n']
Detokenized (015): ['message_thread', '=', 'model', '.', 'message##th##rea##d', '(', 'ok##c_id', '=', 'self', '.', 'thread', '.', 'id', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "new_messages = [ message for message in self . thread . messages \n"
Original    (013): ['new_messages', '=', '[', 'message', 'for', 'message', 'in', 'self', '.', 'thread', '.', 'messages', '\\n']
Tokenized   (018): ['[CLS]', 'new', '_', 'messages', '=', '[', 'message', 'for', 'message', 'in', 'self', '.', 'thread', '.', 'messages', '\\', 'n', '[SEP]']
Filtered   (016): ['new', '_', 'messages', '=', '[', 'message', 'for', 'message', 'in', 'self', '.', 'thread', '.', 'messages', '\\', 'n']
Detokenized (013): ['new_messages', '=', '[', 'message', 'for', 'message', 'in', 'self', '.', 'thread', '.', 'messages', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "new_message_model = model . Message ( okc_id = new_message . id , \n"
Original    (013): ['new_message_model', '=', 'model', '.', 'Message', '(', 'okc_id', '=', 'new_message', '.', 'id', ',', '\\n']
Tokenized   (025): ['[CLS]', 'new', '_', 'message', '_', 'model', '=', 'model', '.', 'message', '(', 'ok', '##c', '_', 'id', '=', 'new', '_', 'message', '.', 'id', ',', '\\', 'n', '[SEP]']
Filtered   (023): ['new', '_', 'message', '_', 'model', '=', 'model', '.', 'message', '(', 'ok', '##c', '_', 'id', '=', 'new', '_', 'message', '.', 'id', ',', '\\', 'n']
Detokenized (013): ['new_message_model', '=', 'model', '.', 'message', '(', 'ok##c_id', '=', 'new_message', '.', 'id', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "time_sent = new_message . time_sent ) \n"
Original    (007): ['time_sent', '=', 'new_message', '.', 'time_sent', ')', '\\n']
Tokenized   (016): ['[CLS]', 'time', '_', 'sent', '=', 'new', '_', 'message', '.', 'time', '_', 'sent', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['time', '_', 'sent', '=', 'new', '_', 'message', '.', 'time', '_', 'sent', ')', '\\', 'n']
Detokenized (007): ['time_sent', '=', 'new_message', '.', 'time_sent', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "mailbox . Sync ( user ) . all ( ) \n"
Original    (011): ['mailbox', '.', 'Sync', '(', 'user', ')', '.', 'all', '(', ')', '\\n']
Tokenized   (015): ['[CLS]', 'mail', '##box', '.', 'sync', '(', 'user', ')', '.', 'all', '(', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['mail', '##box', '.', 'sync', '(', 'user', ')', '.', 'all', '(', ')', '\\', 'n']
Detokenized (011): ['mail##box', '.', 'sync', '(', 'user', ')', '.', 'all', '(', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "user_model . upsert_model ( id_key = ) \n"
Original    (008): ['user_model', '.', 'upsert_model', '(', 'id_key', '=', ')', '\\n']
Tokenized   (018): ['[CLS]', 'user', '_', 'model', '.', 'ups', '##ert', '_', 'model', '(', 'id', '_', 'key', '=', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['user', '_', 'model', '.', 'ups', '##ert', '_', 'model', '(', 'id', '_', 'key', '=', ')', '\\', 'n']
Detokenized (008): ['user_model', '.', 'ups##ert_model', '(', 'id_key', '=', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "response_dict = user . photo . upload_and_confirm ( user . quickmatch ( ) . photo_infos [ 0 ] ) \n"
Original    (020): ['response_dict', '=', 'user', '.', 'photo', '.', 'upload_and_confirm', '(', 'user', '.', 'quickmatch', '(', ')', '.', 'photo_infos', '[', '0', ']', ')', '\\n']
Tokenized   (036): ['[CLS]', 'response', '_', 'di', '##ct', '=', 'user', '.', 'photo', '.', 'up', '##load', '_', 'and', '_', 'confirm', '(', 'user', '.', 'quick', '##mat', '##ch', '(', ')', '.', 'photo', '_', 'info', '##s', '[', '0', ']', ')', '\\', 'n', '[SEP]']
Filtered   (034): ['response', '_', 'di', '##ct', '=', 'user', '.', 'photo', '.', 'up', '##load', '_', 'and', '_', 'confirm', '(', 'user', '.', 'quick', '##mat', '##ch', '(', ')', '.', 'photo', '_', 'info', '##s', '[', '0', ']', ')', '\\', 'n']
Detokenized (020): ['response_di##ct', '=', 'user', '.', 'photo', '.', 'up##load_and_confirm', '(', 'user', '.', 'quick##mat##ch', '(', ')', '.', 'photo_info##s', '[', '0', ']', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "vcr_live_sleep ( 2 ) \n"
Original    (005): ['vcr_live_sleep', '(', '2', ')', '\\n']
Tokenized   (013): ['[CLS]', 'vc', '##r', '_', 'live', '_', 'sleep', '(', '2', ')', '\\', 'n', '[SEP]']
Filtered   (011): ['vc', '##r', '_', 'live', '_', 'sleep', '(', '2', ')', '\\', 'n']
Detokenized (005): ['vc##r_live_sleep', '(', '2', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "b2_h = shared_zeros ( ( self . hp . batch_size , n_h ) ) \n"
Original    (015): ['b2_h', '=', 'shared_zeros', '(', '(', 'self', '.', 'hp', '.', 'batch_size', ',', 'n_h', ')', ')', '\\n']
Tokenized   (028): ['[CLS]', 'b', '##2', '_', 'h', '=', 'shared', '_', 'zero', '##s', '(', '(', 'self', '.', 'hp', '.', 'batch', '_', 'size', ',', 'n', '_', 'h', ')', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['b', '##2', '_', 'h', '=', 'shared', '_', 'zero', '##s', '(', '(', 'self', '.', 'hp', '.', 'batch', '_', 'size', ',', 'n', '_', 'h', ')', ')', '\\', 'n']
Detokenized (015): ['b##2_h', '=', 'shared_zero##s', '(', '(', 'self', '.', 'hp', '.', 'batch_size', ',', 'n_h', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "W1 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n"
Original    (019): ['W1', '=', 'shared_normal', '(', '(', 'n_h', ',', 'n_h', '*', 'gates', ')', ',', 'scale', '=', 'scale', '*', '1.5', ')', '\\n']
Tokenized   (031): ['[CLS]', 'w', '##1', '=', 'shared', '_', 'normal', '(', '(', 'n', '_', 'h', ',', 'n', '_', 'h', '*', 'gates', ')', ',', 'scale', '=', 'scale', '*', '1', '.', '5', ')', '\\', 'n', '[SEP]']
Filtered   (029): ['w', '##1', '=', 'shared', '_', 'normal', '(', '(', 'n', '_', 'h', ',', 'n', '_', 'h', '*', 'gates', ')', ',', 'scale', '=', 'scale', '*', '1', '.', '5', ')', '\\', 'n']
Detokenized (019): ['w##1', '=', 'shared_normal', '(', '(', 'n_h', ',', 'n_h', '*', 'gates', ')', ',', 'scale', '=', 'scale', '*', '1.5', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "b1 = shared_zeros ( ( n_h * gates ) ) \n"
Original    (011): ['b1', '=', 'shared_zeros', '(', '(', 'n_h', '*', 'gates', ')', ')', '\\n']
Tokenized   (019): ['[CLS]', 'b1', '=', 'shared', '_', 'zero', '##s', '(', '(', 'n', '_', 'h', '*', 'gates', ')', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['b1', '=', 'shared', '_', 'zero', '##s', '(', '(', 'n', '_', 'h', '*', 'gates', ')', ')', '\\', 'n']
Detokenized (011): ['b1', '=', 'shared_zero##s', '(', '(', 'n_h', '*', 'gates', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "b2 = shared_zeros ( ( n_h * gates , ) ) \n"
Original    (012): ['b2', '=', 'shared_zeros', '(', '(', 'n_h', '*', 'gates', ',', ')', ')', '\\n']
Tokenized   (021): ['[CLS]', 'b', '##2', '=', 'shared', '_', 'zero', '##s', '(', '(', 'n', '_', 'h', '*', 'gates', ',', ')', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['b', '##2', '=', 'shared', '_', 'zero', '##s', '(', '(', 'n', '_', 'h', '*', 'gates', ',', ')', ')', '\\', 'n']
Detokenized (012): ['b##2', '=', 'shared_zero##s', '(', '(', 'n_h', '*', 'gates', ',', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "i_on = T . nnet . sigmoid ( g_on [ : , : n_h ] ) \n"
Original    (017): ['i_on', '=', 'T', '.', 'nnet', '.', 'sigmoid', '(', 'g_on', '[', ':', ',', ':', 'n_h', ']', ')', '\\n']
Tokenized   (029): ['[CLS]', 'i', '_', 'on', '=', 't', '.', 'n', '##net', '.', 'si', '##gm', '##oid', '(', 'g', '_', 'on', '[', ':', ',', ':', 'n', '_', 'h', ']', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['i', '_', 'on', '=', 't', '.', 'n', '##net', '.', 'si', '##gm', '##oid', '(', 'g', '_', 'on', '[', ':', ',', ':', 'n', '_', 'h', ']', ')', '\\', 'n']
Detokenized (017): ['i_on', '=', 't', '.', 'n##net', '.', 'si##gm##oid', '(', 'g_on', '[', ':', ',', ':', 'n_h', ']', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "f_on = T . nnet . sigmoid ( g_on [ : , n_h : 2 * n_h ] ) \n"
Original    (020): ['f_on', '=', 'T', '.', 'nnet', '.', 'sigmoid', '(', 'g_on', '[', ':', ',', 'n_h', ':', '2', '*', 'n_h', ']', ')', '\\n']
Tokenized   (034): ['[CLS]', 'f', '_', 'on', '=', 't', '.', 'n', '##net', '.', 'si', '##gm', '##oid', '(', 'g', '_', 'on', '[', ':', ',', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '\\', 'n', '[SEP]']
Filtered   (032): ['f', '_', 'on', '=', 't', '.', 'n', '##net', '.', 'si', '##gm', '##oid', '(', 'g', '_', 'on', '[', ':', ',', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '\\', 'n']
Detokenized (020): ['f_on', '=', 't', '.', 'n##net', '.', 'si##gm##oid', '(', 'g_on', '[', ':', ',', 'n_h', ':', '2', '*', 'n_h', ']', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "o_on = T . nnet . sigmoid ( g_on [ : , 2 * n_h : 3 * n_h ] ) \n"
Original    (022): ['o_on', '=', 'T', '.', 'nnet', '.', 'sigmoid', '(', 'g_on', '[', ':', ',', '2', '*', 'n_h', ':', '3', '*', 'n_h', ']', ')', '\\n']
Tokenized   (036): ['[CLS]', 'o', '_', 'on', '=', 't', '.', 'n', '##net', '.', 'si', '##gm', '##oid', '(', 'g', '_', 'on', '[', ':', ',', '2', '*', 'n', '_', 'h', ':', '3', '*', 'n', '_', 'h', ']', ')', '\\', 'n', '[SEP]']
Filtered   (034): ['o', '_', 'on', '=', 't', '.', 'n', '##net', '.', 'si', '##gm', '##oid', '(', 'g', '_', 'on', '[', ':', ',', '2', '*', 'n', '_', 'h', ':', '3', '*', 'n', '_', 'h', ']', ')', '\\', 'n']
Detokenized (022): ['o_on', '=', 't', '.', 'n##net', '.', 'si##gm##oid', '(', 'g_on', '[', ':', ',', '2', '*', 'n_h', ':', '3', '*', 'n_h', ']', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "h_t = T . tanh ( T . dot ( X , W [ : , 1 * n_h : 2 * n_h ] ) + T . dot ( h , U [ : , 1 * n_h : 2 * n_h ] ) + b [ 1 * n_h : 2 * n_h ] ) \n"
Original    (058): ['h_t', '=', 'T', '.', 'tanh', '(', 'T', '.', 'dot', '(', 'X', ',', 'W', '[', ':', ',', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '+', 'T', '.', 'dot', '(', 'h', ',', 'U', '[', ':', ',', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '+', 'b', '[', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '\\n']
Tokenized   (076): ['[CLS]', 'h', '_', 't', '=', 't', '.', 'tan', '##h', '(', 't', '.', 'dot', '(', 'x', ',', 'w', '[', ':', ',', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '+', 't', '.', 'dot', '(', 'h', ',', 'u', '[', ':', ',', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '+', 'b', '[', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '\\', 'n', '[SEP]']
Filtered   (074): ['h', '_', 't', '=', 't', '.', 'tan', '##h', '(', 't', '.', 'dot', '(', 'x', ',', 'w', '[', ':', ',', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '+', 't', '.', 'dot', '(', 'h', ',', 'u', '[', ':', ',', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '+', 'b', '[', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '\\', 'n']
Detokenized (058): ['h_t', '=', 't', '.', 'tan##h', '(', 't', '.', 'dot', '(', 'x', ',', 'w', '[', ':', ',', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '+', 't', '.', 'dot', '(', 'h', ',', 'u', '[', ':', ',', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '+', 'b', '[', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '\\n']
Counter: 74
===================================================================
Hidden states:  (13, 58, 768)
# Extracted words:  58
Sentence         : "te_cost , te_h_updates = model ( self . X , self . params , 0. ) \n"
Original    (017): ['te_cost', ',', 'te_h_updates', '=', 'model', '(', 'self', '.', 'X', ',', 'self', '.', 'params', ',', '0.', ')', '\\n']
Tokenized   (028): ['[CLS]', 'te', '_', 'cost', ',', 'te', '_', 'h', '_', 'updates', '=', 'model', '(', 'self', '.', 'x', ',', 'self', '.', 'para', '##ms', ',', '0', '.', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['te', '_', 'cost', ',', 'te', '_', 'h', '_', 'updates', '=', 'model', '(', 'self', '.', 'x', ',', 'self', '.', 'para', '##ms', ',', '0', '.', ')', '\\', 'n']
Detokenized (017): ['te_cost', ',', 'te_h_updates', '=', 'model', '(', 'self', '.', 'x', ',', 'self', '.', 'para##ms', ',', '0.', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "csvWriter = csv . writer ( sys . stdout , delimiter = separator , quotechar = quote , \n"
Original    (019): ['csvWriter', '=', 'csv', '.', 'writer', '(', 'sys', '.', 'stdout', ',', 'delimiter', '=', 'separator', ',', 'quotechar', '=', 'quote', ',', '\\n']
Tokenized   (034): ['[CLS]', 'cs', '##v', '##writer', '=', 'cs', '##v', '.', 'writer', '(', 'sy', '##s', '.', 'st', '##dou', '##t', ',', 'del', '##imi', '##ter', '=', 'sep', '##arat', '##or', ',', 'quote', '##cha', '##r', '=', 'quote', ',', '\\', 'n', '[SEP]']
Filtered   (032): ['cs', '##v', '##writer', '=', 'cs', '##v', '.', 'writer', '(', 'sy', '##s', '.', 'st', '##dou', '##t', ',', 'del', '##imi', '##ter', '=', 'sep', '##arat', '##or', ',', 'quote', '##cha', '##r', '=', 'quote', ',', '\\', 'n']
Detokenized (019): ['cs##v##writer', '=', 'cs##v', '.', 'writer', '(', 'sy##s', '.', 'st##dou##t', ',', 'del##imi##ter', '=', 'sep##arat##or', ',', 'quote##cha##r', '=', 'quote', ',', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "IColumnProvider_Methods = IPersist_Methods + [ "Initialize" , "GetColumnInfo" , "GetItemData" ] \n"
Original    (012): ['IColumnProvider_Methods', '=', 'IPersist_Methods', '+', '[', '"Initialize"', ',', '"GetColumnInfo"', ',', '"GetItemData"', ']', '\\n']
Tokenized   (040): ['[CLS]', 'ic', '##ol', '##um', '##np', '##rov', '##ider', '_', 'methods', '=', 'ip', '##ers', '##ist', '_', 'methods', '+', '[', '"', 'initial', '##ize', '"', ',', '"', 'get', '##col', '##um', '##nin', '##fo', '"', ',', '"', 'get', '##ite', '##md', '##ata', '"', ']', '\\', 'n', '[SEP]']
Filtered   (038): ['ic', '##ol', '##um', '##np', '##rov', '##ider', '_', 'methods', '=', 'ip', '##ers', '##ist', '_', 'methods', '+', '[', '"', 'initial', '##ize', '"', ',', '"', 'get', '##col', '##um', '##nin', '##fo', '"', ',', '"', 'get', '##ite', '##md', '##ata', '"', ']', '\\', 'n']
Detokenized (012): ['ic##ol##um##np##rov##ider_methods', '=', 'ip##ers##ist_methods', '+', '[', '"initial##ize"', ',', '"get##col##um##nin##fo"', ',', '"get##ite##md##ata"', ']', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "_com_interfaces_ = [ pythoncom . IID_IPersist , \n"
Original    (008): ['_com_interfaces_', '=', '[', 'pythoncom', '.', 'IID_IPersist', ',', '\\n']
Tokenized   (021): ['[CLS]', '_', 'com', '_', 'interfaces', '_', '=', '[', 'python', '##com', '.', 'ii', '##d', '_', 'ip', '##ers', '##ist', ',', '\\', 'n', '[SEP]']
Filtered   (019): ['_', 'com', '_', 'interfaces', '_', '=', '[', 'python', '##com', '.', 'ii', '##d', '_', 'ip', '##ers', '##ist', ',', '\\', 'n']
Detokenized (008): ['_com_interfaces_', '=', '[', 'python##com', '.', 'ii##d_ip##ers##ist', ',', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "20 , #cChars \n"
Original    (004): ['20', ',', '#cChars', '\\n']
Tokenized   (010): ['[CLS]', '20', ',', '#', 'cc', '##har', '##s', '\\', 'n', '[SEP]']
Filtered   (008): ['20', ',', '#', 'cc', '##har', '##s', '\\', 'n']
Detokenized (004): ['20', ',', '#cc##har##s', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "fmt_id == self . _reg_clsid_ \n"
Original    (006): ['fmt_id', '==', 'self', '.', '_reg_clsid_', '\\n']
Tokenized   (019): ['[CLS]', 'fm', '##t', '_', 'id', '=', '=', 'self', '.', '_', 'reg', '_', 'cl', '##si', '##d', '_', '\\', 'n', '[SEP]']
Filtered   (017): ['fm', '##t', '_', 'id', '=', '=', 'self', '.', '_', 'reg', '_', 'cl', '##si', '##d', '_', '\\', 'n']
Detokenized (006): ['fm##t_id', '==', 'self', '.', '_reg_cl##si##d_', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : ""Folder\\\\ShellEx\\\\ColumnHandlers\\\\" + str ( ColumnProvider . _reg_clsid_ ) ) \n"
Original    (010): ['"Folder\\\\\\\\ShellEx\\\\\\\\ColumnHandlers\\\\\\\\"', '+', 'str', '(', 'ColumnProvider', '.', '_reg_clsid_', ')', ')', '\\n']
Tokenized   (042): ['[CLS]', '"', 'folder', '\\', '\\', '\\', '\\', 'shell', '##ex', '\\', '\\', '\\', '\\', 'column', '##hand', '##lers', '\\', '\\', '\\', '\\', '"', '+', 'st', '##r', '(', 'column', '##pro', '##vid', '##er', '.', '_', 'reg', '_', 'cl', '##si', '##d', '_', ')', ')', '\\', 'n', '[SEP]']
Filtered   (040): ['"', 'folder', '\\', '\\', '\\', '\\', 'shell', '##ex', '\\', '\\', '\\', '\\', 'column', '##hand', '##lers', '\\', '\\', '\\', '\\', '"', '+', 'st', '##r', '(', 'column', '##pro', '##vid', '##er', '.', '_', 'reg', '_', 'cl', '##si', '##d', '_', ')', ')', '\\', 'n']
Detokenized (010): ['"folder\\\\\\\\shell##ex\\\\\\\\column##hand##lers\\\\\\\\"', '+', 'st##r', '(', 'column##pro##vid##er', '.', '_reg_cl##si##d_', ')', ')', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "_winreg . SetValueEx ( key , None , 0 , _winreg . REG_SZ , ColumnProvider . _reg_desc_ ) \n"
Original    (019): ['_winreg', '.', 'SetValueEx', '(', 'key', ',', 'None', ',', '0', ',', '_winreg', '.', 'REG_SZ', ',', 'ColumnProvider', '.', '_reg_desc_', ')', '\\n']
Tokenized   (042): ['[CLS]', '_', 'win', '##re', '##g', '.', 'set', '##val', '##ue', '##ex', '(', 'key', ',', 'none', ',', '0', ',', '_', 'win', '##re', '##g', '.', 'reg', '_', 's', '##z', ',', 'column', '##pro', '##vid', '##er', '.', '_', 'reg', '_', 'des', '##c', '_', ')', '\\', 'n', '[SEP]']
Filtered   (040): ['_', 'win', '##re', '##g', '.', 'set', '##val', '##ue', '##ex', '(', 'key', ',', 'none', ',', '0', ',', '_', 'win', '##re', '##g', '.', 'reg', '_', 's', '##z', ',', 'column', '##pro', '##vid', '##er', '.', '_', 'reg', '_', 'des', '##c', '_', ')', '\\', 'n']
Detokenized (019): ['_win##re##g', '.', 'set##val##ue##ex', '(', 'key', ',', 'none', ',', '0', ',', '_win##re##g', '.', 'reg_s##z', ',', 'column##pro##vid##er', '.', '_reg_des##c_', ')', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "register . UseCommandLine ( ColumnProvider , \n"
Original    (007): ['register', '.', 'UseCommandLine', '(', 'ColumnProvider', ',', '\\n']
Tokenized   (017): ['[CLS]', 'register', '.', 'use', '##com', '##man', '##dl', '##ine', '(', 'column', '##pro', '##vid', '##er', ',', '\\', 'n', '[SEP]']
Filtered   (015): ['register', '.', 'use', '##com', '##man', '##dl', '##ine', '(', 'column', '##pro', '##vid', '##er', ',', '\\', 'n']
Detokenized (007): ['register', '.', 'use##com##man##dl##ine', '(', 'column##pro##vid##er', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "aliases = MultipleValueField ( required = False ) \n"
Original    (009): ['aliases', '=', 'MultipleValueField', '(', 'required', '=', 'False', ')', '\\n']
Tokenized   (016): ['[CLS]', 'alias', '##es', '=', 'multiple', '##val', '##ue', '##field', '(', 'required', '=', 'false', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['alias', '##es', '=', 'multiple', '##val', '##ue', '##field', '(', 'required', '=', 'false', ')', '\\', 'n']
Detokenized (009): ['alias##es', '=', 'multiple##val##ue##field', '(', 'required', '=', 'false', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "StoredQueryParameter = namedtuple ( "StoredQueryParameter" , ( , , , , \n"
Original    (012): ['StoredQueryParameter', '=', 'namedtuple', '(', '"StoredQueryParameter"', ',', '(', ',', ',', ',', ',', '\\n']
Tokenized   (027): ['[CLS]', 'stored', '##que', '##ry', '##para', '##meter', '=', 'named', '##tu', '##ple', '(', '"', 'stored', '##que', '##ry', '##para', '##meter', '"', ',', '(', ',', ',', ',', ',', '\\', 'n', '[SEP]']
Filtered   (025): ['stored', '##que', '##ry', '##para', '##meter', '=', 'named', '##tu', '##ple', '(', '"', 'stored', '##que', '##ry', '##para', '##meter', '"', ',', '(', ',', ',', ',', ',', '\\', 'n']
Detokenized (012): ['stored##que##ry##para##meter', '=', 'named##tu##ple', '(', '"stored##que##ry##para##meter"', ',', '(', ',', ',', ',', ',', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "fts = list ( self . models . keys ( ) ) \n"
Original    (013): ['fts', '=', 'list', '(', 'self', '.', 'models', '.', 'keys', '(', ')', ')', '\\n']
Tokenized   (017): ['[CLS]', 'ft', '##s', '=', 'list', '(', 'self', '.', 'models', '.', 'keys', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['ft', '##s', '=', 'list', '(', 'self', '.', 'models', '.', 'keys', '(', ')', ')', '\\', 'n']
Detokenized (013): ['ft##s', '=', 'list', '(', 'self', '.', 'models', '.', 'keys', '(', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "sort_by = parms . cleaned_data [ ] \n"
Original    (008): ['sort_by', '=', 'parms', '.', 'cleaned_data', '[', ']', '\\n']
Tokenized   (016): ['[CLS]', 'sort', '_', 'by', '=', 'par', '##ms', '.', 'cleaned', '_', 'data', '[', ']', '\\', 'n', '[SEP]']
Filtered   (014): ['sort', '_', 'by', '=', 'par', '##ms', '.', 'cleaned', '_', 'data', '[', ']', '\\', 'n']
Detokenized (008): ['sort_by', '=', 'par##ms', '.', 'cleaned_data', '[', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "geometry_field = self . geometries [ type_names [ 0 ] ] \n"
Original    (012): ['geometry_field', '=', 'self', '.', 'geometries', '[', 'type_names', '[', '0', ']', ']', '\\n']
Tokenized   (021): ['[CLS]', 'geometry', '_', 'field', '=', 'self', '.', 'geo', '##met', '##ries', '[', 'type', '_', 'names', '[', '0', ']', ']', '\\', 'n', '[SEP]']
Filtered   (019): ['geometry', '_', 'field', '=', 'self', '.', 'geo', '##met', '##ries', '[', 'type', '_', 'names', '[', '0', ']', ']', '\\', 'n']
Detokenized (012): ['geometry_field', '=', 'self', '.', 'geo##met##ries', '[', 'type_names', '[', '0', ']', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "mxy = mxy ) \n"
Original    (005): ['mxy', '=', 'mxy', ')', '\\n']
Tokenized   (010): ['[CLS]', 'mx', '##y', '=', 'mx', '##y', ')', '\\', 'n', '[SEP]']
Filtered   (008): ['mx', '##y', '=', 'mx', '##y', ')', '\\', 'n']
Detokenized (005): ['mx##y', '=', 'mx##y', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "query_set = query_set . order_by ( * sort_by ) \n"
Original    (010): ['query_set', '=', 'query_set', '.', 'order_by', '(', '*', 'sort_by', ')', '\\n']
Tokenized   (021): ['[CLS]', 'query', '_', 'set', '=', 'query', '_', 'set', '.', 'order', '_', 'by', '(', '*', 'sort', '_', 'by', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['query', '_', 'set', '=', 'query', '_', 'set', '.', 'order', '_', 'by', '(', '*', 'sort', '_', 'by', ')', '\\', 'n']
Detokenized (010): ['query_set', '=', 'query_set', '.', 'order_by', '(', '*', 'sort_by', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "output_format = root . get ( , ) \n"
Original    (009): ['output_format', '=', 'root', '.', 'get', '(', ',', ')', '\\n']
Tokenized   (014): ['[CLS]', 'output', '_', 'format', '=', 'root', '.', 'get', '(', ',', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['output', '_', 'format', '=', 'root', '.', 'get', '(', ',', ')', '\\', 'n']
Detokenized (009): ['output_format', '=', 'root', '.', 'get', '(', ',', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "type_names . append ( ( namespace , name ) ) \n"
Original    (011): ['type_names', '.', 'append', '(', '(', 'namespace', ',', 'name', ')', ')', '\\n']
Tokenized   (018): ['[CLS]', 'type', '_', 'names', '.', 'app', '##end', '(', '(', 'names', '##pace', ',', 'name', ')', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['type', '_', 'names', '.', 'app', '##end', '(', '(', 'names', '##pace', ',', 'name', ')', ')', '\\', 'n']
Detokenized (011): ['type_names', '.', 'app##end', '(', '(', 'names##pace', ',', 'name', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""schema" : feature_type . schema , \n"
Original    (007): ['"schema"', ':', 'feature_type', '.', 'schema', ',', '\\n']
Tokenized   (016): ['[CLS]', '"', 'sc', '##hema', '"', ':', 'feature', '_', 'type', '.', 'sc', '##hema', ',', '\\', 'n', '[SEP]']
Filtered   (014): ['"', 'sc', '##hema', '"', ':', 'feature', '_', 'type', '.', 'sc', '##hema', ',', '\\', 'n']
Detokenized (007): ['"sc##hema"', ':', 'feature_type', '.', 'sc##hema', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""ns_name" : feature_type . ns_name \n"
Original    (006): ['"ns_name"', ':', 'feature_type', '.', 'ns_name', '\\n']
Tokenized   (017): ['[CLS]', '"', 'ns', '_', 'name', '"', ':', 'feature', '_', 'type', '.', 'ns', '_', 'name', '\\', 'n', '[SEP]']
Filtered   (015): ['"', 'ns', '_', 'name', '"', ':', 'feature', '_', 'type', '.', 'ns', '_', 'name', '\\', 'n']
Detokenized (006): ['"ns_name"', ':', 'feature_type', '.', 'ns_name', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "db_params = settings . DATABASES [ response . db ] \n"
Original    (011): ['db_params', '=', 'settings', '.', 'DATABASES', '[', 'response', '.', 'db', ']', '\\n']
Tokenized   (017): ['[CLS]', 'db', '_', 'para', '##ms', '=', 'settings', '.', 'databases', '[', 'response', '.', 'db', ']', '\\', 'n', '[SEP]']
Filtered   (015): ['db', '_', 'para', '##ms', '=', 'settings', '.', 'databases', '[', 'response', '.', 'db', ']', '\\', 'n']
Detokenized (011): ['db_para##ms', '=', 'settings', '.', 'databases', '[', 'response', '.', 'db', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "parameters = tuple ( [ adapt ( p ) for p in parameters ] ) \n"
Original    (016): ['parameters', '=', 'tuple', '(', '[', 'adapt', '(', 'p', ')', 'for', 'p', 'in', 'parameters', ']', ')', '\\n']
Tokenized   (020): ['[CLS]', 'parameters', '=', 'tu', '##ple', '(', '[', 'adapt', '(', 'p', ')', 'for', 'p', 'in', 'parameters', ']', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['parameters', '=', 'tu', '##ple', '(', '[', 'adapt', '(', 'p', ')', 'for', 'p', 'in', 'parameters', ']', ')', '\\', 'n']
Detokenized (016): ['parameters', '=', 'tu##ple', '(', '[', 'adapt', '(', 'p', ')', 'for', 'p', 'in', 'parameters', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "connection_string = "PG:dbname=\'{db}\'" . format ( db = db_params [ ] ) \n"
Original    (013): ['connection_string', '=', '"PG:dbname=\\\'{db}\\\'"', '.', 'format', '(', 'db', '=', 'db_params', '[', ']', ')', '\\n']
Tokenized   (034): ['[CLS]', 'connection', '_', 'string', '=', '"', 'pg', ':', 'db', '##name', '=', '\\', "'", '{', 'db', '}', '\\', "'", '"', '.', 'format', '(', 'db', '=', 'db', '_', 'para', '##ms', '[', ']', ')', '\\', 'n', '[SEP]']
Filtered   (032): ['connection', '_', 'string', '=', '"', 'pg', ':', 'db', '##name', '=', '\\', "'", '{', 'db', '}', '\\', "'", '"', '.', 'format', '(', 'db', '=', 'db', '_', 'para', '##ms', '[', ']', ')', '\\', 'n']
Detokenized (013): ['connection_string', '=', '"pg:db##name=\\\'{db}\\\'"', '.', 'format', '(', 'db', '=', 'db_para##ms', '[', ']', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "etree . SubElement ( p , ) . text = parameter . abstractS \n"
Original    (014): ['etree', '.', 'SubElement', '(', 'p', ',', ')', '.', 'text', '=', 'parameter', '.', 'abstractS', '\\n']
Tokenized   (020): ['[CLS]', 'et', '##ree', '.', 'sub', '##ele', '##ment', '(', 'p', ',', ')', '.', 'text', '=', 'parameter', '.', 'abstracts', '\\', 'n', '[SEP]']
Filtered   (018): ['et', '##ree', '.', 'sub', '##ele', '##ment', '(', 'p', ',', ')', '.', 'text', '=', 'parameter', '.', 'abstracts', '\\', 'n']
Detokenized (014): ['et##ree', '.', 'sub##ele##ment', '(', 'p', ',', ')', '.', 'text', '=', 'parameter', '.', 'abstracts', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : ""isPrivate" : parameter . query_expression . private == True , \n"
Original    (011): ['"isPrivate"', ':', 'parameter', '.', 'query_expression', '.', 'private', '==', 'True', ',', '\\n']
Tokenized   (021): ['[CLS]', '"', 'is', '##pr', '##ivate', '"', ':', 'parameter', '.', 'query', '_', 'expression', '.', 'private', '=', '=', 'true', ',', '\\', 'n', '[SEP]']
Filtered   (019): ['"', 'is', '##pr', '##ivate', '"', ':', 'parameter', '.', 'query', '_', 'expression', '.', 'private', '=', '=', 'true', ',', '\\', 'n']
Detokenized (011): ['"is##pr##ivate"', ':', 'parameter', '.', 'query_expression', '.', 'private', '==', 'true', ',', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""language" : parameter . query_expression . language , \n"
Original    (009): ['"language"', ':', 'parameter', '.', 'query_expression', '.', 'language', ',', '\\n']
Tokenized   (016): ['[CLS]', '"', 'language', '"', ':', 'parameter', '.', 'query', '_', 'expression', '.', 'language', ',', '\\', 'n', '[SEP]']
Filtered   (014): ['"', 'language', '"', ':', 'parameter', '.', 'query', '_', 'expression', '.', 'language', ',', '\\', 'n']
Detokenized (009): ['"language"', ':', 'parameter', '.', 'query_expression', '.', 'language', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ""returnFeatureTypes" : . join ( parameter . query_expression . return_feature_types } ) . text = parameter . query_expression . text \n"
Original    (021): ['"returnFeatureTypes"', ':', '.', 'join', '(', 'parameter', '.', 'query_expression', '.', 'return_feature_types', '}', ')', '.', 'text', '=', 'parameter', '.', 'query_expression', '.', 'text', '\\n']
Tokenized   (039): ['[CLS]', '"', 'return', '##fe', '##at', '##ure', '##type', '##s', '"', ':', '.', 'join', '(', 'parameter', '.', 'query', '_', 'expression', '.', 'return', '_', 'feature', '_', 'types', '}', ')', '.', 'text', '=', 'parameter', '.', 'query', '_', 'expression', '.', 'text', '\\', 'n', '[SEP]']
Filtered   (037): ['"', 'return', '##fe', '##at', '##ure', '##type', '##s', '"', ':', '.', 'join', '(', 'parameter', '.', 'query', '_', 'expression', '.', 'return', '_', 'feature', '_', 'types', '}', ')', '.', 'text', '=', 'parameter', '.', 'query', '_', 'expression', '.', 'text', '\\', 'n']
Detokenized (021): ['"return##fe##at##ure##type##s"', ':', '.', 'join', '(', 'parameter', '.', 'query_expression', '.', 'return_feature_types', '}', ')', '.', 'text', '=', 'parameter', '.', 'query_expression', '.', 'text', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : ""endpoint" : request . build_absolute_uri ( ) . split ( ) [ 0 ] , \n"
Original    (016): ['"endpoint"', ':', 'request', '.', 'build_absolute_uri', '(', ')', '.', 'split', '(', ')', '[', '0', ']', ',', '\\n']
Tokenized   (027): ['[CLS]', '"', 'end', '##point', '"', ':', 'request', '.', 'build', '_', 'absolute', '_', 'ur', '##i', '(', ')', '.', 'split', '(', ')', '[', '0', ']', ',', '\\', 'n', '[SEP]']
Filtered   (025): ['"', 'end', '##point', '"', ':', 'request', '.', 'build', '_', 'absolute', '_', 'ur', '##i', '(', ')', '.', 'split', '(', ')', '[', '0', ']', ',', '\\', 'n']
Detokenized (016): ['"end##point"', ':', 'request', '.', 'build_absolute_ur##i', '(', ')', '.', 'split', '(', ')', '[', '0', ']', ',', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : ""output_formats" : [ ogr . GetDriver ( drv ) . GetName ( ) for drv in range ( ogr . GetDriverCount ( ) ) "addr_street" : self . addr_street , \n"
Original    (031): ['"output_formats"', ':', '[', 'ogr', '.', 'GetDriver', '(', 'drv', ')', '.', 'GetName', '(', ')', 'for', 'drv', 'in', 'range', '(', 'ogr', '.', 'GetDriverCount', '(', ')', ')', '"addr_street"', ':', 'self', '.', 'addr_street', ',', '\\n']
Tokenized   (056): ['[CLS]', '"', 'output', '_', 'formats', '"', ':', '[', 'og', '##r', '.', 'get', '##drive', '##r', '(', 'dr', '##v', ')', '.', 'get', '##name', '(', ')', 'for', 'dr', '##v', 'in', 'range', '(', 'og', '##r', '.', 'get', '##drive', '##rco', '##unt', '(', ')', ')', '"', 'add', '##r', '_', 'street', '"', ':', 'self', '.', 'add', '##r', '_', 'street', ',', '\\', 'n', '[SEP]']
Filtered   (054): ['"', 'output', '_', 'formats', '"', ':', '[', 'og', '##r', '.', 'get', '##drive', '##r', '(', 'dr', '##v', ')', '.', 'get', '##name', '(', ')', 'for', 'dr', '##v', 'in', 'range', '(', 'og', '##r', '.', 'get', '##drive', '##rco', '##unt', '(', ')', ')', '"', 'add', '##r', '_', 'street', '"', ':', 'self', '.', 'add', '##r', '_', 'street', ',', '\\', 'n']
Detokenized (031): ['"output_formats"', ':', '[', 'og##r', '.', 'get##drive##r', '(', 'dr##v', ')', '.', 'get##name', '(', ')', 'for', 'dr##v', 'in', 'range', '(', 'og##r', '.', 'get##drive##rco##unt', '(', ')', ')', '"add##r_street"', ':', 'self', '.', 'add##r_street', ',', '\\n']
Counter: 54
===================================================================
Hidden states:  (13, 31, 768)
# Extracted words:  31
Sentence         : ""feature_versioning" : self . adapter . supports_feature_versioning ( ) , \n"
Original    (011): ['"feature_versioning"', ':', 'self', '.', 'adapter', '.', 'supports_feature_versioning', '(', ')', ',', '\\n']
Tokenized   (025): ['[CLS]', '"', 'feature', '_', 'version', '##ing', '"', ':', 'self', '.', 'adapt', '##er', '.', 'supports', '_', 'feature', '_', 'version', '##ing', '(', ')', ',', '\\', 'n', '[SEP]']
Filtered   (023): ['"', 'feature', '_', 'version', '##ing', '"', ':', 'self', '.', 'adapt', '##er', '.', 'supports', '_', 'feature', '_', 'version', '##ing', '(', ')', ',', '\\', 'n']
Detokenized (011): ['"feature_version##ing"', ':', 'self', '.', 'adapt##er', '.', 'supports_feature_version##ing', '(', ')', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""date" : datetime . now ( ) , \n"
Original    (009): ['"date"', ':', 'datetime', '.', 'now', '(', ')', ',', '\\n']
Tokenized   (015): ['[CLS]', '"', 'date', '"', ':', 'date', '##time', '.', 'now', '(', ')', ',', '\\', 'n', '[SEP]']
Filtered   (013): ['"', 'date', '"', ':', 'date', '##time', '.', 'now', '(', ')', ',', '\\', 'n']
Detokenized (009): ['"date"', ':', 'date##time', '.', 'now', '(', ')', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "matchItem . setText ( 3 , unicode ( self . data [ "Matches" ] ) ) \n"
Original    (017): ['matchItem', '.', 'setText', '(', '3', ',', 'unicode', '(', 'self', '.', 'data', '[', '"Matches"', ']', ')', ')', '\\n']
Tokenized   (025): ['[CLS]', 'match', '##ite', '##m', '.', 'set', '##text', '(', '3', ',', 'unicode', '(', 'self', '.', 'data', '[', '"', 'matches', '"', ']', ')', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['match', '##ite', '##m', '.', 'set', '##text', '(', '3', ',', 'unicode', '(', 'self', '.', 'data', '[', '"', 'matches', '"', ']', ')', ')', '\\', 'n']
Detokenized (017): ['match##ite##m', '.', 'set##text', '(', '3', ',', 'unicode', '(', 'self', '.', 'data', '[', '"matches"', ']', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "roundItem . setText ( 3 , unicode ( opponent [ 2 ] ) ) \n"
Original    (015): ['roundItem', '.', 'setText', '(', '3', ',', 'unicode', '(', 'opponent', '[', '2', ']', ')', ')', '\\n']
Tokenized   (021): ['[CLS]', 'round', '##ite', '##m', '.', 'set', '##text', '(', '3', ',', 'unicode', '(', 'opponent', '[', '2', ']', ')', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['round', '##ite', '##m', '.', 'set', '##text', '(', '3', ',', 'unicode', '(', 'opponent', '[', '2', ']', ')', ')', '\\', 'n']
Detokenized (015): ['round##ite##m', '.', 'set##text', '(', '3', ',', 'unicode', '(', 'opponent', '[', '2', ']', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "opponent [ 3 ] = roundItem \n"
Original    (007): ['opponent', '[', '3', ']', '=', 'roundItem', '\\n']
Tokenized   (012): ['[CLS]', 'opponent', '[', '3', ']', '=', 'round', '##ite', '##m', '\\', 'n', '[SEP]']
Filtered   (010): ['opponent', '[', '3', ']', '=', 'round', '##ite', '##m', '\\', 'n']
Detokenized (007): ['opponent', '[', '3', ']', '=', 'round##ite##m', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "M = np . matrix ( [ [ 2 , 3 , 4 ] , \n"
Original    (016): ['M', '=', 'np', '.', 'matrix', '(', '[', '[', '2', ',', '3', ',', '4', ']', ',', '\\n']
Tokenized   (019): ['[CLS]', 'm', '=', 'np', '.', 'matrix', '(', '[', '[', '2', ',', '3', ',', '4', ']', ',', '\\', 'n', '[SEP]']
Filtered   (017): ['m', '=', 'np', '.', 'matrix', '(', '[', '[', '2', ',', '3', ',', '4', ']', ',', '\\', 'n']
Detokenized (016): ['m', '=', 'np', '.', 'matrix', '(', '[', '[', '2', ',', '3', ',', '4', ']', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "matrix = Matrix ( M , mtype = ) \n"
Original    (010): ['matrix', '=', 'Matrix', '(', 'M', ',', 'mtype', '=', ')', '\\n']
Tokenized   (014): ['[CLS]', 'matrix', '=', 'matrix', '(', 'm', ',', 'mt', '##ype', '=', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['matrix', '=', 'matrix', '(', 'm', ',', 'mt', '##ype', '=', ')', '\\', 'n']
Detokenized (010): ['matrix', '=', 'matrix', '(', 'm', ',', 'mt##ype', '=', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "math = Math ( data = [ , vec_name , , Matrix ( M * a ) ] ) \n"
Original    (020): ['math', '=', 'Math', '(', 'data', '=', '[', ',', 'vec_name', ',', ',', 'Matrix', '(', 'M', '*', 'a', ')', ']', ')', '\\n']
Tokenized   (026): ['[CLS]', 'math', '=', 'math', '(', 'data', '=', '[', ',', 've', '##c', '_', 'name', ',', ',', 'matrix', '(', 'm', '*', 'a', ')', ']', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['math', '=', 'math', '(', 'data', '=', '[', ',', 've', '##c', '_', 'name', ',', ',', 'matrix', '(', 'm', '*', 'a', ')', ']', ')', '\\', 'n']
Detokenized (020): ['math', '=', 'math', '(', 'data', '=', '[', ',', 've##c_name', ',', ',', 'matrix', '(', 'm', '*', 'a', ')', ']', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "q2 = Quantity ( v , format_cb = lambda x : str ( int ( x ) ) ) \n"
Original    (020): ['q2', '=', 'Quantity', '(', 'v', ',', 'format_cb', '=', 'lambda', 'x', ':', 'str', '(', 'int', '(', 'x', ')', ')', ')', '\\n']
Tokenized   (027): ['[CLS]', 'q', '##2', '=', 'quantity', '(', 'v', ',', 'format', '_', 'cb', '=', 'lambda', 'x', ':', 'st', '##r', '(', 'int', '(', 'x', ')', ')', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['q', '##2', '=', 'quantity', '(', 'v', ',', 'format', '_', 'cb', '=', 'lambda', 'x', ':', 'st', '##r', '(', 'int', '(', 'x', ')', ')', ')', '\\', 'n']
Detokenized (020): ['q##2', '=', 'quantity', '(', 'v', ',', 'format_cb', '=', 'lambda', 'x', ':', 'st##r', '(', 'int', '(', 'x', ')', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "q3 = Quantity ( v , options = { : } ) \n"
Original    (013): ['q3', '=', 'Quantity', '(', 'v', ',', 'options', '=', '{', ':', '}', ')', '\\n']
Tokenized   (017): ['[CLS]', 'q', '##3', '=', 'quantity', '(', 'v', ',', 'options', '=', '{', ':', '}', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['q', '##3', '=', 'quantity', '(', 'v', ',', 'options', '=', '{', ':', '}', ')', '\\', 'n']
Detokenized (013): ['q##3', '=', 'quantity', '(', 'v', ',', 'options', '=', '{', ':', '}', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "test_dimensionality_to_siunitx ( ) \n"
Original    (004): ['test_dimensionality_to_siunitx', '(', ')', '\\n']
Tokenized   (017): ['[CLS]', 'test', '_', 'dimensional', '##ity', '_', 'to', '_', 'si', '##uni', '##t', '##x', '(', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['test', '_', 'dimensional', '##ity', '_', 'to', '_', 'si', '##uni', '##t', '##x', '(', ')', '\\', 'n']
Detokenized (004): ['test_dimensional##ity_to_si##uni##t##x', '(', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "ph = put_handler . put_handler ( fs , ) \n"
Original    (010): ['ph', '=', 'put_handler', '.', 'put_handler', '(', 'fs', ',', ')', '\\n']
Tokenized   (018): ['[CLS]', 'ph', '=', 'put', '_', 'handler', '.', 'put', '_', 'handler', '(', 'f', '##s', ',', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['ph', '=', 'put', '_', 'handler', '.', 'put', '_', 'handler', '(', 'f', '##s', ',', ')', '\\', 'n']
Detokenized (010): ['ph', '=', 'put_handler', '.', 'put_handler', '(', 'f##s', ',', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "hs = http_server . http_server ( ip = , port = 8080 ) \n"
Original    (014): ['hs', '=', 'http_server', '.', 'http_server', '(', 'ip', '=', ',', 'port', '=', '8080', ')', '\\n']
Tokenized   (022): ['[CLS]', 'hs', '=', 'http', '_', 'server', '.', 'http', '_', 'server', '(', 'ip', '=', ',', 'port', '=', '80', '##80', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['hs', '=', 'http', '_', 'server', '.', 'http', '_', 'server', '(', 'ip', '=', ',', 'port', '=', '80', '##80', ')', '\\', 'n']
Detokenized (014): ['hs', '=', 'http_server', '.', 'http_server', '(', 'ip', '=', ',', 'port', '=', '80##80', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "num_trans = num_requests * num_conns \n"
Original    (006): ['num_trans', '=', 'num_requests', '*', 'num_conns', '\\n']
Tokenized   (019): ['[CLS]', 'nu', '##m', '_', 'trans', '=', 'nu', '##m', '_', 'requests', '*', 'nu', '##m', '_', 'con', '##ns', '\\', 'n', '[SEP]']
Filtered   (017): ['nu', '##m', '_', 'trans', '=', 'nu', '##m', '_', 'requests', '*', 'nu', '##m', '_', 'con', '##ns', '\\', 'n']
Detokenized (006): ['nu##m_trans', '=', 'nu##m_requests', '*', 'nu##m_con##ns', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "trans_per_sec = num_trans / total_time \n"
Original    (006): ['trans_per_sec', '=', 'num_trans', '/', 'total_time', '\\n']
Tokenized   (018): ['[CLS]', 'trans', '_', 'per', '_', 'sec', '=', 'nu', '##m', '_', 'trans', '/', 'total', '_', 'time', '\\', 'n', '[SEP]']
Filtered   (016): ['trans', '_', 'per', '_', 'sec', '=', 'nu', '##m', '_', 'trans', '/', 'total', '_', 'time', '\\', 'n']
Detokenized (006): ['trans_per_sec', '=', 'nu##m_trans', '/', 'total_time', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "map ( str , ( num_conns , num_requests , request_size , throughput , trans_per_sec ) \n"
Original    (016): ['map', '(', 'str', ',', '(', 'num_conns', ',', 'num_requests', ',', 'request_size', ',', 'throughput', ',', 'trans_per_sec', ')', '\\n']
Tokenized   (034): ['[CLS]', 'map', '(', 'st', '##r', ',', '(', 'nu', '##m', '_', 'con', '##ns', ',', 'nu', '##m', '_', 'requests', ',', 'request', '_', 'size', ',', 'through', '##put', ',', 'trans', '_', 'per', '_', 'sec', ')', '\\', 'n', '[SEP]']
Filtered   (032): ['map', '(', 'st', '##r', ',', '(', 'nu', '##m', '_', 'con', '##ns', ',', 'nu', '##m', '_', 'requests', ',', 'request', '_', 'size', ',', 'through', '##put', ',', 'trans', '_', 'per', '_', 'sec', ')', '\\', 'n']
Detokenized (016): ['map', '(', 'st##r', ',', '(', 'nu##m_con##ns', ',', 'nu##m_requests', ',', 'request_size', ',', 'through##put', ',', 'trans_per_sec', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "queue . add_task ( task , 3 ) \n"
Original    (009): ['queue', '.', 'add_task', '(', 'task', ',', '3', ')', '\\n']
Tokenized   (014): ['[CLS]', 'queue', '.', 'add', '_', 'task', '(', 'task', ',', '3', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['queue', '.', 'add', '_', 'task', '(', 'task', ',', '3', ')', '\\', 'n']
Detokenized (009): ['queue', '.', 'add_task', '(', 'task', ',', '3', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "futures . append ( queue . yield_task ( task , 3 ) ) \n"
Original    (014): ['futures', '.', 'append', '(', 'queue', '.', 'yield_task', '(', 'task', ',', '3', ')', ')', '\\n']
Tokenized   (020): ['[CLS]', 'futures', '.', 'app', '##end', '(', 'queue', '.', 'yield', '_', 'task', '(', 'task', ',', '3', ')', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['futures', '.', 'app', '##end', '(', 'queue', '.', 'yield', '_', 'task', '(', 'task', ',', '3', ')', ')', '\\', 'n']
Detokenized (014): ['futures', '.', 'app##end', '(', 'queue', '.', 'yield_task', '(', 'task', ',', '3', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "task_results [ : ] = res \n"
Original    (007): ['task_results', '[', ':', ']', '=', 'res', '\\n']
Tokenized   (012): ['[CLS]', 'task', '_', 'results', '[', ':', ']', '=', 'res', '\\', 'n', '[SEP]']
Filtered   (010): ['task', '_', 'results', '[', ':', ']', '=', 'res', '\\', 'n']
Detokenized (007): ['task_results', '[', ':', ']', '=', 'res', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "shuffle ( self . __queued_servers ) \n"
Original    (007): ['shuffle', '(', 'self', '.', '__queued_servers', ')', '\\n']
Tokenized   (015): ['[CLS]', 'shuffle', '(', 'self', '.', '_', '_', 'queue', '##d', '_', 'servers', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['shuffle', '(', 'self', '.', '_', '_', 'queue', '##d', '_', 'servers', ')', '\\', 'n']
Detokenized (007): ['shuffle', '(', 'self', '.', '__queue##d_servers', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "event_name = event [ ] \n"
Original    (006): ['event_name', '=', 'event', '[', ']', '\\n']
Tokenized   (011): ['[CLS]', 'event', '_', 'name', '=', 'event', '[', ']', '\\', 'n', '[SEP]']
Filtered   (009): ['event', '_', 'name', '=', 'event', '[', ']', '\\', 'n']
Detokenized (006): ['event_name', '=', 'event', '[', ']', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "event_data = zlib . compress ( pickle . dumps ( event ) ) \n"
Original    (014): ['event_data', '=', 'zlib', '.', 'compress', '(', 'pickle', '.', 'dumps', '(', 'event', ')', ')', '\\n']
Tokenized   (023): ['[CLS]', 'event', '_', 'data', '=', 'z', '##lib', '.', 'com', '##press', '(', 'pick', '##le', '.', 'dump', '##s', '(', 'event', ')', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['event', '_', 'data', '=', 'z', '##lib', '.', 'com', '##press', '(', 'pick', '##le', '.', 'dump', '##s', '(', 'event', ')', ')', '\\', 'n']
Detokenized (014): ['event_data', '=', 'z##lib', '.', 'com##press', '(', 'pick##le', '.', 'dump##s', '(', 'event', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "path_only , query = self . _split_path ( path ) \n"
Original    (011): ['path_only', ',', 'query', '=', 'self', '.', '_split_path', '(', 'path', ')', '\\n']
Tokenized   (019): ['[CLS]', 'path', '_', 'only', ',', 'query', '=', 'self', '.', '_', 'split', '_', 'path', '(', 'path', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['path', '_', 'only', ',', 'query', '=', 'self', '.', '_', 'split', '_', 'path', '(', 'path', ')', '\\', 'n']
Detokenized (011): ['path_only', ',', 'query', '=', 'self', '.', '_split_path', '(', 'path', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "break ; \n"
Original    (003): ['break', ';', '\\n']
Tokenized   (006): ['[CLS]', 'break', ';', '\\', 'n', '[SEP]']
Filtered   (004): ['break', ';', '\\', 'n']
Detokenized (003): ['break', ';', '\\n']
Counter: 4
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "u . email = user [ 2 ] \n"
Original    (009): ['u', '.', 'email', '=', 'user', '[', '2', ']', '\\n']
Tokenized   (012): ['[CLS]', 'u', '.', 'email', '=', 'user', '[', '2', ']', '\\', 'n', '[SEP]']
Filtered   (010): ['u', '.', 'email', '=', 'user', '[', '2', ']', '\\', 'n']
Detokenized (009): ['u', '.', 'email', '=', 'user', '[', '2', ']', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "trac_components = list ( [ ] ) \n"
Original    (008): ['trac_components', '=', 'list', '(', '[', ']', ')', '\\n']
Tokenized   (014): ['[CLS]', 'tr', '##ac', '_', 'components', '=', 'list', '(', '[', ']', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['tr', '##ac', '_', 'components', '=', 'list', '(', '[', ']', ')', '\\', 'n']
Detokenized (008): ['tr##ac_components', '=', 'list', '(', '[', ']', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "component . owner = self . _get_user_login ( component . owner ) \n"
Original    (013): ['component', '.', 'owner', '=', 'self', '.', '_get_user_login', '(', 'component', '.', 'owner', ')', '\\n']
Tokenized   (022): ['[CLS]', 'component', '.', 'owner', '=', 'self', '.', '_', 'get', '_', 'user', '_', 'log', '##in', '(', 'component', '.', 'owner', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['component', '.', 'owner', '=', 'self', '.', '_', 'get', '_', 'user', '_', 'log', '##in', '(', 'component', '.', 'owner', ')', '\\', 'n']
Detokenized (013): ['component', '.', 'owner', '=', 'self', '.', '_get_user_log##in', '(', 'component', '.', 'owner', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "networks [ pkt . pduSource ] . append ( pkt . wirtnNetwork ) \n"
Original    (014): ['networks', '[', 'pkt', '.', 'pduSource', ']', '.', 'append', '(', 'pkt', '.', 'wirtnNetwork', ')', '\\n']
Tokenized   (027): ['[CLS]', 'networks', '[', 'p', '##kt', '.', 'pd', '##uso', '##ur', '##ce', ']', '.', 'app', '##end', '(', 'p', '##kt', '.', 'wi', '##rt', '##nne', '##t', '##work', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['networks', '[', 'p', '##kt', '.', 'pd', '##uso', '##ur', '##ce', ']', '.', 'app', '##end', '(', 'p', '##kt', '.', 'wi', '##rt', '##nne', '##t', '##work', ')', '\\', 'n']
Detokenized (014): ['networks', '[', 'p##kt', '.', 'pd##uso##ur##ce', ']', '.', 'app##end', '(', 'p##kt', '.', 'wi##rt##nne##t##work', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "filterSource = Address ( sys . argv [ i + 1 ] ) \n"
Original    (014): ['filterSource', '=', 'Address', '(', 'sys', '.', 'argv', '[', 'i', '+', '1', ']', ')', '\\n']
Tokenized   (022): ['[CLS]', 'filters', '##our', '##ce', '=', 'address', '(', 'sy', '##s', '.', 'ar', '##g', '##v', '[', 'i', '+', '1', ']', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['filters', '##our', '##ce', '=', 'address', '(', 'sy', '##s', '.', 'ar', '##g', '##v', '[', 'i', '+', '1', ']', ')', '\\', 'n']
Detokenized (014): ['filters##our##ce', '=', 'address', '(', 'sy##s', '.', 'ar##g##v', '[', 'i', '+', '1', ']', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "net_count . sort ( lambda x , y : cmp ( y [ 1 ] , x [ 1 ] ) ) \n"
Original    (023): ['net_count', '.', 'sort', '(', 'lambda', 'x', ',', 'y', ':', 'cmp', '(', 'y', '[', '1', ']', ',', 'x', '[', '1', ']', ')', ')', '\\n']
Tokenized   (029): ['[CLS]', 'net', '_', 'count', '.', 'sort', '(', 'lambda', 'x', ',', 'y', ':', 'cm', '##p', '(', 'y', '[', '1', ']', ',', 'x', '[', '1', ']', ')', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['net', '_', 'count', '.', 'sort', '(', 'lambda', 'x', ',', 'y', ':', 'cm', '##p', '(', 'y', '[', '1', ']', ',', 'x', '[', '1', ']', ')', ')', '\\', 'n']
Detokenized (023): ['net_count', '.', 'sort', '(', 'lambda', 'x', ',', 'y', ':', 'cm##p', '(', 'y', '[', '1', ']', ',', 'x', '[', '1', ']', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "strm = StringIO ( self . pickleBuffer ) \n"
Original    (009): ['strm', '=', 'StringIO', '(', 'self', '.', 'pickleBuffer', ')', '\\n']
Tokenized   (017): ['[CLS]', 'st', '##rm', '=', 'string', '##io', '(', 'self', '.', 'pick', '##le', '##bu', '##ffer', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['st', '##rm', '=', 'string', '##io', '(', 'self', '.', 'pick', '##le', '##bu', '##ffer', ')', '\\', 'n']
Detokenized (009): ['st##rm', '=', 'string##io', '(', 'self', '.', 'pick##le##bu##ffer', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "pdu . pduSource = self . peer \n"
Original    (008): ['pdu', '.', 'pduSource', '=', 'self', '.', 'peer', '\\n']
Tokenized   (015): ['[CLS]', 'pd', '##u', '.', 'pd', '##uso', '##ur', '##ce', '=', 'self', '.', 'peer', '\\', 'n', '[SEP]']
Filtered   (013): ['pd', '##u', '.', 'pd', '##uso', '##ur', '##ce', '=', 'self', '.', 'peer', '\\', 'n']
Detokenized (008): ['pd##u', '.', 'pd##uso##ur##ce', '=', 'self', '.', 'peer', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "connect_task . install_task ( _time ( ) + self . reconnect [ actor . peer ] ) \n"
Original    (018): ['connect_task', '.', 'install_task', '(', '_time', '(', ')', '+', 'self', '.', 'reconnect', '[', 'actor', '.', 'peer', ']', ')', '\\n']
Tokenized   (028): ['[CLS]', 'connect', '_', 'task', '.', 'install', '_', 'task', '(', '_', 'time', '(', ')', '+', 'self', '.', 'rec', '##onne', '##ct', '[', 'actor', '.', 'peer', ']', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['connect', '_', 'task', '.', 'install', '_', 'task', '(', '_', 'time', '(', ')', '+', 'self', '.', 'rec', '##onne', '##ct', '[', 'actor', '.', 'peer', ']', ')', '\\', 'n']
Detokenized (018): ['connect_task', '.', 'install_task', '(', '_time', '(', ')', '+', 'self', '.', 'rec##onne##ct', '[', 'actor', '.', 'peer', ']', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "asyncore . dispatcher . __init__ ( self , sock ) \n"
Original    (011): ['asyncore', '.', 'dispatcher', '.', '__init__', '(', 'self', ',', 'sock', ')', '\\n']
Tokenized   (022): ['[CLS]', 'as', '##yn', '##core', '.', 'dispatch', '##er', '.', '_', '_', 'in', '##it', '_', '_', '(', 'self', ',', 'sock', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['as', '##yn', '##core', '.', 'dispatch', '##er', '.', '_', '_', 'in', '##it', '_', '_', '(', 'self', ',', 'sock', ')', '\\', 'n']
Detokenized (011): ['as##yn##core', '.', 'dispatch##er', '.', '__in##it__', '(', 'self', ',', 'sock', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "TCPServerDirector . _warning ( , err ) \n"
Original    (008): ['TCPServerDirector', '.', '_warning', '(', ',', 'err', ')', '\\n']
Tokenized   (019): ['[CLS]', 'tc', '##pse', '##r', '##ver', '##di', '##re', '##ctor', '.', '_', 'warning', '(', ',', 'er', '##r', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['tc', '##pse', '##r', '##ver', '##di', '##re', '##ctor', '.', '_', 'warning', '(', ',', 'er', '##r', ')', '\\', 'n']
Detokenized (008): ['tc##pse##r##ver##di##re##ctor', '.', '_warning', '(', ',', 'er##r', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "buff = packet [ 1 ] \n"
Original    (007): ['buff', '=', 'packet', '[', '1', ']', '\\n']
Tokenized   (010): ['[CLS]', 'buff', '=', 'packet', '[', '1', ']', '\\', 'n', '[SEP]']
Filtered   (008): ['buff', '=', 'packet', '[', '1', ']', '\\', 'n']
Detokenized (007): ['buff', '=', 'packet', '[', '1', ']', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "fileIdentifier = ( obj_type , obj_inst ) , \n"
Original    (009): ['fileIdentifier', '=', '(', 'obj_type', ',', 'obj_inst', ')', ',', '\\n']
Tokenized   (023): ['[CLS]', 'file', '##ide', '##nti', '##fi', '##er', '=', '(', 'ob', '##j', '_', 'type', ',', 'ob', '##j', '_', 'ins', '##t', ')', ',', '\\', 'n', '[SEP]']
Filtered   (021): ['file', '##ide', '##nti', '##fi', '##er', '=', '(', 'ob', '##j', '_', 'type', ',', 'ob', '##j', '_', 'ins', '##t', ')', ',', '\\', 'n']
Detokenized (009): ['file##ide##nti##fi##er', '=', '(', 'ob##j_type', ',', 'ob##j_ins##t', ')', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "record_data = list ( args [ 4 : ] ) \n"
Original    (011): ['record_data', '=', 'list', '(', 'args', '[', '4', ':', ']', ')', '\\n']
Tokenized   (017): ['[CLS]', 'record', '_', 'data', '=', 'list', '(', 'ar', '##gs', '[', '4', ':', ']', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['record', '_', 'data', '=', 'list', '(', 'ar', '##gs', '[', '4', ':', ']', ')', '\\', 'n']
Detokenized (011): ['record_data', '=', 'list', '(', 'ar##gs', '[', '4', ':', ']', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "accessMethod = AtomicWriteFileRequestAccessMethodChoice ( \n"
Original    (005): ['accessMethod', '=', 'AtomicWriteFileRequestAccessMethodChoice', '(', '\\n']
Tokenized   (023): ['[CLS]', 'access', '##met', '##ho', '##d', '=', 'atomic', '##write', '##fi', '##ler', '##e', '##quest', '##ac', '##ces', '##sm', '##eth', '##od', '##cho', '##ice', '(', '\\', 'n', '[SEP]']
Filtered   (021): ['access', '##met', '##ho', '##d', '=', 'atomic', '##write', '##fi', '##ler', '##e', '##quest', '##ac', '##ces', '##sm', '##eth', '##od', '##cho', '##ice', '(', '\\', 'n']
Detokenized (005): ['access##met##ho##d', '=', 'atomic##write##fi##ler##e##quest##ac##ces##sm##eth##od##cho##ice', '(', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "objectIdentifier = int ( args . ini . objectidentifier ) , \n"
Original    (012): ['objectIdentifier', '=', 'int', '(', 'args', '.', 'ini', '.', 'objectidentifier', ')', ',', '\\n']
Tokenized   (025): ['[CLS]', 'object', '##ide', '##nti', '##fi', '##er', '=', 'int', '(', 'ar', '##gs', '.', 'in', '##i', '.', 'object', '##ide', '##nti', '##fi', '##er', ')', ',', '\\', 'n', '[SEP]']
Filtered   (023): ['object', '##ide', '##nti', '##fi', '##er', '=', 'int', '(', 'ar', '##gs', '.', 'in', '##i', '.', 'object', '##ide', '##nti', '##fi', '##er', ')', ',', '\\', 'n']
Detokenized (012): ['object##ide##nti##fi##er', '=', 'int', '(', 'ar##gs', '.', 'in##i', '.', 'object##ide##nti##fi##er', ')', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "this_application = TestApplication ( this_device , args . ini . address ) \n"
Original    (013): ['this_application', '=', 'TestApplication', '(', 'this_device', ',', 'args', '.', 'ini', '.', 'address', ')', '\\n']
Tokenized   (025): ['[CLS]', 'this', '_', 'application', '=', 'test', '##app', '##lica', '##tion', '(', 'this', '_', 'device', ',', 'ar', '##gs', '.', 'in', '##i', '.', 'address', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['this', '_', 'application', '=', 'test', '##app', '##lica', '##tion', '(', 'this', '_', 'device', ',', 'ar', '##gs', '.', 'in', '##i', '.', 'address', ')', '\\', 'n']
Detokenized (013): ['this_application', '=', 'test##app##lica##tion', '(', 'this_device', ',', 'ar##gs', '.', 'in##i', '.', 'address', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "_log . debug ( "running" ) \n"
Original    (007): ['_log', '.', 'debug', '(', '"running"', ')', '\\n']
Tokenized   (015): ['[CLS]', '_', 'log', '.', 'de', '##bu', '##g', '(', '"', 'running', '"', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['_', 'log', '.', 'de', '##bu', '##g', '(', '"', 'running', '"', ')', '\\', 'n']
Detokenized (007): ['_log', '.', 'de##bu##g', '(', '"running"', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "Status . FAILED_TO_ADD_TO_CLIENT : % ( COLOR_FAILED_TO_ADD_TO_CLIENT , Color . ENDC ) , \n"
Original    (014): ['Status', '.', 'FAILED_TO_ADD_TO_CLIENT', ':', '%', '(', 'COLOR_FAILED_TO_ADD_TO_CLIENT', ',', 'Color', '.', 'ENDC', ')', ',', '\\n']
Tokenized   (036): ['[CLS]', 'status', '.', 'failed', '_', 'to', '_', 'add', '_', 'to', '_', 'client', ':', '%', '(', 'color', '_', 'failed', '_', 'to', '_', 'add', '_', 'to', '_', 'client', ',', 'color', '.', 'end', '##c', ')', ',', '\\', 'n', '[SEP]']
Filtered   (034): ['status', '.', 'failed', '_', 'to', '_', 'add', '_', 'to', '_', 'client', ':', '%', '(', 'color', '_', 'failed', '_', 'to', '_', 'add', '_', 'to', '_', 'client', ',', 'color', '.', 'end', '##c', ')', ',', '\\', 'n']
Detokenized (014): ['status', '.', 'failed_to_add_to_client', ':', '%', '(', 'color_failed_to_add_to_client', ',', 'color', '.', 'end##c', ')', ',', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "end_size += f [ ] \n"
Original    (006): ['end_size', '+=', 'f', '[', ']', '\\n']
Tokenized   (012): ['[CLS]', 'end', '_', 'size', '+', '=', 'f', '[', ']', '\\', 'n', '[SEP]']
Filtered   (010): ['end', '_', 'size', '+', '=', 'f', '[', ']', '\\', 'n']
Detokenized (006): ['end_size', '+=', 'f', '[', ']', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "files_to_check += self . db . find_hash_varying_size ( f [ ] ) \n"
Original    (013): ['files_to_check', '+=', 'self', '.', 'db', '.', 'find_hash_varying_size', '(', 'f', '[', ']', ')', '\\n']
Tokenized   (027): ['[CLS]', 'files', '_', 'to', '_', 'check', '+', '=', 'self', '.', 'db', '.', 'find', '_', 'hash', '_', 'varying', '_', 'size', '(', 'f', '[', ']', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['files', '_', 'to', '_', 'check', '+', '=', 'self', '.', 'db', '.', 'find', '_', 'hash', '_', 'varying', '_', 'size', '(', 'f', '[', ']', ')', '\\', 'n']
Detokenized (013): ['files_to_check', '+=', 'self', '.', 'db', '.', 'find_hash_varying_size', '(', 'f', '[', ']', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "match_start , match_end = pieces . match_file ( db_file , start_size , end_size ) \n"
Original    (015): ['match_start', ',', 'match_end', '=', 'pieces', '.', 'match_file', '(', 'db_file', ',', 'start_size', ',', 'end_size', ')', '\\n']
Tokenized   (030): ['[CLS]', 'match', '_', 'start', ',', 'match', '_', 'end', '=', 'pieces', '.', 'match', '_', 'file', '(', 'db', '_', 'file', ',', 'start', '_', 'size', ',', 'end', '_', 'size', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['match', '_', 'start', ',', 'match', '_', 'end', '=', 'pieces', '.', 'match', '_', 'file', '(', 'db', '_', 'file', ',', 'start', '_', 'size', ',', 'end', '_', 'size', ')', '\\', 'n']
Detokenized (015): ['match_start', ',', 'match_end', '=', 'pieces', '.', 'match_file', '(', 'db_file', ',', 'start_size', ',', 'end_size', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "path_files [ os . path . join ( * path ) ] . append ( { \n"
Original    (017): ['path_files', '[', 'os', '.', 'path', '.', 'join', '(', '*', 'path', ')', ']', '.', 'append', '(', '{', '\\n']
Tokenized   (023): ['[CLS]', 'path', '_', 'files', '[', 'os', '.', 'path', '.', 'join', '(', '*', 'path', ')', ']', '.', 'app', '##end', '(', '{', '\\', 'n', '[SEP]']
Filtered   (021): ['path', '_', 'files', '[', 'os', '.', 'path', '.', 'join', '(', '*', 'path', ')', ']', '.', 'app', '##end', '(', '{', '\\', 'n']
Detokenized (017): ['path_files', '[', 'os', '.', 'path', '.', 'join', '(', '*', 'path', ')', ']', '.', 'app##end', '(', '{', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "files_sorted [ . join ( orig_path ) ] = i \n"
Original    (011): ['files_sorted', '[', '.', 'join', '(', 'orig_path', ')', ']', '=', 'i', '\\n']
Tokenized   (019): ['[CLS]', 'files', '_', 'sorted', '[', '.', 'join', '(', 'or', '##ig', '_', 'path', ')', ']', '=', 'i', '\\', 'n', '[SEP]']
Filtered   (017): ['files', '_', 'sorted', '[', '.', 'join', '(', 'or', '##ig', '_', 'path', ')', ']', '=', 'i', '\\', 'n']
Detokenized (011): ['files_sorted', '[', '.', 'join', '(', 'or##ig_path', ')', ']', '=', 'i', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "found_size , missing_size = 0 , 0 \n"
Original    (008): ['found_size', ',', 'missing_size', '=', '0', ',', '0', '\\n']
Tokenized   (015): ['[CLS]', 'found', '_', 'size', ',', 'missing', '_', 'size', '=', '0', ',', '0', '\\', 'n', '[SEP]']
Filtered   (013): ['found', '_', 'size', ',', 'missing', '_', 'size', '=', '0', ',', '0', '\\', 'n']
Detokenized (008): ['found_size', ',', 'missing_size', '=', '0', ',', '0', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "output_fp . write ( * write_bytes ) \n"
Original    (008): ['output_fp', '.', 'write', '(', '*', 'write_bytes', ')', '\\n']
Tokenized   (016): ['[CLS]', 'output', '_', 'f', '##p', '.', 'write', '(', '*', 'write', '_', 'bytes', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['output', '_', 'f', '##p', '.', 'write', '(', '*', 'write', '_', 'bytes', ')', '\\', 'n']
Detokenized (008): ['output_f##p', '.', 'write', '(', '*', 'write_bytes', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "bytes_written += read_bytes \n"
Original    (004): ['bytes_written', '+=', 'read_bytes', '\\n']
Tokenized   (012): ['[CLS]', 'bytes', '_', 'written', '+', '=', 'read', '_', 'bytes', '\\', 'n', '[SEP]']
Filtered   (010): ['bytes', '_', 'written', '+', '=', 'read', '_', 'bytes', '\\', 'n']
Detokenized (004): ['bytes_written', '+=', 'read_bytes', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "missing_percent = ( missing_size / ( found_size + missing_size ) ) * 100 \n"
Original    (014): ['missing_percent', '=', '(', 'missing_size', '/', '(', 'found_size', '+', 'missing_size', ')', ')', '*', '100', '\\n']
Tokenized   (025): ['[CLS]', 'missing', '_', 'percent', '=', '(', 'missing', '_', 'size', '/', '(', 'found', '_', 'size', '+', 'missing', '_', 'size', ')', ')', '*', '100', '\\', 'n', '[SEP]']
Filtered   (023): ['missing', '_', 'percent', '=', '(', 'missing', '_', 'size', '/', '(', 'found', '_', 'size', '+', 'missing', '_', 'size', ')', ')', '*', '100', '\\', 'n']
Detokenized (014): ['missing_percent', '=', '(', 'missing_size', '/', '(', 'found_size', '+', 'missing_size', ')', ')', '*', '100', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "found_percent = 100 - missing_percent \n"
Original    (006): ['found_percent', '=', '100', '-', 'missing_percent', '\\n']
Tokenized   (013): ['[CLS]', 'found', '_', 'percent', '=', '100', '-', 'missing', '_', 'percent', '\\', 'n', '[SEP]']
Filtered   (011): ['found', '_', 'percent', '=', '100', '-', 'missing', '_', 'percent', '\\', 'n']
Detokenized (006): ['found_percent', '=', '100', '-', 'missing_percent', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "would_not_add = missing_size and missing_percent > self . add_limit_percent or missing_size > \n"
Original    (013): ['would_not_add', '=', 'missing_size', 'and', 'missing_percent', '>', 'self', '.', 'add_limit_percent', 'or', 'missing_size', '>', '\\n']
Tokenized   (030): ['[CLS]', 'would', '_', 'not', '_', 'add', '=', 'missing', '_', 'size', 'and', 'missing', '_', 'percent', '>', 'self', '.', 'add', '_', 'limit', '_', 'percent', 'or', 'missing', '_', 'size', '>', '\\', 'n', '[SEP]']
Filtered   (028): ['would', '_', 'not', '_', 'add', '=', 'missing', '_', 'size', 'and', 'missing', '_', 'percent', '>', 'self', '.', 'add', '_', 'limit', '_', 'percent', 'or', 'missing', '_', 'size', '>', '\\', 'n']
Detokenized (013): ['would_not_add', '=', 'missing_size', 'and', 'missing_percent', '>', 'self', '.', 'add_limit_percent', 'or', 'missing_size', '>', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "LEGO_PALETTE = ( , , , , , , ) \n"
Original    (011): ['LEGO_PALETTE', '=', '(', ',', ',', ',', ',', ',', ',', ')', '\\n']
Tokenized   (016): ['[CLS]', 'lego', '_', 'palette', '=', '(', ',', ',', ',', ',', ',', ',', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['lego', '_', 'palette', '=', '(', ',', ',', ',', ',', ',', ',', ')', '\\', 'n']
Detokenized (011): ['lego_palette', '=', '(', ',', ',', ',', ',', ',', ',', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "Draft4Validator , RefResolver , create , extend , validator_for , validate , \n"
Original    (013): ['Draft4Validator', ',', 'RefResolver', ',', 'create', ',', 'extend', ',', 'validator_for', ',', 'validate', ',', '\\n']
Tokenized   (027): ['[CLS]', 'draft', '##4', '##val', '##ida', '##tor', ',', 'ref', '##res', '##ol', '##ver', ',', 'create', ',', 'extend', ',', 'valid', '##ator', '_', 'for', ',', 'valid', '##ate', ',', '\\', 'n', '[SEP]']
Filtered   (025): ['draft', '##4', '##val', '##ida', '##tor', ',', 'ref', '##res', '##ol', '##ver', ',', 'create', ',', 'extend', ',', 'valid', '##ator', '_', 'for', ',', 'valid', '##ate', ',', '\\', 'n']
Detokenized (013): ['draft##4##val##ida##tor', ',', 'ref##res##ol##ver', ',', 'create', ',', 'extend', ',', 'valid##ator_for', ',', 'valid##ate', ',', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "u"enum" : [ [ "a" , "b" , "c" ] , [ "d" , "e" , "f" ] ] , \n"
Original    (021): ['u"enum"', ':', '[', '[', '"a"', ',', '"b"', ',', '"c"', ']', ',', '[', '"d"', ',', '"e"', ',', '"f"', ']', ']', ',', '\\n']
Tokenized   (040): ['[CLS]', 'u', '"', 'en', '##um', '"', ':', '[', '[', '"', 'a', '"', ',', '"', 'b', '"', ',', '"', 'c', '"', ']', ',', '[', '"', 'd', '"', ',', '"', 'e', '"', ',', '"', 'f', '"', ']', ']', ',', '\\', 'n', '[SEP]']
Filtered   (038): ['u', '"', 'en', '##um', '"', ':', '[', '[', '"', 'a', '"', ',', '"', 'b', '"', ',', '"', 'c', '"', ']', ',', '[', '"', 'd', '"', ',', '"', 'e', '"', ',', '"', 'f', '"', ']', ']', ',', '\\', 'n']
Detokenized (021): ['u"en##um"', ':', '[', '[', '"a"', ',', '"b"', ',', '"c"', ']', ',', '[', '"d"', ',', '"e"', ',', '"f"', ']', ']', ',', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "got = ( e . message for e in self . validator . iter_errors ( instance , schema ) ) \n"
Original    (021): ['got', '=', '(', 'e', '.', 'message', 'for', 'e', 'in', 'self', '.', 'validator', '.', 'iter_errors', '(', 'instance', ',', 'schema', ')', ')', '\\n']
Tokenized   (029): ['[CLS]', 'got', '=', '(', 'e', '.', 'message', 'for', 'e', 'in', 'self', '.', 'valid', '##ator', '.', 'it', '##er', '_', 'errors', '(', 'instance', ',', 'sc', '##hema', ')', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['got', '=', '(', 'e', '.', 'message', 'for', 'e', 'in', 'self', '.', 'valid', '##ator', '.', 'it', '##er', '_', 'errors', '(', 'instance', ',', 'sc', '##hema', ')', ')', '\\', 'n']
Detokenized (021): ['got', '=', '(', 'e', '.', 'message', 'for', 'e', 'in', 'self', '.', 'valid##ator', '.', 'it##er_errors', '(', 'instance', ',', 'sc##hema', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "checker . checks ( u"thing" ) ( check_fn ) \n"
Original    (010): ['checker', '.', 'checks', '(', 'u"thing"', ')', '(', 'check_fn', ')', '\\n']
Tokenized   (020): ['[CLS]', 'check', '##er', '.', 'checks', '(', 'u', '"', 'thing', '"', ')', '(', 'check', '_', 'f', '##n', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['check', '##er', '.', 'checks', '(', 'u', '"', 'thing', '"', ')', '(', 'check', '_', 'f', '##n', ')', '\\', 'n']
Detokenized (010): ['check##er', '.', 'checks', '(', 'u"thing"', ')', '(', 'check_f##n', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "deque ( [ "type" , 1 , "properties" , "foo" , "enum" ] ) , \n"
Original    (016): ['deque', '(', '[', '"type"', ',', '1', ',', '"properties"', ',', '"foo"', ',', '"enum"', ']', ')', ',', '\\n']
Tokenized   (029): ['[CLS]', 'de', '##que', '(', '[', '"', 'type', '"', ',', '1', ',', '"', 'properties', '"', ',', '"', 'foo', '"', ',', '"', 'en', '##um', '"', ']', ')', ',', '\\', 'n', '[SEP]']
Filtered   (027): ['de', '##que', '(', '[', '"', 'type', '"', ',', '1', ',', '"', 'properties', '"', ',', '"', 'foo', '"', ',', '"', 'en', '##um', '"', ']', ')', ',', '\\', 'n']
Detokenized (016): ['de##que', '(', '[', '"type"', ',', '1', ',', '"properties"', ',', '"foo"', ',', '"en##um"', ']', ')', ',', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : ""baz" : { "minItems" : 2 } , \n"
Original    (009): ['"baz"', ':', '{', '"minItems"', ':', '2', '}', ',', '\\n']
Tokenized   (019): ['[CLS]', '"', 'ba', '##z', '"', ':', '{', '"', 'mini', '##tem', '##s', '"', ':', '2', '}', ',', '\\', 'n', '[SEP]']
Filtered   (017): ['"', 'ba', '##z', '"', ':', '{', '"', 'mini', '##tem', '##s', '"', ':', '2', '}', ',', '\\', 'n']
Detokenized (009): ['"ba##z"', ':', '{', '"mini##tem##s"', ':', '2', '}', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ""required" : [ "root" ] , \n"
Original    (007): ['"required"', ':', '[', '"root"', ']', ',', '\\n']
Tokenized   (014): ['[CLS]', '"', 'required', '"', ':', '[', '"', 'root', '"', ']', ',', '\\', 'n', '[SEP]']
Filtered   (012): ['"', 'required', '"', ':', '[', '"', 'root', '"', ']', ',', '\\', 'n']
Detokenized (007): ['"required"', ':', '[', '"root"', ']', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "e2 . absolute_schema_path , deque ( \n"
Original    (007): ['e2', '.', 'absolute_schema_path', ',', 'deque', '(', '\\n']
Tokenized   (017): ['[CLS]', 'e', '##2', '.', 'absolute', '_', 'sc', '##hema', '_', 'path', ',', 'de', '##que', '(', '\\', 'n', '[SEP]']
Filtered   (015): ['e', '##2', '.', 'absolute', '_', 'sc', '##hema', '_', 'path', ',', 'de', '##que', '(', '\\', 'n']
Detokenized (007): ['e##2', '.', 'absolute_sc##hema_path', ',', 'de##que', '(', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""additionalProperties" : { "type" : "integer" , "minimum" : 5 } \n"
Original    (012): ['"additionalProperties"', ':', '{', '"type"', ':', '"integer"', ',', '"minimum"', ':', '5', '}', '\\n']
Tokenized   (026): ['[CLS]', '"', 'additional', '##pro', '##per', '##ties', '"', ':', '{', '"', 'type', '"', ':', '"', 'integer', '"', ',', '"', 'minimum', '"', ':', '5', '}', '\\', 'n', '[SEP]']
Filtered   (024): ['"', 'additional', '##pro', '##per', '##ties', '"', ':', '{', '"', 'type', '"', ':', '"', 'integer', '"', ',', '"', 'minimum', '"', ':', '5', '}', '\\', 'n']
Detokenized (012): ['"additional##pro##per##ties"', ':', '{', '"type"', ':', '"integer"', ',', '"minimum"', ':', '5', '}', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : ""bar" : { "type" : "string" } , \n"
Original    (009): ['"bar"', ':', '{', '"type"', ':', '"string"', '}', ',', '\\n']
Tokenized   (018): ['[CLS]', '"', 'bar', '"', ':', '{', '"', 'type', '"', ':', '"', 'string', '"', '}', ',', '\\', 'n', '[SEP]']
Filtered   (016): ['"', 'bar', '"', ':', '{', '"', 'type', '"', ':', '"', 'string', '"', '}', ',', '\\', 'n']
Detokenized (009): ['"bar"', ':', '{', '"type"', ':', '"string"', '}', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ""foo" : { "minimum" : 5 } \n"
Original    (008): ['"foo"', ':', '{', '"minimum"', ':', '5', '}', '\\n']
Tokenized   (015): ['[CLS]', '"', 'foo', '"', ':', '{', '"', 'minimum', '"', ':', '5', '}', '\\', 'n', '[SEP]']
Filtered   (013): ['"', 'foo', '"', ':', '{', '"', 'minimum', '"', ':', '5', '}', '\\', 'n']
Detokenized (008): ['"foo"', ':', '{', '"minimum"', ':', '5', '}', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : ""items" : [ { } ] , \n"
Original    (008): ['"items"', ':', '[', '{', '}', ']', ',', '\\n']
Tokenized   (013): ['[CLS]', '"', 'items', '"', ':', '[', '{', '}', ']', ',', '\\', 'n', '[SEP]']
Filtered   (011): ['"', 'items', '"', ':', '[', '{', '}', ']', ',', '\\', 'n']
Detokenized (008): ['"items"', ':', '[', '{', '}', ']', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "validate ( instance = instance , schema = { my_property : my_value } ) \n"
Original    (015): ['validate', '(', 'instance', '=', 'instance', ',', 'schema', '=', '{', 'my_property', ':', 'my_value', '}', ')', '\\n']
Tokenized   (024): ['[CLS]', 'valid', '##ate', '(', 'instance', '=', 'instance', ',', 'sc', '##hema', '=', '{', 'my', '_', 'property', ':', 'my', '_', 'value', '}', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['valid', '##ate', '(', 'instance', '=', 'instance', ',', 'sc', '##hema', '=', '{', 'my', '_', 'property', ':', 'my', '_', 'value', '}', ')', '\\', 'n']
Detokenized (015): ['valid##ate', '(', 'instance', '=', 'instance', ',', 'sc##hema', '=', '{', 'my_property', ':', 'my_value', '}', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "chk_schema . assert_called_once_with ( { } ) \n"
Original    (008): ['chk_schema', '.', 'assert_called_once_with', '(', '{', '}', ')', '\\n']
Tokenized   (021): ['[CLS]', 'ch', '##k', '_', 'sc', '##hema', '.', 'assert', '_', 'called', '_', 'once', '_', 'with', '(', '{', '}', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['ch', '##k', '_', 'sc', '##hema', '.', 'assert', '_', 'called', '_', 'once', '_', 'with', '(', '{', '}', ')', '\\', 'n']
Detokenized (008): ['ch##k_sc##hema', '.', 'assert_called_once_with', '(', '{', '}', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "stored_schema = { "stored" : "schema" } \n"
Original    (008): ['stored_schema', '=', '{', '"stored"', ':', '"schema"', '}', '\\n']
Tokenized   (019): ['[CLS]', 'stored', '_', 'sc', '##hema', '=', '{', '"', 'stored', '"', ':', '"', 'sc', '##hema', '"', '}', '\\', 'n', '[SEP]']
Filtered   (017): ['stored', '_', 'sc', '##hema', '=', '{', '"', 'stored', '"', ':', '"', 'sc', '##hema', '"', '}', '\\', 'n']
Detokenized (008): ['stored_sc##hema', '=', '{', '"stored"', ':', '"sc##hema"', '}', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : ""ports" : \n"
Original    (003): ['"ports"', ':', '\\n']
Tokenized   (008): ['[CLS]', '"', 'ports', '"', ':', '\\', 'n', '[SEP]']
Filtered   (006): ['"', 'ports', '"', ':', '\\', 'n']
Detokenized (003): ['"ports"', ':', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "l2Report . generateReport ( pod . id , True , False ) \n"
Original    (013): ['l2Report', '.', 'generateReport', '(', 'pod', '.', 'id', ',', 'True', ',', 'False', ')', '\\n']
Tokenized   (021): ['[CLS]', 'l', '##2', '##re', '##port', '.', 'generate', '##re', '##port', '(', 'pod', '.', 'id', ',', 'true', ',', 'false', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['l', '##2', '##re', '##port', '.', 'generate', '##re', '##port', '(', 'pod', '.', 'id', ',', 'true', ',', 'false', ')', '\\', 'n']
Detokenized (013): ['l##2##re##port', '.', 'generate##re##port', '(', 'pod', '.', 'id', ',', 'true', ',', 'false', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "_YAML_ = splitext ( __file__ ) [ 0 ] + \n"
Original    (011): ['_YAML_', '=', 'splitext', '(', '__file__', ')', '[', '0', ']', '+', '\\n']
Tokenized   (023): ['[CLS]', '_', 'ya', '##ml', '_', '=', 'split', '##ex', '##t', '(', '_', '_', 'file', '_', '_', ')', '[', '0', ']', '+', '\\', 'n', '[SEP]']
Filtered   (021): ['_', 'ya', '##ml', '_', '=', 'split', '##ex', '##t', '(', '_', '_', 'file', '_', '_', ')', '[', '0', ']', '+', '\\', 'n']
Detokenized (011): ['_ya##ml_', '=', 'split##ex##t', '(', '__file__', ')', '[', '0', ']', '+', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "globals ( ) . update ( loadyaml ( _YAML_ ) ) \n"
Original    (012): ['globals', '(', ')', '.', 'update', '(', 'loadyaml', '(', '_YAML_', ')', ')', '\\n']
Tokenized   (021): ['[CLS]', 'global', '##s', '(', ')', '.', 'update', '(', 'load', '##yam', '##l', '(', '_', 'ya', '##ml', '_', ')', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['global', '##s', '(', ')', '.', 'update', '(', 'load', '##yam', '##l', '(', '_', 'ya', '##ml', '_', ')', ')', '\\', 'n']
Detokenized (012): ['global##s', '(', ')', '.', 'update', '(', 'load##yam##l', '(', '_ya##ml_', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "gather_facts = False ) \n"
Original    (005): ['gather_facts', '=', 'False', ')', '\\n']
Tokenized   (010): ['[CLS]', 'gather', '_', 'facts', '=', 'false', ')', '\\', 'n', '[SEP]']
Filtered   (008): ['gather', '_', 'facts', '=', 'false', ')', '\\', 'n']
Detokenized (005): ['gather_facts', '=', 'false', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "DEFAULT_API_URLS = ( , \n"
Original    (005): ['DEFAULT_API_URLS', '=', '(', ',', '\\n']
Tokenized   (013): ['[CLS]', 'default', '_', 'api', '_', 'ur', '##ls', '=', '(', ',', '\\', 'n', '[SEP]']
Filtered   (011): ['default', '_', 'api', '_', 'ur', '##ls', '=', '(', ',', '\\', 'n']
Detokenized (005): ['default_api_ur##ls', '=', '(', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "BAD_STATUS_CODES = [ , , , \n"
Original    (007): ['BAD_STATUS_CODES', '=', '[', ',', ',', ',', '\\n']
Tokenized   (014): ['[CLS]', 'bad', '_', 'status', '_', 'codes', '=', '[', ',', ',', ',', '\\', 'n', '[SEP]']
Filtered   (012): ['bad', '_', 'status', '_', 'codes', '=', '[', ',', ',', ',', '\\', 'n']
Detokenized (007): ['bad_status_codes', '=', '[', ',', ',', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "translate_otp = True , api_urls = DEFAULT_API_URLS , \n"
Original    (009): ['translate_otp', '=', 'True', ',', 'api_urls', '=', 'DEFAULT_API_URLS', ',', '\\n']
Tokenized   (023): ['[CLS]', 'translate', '_', 'ot', '##p', '=', 'true', ',', 'api', '_', 'ur', '##ls', '=', 'default', '_', 'api', '_', 'ur', '##ls', ',', '\\', 'n', '[SEP]']
Filtered   (021): ['translate', '_', 'ot', '##p', '=', 'true', ',', 'api', '_', 'ur', '##ls', '=', 'default', '_', 'api', '_', 'ur', '##ls', ',', '\\', 'n']
Detokenized (009): ['translate_ot##p', '=', 'true', ',', 'api_ur##ls', '=', 'default_api_ur##ls', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "rand_str = b ( os . urandom ( 30 ) ) \n"
Original    (012): ['rand_str', '=', 'b', '(', 'os', '.', 'urandom', '(', '30', ')', ')', '\\n']
Tokenized   (020): ['[CLS]', 'rand', '_', 'st', '##r', '=', 'b', '(', 'os', '.', 'ur', '##ando', '##m', '(', '30', ')', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['rand', '_', 'st', '##r', '=', 'b', '(', 'os', '.', 'ur', '##ando', '##m', '(', '30', ')', ')', '\\', 'n']
Detokenized (012): ['rand_st##r', '=', 'b', '(', 'os', '.', 'ur##ando##m', '(', '30', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "nonce = base64 . b64encode ( rand_str , b ( ) ) [ : 25 ] . decode ( ) \n"
Original    (021): ['nonce', '=', 'base64', '.', 'b64encode', '(', 'rand_str', ',', 'b', '(', ')', ')', '[', ':', '25', ']', '.', 'decode', '(', ')', '\\n']
Tokenized   (033): ['[CLS]', 'non', '##ce', '=', 'base', '##64', '.', 'b', '##64', '##en', '##code', '(', 'rand', '_', 'st', '##r', ',', 'b', '(', ')', ')', '[', ':', '25', ']', '.', 'deco', '##de', '(', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['non', '##ce', '=', 'base', '##64', '.', 'b', '##64', '##en', '##code', '(', 'rand', '_', 'st', '##r', ',', 'b', '(', ')', ')', '[', ':', '25', ']', '.', 'deco', '##de', '(', ')', '\\', 'n']
Detokenized (021): ['non##ce', '=', 'base##64', '.', 'b##64##en##code', '(', 'rand_st##r', ',', 'b', '(', ')', ')', '[', ':', '25', ']', '.', 'deco##de', '(', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "otp . otp , nonce , \n"
Original    (007): ['otp', '.', 'otp', ',', 'nonce', ',', '\\n']
Tokenized   (013): ['[CLS]', 'ot', '##p', '.', 'ot', '##p', ',', 'non', '##ce', ',', '\\', 'n', '[SEP]']
Filtered   (011): ['ot', '##p', '.', 'ot', '##p', ',', 'non', '##ce', ',', '\\', 'n']
Detokenized (007): ['ot##p', '.', 'ot##p', ',', 'non##ce', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "pairs_string = . join ( [ . join ( pair ) for pair in pairs_sorted ] ) \n"
Original    (018): ['pairs_string', '=', '.', 'join', '(', '[', '.', 'join', '(', 'pair', ')', 'for', 'pair', 'in', 'pairs_sorted', ']', ')', '\\n']
Tokenized   (025): ['[CLS]', 'pairs', '_', 'string', '=', '.', 'join', '(', '[', '.', 'join', '(', 'pair', ')', 'for', 'pair', 'in', 'pairs', '_', 'sorted', ']', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['pairs', '_', 'string', '=', '.', 'join', '(', '[', '.', 'join', '(', 'pair', ')', 'for', 'pair', 'in', 'pairs', '_', 'sorted', ']', ')', '\\', 'n']
Detokenized (018): ['pairs_string', '=', '.', 'join', '(', '[', '.', 'join', '(', 'pair', ')', 'for', 'pair', 'in', 'pairs_sorted', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "digest = hmac . new ( self . key , b ( pairs_string ) , hashlib . sha1 ) . digest ( ) \n"
Original    (024): ['digest', '=', 'hmac', '.', 'new', '(', 'self', '.', 'key', ',', 'b', '(', 'pairs_string', ')', ',', 'hashlib', '.', 'sha1', ')', '.', 'digest', '(', ')', '\\n']
Tokenized   (032): ['[CLS]', 'digest', '=', 'hm', '##ac', '.', 'new', '(', 'self', '.', 'key', ',', 'b', '(', 'pairs', '_', 'string', ')', ',', 'hash', '##lib', '.', 'sha', '##1', ')', '.', 'digest', '(', ')', '\\', 'n', '[SEP]']
Filtered   (030): ['digest', '=', 'hm', '##ac', '.', 'new', '(', 'self', '.', 'key', ',', 'b', '(', 'pairs', '_', 'string', ')', ',', 'hash', '##lib', '.', 'sha', '##1', ')', '.', 'digest', '(', ')', '\\', 'n']
Detokenized (024): ['digest', '=', 'hm##ac', '.', 'new', '(', 'self', '.', 'key', ',', 'b', '(', 'pairs_string', ')', ',', 'hash##lib', '.', 'sha##1', ')', '.', 'digest', '(', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "signature = ( [ unquote ( v ) for k , v in pairs if k == ] or [ None ] ) [ 0 ] \n"
Original    (027): ['signature', '=', '(', '[', 'unquote', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '==', ']', 'or', '[', 'None', ']', ')', '[', '0', ']', '\\n']
Tokenized   (033): ['[CLS]', 'signature', '=', '(', '[', 'un', '##qu', '##ote', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '=', '=', ']', 'or', '[', 'none', ']', ')', '[', '0', ']', '\\', 'n', '[SEP]']
Filtered   (031): ['signature', '=', '(', '[', 'un', '##qu', '##ote', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '=', '=', ']', 'or', '[', 'none', ']', ')', '[', '0', ']', '\\', 'n']
Detokenized (027): ['signature', '=', '(', '[', 'un##qu##ote', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '==', ']', 'or', '[', 'none', ']', ')', '[', '0', ']', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "query_string = . join ( [ k + + v for k , v in pairs if k != ] ) \n"
Original    (022): ['query_string', '=', '.', 'join', '(', '[', 'k', '+', '+', 'v', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '!=', ']', ')', '\\n']
Tokenized   (028): ['[CLS]', 'query', '_', 'string', '=', '.', 'join', '(', '[', 'k', '+', '+', 'v', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '!', '=', ']', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['query', '_', 'string', '=', '.', 'join', '(', '[', 'k', '+', '+', 'v', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '!', '=', ']', ')', '\\', 'n']
Detokenized (022): ['query_string', '=', '.', 'join', '(', '[', 'k', '+', '+', 'v', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '!=', ']', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "pairs = ( x . split ( , 1 ) for x in query_string . split ( ) ) \n"
Original    (020): ['pairs', '=', '(', 'x', '.', 'split', '(', ',', '1', ')', 'for', 'x', 'in', 'query_string', '.', 'split', '(', ')', ')', '\\n']
Tokenized   (025): ['[CLS]', 'pairs', '=', '(', 'x', '.', 'split', '(', ',', '1', ')', 'for', 'x', 'in', 'query', '_', 'string', '.', 'split', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['pairs', '=', '(', 'x', '.', 'split', '(', ',', '1', ')', 'for', 'x', 'in', 'query', '_', 'string', '.', 'split', '(', ')', ')', '\\', 'n']
Detokenized (020): ['pairs', '=', '(', 'x', '.', 'split', '(', ',', '1', ')', 'for', 'x', 'in', 'query_string', '.', 'split', '(', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "py_modules = [ ] , \n"
Original    (006): ['py_modules', '=', '[', ']', ',', '\\n']
Tokenized   (012): ['[CLS]', 'p', '##y', '_', 'modules', '=', '[', ']', ',', '\\', 'n', '[SEP]']
Filtered   (010): ['p', '##y', '_', 'modules', '=', '[', ']', ',', '\\', 'n']
Detokenized (006): ['p##y_modules', '=', '[', ']', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "submitter , msg = result [ 0 ] \n"
Original    (009): ['submitter', ',', 'msg', '=', 'result', '[', '0', ']', '\\n']
Tokenized   (014): ['[CLS]', 'submit', '##ter', ',', 'ms', '##g', '=', 'result', '[', '0', ']', '\\', 'n', '[SEP]']
Filtered   (012): ['submit', '##ter', ',', 'ms', '##g', '=', 'result', '[', '0', ']', '\\', 'n']
Detokenized (009): ['submit##ter', ',', 'ms##g', '=', 'result', '[', '0', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "contact = self . line_interface . _get_contact_by_id ( me . id ) \n"
Original    (013): ['contact', '=', 'self', '.', 'line_interface', '.', '_get_contact_by_id', '(', 'me', '.', 'id', ')', '\\n']
Tokenized   (025): ['[CLS]', 'contact', '=', 'self', '.', 'line', '_', 'interface', '.', '_', 'get', '_', 'contact', '_', 'by', '_', 'id', '(', 'me', '.', 'id', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['contact', '=', 'self', '.', 'line', '_', 'interface', '.', '_', 'get', '_', 'contact', '_', 'by', '_', 'id', '(', 'me', '.', 'id', ')', '\\', 'n']
Detokenized (013): ['contact', '=', 'self', '.', 'line_interface', '.', '_get_contact_by_id', '(', 'me', '.', 'id', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "ok_ ( me_display_name == me . name ) \n"
Original    (009): ['ok_', '(', 'me_display_name', '==', 'me', '.', 'name', ')', '\\n']
Tokenized   (018): ['[CLS]', 'ok', '_', '(', 'me', '_', 'display', '_', 'name', '=', '=', 'me', '.', 'name', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['ok', '_', '(', 'me', '_', 'display', '_', 'name', '=', '=', 'me', '.', 'name', ')', '\\', 'n']
Detokenized (009): ['ok_', '(', 'me_display_name', '==', 'me', '.', 'name', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "transport . get_extra_info . return_value = None \n"
Original    (008): ['transport', '.', 'get_extra_info', '.', 'return_value', '=', 'None', '\\n']
Tokenized   (017): ['[CLS]', 'transport', '.', 'get', '_', 'extra', '_', 'info', '.', 'return', '_', 'value', '=', 'none', '\\', 'n', '[SEP]']
Filtered   (015): ['transport', '.', 'get', '_', 'extra', '_', 'info', '.', 'return', '_', 'value', '=', 'none', '\\', 'n']
Detokenized (008): ['transport', '.', 'get_extra_info', '.', 'return_value', '=', 'none', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "ShortenerSettings = namedtuple ( , [ \n"
Original    (007): ['ShortenerSettings', '=', 'namedtuple', '(', ',', '[', '\\n']
Tokenized   (015): ['[CLS]', 'short', '##ener', '##sett', '##ings', '=', 'named', '##tu', '##ple', '(', ',', '[', '\\', 'n', '[SEP]']
Filtered   (013): ['short', '##ener', '##sett', '##ings', '=', 'named', '##tu', '##ple', '(', ',', '[', '\\', 'n']
Detokenized (007): ['short##ener##sett##ings', '=', 'named##tu##ple', '(', ',', '[', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "right_to_left = [ , ] , \n"
Original    (007): ['right_to_left', '=', '[', ',', ']', ',', '\\n']
Tokenized   (014): ['[CLS]', 'right', '_', 'to', '_', 'left', '=', '[', ',', ']', ',', '\\', 'n', '[SEP]']
Filtered   (012): ['right', '_', 'to', '_', 'left', '=', '[', ',', ']', ',', '\\', 'n']
Detokenized (007): ['right_to_left', '=', '[', ',', ']', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "shortener = { } , \n"
Original    (006): ['shortener', '=', '{', '}', ',', '\\n']
Tokenized   (010): ['[CLS]', 'short', '##ener', '=', '{', '}', ',', '\\', 'n', '[SEP]']
Filtered   (008): ['short', '##ener', '=', '{', '}', ',', '\\', 'n']
Detokenized (006): ['short##ener', '=', '{', '}', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "workers_pool = 10 , \n"
Original    (005): ['workers_pool', '=', '10', ',', '\\n']
Tokenized   (010): ['[CLS]', 'workers', '_', 'pool', '=', '10', ',', '\\', 'n', '[SEP]']
Filtered   (008): ['workers', '_', 'pool', '=', '10', ',', '\\', 'n']
Detokenized (005): ['workers_pool', '=', '10', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "cms_service_host = "http://localhost:5001" \n"
Original    (004): ['cms_service_host', '=', '"http://localhost:5001"', '\\n']
Tokenized   (023): ['[CLS]', 'cm', '##s', '_', 'service', '_', 'host', '=', '"', 'http', ':', '/', '/', 'local', '##hos', '##t', ':', '500', '##1', '"', '\\', 'n', '[SEP]']
Filtered   (021): ['cm', '##s', '_', 'service', '_', 'host', '=', '"', 'http', ':', '/', '/', 'local', '##hos', '##t', ':', '500', '##1', '"', '\\', 'n']
Detokenized (004): ['cm##s_service_host', '=', '"http://local##hos##t:500##1"', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "subparsers = args . add_subparsers ( help = , dest = ) \n"
Original    (013): ['subparsers', '=', 'args', '.', 'add_subparsers', '(', 'help', '=', ',', 'dest', '=', ')', '\\n']
Tokenized   (026): ['[CLS]', 'sub', '##par', '##ser', '##s', '=', 'ar', '##gs', '.', 'add', '_', 'sub', '##par', '##ser', '##s', '(', 'help', '=', ',', 'des', '##t', '=', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['sub', '##par', '##ser', '##s', '=', 'ar', '##gs', '.', 'add', '_', 'sub', '##par', '##ser', '##s', '(', 'help', '=', ',', 'des', '##t', '=', ')', '\\', 'n']
Detokenized (013): ['sub##par##ser##s', '=', 'ar##gs', '.', 'add_sub##par##ser##s', '(', 'help', '=', ',', 'des##t', '=', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "template_parser . add_argument ( , \n"
Original    (006): ['template_parser', '.', 'add_argument', '(', ',', '\\n']
Tokenized   (014): ['[CLS]', 'template', '_', 'par', '##ser', '.', 'add', '_', 'argument', '(', ',', '\\', 'n', '[SEP]']
Filtered   (012): ['template', '_', 'par', '##ser', '.', 'add', '_', 'argument', '(', ',', '\\', 'n']
Detokenized (006): ['template_par##ser', '.', 'add_argument', '(', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "config_parser . add_argument ( , help = ) \n"
Original    (009): ['config_parser', '.', 'add_argument', '(', ',', 'help', '=', ')', '\\n']
Tokenized   (019): ['[CLS]', 'con', '##fi', '##g', '_', 'par', '##ser', '.', 'add', '_', 'argument', '(', ',', 'help', '=', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['con', '##fi', '##g', '_', 'par', '##ser', '.', 'add', '_', 'argument', '(', ',', 'help', '=', ')', '\\', 'n']
Detokenized (009): ['con##fi##g_par##ser', '.', 'add_argument', '(', ',', 'help', '=', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "gui_parser . add_argument ( , , type = str , help = ) \n"
Original    (014): ['gui_parser', '.', 'add_argument', '(', ',', ',', 'type', '=', 'str', ',', 'help', '=', ')', '\\n']
Tokenized   (023): ['[CLS]', 'gui', '_', 'par', '##ser', '.', 'add', '_', 'argument', '(', ',', ',', 'type', '=', 'st', '##r', ',', 'help', '=', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['gui', '_', 'par', '##ser', '.', 'add', '_', 'argument', '(', ',', ',', 'type', '=', 'st', '##r', ',', 'help', '=', ')', '\\', 'n']
Detokenized (014): ['gui_par##ser', '.', 'add_argument', '(', ',', ',', 'type', '=', 'st##r', ',', 'help', '=', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "changePwdResult = conn . extend . microsoft . modify_password ( user_dn , newpassword ) \n"
Original    (015): ['changePwdResult', '=', 'conn', '.', 'extend', '.', 'microsoft', '.', 'modify_password', '(', 'user_dn', ',', 'newpassword', ')', '\\n']
Tokenized   (030): ['[CLS]', 'change', '##pw', '##dre', '##sul', '##t', '=', 'con', '##n', '.', 'extend', '.', 'microsoft', '.', 'modify', '_', 'password', '(', 'user', '_', 'd', '##n', ',', 'new', '##pass', '##word', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['change', '##pw', '##dre', '##sul', '##t', '=', 'con', '##n', '.', 'extend', '.', 'microsoft', '.', 'modify', '_', 'password', '(', 'user', '_', 'd', '##n', ',', 'new', '##pass', '##word', ')', '\\', 'n']
Detokenized (015): ['change##pw##dre##sul##t', '=', 'con##n', '.', 'extend', '.', 'microsoft', '.', 'modify_password', '(', 'user_d##n', ',', 'new##pass##word', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "cap_path = os . path . join ( caps_directory , ) \n"
Original    (012): ['cap_path', '=', 'os', '.', 'path', '.', 'join', '(', 'caps_directory', ',', ')', '\\n']
Tokenized   (019): ['[CLS]', 'cap', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'caps', '_', 'directory', ',', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['cap', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'caps', '_', 'directory', ',', ')', '\\', 'n']
Detokenized (012): ['cap_path', '=', 'os', '.', 'path', '.', 'join', '(', 'caps_directory', ',', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "cap . eventloop . stop ( ) \n"
Original    (008): ['cap', '.', 'eventloop', '.', 'stop', '(', ')', '\\n']
Tokenized   (013): ['[CLS]', 'cap', '.', 'event', '##lo', '##op', '.', 'stop', '(', ')', '\\', 'n', '[SEP]']
Filtered   (011): ['cap', '.', 'event', '##lo', '##op', '.', 'stop', '(', ')', '\\', 'n']
Detokenized (008): ['cap', '.', 'event##lo##op', '.', 'stop', '(', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "flush = Service ( name = , \n"
Original    (008): ['flush', '=', 'Service', '(', 'name', '=', ',', '\\n']
Tokenized   (011): ['[CLS]', 'flush', '=', 'service', '(', 'name', '=', ',', '\\', 'n', '[SEP]']
Filtered   (009): ['flush', '=', 'service', '(', 'name', '=', ',', '\\', 'n']
Detokenized (008): ['flush', '=', 'service', '(', 'name', '=', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "sourceIds = [ d [ ] for d in response . json ] \n"
Original    (014): ['sourceIds', '=', '[', 'd', '[', ']', 'for', 'd', 'in', 'response', '.', 'json', ']', '\\n']
Tokenized   (019): ['[CLS]', 'source', '##ids', '=', '[', 'd', '[', ']', 'for', 'd', 'in', 'response', '.', 'j', '##son', ']', '\\', 'n', '[SEP]']
Filtered   (017): ['source', '##ids', '=', '[', 'd', '[', ']', 'for', 'd', 'in', 'response', '.', 'j', '##son', ']', '\\', 'n']
Detokenized (014): ['source##ids', '=', '[', 'd', '[', ']', 'for', 'd', 'in', 'response', '.', 'j##son', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : ""folder." ) \n"
Original    (003): ['"folder."', ')', '\\n']
Tokenized   (009): ['[CLS]', '"', 'folder', '.', '"', ')', '\\', 'n', '[SEP]']
Filtered   (007): ['"', 'folder', '.', '"', ')', '\\', 'n']
Detokenized (003): ['"folder."', ')', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "minerva_metadata [ ] = { \n"
Original    (006): ['minerva_metadata', '[', ']', '=', '{', '\\n']
Tokenized   (011): ['[CLS]', 'minerva', '_', 'metadata', '[', ']', '=', '{', '\\', 'n', '[SEP]']
Filtered   (009): ['minerva', '_', 'metadata', '[', ']', '=', '{', '\\', 'n']
Detokenized (006): ['minerva_metadata', '[', ']', '=', '{', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "Description ( ) \n"
Original    (004): ['Description', '(', ')', '\\n']
Tokenized   (007): ['[CLS]', 'description', '(', ')', '\\', 'n', '[SEP]']
Filtered   (005): ['description', '(', ')', '\\', 'n']
Detokenized (004): ['description', '(', ')', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "matches = re . findall ( "(\'|\\")(\\S+)(\'|\\")" , text ) \n"
Original    (011): ['matches', '=', 're', '.', 'findall', '(', '"(\\\'|\\\\")(\\\\S+)(\\\'|\\\\")"', ',', 'text', ')', '\\n']
Tokenized   (038): ['[CLS]', 'matches', '=', 're', '.', 'find', '##all', '(', '"', '(', '\\', "'", '|', '\\', '\\', '"', ')', '(', '\\', '\\', 's', '+', ')', '(', '\\', "'", '|', '\\', '\\', '"', ')', '"', ',', 'text', ')', '\\', 'n', '[SEP]']
Filtered   (036): ['matches', '=', 're', '.', 'find', '##all', '(', '"', '(', '\\', "'", '|', '\\', '\\', '"', ')', '(', '\\', '\\', 's', '+', ')', '(', '\\', "'", '|', '\\', '\\', '"', ')', '"', ',', 'text', ')', '\\', 'n']
Detokenized (011): ['matches', '=', 're', '.', 'find##all', '(', '"(\\\'|\\\\")(\\\\s+)(\\\'|\\\\")"', ',', 'text', ')', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "package_data = { : [ ] } , \n"
Original    (009): ['package_data', '=', '{', ':', '[', ']', '}', ',', '\\n']
Tokenized   (014): ['[CLS]', 'package', '_', 'data', '=', '{', ':', '[', ']', '}', ',', '\\', 'n', '[SEP]']
Filtered   (012): ['package', '_', 'data', '=', '{', ':', '[', ']', '}', ',', '\\', 'n']
Detokenized (009): ['package_data', '=', '{', ':', '[', ']', '}', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "list_permissions = [ , , , ] \n"
Original    (008): ['list_permissions', '=', '[', ',', ',', ',', ']', '\\n']
Tokenized   (014): ['[CLS]', 'list', '_', 'permission', '##s', '=', '[', ',', ',', ',', ']', '\\', 'n', '[SEP]']
Filtered   (012): ['list', '_', 'permission', '##s', '=', '[', ',', ',', ',', ']', '\\', 'n']
Detokenized (008): ['list_permission##s', '=', '[', ',', ',', ',', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "option_list = BaseCommand . option_list + ( \n"
Original    (008): ['option_list', '=', 'BaseCommand', '.', 'option_list', '+', '(', '\\n']
Tokenized   (018): ['[CLS]', 'option', '_', 'list', '=', 'base', '##com', '##man', '##d', '.', 'option', '_', 'list', '+', '(', '\\', 'n', '[SEP]']
Filtered   (016): ['option', '_', 'list', '=', 'base', '##com', '##man', '##d', '.', 'option', '_', 'list', '+', '(', '\\', 'n']
Detokenized (008): ['option_list', '=', 'base##com##man##d', '.', 'option_list', '+', '(', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "make_option ( , \n"
Original    (004): ['make_option', '(', ',', '\\n']
Tokenized   (009): ['[CLS]', 'make', '_', 'option', '(', ',', '\\', 'n', '[SEP]']
Filtered   (007): ['make', '_', 'option', '(', ',', '\\', 'n']
Detokenized (004): ['make_option', '(', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "confirm_token = Column ( Unicode ( 100 ) ) \n"
Original    (010): ['confirm_token', '=', 'Column', '(', 'Unicode', '(', '100', ')', ')', '\\n']
Tokenized   (015): ['[CLS]', 'confirm', '_', 'token', '=', 'column', '(', 'unicode', '(', '100', ')', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['confirm', '_', 'token', '=', 'column', '(', 'unicode', '(', '100', ')', ')', '\\', 'n']
Detokenized (010): ['confirm_token', '=', 'column', '(', 'unicode', '(', '100', ')', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "creation_date = Column ( DateTime ( ) , nullable = False ) \n"
Original    (013): ['creation_date', '=', 'Column', '(', 'DateTime', '(', ')', ',', 'nullable', '=', 'False', ')', '\\n']
Tokenized   (020): ['[CLS]', 'creation', '_', 'date', '=', 'column', '(', 'date', '##time', '(', ')', ',', 'null', '##able', '=', 'false', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['creation', '_', 'date', '=', 'column', '(', 'date', '##time', '(', ')', ',', 'null', '##able', '=', 'false', ')', '\\', 'n']
Detokenized (013): ['creation_date', '=', 'column', '(', 'date##time', '(', ')', ',', 'null##able', '=', 'false', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "last_login_date = Column ( DateTime ( ) ) \n"
Original    (009): ['last_login_date', '=', 'Column', '(', 'DateTime', '(', ')', ')', '\\n']
Tokenized   (018): ['[CLS]', 'last', '_', 'log', '##in', '_', 'date', '=', 'column', '(', 'date', '##time', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['last', '_', 'log', '##in', '_', 'date', '=', 'column', '(', 'date', '##time', '(', ')', ')', '\\', 'n']
Detokenized (009): ['last_log##in_date', '=', 'column', '(', 'date##time', '(', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "SHARING_ROLES = [ , , ] \n"
Original    (007): ['SHARING_ROLES', '=', '[', ',', ',', ']', '\\n']
Tokenized   (012): ['[CLS]', 'sharing', '_', 'roles', '=', '[', ',', ',', ']', '\\', 'n', '[SEP]']
Filtered   (010): ['sharing', '_', 'roles', '=', '[', ',', ',', ']', '\\', 'n']
Detokenized (007): ['sharing_roles', '=', '[', ',', ',', ']', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "USER_MANAGEMENT_ROLES = SHARING_ROLES + [ ] \n"
Original    (007): ['USER_MANAGEMENT_ROLES', '=', 'SHARING_ROLES', '+', '[', ']', '\\n']
Tokenized   (016): ['[CLS]', 'user', '_', 'management', '_', 'roles', '=', 'sharing', '_', 'roles', '+', '[', ']', '\\', 'n', '[SEP]']
Filtered   (014): ['user', '_', 'management', '_', 'roles', '=', 'sharing', '_', 'roles', '+', '[', ']', '\\', 'n']
Detokenized (007): ['user_management_roles', '=', 'sharing_roles', '+', '[', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "_DEFAULT_SHARING_ROLES = SHARING_ROLES [ : ] \n"
Original    (007): ['_DEFAULT_SHARING_ROLES', '=', 'SHARING_ROLES', '[', ':', ']', '\\n']
Tokenized   (017): ['[CLS]', '_', 'default', '_', 'sharing', '_', 'roles', '=', 'sharing', '_', 'roles', '[', ':', ']', '\\', 'n', '[SEP]']
Filtered   (015): ['_', 'default', '_', 'sharing', '_', 'roles', '=', 'sharing', '_', 'roles', '[', ':', ']', '\\', 'n']
Detokenized (007): ['_default_sharing_roles', '=', 'sharing_roles', '[', ':', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "principal = get_principals ( ) . get ( name ) \n"
Original    (011): ['principal', '=', 'get_principals', '(', ')', '.', 'get', '(', 'name', ')', '\\n']
Tokenized   (016): ['[CLS]', 'principal', '=', 'get', '_', 'principals', '(', ')', '.', 'get', '(', 'name', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['principal', '=', 'get', '_', 'principals', '(', ')', '.', 'get', '(', 'name', ')', '\\', 'n']
Detokenized (011): ['principal', '=', 'get_principals', '(', ')', '.', 'get', '(', 'name', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "lg for lg in context . local_groups \n"
Original    (008): ['lg', 'for', 'lg', 'in', 'context', '.', 'local_groups', '\\n']
Tokenized   (015): ['[CLS]', 'l', '##g', 'for', 'l', '##g', 'in', 'context', '.', 'local', '_', 'groups', '\\', 'n', '[SEP]']
Filtered   (013): ['l', '##g', 'for', 'l', '##g', 'in', 'context', '.', 'local', '_', 'groups', '\\', 'n']
Detokenized (008): ['l##g', 'for', 'l##g', 'in', 'context', '.', 'local_groups', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "LocalGroup ( context , name , unicode ( group_name ) ) \n"
Original    (012): ['LocalGroup', '(', 'context', ',', 'name', ',', 'unicode', '(', 'group_name', ')', ')', '\\n']
Tokenized   (018): ['[CLS]', 'local', '##group', '(', 'context', ',', 'name', ',', 'unicode', '(', 'group', '_', 'name', ')', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['local', '##group', '(', 'context', ',', 'name', ',', 'unicode', '(', 'group', '_', 'name', ')', ')', '\\', 'n']
Detokenized (012): ['local##group', '(', 'context', ',', 'name', ',', 'unicode', '(', 'group_name', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "filters . append ( func . lower ( col ) . like ( value ) ) \n"
Original    (017): ['filters', '.', 'append', '(', 'func', '.', 'lower', '(', 'col', ')', '.', 'like', '(', 'value', ')', ')', '\\n']
Tokenized   (022): ['[CLS]', 'filters', '.', 'app', '##end', '(', 'fun', '##c', '.', 'lower', '(', 'col', ')', '.', 'like', '(', 'value', ')', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['filters', '.', 'app', '##end', '(', 'fun', '##c', '.', 'lower', '(', 'col', ')', '.', 'like', '(', 'value', ')', ')', '\\', 'n']
Detokenized (017): ['filters', '.', 'app##end', '(', 'fun##c', '.', 'lower', '(', 'col', ')', '.', 'like', '(', 'value', ')', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "bcrypt . hashpw ( password . encode ( ) , hashed . encode ( ) ) ) \n"
Original    (018): ['bcrypt', '.', 'hashpw', '(', 'password', '.', 'encode', '(', ')', ',', 'hashed', '.', 'encode', '(', ')', ')', ')', '\\n']
Tokenized   (027): ['[CLS]', 'bc', '##ry', '##pt', '.', 'hash', '##pw', '(', 'password', '.', 'en', '##code', '(', ')', ',', 'hash', '##ed', '.', 'en', '##code', '(', ')', ')', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['bc', '##ry', '##pt', '.', 'hash', '##pw', '(', 'password', '.', 'en', '##code', '(', ')', ',', 'hash', '##ed', '.', 'en', '##code', '(', ')', ')', ')', '\\', 'n']
Detokenized (018): ['bc##ry##pt', '.', 'hash##pw', '(', 'password', '.', 'en##code', '(', ')', ',', 'hash##ed', '.', 'en##code', '(', ')', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "browser . open ( . format ( BASE_URL ) ) \n"
Original    (011): ['browser', '.', 'open', '(', '.', 'format', '(', 'BASE_URL', ')', ')', '\\n']
Tokenized   (017): ['[CLS]', 'browser', '.', 'open', '(', '.', 'format', '(', 'base', '_', 'ur', '##l', ')', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['browser', '.', 'open', '(', '.', 'format', '(', 'base', '_', 'ur', '##l', ')', ')', '\\', 'n']
Detokenized (011): ['browser', '.', 'open', '(', '.', 'format', '(', 'base_ur##l', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "AUTHORS = open ( os . path . join ( here , ) ) . read ( ) \n"
Original    (019): ['AUTHORS', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ')', '.', 'read', '(', ')', '\\n']
Tokenized   (022): ['[CLS]', 'authors', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ')', '.', 'read', '(', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['authors', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ')', '.', 'read', '(', ')', '\\', 'n']
Detokenized (019): ['authors', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ')', '.', 'read', '(', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "fixed_boxes ) : \n"
Original    (004): ['fixed_boxes', ')', ':', '\\n']
Tokenized   (009): ['[CLS]', 'fixed', '_', 'boxes', ')', ':', '\\', 'n', '[SEP]']
Filtered   (007): ['fixed', '_', 'boxes', ')', ':', '\\', 'n']
Detokenized (004): ['fixed_boxes', ')', ':', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "resolve_percentages ( box , ( containing_block . width , containing_block . height ) ) \n"
Original    (015): ['resolve_percentages', '(', 'box', ',', '(', 'containing_block', '.', 'width', ',', 'containing_block', '.', 'height', ')', ')', '\\n']
Tokenized   (025): ['[CLS]', 'resolve', '_', 'percentage', '##s', '(', 'box', ',', '(', 'containing', '_', 'block', '.', 'width', ',', 'containing', '_', 'block', '.', 'height', ')', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['resolve', '_', 'percentage', '##s', '(', 'box', ',', '(', 'containing', '_', 'block', '.', 'width', ',', 'containing', '_', 'block', '.', 'height', ')', ')', '\\', 'n']
Detokenized (015): ['resolve_percentage##s', '(', 'box', ',', '(', 'containing_block', '.', 'width', ',', 'containing_block', '.', 'height', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "box , _ , _ , _ , _ = block_container_layout ( \n"
Original    (013): ['box', ',', '_', ',', '_', ',', '_', ',', '_', '=', 'block_container_layout', '(', '\\n']
Tokenized   (020): ['[CLS]', 'box', ',', '_', ',', '_', ',', '_', ',', '_', '=', 'block', '_', 'container', '_', 'layout', '(', '\\', 'n', '[SEP]']
Filtered   (018): ['box', ',', '_', ',', '_', ',', '_', ',', '_', '=', 'block', '_', 'container', '_', 'layout', '(', '\\', 'n']
Detokenized (013): ['box', ',', '_', ',', '_', ',', '_', ',', '_', '=', 'block_container_layout', '(', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "skip_stack = None , device_size = device_size , page_is_empty = False , \n"
Original    (013): ['skip_stack', '=', 'None', ',', 'device_size', '=', 'device_size', ',', 'page_is_empty', '=', 'False', ',', '\\n']
Tokenized   (026): ['[CLS]', 'skip', '_', 'stack', '=', 'none', ',', 'device', '_', 'size', '=', 'device', '_', 'size', ',', 'page', '_', 'is', '_', 'empty', '=', 'false', ',', '\\', 'n', '[SEP]']
Filtered   (024): ['skip', '_', 'stack', '=', 'none', ',', 'device', '_', 'size', '=', 'device', '_', 'size', ',', 'page', '_', 'is', '_', 'empty', '=', 'false', ',', '\\', 'n']
Detokenized (013): ['skip_stack', '=', 'none', ',', 'device_size', '=', 'device_size', ',', 'page_is_empty', '=', 'false', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "list_marker_layout ( context , box ) \n"
Original    (007): ['list_marker_layout', '(', 'context', ',', 'box', ')', '\\n']
Tokenized   (014): ['[CLS]', 'list', '_', 'marker', '_', 'layout', '(', 'context', ',', 'box', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['list', '_', 'marker', '_', 'layout', '(', 'context', ',', 'box', ')', '\\', 'n']
Detokenized (007): ['list_marker_layout', '(', 'context', ',', 'box', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "hypothetical_position = box . position_y + collapsed_margin \n"
Original    (008): ['hypothetical_position', '=', 'box', '.', 'position_y', '+', 'collapsed_margin', '\\n']
Tokenized   (017): ['[CLS]', 'hypothetical', '_', 'position', '=', 'box', '.', 'position', '_', 'y', '+', 'collapsed', '_', 'margin', '\\', 'n', '[SEP]']
Filtered   (015): ['hypothetical', '_', 'position', '=', 'box', '.', 'position', '_', 'y', '+', 'collapsed', '_', 'margin', '\\', 'n']
Detokenized (008): ['hypothetical_position', '=', 'box', '.', 'position_y', '+', 'collapsed_margin', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "box_width = box . margin_width ( ) if outer else box . border_width ( ) \n"
Original    (016): ['box_width', '=', 'box', '.', 'margin_width', '(', ')', 'if', 'outer', 'else', 'box', '.', 'border_width', '(', ')', '\\n']
Tokenized   (025): ['[CLS]', 'box', '_', 'width', '=', 'box', '.', 'margin', '_', 'width', '(', ')', 'if', 'outer', 'else', 'box', '.', 'border', '_', 'width', '(', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['box', '_', 'width', '=', 'box', '.', 'margin', '_', 'width', '(', ')', 'if', 'outer', 'else', 'box', '.', 'border', '_', 'width', '(', ')', '\\', 'n']
Detokenized (016): ['box_width', '=', 'box', '.', 'margin_width', '(', ')', 'if', 'outer', 'else', 'box', '.', 'border_width', '(', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "max_right_bound -= box . margin_right \n"
Original    (006): ['max_right_bound', '-=', 'box', '.', 'margin_right', '\\n']
Tokenized   (016): ['[CLS]', 'max', '_', 'right', '_', 'bound', '-', '=', 'box', '.', 'margin', '_', 'right', '\\', 'n', '[SEP]']
Filtered   (014): ['max', '_', 'right', '_', 'bound', '-', '=', 'box', '.', 'margin', '_', 'right', '\\', 'n']
Detokenized (006): ['max_right_bound', '-=', 'box', '.', 'margin_right', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "shape . position_y + shape . margin_height ( ) \n"
Original    (010): ['shape', '.', 'position_y', '+', 'shape', '.', 'margin_height', '(', ')', '\\n']
Tokenized   (017): ['[CLS]', 'shape', '.', 'position', '_', 'y', '+', 'shape', '.', 'margin', '_', 'height', '(', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['shape', '.', 'position', '_', 'y', '+', 'shape', '.', 'margin', '_', 'height', '(', ')', '\\', 'n']
Detokenized (010): ['shape', '.', 'position_y', '+', 'shape', '.', 'margin_height', '(', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "urlpatterns = patterns ( , \n"
Original    (006): ['urlpatterns', '=', 'patterns', '(', ',', '\\n']
Tokenized   (012): ['[CLS]', 'ur', '##lp', '##atter', '##ns', '=', 'patterns', '(', ',', '\\', 'n', '[SEP]']
Filtered   (010): ['ur', '##lp', '##atter', '##ns', '=', 'patterns', '(', ',', '\\', 'n']
Detokenized (006): ['ur##lp##atter##ns', '=', 'patterns', '(', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "obj1 , obj2 = qs \n"
Original    (006): ['obj1', ',', 'obj2', '=', 'qs', '\\n']
Tokenized   (014): ['[CLS]', 'ob', '##j', '##1', ',', 'ob', '##j', '##2', '=', 'q', '##s', '\\', 'n', '[SEP]']
Filtered   (012): ['ob', '##j', '##1', ',', 'ob', '##j', '##2', '=', 'q', '##s', '\\', 'n']
Detokenized (006): ['ob##j##1', ',', 'ob##j##2', '=', 'q##s', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "n1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n"
Original    (022): ['n1', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'pk', '=', 'self', '.', 'normal_id', '[', '1', ']', ')', '\\n']
Tokenized   (029): ['[CLS]', 'n', '##1', '=', 'normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'p', '##k', '=', 'self', '.', 'normal', '_', 'id', '[', '1', ']', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['n', '##1', '=', 'normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'p', '##k', '=', 'self', '.', 'normal', '_', 'id', '[', '1', ']', ')', '\\', 'n']
Detokenized (022): ['n##1', '=', 'normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'p##k', '=', 'self', '.', 'normal_id', '[', '1', ']', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "shared_field = NEW_SHARED , translated_field = NEW_TRANSLATED \n"
Original    (008): ['shared_field', '=', 'NEW_SHARED', ',', 'translated_field', '=', 'NEW_TRANSLATED', '\\n']
Tokenized   (019): ['[CLS]', 'shared', '_', 'field', '=', 'new', '_', 'shared', ',', 'translated', '_', 'field', '=', 'new', '_', 'translated', '\\', 'n', '[SEP]']
Filtered   (017): ['shared', '_', 'field', '=', 'new', '_', 'shared', ',', 'translated', '_', 'field', '=', 'new', '_', 'translated', '\\', 'n']
Detokenized (008): ['shared_field', '=', 'new_shared', ',', 'translated_field', '=', 'new_translated', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "NORMAL [ 2 ] . shared_field ] ) \n"
Original    (009): ['NORMAL', '[', '2', ']', '.', 'shared_field', ']', ')', '\\n']
Tokenized   (014): ['[CLS]', 'normal', '[', '2', ']', '.', 'shared', '_', 'field', ']', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['normal', '[', '2', ']', '.', 'shared', '_', 'field', ']', ')', '\\', 'n']
Detokenized (009): ['normal', '[', '2', ']', '.', 'shared_field', ']', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "ja = Normal . objects . language ( ) . get ( pk = en . pk ) \n"
Original    (019): ['ja', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'pk', '=', 'en', '.', 'pk', ')', '\\n']
Tokenized   (024): ['[CLS]', 'ja', '=', 'normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'p', '##k', '=', 'en', '.', 'p', '##k', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['ja', '=', 'normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'p', '##k', '=', 'en', '.', 'p', '##k', ')', '\\', 'n']
Detokenized (019): ['ja', '=', 'normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'p##k', '=', 'en', '.', 'p##k', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "AggregateModel . objects . language ( "en" ) . create ( number = 0 , translated_number = 0 ) \n"
Original    (020): ['AggregateModel', '.', 'objects', '.', 'language', '(', '"en"', ')', '.', 'create', '(', 'number', '=', '0', ',', 'translated_number', '=', '0', ')', '\\n']
Tokenized   (029): ['[CLS]', 'aggregate', '##mo', '##del', '.', 'objects', '.', 'language', '(', '"', 'en', '"', ')', '.', 'create', '(', 'number', '=', '0', ',', 'translated', '_', 'number', '=', '0', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['aggregate', '##mo', '##del', '.', 'objects', '.', 'language', '(', '"', 'en', '"', ')', '.', 'create', '(', 'number', '=', '0', ',', 'translated', '_', 'number', '=', '0', ')', '\\', 'n']
Detokenized (020): ['aggregate##mo##del', '.', 'objects', '.', 'language', '(', '"en"', ')', '.', 'create', '(', 'number', '=', '0', ',', 'translated_number', '=', '0', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "shared_contains_two = Q ( shared_field__contains = ) \n"
Original    (008): ['shared_contains_two', '=', 'Q', '(', 'shared_field__contains', '=', ')', '\\n']
Tokenized   (020): ['[CLS]', 'shared', '_', 'contains', '_', 'two', '=', 'q', '(', 'shared', '_', 'field', '_', '_', 'contains', '=', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['shared', '_', 'contains', '_', 'two', '=', 'q', '(', 'shared', '_', 'field', '_', '_', 'contains', '=', ')', '\\', 'n']
Detokenized (008): ['shared_contains_two', '=', 'q', '(', 'shared_field__contains', '=', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "normal_one = Q ( normal_field = STANDARD [ 1 ] . normal_field ) \n"
Original    (014): ['normal_one', '=', 'Q', '(', 'normal_field', '=', 'STANDARD', '[', '1', ']', '.', 'normal_field', ')', '\\n']
Tokenized   (023): ['[CLS]', 'normal', '_', 'one', '=', 'q', '(', 'normal', '_', 'field', '=', 'standard', '[', '1', ']', '.', 'normal', '_', 'field', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['normal', '_', 'one', '=', 'q', '(', 'normal', '_', 'field', '=', 'standard', '[', '1', ']', '.', 'normal', '_', 'field', ')', '\\', 'n']
Detokenized (014): ['normal_one', '=', 'q', '(', 'normal_field', '=', 'standard', '[', '1', ']', '.', 'normal_field', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "shared_one = Q ( normal__shared_field = NORMAL [ STANDARD [ 1 ] . normal ] . shared_field ) \n"
Original    (019): ['shared_one', '=', 'Q', '(', 'normal__shared_field', '=', 'NORMAL', '[', 'STANDARD', '[', '1', ']', '.', 'normal', ']', '.', 'shared_field', ')', '\\n']
Tokenized   (031): ['[CLS]', 'shared', '_', 'one', '=', 'q', '(', 'normal', '_', '_', 'shared', '_', 'field', '=', 'normal', '[', 'standard', '[', '1', ']', '.', 'normal', ']', '.', 'shared', '_', 'field', ')', '\\', 'n', '[SEP]']
Filtered   (029): ['shared', '_', 'one', '=', 'q', '(', 'normal', '_', '_', 'shared', '_', 'field', '=', 'normal', '[', 'standard', '[', '1', ']', '.', 'normal', ']', '.', 'shared', '_', 'field', ')', '\\', 'n']
Detokenized (019): ['shared_one', '=', 'q', '(', 'normal__shared_field', '=', 'normal', '[', 'standard', '[', '1', ']', '.', 'normal', ']', '.', 'shared_field', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "translated_one_en = Q ( normal__translated_field = NORMAL [ STANDARD [ 1 ] . normal ] . translated_field [ translated_two_en = Q ( normal__translated_field = NORMAL [ STANDARD [ 2 ] . normal ] . translated_field [ \n"
Original    (037): ['translated_one_en', '=', 'Q', '(', 'normal__translated_field', '=', 'NORMAL', '[', 'STANDARD', '[', '1', ']', '.', 'normal', ']', '.', 'translated_field', '[', 'translated_two_en', '=', 'Q', '(', 'normal__translated_field', '=', 'NORMAL', '[', 'STANDARD', '[', '2', ']', '.', 'normal', ']', '.', 'translated_field', '[', '\\n']
Tokenized   (062): ['[CLS]', 'translated', '_', 'one', '_', 'en', '=', 'q', '(', 'normal', '_', '_', 'translated', '_', 'field', '=', 'normal', '[', 'standard', '[', '1', ']', '.', 'normal', ']', '.', 'translated', '_', 'field', '[', 'translated', '_', 'two', '_', 'en', '=', 'q', '(', 'normal', '_', '_', 'translated', '_', 'field', '=', 'normal', '[', 'standard', '[', '2', ']', '.', 'normal', ']', '.', 'translated', '_', 'field', '[', '\\', 'n', '[SEP]']
Filtered   (060): ['translated', '_', 'one', '_', 'en', '=', 'q', '(', 'normal', '_', '_', 'translated', '_', 'field', '=', 'normal', '[', 'standard', '[', '1', ']', '.', 'normal', ']', '.', 'translated', '_', 'field', '[', 'translated', '_', 'two', '_', 'en', '=', 'q', '(', 'normal', '_', '_', 'translated', '_', 'field', '=', 'normal', '[', 'standard', '[', '2', ']', '.', 'normal', ']', '.', 'translated', '_', 'field', '[', '\\', 'n']
Detokenized (037): ['translated_one_en', '=', 'q', '(', 'normal__translated_field', '=', 'normal', '[', 'standard', '[', '1', ']', '.', 'normal', ']', '.', 'translated_field', '[', 'translated_two_en', '=', 'q', '(', 'normal__translated_field', '=', 'normal', '[', 'standard', '[', '2', ']', '.', 'normal', ']', '.', 'translated_field', '[', '\\n']
Counter: 60
===================================================================
Hidden states:  (13, 37, 768)
# Extracted words:  37
Sentence         : "qs = manager . filter ( shared_one & ~ translated_two_en ) \n"
Original    (012): ['qs', '=', 'manager', '.', 'filter', '(', 'shared_one', '&', '~', 'translated_two_en', ')', '\\n']
Tokenized   (022): ['[CLS]', 'q', '##s', '=', 'manager', '.', 'filter', '(', 'shared', '_', 'one', '&', '~', 'translated', '_', 'two', '_', 'en', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['q', '##s', '=', 'manager', '.', 'filter', '(', 'shared', '_', 'one', '&', '~', 'translated', '_', 'two', '_', 'en', ')', '\\', 'n']
Detokenized (012): ['q##s', '=', 'manager', '.', 'filter', '(', 'shared_one', '&', '~', 'translated_two_en', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "Normal . objects . language ( ) . complex_filter , \n"
Original    (011): ['Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'complex_filter', ',', '\\n']
Tokenized   (016): ['[CLS]', 'normal', '.', 'objects', '.', 'language', '(', ')', '.', 'complex', '_', 'filter', ',', '\\', 'n', '[SEP]']
Filtered   (014): ['normal', '.', 'objects', '.', 'language', '(', ')', '.', 'complex', '_', 'filter', ',', '\\', 'n']
Detokenized (011): ['normal', '.', 'objects', '.', 'language', '(', ')', '.', 'complex_filter', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "analytics . track ( user_id , "Activate" , { \n"
Original    (010): ['analytics', '.', 'track', '(', 'user_id', ',', '"Activate"', ',', '{', '\\n']
Tokenized   (017): ['[CLS]', 'analytics', '.', 'track', '(', 'user', '_', 'id', ',', '"', 'activate', '"', ',', '{', '\\', 'n', '[SEP]']
Filtered   (015): ['analytics', '.', 'track', '(', 'user', '_', 'id', ',', '"', 'activate', '"', ',', '{', '\\', 'n']
Detokenized (010): ['analytics', '.', 'track', '(', 'user_id', ',', '"activate"', ',', '{', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sublime_plugin . WindowCommand . __init__ ( self , * args , ** kwargs ) \n"
Original    (015): ['sublime_plugin', '.', 'WindowCommand', '.', '__init__', '(', 'self', ',', '*', 'args', ',', '**', 'kwargs', ')', '\\n']
Tokenized   (033): ['[CLS]', 'sublime', '_', 'plug', '##in', '.', 'window', '##com', '##man', '##d', '.', '_', '_', 'in', '##it', '_', '_', '(', 'self', ',', '*', 'ar', '##gs', ',', '*', '*', 'kw', '##ar', '##gs', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['sublime', '_', 'plug', '##in', '.', 'window', '##com', '##man', '##d', '.', '_', '_', 'in', '##it', '_', '_', '(', 'self', ',', '*', 'ar', '##gs', ',', '*', '*', 'kw', '##ar', '##gs', ')', '\\', 'n']
Detokenized (015): ['sublime_plug##in', '.', 'window##com##man##d', '.', '__in##it__', '(', 'self', ',', '*', 'ar##gs', ',', '**', 'kw##ar##gs', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : ""show_response" , { "title" : title , "text" : text } ) \n"
Original    (013): ['"show_response"', ',', '{', '"title"', ':', 'title', ',', '"text"', ':', 'text', '}', ')', '\\n']
Tokenized   (024): ['[CLS]', '"', 'show', '_', 'response', '"', ',', '{', '"', 'title', '"', ':', 'title', ',', '"', 'text', '"', ':', 'text', '}', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['"', 'show', '_', 'response', '"', ',', '{', '"', 'title', '"', ':', 'title', ',', '"', 'text', '"', ':', 'text', '}', ')', '\\', 'n']
Detokenized (013): ['"show_response"', ',', '{', '"title"', ':', 'title', ',', '"text"', ':', 'text', '}', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "ensure_ascii = False \n"
Original    (004): ['ensure_ascii', '=', 'False', '\\n']
Tokenized   (011): ['[CLS]', 'ensure', '_', 'as', '##ci', '##i', '=', 'false', '\\', 'n', '[SEP]']
Filtered   (009): ['ensure', '_', 'as', '##ci', '##i', '=', 'false', '\\', 'n']
Detokenized (004): ['ensure_as##ci##i', '=', 'false', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "syntax = "Packages/JavaScript/JSON.tmLanguage" ) \n"
Original    (005): ['syntax', '=', '"Packages/JavaScript/JSON.tmLanguage"', ')', '\\n']
Tokenized   (022): ['[CLS]', 'syntax', '=', '"', 'packages', '/', 'java', '##script', '/', 'j', '##son', '.', 't', '##ml', '##ang', '##ua', '##ge', '"', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['syntax', '=', '"', 'packages', '/', 'java', '##script', '/', 'j', '##son', '.', 't', '##ml', '##ang', '##ua', '##ge', '"', ')', '\\', 'n']
Detokenized (005): ['syntax', '=', '"packages/java##script/j##son.t##ml##ang##ua##ge"', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "scroll = self . settings . scroll_size \n"
Original    (008): ['scroll', '=', 'self', '.', 'settings', '.', 'scroll_size', '\\n']
Tokenized   (013): ['[CLS]', 'scroll', '=', 'self', '.', 'settings', '.', 'scroll', '_', 'size', '\\', 'n', '[SEP]']
Filtered   (011): ['scroll', '=', 'self', '.', 'settings', '.', 'scroll', '_', 'size', '\\', 'n']
Detokenized (008): ['scroll', '=', 'self', '.', 'settings', '.', 'scroll_size', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : ""show_panel" , { "panel" : "output.elasticsearch" } ) \n"
Original    (009): ['"show_panel"', ',', '{', '"panel"', ':', '"output.elasticsearch"', '}', ')', '\\n']
Tokenized   (024): ['[CLS]', '"', 'show', '_', 'panel', '"', ',', '{', '"', 'panel', '"', ':', '"', 'output', '.', 'elastic', '##sea', '##rch', '"', '}', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['"', 'show', '_', 'panel', '"', ',', '{', '"', 'panel', '"', ':', '"', 'output', '.', 'elastic', '##sea', '##rch', '"', '}', ')', '\\', 'n']
Detokenized (009): ['"show_panel"', ',', '{', '"panel"', ':', '"output.elastic##sea##rch"', '}', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "panel . set_read_only ( True ) \n"
Original    (007): ['panel', '.', 'set_read_only', '(', 'True', ')', '\\n']
Tokenized   (014): ['[CLS]', 'panel', '.', 'set', '_', 'read', '_', 'only', '(', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['panel', '.', 'set', '_', 'read', '_', 'only', '(', 'true', ')', '\\', 'n']
Detokenized (007): ['panel', '.', 'set_read_only', '(', 'true', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "400 : RequestError , \n"
Original    (005): ['400', ':', 'RequestError', ',', '\\n']
Tokenized   (010): ['[CLS]', '400', ':', 'request', '##er', '##ror', ',', '\\', 'n', '[SEP]']
Filtered   (008): ['400', ':', 'request', '##er', '##ror', ',', '\\', 'n']
Detokenized (005): ['400', ':', 'request##er##ror', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "TestConfigFileSource . ConcreteConfigFileSource ) \n"
Original    (005): ['TestConfigFileSource', '.', 'ConcreteConfigFileSource', ')', '\\n']
Tokenized   (022): ['[CLS]', 'test', '##con', '##fi', '##gf', '##ile', '##so', '##ur', '##ce', '.', 'concrete', '##con', '##fi', '##gf', '##ile', '##so', '##ur', '##ce', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['test', '##con', '##fi', '##gf', '##ile', '##so', '##ur', '##ce', '.', 'concrete', '##con', '##fi', '##gf', '##ile', '##so', '##ur', '##ce', ')', '\\', 'n']
Detokenized (005): ['test##con##fi##gf##ile##so##ur##ce', '.', 'concrete##con##fi##gf##ile##so##ur##ce', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "ReferenceReachabilityTester . TwoWayScopeReferenceAttacher . attach ( self . _scope_tree ) \n"
Original    (011): ['ReferenceReachabilityTester', '.', 'TwoWayScopeReferenceAttacher', '.', 'attach', '(', 'self', '.', '_scope_tree', ')', '\\n']
Tokenized   (029): ['[CLS]', 'reference', '##rea', '##cha', '##bility', '##test', '##er', '.', 'two', '##ways', '##cope', '##re', '##ference', '##att', '##ache', '##r', '.', 'attach', '(', 'self', '.', '_', 'scope', '_', 'tree', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['reference', '##rea', '##cha', '##bility', '##test', '##er', '.', 'two', '##ways', '##cope', '##re', '##ference', '##att', '##ache', '##r', '.', 'attach', '(', 'self', '.', '_', 'scope', '_', 'tree', ')', '\\', 'n']
Detokenized (011): ['reference##rea##cha##bility##test##er', '.', 'two##ways##cope##re##ference##att##ache##r', '.', 'attach', '(', 'self', '.', '_scope_tree', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "declaring_id_node [ REFERECED_FLAG ] = True \n"
Original    (007): ['declaring_id_node', '[', 'REFERECED_FLAG', ']', '=', 'True', '\\n']
Tokenized   (018): ['[CLS]', 'declaring', '_', 'id', '_', 'node', '[', 'refer', '##ece', '##d', '_', 'flag', ']', '=', 'true', '\\', 'n', '[SEP]']
Filtered   (016): ['declaring', '_', 'id', '_', 'node', '[', 'refer', '##ece', '##d', '_', 'flag', ']', '=', 'true', '\\', 'n']
Detokenized (007): ['declaring_id_node', '[', 'refer##ece##d_flag', ']', '=', 'true', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "node_type = NodeType ( node [ ] ) \n"
Original    (009): ['node_type', '=', 'NodeType', '(', 'node', '[', ']', ')', '\\n']
Tokenized   (015): ['[CLS]', 'node', '_', 'type', '=', 'node', '##type', '(', 'node', '[', ']', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['node', '_', 'type', '=', 'node', '##type', '(', 'node', '[', ']', ')', '\\', 'n']
Detokenized (009): ['node_type', '=', 'node##type', '(', 'node', '[', ']', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "is_set_cmd = excmd_node [ ] [ ] . get ( ) in SetCommandFamily \n"
Original    (014): ['is_set_cmd', '=', 'excmd_node', '[', ']', '[', ']', '.', 'get', '(', ')', 'in', 'SetCommandFamily', '\\n']
Tokenized   (031): ['[CLS]', 'is', '_', 'set', '_', 'cm', '##d', '=', 'ex', '##cm', '##d', '_', 'node', '[', ']', '[', ']', '.', 'get', '(', ')', 'in', 'set', '##com', '##man', '##df', '##ami', '##ly', '\\', 'n', '[SEP]']
Filtered   (029): ['is', '_', 'set', '_', 'cm', '##d', '=', 'ex', '##cm', '##d', '_', 'node', '[', ']', '[', ']', '.', 'get', '(', ')', 'in', 'set', '##com', '##man', '##df', '##ami', '##ly', '\\', 'n']
Detokenized (014): ['is_set_cm##d', '=', 'ex##cm##d_node', '[', ']', '[', ']', '.', 'get', '(', ')', 'in', 'set##com##man##df##ami##ly', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "option_name = re . match ( , option_expr ) . group ( 0 ) \n"
Original    (015): ['option_name', '=', 're', '.', 'match', '(', ',', 'option_expr', ')', '.', 'group', '(', '0', ')', '\\n']
Tokenized   (023): ['[CLS]', 'option', '_', 'name', '=', 're', '.', 'match', '(', ',', 'option', '_', 'ex', '##pr', ')', '.', 'group', '(', '0', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['option', '_', 'name', '=', 're', '.', 'match', '(', ',', 'option', '_', 'ex', '##pr', ')', '.', 'group', '(', '0', ')', '\\', 'n']
Detokenized (015): ['option_name', '=', 're', '.', 'match', '(', ',', 'option_ex##pr', ')', '.', 'group', '(', '0', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "is_valid = option_name not in AbbreviationsIncludingInvertPrefix \n"
Original    (007): ['is_valid', '=', 'option_name', 'not', 'in', 'AbbreviationsIncludingInvertPrefix', '\\n']
Tokenized   (022): ['[CLS]', 'is', '_', 'valid', '=', 'option', '_', 'name', 'not', 'in', 'abbreviation', '##sin', '##cl', '##uding', '##in', '##vert', '##pre', '##fi', '##x', '\\', 'n', '[SEP]']
Filtered   (020): ['is', '_', 'valid', '=', 'option', '_', 'name', 'not', 'in', 'abbreviation', '##sin', '##cl', '##uding', '##in', '##vert', '##pre', '##fi', '##x', '\\', 'n']
Detokenized (007): ['is_valid', '=', 'option_name', 'not', 'in', 'abbreviation##sin##cl##uding##in##vert##pre##fi##x', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "stderr . setFormatter ( logging . Formatter ( \n"
Original    (009): ['stderr', '.', 'setFormatter', '(', 'logging', '.', 'Formatter', '(', '\\n']
Tokenized   (017): ['[CLS]', 'st', '##der', '##r', '.', 'set', '##form', '##atter', '(', 'logging', '.', 'format', '##ter', '(', '\\', 'n', '[SEP]']
Filtered   (015): ['st', '##der', '##r', '.', 'set', '##form', '##atter', '(', 'logging', '.', 'format', '##ter', '(', '\\', 'n']
Detokenized (009): ['st##der##r', '.', 'set##form##atter', '(', 'logging', '.', 'format##ter', '(', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "level = level if level else os . environ . get ( , ) \n"
Original    (015): ['level', '=', 'level', 'if', 'level', 'else', 'os', '.', 'environ', '.', 'get', '(', ',', ')', '\\n']
Tokenized   (020): ['[CLS]', 'level', '=', 'level', 'if', 'level', 'else', 'os', '.', 'en', '##vir', '##on', '.', 'get', '(', ',', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['level', '=', 'level', 'if', 'level', 'else', 'os', '.', 'en', '##vir', '##on', '.', 'get', '(', ',', ')', '\\', 'n']
Detokenized (015): ['level', '=', 'level', 'if', 'level', 'else', 'os', '.', 'en##vir##on', '.', 'get', '(', ',', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "g_s = g0 * g_c * g_R * g_D * g_T * g_M \n"
Original    (014): ['g_s', '=', 'g0', '*', 'g_c', '*', 'g_R', '*', 'g_D', '*', 'g_T', '*', 'g_M', '\\n']
Tokenized   (030): ['[CLS]', 'g', '_', 's', '=', 'g', '##0', '*', 'g', '_', 'c', '*', 'g', '_', 'r', '*', 'g', '_', 'd', '*', 'g', '_', 't', '*', 'g', '_', 'm', '\\', 'n', '[SEP]']
Filtered   (028): ['g', '_', 's', '=', 'g', '##0', '*', 'g', '_', 'c', '*', 'g', '_', 'r', '*', 'g', '_', 'd', '*', 'g', '_', 't', '*', 'g', '_', 'm', '\\', 'n']
Detokenized (014): ['g_s', '=', 'g##0', '*', 'g_c', '*', 'g_r', '*', 'g_d', '*', 'g_t', '*', 'g_m', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "g_T = ( ( TK - TL ) * ( TH - TK ) ** alpha_T ) / ( ( T0 - TL ) * ( TH - T0 ) ** alpha_T ) \n"
Original    (034): ['g_T', '=', '(', '(', 'TK', '-', 'TL', ')', '*', '(', 'TH', '-', 'TK', ')', '**', 'alpha_T', ')', '/', '(', '(', 'T0', '-', 'TL', ')', '*', '(', 'TH', '-', 'T0', ')', '**', 'alpha_T', ')', '\\n']
Tokenized   (051): ['[CLS]', 'g', '_', 't', '=', '(', '(', 't', '##k', '-', 't', '##l', ')', '*', '(', 'th', '-', 't', '##k', ')', '*', '*', 'alpha', '_', 't', ')', '/', '(', '(', 't', '##0', '-', 't', '##l', ')', '*', '(', 'th', '-', 't', '##0', ')', '*', '*', 'alpha', '_', 't', ')', '\\', 'n', '[SEP]']
Filtered   (049): ['g', '_', 't', '=', '(', '(', 't', '##k', '-', 't', '##l', ')', '*', '(', 'th', '-', 't', '##k', ')', '*', '*', 'alpha', '_', 't', ')', '/', '(', '(', 't', '##0', '-', 't', '##l', ')', '*', '(', 'th', '-', 't', '##0', ')', '*', '*', 'alpha', '_', 't', ')', '\\', 'n']
Detokenized (034): ['g_t', '=', '(', '(', 't##k', '-', 't##l', ')', '*', '(', 'th', '-', 't##k', ')', '**', 'alpha_t', ')', '/', '(', '(', 't##0', '-', 't##l', ')', '*', '(', 'th', '-', 't##0', ')', '**', 'alpha_t', ')', '\\n']
Counter: 49
===================================================================
Hidden states:  (13, 34, 768)
# Extracted words:  34
Sentence         : "r_a = AeroReist ( um , zm , z0 , d ) \n"
Original    (013): ['r_a', '=', 'AeroReist', '(', 'um', ',', 'zm', ',', 'z0', ',', 'd', ')', '\\n']
Tokenized   (022): ['[CLS]', 'r', '_', 'a', '=', 'aero', '##re', '##ist', '(', 'um', ',', 'z', '##m', ',', 'z', '##0', ',', 'd', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['r', '_', 'a', '=', 'aero', '##re', '##ist', '(', 'um', ',', 'z', '##m', ',', 'z', '##0', ',', 'd', ')', '\\', 'n']
Detokenized (013): ['r_a', '=', 'aero##re##ist', '(', 'um', ',', 'z##m', ',', 'z##0', ',', 'd', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "r_s = SurfResist ( g0 , S , D , Tc , SM , SM0 ) \n"
Original    (017): ['r_s', '=', 'SurfResist', '(', 'g0', ',', 'S', ',', 'D', ',', 'Tc', ',', 'SM', ',', 'SM0', ')', '\\n']
Tokenized   (026): ['[CLS]', 'r', '_', 's', '=', 'surf', '##res', '##ist', '(', 'g', '##0', ',', 's', ',', 'd', ',', 'tc', ',', 'sm', ',', 'sm', '##0', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['r', '_', 's', '=', 'surf', '##res', '##ist', '(', 'g', '##0', ',', 's', ',', 'd', ',', 'tc', ',', 'sm', ',', 'sm', '##0', ')', '\\', 'n']
Detokenized (017): ['r_s', '=', 'surf##res##ist', '(', 'g##0', ',', 's', ',', 'd', ',', 'tc', ',', 'sm', ',', 'sm##0', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "LE = ( delta * Rn + ( rho_a * cP * D ) / r_a ) / ( delta + gamma * ( 1.0 + r_s / r_a ) ) \n"
Original    (032): ['LE', '=', '(', 'delta', '*', 'Rn', '+', '(', 'rho_a', '*', 'cP', '*', 'D', ')', '/', 'r_a', ')', '/', '(', 'delta', '+', 'gamma', '*', '(', '1.0', '+', 'r_s', '/', 'r_a', ')', ')', '\\n']
Tokenized   (046): ['[CLS]', 'le', '=', '(', 'delta', '*', 'rn', '+', '(', 'r', '##ho', '_', 'a', '*', 'cp', '*', 'd', ')', '/', 'r', '_', 'a', ')', '/', '(', 'delta', '+', 'gamma', '*', '(', '1', '.', '0', '+', 'r', '_', 's', '/', 'r', '_', 'a', ')', ')', '\\', 'n', '[SEP]']
Filtered   (044): ['le', '=', '(', 'delta', '*', 'rn', '+', '(', 'r', '##ho', '_', 'a', '*', 'cp', '*', 'd', ')', '/', 'r', '_', 'a', ')', '/', '(', 'delta', '+', 'gamma', '*', '(', '1', '.', '0', '+', 'r', '_', 's', '/', 'r', '_', 'a', ')', ')', '\\', 'n']
Detokenized (032): ['le', '=', '(', 'delta', '*', 'rn', '+', '(', 'r##ho_a', '*', 'cp', '*', 'd', ')', '/', 'r_a', ')', '/', '(', 'delta', '+', 'gamma', '*', '(', '1.0', '+', 'r_s', '/', 'r_a', ')', ')', '\\n']
Counter: 44
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "glClearColor ( * background_color ) \n"
Original    (006): ['glClearColor', '(', '*', 'background_color', ')', '\\n']
Tokenized   (015): ['[CLS]', 'g', '##lc', '##lea', '##rco', '##lor', '(', '*', 'background', '_', 'color', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['g', '##lc', '##lea', '##rco', '##lor', '(', '*', 'background', '_', 'color', ')', '\\', 'n']
Detokenized (006): ['g##lc##lea##rco##lor', '(', '*', 'background_color', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "glScissor ( x , y , width , height ) \n"
Original    (011): ['glScissor', '(', 'x', ',', 'y', ',', 'width', ',', 'height', ')', '\\n']
Tokenized   (017): ['[CLS]', 'g', '##ls', '##cis', '##sor', '(', 'x', ',', 'y', ',', 'width', ',', 'height', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['g', '##ls', '##cis', '##sor', '(', 'x', ',', 'y', ',', 'width', ',', 'height', ')', '\\', 'n']
Detokenized (011): ['g##ls##cis##sor', '(', 'x', ',', 'y', ',', 'width', ',', 'height', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "glOrtho ( x , x + width , y , y + height , - 1 , 1 ) \n"
Original    (020): ['glOrtho', '(', 'x', ',', 'x', '+', 'width', ',', 'y', ',', 'y', '+', 'height', ',', '-', '1', ',', '1', ')', '\\n']
Tokenized   (026): ['[CLS]', 'g', '##lor', '##th', '##o', '(', 'x', ',', 'x', '+', 'width', ',', 'y', ',', 'y', '+', 'height', ',', '-', '1', ',', '1', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['g', '##lor', '##th', '##o', '(', 'x', ',', 'x', '+', 'width', ',', 'y', ',', 'y', '+', 'height', ',', '-', '1', ',', '1', ')', '\\', 'n']
Detokenized (020): ['g##lor##th##o', '(', 'x', ',', 'x', '+', 'width', ',', 'y', ',', 'y', '+', 'height', ',', '-', '1', ',', '1', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "glNormal3f ( 0 , 1. , 0 ) \n"
Original    (009): ['glNormal3f', '(', '0', ',', '1.', ',', '0', ')', '\\n']
Tokenized   (018): ['[CLS]', 'g', '##ln', '##or', '##mal', '##3', '##f', '(', '0', ',', '1', '.', ',', '0', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['g', '##ln', '##or', '##mal', '##3', '##f', '(', '0', ',', '1', '.', ',', '0', ')', '\\', 'n']
Detokenized (009): ['g##ln##or##mal##3##f', '(', '0', ',', '1.', ',', '0', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "glVertex ( n , n , p ) \n"
Original    (009): ['glVertex', '(', 'n', ',', 'n', ',', 'p', ')', '\\n']
Tokenized   (015): ['[CLS]', 'g', '##lver', '##te', '##x', '(', 'n', ',', 'n', ',', 'p', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['g', '##lver', '##te', '##x', '(', 'n', ',', 'n', ',', 'p', ')', '\\', 'n']
Detokenized (009): ['g##lver##te##x', '(', 'n', ',', 'n', ',', 'p', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "companies = [ path for path in paths \n"
Original    (009): ['companies', '=', '[', 'path', 'for', 'path', 'in', 'paths', '\\n']
Tokenized   (012): ['[CLS]', 'companies', '=', '[', 'path', 'for', 'path', 'in', 'paths', '\\', 'n', '[SEP]']
Filtered   (010): ['companies', '=', '[', 'path', 'for', 'path', 'in', 'paths', '\\', 'n']
Detokenized (009): ['companies', '=', '[', 'path', 'for', 'path', 'in', 'paths', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "and os . path . exists ( os . path . join ( folder , path , ) ) ] \n"
Original    (021): ['and', 'os', '.', 'path', '.', 'exists', '(', 'os', '.', 'path', '.', 'join', '(', 'folder', ',', 'path', ',', ')', ')', ']', '\\n']
Tokenized   (024): ['[CLS]', 'and', 'os', '.', 'path', '.', 'exists', '(', 'os', '.', 'path', '.', 'join', '(', 'folder', ',', 'path', ',', ')', ')', ']', '\\', 'n', '[SEP]']
Filtered   (022): ['and', 'os', '.', 'path', '.', 'exists', '(', 'os', '.', 'path', '.', 'join', '(', 'folder', ',', 'path', ',', ')', ')', ']', '\\', 'n']
Detokenized (021): ['and', 'os', '.', 'path', '.', 'exists', '(', 'os', '.', 'path', '.', 'join', '(', 'folder', ',', 'path', ',', ')', ')', ']', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "folder = os . path . join ( root_folder , , , ) \n"
Original    (014): ['folder', '=', 'os', '.', 'path', '.', 'join', '(', 'root_folder', ',', ',', ',', ')', '\\n']
Tokenized   (019): ['[CLS]', 'folder', '=', 'os', '.', 'path', '.', 'join', '(', 'root', '_', 'folder', ',', ',', ',', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['folder', '=', 'os', '.', 'path', '.', 'join', '(', 'root', '_', 'folder', ',', ',', ',', ')', '\\', 'n']
Detokenized (014): ['folder', '=', 'os', '.', 'path', '.', 'join', '(', 'root_folder', ',', ',', ',', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "scripts = [ , \n"
Original    (005): ['scripts', '=', '[', ',', '\\n']
Tokenized   (008): ['[CLS]', 'scripts', '=', '[', ',', '\\', 'n', '[SEP]']
Filtered   (006): ['scripts', '=', '[', ',', '\\', 'n']
Detokenized (005): ['scripts', '=', '[', ',', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "pro2 . predict ( ) from collections import OrderedDict \n"
Original    (010): ['pro2', '.', 'predict', '(', ')', 'from', 'collections', 'import', 'OrderedDict', '\\n']
Tokenized   (015): ['[CLS]', 'pro', '##2', '.', 'predict', '(', ')', 'from', 'collections', 'import', 'ordered', '##dict', '\\', 'n', '[SEP]']
Filtered   (013): ['pro', '##2', '.', 'predict', '(', ')', 'from', 'collections', 'import', 'ordered', '##dict', '\\', 'n']
Detokenized (010): ['pro##2', '.', 'predict', '(', ')', 'from', 'collections', 'import', 'ordered##dict', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : ""dimensions." % ( self . __class__ . __name__ , shape ) ) \n"
Original    (013): ['"dimensions."', '%', '(', 'self', '.', '__class__', '.', '__name__', ',', 'shape', ')', ')', '\\n']
Tokenized   (027): ['[CLS]', '"', 'dimensions', '.', '"', '%', '(', 'self', '.', '_', '_', 'class', '_', '_', '.', '_', '_', 'name', '_', '_', ',', 'shape', ')', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['"', 'dimensions', '.', '"', '%', '(', 'self', '.', '_', '_', 'class', '_', '_', '.', '_', '_', 'name', '_', '_', ',', 'shape', ')', ')', '\\', 'n']
Detokenized (013): ['"dimensions."', '%', '(', 'self', '.', '__class__', '.', '__name__', ',', 'shape', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "only = set ( tag for tag , value in tags . items ( ) if value ) \n"
Original    (019): ['only', '=', 'set', '(', 'tag', 'for', 'tag', ',', 'value', 'in', 'tags', '.', 'items', '(', ')', 'if', 'value', ')', '\\n']
Tokenized   (022): ['[CLS]', 'only', '=', 'set', '(', 'tag', 'for', 'tag', ',', 'value', 'in', 'tags', '.', 'items', '(', ')', 'if', 'value', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['only', '=', 'set', '(', 'tag', 'for', 'tag', ',', 'value', 'in', 'tags', '.', 'items', '(', ')', 'if', 'value', ')', '\\', 'n']
Detokenized (019): ['only', '=', 'set', '(', 'tag', 'for', 'tag', ',', 'value', 'in', 'tags', '.', 'items', '(', ')', 'if', 'value', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "crop1 = [ None , , , ] \n"
Original    (009): ['crop1', '=', '[', 'None', ',', ',', ',', ']', '\\n']
Tokenized   (013): ['[CLS]', 'crop', '##1', '=', '[', 'none', ',', ',', ',', ']', '\\', 'n', '[SEP]']
Filtered   (011): ['crop', '##1', '=', '[', 'none', ',', ',', ',', ']', '\\', 'n']
Detokenized (009): ['crop##1', '=', '[', 'none', ',', ',', ',', ']', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "outs = [ o . eval ( ) for o in outs ] \n"
Original    (014): ['outs', '=', '[', 'o', '.', 'eval', '(', ')', 'for', 'o', 'in', 'outs', ']', '\\n']
Tokenized   (018): ['[CLS]', 'outs', '=', '[', 'o', '.', 'eva', '##l', '(', ')', 'for', 'o', 'in', 'outs', ']', '\\', 'n', '[SEP]']
Filtered   (016): ['outs', '=', '[', 'o', '.', 'eva', '##l', '(', ')', 'for', 'o', 'in', 'outs', ']', '\\', 'n']
Detokenized (014): ['outs', '=', '[', 'o', '.', 'eva##l', '(', ')', 'for', 'o', 'in', 'outs', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "crop_test ( crop_x , [ x0 , x1 , x2 , x0 , x1 , x2 ] , \n"
Original    (019): ['crop_test', '(', 'crop_x', ',', '[', 'x0', ',', 'x1', ',', 'x2', ',', 'x0', ',', 'x1', ',', 'x2', ']', ',', '\\n']
Tokenized   (032): ['[CLS]', 'crop', '_', 'test', '(', 'crop', '_', 'x', ',', '[', 'x', '##0', ',', 'x', '##1', ',', 'x', '##2', ',', 'x', '##0', ',', 'x', '##1', ',', 'x', '##2', ']', ',', '\\', 'n', '[SEP]']
Filtered   (030): ['crop', '_', 'test', '(', 'crop', '_', 'x', ',', '[', 'x', '##0', ',', 'x', '##1', ',', 'x', '##2', ',', 'x', '##0', ',', 'x', '##1', ',', 'x', '##2', ']', ',', '\\', 'n']
Detokenized (019): ['crop_test', '(', 'crop_x', ',', '[', 'x##0', ',', 'x##1', ',', 'x##2', ',', 'x##0', ',', 'x##1', ',', 'x##2', ']', ',', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "cropping = [ ] * 2 ) \n"
Original    (008): ['cropping', '=', '[', ']', '*', '2', ')', '\\n']
Tokenized   (012): ['[CLS]', 'crop', '##ping', '=', '[', ']', '*', '2', ')', '\\', 'n', '[SEP]']
Filtered   (010): ['crop', '##ping', '=', '[', ']', '*', '2', ')', '\\', 'n']
Detokenized (008): ['crop##ping', '=', '[', ']', '*', '2', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "desired_result_0 = numpy . concatenate ( [ x0 [ : , : 2 ] , x1 [ : , : 2 ] ] , axis = 0 ) \n"
Original    (029): ['desired_result_0', '=', 'numpy', '.', 'concatenate', '(', '[', 'x0', '[', ':', ',', ':', '2', ']', ',', 'x1', '[', ':', ',', ':', '2', ']', ']', ',', 'axis', '=', '0', ')', '\\n']
Tokenized   (042): ['[CLS]', 'desired', '_', 'result', '_', '0', '=', 'nu', '##mp', '##y', '.', 'con', '##cate', '##nate', '(', '[', 'x', '##0', '[', ':', ',', ':', '2', ']', ',', 'x', '##1', '[', ':', ',', ':', '2', ']', ']', ',', 'axis', '=', '0', ')', '\\', 'n', '[SEP]']
Filtered   (040): ['desired', '_', 'result', '_', '0', '=', 'nu', '##mp', '##y', '.', 'con', '##cate', '##nate', '(', '[', 'x', '##0', '[', ':', ',', ':', '2', ']', ',', 'x', '##1', '[', ':', ',', ':', '2', ']', ']', ',', 'axis', '=', '0', ')', '\\', 'n']
Detokenized (029): ['desired_result_0', '=', 'nu##mp##y', '.', 'con##cate##nate', '(', '[', 'x##0', '[', ':', ',', ':', '2', ']', ',', 'x##1', '[', ':', ',', ':', '2', ']', ']', ',', 'axis', '=', '0', ')', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "desired_result_1 = numpy . concatenate ( [ x0 [ : 4 , : ] , x1 [ : 4 , : ] ] , axis = 1 ) \n"
Original    (029): ['desired_result_1', '=', 'numpy', '.', 'concatenate', '(', '[', 'x0', '[', ':', '4', ',', ':', ']', ',', 'x1', '[', ':', '4', ',', ':', ']', ']', ',', 'axis', '=', '1', ')', '\\n']
Tokenized   (042): ['[CLS]', 'desired', '_', 'result', '_', '1', '=', 'nu', '##mp', '##y', '.', 'con', '##cate', '##nate', '(', '[', 'x', '##0', '[', ':', '4', ',', ':', ']', ',', 'x', '##1', '[', ':', '4', ',', ':', ']', ']', ',', 'axis', '=', '1', ')', '\\', 'n', '[SEP]']
Filtered   (040): ['desired', '_', 'result', '_', '1', '=', 'nu', '##mp', '##y', '.', 'con', '##cate', '##nate', '(', '[', 'x', '##0', '[', ':', '4', ',', ':', ']', ',', 'x', '##1', '[', ':', '4', ',', ':', ']', ']', ',', 'axis', '=', '1', ')', '\\', 'n']
Detokenized (029): ['desired_result_1', '=', 'nu##mp##y', '.', 'con##cate##nate', '(', '[', 'x##0', '[', ':', '4', ',', ':', ']', ',', 'x##1', '[', ':', '4', ',', ':', ']', ']', ',', 'axis', '=', '1', ')', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "inputs = [ theano . shared ( a ) , \n"
Original    (011): ['inputs', '=', '[', 'theano', '.', 'shared', '(', 'a', ')', ',', '\\n']
Tokenized   (015): ['[CLS]', 'inputs', '=', '[', 'the', '##ano', '.', 'shared', '(', 'a', ')', ',', '\\', 'n', '[SEP]']
Filtered   (013): ['inputs', '=', '[', 'the', '##ano', '.', 'shared', '(', 'a', ')', ',', '\\', 'n']
Detokenized (011): ['inputs', '=', '[', 'the##ano', '.', 'shared', '(', 'a', ')', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "theano . shared ( b ) ] \n"
Original    (008): ['theano', '.', 'shared', '(', 'b', ')', ']', '\\n']
Tokenized   (012): ['[CLS]', 'the', '##ano', '.', 'shared', '(', 'b', ')', ']', '\\', 'n', '[SEP]']
Filtered   (010): ['the', '##ano', '.', 'shared', '(', 'b', ')', ']', '\\', 'n']
Detokenized (008): ['the##ano', '.', 'shared', '(', 'b', ')', ']', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "mask = os . urandom ( 4 ) if mask else None \n"
Original    (013): ['mask', '=', 'os', '.', 'urandom', '(', '4', ')', 'if', 'mask', 'else', 'None', '\\n']
Tokenized   (018): ['[CLS]', 'mask', '=', 'os', '.', 'ur', '##ando', '##m', '(', '4', ')', 'if', 'mask', 'else', 'none', '\\', 'n', '[SEP]']
Filtered   (016): ['mask', '=', 'os', '.', 'ur', '##ando', '##m', '(', '4', ')', 'if', 'mask', 'else', 'none', '\\', 'n']
Detokenized (013): ['mask', '=', 'os', '.', 'ur##ando##m', '(', '4', ')', 'if', 'mask', 'else', 'none', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "masking_key = mask , fin = 1 ) . build ( ) \n"
Original    (013): ['masking_key', '=', 'mask', ',', 'fin', '=', '1', ')', '.', 'build', '(', ')', '\\n']
Tokenized   (019): ['[CLS]', 'mask', '##ing', '_', 'key', '=', 'mask', ',', 'fin', '=', '1', ')', '.', 'build', '(', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['mask', '##ing', '_', 'key', '=', 'mask', ',', 'fin', '=', '1', ')', '.', 'build', '(', ')', '\\', 'n']
Detokenized (013): ['mask##ing_key', '=', 'mask', ',', 'fin', '=', '1', ')', '.', 'build', '(', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "fin = fin ) . build ( ) \n"
Original    (009): ['fin', '=', 'fin', ')', '.', 'build', '(', ')', '\\n']
Tokenized   (012): ['[CLS]', 'fin', '=', 'fin', ')', '.', 'build', '(', ')', '\\', 'n', '[SEP]']
Filtered   (010): ['fin', '=', 'fin', ')', '.', 'build', '(', ')', '\\', 'n']
Detokenized (009): ['fin', '=', 'fin', ')', '.', 'build', '(', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "flags = unpack ( , msg [ idx : idx + 4 ] ) [ 0 ] \n"
Original    (018): ['flags', '=', 'unpack', '(', ',', 'msg', '[', 'idx', ':', 'idx', '+', '4', ']', ')', '[', '0', ']', '\\n']
Tokenized   (025): ['[CLS]', 'flags', '=', 'un', '##pack', '(', ',', 'ms', '##g', '[', 'id', '##x', ':', 'id', '##x', '+', '4', ']', ')', '[', '0', ']', '\\', 'n', '[SEP]']
Filtered   (023): ['flags', '=', 'un', '##pack', '(', ',', 'ms', '##g', '[', 'id', '##x', ':', 'id', '##x', '+', '4', ']', ')', '[', '0', ']', '\\', 'n']
Detokenized (018): ['flags', '=', 'un##pack', '(', ',', 'ms##g', '[', 'id##x', ':', 'id##x', '+', '4', ']', ')', '[', '0', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "pdc = req . get_options ( ) [ ] \n"
Original    (010): ['pdc', '=', 'req', '.', 'get_options', '(', ')', '[', ']', '\\n']
Tokenized   (017): ['[CLS]', 'pd', '##c', '=', 're', '##q', '.', 'get', '_', 'options', '(', ')', '[', ']', '\\', 'n', '[SEP]']
Filtered   (015): ['pd', '##c', '=', 're', '##q', '.', 'get', '_', 'options', '(', ')', '[', ']', '\\', 'n']
Detokenized (010): ['pd##c', '=', 're##q', '.', 'get_options', '(', ')', '[', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "bdc = req . get_options ( ) . get ( , False ) \n"
Original    (014): ['bdc', '=', 'req', '.', 'get_options', '(', ')', '.', 'get', '(', ',', 'False', ')', '\\n']
Tokenized   (021): ['[CLS]', 'b', '##dc', '=', 're', '##q', '.', 'get', '_', 'options', '(', ')', '.', 'get', '(', ',', 'false', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['b', '##dc', '=', 're', '##q', '.', 'get', '_', 'options', '(', ')', '.', 'get', '(', ',', 'false', ')', '\\', 'n']
Detokenized (014): ['b##dc', '=', 're##q', '.', 'get_options', '(', ')', '.', 'get', '(', ',', 'false', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "decoded_path = urllib . unquote ( url . path ) [ 1 : ] \n"
Original    (015): ['decoded_path', '=', 'urllib', '.', 'unquote', '(', 'url', '.', 'path', ')', '[', '1', ':', ']', '\\n']
Tokenized   (026): ['[CLS]', 'deco', '##ded', '_', 'path', '=', 'ur', '##lli', '##b', '.', 'un', '##qu', '##ote', '(', 'ur', '##l', '.', 'path', ')', '[', '1', ':', ']', '\\', 'n', '[SEP]']
Filtered   (024): ['deco', '##ded', '_', 'path', '=', 'ur', '##lli', '##b', '.', 'un', '##qu', '##ote', '(', 'ur', '##l', '.', 'path', ')', '[', '1', ':', ']', '\\', 'n']
Detokenized (015): ['deco##ded_path', '=', 'ur##lli##b', '.', 'un##qu##ote', '(', 'ur##l', '.', 'path', ')', '[', '1', ':', ']', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "rules = . join ( req . requires ( ) ) . strip ( ) \n"
Original    (016): ['rules', '=', '.', 'join', '(', 'req', '.', 'requires', '(', ')', ')', '.', 'strip', '(', ')', '\\n']
Tokenized   (020): ['[CLS]', 'rules', '=', '.', 'join', '(', 're', '##q', '.', 'requires', '(', ')', ')', '.', 'strip', '(', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['rules', '=', '.', 'join', '(', 're', '##q', '.', 'requires', '(', ')', ')', '.', 'strip', '(', ')', '\\', 'n']
Detokenized (016): ['rules', '=', '.', 'join', '(', 're##q', '.', 'requires', '(', ')', ')', '.', 'strip', '(', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n"
Original    (018): ['domain', '=', 'req', '.', 'get_options', '(', ')', '.', 'get', '(', ',', 'req', '.', 'auth_name', '(', ')', ')', '\\n']
Tokenized   (028): ['[CLS]', 'domain', '=', 're', '##q', '.', 'get', '_', 'options', '(', ')', '.', 'get', '(', ',', 're', '##q', '.', 'au', '##th', '_', 'name', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['domain', '=', 're', '##q', '.', 'get', '_', 'options', '(', ')', '.', 'get', '(', ',', 're', '##q', '.', 'au', '##th', '_', 'name', '(', ')', ')', '\\', 'n']
Detokenized (018): ['domain', '=', 're##q', '.', 'get_options', '(', ')', '.', 'get', '(', ',', 're##q', '.', 'au##th_name', '(', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "auth_headers = req . headers_in . get ( , [ ] ) \n"
Original    (013): ['auth_headers', '=', 'req', '.', 'headers_in', '.', 'get', '(', ',', '[', ']', ')', '\\n']
Tokenized   (024): ['[CLS]', 'au', '##th', '_', 'header', '##s', '=', 're', '##q', '.', 'header', '##s', '_', 'in', '.', 'get', '(', ',', '[', ']', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['au', '##th', '_', 'header', '##s', '=', 're', '##q', '.', 'header', '##s', '_', 'in', '.', 'get', '(', ',', '[', ']', ')', '\\', 'n']
Detokenized (013): ['au##th_header##s', '=', 're##q', '.', 'header##s_in', '.', 'get', '(', ',', '[', ']', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "set_remote_user ( req , ah_data [ 1 ] , domain ) \n"
Original    (012): ['set_remote_user', '(', 'req', ',', 'ah_data', '[', '1', ']', ',', 'domain', ')', '\\n']
Tokenized   (022): ['[CLS]', 'set', '_', 'remote', '_', 'user', '(', 're', '##q', ',', 'ah', '_', 'data', '[', '1', ']', ',', 'domain', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['set', '_', 'remote', '_', 'user', '(', 're', '##q', ',', 'ah', '_', 'data', '[', '1', ']', ',', 'domain', ')', '\\', 'n']
Detokenized (012): ['set_remote_user', '(', 're##q', ',', 'ah_data', '[', '1', ']', ',', 'domain', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "dict = json . loads ( request . data . decode ( ) ) \n"
Original    (015): ['dict', '=', 'json', '.', 'loads', '(', 'request', '.', 'data', '.', 'decode', '(', ')', ')', '\\n']
Tokenized   (021): ['[CLS]', 'di', '##ct', '=', 'j', '##son', '.', 'loads', '(', 'request', '.', 'data', '.', 'deco', '##de', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['di', '##ct', '=', 'j', '##son', '.', 'loads', '(', 'request', '.', 'data', '.', 'deco', '##de', '(', ')', ')', '\\', 'n']
Detokenized (015): ['di##ct', '=', 'j##son', '.', 'loads', '(', 'request', '.', 'data', '.', 'deco##de', '(', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "rv = self . app . delete ( . format ( id ) ) \n"
Original    (015): ['rv', '=', 'self', '.', 'app', '.', 'delete', '(', '.', 'format', '(', 'id', ')', ')', '\\n']
Tokenized   (019): ['[CLS]', 'rv', '=', 'self', '.', 'app', '.', 'del', '##ete', '(', '.', 'format', '(', 'id', ')', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['rv', '=', 'self', '.', 'app', '.', 'del', '##ete', '(', '.', 'format', '(', 'id', ')', ')', '\\', 'n']
Detokenized (015): ['rv', '=', 'self', '.', 'app', '.', 'del##ete', '(', '.', 'format', '(', 'id', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "configure_logging ( "index_pages_logging.config" , "index_pages.log" ) \n"
Original    (007): ['configure_logging', '(', '"index_pages_logging.config"', ',', '"index_pages.log"', ')', '\\n']
Tokenized   (031): ['[CLS]', 'con', '##fi', '##gur', '##e', '_', 'logging', '(', '"', 'index', '_', 'pages', '_', 'logging', '.', 'con', '##fi', '##g', '"', ',', '"', 'index', '_', 'pages', '.', 'log', '"', ')', '\\', 'n', '[SEP]']
Filtered   (029): ['con', '##fi', '##gur', '##e', '_', 'logging', '(', '"', 'index', '_', 'pages', '_', 'logging', '.', 'con', '##fi', '##g', '"', ',', '"', 'index', '_', 'pages', '.', 'log', '"', ')', '\\', 'n']
Detokenized (007): ['con##fi##gur##e_logging', '(', '"index_pages_logging.con##fi##g"', ',', '"index_pages.log"', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "ocr_file = join ( dir , ) \n"
Original    (008): ['ocr_file', '=', 'join', '(', 'dir', ',', ')', '\\n']
Tokenized   (014): ['[CLS]', 'o', '##cr', '_', 'file', '=', 'join', '(', 'dir', ',', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['o', '##cr', '_', 'file', '=', 'join', '(', 'dir', ',', ')', '\\', 'n']
Detokenized (008): ['o##cr_file', '=', 'join', '(', 'dir', ',', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "expected_text = { "eng" : file ( join ( dir , ) ) . read ( ) . decode ( ) } \n"
Original    (023): ['expected_text', '=', '{', '"eng"', ':', 'file', '(', 'join', '(', 'dir', ',', ')', ')', '.', 'read', '(', ')', '.', 'decode', '(', ')', '}', '\\n']
Tokenized   (031): ['[CLS]', 'expected', '_', 'text', '=', '{', '"', 'eng', '"', ':', 'file', '(', 'join', '(', 'dir', ',', ')', ')', '.', 'read', '(', ')', '.', 'deco', '##de', '(', ')', '}', '\\', 'n', '[SEP]']
Filtered   (029): ['expected', '_', 'text', '=', '{', '"', 'eng', '"', ':', 'file', '(', 'join', '(', 'dir', ',', ')', ')', '.', 'read', '(', ')', '.', 'deco', '##de', '(', ')', '}', '\\', 'n']
Detokenized (023): ['expected_text', '=', '{', '"eng"', ':', 'file', '(', 'join', '(', 'dir', ',', ')', ')', '.', 'read', '(', ')', '.', 'deco##de', '(', ')', '}', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "tuples . append ( ( os . path . join ( root , pair [ 0 ] ) , np . int32 ( pair [ 1 ] ) ) ) \n"
Original    (031): ['tuples', '.', 'append', '(', '(', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'pair', '[', '0', ']', ')', ',', 'np', '.', 'int32', '(', 'pair', '[', '1', ']', ')', ')', ')', '\\n']
Tokenized   (037): ['[CLS]', 'tu', '##ples', '.', 'app', '##end', '(', '(', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'pair', '[', '0', ']', ')', ',', 'np', '.', 'int', '##32', '(', 'pair', '[', '1', ']', ')', ')', ')', '\\', 'n', '[SEP]']
Filtered   (035): ['tu', '##ples', '.', 'app', '##end', '(', '(', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'pair', '[', '0', ']', ')', ',', 'np', '.', 'int', '##32', '(', 'pair', '[', '1', ']', ')', ')', ')', '\\', 'n']
Detokenized (031): ['tu##ples', '.', 'app##end', '(', '(', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'pair', '[', '0', ']', ')', ',', 'np', '.', 'int##32', '(', 'pair', '[', '1', ']', ')', ')', ')', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 31, 768)
# Extracted words:  31
Sentence         : "val_list = load_image_list ( args . val , args . root ) \n"
Original    (013): ['val_list', '=', 'load_image_list', '(', 'args', '.', 'val', ',', 'args', '.', 'root', ')', '\\n']
Tokenized   (024): ['[CLS]', 'val', '_', 'list', '=', 'load', '_', 'image', '_', 'list', '(', 'ar', '##gs', '.', 'val', ',', 'ar', '##gs', '.', 'root', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['val', '_', 'list', '=', 'load', '_', 'image', '_', 'list', '(', 'ar', '##gs', '.', 'val', ',', 'ar', '##gs', '.', 'root', ')', '\\', 'n']
Detokenized (013): ['val_list', '=', 'load_image_list', '(', 'ar##gs', '.', 'val', ',', 'ar##gs', '.', 'root', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "mean_image = pickle . load ( open ( args . mean , ) ) \n"
Original    (015): ['mean_image', '=', 'pickle', '.', 'load', '(', 'open', '(', 'args', '.', 'mean', ',', ')', ')', '\\n']
Tokenized   (022): ['[CLS]', 'mean', '_', 'image', '=', 'pick', '##le', '.', 'load', '(', 'open', '(', 'ar', '##gs', '.', 'mean', ',', ')', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['mean', '_', 'image', '=', 'pick', '##le', '.', 'load', '(', 'open', '(', 'ar', '##gs', '.', 'mean', ',', ')', ')', '\\', 'n']
Detokenized (015): ['mean_image', '=', 'pick##le', '.', 'load', '(', 'open', '(', 'ar##gs', '.', 'mean', ',', ')', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "cropwidth = 256 - model . insize \n"
Original    (008): ['cropwidth', '=', '256', '-', 'model', '.', 'insize', '\\n']
Tokenized   (015): ['[CLS]', 'crop', '##wi', '##dt', '##h', '=', '256', '-', 'model', '.', 'ins', '##ize', '\\', 'n', '[SEP]']
Filtered   (013): ['crop', '##wi', '##dt', '##h', '=', '256', '-', 'model', '.', 'ins', '##ize', '\\', 'n']
Detokenized (008): ['crop##wi##dt##h', '=', '256', '-', 'model', '.', 'ins##ize', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "left = random . randint ( 0 , cropwidth - 1 ) \n"
Original    (013): ['left', '=', 'random', '.', 'randint', '(', '0', ',', 'cropwidth', '-', '1', ')', '\\n']
Tokenized   (020): ['[CLS]', 'left', '=', 'random', '.', 'rand', '##int', '(', '0', ',', 'crop', '##wi', '##dt', '##h', '-', '1', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['left', '=', 'random', '.', 'rand', '##int', '(', '0', ',', 'crop', '##wi', '##dt', '##h', '-', '1', ')', '\\', 'n']
Detokenized (013): ['left', '=', 'random', '.', 'rand##int', '(', '0', ',', 'crop##wi##dt##h', '-', '1', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "image /= 255 \n"
Original    (004): ['image', '/=', '255', '\\n']
Tokenized   (008): ['[CLS]', 'image', '/', '=', '255', '\\', 'n', '[SEP]']
Filtered   (006): ['image', '/', '=', '255', '\\', 'n']
Detokenized (004): ['image', '/=', '255', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "val_batch_pool = [ None ] * args . val_batchsize \n"
Original    (010): ['val_batch_pool', '=', '[', 'None', ']', '*', 'args', '.', 'val_batchsize', '\\n']
Tokenized   (022): ['[CLS]', 'val', '_', 'batch', '_', 'pool', '=', '[', 'none', ']', '*', 'ar', '##gs', '.', 'val', '_', 'batch', '##si', '##ze', '\\', 'n', '[SEP]']
Filtered   (020): ['val', '_', 'batch', '_', 'pool', '=', '[', 'none', ']', '*', 'ar', '##gs', '.', 'val', '_', 'batch', '##si', '##ze', '\\', 'n']
Detokenized (010): ['val_batch_pool', '=', '[', 'none', ']', '*', 'ar##gs', '.', 'val_batch##si##ze', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "perm = np . random . permutation ( len ( train_list ) ) \n"
Original    (014): ['perm', '=', 'np', '.', 'random', '.', 'permutation', '(', 'len', '(', 'train_list', ')', ')', '\\n']
Tokenized   (022): ['[CLS]', 'per', '##m', '=', 'np', '.', 'random', '.', 'per', '##mut', '##ation', '(', 'len', '(', 'train', '_', 'list', ')', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['per', '##m', '=', 'np', '.', 'random', '.', 'per', '##mut', '##ation', '(', 'len', '(', 'train', '_', 'list', ')', ')', '\\', 'n']
Detokenized (014): ['per##m', '=', 'np', '.', 'random', '.', 'per##mut##ation', '(', 'len', '(', 'train_list', ')', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "batch_pool [ i ] = pool . apply_async ( read_image , ( path , False , True ) ) \n"
Original    (020): ['batch_pool', '[', 'i', ']', '=', 'pool', '.', 'apply_async', '(', 'read_image', ',', '(', 'path', ',', 'False', ',', 'True', ')', ')', '\\n']
Tokenized   (031): ['[CLS]', 'batch', '_', 'pool', '[', 'i', ']', '=', 'pool', '.', 'apply', '_', 'as', '##yn', '##c', '(', 'read', '_', 'image', ',', '(', 'path', ',', 'false', ',', 'true', ')', ')', '\\', 'n', '[SEP]']
Filtered   (029): ['batch', '_', 'pool', '[', 'i', ']', '=', 'pool', '.', 'apply', '_', 'as', '##yn', '##c', '(', 'read', '_', 'image', ',', '(', 'path', ',', 'false', ',', 'true', ')', ')', '\\', 'n']
Detokenized (020): ['batch_pool', '[', 'i', ']', '=', 'pool', '.', 'apply_as##yn##c', '(', 'read_image', ',', '(', 'path', ',', 'false', ',', 'true', ')', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "read_image , ( path , True , False ) ) \n"
Original    (011): ['read_image', ',', '(', 'path', ',', 'True', ',', 'False', ')', ')', '\\n']
Tokenized   (016): ['[CLS]', 'read', '_', 'image', ',', '(', 'path', ',', 'true', ',', 'false', ')', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['read', '_', 'image', ',', '(', 'path', ',', 'true', ',', 'false', ')', ')', '\\', 'n']
Detokenized (011): ['read_image', ',', '(', 'path', ',', 'true', ',', 'false', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "val_count = val_loss = val_accuracy = 0 \n"
Original    (008): ['val_count', '=', 'val_loss', '=', 'val_accuracy', '=', '0', '\\n']
Tokenized   (017): ['[CLS]', 'val', '_', 'count', '=', 'val', '_', 'loss', '=', 'val', '_', 'accuracy', '=', '0', '\\', 'n', '[SEP]']
Filtered   (015): ['val', '_', 'count', '=', 'val', '_', 'loss', '=', 'val', '_', 'accuracy', '=', '0', '\\', 'n']
Detokenized (008): ['val_count', '=', 'val_loss', '=', 'val_accuracy', '=', '0', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "duration = time . time ( ) - val_begin_at \n"
Original    (010): ['duration', '=', 'time', '.', 'time', '(', ')', '-', 'val_begin_at', '\\n']
Tokenized   (017): ['[CLS]', 'duration', '=', 'time', '.', 'time', '(', ')', '-', 'val', '_', 'begin', '_', 'at', '\\', 'n', '[SEP]']
Filtered   (015): ['duration', '=', 'time', '.', 'time', '(', ')', '-', 'val', '_', 'begin', '_', 'at', '\\', 'n']
Detokenized (010): ['duration', '=', 'time', '.', 'time', '(', ')', '-', 'val_begin_at', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "mean_error = 1 - val_accuracy * args . val_batchsize / 50000 \n"
Original    (012): ['mean_error', '=', '1', '-', 'val_accuracy', '*', 'args', '.', 'val_batchsize', '/', '50000', '\\n']
Tokenized   (025): ['[CLS]', 'mean', '_', 'error', '=', '1', '-', 'val', '_', 'accuracy', '*', 'ar', '##gs', '.', 'val', '_', 'batch', '##si', '##ze', '/', '5000', '##0', '\\', 'n', '[SEP]']
Filtered   (023): ['mean', '_', 'error', '=', '1', '-', 'val', '_', 'accuracy', '*', 'ar', '##gs', '.', 'val', '_', 'batch', '##si', '##ze', '/', '5000', '##0', '\\', 'n']
Detokenized (012): ['mean_error', '=', '1', '-', 'val_accuracy', '*', 'ar##gs', '.', 'val_batch##si##ze', '/', '5000##0', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "serializers . save_hdf5 ( args . outstate , optimizer ) \n"
Original    (011): ['serializers', '.', 'save_hdf5', '(', 'args', '.', 'outstate', ',', 'optimizer', ')', '\\n']
Tokenized   (024): ['[CLS]', 'serial', '##izer', '##s', '.', 'save', '_', 'hd', '##f', '##5', '(', 'ar', '##gs', '.', 'outs', '##tate', ',', 'opt', '##imi', '##zer', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['serial', '##izer', '##s', '.', 'save', '_', 'hd', '##f', '##5', '(', 'ar', '##gs', '.', 'outs', '##tate', ',', 'opt', '##imi', '##zer', ')', '\\', 'n']
Detokenized (011): ['serial##izer##s', '.', 'save_hd##f##5', '(', 'ar##gs', '.', 'outs##tate', ',', 'opt##imi##zer', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "boards_name = [ slugify ( b [ ] ) for b in SETTINGS . get ( , { } ) . values ( ) ] \n"
Original    (026): ['boards_name', '=', '[', 'slugify', '(', 'b', '[', ']', ')', 'for', 'b', 'in', 'SETTINGS', '.', 'get', '(', ',', '{', '}', ')', '.', 'values', '(', ')', ']', '\\n']
Tokenized   (032): ['[CLS]', 'boards', '_', 'name', '=', '[', 'slug', '##ify', '(', 'b', '[', ']', ')', 'for', 'b', 'in', 'settings', '.', 'get', '(', ',', '{', '}', ')', '.', 'values', '(', ')', ']', '\\', 'n', '[SEP]']
Filtered   (030): ['boards', '_', 'name', '=', '[', 'slug', '##ify', '(', 'b', '[', ']', ')', 'for', 'b', 'in', 'settings', '.', 'get', '(', ',', '{', '}', ')', '.', 'values', '(', ')', ']', '\\', 'n']
Detokenized (026): ['boards_name', '=', '[', 'slug##ify', '(', 'b', '[', ']', ')', 'for', 'b', 'in', 'settings', '.', 'get', '(', ',', '{', '}', ')', '.', 'values', '(', ')', ']', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "LOGGING . info ( . format ( board_name , result ) ) \n"
Original    (013): ['LOGGING', '.', 'info', '(', '.', 'format', '(', 'board_name', ',', 'result', ')', ')', '\\n']
Tokenized   (018): ['[CLS]', 'logging', '.', 'info', '(', '.', 'format', '(', 'board', '_', 'name', ',', 'result', ')', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['logging', '.', 'info', '(', '.', 'format', '(', 'board', '_', 'name', ',', 'result', ')', ')', '\\', 'n']
Detokenized (013): ['logging', '.', 'info', '(', '.', 'format', '(', 'board_name', ',', 'result', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "is_py2 = _ver [ 0 ] == 2 \n"
Original    (009): ['is_py2', '=', '_ver', '[', '0', ']', '==', '2', '\\n']
Tokenized   (019): ['[CLS]', 'is', '_', 'p', '##y', '##2', '=', '_', 've', '##r', '[', '0', ']', '=', '=', '2', '\\', 'n', '[SEP]']
Filtered   (017): ['is', '_', 'p', '##y', '##2', '=', '_', 've', '##r', '[', '0', ']', '=', '=', '2', '\\', 'n']
Detokenized (009): ['is_p##y##2', '=', '_ve##r', '[', '0', ']', '==', '2', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "is_py2_7_9_or_later = _ver [ 0 ] >= 2 and _ver [ 1 ] >= 7 and _ver [ 2 ] >= 9 \n"
Original    (023): ['is_py2_7_9_or_later', '=', '_ver', '[', '0', ']', '>=', '2', 'and', '_ver', '[', '1', ']', '>=', '7', 'and', '_ver', '[', '2', ']', '>=', '9', '\\n']
Tokenized   (047): ['[CLS]', 'is', '_', 'p', '##y', '##2', '_', '7', '_', '9', '_', 'or', '_', 'later', '=', '_', 've', '##r', '[', '0', ']', '>', '=', '2', 'and', '_', 've', '##r', '[', '1', ']', '>', '=', '7', 'and', '_', 've', '##r', '[', '2', ']', '>', '=', '9', '\\', 'n', '[SEP]']
Filtered   (045): ['is', '_', 'p', '##y', '##2', '_', '7', '_', '9', '_', 'or', '_', 'later', '=', '_', 've', '##r', '[', '0', ']', '>', '=', '2', 'and', '_', 've', '##r', '[', '1', ']', '>', '=', '7', 'and', '_', 've', '##r', '[', '2', ']', '>', '=', '9', '\\', 'n']
Detokenized (023): ['is_p##y##2_7_9_or_later', '=', '_ve##r', '[', '0', ']', '>=', '2', 'and', '_ve##r', '[', '1', ']', '>=', '7', 'and', '_ve##r', '[', '2', ']', '>=', '9', '\\n']
Counter: 45
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "is_py3_3 = is_py3 and _ver [ 1 ] == 3 \n"
Original    (011): ['is_py3_3', '=', 'is_py3', 'and', '_ver', '[', '1', ']', '==', '3', '\\n']
Tokenized   (027): ['[CLS]', 'is', '_', 'p', '##y', '##3', '_', '3', '=', 'is', '_', 'p', '##y', '##3', 'and', '_', 've', '##r', '[', '1', ']', '=', '=', '3', '\\', 'n', '[SEP]']
Filtered   (025): ['is', '_', 'p', '##y', '##3', '_', '3', '=', 'is', '_', 'p', '##y', '##3', 'and', '_', 've', '##r', '[', '1', ']', '=', '=', '3', '\\', 'n']
Detokenized (011): ['is_p##y##3_3', '=', 'is_p##y##3', 'and', '_ve##r', '[', '1', ']', '==', '3', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "strategy = zlib . Z_DEFAULT_STRATEGY ) : \n"
Original    (008): ['strategy', '=', 'zlib', '.', 'Z_DEFAULT_STRATEGY', ')', ':', '\\n']
Tokenized   (016): ['[CLS]', 'strategy', '=', 'z', '##lib', '.', 'z', '_', 'default', '_', 'strategy', ')', ':', '\\', 'n', '[SEP]']
Filtered   (014): ['strategy', '=', 'z', '##lib', '.', 'z', '_', 'default', '_', 'strategy', ')', ':', '\\', 'n']
Detokenized (008): ['strategy', '=', 'z##lib', '.', 'z_default_strategy', ')', ':', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "secure = self . secure \n"
Original    (006): ['secure', '=', 'self', '.', 'secure', '\\n']
Tokenized   (009): ['[CLS]', 'secure', '=', 'self', '.', 'secure', '\\', 'n', '[SEP]']
Filtered   (007): ['secure', '=', 'self', '.', 'secure', '\\', 'n']
Detokenized (006): ['secure', '=', 'self', '.', 'secure', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "e . huffman_coder = HuffmanEncoder ( REQUEST_CODES , REQUEST_CODES_LENGTH ) \n"
Original    (011): ['e', '.', 'huffman_coder', '=', 'HuffmanEncoder', '(', 'REQUEST_CODES', ',', 'REQUEST_CODES_LENGTH', ')', '\\n']
Tokenized   (028): ['[CLS]', 'e', '.', 'huff', '##man', '_', 'code', '##r', '=', 'huff', '##man', '##en', '##code', '##r', '(', 'request', '_', 'codes', ',', 'request', '_', 'codes', '_', 'length', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['e', '.', 'huff', '##man', '_', 'code', '##r', '=', 'huff', '##man', '##en', '##code', '##r', '(', 'request', '_', 'codes', ',', 'request', '_', 'codes', '_', 'length', ')', '\\', 'n']
Detokenized (011): ['e', '.', 'huff##man_code##r', '=', 'huff##man##en##code##r', '(', 'request_codes', ',', 'request_codes_length', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "train_seq = corpus . read_sequence_list_conll ( input_data , max_sent_len = 15 , max_nr_sent = 1000 ) \n"
Original    (017): ['train_seq', '=', 'corpus', '.', 'read_sequence_list_conll', '(', 'input_data', ',', 'max_sent_len', '=', '15', ',', 'max_nr_sent', '=', '1000', ')', '\\n']
Tokenized   (040): ['[CLS]', 'train', '_', 'se', '##q', '=', 'corpus', '.', 'read', '_', 'sequence', '_', 'list', '_', 'con', '##ll', '(', 'input', '_', 'data', ',', 'max', '_', 'sent', '_', 'len', '=', '15', ',', 'max', '_', 'nr', '_', 'sent', '=', '1000', ')', '\\', 'n', '[SEP]']
Filtered   (038): ['train', '_', 'se', '##q', '=', 'corpus', '.', 'read', '_', 'sequence', '_', 'list', '_', 'con', '##ll', '(', 'input', '_', 'data', ',', 'max', '_', 'sent', '_', 'len', '=', '15', ',', 'max', '_', 'nr', '_', 'sent', '=', '1000', ')', '\\', 'n']
Detokenized (017): ['train_se##q', '=', 'corpus', '.', 'read_sequence_list_con##ll', '(', 'input_data', ',', 'max_sent_len', '=', '15', ',', 'max_nr_sent', '=', '1000', ')', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "pickle . dump ( ( corpus . word_dict , corpus . tag_dict ) , open ( , ) ) \n"
Original    (020): ['pickle', '.', 'dump', '(', '(', 'corpus', '.', 'word_dict', ',', 'corpus', '.', 'tag_dict', ')', ',', 'open', '(', ',', ')', ')', '\\n']
Tokenized   (030): ['[CLS]', 'pick', '##le', '.', 'dump', '(', '(', 'corpus', '.', 'word', '_', 'di', '##ct', ',', 'corpus', '.', 'tag', '_', 'di', '##ct', ')', ',', 'open', '(', ',', ')', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['pick', '##le', '.', 'dump', '(', '(', 'corpus', '.', 'word', '_', 'di', '##ct', ',', 'corpus', '.', 'tag', '_', 'di', '##ct', ')', ',', 'open', '(', ',', ')', ')', '\\', 'n']
Detokenized (020): ['pick##le', '.', 'dump', '(', '(', 'corpus', '.', 'word_di##ct', ',', 'corpus', '.', 'tag_di##ct', ')', ',', 'open', '(', ',', ')', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "total = np . zeros ( self . features . n_feats ) \n"
Original    (013): ['total', '=', 'np', '.', 'zeros', '(', 'self', '.', 'features', '.', 'n_feats', ')', '\\n']
Tokenized   (020): ['[CLS]', 'total', '=', 'np', '.', 'zero', '##s', '(', 'self', '.', 'features', '.', 'n', '_', 'feat', '##s', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['total', '=', 'np', '.', 'zero', '##s', '(', 'self', '.', 'features', '.', 'n', '_', 'feat', '##s', ')', '\\', 'n']
Detokenized (013): ['total', '=', 'np', '.', 'zero##s', '(', 'self', '.', 'features', '.', 'n_feat##s', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "scores = self . features . compute_scores ( feats , self . weights ) \n"
Original    (015): ['scores', '=', 'self', '.', 'features', '.', 'compute_scores', '(', 'feats', ',', 'self', '.', 'weights', ')', '\\n']
Tokenized   (021): ['[CLS]', 'scores', '=', 'self', '.', 'features', '.', 'compute', '_', 'scores', '(', 'feat', '##s', ',', 'self', '.', 'weights', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['scores', '=', 'self', '.', 'features', '.', 'compute', '_', 'scores', '(', 'feat', '##s', ',', 'self', '.', 'weights', ')', '\\', 'n']
Detokenized (015): ['scores', '=', 'self', '.', 'features', '.', 'compute_scores', '(', 'feat##s', ',', 'self', '.', 'weights', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "t0 = 1.0 / ( sigma * eta0 ) \n"
Original    (010): ['t0', '=', '1.0', '/', '(', 'sigma', '*', 'eta0', ')', '\\n']
Tokenized   (017): ['[CLS]', 't', '##0', '=', '1', '.', '0', '/', '(', 'sigma', '*', 'eta', '##0', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['t', '##0', '=', '1', '.', '0', '/', '(', 'sigma', '*', 'eta', '##0', ')', '\\', 'n']
Detokenized (010): ['t##0', '=', '1.0', '/', '(', 'sigma', '*', 'eta##0', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "marginals , logZ = self . decoder . parse_marginals_nonproj ( scores ) \n"
Original    (013): ['marginals', ',', 'logZ', '=', 'self', '.', 'decoder', '.', 'parse_marginals_nonproj', '(', 'scores', ')', '\\n']
Tokenized   (027): ['[CLS]', 'marginal', '##s', ',', 'log', '##z', '=', 'self', '.', 'deco', '##der', '.', 'par', '##se', '_', 'marginal', '##s', '_', 'non', '##pro', '##j', '(', 'scores', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['marginal', '##s', ',', 'log', '##z', '=', 'self', '.', 'deco', '##der', '.', 'par', '##se', '_', 'marginal', '##s', '_', 'non', '##pro', '##j', '(', 'scores', ')', '\\', 'n']
Detokenized (013): ['marginal##s', ',', 'log##z', '=', 'self', '.', 'deco##der', '.', 'par##se_marginal##s_non##pro##j', '(', 'scores', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "score_corr += scores [ h , m ] \n"
Original    (009): ['score_corr', '+=', 'scores', '[', 'h', ',', 'm', ']', '\\n']
Tokenized   (016): ['[CLS]', 'score', '_', 'co', '##rr', '+', '=', 'scores', '[', 'h', ',', 'm', ']', '\\', 'n', '[SEP]']
Filtered   (014): ['score', '_', 'co', '##rr', '+', '=', 'scores', '[', 'h', ',', 'm', ']', '\\', 'n']
Detokenized (009): ['score_co##rr', '+=', 'scores', '[', 'h', ',', 'm', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "features = self . add_final_features ( sequence , sequence . y [ - 1 ] , features ) \n"
Original    (019): ['features', '=', 'self', '.', 'add_final_features', '(', 'sequence', ',', 'sequence', '.', 'y', '[', '-', '1', ']', ',', 'features', ')', '\\n']
Tokenized   (026): ['[CLS]', 'features', '=', 'self', '.', 'add', '_', 'final', '_', 'features', '(', 'sequence', ',', 'sequence', '.', 'y', '[', '-', '1', ']', ',', 'features', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['features', '=', 'self', '.', 'add', '_', 'final', '_', 'features', '(', 'sequence', ',', 'sequence', '.', 'y', '[', '-', '1', ']', ',', 'features', ')', '\\', 'n']
Detokenized (019): ['features', '=', 'self', '.', 'add_final_features', '(', 'sequence', ',', 'sequence', '.', 'y', '[', '-', '1', ']', ',', 'features', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "node_idx = self . add_emission_features ( sequence , pos , y , node_idx ) \n"
Original    (015): ['node_idx', '=', 'self', '.', 'add_emission_features', '(', 'sequence', ',', 'pos', ',', 'y', ',', 'node_idx', ')', '\\n']
Tokenized   (029): ['[CLS]', 'node', '_', 'id', '##x', '=', 'self', '.', 'add', '_', 'emission', '_', 'features', '(', 'sequence', ',', 'po', '##s', ',', 'y', ',', 'node', '_', 'id', '##x', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['node', '_', 'id', '##x', '=', 'self', '.', 'add', '_', 'emission', '_', 'features', '(', 'sequence', ',', 'po', '##s', ',', 'y', ',', 'node', '_', 'id', '##x', ')', '\\', 'n']
Detokenized (015): ['node_id##x', '=', 'self', '.', 'add_emission_features', '(', 'sequence', ',', 'po##s', ',', 'y', ',', 'node_id##x', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "edge_idx = self . add_final_features ( sequence , y_prev , edge_idx ) \n"
Original    (013): ['edge_idx', '=', 'self', '.', 'add_final_features', '(', 'sequence', ',', 'y_prev', ',', 'edge_idx', ')', '\\n']
Tokenized   (029): ['[CLS]', 'edge', '_', 'id', '##x', '=', 'self', '.', 'add', '_', 'final', '_', 'features', '(', 'sequence', ',', 'y', '_', 'pre', '##v', ',', 'edge', '_', 'id', '##x', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['edge', '_', 'id', '##x', '=', 'self', '.', 'add', '_', 'final', '_', 'features', '(', 'sequence', ',', 'y', '_', 'pre', '##v', ',', 'edge', '_', 'id', '##x', ')', '\\', 'n']
Detokenized (013): ['edge_id##x', '=', 'self', '.', 'add_final_features', '(', 'sequence', ',', 'y_pre##v', ',', 'edge_id##x', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "y_name = self . dataset . y_dict . get_label_name ( y ) \n"
Original    (013): ['y_name', '=', 'self', '.', 'dataset', '.', 'y_dict', '.', 'get_label_name', '(', 'y', ')', '\\n']
Tokenized   (026): ['[CLS]', 'y', '_', 'name', '=', 'self', '.', 'data', '##set', '.', 'y', '_', 'di', '##ct', '.', 'get', '_', 'label', '_', 'name', '(', 'y', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['y', '_', 'name', '=', 'self', '.', 'data', '##set', '.', 'y', '_', 'di', '##ct', '.', 'get', '_', 'label', '_', 'name', '(', 'y', ')', '\\', 'n']
Detokenized (013): ['y_name', '=', 'self', '.', 'data##set', '.', 'y_di##ct', '.', 'get_label_name', '(', 'y', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "feat_name = "prev_tag:%s::%s" % ( y_prev_name , y_name ) \n"
Original    (010): ['feat_name', '=', '"prev_tag:%s::%s"', '%', '(', 'y_prev_name', ',', 'y_name', ')', '\\n']
Tokenized   (034): ['[CLS]', 'feat', '_', 'name', '=', '"', 'pre', '##v', '_', 'tag', ':', '%', 's', ':', ':', '%', 's', '"', '%', '(', 'y', '_', 'pre', '##v', '_', 'name', ',', 'y', '_', 'name', ')', '\\', 'n', '[SEP]']
Filtered   (032): ['feat', '_', 'name', '=', '"', 'pre', '##v', '_', 'tag', ':', '%', 's', ':', ':', '%', 's', '"', '%', '(', 'y', '_', 'pre', '##v', '_', 'name', ',', 'y', '_', 'name', ')', '\\', 'n']
Detokenized (010): ['feat_name', '=', '"pre##v_tag:%s::%s"', '%', '(', 'y_pre##v_name', ',', 'y_name', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "point_geom . AddPoint ( point [ 0 ] , point [ 1 ] ) \n"
Original    (015): ['point_geom', '.', 'AddPoint', '(', 'point', '[', '0', ']', ',', 'point', '[', '1', ']', ')', '\\n']
Tokenized   (022): ['[CLS]', 'point', '_', 'geo', '##m', '.', 'add', '##point', '(', 'point', '[', '0', ']', ',', 'point', '[', '1', ']', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['point', '_', 'geo', '##m', '.', 'add', '##point', '(', 'point', '[', '0', ']', ',', 'point', '[', '1', ']', ')', '\\', 'n']
Detokenized (015): ['point_geo##m', '.', 'add##point', '(', 'point', '[', '0', ']', ',', 'point', '[', '1', ']', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "longitudes = [ 100 , 110 , 120 , 130 , 140 ] \n"
Original    (014): ['longitudes', '=', '[', '100', ',', '110', ',', '120', ',', '130', ',', '140', ']', '\\n']
Tokenized   (018): ['[CLS]', 'longitude', '##s', '=', '[', '100', ',', '110', ',', '120', ',', '130', ',', '140', ']', '\\', 'n', '[SEP]']
Filtered   (016): ['longitude', '##s', '=', '[', '100', ',', '110', ',', '120', ',', '130', ',', '140', ']', '\\', 'n']
Detokenized (014): ['longitude##s', '=', '[', '100', ',', '110', ',', '120', ',', '130', ',', '140', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "point_1 . AddPoint ( longitudes [ 0 ] , latitudes [ 0 ] , elevation ) \n"
Original    (017): ['point_1', '.', 'AddPoint', '(', 'longitudes', '[', '0', ']', ',', 'latitudes', '[', '0', ']', ',', 'elevation', ')', '\\n']
Tokenized   (025): ['[CLS]', 'point', '_', '1', '.', 'add', '##point', '(', 'longitude', '##s', '[', '0', ']', ',', 'latitude', '##s', '[', '0', ']', ',', 'elevation', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['point', '_', '1', '.', 'add', '##point', '(', 'longitude', '##s', '[', '0', ']', ',', 'latitude', '##s', '[', '0', ']', ',', 'elevation', ')', '\\', 'n']
Detokenized (017): ['point_1', '.', 'add##point', '(', 'longitude##s', '[', '0', ']', ',', 'latitude##s', '[', '0', ']', ',', 'elevation', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "di = np . array ( [ x for x in range ( projected_X . shape [ 1 ] ) ] ) \n"
Original    (023): ['di', '=', 'np', '.', 'array', '(', '[', 'x', 'for', 'x', 'in', 'range', '(', 'projected_X', '.', 'shape', '[', '1', ']', ')', ']', ')', '\\n']
Tokenized   (028): ['[CLS]', 'di', '=', 'np', '.', 'array', '(', '[', 'x', 'for', 'x', 'in', 'range', '(', 'projected', '_', 'x', '.', 'shape', '[', '1', ']', ')', ']', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['di', '=', 'np', '.', 'array', '(', '[', 'x', 'for', 'x', 'in', 'range', '(', 'projected', '_', 'x', '.', 'shape', '[', '1', ']', ')', ']', ')', '\\', 'n']
Detokenized (023): ['di', '=', 'np', '.', 'array', '(', '[', 'x', 'for', 'x', 'in', 'range', '(', 'projected_x', '.', 'shape', '[', '1', ']', ')', ']', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "projected_X = np . c_ [ ids , projected_X ] \n"
Original    (011): ['projected_X', '=', 'np', '.', 'c_', '[', 'ids', ',', 'projected_X', ']', '\\n']
Tokenized   (020): ['[CLS]', 'projected', '_', 'x', '=', 'np', '.', 'c', '_', '[', 'id', '##s', ',', 'projected', '_', 'x', ']', '\\', 'n', '[SEP]']
Filtered   (018): ['projected', '_', 'x', '=', 'np', '.', 'c', '_', '[', 'id', '##s', ',', 'projected', '_', 'x', ']', '\\', 'n']
Detokenized (011): ['projected_x', '=', 'np', '.', 'c_', '[', 'id##s', ',', 'projected_x', ']', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "clusterer . fit ( inverse_x [ : , 1 : ] ) \n"
Original    (013): ['clusterer', '.', 'fit', '(', 'inverse_x', '[', ':', ',', '1', ':', ']', ')', '\\n']
Tokenized   (019): ['[CLS]', 'cluster', '##er', '.', 'fit', '(', 'inverse', '_', 'x', '[', ':', ',', '1', ':', ']', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['cluster', '##er', '.', 'fit', '(', 'inverse', '_', 'x', '[', ':', ',', '1', ':', ']', ')', '\\', 'n']
Detokenized (013): ['cluster##er', '.', 'fit', '(', 'inverse_x', '[', ':', ',', '1', ':', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "complex [ "meta" ] = self . projection \n"
Original    (009): ['complex', '[', '"meta"', ']', '=', 'self', '.', 'projection', '\\n']
Tokenized   (014): ['[CLS]', 'complex', '[', '"', 'meta', '"', ']', '=', 'self', '.', 'projection', '\\', 'n', '[SEP]']
Filtered   (012): ['complex', '[', '"', 'meta', '"', ']', '=', 'self', '.', 'projection', '\\', 'n']
Detokenized (009): ['complex', '[', '"meta"', ']', '=', 'self', '.', 'projection', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( complex ~~ k2e [ k ] = e \n"
Original    (040): ['json_s', '[', '"nodes"', ']', '.', 'append', '(', '{', '"name"', ':', 'str', '(', 'k', ')', ',', '"tooltip"', ':', 'tooltip_s', ',', '"group"', ':', '2', '*', 'int', '(', 'np', '.', 'log', '(', 'len', '(', 'complex', '~~', 'k2e', '[', 'k', ']', '=', 'e', '\\n']
Tokenized   (063): ['[CLS]', 'j', '##son', '_', 's', '[', '"', 'nodes', '"', ']', '.', 'app', '##end', '(', '{', '"', 'name', '"', ':', 'st', '##r', '(', 'k', ')', ',', '"', 'tool', '##tip', '"', ':', 'tool', '##tip', '_', 's', ',', '"', 'group', '"', ':', '2', '*', 'int', '(', 'np', '.', 'log', '(', 'len', '(', 'complex', '~', '~', 'k', '##2', '##e', '[', 'k', ']', '=', 'e', '\\', 'n', '[SEP]']
Filtered   (061): ['j', '##son', '_', 's', '[', '"', 'nodes', '"', ']', '.', 'app', '##end', '(', '{', '"', 'name', '"', ':', 'st', '##r', '(', 'k', ')', ',', '"', 'tool', '##tip', '"', ':', 'tool', '##tip', '_', 's', ',', '"', 'group', '"', ':', '2', '*', 'int', '(', 'np', '.', 'log', '(', 'len', '(', 'complex', '~', '~', 'k', '##2', '##e', '[', 'k', ']', '=', 'e', '\\', 'n']
Detokenized (040): ['j##son_s', '[', '"nodes"', ']', '.', 'app##end', '(', '{', '"name"', ':', 'st##r', '(', 'k', ')', ',', '"tool##tip"', ':', 'tool##tip_s', ',', '"group"', ':', '2', '*', 'int', '(', 'np', '.', 'log', '(', 'len', '(', 'complex', '~~', 'k##2##e', '[', 'k', ']', '=', 'e', '\\n']
Counter: 61
===================================================================
Hidden states:  (13, 40, 768)
# Extracted words:  40
Sentence         : "width_js = "%s" % width_html \n"
Original    (006): ['width_js', '=', '"%s"', '%', 'width_html', '\\n']
Tokenized   (017): ['[CLS]', 'width', '_', 'j', '##s', '=', '"', '%', 's', '"', '%', 'width', '_', 'html', '\\', 'n', '[SEP]']
Filtered   (015): ['width', '_', 'j', '##s', '=', '"', '%', 's', '"', '%', 'width', '_', 'html', '\\', 'n']
Detokenized (006): ['width_j##s', '=', '"%s"', '%', 'width_html', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "new_settings [ interface ] [ ] [ % protocol ] = server \n"
Original    (013): ['new_settings', '[', 'interface', ']', '[', ']', '[', '%', 'protocol', ']', '=', 'server', '\\n']
Tokenized   (018): ['[CLS]', 'new', '_', 'settings', '[', 'interface', ']', '[', ']', '[', '%', 'protocol', ']', '=', 'server', '\\', 'n', '[SEP]']
Filtered   (016): ['new', '_', 'settings', '[', 'interface', ']', '[', ']', '[', '%', 'protocol', ']', '=', 'server', '\\', 'n']
Detokenized (013): ['new_settings', '[', 'interface', ']', '[', ']', '[', '%', 'protocol', ']', '=', 'server', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "setups = ( less_setup , sass_setup , stylus_setup , sass_erb_setup ) \n"
Original    (012): ['setups', '=', '(', 'less_setup', ',', 'sass_setup', ',', 'stylus_setup', ',', 'sass_erb_setup', ')', '\\n']
Tokenized   (030): ['[CLS]', 'setup', '##s', '=', '(', 'less', '_', 'setup', ',', 'sas', '##s', '_', 'setup', ',', 'st', '##ylus', '_', 'setup', ',', 'sas', '##s', '_', 'er', '##b', '_', 'setup', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['setup', '##s', '=', '(', 'less', '_', 'setup', ',', 'sas', '##s', '_', 'setup', ',', 'st', '##ylus', '_', 'setup', ',', 'sas', '##s', '_', 'er', '##b', '_', 'setup', ')', '\\', 'n']
Detokenized (012): ['setup##s', '=', '(', 'less_setup', ',', 'sas##s_setup', ',', 'st##ylus_setup', ',', 'sas##s_er##b_setup', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "fn = self . view . file_name ( ) . encode ( "utf_8" ) \n"
Original    (015): ['fn', '=', 'self', '.', 'view', '.', 'file_name', '(', ')', '.', 'encode', '(', '"utf_8"', ')', '\\n']
Tokenized   (027): ['[CLS]', 'f', '##n', '=', 'self', '.', 'view', '.', 'file', '_', 'name', '(', ')', '.', 'en', '##code', '(', '"', 'ut', '##f', '_', '8', '"', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['f', '##n', '=', 'self', '.', 'view', '.', 'file', '_', 'name', '(', ')', '.', 'en', '##code', '(', '"', 'ut', '##f', '_', '8', '"', ')', '\\', 'n']
Detokenized (015): ['f##n', '=', 'self', '.', 'view', '.', 'file_name', '(', ')', '.', 'en##code', '(', '"ut##f_8"', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "compiled_regex = re . compile ( chosen_setup . regex , re . MULTILINE ) \n"
Original    (015): ['compiled_regex', '=', 're', '.', 'compile', '(', 'chosen_setup', '.', 'regex', ',', 're', '.', 'MULTILINE', ')', '\\n']
Tokenized   (026): ['[CLS]', 'compiled', '_', 'reg', '##ex', '=', 're', '.', 'com', '##pile', '(', 'chosen', '_', 'setup', '.', 'reg', '##ex', ',', 're', '.', 'multi', '##line', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['compiled', '_', 'reg', '##ex', '=', 're', '.', 'com', '##pile', '(', 'chosen', '_', 'setup', '.', 'reg', '##ex', ',', 're', '.', 'multi', '##line', ')', '\\', 'n']
Detokenized (015): ['compiled_reg##ex', '=', 're', '.', 'com##pile', '(', 'chosen_setup', '.', 'reg##ex', ',', 're', '.', 'multi##line', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "file_dir = os . path . dirname ( fn ) . decode ( "utf-8" ) \n"
Original    (016): ['file_dir', '=', 'os', '.', 'path', '.', 'dirname', '(', 'fn', ')', '.', 'decode', '(', '"utf-8"', ')', '\\n']
Tokenized   (029): ['[CLS]', 'file', '_', 'dir', '=', 'os', '.', 'path', '.', 'dir', '##name', '(', 'f', '##n', ')', '.', 'deco', '##de', '(', '"', 'ut', '##f', '-', '8', '"', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['file', '_', 'dir', '=', 'os', '.', 'path', '.', 'dir', '##name', '(', 'f', '##n', ')', '.', 'deco', '##de', '(', '"', 'ut', '##f', '-', '8', '"', ')', '\\', 'n']
Detokenized (016): ['file_dir', '=', 'os', '.', 'path', '.', 'dir##name', '(', 'f##n', ')', '.', 'deco##de', '(', '"ut##f-8"', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "partial_filename = fn_split [ 0 ] + "/_" + fn_split [ 1 ] \n"
Original    (014): ['partial_filename', '=', 'fn_split', '[', '0', ']', '+', '"/_"', '+', 'fn_split', '[', '1', ']', '\\n']
Tokenized   (029): ['[CLS]', 'partial', '_', 'file', '##name', '=', 'f', '##n', '_', 'split', '[', '0', ']', '+', '"', '/', '_', '"', '+', 'f', '##n', '_', 'split', '[', '1', ']', '\\', 'n', '[SEP]']
Filtered   (027): ['partial', '_', 'file', '##name', '=', 'f', '##n', '_', 'split', '[', '0', ']', '+', '"', '/', '_', '"', '+', 'f', '##n', '_', 'split', '[', '1', ']', '\\', 'n']
Detokenized (014): ['partial_file##name', '=', 'f##n_split', '[', '0', ']', '+', '"/_"', '+', 'f##n_split', '[', '1', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "imported_vars = imported_vars + m \n"
Original    (006): ['imported_vars', '=', 'imported_vars', '+', 'm', '\\n']
Tokenized   (015): ['[CLS]', 'imported', '_', 'var', '##s', '=', 'imported', '_', 'var', '##s', '+', 'm', '\\', 'n', '[SEP]']
Filtered   (013): ['imported', '_', 'var', '##s', '=', 'imported', '_', 'var', '##s', '+', 'm', '\\', 'n']
Detokenized (006): ['imported_var##s', '=', 'imported_var##s', '+', 'm', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : ""params" : [ { \n"
Original    (005): ['"params"', ':', '[', '{', '\\n']
Tokenized   (011): ['[CLS]', '"', 'para', '##ms', '"', ':', '[', '{', '\\', 'n', '[SEP]']
Filtered   (009): ['"', 'para', '##ms', '"', ':', '[', '{', '\\', 'n']
Detokenized (005): ['"para##ms"', ':', '[', '{', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "LAT_MAX = + 90.0 \n"
Original    (005): ['LAT_MAX', '=', '+', '90.0', '\\n']
Tokenized   (013): ['[CLS]', 'la', '##t', '_', 'max', '=', '+', '90', '.', '0', '\\', 'n', '[SEP]']
Filtered   (011): ['la', '##t', '_', 'max', '=', '+', '90', '.', '0', '\\', 'n']
Detokenized (005): ['la##t_max', '=', '+', '90.0', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""purple" , "teal" , "lightgray" ] \n"
Original    (007): ['"purple"', ',', '"teal"', ',', '"lightgray"', ']', '\\n']
Tokenized   (019): ['[CLS]', '"', 'purple', '"', ',', '"', 'tea', '##l', '"', ',', '"', 'light', '##gra', '##y', '"', ']', '\\', 'n', '[SEP]']
Filtered   (017): ['"', 'purple', '"', ',', '"', 'tea', '##l', '"', ',', '"', 'light', '##gra', '##y', '"', ']', '\\', 'n']
Detokenized (007): ['"purple"', ',', '"tea##l"', ',', '"light##gra##y"', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "numrange = None , default = None , max_width = 72 ) : \n"
Original    (014): ['numrange', '=', 'None', ',', 'default', '=', 'None', ',', 'max_width', '=', '72', ')', ':', '\\n']
Tokenized   (022): ['[CLS]', 'nu', '##m', '##rang', '##e', '=', 'none', ',', 'default', '=', 'none', ',', 'max', '_', 'width', '=', '72', ')', ':', '\\', 'n', '[SEP]']
Filtered   (020): ['nu', '##m', '##rang', '##e', '=', 'none', ',', 'default', '=', 'none', ',', 'max', '_', 'width', '=', '72', ')', ':', '\\', 'n']
Detokenized (014): ['nu##m##rang##e', '=', 'none', ',', 'default', '=', 'none', ',', 'max_width', '=', '72', ')', ':', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "found_letter . lower ( ) == default . lower ( ) ) ) : \n"
Original    (015): ['found_letter', '.', 'lower', '(', ')', '==', 'default', '.', 'lower', '(', ')', ')', ')', ':', '\\n']
Tokenized   (021): ['[CLS]', 'found', '_', 'letter', '.', 'lower', '(', ')', '=', '=', 'default', '.', 'lower', '(', ')', ')', ')', ':', '\\', 'n', '[SEP]']
Filtered   (019): ['found', '_', 'letter', '.', 'lower', '(', ')', '=', '=', 'default', '.', 'lower', '(', ')', ')', ')', ':', '\\', 'n']
Detokenized (015): ['found_letter', '.', 'lower', '(', ')', '==', 'default', '.', 'lower', '(', ')', ')', ')', ':', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "option [ : index ] + show_letter + option [ index + 1 : ] \n"
Original    (016): ['option', '[', ':', 'index', ']', '+', 'show_letter', '+', 'option', '[', 'index', '+', '1', ':', ']', '\\n']
Tokenized   (021): ['[CLS]', 'option', '[', ':', 'index', ']', '+', 'show', '_', 'letter', '+', 'option', '[', 'index', '+', '1', ':', ']', '\\', 'n', '[SEP]']
Filtered   (019): ['option', '[', ':', 'index', ']', '+', 'show', '_', 'letter', '+', 'option', '[', 'index', '+', '1', ':', ']', '\\', 'n']
Detokenized (016): ['option', '[', ':', 'index', ']', '+', 'show_letter', '+', 'option', '[', 'index', '+', '1', ':', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "display_letters . append ( found_letter . upper ( ) ) \n"
Original    (011): ['display_letters', '.', 'append', '(', 'found_letter', '.', 'upper', '(', ')', ')', '\\n']
Tokenized   (019): ['[CLS]', 'display', '_', 'letters', '.', 'app', '##end', '(', 'found', '_', 'letter', '.', 'upper', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['display', '_', 'letters', '.', 'app', '##end', '(', 'found', '_', 'letter', '.', 'upper', '(', ')', ')', '\\', 'n']
Detokenized (011): ['display_letters', '.', 'app##end', '(', 'found_letter', '.', 'upper', '(', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "default_name = self . colorize ( , default_name ) \n"
Original    (010): ['default_name', '=', 'self', '.', 'colorize', '(', ',', 'default_name', ')', '\\n']
Tokenized   (018): ['[CLS]', 'default', '_', 'name', '=', 'self', '.', 'color', '##ize', '(', ',', 'default', '_', 'name', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['default', '_', 'name', '=', 'self', '.', 'color', '##ize', '(', ',', 'default', '_', 'name', ')', '\\', 'n']
Detokenized (010): ['default_name', '=', 'self', '.', 'color##ize', '(', ',', 'default_name', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "prompt_parts . append ( tmpl % default_name ) \n"
Original    (009): ['prompt_parts', '.', 'append', '(', 'tmpl', '%', 'default_name', ')', '\\n']
Tokenized   (019): ['[CLS]', 'prompt', '_', 'parts', '.', 'app', '##end', '(', 't', '##mp', '##l', '%', 'default', '_', 'name', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['prompt', '_', 'parts', '.', 'app', '##end', '(', 't', '##mp', '##l', '%', 'default', '_', 'name', ')', '\\', 'n']
Detokenized (009): ['prompt_parts', '.', 'app##end', '(', 't##mp##l', '%', 'default_name', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "matcher = SequenceMatcher ( lambda x : False , a , b ) \n"
Original    (014): ['matcher', '=', 'SequenceMatcher', '(', 'lambda', 'x', ':', 'False', ',', 'a', ',', 'b', ')', '\\n']
Tokenized   (020): ['[CLS]', 'match', '##er', '=', 'sequence', '##mat', '##cher', '(', 'lambda', 'x', ':', 'false', ',', 'a', ',', 'b', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['match', '##er', '=', 'sequence', '##mat', '##cher', '(', 'lambda', 'x', ':', 'false', ',', 'a', ',', 'b', ')', '\\', 'n']
Detokenized (014): ['match##er', '=', 'sequence##mat##cher', '(', 'lambda', 'x', ':', 'false', ',', 'a', ',', 'b', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "b_out . append ( self . colorize ( color , b [ b_start : b_end ] ) ) \n"
Original    (019): ['b_out', '.', 'append', '(', 'self', '.', 'colorize', '(', 'color', ',', 'b', '[', 'b_start', ':', 'b_end', ']', ')', ')', '\\n']
Tokenized   (030): ['[CLS]', 'b', '_', 'out', '.', 'app', '##end', '(', 'self', '.', 'color', '##ize', '(', 'color', ',', 'b', '[', 'b', '_', 'start', ':', 'b', '_', 'end', ']', ')', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['b', '_', 'out', '.', 'app', '##end', '(', 'self', '.', 'color', '##ize', '(', 'color', ',', 'b', '[', 'b', '_', 'start', ':', 'b', '_', 'end', ']', ')', ')', '\\', 'n']
Detokenized (019): ['b_out', '.', 'app##end', '(', 'self', '.', 'color##ize', '(', 'color', ',', 'b', '[', 'b_start', ':', 'b_end', ']', ')', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "variable = % varname \n"
Original    (005): ['variable', '=', '%', 'varname', '\\n']
Tokenized   (009): ['[CLS]', 'variable', '=', '%', 'var', '##name', '\\', 'n', '[SEP]']
Filtered   (007): ['variable', '=', '%', 'var', '##name', '\\', 'n']
Detokenized (005): ['variable', '=', '%', 'var##name', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "62 : , \n"
Original    (004): ['62', ':', ',', '\\n']
Tokenized   (007): ['[CLS]', '62', ':', ',', '\\', 'n', '[SEP]']
Filtered   (005): ['62', ':', ',', '\\', 'n']
Detokenized (004): ['62', ':', ',', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "_push . update ( { \n"
Original    (006): ['_push', '.', 'update', '(', '{', '\\n']
Tokenized   (010): ['[CLS]', '_', 'push', '.', 'update', '(', '{', '\\', 'n', '[SEP]']
Filtered   (008): ['_', 'push', '.', 'update', '(', '{', '\\', 'n']
Detokenized (006): ['_push', '.', 'update', '(', '{', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "_readonly = Entity . _readonly | { , } \n"
Original    (010): ['_readonly', '=', 'Entity', '.', '_readonly', '|', '{', ',', '}', '\\n']
Tokenized   (019): ['[CLS]', '_', 'read', '##on', '##ly', '=', 'entity', '.', '_', 'read', '##on', '##ly', '|', '{', ',', '}', '\\', 'n', '[SEP]']
Filtered   (017): ['_', 'read', '##on', '##ly', '=', 'entity', '.', '_', 'read', '##on', '##ly', '|', '{', ',', '}', '\\', 'n']
Detokenized (010): ['_read##on##ly', '=', 'entity', '.', '_read##on##ly', '|', '{', ',', '}', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "remove_ids = [ 6 , 7 ] \n"
Original    (008): ['remove_ids', '=', '[', '6', ',', '7', ']', '\\n']
Tokenized   (014): ['[CLS]', 'remove', '_', 'id', '##s', '=', '[', '6', ',', '7', ']', '\\', 'n', '[SEP]']
Filtered   (012): ['remove', '_', 'id', '##s', '=', '[', '6', ',', '7', ']', '\\', 'n']
Detokenized (008): ['remove_id##s', '=', '[', '6', ',', '7', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "remove_advertiser_ids = [ 8 , 9 , 10 ] \n"
Original    (010): ['remove_advertiser_ids', '=', '[', '8', ',', '9', ',', '10', ']', '\\n']
Tokenized   (020): ['[CLS]', 'remove', '_', 'ad', '##vert', '##iser', '_', 'id', '##s', '=', '[', '8', ',', '9', ',', '10', ']', '\\', 'n', '[SEP]']
Filtered   (018): ['remove', '_', 'ad', '##vert', '##iser', '_', 'id', '##s', '=', '[', '8', ',', '9', ',', '10', ']', '\\', 'n']
Detokenized (010): ['remove_ad##vert##iser_id##s', '=', '[', '8', ',', '9', ',', '10', ']', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "num_users , num_items = dataset . shape \n"
Original    (008): ['num_users', ',', 'num_items', '=', 'dataset', '.', 'shape', '\\n']
Tokenized   (018): ['[CLS]', 'nu', '##m', '_', 'users', ',', 'nu', '##m', '_', 'items', '=', 'data', '##set', '.', 'shape', '\\', 'n', '[SEP]']
Filtered   (016): ['nu', '##m', '_', 'users', ',', 'nu', '##m', '_', 'items', '=', 'data', '##set', '.', 'shape', '\\', 'n']
Detokenized (008): ['nu##m_users', ',', 'nu##m_items', '=', 'data##set', '.', 'shape', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "async_job = view . map_async ( process , tasks , retries = 2 ) \n"
Original    (015): ['async_job', '=', 'view', '.', 'map_async', '(', 'process', ',', 'tasks', ',', 'retries', '=', '2', ')', '\\n']
Tokenized   (027): ['[CLS]', 'as', '##yn', '##c', '_', 'job', '=', 'view', '.', 'map', '_', 'as', '##yn', '##c', '(', 'process', ',', 'tasks', ',', 're', '##tries', '=', '2', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['as', '##yn', '##c', '_', 'job', '=', 'view', '.', 'map', '_', 'as', '##yn', '##c', '(', 'process', ',', 'tasks', ',', 're', '##tries', '=', '2', ')', '\\', 'n']
Detokenized (015): ['as##yn##c_job', '=', 'view', '.', 'map_as##yn##c', '(', 'process', ',', 'tasks', ',', 're##tries', '=', '2', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "remaining = len ( tasks ) - len ( done ) \n"
Original    (012): ['remaining', '=', 'len', '(', 'tasks', ')', '-', 'len', '(', 'done', ')', '\\n']
Tokenized   (015): ['[CLS]', 'remaining', '=', 'len', '(', 'tasks', ')', '-', 'len', '(', 'done', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['remaining', '=', 'len', '(', 'tasks', ')', '-', 'len', '(', 'done', ')', '\\', 'n']
Detokenized (012): ['remaining', '=', 'len', '(', 'tasks', ')', '-', 'len', '(', 'done', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "num_items , type ( model ) . __name__ , simsfile ) \n"
Original    (012): ['num_items', ',', 'type', '(', 'model', ')', '.', '__name__', ',', 'simsfile', ')', '\\n']
Tokenized   (024): ['[CLS]', 'nu', '##m', '_', 'items', ',', 'type', '(', 'model', ')', '.', '_', '_', 'name', '_', '_', ',', 'sims', '##fi', '##le', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['nu', '##m', '_', 'items', ',', 'type', '(', 'model', ')', '.', '_', '_', 'name', '_', '_', ',', 'sims', '##fi', '##le', ')', '\\', 'n']
Detokenized (012): ['nu##m_items', ',', 'type', '(', 'model', ')', '.', '__name__', ',', 'sims##fi##le', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "before = text [ : len ( text ) - len ( like ) ] \n"
Original    (016): ['before', '=', 'text', '[', ':', 'len', '(', 'text', ')', '-', 'len', '(', 'like', ')', ']', '\\n']
Tokenized   (019): ['[CLS]', 'before', '=', 'text', '[', ':', 'len', '(', 'text', ')', '-', 'len', '(', 'like', ')', ']', '\\', 'n', '[SEP]']
Filtered   (017): ['before', '=', 'text', '[', ':', 'len', '(', 'text', ')', '-', 'len', '(', 'like', ')', ']', '\\', 'n']
Detokenized (016): ['before', '=', 'text', '[', ':', 'len', '(', 'text', ')', '-', 'len', '(', 'like', ')', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "Version = namedtuple ( , ) \n"
Original    (007): ['Version', '=', 'namedtuple', '(', ',', ')', '\\n']
Tokenized   (012): ['[CLS]', 'version', '=', 'named', '##tu', '##ple', '(', ',', ')', '\\', 'n', '[SEP]']
Filtered   (010): ['version', '=', 'named', '##tu', '##ple', '(', ',', ')', '\\', 'n']
Detokenized (007): ['version', '=', 'named##tu##ple', '(', ',', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "_defaults = collections . OrderedDict ( [ \n"
Original    (008): ['_defaults', '=', 'collections', '.', 'OrderedDict', '(', '[', '\\n']
Tokenized   (014): ['[CLS]', '_', 'default', '##s', '=', 'collections', '.', 'ordered', '##dict', '(', '[', '\\', 'n', '[SEP]']
Filtered   (012): ['_', 'default', '##s', '=', 'collections', '.', 'ordered', '##dict', '(', '[', '\\', 'n']
Detokenized (008): ['_default##s', '=', 'collections', '.', 'ordered##dict', '(', '[', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "rootDirectory = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , , ) \n"
Original    (027): ['rootDirectory', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'realpath', '(', '__file__', ')', ')', ',', ',', ')', '\\n']
Tokenized   (040): ['[CLS]', 'root', '##di', '##re', '##ctor', '##y', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dir', '##name', '(', 'os', '.', 'path', '.', 'real', '##path', '(', '_', '_', 'file', '_', '_', ')', ')', ',', ',', ')', '\\', 'n', '[SEP]']
Filtered   (038): ['root', '##di', '##re', '##ctor', '##y', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dir', '##name', '(', 'os', '.', 'path', '.', 'real', '##path', '(', '_', '_', 'file', '_', '_', ')', ')', ',', ',', ')', '\\', 'n']
Detokenized (027): ['root##di##re##ctor##y', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dir##name', '(', 'os', '.', 'path', '.', 'real##path', '(', '__file__', ')', ')', ',', ',', ')', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "httpd . handle_request ( ) from . import TestEnable # \n"
Original    (011): ['httpd', '.', 'handle_request', '(', ')', 'from', '.', 'import', 'TestEnable', '#', '\\n']
Tokenized   (019): ['[CLS]', 'http', '##d', '.', 'handle', '_', 'request', '(', ')', 'from', '.', 'import', 'test', '##ena', '##ble', '#', '\\', 'n', '[SEP]']
Filtered   (017): ['http', '##d', '.', 'handle', '_', 'request', '(', ')', 'from', '.', 'import', 'test', '##ena', '##ble', '#', '\\', 'n']
Detokenized (011): ['http##d', '.', 'handle_request', '(', ')', 'from', '.', 'import', 'test##ena##ble', '#', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "field_names = tuple ( field_names ) , \n"
Original    (008): ['field_names', '=', 'tuple', '(', 'field_names', ')', ',', '\\n']
Tokenized   (016): ['[CLS]', 'field', '_', 'names', '=', 'tu', '##ple', '(', 'field', '_', 'names', ')', ',', '\\', 'n', '[SEP]']
Filtered   (014): ['field', '_', 'names', '=', 'tu', '##ple', '(', 'field', '_', 'names', ')', ',', '\\', 'n']
Detokenized (008): ['field_names', '=', 'tu##ple', '(', 'field_names', ')', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "arg_list = repr ( tuple ( field_names ) ) . replace ( "\'" , "" ) [ 1 : - 1 ] , \n"
Original    (024): ['arg_list', '=', 'repr', '(', 'tuple', '(', 'field_names', ')', ')', '.', 'replace', '(', '"\\\'"', ',', '""', ')', '[', '1', ':', '-', '1', ']', ',', '\\n']
Tokenized   (038): ['[CLS]', 'ar', '##g', '_', 'list', '=', 'rep', '##r', '(', 'tu', '##ple', '(', 'field', '_', 'names', ')', ')', '.', 'replace', '(', '"', '\\', "'", '"', ',', '"', '"', ')', '[', '1', ':', '-', '1', ']', ',', '\\', 'n', '[SEP]']
Filtered   (036): ['ar', '##g', '_', 'list', '=', 'rep', '##r', '(', 'tu', '##ple', '(', 'field', '_', 'names', ')', ')', '.', 'replace', '(', '"', '\\', "'", '"', ',', '"', '"', ')', '[', '1', ':', '-', '1', ']', ',', '\\', 'n']
Detokenized (024): ['ar##g_list', '=', 'rep##r', '(', 'tu##ple', '(', 'field_names', ')', ')', '.', 'replace', '(', '"\\\'"', ',', '""', ')', '[', '1', ':', '-', '1', ']', ',', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "repr_fmt = . join ( _repr_template . format ( name = name ) \n"
Original    (014): ['repr_fmt', '=', '.', 'join', '(', '_repr_template', '.', 'format', '(', 'name', '=', 'name', ')', '\\n']
Tokenized   (025): ['[CLS]', 'rep', '##r', '_', 'fm', '##t', '=', '.', 'join', '(', '_', 'rep', '##r', '_', 'template', '.', 'format', '(', 'name', '=', 'name', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['rep', '##r', '_', 'fm', '##t', '=', '.', 'join', '(', '_', 'rep', '##r', '_', 'template', '.', 'format', '(', 'name', '=', 'name', ')', '\\', 'n']
Detokenized (014): ['rep##r_fm##t', '=', '.', 'join', '(', '_rep##r_template', '.', 'format', '(', 'name', '=', 'name', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "field_defs = . join ( _field_template . format ( index = index , name = name ) \n"
Original    (018): ['field_defs', '=', '.', 'join', '(', '_field_template', '.', 'format', '(', 'index', '=', 'index', ',', 'name', '=', 'name', ')', '\\n']
Tokenized   (027): ['[CLS]', 'field', '_', 'def', '##s', '=', '.', 'join', '(', '_', 'field', '_', 'template', '.', 'format', '(', 'index', '=', 'index', ',', 'name', '=', 'name', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['field', '_', 'def', '##s', '=', '.', 'join', '(', '_', 'field', '_', 'template', '.', 'format', '(', 'index', '=', 'index', ',', 'name', '=', 'name', ')', '\\', 'n']
Detokenized (018): ['field_def##s', '=', '.', 'join', '(', '_field_template', '.', 'format', '(', 'index', '=', 'index', ',', 'name', '=', 'name', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "OrderedDict = OrderedDict , _property = property , _tuple = tuple ) \n"
Original    (013): ['OrderedDict', '=', 'OrderedDict', ',', '_property', '=', 'property', ',', '_tuple', '=', 'tuple', ')', '\\n']
Tokenized   (022): ['[CLS]', 'ordered', '##dict', '=', 'ordered', '##dict', ',', '_', 'property', '=', 'property', ',', '_', 'tu', '##ple', '=', 'tu', '##ple', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['ordered', '##dict', '=', 'ordered', '##dict', ',', '_', 'property', '=', 'property', ',', '_', 'tu', '##ple', '=', 'tu', '##ple', ')', '\\', 'n']
Detokenized (013): ['ordered##dict', '=', 'ordered##dict', ',', '_property', '=', 'property', ',', '_tu##ple', '=', 'tu##ple', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "xx = Xdf [ ] . values \n"
Original    (008): ['xx', '=', 'Xdf', '[', ']', '.', 'values', '\\n']
Tokenized   (012): ['[CLS]', 'xx', '=', 'x', '##df', '[', ']', '.', 'values', '\\', 'n', '[SEP]']
Filtered   (010): ['xx', '=', 'x', '##df', '[', ']', '.', 'values', '\\', 'n']
Detokenized (008): ['xx', '=', 'x##df', '[', ']', '.', 'values', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "X_CD13 , Y_CD13 = util . get_data ( cd13 , y_names = [ , ] ) \n"
Original    (017): ['X_CD13', ',', 'Y_CD13', '=', 'util', '.', 'get_data', '(', 'cd13', ',', 'y_names', '=', '[', ',', ']', ')', '\\n']
Tokenized   (032): ['[CLS]', 'x', '_', 'cd', '##13', ',', 'y', '_', 'cd', '##13', '=', 'ut', '##il', '.', 'get', '_', 'data', '(', 'cd', '##13', ',', 'y', '_', 'names', '=', '[', ',', ']', ')', '\\', 'n', '[SEP]']
Filtered   (030): ['x', '_', 'cd', '##13', ',', 'y', '_', 'cd', '##13', '=', 'ut', '##il', '.', 'get', '_', 'data', '(', 'cd', '##13', ',', 'y', '_', 'names', '=', '[', ',', ']', ')', '\\', 'n']
Detokenized (017): ['x_cd##13', ',', 'y_cd##13', '=', 'ut##il', '.', 'get_data', '(', 'cd##13', ',', 'y_names', '=', '[', ',', ']', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "cd33 = human_data . xs ( , level = , drop_level = False ) \n"
Original    (015): ['cd33', '=', 'human_data', '.', 'xs', '(', ',', 'level', '=', ',', 'drop_level', '=', 'False', ')', '\\n']
Tokenized   (024): ['[CLS]', 'cd', '##33', '=', 'human', '_', 'data', '.', 'x', '##s', '(', ',', 'level', '=', ',', 'drop', '_', 'level', '=', 'false', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['cd', '##33', '=', 'human', '_', 'data', '.', 'x', '##s', '(', ',', 'level', '=', ',', 'drop', '_', 'level', '=', 'false', ')', '\\', 'n']
Detokenized (015): ['cd##33', '=', 'human_data', '.', 'x##s', '(', ',', 'level', '=', ',', 'drop_level', '=', 'false', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "X_CD33 , Y_CD33 = util . get_data ( cd33 , y_names = [ , , ] ) \n"
Original    (018): ['X_CD33', ',', 'Y_CD33', '=', 'util', '.', 'get_data', '(', 'cd33', ',', 'y_names', '=', '[', ',', ',', ']', ')', '\\n']
Tokenized   (033): ['[CLS]', 'x', '_', 'cd', '##33', ',', 'y', '_', 'cd', '##33', '=', 'ut', '##il', '.', 'get', '_', 'data', '(', 'cd', '##33', ',', 'y', '_', 'names', '=', '[', ',', ',', ']', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['x', '_', 'cd', '##33', ',', 'y', '_', 'cd', '##33', '=', 'ut', '##il', '.', 'get', '_', 'data', '(', 'cd', '##33', ',', 'y', '_', 'names', '=', '[', ',', ',', ']', ')', '\\', 'n']
Detokenized (018): ['x_cd##33', ',', 'y_cd##33', '=', 'ut##il', '.', 'get_data', '(', 'cd##33', ',', 'y_names', '=', '[', ',', ',', ']', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "X_CD15 , Y_CD15 = util . get_data ( cd15 , y_names = [ ] ) \n"
Original    (016): ['X_CD15', ',', 'Y_CD15', '=', 'util', '.', 'get_data', '(', 'cd15', ',', 'y_names', '=', '[', ']', ')', '\\n']
Tokenized   (031): ['[CLS]', 'x', '_', 'cd', '##15', ',', 'y', '_', 'cd', '##15', '=', 'ut', '##il', '.', 'get', '_', 'data', '(', 'cd', '##15', ',', 'y', '_', 'names', '=', '[', ']', ')', '\\', 'n', '[SEP]']
Filtered   (029): ['x', '_', 'cd', '##15', ',', 'y', '_', 'cd', '##15', '=', 'ut', '##il', '.', 'get', '_', 'data', '(', 'cd', '##15', ',', 'y', '_', 'names', '=', '[', ']', ')', '\\', 'n']
Detokenized (016): ['x_cd##15', ',', 'y_cd##15', '=', 'ut##il', '.', 'get_data', '(', 'cd##15', ',', 'y_names', '=', '[', ']', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "mouse_Y = pandas . concat ( [ mouse_Y , Y ] , axis = 0 ) \n"
Original    (017): ['mouse_Y', '=', 'pandas', '.', 'concat', '(', '[', 'mouse_Y', ',', 'Y', ']', ',', 'axis', '=', '0', ')', '\\n']
Tokenized   (026): ['[CLS]', 'mouse', '_', 'y', '=', 'panda', '##s', '.', 'con', '##cat', '(', '[', 'mouse', '_', 'y', ',', 'y', ']', ',', 'axis', '=', '0', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['mouse', '_', 'y', '=', 'panda', '##s', '.', 'con', '##cat', '(', '[', 'mouse', '_', 'y', ',', 'y', ']', ',', 'axis', '=', '0', ')', '\\', 'n']
Detokenized (017): ['mouse_y', '=', 'panda##s', '.', 'con##cat', '(', '[', 'mouse_y', ',', 'y', ']', ',', 'axis', '=', '0', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "mouse_data = pandas . read_excel ( data_file , sheetname = 1 , index_col = [ 0 , 1 ] ) \n"
Original    (021): ['mouse_data', '=', 'pandas', '.', 'read_excel', '(', 'data_file', ',', 'sheetname', '=', '1', ',', 'index_col', '=', '[', '0', ',', '1', ']', ')', '\\n']
Tokenized   (034): ['[CLS]', 'mouse', '_', 'data', '=', 'panda', '##s', '.', 'read', '_', 'excel', '(', 'data', '_', 'file', ',', 'sheet', '##name', '=', '1', ',', 'index', '_', 'col', '=', '[', '0', ',', '1', ']', ')', '\\', 'n', '[SEP]']
Filtered   (032): ['mouse', '_', 'data', '=', 'panda', '##s', '.', 'read', '_', 'excel', '(', 'data', '_', 'file', ',', 'sheet', '##name', '=', '1', ',', 'index', '_', 'col', '=', '[', '0', ',', '1', ']', ')', '\\', 'n']
Detokenized (021): ['mouse_data', '=', 'panda##s', '.', 'read_excel', '(', 'data_file', ',', 'sheet##name', '=', '1', ',', 'index_col', '=', '[', '0', ',', '1', ']', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "data_efficient [ ] = 1. \n"
Original    (006): ['data_efficient', '[', ']', '=', '1.', '\\n']
Tokenized   (012): ['[CLS]', 'data', '_', 'efficient', '[', ']', '=', '1', '.', '\\', 'n', '[SEP]']
Filtered   (010): ['data', '_', 'efficient', '[', ']', '=', '1', '.', '\\', 'n']
Detokenized (006): ['data_efficient', '[', ']', '=', '1.', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "exp_data [ ] = exp_data . groupby ( ) [ ] . transform ( exp_data [ ] = exp_data . groupby ( ) [ ] . transform ( \n"
Original    (029): ['exp_data', '[', ']', '=', 'exp_data', '.', 'groupby', '(', ')', '[', ']', '.', 'transform', '(', 'exp_data', '[', ']', '=', 'exp_data', '.', 'groupby', '(', ')', '[', ']', '.', 'transform', '(', '\\n']
Tokenized   (046): ['[CLS]', 'ex', '##p', '_', 'data', '[', ']', '=', 'ex', '##p', '_', 'data', '.', 'group', '##by', '(', ')', '[', ']', '.', 'transform', '(', 'ex', '##p', '_', 'data', '[', ']', '=', 'ex', '##p', '_', 'data', '.', 'group', '##by', '(', ')', '[', ']', '.', 'transform', '(', '\\', 'n', '[SEP]']
Filtered   (044): ['ex', '##p', '_', 'data', '[', ']', '=', 'ex', '##p', '_', 'data', '.', 'group', '##by', '(', ')', '[', ']', '.', 'transform', '(', 'ex', '##p', '_', 'data', '[', ']', '=', 'ex', '##p', '_', 'data', '.', 'group', '##by', '(', ')', '[', ']', '.', 'transform', '(', '\\', 'n']
Detokenized (029): ['ex##p_data', '[', ']', '=', 'ex##p_data', '.', 'group##by', '(', ')', '[', ']', '.', 'transform', '(', 'ex##p_data', '[', ']', '=', 'ex##p_data', '.', 'group##by', '(', ')', '[', ']', '.', 'transform', '(', '\\n']
Counter: 44
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "aggregated [ ] = aggregated [ [ , ] ] . mean ( axis = 1 ) \n"
Original    (018): ['aggregated', '[', ']', '=', 'aggregated', '[', '[', ',', ']', ']', '.', 'mean', '(', 'axis', '=', '1', ')', '\\n']
Tokenized   (023): ['[CLS]', 'aggregate', '##d', '[', ']', '=', 'aggregate', '##d', '[', '[', ',', ']', ']', '.', 'mean', '(', 'axis', '=', '1', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['aggregate', '##d', '[', ']', '=', 'aggregate', '##d', '[', '[', ',', ']', ']', '.', 'mean', '(', 'axis', '=', '1', ')', '\\', 'n']
Detokenized (018): ['aggregate##d', '[', ']', '=', 'aggregate##d', '[', '[', ',', ']', ']', '.', 'mean', '(', 'axis', '=', '1', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "known_pairs = { : [ , , , ] , \n"
Original    (011): ['known_pairs', '=', '{', ':', '[', ',', ',', ',', ']', ',', '\\n']
Tokenized   (016): ['[CLS]', 'known', '_', 'pairs', '=', '{', ':', '[', ',', ',', ',', ']', ',', '\\', 'n', '[SEP]']
Filtered   (014): ['known', '_', 'pairs', '=', '{', ':', '[', ',', ',', ',', ']', ',', '\\', 'n']
Detokenized (011): ['known_pairs', '=', '{', ':', '[', ',', ',', ',', ']', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "drugs_to_genes [ ] . extend ( [ , , , , , \n"
Original    (013): ['drugs_to_genes', '[', ']', '.', 'extend', '(', '[', ',', ',', ',', ',', ',', '\\n']
Tokenized   (020): ['[CLS]', 'drugs', '_', 'to', '_', 'genes', '[', ']', '.', 'extend', '(', '[', ',', ',', ',', ',', ',', '\\', 'n', '[SEP]']
Filtered   (018): ['drugs', '_', 'to', '_', 'genes', '[', ']', '.', 'extend', '(', '[', ',', ',', ',', ',', ',', '\\', 'n']
Detokenized (013): ['drugs_to_genes', '[', ']', '.', 'extend', '(', '[', ',', ',', ',', ',', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "Xtmp [ ] = drug \n"
Original    (006): ['Xtmp', '[', ']', '=', 'drug', '\\n']
Tokenized   (011): ['[CLS]', 'x', '##tm', '##p', '[', ']', '=', 'drug', '\\', 'n', '[SEP]']
Filtered   (009): ['x', '##tm', '##p', '[', ']', '=', 'drug', '\\', 'n']
Detokenized (006): ['x##tm##p', '[', ']', '=', 'drug', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "y_rank = pandas . concat ( ( y_rank , y_ranktmp ) , axis = 0 ) \n"
Original    (017): ['y_rank', '=', 'pandas', '.', 'concat', '(', '(', 'y_rank', ',', 'y_ranktmp', ')', ',', 'axis', '=', '0', ')', '\\n']
Tokenized   (030): ['[CLS]', 'y', '_', 'rank', '=', 'panda', '##s', '.', 'con', '##cat', '(', '(', 'y', '_', 'rank', ',', 'y', '_', 'rank', '##tm', '##p', ')', ',', 'axis', '=', '0', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['y', '_', 'rank', '=', 'panda', '##s', '.', 'con', '##cat', '(', '(', 'y', '_', 'rank', ',', 'y', '_', 'rank', '##tm', '##p', ')', ',', 'axis', '=', '0', ')', '\\', 'n']
Detokenized (017): ['y_rank', '=', 'panda##s', '.', 'con##cat', '(', '(', 'y_rank', ',', 'y_rank##tm##p', ')', ',', 'axis', '=', '0', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "experiments [ ] = [ , , , ] \n"
Original    (010): ['experiments', '[', ']', '=', '[', ',', ',', ',', ']', '\\n']
Tokenized   (013): ['[CLS]', 'experiments', '[', ']', '=', '[', ',', ',', ',', ']', '\\', 'n', '[SEP]']
Filtered   (011): ['experiments', '[', ']', '=', '[', ',', ',', ',', ']', '\\', 'n']
Detokenized (010): ['experiments', '[', ']', '=', '[', ',', ',', ',', ']', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "data_tmp [ "variance" ] = np . var ( data_tmp . values , axis = 1 ) \n"
Original    (018): ['data_tmp', '[', '"variance"', ']', '=', 'np', '.', 'var', '(', 'data_tmp', '.', 'values', ',', 'axis', '=', '1', ')', '\\n']
Tokenized   (029): ['[CLS]', 'data', '_', 't', '##mp', '[', '"', 'variance', '"', ']', '=', 'np', '.', 'var', '(', 'data', '_', 't', '##mp', '.', 'values', ',', 'axis', '=', '1', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['data', '_', 't', '##mp', '[', '"', 'variance', '"', ']', '=', 'np', '.', 'var', '(', 'data', '_', 't', '##mp', '.', 'values', ',', 'axis', '=', '1', ')', '\\', 'n']
Detokenized (018): ['data_t##mp', '[', '"variance"', ']', '=', 'np', '.', 'var', '(', 'data_t##mp', '.', 'values', ',', 'axis', '=', '1', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "gene_position_xu , target_genes_xu , Xdf_xu , Y_xu = read_xu_et_al ( data_file3 , learn_options ) \n"
Original    (015): ['gene_position_xu', ',', 'target_genes_xu', ',', 'Xdf_xu', ',', 'Y_xu', '=', 'read_xu_et_al', '(', 'data_file3', ',', 'learn_options', ')', '\\n']
Tokenized   (042): ['[CLS]', 'gene', '_', 'position', '_', 'xu', ',', 'target', '_', 'genes', '_', 'xu', ',', 'x', '##df', '_', 'xu', ',', 'y', '_', 'xu', '=', 'read', '_', 'xu', '_', 'et', '_', 'al', '(', 'data', '_', 'file', '##3', ',', 'learn', '_', 'options', ')', '\\', 'n', '[SEP]']
Filtered   (040): ['gene', '_', 'position', '_', 'xu', ',', 'target', '_', 'genes', '_', 'xu', ',', 'x', '##df', '_', 'xu', ',', 'y', '_', 'xu', '=', 'read', '_', 'xu', '_', 'et', '_', 'al', '(', 'data', '_', 'file', '##3', ',', 'learn', '_', 'options', ')', '\\', 'n']
Detokenized (015): ['gene_position_xu', ',', 'target_genes_xu', ',', 'x##df_xu', ',', 'y_xu', '=', 'read_xu_et_al', '(', 'data_file##3', ',', 'learn_options', ')', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "annotations , gene_position1 , target_genes1 , Xdf1 , Y1 = read_V1_data ( data_file , learn_options ) \n"
Original    (017): ['annotations', ',', 'gene_position1', ',', 'target_genes1', ',', 'Xdf1', ',', 'Y1', '=', 'read_V1_data', '(', 'data_file', ',', 'learn_options', ')', '\\n']
Tokenized   (040): ['[CLS]', 'ann', '##ota', '##tions', ',', 'gene', '_', 'position', '##1', ',', 'target', '_', 'genes', '##1', ',', 'x', '##df', '##1', ',', 'y', '##1', '=', 'read', '_', 'v', '##1', '_', 'data', '(', 'data', '_', 'file', ',', 'learn', '_', 'options', ')', '\\', 'n', '[SEP]']
Filtered   (038): ['ann', '##ota', '##tions', ',', 'gene', '_', 'position', '##1', ',', 'target', '_', 'genes', '##1', ',', 'x', '##df', '##1', ',', 'y', '##1', '=', 'read', '_', 'v', '##1', '_', 'data', '(', 'data', '_', 'file', ',', 'learn', '_', 'options', ')', '\\', 'n']
Detokenized (017): ['ann##ota##tions', ',', 'gene_position##1', ',', 'target_genes##1', ',', 'x##df##1', ',', 'y##1', '=', 'read_v##1_data', '(', 'data_file', ',', 'learn_options', ')', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "Y_cols_to_keep = np . unique ( [ , , , \n"
Original    (011): ['Y_cols_to_keep', '=', 'np', '.', 'unique', '(', '[', ',', ',', ',', '\\n']
Tokenized   (021): ['[CLS]', 'y', '_', 'col', '##s', '_', 'to', '_', 'keep', '=', 'np', '.', 'unique', '(', '[', ',', ',', ',', '\\', 'n', '[SEP]']
Filtered   (019): ['y', '_', 'col', '##s', '_', 'to', '_', 'keep', '=', 'np', '.', 'unique', '(', '[', ',', ',', ',', '\\', 'n']
Detokenized (011): ['y_col##s_to_keep', '=', 'np', '.', 'unique', '(', '[', ',', ',', ',', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "gene_position = pandas . concat ( ( gene_position1 , gene_position2 ) ) \n"
Original    (013): ['gene_position', '=', 'pandas', '.', 'concat', '(', '(', 'gene_position1', ',', 'gene_position2', ')', ')', '\\n']
Tokenized   (026): ['[CLS]', 'gene', '_', 'position', '=', 'panda', '##s', '.', 'con', '##cat', '(', '(', 'gene', '_', 'position', '##1', ',', 'gene', '_', 'position', '##2', ')', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['gene', '_', 'position', '=', 'panda', '##s', '.', 'con', '##cat', '(', '(', 'gene', '_', 'position', '##1', ',', 'gene', '_', 'position', '##2', ')', ')', '\\', 'n']
Detokenized (013): ['gene_position', '=', 'panda##s', '.', 'con##cat', '(', '(', 'gene_position##1', ',', 'gene_position##2', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "onedupind = np . where ( Y . index . duplicated ( ) ) [ 0 ] [ 0 ] \n"
Original    (021): ['onedupind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'duplicated', '(', ')', ')', '[', '0', ']', '[', '0', ']', '\\n']
Tokenized   (028): ['[CLS]', 'one', '##du', '##pin', '##d', '=', 'np', '.', 'where', '(', 'y', '.', 'index', '.', 'duplicate', '##d', '(', ')', ')', '[', '0', ']', '[', '0', ']', '\\', 'n', '[SEP]']
Filtered   (026): ['one', '##du', '##pin', '##d', '=', 'np', '.', 'where', '(', 'y', '.', 'index', '.', 'duplicate', '##d', '(', ')', ')', '[', '0', ']', '[', '0', ']', '\\', 'n']
Detokenized (021): ['one##du##pin##d', '=', 'np', '.', 'where', '(', 'y', '.', 'index', '.', 'duplicate##d', '(', ')', ')', '[', '0', ']', '[', '0', ']', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "alldupind = np . where ( Y . index . get_level_values ( 0 ) . values == Y . index [ onedupind ] [ 0 ] ) [ 0 ] \n"
Original    (031): ['alldupind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'get_level_values', '(', '0', ')', '.', 'values', '==', 'Y', '.', 'index', '[', 'onedupind', ']', '[', '0', ']', ')', '[', '0', ']', '\\n']
Tokenized   (045): ['[CLS]', 'all', '##du', '##pin', '##d', '=', 'np', '.', 'where', '(', 'y', '.', 'index', '.', 'get', '_', 'level', '_', 'values', '(', '0', ')', '.', 'values', '=', '=', 'y', '.', 'index', '[', 'one', '##du', '##pin', '##d', ']', '[', '0', ']', ')', '[', '0', ']', '\\', 'n', '[SEP]']
Filtered   (043): ['all', '##du', '##pin', '##d', '=', 'np', '.', 'where', '(', 'y', '.', 'index', '.', 'get', '_', 'level', '_', 'values', '(', '0', ')', '.', 'values', '=', '=', 'y', '.', 'index', '[', 'one', '##du', '##pin', '##d', ']', '[', '0', ']', ')', '[', '0', ']', '\\', 'n']
Detokenized (031): ['all##du##pin##d', '=', 'np', '.', 'where', '(', 'y', '.', 'index', '.', 'get_level_values', '(', '0', ')', '.', 'values', '==', 'y', '.', 'index', '[', 'one##du##pin##d', ']', '[', '0', ']', ')', '[', '0', ']', '\\n']
Counter: 43
===================================================================
Hidden states:  (13, 31, 768)
# Extracted words:  31
Sentence         : "newindex [ onedupind ] = ( newindex [ onedupind ] [ 0 ] , newindex [ onedupind ] [ 1 ] , "nodrug2" ) \n"
Original    (025): ['newindex', '[', 'onedupind', ']', '=', '(', 'newindex', '[', 'onedupind', ']', '[', '0', ']', ',', 'newindex', '[', 'onedupind', ']', '[', '1', ']', ',', '"nodrug2"', ')', '\\n']
Tokenized   (047): ['[CLS]', 'new', '##ind', '##ex', '[', 'one', '##du', '##pin', '##d', ']', '=', '(', 'new', '##ind', '##ex', '[', 'one', '##du', '##pin', '##d', ']', '[', '0', ']', ',', 'new', '##ind', '##ex', '[', 'one', '##du', '##pin', '##d', ']', '[', '1', ']', ',', '"', 'nod', '##rug', '##2', '"', ')', '\\', 'n', '[SEP]']
Filtered   (045): ['new', '##ind', '##ex', '[', 'one', '##du', '##pin', '##d', ']', '=', '(', 'new', '##ind', '##ex', '[', 'one', '##du', '##pin', '##d', ']', '[', '0', ']', ',', 'new', '##ind', '##ex', '[', 'one', '##du', '##pin', '##d', ']', '[', '1', ']', ',', '"', 'nod', '##rug', '##2', '"', ')', '\\', 'n']
Detokenized (025): ['new##ind##ex', '[', 'one##du##pin##d', ']', '=', '(', 'new##ind##ex', '[', 'one##du##pin##d', ']', '[', '0', ']', ',', 'new##ind##ex', '[', 'one##du##pin##d', ']', '[', '1', ']', ',', '"nod##rug##2"', ')', '\\n']
Counter: 45
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "Xdf . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n"
Original    (021): ['Xdf', '.', 'index', '=', 'pandas', '.', 'MultiIndex', '.', 'from_tuples', '(', 'newindex', ',', 'names', '=', 'Y', '.', 'index', '.', 'names', ')', '\\n']
Tokenized   (033): ['[CLS]', 'x', '##df', '.', 'index', '=', 'panda', '##s', '.', 'multi', '##ind', '##ex', '.', 'from', '_', 'tu', '##ples', '(', 'new', '##ind', '##ex', ',', 'names', '=', 'y', '.', 'index', '.', 'names', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['x', '##df', '.', 'index', '=', 'panda', '##s', '.', 'multi', '##ind', '##ex', '.', 'from', '_', 'tu', '##ples', '(', 'new', '##ind', '##ex', ',', 'names', '=', 'y', '.', 'index', '.', 'names', ')', '\\', 'n']
Detokenized (021): ['x##df', '.', 'index', '=', 'panda##s', '.', 'multi##ind##ex', '.', 'from_tu##ples', '(', 'new##ind##ex', ',', 'names', '=', 'y', '.', 'index', '.', 'names', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "mouse_genes = Xdf [ Xdf [ ] == ] [ ] . unique ( ) \n"
Original    (016): ['mouse_genes', '=', 'Xdf', '[', 'Xdf', '[', ']', '==', ']', '[', ']', '.', 'unique', '(', ')', '\\n']
Tokenized   (024): ['[CLS]', 'mouse', '_', 'genes', '=', 'x', '##df', '[', 'x', '##df', '[', ']', '=', '=', ']', '[', ']', '.', 'unique', '(', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['mouse', '_', 'genes', '=', 'x', '##df', '[', 'x', '##df', '[', ']', '=', '=', ']', '[', ']', '.', 'unique', '(', ')', '\\', 'n']
Detokenized (016): ['mouse_genes', '=', 'x##df', '[', 'x##df', '[', ']', '==', ']', '[', ']', '.', 'unique', '(', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "all_genes = get_V3_genes ( None , None ) return np . setdiff1d ( all_genes , mouse_genes ) \n"
Original    (018): ['all_genes', '=', 'get_V3_genes', '(', 'None', ',', 'None', ')', 'return', 'np', '.', 'setdiff1d', '(', 'all_genes', ',', 'mouse_genes', ')', '\\n']
Tokenized   (036): ['[CLS]', 'all', '_', 'genes', '=', 'get', '_', 'v', '##3', '_', 'genes', '(', 'none', ',', 'none', ')', 'return', 'np', '.', 'set', '##di', '##ff', '##1', '##d', '(', 'all', '_', 'genes', ',', 'mouse', '_', 'genes', ')', '\\', 'n', '[SEP]']
Filtered   (034): ['all', '_', 'genes', '=', 'get', '_', 'v', '##3', '_', 'genes', '(', 'none', ',', 'none', ')', 'return', 'np', '.', 'set', '##di', '##ff', '##1', '##d', '(', 'all', '_', 'genes', ',', 'mouse', '_', 'genes', ')', '\\', 'n']
Detokenized (018): ['all_genes', '=', 'get_v##3_genes', '(', 'none', ',', 'none', ')', 'return', 'np', '.', 'set##di##ff##1##d', '(', 'all_genes', ',', 'mouse_genes', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "option_value = self . cfg . migrate [ self . option_name ] \n"
Original    (013): ['option_value', '=', 'self', '.', 'cfg', '.', 'migrate', '[', 'self', '.', 'option_name', ']', '\\n']
Tokenized   (021): ['[CLS]', 'option', '_', 'value', '=', 'self', '.', 'cf', '##g', '.', 'migrate', '[', 'self', '.', 'option', '_', 'name', ']', '\\', 'n', '[SEP]']
Filtered   (019): ['option', '_', 'value', '=', 'self', '.', 'cf', '##g', '.', 'migrate', '[', 'self', '.', 'option', '_', 'name', ']', '\\', 'n']
Detokenized (013): ['option_value', '=', 'self', '.', 'cf##g', '.', 'migrate', '[', 'self', '.', 'option_name', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "search_opts_tenant = kwargs . get ( , { } ) \n"
Original    (011): ['search_opts_tenant', '=', 'kwargs', '.', 'get', '(', ',', '{', '}', ')', '\\n']
Tokenized   (021): ['[CLS]', 'search', '_', 'opt', '##s', '_', 'tenant', '=', 'kw', '##ar', '##gs', '.', 'get', '(', ',', '{', '}', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['search', '_', 'opt', '##s', '_', 'tenant', '=', 'kw', '##ar', '##gs', '.', 'get', '(', ',', '{', '}', ')', '\\', 'n']
Detokenized (011): ['search_opt##s_tenant', '=', 'kw##ar##gs', '.', 'get', '(', ',', '{', '}', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "tenants_without_quotas = self . get_tenants_without_quotas ( tenants_src , \n"
Original    (009): ['tenants_without_quotas', '=', 'self', '.', 'get_tenants_without_quotas', '(', 'tenants_src', ',', '\\n']
Tokenized   (027): ['[CLS]', 'tenants', '_', 'without', '_', 'quota', '##s', '=', 'self', '.', 'get', '_', 'tenants', '_', 'without', '_', 'quota', '##s', '(', 'tenants', '_', 'sr', '##c', ',', '\\', 'n', '[SEP]']
Filtered   (025): ['tenants', '_', 'without', '_', 'quota', '##s', '=', 'self', '.', 'get', '_', 'tenants', '_', 'without', '_', 'quota', '##s', '(', 'tenants', '_', 'sr', '##c', ',', '\\', 'n']
Detokenized (009): ['tenants_without_quota##s', '=', 'self', '.', 'get_tenants_without_quota##s', '(', 'tenants_sr##c', ',', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "quot = network_src . show_quota ( tenants_without_quotas [ 0 ] ) \n"
Original    (012): ['quot', '=', 'network_src', '.', 'show_quota', '(', 'tenants_without_quotas', '[', '0', ']', ')', '\\n']
Tokenized   (026): ['[CLS]', 'quo', '##t', '=', 'network', '_', 'sr', '##c', '.', 'show', '_', 'quota', '(', 'tenants', '_', 'without', '_', 'quota', '##s', '[', '0', ']', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['quo', '##t', '=', 'network', '_', 'sr', '##c', '.', 'show', '_', 'quota', '(', 'tenants', '_', 'without', '_', 'quota', '##s', '[', '0', ']', ')', '\\', 'n']
Detokenized (012): ['quo##t', '=', 'network_sr##c', '.', 'show_quota', '(', 'tenants_without_quota##s', '[', '0', ']', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "quot_default_dst [ item_quot ] ) \n"
Original    (006): ['quot_default_dst', '[', 'item_quot', ']', ')', '\\n']
Tokenized   (018): ['[CLS]', 'quo', '##t', '_', 'default', '_', 'ds', '##t', '[', 'item', '_', 'quo', '##t', ']', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['quo', '##t', '_', 'default', '_', 'ds', '##t', '[', 'item', '_', 'quo', '##t', ']', ')', '\\', 'n']
Detokenized (006): ['quo##t_default_ds##t', '[', 'item_quo##t', ']', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "tenants = [ identity_src . keystone_client . tenants . find ( id = tnt_id ) for \n"
Original    (017): ['tenants', '=', '[', 'identity_src', '.', 'keystone_client', '.', 'tenants', '.', 'find', '(', 'id', '=', 'tnt_id', ')', 'for', '\\n']
Tokenized   (027): ['[CLS]', 'tenants', '=', '[', 'identity', '_', 'sr', '##c', '.', 'keystone', '_', 'client', '.', 'tenants', '.', 'find', '(', 'id', '=', 'tnt', '_', 'id', ')', 'for', '\\', 'n', '[SEP]']
Filtered   (025): ['tenants', '=', '[', 'identity', '_', 'sr', '##c', '.', 'keystone', '_', 'client', '.', 'tenants', '.', 'find', '(', 'id', '=', 'tnt', '_', 'id', ')', 'for', '\\', 'n']
Detokenized (017): ['tenants', '=', '[', 'identity_sr##c', '.', 'keystone_client', '.', 'tenants', '.', 'find', '(', 'id', '=', 'tnt_id', ')', 'for', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "tnt_id in filter_tenants_ids_list ] \n"
Original    (005): ['tnt_id', 'in', 'filter_tenants_ids_list', ']', '\\n']
Tokenized   (017): ['[CLS]', 'tnt', '_', 'id', 'in', 'filter', '_', 'tenants', '_', 'id', '##s', '_', 'list', ']', '\\', 'n', '[SEP]']
Filtered   (015): ['tnt', '_', 'id', 'in', 'filter', '_', 'tenants', '_', 'id', '##s', '_', 'list', ']', '\\', 'n']
Detokenized (005): ['tnt_id', 'in', 'filter_tenants_id##s_list', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "instance [ ] [ ] , instance [ ] [ ] ) \n"
Original    (013): ['instance', '[', ']', '[', ']', ',', 'instance', '[', ']', '[', ']', ')', '\\n']
Tokenized   (016): ['[CLS]', 'instance', '[', ']', '[', ']', ',', 'instance', '[', ']', '[', ']', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['instance', '[', ']', '[', ']', ',', 'instance', '[', ']', '[', ']', ')', '\\', 'n']
Detokenized (013): ['instance', '[', ']', '[', ']', ',', 'instance', '[', ']', '[', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "vol [ ] , storage_resource . get_status , , \n"
Original    (010): ['vol', '[', ']', ',', 'storage_resource', '.', 'get_status', ',', ',', '\\n']
Tokenized   (017): ['[CLS]', 'vol', '[', ']', ',', 'storage', '_', 'resource', '.', 'get', '_', 'status', ',', ',', '\\', 'n', '[SEP]']
Filtered   (015): ['vol', '[', ']', ',', 'storage', '_', 'resource', '.', 'get', '_', 'status', ',', ',', '\\', 'n']
Detokenized (010): ['vol', '[', ']', ',', 'storage_resource', '.', 'get_status', ',', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "inst_name = libvirt_instance_name ) ) \n"
Original    (006): ['inst_name', '=', 'libvirt_instance_name', ')', ')', '\\n']
Tokenized   (019): ['[CLS]', 'ins', '##t', '_', 'name', '=', 'li', '##b', '##vir', '##t', '_', 'instance', '_', 'name', ')', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['ins', '##t', '_', 'name', '=', 'li', '##b', '##vir', '##t', '_', 'instance', '_', 'name', ')', ')', '\\', 'n']
Detokenized (006): ['ins##t_name', '=', 'li##b##vir##t_instance_name', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "dst = instance_image_path ( instance_id ) ) \n"
Original    (008): ['dst', '=', 'instance_image_path', '(', 'instance_id', ')', ')', '\\n']
Tokenized   (018): ['[CLS]', 'ds', '##t', '=', 'instance', '_', 'image', '_', 'path', '(', 'instance', '_', 'id', ')', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['ds', '##t', '=', 'instance', '_', 'image', '_', 'path', '(', 'instance', '_', 'id', ')', ')', '\\', 'n']
Detokenized (008): ['ds##t', '=', 'instance_image_path', '(', 'instance_id', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "interface . find ( ) ) \n"
Original    (007): ['interface', '.', 'find', '(', ')', ')', '\\n']
Tokenized   (010): ['[CLS]', 'interface', '.', 'find', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (008): ['interface', '.', 'find', '(', ')', ')', '\\', 'n']
Detokenized (007): ['interface', '.', 'find', '(', ')', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "mac = self . mac , src = self . source_iface , dst = self . target_iface ) \n"
Original    (019): ['mac', '=', 'self', '.', 'mac', ',', 'src', '=', 'self', '.', 'source_iface', ',', 'dst', '=', 'self', '.', 'target_iface', ')', '\\n']
Tokenized   (030): ['[CLS]', 'mac', '=', 'self', '.', 'mac', ',', 'sr', '##c', '=', 'self', '.', 'source', '_', 'if', '##ace', ',', 'ds', '##t', '=', 'self', '.', 'target', '_', 'if', '##ace', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['mac', '=', 'self', '.', 'mac', ',', 'sr', '##c', '=', 'self', '.', 'source', '_', 'if', '##ace', ',', 'ds', '##t', '=', 'self', '.', 'target', '_', 'if', '##ace', ')', '\\', 'n']
Detokenized (019): ['mac', '=', 'self', '.', 'mac', ',', 'sr##c', '=', 'self', '.', 'source_if##ace', ',', 'ds##t', '=', 'self', '.', 'target_if##ace', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "element . attrib = { attr : value } \n"
Original    (010): ['element', '.', 'attrib', '=', '{', 'attr', ':', 'value', '}', '\\n']
Tokenized   (016): ['[CLS]', 'element', '.', 'at', '##tri', '##b', '=', '{', 'at', '##tr', ':', 'value', '}', '\\', 'n', '[SEP]']
Filtered   (014): ['element', '.', 'at', '##tri', '##b', '=', '{', 'at', '##tr', ':', 'value', '}', '\\', 'n']
Detokenized (010): ['element', '.', 'at##tri##b', '=', '{', 'at##tr', ':', 'value', '}', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "rr . run ( copy . format ( src_file = source_object . path , \n"
Original    (015): ['rr', '.', 'run', '(', 'copy', '.', 'format', '(', 'src_file', '=', 'source_object', '.', 'path', ',', '\\n']
Tokenized   (023): ['[CLS]', 'rr', '.', 'run', '(', 'copy', '.', 'format', '(', 'sr', '##c', '_', 'file', '=', 'source', '_', 'object', '.', 'path', ',', '\\', 'n', '[SEP]']
Filtered   (021): ['rr', '.', 'run', '(', 'copy', '.', 'format', '(', 'sr', '##c', '_', 'file', '=', 'source', '_', 'object', '.', 'path', ',', '\\', 'n']
Detokenized (015): ['rr', '.', 'run', '(', 'copy', '.', 'format', '(', 'sr##c_file', '=', 'source_object', '.', 'path', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "max_bytes = sizeof_format . parse_size ( kwargs . pop ( , 0 ) ) \n"
Original    (015): ['max_bytes', '=', 'sizeof_format', '.', 'parse_size', '(', 'kwargs', '.', 'pop', '(', ',', '0', ')', ')', '\\n']
Tokenized   (028): ['[CLS]', 'max', '_', 'bytes', '=', 'size', '##of', '_', 'format', '.', 'par', '##se', '_', 'size', '(', 'kw', '##ar', '##gs', '.', 'pop', '(', ',', '0', ')', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['max', '_', 'bytes', '=', 'size', '##of', '_', 'format', '.', 'par', '##se', '_', 'size', '(', 'kw', '##ar', '##gs', '.', 'pop', '(', ',', '0', ')', ')', '\\', 'n']
Detokenized (015): ['max_bytes', '=', 'size##of_format', '.', 'par##se_size', '(', 'kw##ar##gs', '.', 'pop', '(', ',', '0', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "scenario = os . path . splitext ( scenario_filename ) [ 0 ] \n"
Original    (014): ['scenario', '=', 'os', '.', 'path', '.', 'splitext', '(', 'scenario_filename', ')', '[', '0', ']', '\\n']
Tokenized   (022): ['[CLS]', 'scenario', '=', 'os', '.', 'path', '.', 'split', '##ex', '##t', '(', 'scenario', '_', 'file', '##name', ')', '[', '0', ']', '\\', 'n', '[SEP]']
Filtered   (020): ['scenario', '=', 'os', '.', 'path', '.', 'split', '##ex', '##t', '(', 'scenario', '_', 'file', '##name', ')', '[', '0', ']', '\\', 'n']
Detokenized (014): ['scenario', '=', 'os', '.', 'path', '.', 'split##ex##t', '(', 'scenario_file##name', ')', '[', '0', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "file_name = config . rollback_params [ ] [ ] \n"
Original    (010): ['file_name', '=', 'config', '.', 'rollback_params', '[', ']', '[', ']', '\\n']
Tokenized   (021): ['[CLS]', 'file', '_', 'name', '=', 'con', '##fi', '##g', '.', 'roll', '##back', '_', 'para', '##ms', '[', ']', '[', ']', '\\', 'n', '[SEP]']
Filtered   (019): ['file', '_', 'name', '=', 'con', '##fi', '##g', '.', 'roll', '##back', '_', 'para', '##ms', '[', ']', '[', ']', '\\', 'n']
Detokenized (010): ['file_name', '=', 'con##fi##g', '.', 'roll##back_para##ms', '[', ']', '[', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "pre_file_path = os . path . join ( self . cloudferry_dir , file_name ) \n"
Original    (015): ['pre_file_path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'cloudferry_dir', ',', 'file_name', ')', '\\n']
Tokenized   (028): ['[CLS]', 'pre', '_', 'file', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'cloud', '##fer', '##ry', '_', 'dir', ',', 'file', '_', 'name', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['pre', '_', 'file', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'cloud', '##fer', '##ry', '_', 'dir', ',', 'file', '_', 'name', ')', '\\', 'n']
Detokenized (015): ['pre_file_path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'cloud##fer##ry_dir', ',', 'file_name', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "o2 = C ( 2 ) \n"
Original    (007): ['o2', '=', 'C', '(', '2', ')', '\\n']
Tokenized   (011): ['[CLS]', 'o', '##2', '=', 'c', '(', '2', ')', '\\', 'n', '[SEP]']
Filtered   (009): ['o', '##2', '=', 'c', '(', '2', ')', '\\', 'n']
Detokenized (007): ['o##2', '=', 'c', '(', '2', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "org_tag = request . user . get_profile ( ) . org_tag \n"
Original    (012): ['org_tag', '=', 'request', '.', 'user', '.', 'get_profile', '(', ')', '.', 'org_tag', '\\n']
Tokenized   (021): ['[CLS]', 'org', '_', 'tag', '=', 'request', '.', 'user', '.', 'get', '_', 'profile', '(', ')', '.', 'org', '_', 'tag', '\\', 'n', '[SEP]']
Filtered   (019): ['org', '_', 'tag', '=', 'request', '.', 'user', '.', 'get', '_', 'profile', '(', ')', '.', 'org', '_', 'tag', '\\', 'n']
Detokenized (012): ['org_tag', '=', 'request', '.', 'user', '.', 'get_profile', '(', ')', '.', 'org_tag', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "featureset = Recording . objects . filter ( lat__lt = ne_lat , lat__gt = sw_lat , lon__lt = ne_lon , lon__gt = sw_lon \n"
Original    (024): ['featureset', '=', 'Recording', '.', 'objects', '.', 'filter', '(', 'lat__lt', '=', 'ne_lat', ',', 'lat__gt', '=', 'sw_lat', ',', 'lon__lt', '=', 'ne_lon', ',', 'lon__gt', '=', 'sw_lon', '\\n']
Tokenized   (056): ['[CLS]', 'features', '##et', '=', 'recording', '.', 'objects', '.', 'filter', '(', 'la', '##t', '_', '_', 'lt', '=', 'ne', '_', 'la', '##t', ',', 'la', '##t', '_', '_', 'gt', '=', 'sw', '_', 'la', '##t', ',', 'lo', '##n', '_', '_', 'lt', '=', 'ne', '_', 'lo', '##n', ',', 'lo', '##n', '_', '_', 'gt', '=', 'sw', '_', 'lo', '##n', '\\', 'n', '[SEP]']
Filtered   (054): ['features', '##et', '=', 'recording', '.', 'objects', '.', 'filter', '(', 'la', '##t', '_', '_', 'lt', '=', 'ne', '_', 'la', '##t', ',', 'la', '##t', '_', '_', 'gt', '=', 'sw', '_', 'la', '##t', ',', 'lo', '##n', '_', '_', 'lt', '=', 'ne', '_', 'lo', '##n', ',', 'lo', '##n', '_', '_', 'gt', '=', 'sw', '_', 'lo', '##n', '\\', 'n']
Detokenized (024): ['features##et', '=', 'recording', '.', 'objects', '.', 'filter', '(', 'la##t__lt', '=', 'ne_la##t', ',', 'la##t__gt', '=', 'sw_la##t', ',', 'lo##n__lt', '=', 'ne_lo##n', ',', 'lo##n__gt', '=', 'sw_lo##n', '\\n']
Counter: 54
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "httpresponse_kwargs = { : kwargs . pop ( , None ) } \n"
Original    (013): ['httpresponse_kwargs', '=', '{', ':', 'kwargs', '.', 'pop', '(', ',', 'None', ')', '}', '\\n']
Tokenized   (025): ['[CLS]', 'http', '##res', '##pon', '##se', '_', 'kw', '##ar', '##gs', '=', '{', ':', 'kw', '##ar', '##gs', '.', 'pop', '(', ',', 'none', ')', '}', '\\', 'n', '[SEP]']
Filtered   (023): ['http', '##res', '##pon', '##se', '_', 'kw', '##ar', '##gs', '=', '{', ':', 'kw', '##ar', '##gs', '.', 'pop', '(', ',', 'none', ')', '}', '\\', 'n']
Detokenized (013): ['http##res##pon##se_kw##ar##gs', '=', '{', ':', 'kw##ar##gs', '.', 'pop', '(', ',', 'none', ')', '}', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "is_testing = in sys . argv \n"
Original    (007): ['is_testing', '=', 'in', 'sys', '.', 'argv', '\\n']
Tokenized   (015): ['[CLS]', 'is', '_', 'testing', '=', 'in', 'sy', '##s', '.', 'ar', '##g', '##v', '\\', 'n', '[SEP]']
Filtered   (013): ['is', '_', 'testing', '=', 'in', 'sy', '##s', '.', 'ar', '##g', '##v', '\\', 'n']
Detokenized (007): ['is_testing', '=', 'in', 'sy##s', '.', 'ar##g##v', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "up_time = end_time - self . start_time \n"
Original    (008): ['up_time', '=', 'end_time', '-', 'self', '.', 'start_time', '\\n']
Tokenized   (017): ['[CLS]', 'up', '_', 'time', '=', 'end', '_', 'time', '-', 'self', '.', 'start', '_', 'time', '\\', 'n', '[SEP]']
Filtered   (015): ['up', '_', 'time', '=', 'end', '_', 'time', '-', 'self', '.', 'start', '_', 'time', '\\', 'n']
Detokenized (008): ['up_time', '=', 'end_time', '-', 'self', '.', 'start_time', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "remaining_time = self . count_down_total - datetime . timedelta ( seconds = ( int ( up_time ) ) ) \n"
Original    (020): ['remaining_time', '=', 'self', '.', 'count_down_total', '-', 'datetime', '.', 'timedelta', '(', 'seconds', '=', '(', 'int', '(', 'up_time', ')', ')', ')', '\\n']
Tokenized   (034): ['[CLS]', 'remaining', '_', 'time', '=', 'self', '.', 'count', '_', 'down', '_', 'total', '-', 'date', '##time', '.', 'timed', '##elt', '##a', '(', 'seconds', '=', '(', 'int', '(', 'up', '_', 'time', ')', ')', ')', '\\', 'n', '[SEP]']
Filtered   (032): ['remaining', '_', 'time', '=', 'self', '.', 'count', '_', 'down', '_', 'total', '-', 'date', '##time', '.', 'timed', '##elt', '##a', '(', 'seconds', '=', '(', 'int', '(', 'up', '_', 'time', ')', ')', ')', '\\', 'n']
Detokenized (020): ['remaining_time', '=', 'self', '.', 'count_down_total', '-', 'date##time', '.', 'timed##elt##a', '(', 'seconds', '=', '(', 'int', '(', 'up_time', ')', ')', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "abort_time = time . time ( ) + timeout \n"
Original    (010): ['abort_time', '=', 'time', '.', 'time', '(', ')', '+', 'timeout', '\\n']
Tokenized   (017): ['[CLS]', 'ab', '##ort', '_', 'time', '=', 'time', '.', 'time', '(', ')', '+', 'time', '##out', '\\', 'n', '[SEP]']
Filtered   (015): ['ab', '##ort', '_', 'time', '=', 'time', '.', 'time', '(', ')', '+', 'time', '##out', '\\', 'n']
Detokenized (010): ['ab##ort_time', '=', 'time', '.', 'time', '(', ')', '+', 'time##out', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "elif not stanza . getID ( ) : \n"
Original    (009): ['elif', 'not', 'stanza', '.', 'getID', '(', ')', ':', '\\n']
Tokenized   (014): ['[CLS]', 'eli', '##f', 'not', 'stanza', '.', 'get', '##id', '(', ')', ':', '\\', 'n', '[SEP]']
Filtered   (012): ['eli', '##f', 'not', 'stanza', '.', 'get', '##id', '(', ')', ':', '\\', 'n']
Detokenized (009): ['eli##f', 'not', 'stanza', '.', 'get##id', '(', ')', ':', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "_ID = ` ID ` \n"
Original    (006): ['_ID', '=', '`', 'ID', '`', '\\n']
Tokenized   (010): ['[CLS]', '_', 'id', '=', '`', 'id', '`', '\\', 'n', '[SEP]']
Filtered   (008): ['_', 'id', '=', '`', 'id', '`', '\\', 'n']
Detokenized (006): ['_id', '=', '`', 'id', '`', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "__description__ = , \n"
Original    (004): ['__description__', '=', ',', '\\n']
Tokenized   (011): ['[CLS]', '_', '_', 'description', '_', '_', '=', ',', '\\', 'n', '[SEP]']
Filtered   (009): ['_', '_', 'description', '_', '_', '=', ',', '\\', 'n']
Detokenized (004): ['__description__', '=', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "REQUIRES = [ i . strip ( ) for i in open ( "requirements.txt" ) . readlines ( ) ] \n"
Original    (021): ['REQUIRES', '=', '[', 'i', '.', 'strip', '(', ')', 'for', 'i', 'in', 'open', '(', '"requirements.txt"', ')', '.', 'readlines', '(', ')', ']', '\\n']
Tokenized   (030): ['[CLS]', 'requires', '=', '[', 'i', '.', 'strip', '(', ')', 'for', 'i', 'in', 'open', '(', '"', 'requirements', '.', 'tx', '##t', '"', ')', '.', 'read', '##lines', '(', ')', ']', '\\', 'n', '[SEP]']
Filtered   (028): ['requires', '=', '[', 'i', '.', 'strip', '(', ')', 'for', 'i', 'in', 'open', '(', '"', 'requirements', '.', 'tx', '##t', '"', ')', '.', 'read', '##lines', '(', ')', ']', '\\', 'n']
Detokenized (021): ['requires', '=', '[', 'i', '.', 'strip', '(', ')', 'for', 'i', 'in', 'open', '(', '"requirements.tx##t"', ')', '.', 'read##lines', '(', ')', ']', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "BaseField . __init__ ( self , ** kwargs ) \n"
Original    (010): ['BaseField', '.', '__init__', '(', 'self', ',', '**', 'kwargs', ')', '\\n']
Tokenized   (022): ['[CLS]', 'base', '##field', '.', '_', '_', 'in', '##it', '_', '_', '(', 'self', ',', '*', '*', 'kw', '##ar', '##gs', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['base', '##field', '.', '_', '_', 'in', '##it', '_', '_', '(', 'self', ',', '*', '*', 'kw', '##ar', '##gs', ')', '\\', 'n']
Detokenized (010): ['base##field', '.', '__in##it__', '(', 'self', ',', '**', 'kw##ar##gs', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "is_list = not hasattr ( items , ) \n"
Original    (009): ['is_list', '=', 'not', 'hasattr', '(', 'items', ',', ')', '\\n']
Tokenized   (016): ['[CLS]', 'is', '_', 'list', '=', 'not', 'has', '##att', '##r', '(', 'items', ',', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['is', '_', 'list', '=', 'not', 'has', '##att', '##r', '(', 'items', ',', ')', '\\', 'n']
Detokenized (009): ['is_list', '=', 'not', 'has##att##r', '(', 'items', ',', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "object_map [ ( collection , doc . id ) ] = doc \n"
Original    (013): ['object_map', '[', '(', 'collection', ',', 'doc', '.', 'id', ')', ']', '=', 'doc', '\\n']
Tokenized   (018): ['[CLS]', 'object', '_', 'map', '[', '(', 'collection', ',', 'doc', '.', 'id', ')', ']', '=', 'doc', '\\', 'n', '[SEP]']
Filtered   (016): ['object', '_', 'map', '[', '(', 'collection', ',', 'doc', '.', 'id', ')', ']', '=', 'doc', '\\', 'n']
Detokenized (013): ['object_map', '[', '(', 'collection', ',', 'doc', '.', 'id', ')', ']', '=', 'doc', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "_cls = doc . _data . pop ( , None ) \n"
Original    (012): ['_cls', '=', 'doc', '.', '_data', '.', 'pop', '(', ',', 'None', ')', '\\n']
Tokenized   (018): ['[CLS]', '_', 'cl', '##s', '=', 'doc', '.', '_', 'data', '.', 'pop', '(', ',', 'none', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['_', 'cl', '##s', '=', 'doc', '.', '_', 'data', '.', 'pop', '(', ',', 'none', ')', '\\', 'n']
Detokenized (012): ['_cl##s', '=', 'doc', '.', '_data', '.', 'pop', '(', ',', 'none', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "81.4471435546875 , \n"
Original    (003): ['81.4471435546875', ',', '\\n']
Tokenized   (015): ['[CLS]', '81', '.', '44', '##7', '##14', '##35', '##54', '##6', '##8', '##75', ',', '\\', 'n', '[SEP]']
Filtered   (013): ['81', '.', '44', '##7', '##14', '##35', '##54', '##6', '##8', '##75', ',', '\\', 'n']
Detokenized (003): ['81.44##7##14##35##54##6##8##75', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "23.61432859499169 \n"
Original    (002): ['23.61432859499169', '\\n']
Tokenized   (014): ['[CLS]', '23', '.', '61', '##43', '##28', '##59', '##49', '##9', '##16', '##9', '\\', 'n', '[SEP]']
Filtered   (012): ['23', '.', '61', '##43', '##28', '##59', '##49', '##9', '##16', '##9', '\\', 'n']
Detokenized (002): ['23.61##43##28##59##49##9##16##9', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 2, 768)
# Extracted words:  2
Sentence         : "invalid_coords = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] ] ] ] \n"
Original    (020): ['invalid_coords', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ']', ']', ']', '\\n']
Tokenized   (027): ['[CLS]', 'invalid', '_', 'co', '##ord', '##s', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ']', ']', ']', '\\', 'n', '[SEP]']
Filtered   (025): ['invalid', '_', 'co', '##ord', '##s', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ']', ']', ']', '\\', 'n']
Detokenized (020): ['invalid_co##ord##s', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ']', ']', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "Location ( loc = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ] ) . validate ( ) \n"
Original    (039): ['Location', '(', 'loc', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ',', '[', '5', ',', '6', ']', ',', '[', '1', ',', '2', ']', ']', ']', ']', ')', '.', 'validate', '(', ')', '\\n']
Tokenized   (044): ['[CLS]', 'location', '(', 'lo', '##c', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ',', '[', '5', ',', '6', ']', ',', '[', '1', ',', '2', ']', ']', ']', ']', ')', '.', 'valid', '##ate', '(', ')', '\\', 'n', '[SEP]']
Filtered   (042): ['location', '(', 'lo', '##c', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ',', '[', '5', ',', '6', ']', ',', '[', '1', ',', '2', ']', ']', ']', ']', ')', '.', 'valid', '##ate', '(', ')', '\\', 'n']
Detokenized (039): ['location', '(', 'lo##c', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ',', '[', '5', ',', '6', ']', ',', '[', '1', ',', '2', ']', ']', ']', ']', ')', '.', 'valid##ate', '(', ')', '\\n']
Counter: 42
===================================================================
Hidden states:  (13, 39, 768)
# Extracted words:  39
Sentence         : "Parent ( name = ) . save ( ) \n"
Original    (010): ['Parent', '(', 'name', '=', ')', '.', 'save', '(', ')', '\\n']
Tokenized   (013): ['[CLS]', 'parent', '(', 'name', '=', ')', '.', 'save', '(', ')', '\\', 'n', '[SEP]']
Filtered   (011): ['parent', '(', 'name', '=', ')', '.', 'save', '(', ')', '\\', 'n']
Detokenized (010): ['parent', '(', 'name', '=', ')', '.', 'save', '(', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "echo_payload = Struct ( "echo_payload" , \n"
Original    (007): ['echo_payload', '=', 'Struct', '(', '"echo_payload"', ',', '\\n']
Tokenized   (018): ['[CLS]', 'echo', '_', 'payload', '=', 'st', '##ru', '##ct', '(', '"', 'echo', '_', 'payload', '"', ',', '\\', 'n', '[SEP]']
Filtered   (016): ['echo', '_', 'payload', '=', 'st', '##ru', '##ct', '(', '"', 'echo', '_', 'payload', '"', ',', '\\', 'n']
Detokenized (007): ['echo_payload', '=', 'st##ru##ct', '(', '"echo_payload"', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "Padding ( 2 ) , \n"
Original    (006): ['Padding', '(', '2', ')', ',', '\\n']
Tokenized   (010): ['[CLS]', 'pad', '##ding', '(', '2', ')', ',', '\\', 'n', '[SEP]']
Filtered   (008): ['pad', '##ding', '(', '2', ')', ',', '\\', 'n']
Detokenized (006): ['pad##ding', '(', '2', ')', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "IpAddress ( "host" ) , \n"
Original    (006): ['IpAddress', '(', '"host"', ')', ',', '\\n']
Tokenized   (013): ['[CLS]', 'ipad', '##dre', '##ss', '(', '"', 'host', '"', ')', ',', '\\', 'n', '[SEP]']
Filtered   (011): ['ipad', '##dre', '##ss', '(', '"', 'host', '"', ')', ',', '\\', 'n']
Detokenized (006): ['ipad##dre##ss', '(', '"host"', ')', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "Bytes ( "echo" , 8 ) , \n"
Original    (008): ['Bytes', '(', '"echo"', ',', '8', ')', ',', '\\n']
Tokenized   (013): ['[CLS]', 'bytes', '(', '"', 'echo', '"', ',', '8', ')', ',', '\\', 'n', '[SEP]']
Filtered   (011): ['bytes', '(', '"', 'echo', '"', ',', '8', ')', ',', '\\', 'n']
Detokenized (008): ['bytes', '(', '"echo"', ',', '8', ')', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "dest_unreachable_code = Enum ( Byte ( "code" ) , \n"
Original    (010): ['dest_unreachable_code', '=', 'Enum', '(', 'Byte', '(', '"code"', ')', ',', '\\n']
Tokenized   (024): ['[CLS]', 'des', '##t', '_', 'un', '##rea', '##cha', '##ble', '_', 'code', '=', 'en', '##um', '(', 'byte', '(', '"', 'code', '"', ')', ',', '\\', 'n', '[SEP]']
Filtered   (022): ['des', '##t', '_', 'un', '##rea', '##cha', '##ble', '_', 'code', '=', 'en', '##um', '(', 'byte', '(', '"', 'code', '"', ')', ',', '\\', 'n']
Detokenized (010): ['des##t_un##rea##cha##ble_code', '=', 'en##um', '(', 'byte', '(', '"code"', ')', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "Enum ( Byte ( "type" ) , \n"
Original    (008): ['Enum', '(', 'Byte', '(', '"type"', ')', ',', '\\n']
Tokenized   (014): ['[CLS]', 'en', '##um', '(', 'byte', '(', '"', 'type', '"', ')', ',', '\\', 'n', '[SEP]']
Filtered   (012): ['en', '##um', '(', 'byte', '(', '"', 'type', '"', ')', ',', '\\', 'n']
Detokenized (008): ['en##um', '(', 'byte', '(', '"type"', ')', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "Switch ( "payload" , lambda ctx : ctx . type , \n"
Original    (012): ['Switch', '(', '"payload"', ',', 'lambda', 'ctx', ':', 'ctx', '.', 'type', ',', '\\n']
Tokenized   (019): ['[CLS]', 'switch', '(', '"', 'payload', '"', ',', 'lambda', 'ct', '##x', ':', 'ct', '##x', '.', 'type', ',', '\\', 'n', '[SEP]']
Filtered   (017): ['switch', '(', '"', 'payload', '"', ',', 'lambda', 'ct', '##x', ':', 'ct', '##x', '.', 'type', ',', '\\', 'n']
Detokenized (012): ['switch', '(', '"payload"', ',', 'lambda', 'ct##x', ':', 'ct##x', '.', 'type', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : ""63646566676869" ) . decode ( "hex" ) \n"
Original    (008): ['"63646566676869"', ')', '.', 'decode', '(', '"hex"', ')', '\\n']
Tokenized   (025): ['[CLS]', '"', '63', '##64', '##65', '##66', '##6', '##7', '##6', '##86', '##9', '"', ')', '.', 'deco', '##de', '(', '"', 'he', '##x', '"', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['"', '63', '##64', '##65', '##66', '##6', '##7', '##6', '##86', '##9', '"', ')', '.', 'deco', '##de', '(', '"', 'he', '##x', '"', ')', '\\', 'n']
Detokenized (008): ['"63##64##65##66##6##7##6##86##9"', ')', '.', 'deco##de', '(', '"he##x"', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "cap2 = ( "0000385c02001b006162636465666768696a6b6c6d6e6f70717273747576776162" \n"
Original    (005): ['cap2', '=', '(', '"0000385c02001b006162636465666768696a6b6c6d6e6f70717273747576776162"', '\\n']
Tokenized   (056): ['[CLS]', 'cap', '##2', '=', '(', '"', '000', '##0', '##38', '##5', '##c', '##0', '##200', '##1', '##b', '##00', '##6', '##16', '##26', '##36', '##46', '##56', '##66', '##7', '##6', '##86', '##9', '##6', '##a', '##6', '##b', '##6', '##c', '##6', '##d', '##6', '##e', '##6', '##f', '##70', '##7', '##17', '##27', '##37', '##47', '##57', '##6', '##7', '##7', '##6', '##16', '##2', '"', '\\', 'n', '[SEP]']
Filtered   (054): ['cap', '##2', '=', '(', '"', '000', '##0', '##38', '##5', '##c', '##0', '##200', '##1', '##b', '##00', '##6', '##16', '##26', '##36', '##46', '##56', '##66', '##7', '##6', '##86', '##9', '##6', '##a', '##6', '##b', '##6', '##c', '##6', '##d', '##6', '##e', '##6', '##f', '##70', '##7', '##17', '##27', '##37', '##47', '##57', '##6', '##7', '##7', '##6', '##16', '##2', '"', '\\', 'n']
Detokenized (005): ['cap##2', '=', '(', '"000##0##38##5##c##0##200##1##b##00##6##16##26##36##46##56##66##7##6##86##9##6##a##6##b##6##c##6##d##6##e##6##f##70##7##17##27##37##47##57##6##7##7##6##16##2"', '\\n']
Counter: 54
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "cap3 = ( "0301000000001122aabbccdd0102030405060708" ) . decode ( "hex" ) \n"
Original    (011): ['cap3', '=', '(', '"0301000000001122aabbccdd0102030405060708"', ')', '.', 'decode', '(', '"hex"', ')', '\\n']
Tokenized   (041): ['[CLS]', 'cap', '##3', '=', '(', '"', '03', '##01', '##00', '##00', '##00', '##00', '##11', '##22', '##aa', '##bb', '##cc', '##dd', '##01', '##0', '##20', '##30', '##40', '##50', '##60', '##70', '##8', '"', ')', '.', 'deco', '##de', '(', '"', 'he', '##x', '"', ')', '\\', 'n', '[SEP]']
Filtered   (039): ['cap', '##3', '=', '(', '"', '03', '##01', '##00', '##00', '##00', '##00', '##11', '##22', '##aa', '##bb', '##cc', '##dd', '##01', '##0', '##20', '##30', '##40', '##50', '##60', '##70', '##8', '"', ')', '.', 'deco', '##de', '(', '"', 'he', '##x', '"', ')', '\\', 'n']
Detokenized (011): ['cap##3', '=', '(', '"03##01##00##00##00##00##11##22##aa##bb##cc##dd##01##0##20##30##40##50##60##70##8"', ')', '.', 'deco##de', '(', '"he##x"', ')', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "intps [ ] = dest_target . vlan \n"
Original    (008): ['intps', '[', ']', '=', 'dest_target', '.', 'vlan', '\\n']
Tokenized   (016): ['[CLS]', 'int', '##ps', '[', ']', '=', 'des', '##t', '_', 'target', '.', 'v', '##lan', '\\', 'n', '[SEP]']
Filtered   (014): ['int', '##ps', '[', ']', '=', 'des', '##t', '_', 'target', '.', 'v', '##lan', '\\', 'n']
Detokenized (008): ['int##ps', '[', ']', '=', 'des##t_target', '.', 'v##lan', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "router , interface = ri . split ( ) \n"
Original    (010): ['router', ',', 'interface', '=', 'ri', '.', 'split', '(', ')', '\\n']
Tokenized   (014): ['[CLS]', 'route', '##r', ',', 'interface', '=', 'ri', '.', 'split', '(', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['route', '##r', ',', 'interface', '=', 'ri', '.', 'split', '(', ')', '\\', 'n']
Detokenized (010): ['route##r', ',', 'interface', '=', 'ri', '.', 'split', '(', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "cm = NCSVPNConnectionManager ( ncs_services_url , user , password , port_map , name ) \n"
Original    (015): ['cm', '=', 'NCSVPNConnectionManager', '(', 'ncs_services_url', ',', 'user', ',', 'password', ',', 'port_map', ',', 'name', ')', '\\n']
Tokenized   (034): ['[CLS]', 'cm', '=', 'nc', '##s', '##v', '##p', '##nco', '##nne', '##ction', '##mana', '##ger', '(', 'nc', '##s', '_', 'services', '_', 'ur', '##l', ',', 'user', ',', 'password', ',', 'port', '_', 'map', ',', 'name', ')', '\\', 'n', '[SEP]']
Filtered   (032): ['cm', '=', 'nc', '##s', '##v', '##p', '##nco', '##nne', '##ction', '##mana', '##ger', '(', 'nc', '##s', '_', 'services', '_', 'ur', '##l', ',', 'user', ',', 'password', ',', 'port', '_', 'map', ',', 'name', ')', '\\', 'n']
Detokenized (015): ['cm', '=', 'nc##s##v##p##nco##nne##ction##mana##ger', '(', 'nc##s_services_ur##l', ',', 'user', ',', 'password', ',', 'port_map', ',', 'name', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "soap_resource . registerDecoder ( actions . QUERY_RECURSIVE , self . queryRecursive ) \n"
Original    (013): ['soap_resource', '.', 'registerDecoder', '(', 'actions', '.', 'QUERY_RECURSIVE', ',', 'self', '.', 'queryRecursive', ')', '\\n']
Tokenized   (029): ['[CLS]', 'soap', '_', 'resource', '.', 'register', '##de', '##code', '##r', '(', 'actions', '.', 'query', '_', 'rec', '##urs', '##ive', ',', 'self', '.', 'query', '##re', '##cu', '##rs', '##ive', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['soap', '_', 'resource', '.', 'register', '##de', '##code', '##r', '(', 'actions', '.', 'query', '_', 'rec', '##urs', '##ive', ',', 'self', '.', 'query', '##re', '##cu', '##rs', '##ive', ')', '\\', 'n']
Detokenized (013): ['soap_resource', '.', 'register##de##code##r', '(', 'actions', '.', 'query_rec##urs##ive', ',', 'self', '.', 'query##re##cu##rs##ive', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "soap_fault = soapresource . SOAPFault ( err . getErrorMessage ( ) , ex_element ) \n"
Original    (015): ['soap_fault', '=', 'soapresource', '.', 'SOAPFault', '(', 'err', '.', 'getErrorMessage', '(', ')', ',', 'ex_element', ')', '\\n']
Tokenized   (033): ['[CLS]', 'soap', '_', 'fault', '=', 'soap', '##res', '##our', '##ce', '.', 'soap', '##fa', '##ult', '(', 'er', '##r', '.', 'get', '##er', '##ror', '##mes', '##sa', '##ge', '(', ')', ',', 'ex', '_', 'element', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['soap', '_', 'fault', '=', 'soap', '##res', '##our', '##ce', '.', 'soap', '##fa', '##ult', '(', 'er', '##r', '.', 'get', '##er', '##ror', '##mes', '##sa', '##ge', '(', ')', ',', 'ex', '_', 'element', ')', '\\', 'n']
Detokenized (015): ['soap_fault', '=', 'soap##res##our##ce', '.', 'soap##fa##ult', '(', 'er##r', '.', 'get##er##ror##mes##sa##ge', '(', ')', ',', 'ex_element', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "symmetric = p2ps . symmetricPath or False sd = nsa . Point2PointService ( src_stp , dst_stp , p2ps . capacity , p2ps . directionality , symmetric , \n"
Original    (028): ['symmetric', '=', 'p2ps', '.', 'symmetricPath', 'or', 'False', 'sd', '=', 'nsa', '.', 'Point2PointService', '(', 'src_stp', ',', 'dst_stp', ',', 'p2ps', '.', 'capacity', ',', 'p2ps', '.', 'directionality', ',', 'symmetric', ',', '\\n']
Tokenized   (052): ['[CLS]', 'symmetric', '=', 'p', '##2', '##ps', '.', 'symmetric', '##path', 'or', 'false', 'sd', '=', 'nsa', '.', 'point', '##2', '##points', '##er', '##vic', '##e', '(', 'sr', '##c', '_', 'st', '##p', ',', 'ds', '##t', '_', 'st', '##p', ',', 'p', '##2', '##ps', '.', 'capacity', ',', 'p', '##2', '##ps', '.', 'directional', '##ity', ',', 'symmetric', ',', '\\', 'n', '[SEP]']
Filtered   (050): ['symmetric', '=', 'p', '##2', '##ps', '.', 'symmetric', '##path', 'or', 'false', 'sd', '=', 'nsa', '.', 'point', '##2', '##points', '##er', '##vic', '##e', '(', 'sr', '##c', '_', 'st', '##p', ',', 'ds', '##t', '_', 'st', '##p', ',', 'p', '##2', '##ps', '.', 'capacity', ',', 'p', '##2', '##ps', '.', 'directional', '##ity', ',', 'symmetric', ',', '\\', 'n']
Detokenized (028): ['symmetric', '=', 'p##2##ps', '.', 'symmetric##path', 'or', 'false', 'sd', '=', 'nsa', '.', 'point##2##points##er##vic##e', '(', 'sr##c_st##p', ',', 'ds##t_st##p', ',', 'p##2##ps', '.', 'capacity', ',', 'p##2##ps', '.', 'directional##ity', ',', 'symmetric', ',', '\\n']
Counter: 50
===================================================================
Hidden states:  (13, 28, 768)
# Extracted words:  28
Sentence         : "crt = nsa . Criteria ( criteria . version , schedule , sd ) \n"
Original    (015): ['crt', '=', 'nsa', '.', 'Criteria', '(', 'criteria', '.', 'version', ',', 'schedule', ',', 'sd', ')', '\\n']
Tokenized   (019): ['[CLS]', 'cr', '##t', '=', 'nsa', '.', 'criteria', '(', 'criteria', '.', 'version', ',', 'schedule', ',', 'sd', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['cr', '##t', '=', 'nsa', '.', 'criteria', '(', 'criteria', '.', 'version', ',', 'schedule', ',', 'sd', ')', '\\', 'n']
Detokenized (015): ['cr##t', '=', 'nsa', '.', 'criteria', '(', 'criteria', '.', 'version', ',', 'schedule', ',', 'sd', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "tc = json . load ( open ( tcf ) ) \n"
Original    (012): ['tc', '=', 'json', '.', 'load', '(', 'open', '(', 'tcf', ')', ')', '\\n']
Tokenized   (017): ['[CLS]', 'tc', '=', 'j', '##son', '.', 'load', '(', 'open', '(', 'tc', '##f', ')', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['tc', '=', 'j', '##son', '.', 'load', '(', 'open', '(', 'tc', '##f', ')', ')', '\\', 'n']
Detokenized (012): ['tc', '=', 'j##son', '.', 'load', '(', 'open', '(', 'tc##f', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "source_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , dest_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , start_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 2 ) \n"
Original    (057): ['source_stp', '=', 'nsa', '.', 'STP', '(', ',', ',', 'labels', '=', '[', 'nsa', '.', 'Label', '(', 'nml', '.', 'ETHERNET_VLAN', ',', 'dest_stp', '=', 'nsa', '.', 'STP', '(', ',', ',', 'labels', '=', '[', 'nsa', '.', 'Label', '(', 'nml', '.', 'ETHERNET_VLAN', ',', 'start_time', '=', 'datetime', '.', 'datetime', '.', 'utcnow', '(', ')', '+', 'datetime', '.', 'timedelta', '(', 'seconds', '=', '2', ')', '\\n']
Tokenized   (085): ['[CLS]', 'source', '_', 'st', '##p', '=', 'nsa', '.', 'st', '##p', '(', ',', ',', 'labels', '=', '[', 'nsa', '.', 'label', '(', 'nm', '##l', '.', 'ethernet', '_', 'v', '##lan', ',', 'des', '##t', '_', 'st', '##p', '=', 'nsa', '.', 'st', '##p', '(', ',', ',', 'labels', '=', '[', 'nsa', '.', 'label', '(', 'nm', '##l', '.', 'ethernet', '_', 'v', '##lan', ',', 'start', '_', 'time', '=', 'date', '##time', '.', 'date', '##time', '.', 'utc', '##now', '(', ')', '+', 'date', '##time', '.', 'timed', '##elt', '##a', '(', 'seconds', '=', '2', ')', '\\', 'n', '[SEP]']
Filtered   (083): ['source', '_', 'st', '##p', '=', 'nsa', '.', 'st', '##p', '(', ',', ',', 'labels', '=', '[', 'nsa', '.', 'label', '(', 'nm', '##l', '.', 'ethernet', '_', 'v', '##lan', ',', 'des', '##t', '_', 'st', '##p', '=', 'nsa', '.', 'st', '##p', '(', ',', ',', 'labels', '=', '[', 'nsa', '.', 'label', '(', 'nm', '##l', '.', 'ethernet', '_', 'v', '##lan', ',', 'start', '_', 'time', '=', 'date', '##time', '.', 'date', '##time', '.', 'utc', '##now', '(', ')', '+', 'date', '##time', '.', 'timed', '##elt', '##a', '(', 'seconds', '=', '2', ')', '\\', 'n']
Detokenized (057): ['source_st##p', '=', 'nsa', '.', 'st##p', '(', ',', ',', 'labels', '=', '[', 'nsa', '.', 'label', '(', 'nm##l', '.', 'ethernet_v##lan', ',', 'des##t_st##p', '=', 'nsa', '.', 'st##p', '(', ',', ',', 'labels', '=', '[', 'nsa', '.', 'label', '(', 'nm##l', '.', 'ethernet_v##lan', ',', 'start_time', '=', 'date##time', '.', 'date##time', '.', 'utc##now', '(', ')', '+', 'date##time', '.', 'timed##elt##a', '(', 'seconds', '=', '2', ')', '\\n']
Counter: 83
===================================================================
Hidden states:  (13, 57, 768)
# Extracted words:  57
Sentence         : "end_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 30 ) \n"
Original    (019): ['end_time', '=', 'datetime', '.', 'datetime', '.', 'utcnow', '(', ')', '+', 'datetime', '.', 'timedelta', '(', 'seconds', '=', '30', ')', '\\n']
Tokenized   (030): ['[CLS]', 'end', '_', 'time', '=', 'date', '##time', '.', 'date', '##time', '.', 'utc', '##now', '(', ')', '+', 'date', '##time', '.', 'timed', '##elt', '##a', '(', 'seconds', '=', '30', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['end', '_', 'time', '=', 'date', '##time', '.', 'date', '##time', '.', 'utc', '##now', '(', ')', '+', 'date', '##time', '.', 'timed', '##elt', '##a', '(', 'seconds', '=', '30', ')', '\\', 'n']
Detokenized (019): ['end_time', '=', 'date##time', '.', 'date##time', '.', 'utc##now', '(', ')', '+', 'date##time', '.', 'timed##elt##a', '(', 'seconds', '=', '30', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "connection_id , active , version_consistent , version , timestamp = yield d_down \n"
Original    (013): ['connection_id', ',', 'active', ',', 'version_consistent', ',', 'version', ',', 'timestamp', '=', 'yield', 'd_down', '\\n']
Tokenized   (024): ['[CLS]', 'connection', '_', 'id', ',', 'active', ',', 'version', '_', 'consistent', ',', 'version', ',', 'times', '##tam', '##p', '=', 'yield', 'd', '_', 'down', '\\', 'n', '[SEP]']
Filtered   (022): ['connection', '_', 'id', ',', 'active', ',', 'version', '_', 'consistent', ',', 'version', ',', 'times', '##tam', '##p', '=', 'yield', 'd', '_', 'down', '\\', 'n']
Detokenized (013): ['connection_id', ',', 'active', ',', 'version_consistent', ',', 'version', ',', 'times##tam##p', '=', 'yield', 'd_down', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "scheduler = digits . scheduler . Scheduler ( config_value ( ) , True ) \n"
Original    (015): ['scheduler', '=', 'digits', '.', 'scheduler', '.', 'Scheduler', '(', 'config_value', '(', ')', ',', 'True', ')', '\\n']
Tokenized   (025): ['[CLS]', 'schedule', '##r', '=', 'digits', '.', 'schedule', '##r', '.', 'schedule', '##r', '(', 'con', '##fi', '##g', '_', 'value', '(', ')', ',', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['schedule', '##r', '=', 'digits', '.', 'schedule', '##r', '.', 'schedule', '##r', '(', 'con', '##fi', '##g', '_', 'value', '(', ')', ',', 'true', ')', '\\', 'n']
Detokenized (015): ['schedule##r', '=', 'digits', '.', 'schedule##r', '.', 'schedule##r', '(', 'con##fi##g_value', '(', ')', ',', 'true', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "row_index = int ( params [ ] [ 0 ] ) \n"
Original    (012): ['row_index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '\\n']
Tokenized   (018): ['[CLS]', 'row', '_', 'index', '=', 'int', '(', 'para', '##ms', '[', ']', '[', '0', ']', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['row', '_', 'index', '=', 'int', '(', 'para', '##ms', '[', ']', '[', '0', ']', ')', '\\', 'n']
Detokenized (012): ['row_index', '=', 'int', '(', 'para##ms', '[', ']', '[', '0', ']', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "char_index = int ( params [ ] [ 0 ] ) - 1 \n"
Original    (014): ['char_index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\n']
Tokenized   (020): ['[CLS]', 'char', '_', 'index', '=', 'int', '(', 'para', '##ms', '[', ']', '[', '0', ']', ')', '-', '1', '\\', 'n', '[SEP]']
Filtered   (018): ['char', '_', 'index', '=', 'int', '(', 'para', '##ms', '[', ']', '[', '0', ']', ')', '-', '1', '\\', 'n']
Detokenized (014): ['char_index', '=', 'int', '(', 'para##ms', '[', ']', '[', '0', ']', ')', '-', '1', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "comparator = comparators . index ( params [ ] [ 0 ] ) - 1 \n"
Original    (016): ['comparator', '=', 'comparators', '.', 'index', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\n']
Tokenized   (024): ['[CLS]', 'com', '##para', '##tor', '=', 'com', '##para', '##tors', '.', 'index', '(', 'para', '##ms', '[', ']', '[', '0', ']', ')', '-', '1', '\\', 'n', '[SEP]']
Filtered   (022): ['com', '##para', '##tor', '=', 'com', '##para', '##tors', '.', 'index', '(', 'para', '##ms', '[', ']', '[', '0', ']', ')', '-', '1', '\\', 'n']
Detokenized (016): ['com##para##tor', '=', 'com##para##tors', '.', 'index', '(', 'para##ms', '[', ']', '[', '0', ']', ')', '-', '1', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "truth = ( cmp ( ord ( current_character ) , test_char ) == comparator ) \n"
Original    (016): ['truth', '=', '(', 'cmp', '(', 'ord', '(', 'current_character', ')', ',', 'test_char', ')', '==', 'comparator', ')', '\\n']
Tokenized   (028): ['[CLS]', 'truth', '=', '(', 'cm', '##p', '(', 'or', '##d', '(', 'current', '_', 'character', ')', ',', 'test', '_', 'char', ')', '=', '=', 'com', '##para', '##tor', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['truth', '=', '(', 'cm', '##p', '(', 'or', '##d', '(', 'current', '_', 'character', ')', ',', 'test', '_', 'char', ')', '=', '=', 'com', '##para', '##tor', ')', '\\', 'n']
Detokenized (016): ['truth', '=', '(', 'cm##p', '(', 'or##d', '(', 'current_character', ')', ',', 'test_char', ')', '==', 'com##para##tor', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "start_response ( , [ ( , ) ] ) \n"
Original    (010): ['start_response', '(', ',', '[', '(', ',', ')', ']', ')', '\\n']
Tokenized   (015): ['[CLS]', 'start', '_', 'response', '(', ',', '[', '(', ',', ')', ']', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['start', '_', 'response', '(', ',', '[', '(', ',', ')', ']', ')', '\\', 'n']
Detokenized (010): ['start_response', '(', ',', '[', '(', ',', ')', ']', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "CHARSET = [ chr ( x ) for x in xrange ( 32 , 127 ) ] \n"
Original    (018): ['CHARSET', '=', '[', 'chr', '(', 'x', ')', 'for', 'x', 'in', 'xrange', '(', '32', ',', '127', ')', ']', '\\n']
Tokenized   (025): ['[CLS]', 'char', '##set', '=', '[', 'ch', '##r', '(', 'x', ')', 'for', 'x', 'in', 'x', '##rang', '##e', '(', '32', ',', '127', ')', ']', '\\', 'n', '[SEP]']
Filtered   (023): ['char', '##set', '=', '[', 'ch', '##r', '(', 'x', ')', 'for', 'x', 'in', 'x', '##rang', '##e', '(', '32', ',', '127', ')', ']', '\\', 'n']
Detokenized (018): ['char##set', '=', '[', 'ch##r', '(', 'x', ')', 'for', 'x', 'in', 'x##rang##e', '(', '32', ',', '127', ')', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "obj_struct [ ] = [ int ( bbox . find ( ) . text ) , \n"
Original    (017): ['obj_struct', '[', ']', '=', '[', 'int', '(', 'bbox', '.', 'find', '(', ')', '.', 'text', ')', ',', '\\n']
Tokenized   (026): ['[CLS]', 'ob', '##j', '_', 'st', '##ru', '##ct', '[', ']', '=', '[', 'int', '(', 'bb', '##ox', '.', 'find', '(', ')', '.', 'text', ')', ',', '\\', 'n', '[SEP]']
Filtered   (024): ['ob', '##j', '_', 'st', '##ru', '##ct', '[', ']', '=', '[', 'int', '(', 'bb', '##ox', '.', 'find', '(', ')', '.', 'text', ')', ',', '\\', 'n']
Detokenized (017): ['ob##j_st##ru##ct', '[', ']', '=', '[', 'int', '(', 'bb##ox', '.', 'find', '(', ')', '.', 'text', ')', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "int ( bbox . find ( ) . text ) ] \n"
Original    (012): ['int', '(', 'bbox', '.', 'find', '(', ')', '.', 'text', ')', ']', '\\n']
Tokenized   (016): ['[CLS]', 'int', '(', 'bb', '##ox', '.', 'find', '(', ')', '.', 'text', ')', ']', '\\', 'n', '[SEP]']
Filtered   (014): ['int', '(', 'bb', '##ox', '.', 'find', '(', ')', '.', 'text', ')', ']', '\\', 'n']
Detokenized (012): ['int', '(', 'bb##ox', '.', 'find', '(', ')', '.', 'text', ')', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "mpre = np . concatenate ( ( [ 0. ] , prec , [ 0. ] ) ) \n"
Original    (019): ['mpre', '=', 'np', '.', 'concatenate', '(', '(', '[', '0.', ']', ',', 'prec', ',', '[', '0.', ']', ')', ')', '\\n']
Tokenized   (028): ['[CLS]', 'mp', '##re', '=', 'np', '.', 'con', '##cate', '##nate', '(', '(', '[', '0', '.', ']', ',', 'pre', '##c', ',', '[', '0', '.', ']', ')', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['mp', '##re', '=', 'np', '.', 'con', '##cate', '##nate', '(', '(', '[', '0', '.', ']', ',', 'pre', '##c', ',', '[', '0', '.', ']', ')', ')', '\\', 'n']
Detokenized (019): ['mp##re', '=', 'np', '.', 'con##cate##nate', '(', '(', '[', '0.', ']', ',', 'pre##c', ',', '[', '0.', ']', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "difficult = np . array ( [ x [ ] for x in R ] ) . astype ( np . bool ) \n"
Original    (024): ['difficult', '=', 'np', '.', 'array', '(', '[', 'x', '[', ']', 'for', 'x', 'in', 'R', ']', ')', '.', 'astype', '(', 'np', '.', 'bool', ')', '\\n']
Tokenized   (029): ['[CLS]', 'difficult', '=', 'np', '.', 'array', '(', '[', 'x', '[', ']', 'for', 'x', 'in', 'r', ']', ')', '.', 'as', '##type', '(', 'np', '.', 'boo', '##l', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['difficult', '=', 'np', '.', 'array', '(', '[', 'x', '[', ']', 'for', 'x', 'in', 'r', ']', ')', '.', 'as', '##type', '(', 'np', '.', 'boo', '##l', ')', '\\', 'n']
Detokenized (024): ['difficult', '=', 'np', '.', 'array', '(', '[', 'x', '[', ']', 'for', 'x', 'in', 'r', ']', ')', '.', 'as##type', '(', 'np', '.', 'boo##l', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "det = [ False ] * len ( R ) \n"
Original    (011): ['det', '=', '[', 'False', ']', '*', 'len', '(', 'R', ')', '\\n']
Tokenized   (014): ['[CLS]', 'det', '=', '[', 'false', ']', '*', 'len', '(', 'r', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['det', '=', '[', 'false', ']', '*', 'len', '(', 'r', ')', '\\', 'n']
Detokenized (011): ['det', '=', '[', 'false', ']', '*', 'len', '(', 'r', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "npos = npos + sum ( ~ difficult ) \n"
Original    (010): ['npos', '=', 'npos', '+', 'sum', '(', '~', 'difficult', ')', '\\n']
Tokenized   (015): ['[CLS]', 'np', '##os', '=', 'np', '##os', '+', 'sum', '(', '~', 'difficult', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['np', '##os', '=', 'np', '##os', '+', 'sum', '(', '~', 'difficult', ')', '\\', 'n']
Detokenized (010): ['np##os', '=', 'np##os', '+', 'sum', '(', '~', 'difficult', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "class_recs [ imagename ] = { : bbox , \n"
Original    (010): ['class_recs', '[', 'imagename', ']', '=', '{', ':', 'bbox', ',', '\\n']
Tokenized   (018): ['[CLS]', 'class', '_', 'rec', '##s', '[', 'image', '##name', ']', '=', '{', ':', 'bb', '##ox', ',', '\\', 'n', '[SEP]']
Filtered   (016): ['class', '_', 'rec', '##s', '[', 'image', '##name', ']', '=', '{', ':', 'bb', '##ox', ',', '\\', 'n']
Detokenized (010): ['class_rec##s', '[', 'image##name', ']', '=', '{', ':', 'bb##ox', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "confidence = np . array ( [ float ( x [ 1 ] ) for x in splitlines ] ) \n"
Original    (021): ['confidence', '=', 'np', '.', 'array', '(', '[', 'float', '(', 'x', '[', '1', ']', ')', 'for', 'x', 'in', 'splitlines', ']', ')', '\\n']
Tokenized   (025): ['[CLS]', 'confidence', '=', 'np', '.', 'array', '(', '[', 'float', '(', 'x', '[', '1', ']', ')', 'for', 'x', 'in', 'split', '##lines', ']', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['confidence', '=', 'np', '.', 'array', '(', '[', 'float', '(', 'x', '[', '1', ']', ')', 'for', 'x', 'in', 'split', '##lines', ']', ')', '\\', 'n']
Detokenized (021): ['confidence', '=', 'np', '.', 'array', '(', '[', 'float', '(', 'x', '[', '1', ']', ')', 'for', 'x', 'in', 'split##lines', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "sorted_ind = np . argsort ( - confidence ) \n"
Original    (010): ['sorted_ind', '=', 'np', '.', 'argsort', '(', '-', 'confidence', ')', '\\n']
Tokenized   (017): ['[CLS]', 'sorted', '_', 'ind', '=', 'np', '.', 'ar', '##gs', '##ort', '(', '-', 'confidence', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['sorted', '_', 'ind', '=', 'np', '.', 'ar', '##gs', '##ort', '(', '-', 'confidence', ')', '\\', 'n']
Detokenized (010): ['sorted_ind', '=', 'np', '.', 'ar##gs##ort', '(', '-', 'confidence', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "BB = BB [ sorted_ind , : ] \n"
Original    (009): ['BB', '=', 'BB', '[', 'sorted_ind', ',', ':', ']', '\\n']
Tokenized   (014): ['[CLS]', 'bb', '=', 'bb', '[', 'sorted', '_', 'ind', ',', ':', ']', '\\', 'n', '[SEP]']
Filtered   (012): ['bb', '=', 'bb', '[', 'sorted', '_', 'ind', ',', ':', ']', '\\', 'n']
Detokenized (009): ['bb', '=', 'bb', '[', 'sorted_ind', ',', ':', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "image_ids = [ image_ids [ x ] for x in sorted_ind ] \n"
Original    (013): ['image_ids', '=', '[', 'image_ids', '[', 'x', ']', 'for', 'x', 'in', 'sorted_ind', ']', '\\n']
Tokenized   (024): ['[CLS]', 'image', '_', 'id', '##s', '=', '[', 'image', '_', 'id', '##s', '[', 'x', ']', 'for', 'x', 'in', 'sorted', '_', 'ind', ']', '\\', 'n', '[SEP]']
Filtered   (022): ['image', '_', 'id', '##s', '=', '[', 'image', '_', 'id', '##s', '[', 'x', ']', 'for', 'x', 'in', 'sorted', '_', 'ind', ']', '\\', 'n']
Detokenized (013): ['image_id##s', '=', '[', 'image_id##s', '[', 'x', ']', 'for', 'x', 'in', 'sorted_ind', ']', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "bb = BB [ d , : ] . astype ( float ) \n"
Original    (014): ['bb', '=', 'BB', '[', 'd', ',', ':', ']', '.', 'astype', '(', 'float', ')', '\\n']
Tokenized   (018): ['[CLS]', 'bb', '=', 'bb', '[', 'd', ',', ':', ']', '.', 'as', '##type', '(', 'float', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['bb', '=', 'bb', '[', 'd', ',', ':', ']', '.', 'as', '##type', '(', 'float', ')', '\\', 'n']
Detokenized (014): ['bb', '=', 'bb', '[', 'd', ',', ':', ']', '.', 'as##type', '(', 'float', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "BBGT = R [ ] . astype ( float ) \n"
Original    (011): ['BBGT', '=', 'R', '[', ']', '.', 'astype', '(', 'float', ')', '\\n']
Tokenized   (016): ['[CLS]', 'bb', '##gt', '=', 'r', '[', ']', '.', 'as', '##type', '(', 'float', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['bb', '##gt', '=', 'r', '[', ']', '.', 'as', '##type', '(', 'float', ')', '\\', 'n']
Detokenized (011): ['bb##gt', '=', 'r', '[', ']', '.', 'as##type', '(', 'float', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "iymin = np . maximum ( BBGT [ : , 1 ] , bb [ 1 ] ) \n"
Original    (019): ['iymin', '=', 'np', '.', 'maximum', '(', 'BBGT', '[', ':', ',', '1', ']', ',', 'bb', '[', '1', ']', ')', '\\n']
Tokenized   (025): ['[CLS]', 'i', '##ym', '##in', '=', 'np', '.', 'maximum', '(', 'bb', '##gt', '[', ':', ',', '1', ']', ',', 'bb', '[', '1', ']', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['i', '##ym', '##in', '=', 'np', '.', 'maximum', '(', 'bb', '##gt', '[', ':', ',', '1', ']', ',', 'bb', '[', '1', ']', ')', '\\', 'n']
Detokenized (019): ['i##ym##in', '=', 'np', '.', 'maximum', '(', 'bb##gt', '[', ':', ',', '1', ']', ',', 'bb', '[', '1', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "iw = np . maximum ( ixmax - ixmin + 1. , 0. ) \n"
Original    (015): ['iw', '=', 'np', '.', 'maximum', '(', 'ixmax', '-', 'ixmin', '+', '1.', ',', '0.', ')', '\\n']
Tokenized   (023): ['[CLS]', 'i', '##w', '=', 'np', '.', 'maximum', '(', 'ix', '##max', '-', 'ix', '##min', '+', '1', '.', ',', '0', '.', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['i', '##w', '=', 'np', '.', 'maximum', '(', 'ix', '##max', '-', 'ix', '##min', '+', '1', '.', ',', '0', '.', ')', '\\', 'n']
Detokenized (015): ['i##w', '=', 'np', '.', 'maximum', '(', 'ix##max', '-', 'ix##min', '+', '1.', ',', '0.', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "uni = ( ( bb [ 2 ] - bb [ 0 ] + 1. ) * ( bb [ 3 ] - bb [ 1 ] + 1. ) + \n"
Original    (032): ['uni', '=', '(', '(', 'bb', '[', '2', ']', '-', 'bb', '[', '0', ']', '+', '1.', ')', '*', '(', 'bb', '[', '3', ']', '-', 'bb', '[', '1', ']', '+', '1.', ')', '+', '\\n']
Tokenized   (038): ['[CLS]', 'un', '##i', '=', '(', '(', 'bb', '[', '2', ']', '-', 'bb', '[', '0', ']', '+', '1', '.', ')', '*', '(', 'bb', '[', '3', ']', '-', 'bb', '[', '1', ']', '+', '1', '.', ')', '+', '\\', 'n', '[SEP]']
Filtered   (036): ['un', '##i', '=', '(', '(', 'bb', '[', '2', ']', '-', 'bb', '[', '0', ']', '+', '1', '.', ')', '*', '(', 'bb', '[', '3', ']', '-', 'bb', '[', '1', ']', '+', '1', '.', ')', '+', '\\', 'n']
Detokenized (032): ['un##i', '=', '(', '(', 'bb', '[', '2', ']', '-', 'bb', '[', '0', ']', '+', '1.', ')', '*', '(', 'bb', '[', '3', ']', '-', 'bb', '[', '1', ']', '+', '1.', ')', '+', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "rec = tp / float ( npos ) \n"
Original    (009): ['rec', '=', 'tp', '/', 'float', '(', 'npos', ')', '\\n']
Tokenized   (014): ['[CLS]', 'rec', '=', 't', '##p', '/', 'float', '(', 'np', '##os', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['rec', '=', 't', '##p', '/', 'float', '(', 'np', '##os', ')', '\\', 'n']
Detokenized (009): ['rec', '=', 't##p', '/', 'float', '(', 'np##os', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "prec = tp / ( tp + fp + 1e-10 ) \n"
Original    (012): ['prec', '=', 'tp', '/', '(', 'tp', '+', 'fp', '+', '1e-10', ')', '\\n']
Tokenized   (022): ['[CLS]', 'pre', '##c', '=', 't', '##p', '/', '(', 't', '##p', '+', 'f', '##p', '+', '1', '##e', '-', '10', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['pre', '##c', '=', 't', '##p', '/', '(', 't', '##p', '+', 'f', '##p', '+', '1', '##e', '-', '10', ')', '\\', 'n']
Detokenized (012): ['pre##c', '=', 't##p', '/', '(', 't##p', '+', 'f##p', '+', '1##e-10', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "scale = strip_mantissa ( maxval ) / float ( 1 << ( bits - sign - 1 ) ) \n"
Original    (020): ['scale', '=', 'strip_mantissa', '(', 'maxval', ')', '/', 'float', '(', '1', '<<', '(', 'bits', '-', 'sign', '-', '1', ')', ')', '\\n']
Tokenized   (029): ['[CLS]', 'scale', '=', 'strip', '_', 'man', '##tis', '##sa', '(', 'max', '##val', ')', '/', 'float', '(', '1', '<', '<', '(', 'bits', '-', 'sign', '-', '1', ')', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['scale', '=', 'strip', '_', 'man', '##tis', '##sa', '(', 'max', '##val', ')', '/', 'float', '(', '1', '<', '<', '(', 'bits', '-', 'sign', '-', '1', ')', ')', '\\', 'n']
Detokenized (020): ['scale', '=', 'strip_man##tis##sa', '(', 'max##val', ')', '/', 'float', '(', '1', '<<', '(', 'bits', '-', 'sign', '-', '1', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "ary = np . around ( ary * ( 1.0 / scale ) ) . astype ( np . int64 ) \n"
Original    (022): ['ary', '=', 'np', '.', 'around', '(', 'ary', '*', '(', '1.0', '/', 'scale', ')', ')', '.', 'astype', '(', 'np', '.', 'int64', ')', '\\n']
Tokenized   (031): ['[CLS]', 'ar', '##y', '=', 'np', '.', 'around', '(', 'ar', '##y', '*', '(', '1', '.', '0', '/', 'scale', ')', ')', '.', 'as', '##type', '(', 'np', '.', 'int', '##64', ')', '\\', 'n', '[SEP]']
Filtered   (029): ['ar', '##y', '=', 'np', '.', 'around', '(', 'ar', '##y', '*', '(', '1', '.', '0', '/', 'scale', ')', ')', '.', 'as', '##type', '(', 'np', '.', 'int', '##64', ')', '\\', 'n']
Detokenized (022): ['ar##y', '=', 'np', '.', 'around', '(', 'ar##y', '*', '(', '1.0', '/', 'scale', ')', ')', '.', 'as##type', '(', 'np', '.', 'int##64', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "f2 -= dif \n"
Original    (004): ['f2', '-=', 'dif', '\\n']
Tokenized   (010): ['[CLS]', 'f', '##2', '-', '=', 'di', '##f', '\\', 'n', '[SEP]']
Filtered   (008): ['f', '##2', '-', '=', 'di', '##f', '\\', 'n']
Detokenized (004): ['f##2', '-=', 'di##f', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "slicedF = F [ : , sliceR , sliceS , : ] . reshape ( ( - 1 , K ) ) \n"
Original    (023): ['slicedF', '=', 'F', '[', ':', ',', 'sliceR', ',', 'sliceS', ',', ':', ']', '.', 'reshape', '(', '(', '-', '1', ',', 'K', ')', ')', '\\n']
Tokenized   (030): ['[CLS]', 'sliced', '##f', '=', 'f', '[', ':', ',', 'slice', '##r', ',', 'slices', ',', ':', ']', '.', 'res', '##ha', '##pe', '(', '(', '-', '1', ',', 'k', ')', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['sliced', '##f', '=', 'f', '[', ':', ',', 'slice', '##r', ',', 'slices', ',', ':', ']', '.', 'res', '##ha', '##pe', '(', '(', '-', '1', ',', 'k', ')', ')', '\\', 'n']
Detokenized (023): ['sliced##f', '=', 'f', '[', ':', ',', 'slice##r', ',', 'slices', ',', ':', ']', '.', 'res##ha##pe', '(', '(', '-', '1', ',', 'k', ')', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "K , P , Q , N = E . shape \n"
Original    (012): ['K', ',', 'P', ',', 'Q', ',', 'N', '=', 'E', '.', 'shape', '\\n']
Tokenized   (015): ['[CLS]', 'k', ',', 'p', ',', 'q', ',', 'n', '=', 'e', '.', 'shape', '\\', 'n', '[SEP]']
Filtered   (013): ['k', ',', 'p', ',', 'q', ',', 'n', '=', 'e', '.', 'shape', '\\', 'n']
Detokenized (012): ['k', ',', 'p', ',', 'q', ',', 'n', '=', 'e', '.', 'shape', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "qSlice = [ fconv_slice ( q , S , X , padding [ 0 ] , strides [ 0 ] ) for q in range ( Q ) ] \n"
Original    (030): ['qSlice', '=', '[', 'fconv_slice', '(', 'q', ',', 'S', ',', 'X', ',', 'padding', '[', '0', ']', ',', 'strides', '[', '0', ']', ')', 'for', 'q', 'in', 'range', '(', 'Q', ')', ']', '\\n']
Tokenized   (040): ['[CLS]', 'q', '##sl', '##ice', '=', '[', 'fc', '##on', '##v', '_', 'slice', '(', 'q', ',', 's', ',', 'x', ',', 'pad', '##ding', '[', '0', ']', ',', 'strides', '[', '0', ']', ')', 'for', 'q', 'in', 'range', '(', 'q', ')', ']', '\\', 'n', '[SEP]']
Filtered   (038): ['q', '##sl', '##ice', '=', '[', 'fc', '##on', '##v', '_', 'slice', '(', 'q', ',', 's', ',', 'x', ',', 'pad', '##ding', '[', '0', ']', ',', 'strides', '[', '0', ']', ')', 'for', 'q', 'in', 'range', '(', 'q', ')', ']', '\\', 'n']
Detokenized (030): ['q##sl##ice', '=', '[', 'fc##on##v_slice', '(', 'q', ',', 's', ',', 'x', ',', 'pad##ding', '[', '0', ']', ',', 'strides', '[', '0', ']', ')', 'for', 'q', 'in', 'range', '(', 'q', ')', ']', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 30, 768)
# Extracted words:  30
Sentence         : "slicedE = E [ : , p , q , : ] \n"
Original    (013): ['slicedE', '=', 'E', '[', ':', ',', 'p', ',', 'q', ',', ':', ']', '\\n']
Tokenized   (017): ['[CLS]', 'sliced', '##e', '=', 'e', '[', ':', ',', 'p', ',', 'q', ',', ':', ']', '\\', 'n', '[SEP]']
Filtered   (015): ['sliced', '##e', '=', 'e', '[', ':', ',', 'p', ',', 'q', ',', ':', ']', '\\', 'n']
Detokenized (013): ['sliced##e', '=', 'e', '[', ':', ',', 'p', ',', 'q', ',', ':', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "rcp3 = 1.0 / 3.0 \n"
Original    (006): ['rcp3', '=', '1.0', '/', '3.0', '\\n']
Tokenized   (015): ['[CLS]', 'rc', '##p', '##3', '=', '1', '.', '0', '/', '3', '.', '0', '\\', 'n', '[SEP]']
Filtered   (013): ['rc', '##p', '##3', '=', '1', '.', '0', '/', '3', '.', '0', '\\', 'n']
Detokenized (006): ['rc##p##3', '=', '1.0', '/', '3.0', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "t3 = I [ 1 , : ] + I [ 3 , : ] * 4.0 \n"
Original    (018): ['t3', '=', 'I', '[', '1', ',', ':', ']', '+', 'I', '[', '3', ',', ':', ']', '*', '4.0', '\\n']
Tokenized   (024): ['[CLS]', 't', '##3', '=', 'i', '[', '1', ',', ':', ']', '+', 'i', '[', '3', ',', ':', ']', '*', '4', '.', '0', '\\', 'n', '[SEP]']
Filtered   (022): ['t', '##3', '=', 'i', '[', '1', ',', ':', ']', '+', 'i', '[', '3', ',', ':', ']', '*', '4', '.', '0', '\\', 'n']
Detokenized (018): ['t##3', '=', 'i', '[', '1', ',', ':', ']', '+', 'i', '[', '3', ',', ':', ']', '*', '4.0', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "T1 = np . empty ( ( 3 , 3 ) ) \n"
Original    (013): ['T1', '=', 'np', '.', 'empty', '(', '(', '3', ',', '3', ')', ')', '\\n']
Tokenized   (017): ['[CLS]', 't', '##1', '=', 'np', '.', 'empty', '(', '(', '3', ',', '3', ')', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['t', '##1', '=', 'np', '.', 'empty', '(', '(', '3', ',', '3', ')', ')', '\\', 'n']
Detokenized (013): ['t##1', '=', 'np', '.', 'empty', '(', '(', '3', ',', '3', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "Fw = np . empty ( ( D , D , C , K ) ) \n"
Original    (017): ['Fw', '=', 'np', '.', 'empty', '(', '(', 'D', ',', 'D', ',', 'C', ',', 'K', ')', ')', '\\n']
Tokenized   (021): ['[CLS]', 'f', '##w', '=', 'np', '.', 'empty', '(', '(', 'd', ',', 'd', ',', 'c', ',', 'k', ')', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['f', '##w', '=', 'np', '.', 'empty', '(', '(', 'd', ',', 'd', ',', 'c', ',', 'k', ')', ')', '\\', 'n']
Detokenized (017): ['f##w', '=', 'np', '.', 'empty', '(', '(', 'd', ',', 'd', ',', 'c', ',', 'k', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "sliceI = I [ : , start_y : stop_y , start_x : stop_x , : ] \n"
Original    (017): ['sliceI', '=', 'I', '[', ':', ',', 'start_y', ':', 'stop_y', ',', 'start_x', ':', 'stop_x', ',', ':', ']', '\\n']
Tokenized   (029): ['[CLS]', 'slice', '##i', '=', 'i', '[', ':', ',', 'start', '_', 'y', ':', 'stop', '_', 'y', ',', 'start', '_', 'x', ':', 'stop', '_', 'x', ',', ':', ']', '\\', 'n', '[SEP]']
Filtered   (027): ['slice', '##i', '=', 'i', '[', ':', ',', 'start', '_', 'y', ':', 'stop', '_', 'y', ',', 'start', '_', 'x', ':', 'stop', '_', 'x', ',', ':', ']', '\\', 'n']
Detokenized (017): ['slice##i', '=', 'i', '[', ':', ',', 'start_y', ':', 'stop_y', ',', 'start_x', ':', 'stop_x', ',', ':', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "O [ k , p0 : p1 , q0 : q1 , n ] = Out [ 0 : plen , 0 : qlen ] \n"
Original    (026): ['O', '[', 'k', ',', 'p0', ':', 'p1', ',', 'q0', ':', 'q1', ',', 'n', ']', '=', 'Out', '[', '0', ':', 'plen', ',', '0', ':', 'qlen', ']', '\\n']
Tokenized   (035): ['[CLS]', 'o', '[', 'k', ',', 'p', '##0', ':', 'p', '##1', ',', 'q', '##0', ':', 'q', '##1', ',', 'n', ']', '=', 'out', '[', '0', ':', 'pl', '##en', ',', '0', ':', 'q', '##len', ']', '\\', 'n', '[SEP]']
Filtered   (033): ['o', '[', 'k', ',', 'p', '##0', ':', 'p', '##1', ',', 'q', '##0', ':', 'q', '##1', ',', 'n', ']', '=', 'out', '[', '0', ':', 'pl', '##en', ',', '0', ':', 'q', '##len', ']', '\\', 'n']
Detokenized (026): ['o', '[', 'k', ',', 'p##0', ':', 'p##1', ',', 'q##0', ':', 'q##1', ',', 'n', ']', '=', 'out', '[', '0', ':', 'pl##en', ',', '0', ':', 'q##len', ']', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "start_p , stop_p , pad_p = image_slice ( y , P , B , B ) \n"
Original    (017): ['start_p', ',', 'stop_p', ',', 'pad_p', '=', 'image_slice', '(', 'y', ',', 'P', ',', 'B', ',', 'B', ')', '\\n']
Tokenized   (028): ['[CLS]', 'start', '_', 'p', ',', 'stop', '_', 'p', ',', 'pad', '_', 'p', '=', 'image', '_', 'slice', '(', 'y', ',', 'p', ',', 'b', ',', 'b', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['start', '_', 'p', ',', 'stop', '_', 'p', ',', 'pad', '_', 'p', '=', 'image', '_', 'slice', '(', 'y', ',', 'p', ',', 'b', ',', 'b', ')', '\\', 'n']
Detokenized (017): ['start_p', ',', 'stop_p', ',', 'pad_p', '=', 'image_slice', '(', 'y', ',', 'p', ',', 'b', ',', 'b', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "trans = ( 2 , 2 ) \n"
Original    (008): ['trans', '=', '(', '2', ',', '2', ')', '\\n']
Tokenized   (011): ['[CLS]', 'trans', '=', '(', '2', ',', '2', ')', '\\', 'n', '[SEP]']
Filtered   (009): ['trans', '=', '(', '2', ',', '2', ')', '\\', 'n']
Detokenized (008): ['trans', '=', '(', '2', ',', '2', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "E = np . random . uniform ( - 1.0 , 1.0 , dimO ) \n"
Original    (016): ['E', '=', 'np', '.', 'random', '.', 'uniform', '(', '-', '1.0', ',', '1.0', ',', 'dimO', ')', '\\n']
Tokenized   (024): ['[CLS]', 'e', '=', 'np', '.', 'random', '.', 'uniform', '(', '-', '1', '.', '0', ',', '1', '.', '0', ',', 'dim', '##o', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['e', '=', 'np', '.', 'random', '.', 'uniform', '(', '-', '1', '.', '0', ',', '1', '.', '0', ',', 'dim', '##o', ')', '\\', 'n']
Detokenized (016): ['e', '=', 'np', '.', 'random', '.', 'uniform', '(', '-', '1.0', ',', '1.0', ',', 'dim##o', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "xprop_direct ( E , F , Bd , padding , strides , backward = True ) \n"
Original    (017): ['xprop_direct', '(', 'E', ',', 'F', ',', 'Bd', ',', 'padding', ',', 'strides', ',', 'backward', '=', 'True', ')', '\\n']
Tokenized   (025): ['[CLS]', 'xp', '##rop', '_', 'direct', '(', 'e', ',', 'f', ',', 'b', '##d', ',', 'pad', '##ding', ',', 'strides', ',', 'backward', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['xp', '##rop', '_', 'direct', '(', 'e', ',', 'f', ',', 'b', '##d', ',', 'pad', '##ding', ',', 'strides', ',', 'backward', '=', 'true', ')', '\\', 'n']
Detokenized (017): ['xp##rop_direct', '(', 'e', ',', 'f', ',', 'b##d', ',', 'pad##ding', ',', 'strides', ',', 'backward', '=', 'true', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "xprop_winograd ( E , F , Bw , padding , minimal = minimal , trans = trans , backward = True ) \n"
Original    (023): ['xprop_winograd', '(', 'E', ',', 'F', ',', 'Bw', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ',', 'backward', '=', 'True', ')', '\\n']
Tokenized   (033): ['[CLS]', 'xp', '##rop', '_', 'win', '##og', '##rad', '(', 'e', ',', 'f', ',', 'b', '##w', ',', 'pad', '##ding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ',', 'backward', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['xp', '##rop', '_', 'win', '##og', '##rad', '(', 'e', ',', 'f', ',', 'b', '##w', ',', 'pad', '##ding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ',', 'backward', '=', 'true', ')', '\\', 'n']
Detokenized (023): ['xp##rop_win##og##rad', '(', 'e', ',', 'f', ',', 'b##w', ',', 'pad##ding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ',', 'backward', '=', 'true', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "updat_direct ( I , E , Ud , padding , strides ) \n"
Original    (013): ['updat_direct', '(', 'I', ',', 'E', ',', 'Ud', ',', 'padding', ',', 'strides', ')', '\\n']
Tokenized   (021): ['[CLS]', 'up', '##da', '##t', '_', 'direct', '(', 'i', ',', 'e', ',', 'ud', ',', 'pad', '##ding', ',', 'strides', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['up', '##da', '##t', '_', 'direct', '(', 'i', ',', 'e', ',', 'ud', ',', 'pad', '##ding', ',', 'strides', ')', '\\', 'n']
Detokenized (013): ['up##da##t_direct', '(', 'i', ',', 'e', ',', 'ud', ',', 'pad##ding', ',', 'strides', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "updat_winograd ( I , E , Uw , padding , minimal = minimal , trans = trans ) \n"
Original    (019): ['updat_winograd', '(', 'I', ',', 'E', ',', 'Uw', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ')', '\\n']
Tokenized   (030): ['[CLS]', 'up', '##da', '##t', '_', 'win', '##og', '##rad', '(', 'i', ',', 'e', ',', 'u', '##w', ',', 'pad', '##ding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['up', '##da', '##t', '_', 'win', '##og', '##rad', '(', 'i', ',', 'e', ',', 'u', '##w', ',', 'pad', '##ding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ')', '\\', 'n']
Detokenized (019): ['up##da##t_win##og##rad', '(', 'i', ',', 'e', ',', 'u##w', ',', 'pad##ding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "BranchNode , SkipNode , LRN , ColorNoise ) \n"
Original    (009): ['BranchNode', ',', 'SkipNode', ',', 'LRN', ',', 'ColorNoise', ')', '\\n']
Tokenized   (019): ['[CLS]', 'branch', '##no', '##de', ',', 'skip', '##no', '##de', ',', 'l', '##rn', ',', 'color', '##no', '##ise', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['branch', '##no', '##de', ',', 'skip', '##no', '##de', ',', 'l', '##rn', ',', 'color', '##no', '##ise', ')', '\\', 'n']
Detokenized (009): ['branch##no##de', ',', 'skip##no##de', ',', 'l##rn', ',', 'color##no##ise', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "img_set_options = dict ( repo_dir = args . data_dir , \n"
Original    (011): ['img_set_options', '=', 'dict', '(', 'repo_dir', '=', 'args', '.', 'data_dir', ',', '\\n']
Tokenized   (026): ['[CLS]', 'im', '##g', '_', 'set', '_', 'options', '=', 'di', '##ct', '(', 'rep', '##o', '_', 'dir', '=', 'ar', '##gs', '.', 'data', '_', 'dir', ',', '\\', 'n', '[SEP]']
Filtered   (024): ['im', '##g', '_', 'set', '_', 'options', '=', 'di', '##ct', '(', 'rep', '##o', '_', 'dir', '=', 'ar', '##gs', '.', 'data', '_', 'dir', ',', '\\', 'n']
Detokenized (011): ['im##g_set_options', '=', 'di##ct', '(', 'rep##o_dir', '=', 'ar##gs', '.', 'data_dir', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "subset_pct = 0.09990891117239205 ) \n"
Original    (005): ['subset_pct', '=', '0.09990891117239205', ')', '\\n']
Tokenized   (023): ['[CLS]', 'subset', '_', 'pc', '##t', '=', '0', '.', '09', '##9', '##90', '##8', '##9', '##11', '##17', '##23', '##9', '##20', '##5', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['subset', '_', 'pc', '##t', '=', '0', '.', '09', '##9', '##90', '##8', '##9', '##11', '##17', '##23', '##9', '##20', '##5', ')', '\\', 'n']
Detokenized (005): ['subset_pc##t', '=', '0.09##9##90##8##9##11##17##23##9##20##5', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "do_transforms = False , ** img_set_options ) \n"
Original    (008): ['do_transforms', '=', 'False', ',', '**', 'img_set_options', ')', '\\n']
Tokenized   (019): ['[CLS]', 'do', '_', 'transforms', '=', 'false', ',', '*', '*', 'im', '##g', '_', 'set', '_', 'options', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['do', '_', 'transforms', '=', 'false', ',', '*', '*', 'im', '##g', '_', 'set', '_', 'options', ')', '\\', 'n']
Detokenized (008): ['do_transforms', '=', 'false', ',', '**', 'im##g_set_options', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "test = ImageLoader ( set_name = , scale_range = ( 256 , 384 ) , shuffle = False , \n"
Original    (020): ['test', '=', 'ImageLoader', '(', 'set_name', '=', ',', 'scale_range', '=', '(', '256', ',', '384', ')', ',', 'shuffle', '=', 'False', ',', '\\n']
Tokenized   (030): ['[CLS]', 'test', '=', 'image', '##load', '##er', '(', 'set', '_', 'name', '=', ',', 'scale', '_', 'range', '=', '(', '256', ',', '38', '##4', ')', ',', 'shuffle', '=', 'false', ',', '\\', 'n', '[SEP]']
Filtered   (028): ['test', '=', 'image', '##load', '##er', '(', 'set', '_', 'name', '=', ',', 'scale', '_', 'range', '=', '(', '256', ',', '38', '##4', ')', ',', 'shuffle', '=', 'false', ',', '\\', 'n']
Detokenized (020): ['test', '=', 'image##load##er', '(', 'set_name', '=', ',', 'scale_range', '=', '(', '256', ',', '38##4', ')', ',', 'shuffle', '=', 'false', ',', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "Pooling ( 3 , strides = 2 ) , \n"
Original    (010): ['Pooling', '(', '3', ',', 'strides', '=', '2', ')', ',', '\\n']
Tokenized   (014): ['[CLS]', 'pool', '##ing', '(', '3', ',', 'strides', '=', '2', ')', ',', '\\', 'n', '[SEP]']
Filtered   (012): ['pool', '##ing', '(', '3', ',', 'strides', '=', '2', ')', ',', '\\', 'n']
Detokenized (010): ['pool##ing', '(', '3', ',', 'strides', '=', '2', ')', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "activation = Rectlin ( ) , padding = 1 ) , \n"
Original    (012): ['activation', '=', 'Rectlin', '(', ')', ',', 'padding', '=', '1', ')', ',', '\\n']
Tokenized   (018): ['[CLS]', 'activation', '=', 'rec', '##tl', '##in', '(', ')', ',', 'pad', '##ding', '=', '1', ')', ',', '\\', 'n', '[SEP]']
Filtered   (016): ['activation', '=', 'rec', '##tl', '##in', '(', ')', ',', 'pad', '##ding', '=', '1', ')', ',', '\\', 'n']
Detokenized (012): ['activation', '=', 'rec##tl##in', '(', ')', ',', 'pad##ding', '=', '1', ')', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "Conv ( ( 3 , 3 , 256 ) , init = Gaussian ( scale = 0.03 ) , bias = Constant ( 1 ) , \n"
Original    (027): ['Conv', '(', '(', '3', ',', '3', ',', '256', ')', ',', 'init', '=', 'Gaussian', '(', 'scale', '=', '0.03', ')', ',', 'bias', '=', 'Constant', '(', '1', ')', ',', '\\n']
Tokenized   (036): ['[CLS]', 'con', '##v', '(', '(', '3', ',', '3', ',', '256', ')', ',', 'in', '##it', '=', 'ga', '##uss', '##ian', '(', 'scale', '=', '0', '.', '03', ')', ',', 'bias', '=', 'constant', '(', '1', ')', ',', '\\', 'n', '[SEP]']
Filtered   (034): ['con', '##v', '(', '(', '3', ',', '3', ',', '256', ')', ',', 'in', '##it', '=', 'ga', '##uss', '##ian', '(', 'scale', '=', '0', '.', '03', ')', ',', 'bias', '=', 'constant', '(', '1', ')', ',', '\\', 'n']
Detokenized (027): ['con##v', '(', '(', '3', ',', '3', ',', '256', ')', ',', 'in##it', '=', 'ga##uss##ian', '(', 'scale', '=', '0.03', ')', ',', 'bias', '=', 'constant', '(', '1', ')', ',', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "Dropout ( keep = 1.0 ) , \n"
Original    (008): ['Dropout', '(', 'keep', '=', '1.0', ')', ',', '\\n']
Tokenized   (014): ['[CLS]', 'drop', '##out', '(', 'keep', '=', '1', '.', '0', ')', ',', '\\', 'n', '[SEP]']
Filtered   (012): ['drop', '##out', '(', 'keep', '=', '1', '.', '0', ')', ',', '\\', 'n']
Detokenized (008): ['drop##out', '(', 'keep', '=', '1.0', ')', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "Affine ( nout = 1000 , init = Gaussian ( scale = 0.01 ) , bias = Constant ( - 7 ) , activation = Softmax ( ) ) ] \n"
Original    (031): ['Affine', '(', 'nout', '=', '1000', ',', 'init', '=', 'Gaussian', '(', 'scale', '=', '0.01', ')', ',', 'bias', '=', 'Constant', '(', '-', '7', ')', ',', 'activation', '=', 'Softmax', '(', ')', ')', ']', '\\n']
Tokenized   (042): ['[CLS]', 'af', '##fine', '(', 'no', '##ut', '=', '1000', ',', 'in', '##it', '=', 'ga', '##uss', '##ian', '(', 'scale', '=', '0', '.', '01', ')', ',', 'bias', '=', 'constant', '(', '-', '7', ')', ',', 'activation', '=', 'soft', '##max', '(', ')', ')', ']', '\\', 'n', '[SEP]']
Filtered   (040): ['af', '##fine', '(', 'no', '##ut', '=', '1000', ',', 'in', '##it', '=', 'ga', '##uss', '##ian', '(', 'scale', '=', '0', '.', '01', ')', ',', 'bias', '=', 'constant', '(', '-', '7', ')', ',', 'activation', '=', 'soft', '##max', '(', ')', ')', ']', '\\', 'n']
Detokenized (031): ['af##fine', '(', 'no##ut', '=', '1000', ',', 'in##it', '=', 'ga##uss##ian', '(', 'scale', '=', '0.01', ')', ',', 'bias', '=', 'constant', '(', '-', '7', ')', ',', 'activation', '=', 'soft##max', '(', ')', ')', ']', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 31, 768)
# Extracted words:  31
Sentence         : "weight_sched = Schedule ( [ 22 , 44 , 65 ] , ( 1 / 250. ) ** ( 1 / 3. ) ) \n"
Original    (025): ['weight_sched', '=', 'Schedule', '(', '[', '22', ',', '44', ',', '65', ']', ',', '(', '1', '/', '250.', ')', '**', '(', '1', '/', '3.', ')', ')', '\\n']
Tokenized   (034): ['[CLS]', 'weight', '_', 'sc', '##hed', '=', 'schedule', '(', '[', '22', ',', '44', ',', '65', ']', ',', '(', '1', '/', '250', '.', ')', '*', '*', '(', '1', '/', '3', '.', ')', ')', '\\', 'n', '[SEP]']
Filtered   (032): ['weight', '_', 'sc', '##hed', '=', 'schedule', '(', '[', '22', ',', '44', ',', '65', ']', ',', '(', '1', '/', '250', '.', ')', '*', '*', '(', '1', '/', '3', '.', ')', ')', '\\', 'n']
Detokenized (025): ['weight_sc##hed', '=', 'schedule', '(', '[', '22', ',', '44', ',', '65', ']', ',', '(', '1', '/', '250.', ')', '**', '(', '1', '/', '3.', ')', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "opt_gdm = GradientDescentMomentum ( 0.01 / 10 , 0.9 , wdecay = 0.0005 , schedule = weight_sched , \n"
Original    (019): ['opt_gdm', '=', 'GradientDescentMomentum', '(', '0.01', '/', '10', ',', '0.9', ',', 'wdecay', '=', '0.0005', ',', 'schedule', '=', 'weight_sched', ',', '\\n']
Tokenized   (043): ['[CLS]', 'opt', '_', 'g', '##dm', '=', 'gradient', '##des', '##cent', '##mo', '##ment', '##um', '(', '0', '.', '01', '/', '10', ',', '0', '.', '9', ',', 'w', '##de', '##ca', '##y', '=', '0', '.', '000', '##5', ',', 'schedule', '=', 'weight', '_', 'sc', '##hed', ',', '\\', 'n', '[SEP]']
Filtered   (041): ['opt', '_', 'g', '##dm', '=', 'gradient', '##des', '##cent', '##mo', '##ment', '##um', '(', '0', '.', '01', '/', '10', ',', '0', '.', '9', ',', 'w', '##de', '##ca', '##y', '=', '0', '.', '000', '##5', ',', 'schedule', '=', 'weight', '_', 'sc', '##hed', ',', '\\', 'n']
Detokenized (019): ['opt_g##dm', '=', 'gradient##des##cent##mo##ment##um', '(', '0.01', '/', '10', ',', '0.9', ',', 'w##de##ca##y', '=', '0.000##5', ',', 'schedule', '=', 'weight_sc##hed', ',', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "opt_biases = GradientDescentMomentum ( 0.02 / 10 , 0.9 , schedule = Schedule ( [ 44 ] , 0.1 ) , \n"
Original    (022): ['opt_biases', '=', 'GradientDescentMomentum', '(', '0.02', '/', '10', ',', '0.9', ',', 'schedule', '=', 'Schedule', '(', '[', '44', ']', ',', '0.1', ')', ',', '\\n']
Tokenized   (039): ['[CLS]', 'opt', '_', 'bias', '##es', '=', 'gradient', '##des', '##cent', '##mo', '##ment', '##um', '(', '0', '.', '02', '/', '10', ',', '0', '.', '9', ',', 'schedule', '=', 'schedule', '(', '[', '44', ']', ',', '0', '.', '1', ')', ',', '\\', 'n', '[SEP]']
Filtered   (037): ['opt', '_', 'bias', '##es', '=', 'gradient', '##des', '##cent', '##mo', '##ment', '##um', '(', '0', '.', '02', '/', '10', ',', '0', '.', '9', ',', 'schedule', '=', 'schedule', '(', '[', '44', ']', ',', '0', '.', '1', ')', ',', '\\', 'n']
Detokenized (022): ['opt_bias##es', '=', 'gradient##des##cent##mo##ment##um', '(', '0.02', '/', '10', ',', '0.9', ',', 'schedule', '=', 'schedule', '(', '[', '44', ']', ',', '0.1', ')', ',', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "valmetric = TopKMisclassification ( k = 5 ) \n"
Original    (009): ['valmetric', '=', 'TopKMisclassification', '(', 'k', '=', '5', ')', '\\n']
Tokenized   (017): ['[CLS]', 'val', '##metric', '=', 'top', '##km', '##is', '##class', '##ification', '(', 'k', '=', '5', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['val', '##metric', '=', 'top', '##km', '##is', '##class', '##ification', '(', 'k', '=', '5', ')', '\\', 'n']
Detokenized (009): ['val##metric', '=', 'top##km##is##class##ification', '(', 'k', '=', '5', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "nifm_rng = [ 8 ] \n"
Original    (006): ['nifm_rng', '=', '[', '8', ']', '\\n']
Tokenized   (013): ['[CLS]', 'ni', '##fm', '_', 'rn', '##g', '=', '[', '8', ']', '\\', 'n', '[SEP]']
Filtered   (011): ['ni', '##fm', '_', 'rn', '##g', '=', '[', '8', ']', '\\', 'n']
Detokenized (006): ['ni##fm_rn##g', '=', '[', '8', ']', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "fargs_ . append ( itt . product ( fs_rng , nifm_rng , pad_rng , stride_rng , in_sz_rng , bsz_rng ) ) \n"
Original    (022): ['fargs_', '.', 'append', '(', 'itt', '.', 'product', '(', 'fs_rng', ',', 'nifm_rng', ',', 'pad_rng', ',', 'stride_rng', ',', 'in_sz_rng', ',', 'bsz_rng', ')', ')', '\\n']
Tokenized   (053): ['[CLS]', 'far', '##gs', '_', '.', 'app', '##end', '(', 'it', '##t', '.', 'product', '(', 'f', '##s', '_', 'rn', '##g', ',', 'ni', '##fm', '_', 'rn', '##g', ',', 'pad', '_', 'rn', '##g', ',', 'stride', '_', 'rn', '##g', ',', 'in', '_', 's', '##z', '_', 'rn', '##g', ',', 'bs', '##z', '_', 'rn', '##g', ')', ')', '\\', 'n', '[SEP]']
Filtered   (051): ['far', '##gs', '_', '.', 'app', '##end', '(', 'it', '##t', '.', 'product', '(', 'f', '##s', '_', 'rn', '##g', ',', 'ni', '##fm', '_', 'rn', '##g', ',', 'pad', '_', 'rn', '##g', ',', 'stride', '_', 'rn', '##g', ',', 'in', '_', 's', '##z', '_', 'rn', '##g', ',', 'bs', '##z', '_', 'rn', '##g', ')', ')', '\\', 'n']
Detokenized (022): ['far##gs_', '.', 'app##end', '(', 'it##t', '.', 'product', '(', 'f##s_rn##g', ',', 'ni##fm_rn##g', ',', 'pad_rn##g', ',', 'stride_rn##g', ',', 'in_s##z_rn##g', ',', 'bs##z_rn##g', ')', ')', '\\n']
Counter: 51
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "bsz = inp . shape [ - 1 ] \n"
Original    (010): ['bsz', '=', 'inp', '.', 'shape', '[', '-', '1', ']', '\\n']
Tokenized   (015): ['[CLS]', 'bs', '##z', '=', 'in', '##p', '.', 'shape', '[', '-', '1', ']', '\\', 'n', '[SEP]']
Filtered   (013): ['bs', '##z', '=', 'in', '##p', '.', 'shape', '[', '-', '1', ']', '\\', 'n']
Detokenized (010): ['bs##z', '=', 'in##p', '.', 'shape', '[', '-', '1', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "check_inds = check_inds [ 0 : ncheck ] \n"
Original    (009): ['check_inds', '=', 'check_inds', '[', '0', ':', 'ncheck', ']', '\\n']
Tokenized   (020): ['[CLS]', 'check', '_', 'ind', '##s', '=', 'check', '_', 'ind', '##s', '[', '0', ':', 'nc', '##he', '##ck', ']', '\\', 'n', '[SEP]']
Filtered   (018): ['check', '_', 'ind', '##s', '=', 'check', '_', 'ind', '##s', '[', '0', ':', 'nc', '##he', '##ck', ']', '\\', 'n']
Detokenized (009): ['check_ind##s', '=', 'check_ind##s', '[', '0', ':', 'nc##he##ck', ']', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "inpa = inp . get ( ) . reshape ( inp_lshape ) \n"
Original    (013): ['inpa', '=', 'inp', '.', 'get', '(', ')', '.', 'reshape', '(', 'inp_lshape', ')', '\\n']
Tokenized   (025): ['[CLS]', 'in', '##pa', '=', 'in', '##p', '.', 'get', '(', ')', '.', 'res', '##ha', '##pe', '(', 'in', '##p', '_', 'l', '##sha', '##pe', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['in', '##pa', '=', 'in', '##p', '.', 'get', '(', ')', '.', 'res', '##ha', '##pe', '(', 'in', '##p', '_', 'l', '##sha', '##pe', ')', '\\', 'n']
Detokenized (013): ['in##pa', '=', 'in##p', '.', 'get', '(', ')', '.', 'res##ha##pe', '(', 'in##p_l##sha##pe', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "outshape = ( inp_lshape [ 0 ] , \n"
Original    (009): ['outshape', '=', '(', 'inp_lshape', '[', '0', ']', ',', '\\n']
Tokenized   (019): ['[CLS]', 'outs', '##ha', '##pe', '=', '(', 'in', '##p', '_', 'l', '##sha', '##pe', '[', '0', ']', ',', '\\', 'n', '[SEP]']
Filtered   (017): ['outs', '##ha', '##pe', '=', '(', 'in', '##p', '_', 'l', '##sha', '##pe', '[', '0', ']', ',', '\\', 'n']
Detokenized (009): ['outs##ha##pe', '=', '(', 'in##p_l##sha##pe', '[', '0', ']', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "be . output_dim ( inp_lshape [ 2 ] , fshape [ 1 ] , padding , strides [ 1 ] , pooling = True ) , \n"
Original    (027): ['be', '.', 'output_dim', '(', 'inp_lshape', '[', '2', ']', ',', 'fshape', '[', '1', ']', ',', 'padding', ',', 'strides', '[', '1', ']', ',', 'pooling', '=', 'True', ')', ',', '\\n']
Tokenized   (041): ['[CLS]', 'be', '.', 'output', '_', 'dim', '(', 'in', '##p', '_', 'l', '##sha', '##pe', '[', '2', ']', ',', 'f', '##sha', '##pe', '[', '1', ']', ',', 'pad', '##ding', ',', 'strides', '[', '1', ']', ',', 'pool', '##ing', '=', 'true', ')', ',', '\\', 'n', '[SEP]']
Filtered   (039): ['be', '.', 'output', '_', 'dim', '(', 'in', '##p', '_', 'l', '##sha', '##pe', '[', '2', ']', ',', 'f', '##sha', '##pe', '[', '1', ']', ',', 'pad', '##ding', ',', 'strides', '[', '1', ']', ',', 'pool', '##ing', '=', 'true', ')', ',', '\\', 'n']
Detokenized (027): ['be', '.', 'output_dim', '(', 'in##p_l##sha##pe', '[', '2', ']', ',', 'f##sha##pe', '[', '1', ']', ',', 'pad##ding', ',', 'strides', '[', '1', ']', ',', 'pool##ing', '=', 'true', ')', ',', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "inp_lshape [ - 1 ] ) \n"
Original    (007): ['inp_lshape', '[', '-', '1', ']', ')', '\\n']
Tokenized   (015): ['[CLS]', 'in', '##p', '_', 'l', '##sha', '##pe', '[', '-', '1', ']', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['in', '##p', '_', 'l', '##sha', '##pe', '[', '-', '1', ']', ')', '\\', 'n']
Detokenized (007): ['in##p_l##sha##pe', '[', '-', '1', ']', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "inp_pad [ : , padding : - padding , padding : - padding , : ] = inpa [ : , 0 : , 0 : , : ] \n"
Original    (030): ['inp_pad', '[', ':', ',', 'padding', ':', '-', 'padding', ',', 'padding', ':', '-', 'padding', ',', ':', ']', '=', 'inpa', '[', ':', ',', '0', ':', ',', '0', ':', ',', ':', ']', '\\n']
Tokenized   (041): ['[CLS]', 'in', '##p', '_', 'pad', '[', ':', ',', 'pad', '##ding', ':', '-', 'pad', '##ding', ',', 'pad', '##ding', ':', '-', 'pad', '##ding', ',', ':', ']', '=', 'in', '##pa', '[', ':', ',', '0', ':', ',', '0', ':', ',', ':', ']', '\\', 'n', '[SEP]']
Filtered   (039): ['in', '##p', '_', 'pad', '[', ':', ',', 'pad', '##ding', ':', '-', 'pad', '##ding', ',', 'pad', '##ding', ':', '-', 'pad', '##ding', ',', ':', ']', '=', 'in', '##pa', '[', ':', ',', '0', ':', ',', '0', ':', ',', ':', ']', '\\', 'n']
Detokenized (030): ['in##p_pad', '[', ':', ',', 'pad##ding', ':', '-', 'pad##ding', ',', 'pad##ding', ':', '-', 'pad##ding', ',', ':', ']', '=', 'in##pa', '[', ':', ',', '0', ':', ',', '0', ':', ',', ':', ']', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 30, 768)
# Extracted words:  30
Sentence         : "out_exp [ indC , indh , indw , cnt ] = np . max ( inp_check ) \n"
Original    (018): ['out_exp', '[', 'indC', ',', 'indh', ',', 'indw', ',', 'cnt', ']', '=', 'np', '.', 'max', '(', 'inp_check', ')', '\\n']
Tokenized   (031): ['[CLS]', 'out', '_', 'ex', '##p', '[', 'ind', '##c', ',', 'ind', '##h', ',', 'ind', '##w', ',', 'cn', '##t', ']', '=', 'np', '.', 'max', '(', 'in', '##p', '_', 'check', ')', '\\', 'n', '[SEP]']
Filtered   (029): ['out', '_', 'ex', '##p', '[', 'ind', '##c', ',', 'ind', '##h', ',', 'ind', '##w', ',', 'cn', '##t', ']', '=', 'np', '.', 'max', '(', 'in', '##p', '_', 'check', ')', '\\', 'n']
Detokenized (018): ['out_ex##p', '[', 'ind##c', ',', 'ind##h', ',', 'ind##w', ',', 'cn##t', ']', '=', 'np', '.', 'max', '(', 'in##p_check', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "NervanaObject . be . bsz = batch_size \n"
Original    (008): ['NervanaObject', '.', 'be', '.', 'bsz', '=', 'batch_size', '\\n']
Tokenized   (018): ['[CLS]', 'ne', '##rva', '##na', '##ob', '##ject', '.', 'be', '.', 'bs', '##z', '=', 'batch', '_', 'size', '\\', 'n', '[SEP]']
Filtered   (016): ['ne', '##rva', '##na', '##ob', '##ject', '.', 'be', '.', 'bs', '##z', '=', 'batch', '_', 'size', '\\', 'n']
Detokenized (008): ['ne##rva##na##ob##ject', '.', 'be', '.', 'bs##z', '=', 'batch_size', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "inshape = ( nifm , in_sz , in_sz ) \n"
Original    (010): ['inshape', '=', '(', 'nifm', ',', 'in_sz', ',', 'in_sz', ')', '\\n']
Tokenized   (022): ['[CLS]', 'ins', '##ha', '##pe', '=', '(', 'ni', '##fm', ',', 'in', '_', 's', '##z', ',', 'in', '_', 's', '##z', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['ins', '##ha', '##pe', '=', '(', 'ni', '##fm', ',', 'in', '_', 's', '##z', ',', 'in', '_', 's', '##z', ')', '\\', 'n']
Detokenized (010): ['ins##ha##pe', '=', '(', 'ni##fm', ',', 'in_s##z', ',', 'in_s##z', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "src = "img/file-icon.jpg" , ** kw ) ] \n"
Original    (009): ['src', '=', '"img/file-icon.jpg"', ',', '**', 'kw', ')', ']', '\\n']
Tokenized   (024): ['[CLS]', 'sr', '##c', '=', '"', 'im', '##g', '/', 'file', '-', 'icon', '.', 'jp', '##g', '"', ',', '*', '*', 'kw', ')', ']', '\\', 'n', '[SEP]']
Filtered   (022): ['sr', '##c', '=', '"', 'im', '##g', '/', 'file', '-', 'icon', '.', 'jp', '##g', '"', ',', '*', '*', 'kw', ')', ']', '\\', 'n']
Detokenized (009): ['sr##c', '=', '"im##g/file-icon.jp##g"', ',', '**', 'kw', ')', ']', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "render = lambda r : r . div ( comp . render ( r , model = None ) , r . script ( component_to_update = + self . comp_id , \n"
Original    (032): ['render', '=', 'lambda', 'r', ':', 'r', '.', 'div', '(', 'comp', '.', 'render', '(', 'r', ',', 'model', '=', 'None', ')', ',', 'r', '.', 'script', '(', 'component_to_update', '=', '+', 'self', '.', 'comp_id', ',', '\\n']
Tokenized   (044): ['[CLS]', 'render', '=', 'lambda', 'r', ':', 'r', '.', 'di', '##v', '(', 'com', '##p', '.', 'render', '(', 'r', ',', 'model', '=', 'none', ')', ',', 'r', '.', 'script', '(', 'component', '_', 'to', '_', 'update', '=', '+', 'self', '.', 'com', '##p', '_', 'id', ',', '\\', 'n', '[SEP]']
Filtered   (042): ['render', '=', 'lambda', 'r', ':', 'r', '.', 'di', '##v', '(', 'com', '##p', '.', 'render', '(', 'r', ',', 'model', '=', 'none', ')', ',', 'r', '.', 'script', '(', 'component', '_', 'to', '_', 'update', '=', '+', 'self', '.', 'com', '##p', '_', 'id', ',', '\\', 'n']
Detokenized (032): ['render', '=', 'lambda', 'r', ':', 'r', '.', 'di##v', '(', 'com##p', '.', 'render', '(', 'r', ',', 'model', '=', 'none', ')', ',', 'r', '.', 'script', '(', 'component_to_update', '=', '+', 'self', '.', 'com##p_id', ',', '\\n']
Counter: 42
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "ajax . py2js ( self . crop_height ( ) ) \n"
Original    (011): ['ajax', '.', 'py2js', '(', 'self', '.', 'crop_height', '(', ')', ')', '\\n']
Tokenized   (019): ['[CLS]', 'ajax', '.', 'p', '##y', '##2', '##js', '(', 'self', '.', 'crop', '_', 'height', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['ajax', '.', 'p', '##y', '##2', '##js', '(', 'self', '.', 'crop', '_', 'height', '(', ')', ')', '\\', 'n']
Detokenized (011): ['ajax', '.', 'p##y##2##js', '(', 'self', '.', 'crop_height', '(', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "local_handler = getattr ( self , , None ) \n"
Original    (010): ['local_handler', '=', 'getattr', '(', 'self', ',', ',', 'None', ')', '\\n']
Tokenized   (017): ['[CLS]', 'local', '_', 'handler', '=', 'get', '##att', '##r', '(', 'self', ',', ',', 'none', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['local', '_', 'handler', '=', 'get', '##att', '##r', '(', 'self', ',', ',', 'none', ')', '\\', 'n']
Detokenized (010): ['local_handler', '=', 'get##att##r', '(', 'self', ',', ',', 'none', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "genie2 . client . wrapper . RetryPolicy ( \n"
Original    (009): ['genie2', '.', 'client', '.', 'wrapper', '.', 'RetryPolicy', '(', '\\n']
Tokenized   (018): ['[CLS]', 'genie', '##2', '.', 'client', '.', 'wrap', '##per', '.', 're', '##try', '##pol', '##ic', '##y', '(', '\\', 'n', '[SEP]']
Filtered   (016): ['genie', '##2', '.', 'client', '.', 'wrap', '##per', '.', 're', '##try', '##pol', '##ic', '##y', '(', '\\', 'n']
Detokenized (009): ['genie##2', '.', 'client', '.', 'wrap##per', '.', 're##try##pol##ic##y', '(', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "tries = 8 , none_on_404 = True , no_retry_http_codes = range ( 400 , 500 ) ) \n"
Original    (018): ['tries', '=', '8', ',', 'none_on_404', '=', 'True', ',', 'no_retry_http_codes', '=', 'range', '(', '400', ',', '500', ')', ')', '\\n']
Tokenized   (032): ['[CLS]', 'tries', '=', '8', ',', 'none', '_', 'on', '_', '404', '=', 'true', ',', 'no', '_', 're', '##try', '_', 'http', '_', 'codes', '=', 'range', '(', '400', ',', '500', ')', ')', '\\', 'n', '[SEP]']
Filtered   (030): ['tries', '=', '8', ',', 'none', '_', 'on', '_', '404', '=', 'true', ',', 'no', '_', 're', '##try', '_', 'http', '_', 'codes', '=', 'range', '(', '400', ',', '500', ')', ')', '\\', 'n']
Detokenized (018): ['tries', '=', '8', ',', 'none_on_404', '=', 'true', ',', 'no_re##try_http_codes', '=', 'range', '(', '400', ',', '500', ')', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "tagging . add_argument ( , , dest = , action = conf_action ( context . ami ) , help = \n"
Original    (021): ['tagging', '.', 'add_argument', '(', ',', ',', 'dest', '=', ',', 'action', '=', 'conf_action', '(', 'context', '.', 'ami', ')', ',', 'help', '=', '\\n']
Tokenized   (031): ['[CLS]', 'tag', '##ging', '.', 'add', '_', 'argument', '(', ',', ',', 'des', '##t', '=', ',', 'action', '=', 'con', '##f', '_', 'action', '(', 'context', '.', 'ami', ')', ',', 'help', '=', '\\', 'n', '[SEP]']
Filtered   (029): ['tag', '##ging', '.', 'add', '_', 'argument', '(', ',', ',', 'des', '##t', '=', ',', 'action', '=', 'con', '##f', '_', 'action', '(', 'context', '.', 'ami', ')', ',', 'help', '=', '\\', 'n']
Detokenized (021): ['tag##ging', '.', 'add_argument', '(', ',', ',', 'des##t', '=', ',', 'action', '=', 'con##f_action', '(', 'context', '.', 'ami', ')', ',', 'help', '=', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "BASE_URL = . format ( FQDN ) \n"
Original    (008): ['BASE_URL', '=', '.', 'format', '(', 'FQDN', ')', '\\n']
Tokenized   (017): ['[CLS]', 'base', '_', 'ur', '##l', '=', '.', 'format', '(', 'f', '##q', '##d', '##n', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['base', '_', 'ur', '##l', '=', '.', 'format', '(', 'f', '##q', '##d', '##n', ')', '\\', 'n']
Detokenized (008): ['base_ur##l', '=', '.', 'format', '(', 'f##q##d##n', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test" , config ElasticSearchServiceItem ( region = "us-west-2" , account = "TEST_ACCOUNT" , name = "es_test_2" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_3" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_4" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_5" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_6" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_7" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_8" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_9" , config ] \n"
Original    (137): ['ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-west-2"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_2"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_3"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_4"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_5"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_6"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_7"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_8"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_9"', ',', 'config', ']', '\\n']
Tokenized   (354): ['[CLS]', 'elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '"', ',', 'con', '##fi', '##g', 'elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'us', '-', 'west', '-', '2', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '2', '"', ',', 'con', '##fi', '##g', 'elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '3', '"', ',', 'con', '##fi', '##g', 'elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '4', '"', ',', 'con', '##fi', '##g', 'elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '5', '"', ',', 'con', '##fi', '##g', 'elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '6', '"', ',', 'con', '##fi', '##g', 'elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '7', '"', ',', 'con', '##fi', '##g', 'elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '8', '"', ',', 'con', '##fi', '##g', 'elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '9', '"', ',', 'con', '##fi', '##g', ']', '\\', 'n', '[SEP]']
Filtered   (352): ['elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '"', ',', 'con', '##fi', '##g', 'elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'us', '-', 'west', '-', '2', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '2', '"', ',', 'con', '##fi', '##g', 'elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '3', '"', ',', 'con', '##fi', '##g', 'elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '4', '"', ',', 'con', '##fi', '##g', 'elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '5', '"', ',', 'con', '##fi', '##g', 'elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '6', '"', ',', 'con', '##fi', '##g', 'elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '7', '"', ',', 'con', '##fi', '##g', 'elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '8', '"', ',', 'con', '##fi', '##g', 'elastic', '##sea', '##rch', '##ser', '##vic', '##eit', '##em', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'test', '_', 'account', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '9', '"', ',', 'con', '##fi', '##g', ']', '\\', 'n']
Detokenized (137): ['elastic##sea##rch##ser##vic##eit##em', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"test_account"', ',', 'name', '=', '"es_test"', ',', 'con##fi##g', 'elastic##sea##rch##ser##vic##eit##em', '(', 'region', '=', '"us-west-2"', ',', 'account', '=', '"test_account"', ',', 'name', '=', '"es_test_2"', ',', 'con##fi##g', 'elastic##sea##rch##ser##vic##eit##em', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"test_account"', ',', 'name', '=', '"es_test_3"', ',', 'con##fi##g', 'elastic##sea##rch##ser##vic##eit##em', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"test_account"', ',', 'name', '=', '"es_test_4"', ',', 'con##fi##g', 'elastic##sea##rch##ser##vic##eit##em', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"test_account"', ',', 'name', '=', '"es_test_5"', ',', 'con##fi##g', 'elastic##sea##rch##ser##vic##eit##em', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"test_account"', ',', 'name', '=', '"es_test_6"', ',', 'con##fi##g', 'elastic##sea##rch##ser##vic##eit##em', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"test_account"', ',', 'name', '=', '"es_test_7"', ',', 'con##fi##g', 'elastic##sea##rch##ser##vic##eit##em', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"test_account"', ',', 'name', '=', '"es_test_8"', ',', 'con##fi##g', 'elastic##sea##rch##ser##vic##eit##em', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"test_account"', ',', 'name', '=', '"es_test_9"', ',', 'con##fi##g', ']', '\\n']
Counter: 352
===================================================================
Hidden states:  (13, 137, 768)
# Extracted words:  137
Sentence         : "test_account . role_name = "TEST_ACCOUNT" \n"
Original    (006): ['test_account', '.', 'role_name', '=', '"TEST_ACCOUNT"', '\\n']
Tokenized   (017): ['[CLS]', 'test', '_', 'account', '.', 'role', '_', 'name', '=', '"', 'test', '_', 'account', '"', '\\', 'n', '[SEP]']
Filtered   (015): ['test', '_', 'account', '.', 'role', '_', 'name', '=', '"', 'test', '_', 'account', '"', '\\', 'n']
Detokenized (006): ['test_account', '.', 'role_name', '=', '"test_account"', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "all_clusters . extend ( response [ ] [ if response [ ] [ ] [ ] ~~~ marker = response [ ] [ ] [ ~~ else : \n"
Original    (029): ['all_clusters', '.', 'extend', '(', 'response', '[', ']', '[', 'if', 'response', '[', ']', '[', ']', '[', ']', '~~~', 'marker', '=', 'response', '[', ']', '[', ']', '[', '~~', 'else', ':', '\\n']
Tokenized   (037): ['[CLS]', 'all', '_', 'clusters', '.', 'extend', '(', 'response', '[', ']', '[', 'if', 'response', '[', ']', '[', ']', '[', ']', '~', '~', '~', 'marker', '=', 'response', '[', ']', '[', ']', '[', '~', '~', 'else', ':', '\\', 'n', '[SEP]']
Filtered   (035): ['all', '_', 'clusters', '.', 'extend', '(', 'response', '[', ']', '[', 'if', 'response', '[', ']', '[', ']', '[', ']', '~', '~', '~', 'marker', '=', 'response', '[', ']', '[', ']', '[', '~', '~', 'else', ':', '\\', 'n']
Detokenized (029): ['all_clusters', '.', 'extend', '(', 'response', '[', ']', '[', 'if', 'response', '[', ']', '[', ']', '[', ']', '~~~', 'marker', '=', 'response', '[', ']', '[', ']', '[', '~~', 'else', ':', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "_container_child_objects = ( , ) \n"
Original    (006): ['_container_child_objects', '=', '(', ',', ')', '\\n']
Tokenized   (014): ['[CLS]', '_', 'container', '_', 'child', '_', 'objects', '=', '(', ',', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['_', 'container', '_', 'child', '_', 'objects', '=', '(', ',', ')', '\\', 'n']
Detokenized (006): ['_container_child_objects', '=', '(', ',', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "_recommended_attrs = ( ( ( , np . ndarray , 1 , np . dtype ( ) ) , \n"
Original    (020): ['_recommended_attrs', '=', '(', '(', '(', ',', 'np', '.', 'ndarray', ',', '1', ',', 'np', '.', 'dtype', '(', ')', ')', ',', '\\n']
Tokenized   (031): ['[CLS]', '_', 'recommended', '_', 'at', '##tr', '##s', '=', '(', '(', '(', ',', 'np', '.', 'n', '##dar', '##ray', ',', '1', ',', 'np', '.', 'dt', '##ype', '(', ')', ')', ',', '\\', 'n', '[SEP]']
Filtered   (029): ['_', 'recommended', '_', 'at', '##tr', '##s', '=', '(', '(', '(', ',', 'np', '.', 'n', '##dar', '##ray', ',', '1', ',', 'np', '.', 'dt', '##ype', '(', ')', ')', ',', '\\', 'n']
Detokenized (020): ['_recommended_at##tr##s', '=', '(', '(', '(', ',', 'np', '.', 'n##dar##ray', ',', '1', ',', 'np', '.', 'dt##ype', '(', ')', ')', ',', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "errstr = . format ( errno , pszMsgBuffer . value ) \n"
Original    (012): ['errstr', '=', '.', 'format', '(', 'errno', ',', 'pszMsgBuffer', '.', 'value', ')', '\\n']
Tokenized   (023): ['[CLS]', 'er', '##rst', '##r', '=', '.', 'format', '(', 'er', '##rno', ',', 'ps', '##z', '##ms', '##gb', '##uf', '##fer', '.', 'value', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['er', '##rst', '##r', '=', '.', 'format', '(', 'er', '##rno', ',', 'ps', '##z', '##ms', '##gb', '##uf', '##fer', '.', 'value', ')', '\\', 'n']
Detokenized (012): ['er##rst##r', '=', '.', 'format', '(', 'er##rno', ',', 'ps##z##ms##gb##uf##fer', '.', 'value', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "supported_objects = [ Segment , AnalogSignal , EventArray , SpikeTrain ] \n"
Original    (012): ['supported_objects', '=', '[', 'Segment', ',', 'AnalogSignal', ',', 'EventArray', ',', 'SpikeTrain', ']', '\\n']
Tokenized   (023): ['[CLS]', 'supported', '_', 'objects', '=', '[', 'segment', ',', 'analog', '##si', '##gna', '##l', ',', 'event', '##ar', '##ray', ',', 'spike', '##train', ']', '\\', 'n', '[SEP]']
Filtered   (021): ['supported', '_', 'objects', '=', '[', 'segment', ',', 'analog', '##si', '##gna', '##l', ',', 'event', '##ar', '##ray', ',', 'spike', '##train', ']', '\\', 'n']
Detokenized (012): ['supported_objects', '=', '[', 'segment', ',', 'analog##si##gna##l', ',', 'event##ar##ray', ',', 'spike##train', ']', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "readable_objects = [ Segment ] \n"
Original    (006): ['readable_objects', '=', '[', 'Segment', ']', '\\n']
Tokenized   (012): ['[CLS]', 'read', '##able', '_', 'objects', '=', '[', 'segment', ']', '\\', 'n', '[SEP]']
Filtered   (010): ['read', '##able', '_', 'objects', '=', '[', 'segment', ']', '\\', 'n']
Detokenized (006): ['read##able_objects', '=', '[', 'segment', ']', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "read_params = { Segment : [ ] } \n"
Original    (009): ['read_params', '=', '{', 'Segment', ':', '[', ']', '}', '\\n']
Tokenized   (015): ['[CLS]', 'read', '_', 'para', '##ms', '=', '{', 'segment', ':', '[', ']', '}', '\\', 'n', '[SEP]']
Filtered   (013): ['read', '_', 'para', '##ms', '=', '{', 'segment', ':', '[', ']', '}', '\\', 'n']
Detokenized (009): ['read_para##ms', '=', '{', 'segment', ':', '[', ']', '}', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "labels . append ( str ( pData . value ) ) \n"
Original    (012): ['labels', '.', 'append', '(', 'str', '(', 'pData', '.', 'value', ')', ')', '\\n']
Tokenized   (018): ['[CLS]', 'labels', '.', 'app', '##end', '(', 'st', '##r', '(', 'pd', '##ata', '.', 'value', ')', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['labels', '.', 'app', '##end', '(', 'st', '##r', '(', 'pd', '##ata', '.', 'value', ')', ')', '\\', 'n']
Detokenized (012): ['labels', '.', 'app##end', '(', 'st##r', '(', 'pd##ata', '.', 'value', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "ea . labels = np . array ( labels , dtype = ) \n"
Original    (014): ['ea', '.', 'labels', '=', 'np', '.', 'array', '(', 'labels', ',', 'dtype', '=', ')', '\\n']
Tokenized   (018): ['[CLS]', 'ea', '.', 'labels', '=', 'np', '.', 'array', '(', 'labels', ',', 'dt', '##ype', '=', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['ea', '.', 'labels', '=', 'np', '.', 'array', '(', 'labels', ',', 'dt', '##ype', '=', ')', '\\', 'n']
Detokenized (014): ['ea', '.', 'labels', '=', 'np', '.', 'array', '(', 'labels', ',', 'dt##ype', '=', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "dwStopIndex , ctypes . byref ( pdwContCount ) , pData [ total_read : ] . ctypes total_read += pdwContCount . value \n"
Original    (022): ['dwStopIndex', ',', 'ctypes', '.', 'byref', '(', 'pdwContCount', ')', ',', 'pData', '[', 'total_read', ':', ']', '.', 'ctypes', 'total_read', '+=', 'pdwContCount', '.', 'value', '\\n']
Tokenized   (049): ['[CLS]', 'd', '##ws', '##top', '##ind', '##ex', ',', 'ct', '##ype', '##s', '.', 'by', '##re', '##f', '(', 'pd', '##wc', '##ont', '##co', '##unt', ')', ',', 'pd', '##ata', '[', 'total', '_', 'read', ':', ']', '.', 'ct', '##ype', '##s', 'total', '_', 'read', '+', '=', 'pd', '##wc', '##ont', '##co', '##unt', '.', 'value', '\\', 'n', '[SEP]']
Filtered   (047): ['d', '##ws', '##top', '##ind', '##ex', ',', 'ct', '##ype', '##s', '.', 'by', '##re', '##f', '(', 'pd', '##wc', '##ont', '##co', '##unt', ')', ',', 'pd', '##ata', '[', 'total', '_', 'read', ':', ']', '.', 'ct', '##ype', '##s', 'total', '_', 'read', '+', '=', 'pd', '##wc', '##ont', '##co', '##unt', '.', 'value', '\\', 'n']
Detokenized (022): ['d##ws##top##ind##ex', ',', 'ct##ype##s', '.', 'by##re##f', '(', 'pd##wc##ont##co##unt', ')', ',', 'pd##ata', '[', 'total_read', ':', ']', '.', 'ct##ype##s', 'total_read', '+=', 'pd##wc##ont##co##unt', '.', 'value', '\\n']
Counter: 47
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "anaSig . annotate ( probe_info = str ( pAnalogInfo . szProbeInfo ) ) \n"
Original    (014): ['anaSig', '.', 'annotate', '(', 'probe_info', '=', 'str', '(', 'pAnalogInfo', '.', 'szProbeInfo', ')', ')', '\\n']
Tokenized   (032): ['[CLS]', 'ana', '##si', '##g', '.', 'ann', '##ota', '##te', '(', 'probe', '_', 'info', '=', 'st', '##r', '(', 'pan', '##alo', '##gin', '##fo', '.', 's', '##z', '##pro', '##bei', '##n', '##fo', ')', ')', '\\', 'n', '[SEP]']
Filtered   (030): ['ana', '##si', '##g', '.', 'ann', '##ota', '##te', '(', 'probe', '_', 'info', '=', 'st', '##r', '(', 'pan', '##alo', '##gin', '##fo', '.', 's', '##z', '##pro', '##bei', '##n', '##fo', ')', ')', '\\', 'n']
Detokenized (014): ['ana##si##g', '.', 'ann##ota##te', '(', 'probe_info', '=', 'st##r', '(', 'pan##alo##gin##fo', '.', 's##z##pro##bei##n##fo', ')', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "pData = np . zeros ( ( dwDataBufferSize ) , dtype = ) \n"
Original    (014): ['pData', '=', 'np', '.', 'zeros', '(', '(', 'dwDataBufferSize', ')', ',', 'dtype', '=', ')', '\\n']
Tokenized   (026): ['[CLS]', 'pd', '##ata', '=', 'np', '.', 'zero', '##s', '(', '(', 'd', '##wd', '##ata', '##bu', '##ffer', '##si', '##ze', ')', ',', 'dt', '##ype', '=', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['pd', '##ata', '=', 'np', '.', 'zero', '##s', '(', '(', 'd', '##wd', '##ata', '##bu', '##ffer', '##si', '##ze', ')', ',', 'dt', '##ype', '=', ')', '\\', 'n']
Detokenized (014): ['pd##ata', '=', 'np', '.', 'zero##s', '(', '(', 'd##wd##ata##bu##ffer##si##ze', ')', ',', 'dt##ype', '=', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "waveforms = pq . Quantity ( waveforms , units = str ( pdwSegmentInfo left_sweep = nsample / 2. / float ( pdwSegmentInfo . dSampleRate ) * pq sampling_rate = float ( pdwSegmentInfo . dSampleRate ) * pq . Hz , \n"
Original    (041): ['waveforms', '=', 'pq', '.', 'Quantity', '(', 'waveforms', ',', 'units', '=', 'str', '(', 'pdwSegmentInfo', 'left_sweep', '=', 'nsample', '/', '2.', '/', 'float', '(', 'pdwSegmentInfo', '.', 'dSampleRate', ')', '*', 'pq', 'sampling_rate', '=', 'float', '(', 'pdwSegmentInfo', '.', 'dSampleRate', ')', '*', 'pq', '.', 'Hz', ',', '\\n']
Tokenized   (077): ['[CLS]', 'wave', '##forms', '=', 'p', '##q', '.', 'quantity', '(', 'wave', '##forms', ',', 'units', '=', 'st', '##r', '(', 'pd', '##ws', '##eg', '##ment', '##in', '##fo', 'left', '_', 'sweep', '=', 'nsa', '##mple', '/', '2', '.', '/', 'float', '(', 'pd', '##ws', '##eg', '##ment', '##in', '##fo', '.', 'ds', '##amp', '##ler', '##ate', ')', '*', 'p', '##q', 'sampling', '_', 'rate', '=', 'float', '(', 'pd', '##ws', '##eg', '##ment', '##in', '##fo', '.', 'ds', '##amp', '##ler', '##ate', ')', '*', 'p', '##q', '.', 'hz', ',', '\\', 'n', '[SEP]']
Filtered   (075): ['wave', '##forms', '=', 'p', '##q', '.', 'quantity', '(', 'wave', '##forms', ',', 'units', '=', 'st', '##r', '(', 'pd', '##ws', '##eg', '##ment', '##in', '##fo', 'left', '_', 'sweep', '=', 'nsa', '##mple', '/', '2', '.', '/', 'float', '(', 'pd', '##ws', '##eg', '##ment', '##in', '##fo', '.', 'ds', '##amp', '##ler', '##ate', ')', '*', 'p', '##q', 'sampling', '_', 'rate', '=', 'float', '(', 'pd', '##ws', '##eg', '##ment', '##in', '##fo', '.', 'ds', '##amp', '##ler', '##ate', ')', '*', 'p', '##q', '.', 'hz', ',', '\\', 'n']
Detokenized (041): ['wave##forms', '=', 'p##q', '.', 'quantity', '(', 'wave##forms', ',', 'units', '=', 'st##r', '(', 'pd##ws##eg##ment##in##fo', 'left_sweep', '=', 'nsa##mple', '/', '2.', '/', 'float', '(', 'pd##ws##eg##ment##in##fo', '.', 'ds##amp##ler##ate', ')', '*', 'p##q', 'sampling_rate', '=', 'float', '(', 'pd##ws##eg##ment##in##fo', '.', 'ds##amp##ler##ate', ')', '*', 'p##q', '.', 'hz', ',', '\\n']
Counter: 75
===================================================================
Hidden states:  (13, 41, 768)
# Extracted words:  41
Sentence         : "ctypes . byref ( pNeuralInfo ) , ctypes . sizeof ( pNeuralInfo ) ) \n"
Original    (015): ['ctypes', '.', 'byref', '(', 'pNeuralInfo', ')', ',', 'ctypes', '.', 'sizeof', '(', 'pNeuralInfo', ')', ')', '\\n']
Tokenized   (033): ['[CLS]', 'ct', '##ype', '##s', '.', 'by', '##re', '##f', '(', 'p', '##ne', '##ural', '##in', '##fo', ')', ',', 'ct', '##ype', '##s', '.', 'size', '##of', '(', 'p', '##ne', '##ural', '##in', '##fo', ')', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['ct', '##ype', '##s', '.', 'by', '##re', '##f', '(', 'p', '##ne', '##ural', '##in', '##fo', ')', ',', 'ct', '##ype', '##s', '.', 'size', '##of', '(', 'p', '##ne', '##ural', '##in', '##fo', ')', ')', '\\', 'n']
Detokenized (015): ['ct##ype##s', '.', 'by##re##f', '(', 'p##ne##ural##in##fo', ')', ',', 'ct##ype##s', '.', 'size##of', '(', 'p##ne##ural##in##fo', ')', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "neuroshare . ns_GetNeuralData ( hFile , dwEntityID , dwStartIndex , \n"
Original    (011): ['neuroshare', '.', 'ns_GetNeuralData', '(', 'hFile', ',', 'dwEntityID', ',', 'dwStartIndex', ',', '\\n']
Tokenized   (034): ['[CLS]', 'ne', '##uro', '##sha', '##re', '.', 'ns', '_', 'get', '##ne', '##ural', '##da', '##ta', '(', 'h', '##fi', '##le', ',', 'd', '##wen', '##ti', '##ty', '##id', ',', 'd', '##ws', '##tar', '##tin', '##de', '##x', ',', '\\', 'n', '[SEP]']
Filtered   (032): ['ne', '##uro', '##sha', '##re', '.', 'ns', '_', 'get', '##ne', '##ural', '##da', '##ta', '(', 'h', '##fi', '##le', ',', 'd', '##wen', '##ti', '##ty', '##id', ',', 'd', '##ws', '##tar', '##tin', '##de', '##x', ',', '\\', 'n']
Detokenized (011): ['ne##uro##sha##re', '.', 'ns_get##ne##ural##da##ta', '(', 'h##fi##le', ',', 'd##wen##ti##ty##id', ',', 'd##ws##tar##tin##de##x', ',', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "dwIndexCount , pData . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) ) \n"
Original    (019): ['dwIndexCount', ',', 'pData', '.', 'ctypes', '.', 'data_as', '(', 'ctypes', '.', 'POINTER', '(', 'ctypes', '.', 'c_double', ')', ')', ')', '\\n']
Tokenized   (037): ['[CLS]', 'd', '##wind', '##ex', '##co', '##unt', ',', 'pd', '##ata', '.', 'ct', '##ype', '##s', '.', 'data', '_', 'as', '(', 'ct', '##ype', '##s', '.', 'pointer', '(', 'ct', '##ype', '##s', '.', 'c', '_', 'double', ')', ')', ')', '\\', 'n', '[SEP]']
Filtered   (035): ['d', '##wind', '##ex', '##co', '##unt', ',', 'pd', '##ata', '.', 'ct', '##ype', '##s', '.', 'data', '_', 'as', '(', 'ct', '##ype', '##s', '.', 'pointer', '(', 'ct', '##ype', '##s', '.', 'c', '_', 'double', ')', ')', ')', '\\', 'n']
Detokenized (019): ['d##wind##ex##co##unt', ',', 'pd##ata', '.', 'ct##ype##s', '.', 'data_as', '(', 'ct##ype##s', '.', 'pointer', '(', 'ct##ype##s', '.', 'c_double', ')', ')', ')', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "times = pData * pq . s \n"
Original    (008): ['times', '=', 'pData', '*', 'pq', '.', 's', '\\n']
Tokenized   (013): ['[CLS]', 'times', '=', 'pd', '##ata', '*', 'p', '##q', '.', 's', '\\', 'n', '[SEP]']
Filtered   (011): ['times', '=', 'pd', '##ata', '*', 'p', '##q', '.', 's', '\\', 'n']
Detokenized (008): ['times', '=', 'pd##ata', '*', 'p##q', '.', 's', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "clone_object , TEST_ANNOTATIONS ) \n"
Original    (005): ['clone_object', ',', 'TEST_ANNOTATIONS', ')', '\\n']
Tokenized   (014): ['[CLS]', 'clone', '_', 'object', ',', 'test', '_', 'ann', '##ota', '##tions', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['clone', '_', 'object', ',', 'test', '_', 'ann', '##ota', '##tions', ')', '\\', 'n']
Detokenized (005): ['clone_object', ',', 'test_ann##ota##tions', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "range ( len ( TEST_ANNOTATIONS ) ) ] ) \n"
Original    (010): ['range', '(', 'len', '(', 'TEST_ANNOTATIONS', ')', ')', ']', ')', '\\n']
Tokenized   (017): ['[CLS]', 'range', '(', 'len', '(', 'test', '_', 'ann', '##ota', '##tions', ')', ')', ']', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['range', '(', 'len', '(', 'test', '_', 'ann', '##ota', '##tions', ')', ')', ']', ')', '\\', 'n']
Detokenized (010): ['range', '(', 'len', '(', 'test_ann##ota##tions', ')', ')', ']', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "file_datetime = get_fake_value ( , datetime , seed = 0 ) \n"
Original    (012): ['file_datetime', '=', 'get_fake_value', '(', ',', 'datetime', ',', 'seed', '=', '0', ')', '\\n']
Tokenized   (023): ['[CLS]', 'file', '_', 'date', '##time', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'date', '##time', ',', 'seed', '=', '0', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['file', '_', 'date', '##time', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'date', '##time', ',', 'seed', '=', '0', ')', '\\', 'n']
Detokenized (012): ['file_date##time', '=', 'get_fake_value', '(', ',', 'date##time', ',', 'seed', '=', '0', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "file_origin = get_fake_value ( , str ) \n"
Original    (008): ['file_origin', '=', 'get_fake_value', '(', ',', 'str', ')', '\\n']
Tokenized   (018): ['[CLS]', 'file', '_', 'origin', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'st', '##r', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['file', '_', 'origin', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'st', '##r', ')', '\\', 'n']
Detokenized (008): ['file_origin', '=', 'get_fake_value', '(', ',', 'st##r', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "attrs1 = { : file_datetime , \n"
Original    (007): ['attrs1', '=', '{', ':', 'file_datetime', ',', '\\n']
Tokenized   (016): ['[CLS]', 'at', '##tr', '##s', '##1', '=', '{', ':', 'file', '_', 'date', '##time', ',', '\\', 'n', '[SEP]']
Filtered   (014): ['at', '##tr', '##s', '##1', '=', '{', ':', 'file', '_', 'date', '##time', ',', '\\', 'n']
Detokenized (007): ['at##tr##s##1', '=', '{', ':', 'file_date##time', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "res21 = get_fake_values ( Segment , annotate = True , seed = 0 ) \n"
Original    (015): ['res21', '=', 'get_fake_values', '(', 'Segment', ',', 'annotate', '=', 'True', ',', 'seed', '=', '0', ')', '\\n']
Tokenized   (025): ['[CLS]', 'res', '##21', '=', 'get', '_', 'fake', '_', 'values', '(', 'segment', ',', 'ann', '##ota', '##te', '=', 'true', ',', 'seed', '=', '0', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['res', '##21', '=', 'get', '_', 'fake', '_', 'values', '(', 'segment', ',', 'ann', '##ota', '##te', '=', 'true', ',', 'seed', '=', '0', ')', '\\', 'n']
Detokenized (015): ['res##21', '=', 'get_fake_values', '(', 'segment', ',', 'ann##ota##te', '=', 'true', ',', 'seed', '=', '0', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "res22 = get_fake_values ( , annotate = True , seed = 0 ) \n"
Original    (014): ['res22', '=', 'get_fake_values', '(', ',', 'annotate', '=', 'True', ',', 'seed', '=', '0', ')', '\\n']
Tokenized   (024): ['[CLS]', 'res', '##22', '=', 'get', '_', 'fake', '_', 'values', '(', ',', 'ann', '##ota', '##te', '=', 'true', ',', 'seed', '=', '0', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['res', '##22', '=', 'get', '_', 'fake', '_', 'values', '(', ',', 'ann', '##ota', '##te', '=', 'true', ',', 'seed', '=', '0', ')', '\\', 'n']
Detokenized (014): ['res##22', '=', 'get_fake_values', '(', ',', 'ann##ota##te', '=', 'true', ',', 'seed', '=', '0', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "targ0 = get_fake_value ( , datetime , seed = seed + 0 ) \n"
Original    (014): ['targ0', '=', 'get_fake_value', '(', ',', 'datetime', ',', 'seed', '=', 'seed', '+', '0', ')', '\\n']
Tokenized   (024): ['[CLS]', 'tar', '##g', '##0', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'date', '##time', ',', 'seed', '=', 'seed', '+', '0', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['tar', '##g', '##0', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'date', '##time', ',', 'seed', '=', 'seed', '+', '0', ')', '\\', 'n']
Detokenized (014): ['tar##g##0', '=', 'get_fake_value', '(', ',', 'date##time', ',', 'seed', '=', 'seed', '+', '0', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "targ4 = get_fake_value ( , str , \n"
Original    (008): ['targ4', '=', 'get_fake_value', '(', ',', 'str', ',', '\\n']
Tokenized   (018): ['[CLS]', 'tar', '##g', '##4', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'st', '##r', ',', '\\', 'n', '[SEP]']
Filtered   (016): ['tar', '##g', '##4', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'st', '##r', ',', '\\', 'n']
Detokenized (008): ['tar##g##4', '=', 'get_fake_value', '(', ',', 'st##r', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "seed = seed + 4 , obj = Segment ) \n"
Original    (011): ['seed', '=', 'seed', '+', '4', ',', 'obj', '=', 'Segment', ')', '\\n']
Tokenized   (015): ['[CLS]', 'seed', '=', 'seed', '+', '4', ',', 'ob', '##j', '=', 'segment', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['seed', '=', 'seed', '+', '4', ',', 'ob', '##j', '=', 'segment', ')', '\\', 'n']
Detokenized (011): ['seed', '=', 'seed', '+', '4', ',', 'ob##j', '=', 'segment', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "childobjs = ( , , \n"
Original    (006): ['childobjs', '=', '(', ',', ',', '\\n']
Tokenized   (011): ['[CLS]', 'child', '##ob', '##js', '=', '(', ',', ',', '\\', 'n', '[SEP]']
Filtered   (009): ['child', '##ob', '##js', '=', '(', ',', ',', '\\', 'n']
Detokenized (006): ['child##ob##js', '=', '(', ',', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "children = ( self . sigs1a + self . sigarrs1a + \n"
Original    (012): ['children', '=', '(', 'self', '.', 'sigs1a', '+', 'self', '.', 'sigarrs1a', '+', '\\n']
Tokenized   (022): ['[CLS]', 'children', '=', '(', 'self', '.', 'si', '##gs', '##1', '##a', '+', 'self', '.', 'si', '##gar', '##rs', '##1', '##a', '+', '\\', 'n', '[SEP]']
Filtered   (020): ['children', '=', '(', 'self', '.', 'si', '##gs', '##1', '##a', '+', 'self', '.', 'si', '##gar', '##rs', '##1', '##a', '+', '\\', 'n']
Detokenized (012): ['children', '=', '(', 'self', '.', 'si##gs##1##a', '+', 'self', '.', 'si##gar##rs##1##a', '+', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : ""analogsignals" : self . nchildren ** 2 , \n"
Original    (009): ['"analogsignals"', ':', 'self', '.', 'nchildren', '**', '2', ',', '\\n']
Tokenized   (021): ['[CLS]', '"', 'analog', '##si', '##gna', '##ls', '"', ':', 'self', '.', 'nc', '##hil', '##dre', '##n', '*', '*', '2', ',', '\\', 'n', '[SEP]']
Filtered   (019): ['"', 'analog', '##si', '##gna', '##ls', '"', ':', 'self', '.', 'nc', '##hil', '##dre', '##n', '*', '*', '2', ',', '\\', 'n']
Detokenized (009): ['"analog##si##gna##ls"', ':', 'self', '.', 'nc##hil##dre##n', '**', '2', ',', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ""epocharrays" : self . nchildren , "eventarrays" : self . nchildren , \n"
Original    (013): ['"epocharrays"', ':', 'self', '.', 'nchildren', ',', '"eventarrays"', ':', 'self', '.', 'nchildren', ',', '\\n']
Tokenized   (032): ['[CLS]', '"', 'epoch', '##ar', '##ray', '##s', '"', ':', 'self', '.', 'nc', '##hil', '##dre', '##n', ',', '"', 'event', '##ar', '##ray', '##s', '"', ':', 'self', '.', 'nc', '##hil', '##dre', '##n', ',', '\\', 'n', '[SEP]']
Filtered   (030): ['"', 'epoch', '##ar', '##ray', '##s', '"', ':', 'self', '.', 'nc', '##hil', '##dre', '##n', ',', '"', 'event', '##ar', '##ray', '##s', '"', ':', 'self', '.', 'nc', '##hil', '##dre', '##n', ',', '\\', 'n']
Detokenized (013): ['"epoch##ar##ray##s"', ':', 'self', '.', 'nc##hil##dre##n', ',', '"event##ar##ray##s"', ':', 'self', '.', 'nc##hil##dre##n', ',', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : ""analogsignalarrays" : self . nchildren } \n"
Original    (007): ['"analogsignalarrays"', ':', 'self', '.', 'nchildren', '}', '\\n']
Tokenized   (020): ['[CLS]', '"', 'analog', '##si', '##gna', '##lar', '##ray', '##s', '"', ':', 'self', '.', 'nc', '##hil', '##dre', '##n', '}', '\\', 'n', '[SEP]']
Filtered   (018): ['"', 'analog', '##si', '##gna', '##lar', '##ray', '##s', '"', ':', 'self', '.', 'nc', '##hil', '##dre', '##n', '}', '\\', 'n']
Detokenized (007): ['"analog##si##gna##lar##ray##s"', ':', 'self', '.', 'nc##hil##dre##n', '}', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "targdict = { : 5 } ) \n"
Original    (008): ['targdict', '=', '{', ':', '5', '}', ')', '\\n']
Tokenized   (013): ['[CLS]', 'tar', '##g', '##dict', '=', '{', ':', '5', '}', ')', '\\', 'n', '[SEP]']
Filtered   (011): ['tar', '##g', '##dict', '=', '{', ':', '5', '}', ')', '\\', 'n']
Detokenized (008): ['tar##g##dict', '=', '{', ':', '5', '}', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "res6 = filterdata ( data , name = self . epcs2 [ 0 ] . name , j = 5 ) \n"
Original    (022): ['res6', '=', 'filterdata', '(', 'data', ',', 'name', '=', 'self', '.', 'epcs2', '[', '0', ']', '.', 'name', ',', 'j', '=', '5', ')', '\\n']
Tokenized   (030): ['[CLS]', 'res', '##6', '=', 'filter', '##da', '##ta', '(', 'data', ',', 'name', '=', 'self', '.', 'ep', '##cs', '##2', '[', '0', ']', '.', 'name', ',', 'j', '=', '5', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['res', '##6', '=', 'filter', '##da', '##ta', '(', 'data', ',', 'name', '=', 'self', '.', 'ep', '##cs', '##2', '[', '0', ']', '.', 'name', ',', 'j', '=', '5', ')', '\\', 'n']
Detokenized (022): ['res##6', '=', 'filter##da##ta', '(', 'data', ',', 'name', '=', 'self', '.', 'ep##cs##2', '[', '0', ']', '.', 'name', ',', 'j', '=', '5', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "res7 = filterdata ( data , { : self . epcs2 [ 1 ] . name , : 5 } ) \n"
Original    (022): ['res7', '=', 'filterdata', '(', 'data', ',', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\n']
Tokenized   (030): ['[CLS]', 'res', '##7', '=', 'filter', '##da', '##ta', '(', 'data', ',', '{', ':', 'self', '.', 'ep', '##cs', '##2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['res', '##7', '=', 'filter', '##da', '##ta', '(', 'data', ',', '{', ':', 'self', '.', 'ep', '##cs', '##2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\', 'n']
Detokenized (022): ['res##7', '=', 'filter##da##ta', '(', 'data', ',', '{', ':', 'self', '.', 'ep##cs##2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "res8 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name , : 5 } ) \n"
Original    (024): ['res8', '=', 'filterdata', '(', 'data', ',', 'targdict', '=', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\n']
Tokenized   (034): ['[CLS]', 'res', '##8', '=', 'filter', '##da', '##ta', '(', 'data', ',', 'tar', '##g', '##dict', '=', '{', ':', 'self', '.', 'ep', '##cs', '##2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\', 'n', '[SEP]']
Filtered   (032): ['res', '##8', '=', 'filter', '##da', '##ta', '(', 'data', ',', 'tar', '##g', '##dict', '=', '{', ':', 'self', '.', 'ep', '##cs', '##2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\', 'n']
Detokenized (024): ['res##8', '=', 'filter##da##ta', '(', 'data', ',', 'tar##g##dict', '=', '{', ':', 'self', '.', 'ep##cs##2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "res9 = filterdata ( data , { : self . epcs2 [ 1 ] . name } , j = 5 ) \n"
Original    (023): ['res9', '=', 'filterdata', '(', 'data', ',', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\n']
Tokenized   (031): ['[CLS]', 'res', '##9', '=', 'filter', '##da', '##ta', '(', 'data', ',', '{', ':', 'self', '.', 'ep', '##cs', '##2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\', 'n', '[SEP]']
Filtered   (029): ['res', '##9', '=', 'filter', '##da', '##ta', '(', 'data', ',', '{', ':', 'self', '.', 'ep', '##cs', '##2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\', 'n']
Detokenized (023): ['res##9', '=', 'filter##da##ta', '(', 'data', ',', '{', ':', 'self', '.', 'ep##cs##2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "res10 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name } , j = 5 ) \n"
Original    (025): ['res10', '=', 'filterdata', '(', 'data', ',', 'targdict', '=', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\n']
Tokenized   (035): ['[CLS]', 'res', '##10', '=', 'filter', '##da', '##ta', '(', 'data', ',', 'tar', '##g', '##dict', '=', '{', ':', 'self', '.', 'ep', '##cs', '##2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\', 'n', '[SEP]']
Filtered   (033): ['res', '##10', '=', 'filter', '##da', '##ta', '(', 'data', ',', 'tar', '##g', '##dict', '=', '{', ':', 'self', '.', 'ep', '##cs', '##2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\', 'n']
Detokenized (025): ['res##10', '=', 'filter##da##ta', '(', 'data', ',', 'tar##g##dict', '=', '{', ':', 'self', '.', 'ep##cs##2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "res11 = filterdata ( data , name = self . epcs2 [ 1 ] . name , targdict = { : 5 } ) \n"
Original    (025): ['res11', '=', 'filterdata', '(', 'data', ',', 'name', '=', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', ',', 'targdict', '=', '{', ':', '5', '}', ')', '\\n']
Tokenized   (035): ['[CLS]', 'res', '##11', '=', 'filter', '##da', '##ta', '(', 'data', ',', 'name', '=', 'self', '.', 'ep', '##cs', '##2', '[', '1', ']', '.', 'name', ',', 'tar', '##g', '##dict', '=', '{', ':', '5', '}', ')', '\\', 'n', '[SEP]']
Filtered   (033): ['res', '##11', '=', 'filter', '##da', '##ta', '(', 'data', ',', 'name', '=', 'self', '.', 'ep', '##cs', '##2', '[', '1', ']', '.', 'name', ',', 'tar', '##g', '##dict', '=', '{', ':', '5', '}', ')', '\\', 'n']
Detokenized (025): ['res##11', '=', 'filter##da##ta', '(', 'data', ',', 'name', '=', 'self', '.', 'ep##cs##2', '[', '1', ']', '.', 'name', ',', 'tar##g##dict', '=', '{', ':', '5', '}', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "targ = [ self . epcs1a [ 1 ] ] \n"
Original    (011): ['targ', '=', '[', 'self', '.', 'epcs1a', '[', '1', ']', ']', '\\n']
Tokenized   (018): ['[CLS]', 'tar', '##g', '=', '[', 'self', '.', 'ep', '##cs', '##1', '##a', '[', '1', ']', ']', '\\', 'n', '[SEP]']
Filtered   (016): ['tar', '##g', '=', '[', 'self', '.', 'ep', '##cs', '##1', '##a', '[', '1', ']', ']', '\\', 'n']
Detokenized (011): ['tar##g', '=', '[', 'self', '.', 'ep##cs##1##a', '[', '1', ']', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "res3 = filterdata ( data , [ { : 1 } , { : 2 } ] ) \n"
Original    (019): ['res3', '=', 'filterdata', '(', 'data', ',', '[', '{', ':', '1', '}', ',', '{', ':', '2', '}', ']', ')', '\\n']
Tokenized   (025): ['[CLS]', 'res', '##3', '=', 'filter', '##da', '##ta', '(', 'data', ',', '[', '{', ':', '1', '}', ',', '{', ':', '2', '}', ']', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['res', '##3', '=', 'filter', '##da', '##ta', '(', 'data', ',', '[', '{', ':', '1', '}', ',', '{', ':', '2', '}', ']', ')', '\\', 'n']
Detokenized (019): ['res##3', '=', 'filter##da##ta', '(', 'data', ',', '[', '{', ':', '1', '}', ',', '{', ':', '2', '}', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "res4 = filterdata ( data , { : 1 } , i = 2 ) \n"
Original    (016): ['res4', '=', 'filterdata', '(', 'data', ',', '{', ':', '1', '}', ',', 'i', '=', '2', ')', '\\n']
Tokenized   (022): ['[CLS]', 'res', '##4', '=', 'filter', '##da', '##ta', '(', 'data', ',', '{', ':', '1', '}', ',', 'i', '=', '2', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['res', '##4', '=', 'filter', '##da', '##ta', '(', 'data', ',', '{', ':', '1', '}', ',', 'i', '=', '2', ')', '\\', 'n']
Detokenized (016): ['res##4', '=', 'filter##da##ta', '(', 'data', ',', '{', ':', '1', '}', ',', 'i', '=', '2', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "res5 = filterdata ( data , [ { : 1 } ] , i = 2 ) \n"
Original    (018): ['res5', '=', 'filterdata', '(', 'data', ',', '[', '{', ':', '1', '}', ']', ',', 'i', '=', '2', ')', '\\n']
Tokenized   (024): ['[CLS]', 'res', '##5', '=', 'filter', '##da', '##ta', '(', 'data', ',', '[', '{', ':', '1', '}', ']', ',', 'i', '=', '2', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['res', '##5', '=', 'filter', '##da', '##ta', '(', 'data', ',', '[', '{', ':', '1', '}', ']', ',', 'i', '=', '2', ')', '\\', 'n']
Detokenized (018): ['res##5', '=', 'filter##da##ta', '(', 'data', ',', '[', '{', ':', '1', '}', ']', ',', 'i', '=', '2', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "ann = pretty ( ann ) . replace ( , ) \n"
Original    (012): ['ann', '=', 'pretty', '(', 'ann', ')', '.', 'replace', '(', ',', ')', '\\n']
Tokenized   (015): ['[CLS]', 'ann', '=', 'pretty', '(', 'ann', ')', '.', 'replace', '(', ',', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['ann', '=', 'pretty', '(', 'ann', ')', '.', 'replace', '(', ',', ')', '\\', 'n']
Detokenized (012): ['ann', '=', 'pretty', '(', 'ann', ')', '.', 'replace', '(', ',', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "unit_with_sig = np . array ( [ 0 , 2 , 5 ] ) \n"
Original    (015): ['unit_with_sig', '=', 'np', '.', 'array', '(', '[', '0', ',', '2', ',', '5', ']', ')', '\\n']
Tokenized   (023): ['[CLS]', 'unit', '_', 'with', '_', 'si', '##g', '=', 'np', '.', 'array', '(', '[', '0', ',', '2', ',', '5', ']', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['unit', '_', 'with', '_', 'si', '##g', '=', 'np', '.', 'array', '(', '[', '0', ',', '2', ',', '5', ']', ')', '\\', 'n']
Detokenized (015): ['unit_with_si##g', '=', 'np', '.', 'array', '(', '[', '0', ',', '2', ',', '5', ']', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "rcgs = [ RecordingChannelGroup ( name = , \n"
Original    (009): ['rcgs', '=', '[', 'RecordingChannelGroup', '(', 'name', '=', ',', '\\n']
Tokenized   (015): ['[CLS]', 'rc', '##gs', '=', '[', 'recording', '##channel', '##group', '(', 'name', '=', ',', '\\', 'n', '[SEP]']
Filtered   (013): ['rc', '##gs', '=', '[', 'recording', '##channel', '##group', '(', 'name', '=', ',', '\\', 'n']
Detokenized (009): ['rc##gs', '=', '[', 'recording##channel##group', '(', 'name', '=', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "RecordingChannelGroup ( name = , \n"
Original    (006): ['RecordingChannelGroup', '(', 'name', '=', ',', '\\n']
Tokenized   (011): ['[CLS]', 'recording', '##channel', '##group', '(', 'name', '=', ',', '\\', 'n', '[SEP]']
Filtered   (009): ['recording', '##channel', '##group', '(', 'name', '=', ',', '\\', 'n']
Detokenized (006): ['recording##channel##group', '(', 'name', '=', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "t_start = 0. , t_stop = 10 ) \n"
Original    (009): ['t_start', '=', '0.', ',', 't_stop', '=', '10', ')', '\\n']
Tokenized   (017): ['[CLS]', 't', '_', 'start', '=', '0', '.', ',', 't', '_', 'stop', '=', '10', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['t', '_', 'start', '=', '0', '.', ',', 't', '_', 'stop', '=', '10', ')', '\\', 'n']
Detokenized (009): ['t_start', '=', '0.', ',', 't_stop', '=', '10', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "st . unit = all_unit [ j ] \n"
Original    (009): ['st', '.', 'unit', '=', 'all_unit', '[', 'j', ']', '\\n']
Tokenized   (014): ['[CLS]', 'st', '.', 'unit', '=', 'all', '_', 'unit', '[', 'j', ']', '\\', 'n', '[SEP]']
Filtered   (012): ['st', '.', 'unit', '=', 'all', '_', 'unit', '[', 'j', ']', '\\', 'n']
Detokenized (009): ['st', '.', 'unit', '=', 'all_unit', '[', 'j', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "sampling_rate = 1000. * pq . Hz , \n"
Original    (009): ['sampling_rate', '=', '1000.', '*', 'pq', '.', 'Hz', ',', '\\n']
Tokenized   (016): ['[CLS]', 'sampling', '_', 'rate', '=', '1000', '.', '*', 'p', '##q', '.', 'hz', ',', '\\', 'n', '[SEP]']
Filtered   (014): ['sampling', '_', 'rate', '=', '1000', '.', '*', 'p', '##q', '.', 'hz', ',', '\\', 'n']
Detokenized (009): ['sampling_rate', '=', '1000.', '*', 'p##q', '.', 'hz', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "newseg = seg . construct_subsegment_by_unit ( all_unit [ : 4 ] ) \n"
Original    (013): ['newseg', '=', 'seg', '.', 'construct_subsegment_by_unit', '(', 'all_unit', '[', ':', '4', ']', ')', '\\n']
Tokenized   (029): ['[CLS]', 'news', '##eg', '=', 'se', '##g', '.', 'construct', '_', 'sub', '##se', '##gm', '##ent', '_', 'by', '_', 'unit', '(', 'all', '_', 'unit', '[', ':', '4', ']', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['news', '##eg', '=', 'se', '##g', '.', 'construct', '_', 'sub', '##se', '##gm', '##ent', '_', 'by', '_', 'unit', '(', 'all', '_', 'unit', '[', ':', '4', ']', ')', '\\', 'n']
Detokenized (013): ['news##eg', '=', 'se##g', '.', 'construct_sub##se##gm##ent_by_unit', '(', 'all_unit', '[', ':', '4', ']', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "ind2 = self . unit2 . channel_indexes [ 0 ] \n"
Original    (011): ['ind2', '=', 'self', '.', 'unit2', '.', 'channel_indexes', '[', '0', ']', '\\n']
Tokenized   (019): ['[CLS]', 'ind', '##2', '=', 'self', '.', 'unit', '##2', '.', 'channel', '_', 'index', '##es', '[', '0', ']', '\\', 'n', '[SEP]']
Filtered   (017): ['ind', '##2', '=', 'self', '.', 'unit', '##2', '.', 'channel', '_', 'index', '##es', '[', '0', ']', '\\', 'n']
Detokenized (011): ['ind##2', '=', 'self', '.', 'unit##2', '.', 'channel_index##es', '[', '0', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "result22 = self . seg1 . take_analogsignal_by_channelindex ( [ ind2 ] ) \n"
Original    (013): ['result22', '=', 'self', '.', 'seg1', '.', 'take_analogsignal_by_channelindex', '(', '[', 'ind2', ']', ')', '\\n']
Tokenized   (031): ['[CLS]', 'result', '##22', '=', 'self', '.', 'se', '##g', '##1', '.', 'take', '_', 'analog', '##si', '##gna', '##l', '_', 'by', '_', 'channel', '##ind', '##ex', '(', '[', 'ind', '##2', ']', ')', '\\', 'n', '[SEP]']
Filtered   (029): ['result', '##22', '=', 'self', '.', 'se', '##g', '##1', '.', 'take', '_', 'analog', '##si', '##gna', '##l', '_', 'by', '_', 'channel', '##ind', '##ex', '(', '[', 'ind', '##2', ']', ')', '\\', 'n']
Detokenized (013): ['result##22', '=', 'self', '.', 'se##g##1', '.', 'take_analog##si##gna##l_by_channel##ind##ex', '(', '[', 'ind##2', ']', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "targ1 = [ self . sigarrs1a [ 0 ] [ : , np . array ( [ True ] ) ] , \n"
Original    (023): ['targ1', '=', '[', 'self', '.', 'sigarrs1a', '[', '0', ']', '[', ':', ',', 'np', '.', 'array', '(', '[', 'True', ']', ')', ']', ',', '\\n']
Tokenized   (032): ['[CLS]', 'tar', '##g', '##1', '=', '[', 'self', '.', 'si', '##gar', '##rs', '##1', '##a', '[', '0', ']', '[', ':', ',', 'np', '.', 'array', '(', '[', 'true', ']', ')', ']', ',', '\\', 'n', '[SEP]']
Filtered   (030): ['tar', '##g', '##1', '=', '[', 'self', '.', 'si', '##gar', '##rs', '##1', '##a', '[', '0', ']', '[', ':', ',', 'np', '.', 'array', '(', '[', 'true', ']', ')', ']', ',', '\\', 'n']
Detokenized (023): ['tar##g##1', '=', '[', 'self', '.', 'si##gar##rs##1##a', '[', '0', ']', '[', ':', ',', 'np', '.', 'array', '(', '[', 'true', ']', ')', ']', ',', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "result21 = seg . take_slice_of_analogsignalarray_by_channelindex ( [ ind1 ] ) \n"
Original    (011): ['result21', '=', 'seg', '.', 'take_slice_of_analogsignalarray_by_channelindex', '(', '[', 'ind1', ']', ')', '\\n']
Tokenized   (033): ['[CLS]', 'result', '##21', '=', 'se', '##g', '.', 'take', '_', 'slice', '_', 'of', '_', 'analog', '##si', '##gna', '##lar', '##ray', '_', 'by', '_', 'channel', '##ind', '##ex', '(', '[', 'ind', '##1', ']', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['result', '##21', '=', 'se', '##g', '.', 'take', '_', 'slice', '_', 'of', '_', 'analog', '##si', '##gna', '##lar', '##ray', '_', 'by', '_', 'channel', '##ind', '##ex', '(', '[', 'ind', '##1', ']', ')', '\\', 'n']
Detokenized (011): ['result##21', '=', 'se##g', '.', 'take_slice_of_analog##si##gna##lar##ray_by_channel##ind##ex', '(', '[', 'ind##1', ']', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "json_content = json_content . decode ( "utf-8" ) . replace ( , ) \n"
Original    (014): ['json_content', '=', 'json_content', '.', 'decode', '(', '"utf-8"', ')', '.', 'replace', '(', ',', ')', '\\n']
Tokenized   (029): ['[CLS]', 'j', '##son', '_', 'content', '=', 'j', '##son', '_', 'content', '.', 'deco', '##de', '(', '"', 'ut', '##f', '-', '8', '"', ')', '.', 'replace', '(', ',', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['j', '##son', '_', 'content', '=', 'j', '##son', '_', 'content', '.', 'deco', '##de', '(', '"', 'ut', '##f', '-', '8', '"', ')', '.', 'replace', '(', ',', ')', '\\', 'n']
Detokenized (014): ['j##son_content', '=', 'j##son_content', '.', 'deco##de', '(', '"ut##f-8"', ')', '.', 'replace', '(', ',', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "Image1 = StatisticMap ( name = , collection = self . Collection1 , file = , Image1 . file = SimpleUploadedFile ( , file ( os . path . join ( self . test_path , Image1 . save ( ) \n"
Original    (041): ['Image1', '=', 'StatisticMap', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection1', ',', 'file', '=', ',', 'Image1', '.', 'file', '=', 'SimpleUploadedFile', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', 'Image1', '.', 'save', '(', ')', '\\n']
Tokenized   (057): ['[CLS]', 'image', '##1', '=', 'stat', '##istic', '##ma', '##p', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'collection', '##1', ',', 'file', '=', ',', 'image', '##1', '.', 'file', '=', 'simple', '##up', '##loaded', '##fi', '##le', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', 'image', '##1', '.', 'save', '(', ')', '\\', 'n', '[SEP]']
Filtered   (055): ['image', '##1', '=', 'stat', '##istic', '##ma', '##p', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'collection', '##1', ',', 'file', '=', ',', 'image', '##1', '.', 'file', '=', 'simple', '##up', '##loaded', '##fi', '##le', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', 'image', '##1', '.', 'save', '(', ')', '\\', 'n']
Detokenized (041): ['image##1', '=', 'stat##istic##ma##p', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'collection##1', ',', 'file', '=', ',', 'image##1', '.', 'file', '=', 'simple##up##loaded##fi##le', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', 'image##1', '.', 'save', '(', ')', '\\n']
Counter: 55
===================================================================
Hidden states:  (13, 41, 768)
# Extracted words:  41
Sentence         : "fname = os . path . basename ( os . path . join ( self . test_path , ) ) file_dict = { : SimpleUploadedFile ( fname , zip_file . read ( ) ) } \n"
Original    (036): ['fname', '=', 'os', '.', 'path', '.', 'basename', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', ')', ')', 'file_dict', '=', '{', ':', 'SimpleUploadedFile', '(', 'fname', ',', 'zip_file', '.', 'read', '(', ')', ')', '}', '\\n']
Tokenized   (053): ['[CLS]', 'f', '##name', '=', 'os', '.', 'path', '.', 'base', '##name', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', ')', ')', 'file', '_', 'di', '##ct', '=', '{', ':', 'simple', '##up', '##loaded', '##fi', '##le', '(', 'f', '##name', ',', 'zip', '_', 'file', '.', 'read', '(', ')', ')', '}', '\\', 'n', '[SEP]']
Filtered   (051): ['f', '##name', '=', 'os', '.', 'path', '.', 'base', '##name', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', ')', ')', 'file', '_', 'di', '##ct', '=', '{', ':', 'simple', '##up', '##loaded', '##fi', '##le', '(', 'f', '##name', ',', 'zip', '_', 'file', '.', 'read', '(', ')', ')', '}', '\\', 'n']
Detokenized (036): ['f##name', '=', 'os', '.', 'path', '.', 'base##name', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', ')', ')', 'file_di##ct', '=', '{', ':', 'simple##up##loaded##fi##le', '(', 'f##name', ',', 'zip_file', '.', 'read', '(', ')', ')', '}', '\\n']
Counter: 51
===================================================================
Hidden states:  (13, 36, 768)
# Extracted words:  36
Sentence         : "Image2ss = StatisticMap ( name = , collection = self . Collection3 , file = Image2ss . file = SimpleUploadedFile ( , file ( os . path . join ( self . test_path , Image2ss . save ( ) \n"
Original    (040): ['Image2ss', '=', 'StatisticMap', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection3', ',', 'file', '=', 'Image2ss', '.', 'file', '=', 'SimpleUploadedFile', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', 'Image2ss', '.', 'save', '(', ')', '\\n']
Tokenized   (059): ['[CLS]', 'image', '##2', '##ss', '=', 'stat', '##istic', '##ma', '##p', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'collection', '##3', ',', 'file', '=', 'image', '##2', '##ss', '.', 'file', '=', 'simple', '##up', '##loaded', '##fi', '##le', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', 'image', '##2', '##ss', '.', 'save', '(', ')', '\\', 'n', '[SEP]']
Filtered   (057): ['image', '##2', '##ss', '=', 'stat', '##istic', '##ma', '##p', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'collection', '##3', ',', 'file', '=', 'image', '##2', '##ss', '.', 'file', '=', 'simple', '##up', '##loaded', '##fi', '##le', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', 'image', '##2', '##ss', '.', 'save', '(', ')', '\\', 'n']
Detokenized (040): ['image##2##ss', '=', 'stat##istic##ma##p', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'collection##3', ',', 'file', '=', 'image##2##ss', '.', 'file', '=', 'simple##up##loaded##fi##le', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', 'image##2##ss', '.', 'save', '(', ')', '\\n']
Counter: 57
===================================================================
Hidden states:  (13, 40, 768)
# Extracted words:  40
Sentence         : "acc_new = rho * acc + ( 1 - rho ) * g ** 2 \n"
Original    (016): ['acc_new', '=', 'rho', '*', 'acc', '+', '(', '1', '-', 'rho', ')', '*', 'g', '**', '2', '\\n']
Tokenized   (024): ['[CLS]', 'acc', '_', 'new', '=', 'r', '##ho', '*', 'acc', '+', '(', '1', '-', 'r', '##ho', ')', '*', 'g', '*', '*', '2', '\\', 'n', '[SEP]']
Filtered   (022): ['acc', '_', 'new', '=', 'r', '##ho', '*', 'acc', '+', '(', '1', '-', 'r', '##ho', ')', '*', 'g', '*', '*', '2', '\\', 'n']
Detokenized (016): ['acc_new', '=', 'r##ho', '*', 'acc', '+', '(', '1', '-', 'r##ho', ')', '*', 'g', '**', '2', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "gradient_scaling = T . sqrt ( acc_new + epsilon ) \n"
Original    (011): ['gradient_scaling', '=', 'T', '.', 'sqrt', '(', 'acc_new', '+', 'epsilon', ')', '\\n']
Tokenized   (019): ['[CLS]', 'gradient', '_', 'scaling', '=', 't', '.', 'sq', '##rt', '(', 'acc', '_', 'new', '+', 'epsilon', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['gradient', '_', 'scaling', '=', 't', '.', 'sq', '##rt', '(', 'acc', '_', 'new', '+', 'epsilon', ')', '\\', 'n']
Detokenized (011): ['gradient_scaling', '=', 't', '.', 'sq##rt', '(', 'acc_new', '+', 'epsilon', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "py_x = softmax ( T . dot ( h2 , w_o ) ) \n"
Original    (014): ['py_x', '=', 'softmax', '(', 'T', '.', 'dot', '(', 'h2', ',', 'w_o', ')', ')', '\\n']
Tokenized   (024): ['[CLS]', 'p', '##y', '_', 'x', '=', 'soft', '##max', '(', 't', '.', 'dot', '(', 'h', '##2', ',', 'w', '_', 'o', ')', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['p', '##y', '_', 'x', '=', 'soft', '##max', '(', 't', '.', 'dot', '(', 'h', '##2', ',', 'w', '_', 'o', ')', ')', '\\', 'n']
Detokenized (014): ['p##y_x', '=', 'soft##max', '(', 't', '.', 'dot', '(', 'h##2', ',', 'w_o', ')', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "w_h = init_weights ( ( 784 , 625 ) ) \n"
Original    (011): ['w_h', '=', 'init_weights', '(', '(', '784', ',', '625', ')', ')', '\\n']
Tokenized   (020): ['[CLS]', 'w', '_', 'h', '=', 'in', '##it', '_', 'weights', '(', '(', '78', '##4', ',', '625', ')', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['w', '_', 'h', '=', 'in', '##it', '_', 'weights', '(', '(', '78', '##4', ',', '625', ')', ')', '\\', 'n']
Detokenized (011): ['w_h', '=', 'in##it_weights', '(', '(', '78##4', ',', '625', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "noise_h , noise_h2 , noise_py_x = model ( X , w_h , w_h2 , w_o , 0.2 , 0.5 ) \n"
Original    (021): ['noise_h', ',', 'noise_h2', ',', 'noise_py_x', '=', 'model', '(', 'X', ',', 'w_h', ',', 'w_h2', ',', 'w_o', ',', '0.2', ',', '0.5', ')', '\\n']
Tokenized   (045): ['[CLS]', 'noise', '_', 'h', ',', 'noise', '_', 'h', '##2', ',', 'noise', '_', 'p', '##y', '_', 'x', '=', 'model', '(', 'x', ',', 'w', '_', 'h', ',', 'w', '_', 'h', '##2', ',', 'w', '_', 'o', ',', '0', '.', '2', ',', '0', '.', '5', ')', '\\', 'n', '[SEP]']
Filtered   (043): ['noise', '_', 'h', ',', 'noise', '_', 'h', '##2', ',', 'noise', '_', 'p', '##y', '_', 'x', '=', 'model', '(', 'x', ',', 'w', '_', 'h', ',', 'w', '_', 'h', '##2', ',', 'w', '_', 'o', ',', '0', '.', '2', ',', '0', '.', '5', ')', '\\', 'n']
Detokenized (021): ['noise_h', ',', 'noise_h##2', ',', 'noise_p##y_x', '=', 'model', '(', 'x', ',', 'w_h', ',', 'w_h##2', ',', 'w_o', ',', '0.2', ',', '0.5', ')', '\\n']
Counter: 43
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "y_x = T . argmax ( py_x , axis = 1 ) \n"
Original    (013): ['y_x', '=', 'T', '.', 'argmax', '(', 'py_x', ',', 'axis', '=', '1', ')', '\\n']
Tokenized   (023): ['[CLS]', 'y', '_', 'x', '=', 't', '.', 'ar', '##gm', '##ax', '(', 'p', '##y', '_', 'x', ',', 'axis', '=', '1', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['y', '_', 'x', '=', 't', '.', 'ar', '##gm', '##ax', '(', 'p', '##y', '_', 'x', ',', 'axis', '=', '1', ')', '\\', 'n']
Detokenized (013): ['y_x', '=', 't', '.', 'ar##gm##ax', '(', 'p##y_x', ',', 'axis', '=', '1', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "cost = T . mean ( T . nnet . categorical_crossentropy ( noise_py_x , Y ) ) \n"
Original    (018): ['cost', '=', 'T', '.', 'mean', '(', 'T', '.', 'nnet', '.', 'categorical_crossentropy', '(', 'noise_py_x', ',', 'Y', ')', ')', '\\n']
Tokenized   (033): ['[CLS]', 'cost', '=', 't', '.', 'mean', '(', 't', '.', 'n', '##net', '.', 'cat', '##egorical', '_', 'cross', '##ent', '##rop', '##y', '(', 'noise', '_', 'p', '##y', '_', 'x', ',', 'y', ')', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['cost', '=', 't', '.', 'mean', '(', 't', '.', 'n', '##net', '.', 'cat', '##egorical', '_', 'cross', '##ent', '##rop', '##y', '(', 'noise', '_', 'p', '##y', '_', 'x', ',', 'y', ')', ')', '\\', 'n']
Detokenized (018): ['cost', '=', 't', '.', 'mean', '(', 't', '.', 'n##net', '.', 'cat##egorical_cross##ent##rop##y', '(', 'noise_p##y_x', ',', 'y', ')', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "updates = RMSprop ( cost , params , lr = 0.001 ) \n"
Original    (013): ['updates', '=', 'RMSprop', '(', 'cost', ',', 'params', ',', 'lr', '=', '0.001', ')', '\\n']
Tokenized   (022): ['[CLS]', 'updates', '=', 'rms', '##pro', '##p', '(', 'cost', ',', 'para', '##ms', ',', 'l', '##r', '=', '0', '.', '001', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['updates', '=', 'rms', '##pro', '##p', '(', 'cost', ',', 'para', '##ms', ',', 'l', '##r', '=', '0', '.', '001', ')', '\\', 'n']
Detokenized (013): ['updates', '=', 'rms##pro##p', '(', 'cost', ',', 'para##ms', ',', 'l##r', '=', '0.001', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "train = theano . function ( inputs = [ X , Y ] , outputs = cost , updates = updates , allow_input_downcast = True ) \n"
Original    (027): ['train', '=', 'theano', '.', 'function', '(', 'inputs', '=', '[', 'X', ',', 'Y', ']', ',', 'outputs', '=', 'cost', ',', 'updates', '=', 'updates', ',', 'allow_input_downcast', '=', 'True', ')', '\\n']
Tokenized   (036): ['[CLS]', 'train', '=', 'the', '##ano', '.', 'function', '(', 'inputs', '=', '[', 'x', ',', 'y', ']', ',', 'outputs', '=', 'cost', ',', 'updates', '=', 'updates', ',', 'allow', '_', 'input', '_', 'down', '##cast', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (034): ['train', '=', 'the', '##ano', '.', 'function', '(', 'inputs', '=', '[', 'x', ',', 'y', ']', ',', 'outputs', '=', 'cost', ',', 'updates', '=', 'updates', ',', 'allow', '_', 'input', '_', 'down', '##cast', '=', 'true', ')', '\\', 'n']
Detokenized (027): ['train', '=', 'the##ano', '.', 'function', '(', 'inputs', '=', '[', 'x', ',', 'y', ']', ',', 'outputs', '=', 'cost', ',', 'updates', '=', 'updates', ',', 'allow_input_down##cast', '=', 'true', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "predict = theano . function ( inputs = [ X ] , outputs = y_x , allow_input_downcast = True ) \n"
Original    (021): ['predict', '=', 'theano', '.', 'function', '(', 'inputs', '=', '[', 'X', ']', ',', 'outputs', '=', 'y_x', ',', 'allow_input_downcast', '=', 'True', ')', '\\n']
Tokenized   (032): ['[CLS]', 'predict', '=', 'the', '##ano', '.', 'function', '(', 'inputs', '=', '[', 'x', ']', ',', 'outputs', '=', 'y', '_', 'x', ',', 'allow', '_', 'input', '_', 'down', '##cast', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (030): ['predict', '=', 'the', '##ano', '.', 'function', '(', 'inputs', '=', '[', 'x', ']', ',', 'outputs', '=', 'y', '_', 'x', ',', 'allow', '_', 'input', '_', 'down', '##cast', '=', 'true', ')', '\\', 'n']
Detokenized (021): ['predict', '=', 'the##ano', '.', 'function', '(', 'inputs', '=', '[', 'x', ']', ',', 'outputs', '=', 'y_x', ',', 'allow_input_down##cast', '=', 'true', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "settings . DATABASE_CONFIG_DICT [ ] ) \n"
Original    (007): ['settings', '.', 'DATABASE_CONFIG_DICT', '[', ']', ')', '\\n']
Tokenized   (017): ['[CLS]', 'settings', '.', 'database', '_', 'con', '##fi', '##g', '_', 'di', '##ct', '[', ']', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['settings', '.', 'database', '_', 'con', '##fi', '##g', '_', 'di', '##ct', '[', ']', ')', '\\', 'n']
Detokenized (007): ['settings', '.', 'database_con##fi##g_di##ct', '[', ']', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "TEMPLATES [ 0 ] [ ] [ ] = DEBUG \n"
Original    (011): ['TEMPLATES', '[', '0', ']', '[', ']', '[', ']', '=', 'DEBUG', '\\n']
Tokenized   (017): ['[CLS]', 'template', '##s', '[', '0', ']', '[', ']', '[', ']', '=', 'de', '##bu', '##g', '\\', 'n', '[SEP]']
Filtered   (015): ['template', '##s', '[', '0', ']', '[', ']', '[', ']', '=', 'de', '##bu', '##g', '\\', 'n']
Detokenized (011): ['template##s', '[', '0', ']', '[', ']', '[', ']', '=', 'de##bu##g', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "SECRET_KEY = env ( "DJANGO_SECRET_KEY" , default = ) \n"
Original    (010): ['SECRET_KEY', '=', 'env', '(', '"DJANGO_SECRET_KEY"', ',', 'default', '=', ')', '\\n']
Tokenized   (023): ['[CLS]', 'secret', '_', 'key', '=', 'en', '##v', '(', '"', 'dj', '##ango', '_', 'secret', '_', 'key', '"', ',', 'default', '=', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['secret', '_', 'key', '=', 'en', '##v', '(', '"', 'dj', '##ango', '_', 'secret', '_', 'key', '"', ',', 'default', '=', ')', '\\', 'n']
Detokenized (010): ['secret_key', '=', 'en##v', '(', '"dj##ango_secret_key"', ',', 'default', '=', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "MIDDLEWARE_CLASSES += ( , ) \n"
Original    (006): ['MIDDLEWARE_CLASSES', '+=', '(', ',', ')', '\\n']
Tokenized   (013): ['[CLS]', 'middle', '##ware', '_', 'classes', '+', '=', '(', ',', ')', '\\', 'n', '[SEP]']
Filtered   (011): ['middle', '##ware', '_', 'classes', '+', '=', '(', ',', ')', '\\', 'n']
Detokenized (006): ['middle##ware_classes', '+=', '(', ',', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "INTERNAL_IPS = ( , , ) \n"
Original    (007): ['INTERNAL_IPS', '=', '(', ',', ',', ')', '\\n']
Tokenized   (013): ['[CLS]', 'internal', '_', 'ip', '##s', '=', '(', ',', ',', ')', '\\', 'n', '[SEP]']
Filtered   (011): ['internal', '_', 'ip', '##s', '=', '(', ',', ',', ')', '\\', 'n']
Detokenized (007): ['internal_ip##s', '=', '(', ',', ',', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "redirect_url = request . POST . get ( ) or \n"
Original    (011): ['redirect_url', '=', 'request', '.', 'POST', '.', 'get', '(', ')', 'or', '\\n']
Tokenized   (019): ['[CLS]', 'red', '##ire', '##ct', '_', 'ur', '##l', '=', 'request', '.', 'post', '.', 'get', '(', ')', 'or', '\\', 'n', '[SEP]']
Filtered   (017): ['red', '##ire', '##ct', '_', 'ur', '##l', '=', 'request', '.', 'post', '.', 'get', '(', ')', 'or', '\\', 'n']
Detokenized (011): ['red##ire##ct_ur##l', '=', 'request', '.', 'post', '.', 'get', '(', ')', 'or', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "icon = self . get_plugin_icon ( ) , \n"
Original    (009): ['icon', '=', 'self', '.', 'get_plugin_icon', '(', ')', ',', '\\n']
Tokenized   (017): ['[CLS]', 'icon', '=', 'self', '.', 'get', '_', 'plug', '##in', '_', 'icon', '(', ')', ',', '\\', 'n', '[SEP]']
Filtered   (015): ['icon', '=', 'self', '.', 'get', '_', 'plug', '##in', '_', 'icon', '(', ')', ',', '\\', 'n']
Detokenized (009): ['icon', '=', 'self', '.', 'get_plug##in_icon', '(', ')', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "memoryprofiler_act . setEnabled ( is_memoryprofiler_installed ( ) ) \n"
Original    (009): ['memoryprofiler_act', '.', 'setEnabled', '(', 'is_memoryprofiler_installed', '(', ')', ')', '\\n']
Tokenized   (026): ['[CLS]', 'memory', '##pro', '##fi', '##ler', '_', 'act', '.', 'set', '##ena', '##bled', '(', 'is', '_', 'memory', '##pro', '##fi', '##ler', '_', 'installed', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['memory', '##pro', '##fi', '##ler', '_', 'act', '.', 'set', '##ena', '##bled', '(', 'is', '_', 'memory', '##pro', '##fi', '##ler', '_', 'installed', '(', ')', ')', '\\', 'n']
Detokenized (009): ['memory##pro##fi##ler_act', '.', 'set##ena##bled', '(', 'is_memory##pro##fi##ler_installed', '(', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "wdir , args = None , None \n"
Original    (008): ['wdir', ',', 'args', '=', 'None', ',', 'None', '\\n']
Tokenized   (014): ['[CLS]', 'w', '##di', '##r', ',', 'ar', '##gs', '=', 'none', ',', 'none', '\\', 'n', '[SEP]']
Filtered   (012): ['w', '##di', '##r', ',', 'ar', '##gs', '=', 'none', ',', 'none', '\\', 'n']
Detokenized (008): ['w##di##r', ',', 'ar##gs', '=', 'none', ',', 'none', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "use_colors = self . get_option ( , True ) ) \n"
Original    (011): ['use_colors', '=', 'self', '.', 'get_option', '(', ',', 'True', ')', ')', '\\n']
Tokenized   (018): ['[CLS]', 'use', '_', 'colors', '=', 'self', '.', 'get', '_', 'option', '(', ',', 'true', ')', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['use', '_', 'colors', '=', 'self', '.', 'get', '_', 'option', '(', ',', 'true', ')', ')', '\\', 'n']
Detokenized (011): ['use_colors', '=', 'self', '.', 'get_option', '(', ',', 'true', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "message_type = None , enum_type = None , containing_type = None , \n"
Original    (013): ['message_type', '=', 'None', ',', 'enum_type', '=', 'None', ',', 'containing_type', '=', 'None', ',', '\\n']
Tokenized   (023): ['[CLS]', 'message', '_', 'type', '=', 'none', ',', 'en', '##um', '_', 'type', '=', 'none', ',', 'containing', '_', 'type', '=', 'none', ',', '\\', 'n', '[SEP]']
Filtered   (021): ['message', '_', 'type', '=', 'none', ',', 'en', '##um', '_', 'type', '=', 'none', ',', 'containing', '_', 'type', '=', 'none', ',', '\\', 'n']
Detokenized (013): ['message_type', '=', 'none', ',', 'en##um_type', '=', 'none', ',', 'containing_type', '=', 'none', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "is_extension = False , extension_scope = None , \n"
Original    (009): ['is_extension', '=', 'False', ',', 'extension_scope', '=', 'None', ',', '\\n']
Tokenized   (016): ['[CLS]', 'is', '_', 'extension', '=', 'false', ',', 'extension', '_', 'scope', '=', 'none', ',', '\\', 'n', '[SEP]']
Filtered   (014): ['is', '_', 'extension', '=', 'false', ',', 'extension', '_', 'scope', '=', 'none', ',', '\\', 'n']
Detokenized (009): ['is_extension', '=', 'false', ',', 'extension_scope', '=', 'none', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "_BATCHNOTIFICATIONREQUEST . fields_by_name [ ] . message_type = _PUSHNOTIFICATION \n"
Original    (010): ['_BATCHNOTIFICATIONREQUEST', '.', 'fields_by_name', '[', ']', '.', 'message_type', '=', '_PUSHNOTIFICATION', '\\n']
Tokenized   (027): ['[CLS]', '_', 'batch', '##not', '##ification', '##re', '##quest', '.', 'fields', '_', 'by', '_', 'name', '[', ']', '.', 'message', '_', 'type', '=', '_', 'push', '##not', '##ification', '\\', 'n', '[SEP]']
Filtered   (025): ['_', 'batch', '##not', '##ification', '##re', '##quest', '.', 'fields', '_', 'by', '_', 'name', '[', ']', '.', 'message', '_', 'type', '=', '_', 'push', '##not', '##ification', '\\', 'n']
Detokenized (010): ['_batch##not##ification##re##quest', '.', 'fields_by_name', '[', ']', '.', 'message_type', '=', '_push##not##ification', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "expected_zip_content = [ "manifest.json" , "sd_bl.bin" , "sd_bl.dat" ] \n"
Original    (010): ['expected_zip_content', '=', '[', '"manifest.json"', ',', '"sd_bl.bin"', ',', '"sd_bl.dat"', ']', '\\n']
Tokenized   (036): ['[CLS]', 'expected', '_', 'zip', '_', 'content', '=', '[', '"', 'manifest', '.', 'j', '##son', '"', ',', '"', 'sd', '_', 'b', '##l', '.', 'bin', '"', ',', '"', 'sd', '_', 'b', '##l', '.', 'dat', '"', ']', '\\', 'n', '[SEP]']
Filtered   (034): ['expected', '_', 'zip', '_', 'content', '=', '[', '"', 'manifest', '.', 'j', '##son', '"', ',', '"', 'sd', '_', 'b', '##l', '.', 'bin', '"', ',', '"', 'sd', '_', 'b', '##l', '.', 'dat', '"', ']', '\\', 'n']
Detokenized (010): ['expected_zip_content', '=', '[', '"manifest.j##son"', ',', '"sd_b##l.bin"', ',', '"sd_b##l.dat"', ']', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sd_req = [ 0x1000 , 0xffff ] , \n"
Original    (009): ['sd_req', '=', '[', '0x1000', ',', '0xffff', ']', ',', '\\n']
Tokenized   (021): ['[CLS]', 'sd', '_', 're', '##q', '=', '[', '0', '##x', '##100', '##0', ',', '0', '##x', '##ff', '##ff', ']', ',', '\\', 'n', '[SEP]']
Filtered   (019): ['sd', '_', 're', '##q', '=', '[', '0', '##x', '##100', '##0', ',', '0', '##x', '##ff', '##ff', ']', ',', '\\', 'n']
Detokenized (009): ['sd_re##q', '=', '[', '0##x##100##0', ',', '0##x##ff##ff', ']', ',', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "pkg_name = os . path . join ( self . work_directory , "mypackage.zip" ) \n"
Original    (015): ['pkg_name', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work_directory', ',', '"mypackage.zip"', ')', '\\n']
Tokenized   (030): ['[CLS]', 'p', '##k', '##g', '_', 'name', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work', '_', 'directory', ',', '"', 'my', '##pack', '##age', '.', 'zip', '"', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['p', '##k', '##g', '_', 'name', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work', '_', 'directory', ',', '"', 'my', '##pack', '##age', '.', 'zip', '"', ')', '\\', 'n']
Detokenized (015): ['p##k##g_name', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work_directory', ',', '"my##pack##age.zip"', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "manifest = self . p . unpack_package ( os . path . join ( self . work_directory , pkg_name ) , unpacked_dir ) \n"
Original    (024): ['manifest', '=', 'self', '.', 'p', '.', 'unpack_package', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work_directory', ',', 'pkg_name', ')', ',', 'unpacked_dir', ')', '\\n']
Tokenized   (040): ['[CLS]', 'manifest', '=', 'self', '.', 'p', '.', 'un', '##pack', '_', 'package', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work', '_', 'directory', ',', 'p', '##k', '##g', '_', 'name', ')', ',', 'un', '##pack', '##ed', '_', 'dir', ')', '\\', 'n', '[SEP]']
Filtered   (038): ['manifest', '=', 'self', '.', 'p', '.', 'un', '##pack', '_', 'package', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work', '_', 'directory', ',', 'p', '##k', '##g', '_', 'name', ')', ',', 'un', '##pack', '##ed', '_', 'dir', ')', '\\', 'n']
Detokenized (024): ['manifest', '=', 'self', '.', 'p', '.', 'un##pack_package', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work_directory', ',', 'p##k##g_name', ')', ',', 'un##pack##ed_dir', ')', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "style = wx . richtext . RE_MULTILINE , value = ) \n"
Original    (012): ['style', '=', 'wx', '.', 'richtext', '.', 'RE_MULTILINE', ',', 'value', '=', ')', '\\n']
Tokenized   (020): ['[CLS]', 'style', '=', 'w', '##x', '.', 'rich', '##text', '.', 're', '_', 'multi', '##line', ',', 'value', '=', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['style', '=', 'w', '##x', '.', 'rich', '##text', '.', 're', '_', 'multi', '##line', ',', 'value', '=', ')', '\\', 'n']
Detokenized (012): ['style', '=', 'w##x', '.', 'rich##text', '.', 're_multi##line', ',', 'value', '=', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "fgSizer1 . Add ( self . lblChangelt , 0 , wx . ALL , 5 ) \n"
Original    (017): ['fgSizer1', '.', 'Add', '(', 'self', '.', 'lblChangelt', ',', '0', ',', 'wx', '.', 'ALL', ',', '5', ')', '\\n']
Tokenized   (027): ['[CLS]', 'f', '##gs', '##izer', '##1', '.', 'add', '(', 'self', '.', 'lb', '##lch', '##ange', '##lt', ',', '0', ',', 'w', '##x', '.', 'all', ',', '5', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['f', '##gs', '##izer', '##1', '.', 'add', '(', 'self', '.', 'lb', '##lch', '##ange', '##lt', ',', '0', ',', 'w', '##x', '.', 'all', ',', '5', ')', '\\', 'n']
Detokenized (017): ['f##gs##izer##1', '.', 'add', '(', 'self', '.', 'lb##lch##ange##lt', ',', '0', ',', 'w##x', '.', 'all', ',', '5', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "sbThreshold . Add ( fgSizer1 , 0 , wx . EXPAND , 5 ) \n"
Original    (015): ['sbThreshold', '.', 'Add', '(', 'fgSizer1', ',', '0', ',', 'wx', '.', 'EXPAND', ',', '5', ')', '\\n']
Tokenized   (025): ['[CLS]', 'sb', '##th', '##resh', '##old', '.', 'add', '(', 'f', '##gs', '##izer', '##1', ',', '0', ',', 'w', '##x', '.', 'expand', ',', '5', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['sb', '##th', '##resh', '##old', '.', 'add', '(', 'f', '##gs', '##izer', '##1', ',', '0', ',', 'w', '##x', '.', 'expand', ',', '5', ')', '\\', 'n']
Detokenized (015): ['sb##th##resh##old', '.', 'add', '(', 'f##gs##izer##1', ',', '0', ',', 'w##x', '.', 'expand', ',', '5', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "bsValueThresh . Add ( sbThreshold , 1 , 0 , 5 ) \n"
Original    (013): ['bsValueThresh', '.', 'Add', '(', 'sbThreshold', ',', '1', ',', '0', ',', '5', ')', '\\n']
Tokenized   (023): ['[CLS]', 'bs', '##val', '##uet', '##hre', '##sh', '.', 'add', '(', 'sb', '##th', '##resh', '##old', ',', '1', ',', '0', ',', '5', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['bs', '##val', '##uet', '##hre', '##sh', '.', 'add', '(', 'sb', '##th', '##resh', '##old', ',', '1', ',', '0', ',', '5', ')', '\\', 'n']
Detokenized (013): ['bs##val##uet##hre##sh', '.', 'add', '(', 'sb##th##resh##old', ',', '1', ',', '0', ',', '5', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "cbGapTimeChoices = [ u"second" , u"minute" , u"hour" , u"day" ] \n"
Original    (012): ['cbGapTimeChoices', '=', '[', 'u"second"', ',', 'u"minute"', ',', 'u"hour"', ',', 'u"day"', ']', '\\n']
Tokenized   (032): ['[CLS]', 'cb', '##ga', '##pt', '##ime', '##cho', '##ices', '=', '[', 'u', '"', 'second', '"', ',', 'u', '"', 'minute', '"', ',', 'u', '"', 'hour', '"', ',', 'u', '"', 'day', '"', ']', '\\', 'n', '[SEP]']
Filtered   (030): ['cb', '##ga', '##pt', '##ime', '##cho', '##ices', '=', '[', 'u', '"', 'second', '"', ',', 'u', '"', 'minute', '"', ',', 'u', '"', 'hour', '"', ',', 'u', '"', 'day', '"', ']', '\\', 'n']
Detokenized (012): ['cb##ga##pt##ime##cho##ices', '=', '[', 'u"second"', ',', 'u"minute"', ',', 'u"hour"', ',', 'u"day"', ']', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "fmt24hr = True , spinButton = self . sbBefore , oob_color = ) \n"
Original    (014): ['fmt24hr', '=', 'True', ',', 'spinButton', '=', 'self', '.', 'sbBefore', ',', 'oob_color', '=', ')', '\\n']
Tokenized   (028): ['[CLS]', 'fm', '##t', '##24', '##hr', '=', 'true', ',', 'spin', '##bu', '##tton', '=', 'self', '.', 'sb', '##be', '##for', '##e', ',', 'o', '##ob', '_', 'color', '=', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['fm', '##t', '##24', '##hr', '=', 'true', ',', 'spin', '##bu', '##tton', '=', 'self', '.', 'sb', '##be', '##for', '##e', ',', 'o', '##ob', '_', 'color', '=', ')', '\\', 'n']
Detokenized (014): ['fm##t##24##hr', '=', 'true', ',', 'spin##bu##tton', '=', 'self', '.', 'sb##be##for##e', ',', 'o##ob_color', '=', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "bsButtons . Add ( self . btnOK , 1 , wx . ALL | wx . EXPAND , 5 ) \n"
Original    (021): ['bsButtons', '.', 'Add', '(', 'self', '.', 'btnOK', ',', '1', ',', 'wx', '.', 'ALL', '|', 'wx', '.', 'EXPAND', ',', '5', ')', '\\n']
Tokenized   (031): ['[CLS]', 'bs', '##bu', '##tton', '##s', '.', 'add', '(', 'self', '.', 'bt', '##no', '##k', ',', '1', ',', 'w', '##x', '.', 'all', '|', 'w', '##x', '.', 'expand', ',', '5', ')', '\\', 'n', '[SEP]']
Filtered   (029): ['bs', '##bu', '##tton', '##s', '.', 'add', '(', 'self', '.', 'bt', '##no', '##k', ',', '1', ',', 'w', '##x', '.', 'all', '|', 'w', '##x', '.', 'expand', ',', '5', ')', '\\', 'n']
Detokenized (021): ['bs##bu##tton##s', '.', 'add', '(', 'self', '.', 'bt##no##k', ',', '1', ',', 'w##x', '.', 'all', '|', 'w##x', '.', 'expand', ',', '5', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "mat ( op2 . INC , ( elem_node [ op2 . i [ 0 ] ] , \n"
Original    (018): ['mat', '(', 'op2', '.', 'INC', ',', '(', 'elem_node', '[', 'op2', '.', 'i', '[', '0', ']', ']', ',', '\\n']
Tokenized   (026): ['[CLS]', 'mat', '(', 'op', '##2', '.', 'inc', ',', '(', 'el', '##em', '_', 'node', '[', 'op', '##2', '.', 'i', '[', '0', ']', ']', ',', '\\', 'n', '[SEP]']
Filtered   (024): ['mat', '(', 'op', '##2', '.', 'inc', ',', '(', 'el', '##em', '_', 'node', '[', 'op', '##2', '.', 'i', '[', '0', ']', ']', ',', '\\', 'n']
Detokenized (018): ['mat', '(', 'op##2', '.', 'inc', ',', '(', 'el##em_node', '[', 'op##2', '.', 'i', '[', '0', ']', ']', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "elem_node [ op2 . i [ 1 ] ] ) ) , \n"
Original    (013): ['elem_node', '[', 'op2', '.', 'i', '[', '1', ']', ']', ')', ')', ',', '\\n']
Tokenized   (020): ['[CLS]', 'el', '##em', '_', 'node', '[', 'op', '##2', '.', 'i', '[', '1', ']', ']', ')', ')', ',', '\\', 'n', '[SEP]']
Filtered   (018): ['el', '##em', '_', 'node', '[', 'op', '##2', '.', 'i', '[', '1', ']', ']', ')', ')', ',', '\\', 'n']
Detokenized (013): ['el##em_node', '[', 'op##2', '.', 'i', '[', '1', ']', ']', ')', ')', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "partition_size = NUM_ELE / 2 , \n"
Original    (007): ['partition_size', '=', 'NUM_ELE', '/', '2', ',', '\\n']
Tokenized   (016): ['[CLS]', 'partition', '_', 'size', '=', 'nu', '##m', '_', 'el', '##e', '/', '2', ',', '\\', 'n', '[SEP]']
Filtered   (014): ['partition', '_', 'size', '=', 'nu', '##m', '_', 'el', '##e', '/', '2', ',', '\\', 'n']
Detokenized (007): ['partition_size', '=', 'nu##m_el##e', '/', '2', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "statusBarAx . barh ( [ 0 ] , [ 100.0 * expNumber / len ( data . getPaths ( ) ) ] , \n"
Original    (024): ['statusBarAx', '.', 'barh', '(', '[', '0', ']', ',', '[', '100.0', '*', 'expNumber', '/', 'len', '(', 'data', '.', 'getPaths', '(', ')', ')', ']', ',', '\\n']
Tokenized   (037): ['[CLS]', 'status', '##bara', '##x', '.', 'bar', '##h', '(', '[', '0', ']', ',', '[', '100', '.', '0', '*', 'ex', '##p', '##num', '##ber', '/', 'len', '(', 'data', '.', 'get', '##path', '##s', '(', ')', ')', ']', ',', '\\', 'n', '[SEP]']
Filtered   (035): ['status', '##bara', '##x', '.', 'bar', '##h', '(', '[', '0', ']', ',', '[', '100', '.', '0', '*', 'ex', '##p', '##num', '##ber', '/', 'len', '(', 'data', '.', 'get', '##path', '##s', '(', ')', ')', ']', ',', '\\', 'n']
Detokenized (024): ['status##bara##x', '.', 'bar##h', '(', '[', '0', ']', ',', '[', '100.0', '*', 'ex##p##num##ber', '/', 'len', '(', 'data', '.', 'get##path##s', '(', ')', ')', ']', ',', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "fluxes , errors , photFlags = photometry . multirad ( image , x , y , \n"
Original    (017): ['fluxes', ',', 'errors', ',', 'photFlags', '=', 'photometry', '.', 'multirad', '(', 'image', ',', 'x', ',', 'y', ',', '\\n']
Tokenized   (026): ['[CLS]', 'flux', '##es', ',', 'errors', ',', 'ph', '##ot', '##fl', '##ags', '=', 'photo', '##metry', '.', 'multi', '##rad', '(', 'image', ',', 'x', ',', 'y', ',', '\\', 'n', '[SEP]']
Filtered   (024): ['flux', '##es', ',', 'errors', ',', 'ph', '##ot', '##fl', '##ags', '=', 'photo', '##metry', '.', 'multi', '##rad', '(', 'image', ',', 'x', ',', 'y', ',', '\\', 'n']
Detokenized (017): ['flux##es', ',', 'errors', ',', 'ph##ot##fl##ags', '=', 'photo##metry', '.', 'multi##rad', '(', 'image', ',', 'x', ',', 'y', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "meanComparisonStars , meanComparisonStarErrors = data . calcMeanComparison_multirad ( ccdGain = data . ccdGain ) \n"
Original    (015): ['meanComparisonStars', ',', 'meanComparisonStarErrors', '=', 'data', '.', 'calcMeanComparison_multirad', '(', 'ccdGain', '=', 'data', '.', 'ccdGain', ')', '\\n']
Tokenized   (044): ['[CLS]', 'mean', '##com', '##par', '##ison', '##star', '##s', ',', 'mean', '##com', '##par', '##ison', '##star', '##er', '##ror', '##s', '=', 'data', '.', 'cal', '##cm', '##ean', '##com', '##par', '##ison', '_', 'multi', '##rad', '(', 'cc', '##d', '##gai', '##n', '=', 'data', '.', 'cc', '##d', '##gai', '##n', ')', '\\', 'n', '[SEP]']
Filtered   (042): ['mean', '##com', '##par', '##ison', '##star', '##s', ',', 'mean', '##com', '##par', '##ison', '##star', '##er', '##ror', '##s', '=', 'data', '.', 'cal', '##cm', '##ean', '##com', '##par', '##ison', '_', 'multi', '##rad', '(', 'cc', '##d', '##gai', '##n', '=', 'data', '.', 'cc', '##d', '##gai', '##n', ')', '\\', 'n']
Detokenized (015): ['mean##com##par##ison##star##s', ',', 'mean##com##par##ison##star##er##ror##s', '=', 'data', '.', 'cal##cm##ean##com##par##ison_multi##rad', '(', 'cc##d##gai##n', '=', 'data', '.', 'cc##d##gai##n', ')', '\\n']
Counter: 42
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "lightCurves , lightCurveErrors = data . computeLightCurve_multirad ( meanComparisonStars , \n"
Original    (011): ['lightCurves', ',', 'lightCurveErrors', '=', 'data', '.', 'computeLightCurve_multirad', '(', 'meanComparisonStars', ',', '\\n']
Tokenized   (035): ['[CLS]', 'light', '##cu', '##r', '##ves', ',', 'light', '##cu', '##r', '##ve', '##er', '##ror', '##s', '=', 'data', '.', 'compute', '##light', '##cu', '##r', '##ve', '_', 'multi', '##rad', '(', 'mean', '##com', '##par', '##ison', '##star', '##s', ',', '\\', 'n', '[SEP]']
Filtered   (033): ['light', '##cu', '##r', '##ves', ',', 'light', '##cu', '##r', '##ve', '##er', '##ror', '##s', '=', 'data', '.', 'compute', '##light', '##cu', '##r', '##ve', '_', 'multi', '##rad', '(', 'mean', '##com', '##par', '##ison', '##star', '##s', ',', '\\', 'n']
Detokenized (011): ['light##cu##r##ves', ',', 'light##cu##r##ve##er##ror##s', '=', 'data', '.', 'compute##light##cu##r##ve_multi##rad', '(', 'mean##com##par##ison##star##s', ',', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "json_data = self . _send_request ( , url , params = params ) \n"
Original    (014): ['json_data', '=', 'self', '.', '_send_request', '(', ',', 'url', ',', 'params', '=', 'params', ')', '\\n']
Tokenized   (026): ['[CLS]', 'j', '##son', '_', 'data', '=', 'self', '.', '_', 'send', '_', 'request', '(', ',', 'ur', '##l', ',', 'para', '##ms', '=', 'para', '##ms', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['j', '##son', '_', 'data', '=', 'self', '.', '_', 'send', '_', 'request', '(', ',', 'ur', '##l', ',', 'para', '##ms', '=', 'para', '##ms', ')', '\\', 'n']
Detokenized (014): ['j##son_data', '=', 'self', '.', '_send_request', '(', ',', 'ur##l', ',', 'para##ms', '=', 'para##ms', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "mkdir ( env . hosts_data . log_path ( ) ) \n"
Original    (011): ['mkdir', '(', 'env', '.', 'hosts_data', '.', 'log_path', '(', ')', ')', '\\n']
Tokenized   (021): ['[CLS]', 'mk', '##di', '##r', '(', 'en', '##v', '.', 'hosts', '_', 'data', '.', 'log', '_', 'path', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['mk', '##di', '##r', '(', 'en', '##v', '.', 'hosts', '_', 'data', '.', 'log', '_', 'path', '(', ')', ')', '\\', 'n']
Detokenized (011): ['mk##di##r', '(', 'en##v', '.', 'hosts_data', '.', 'log_path', '(', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "StringIO ( env . hosts_data . celery_supervisor_config ( ) ) , \n"
Original    (012): ['StringIO', '(', 'env', '.', 'hosts_data', '.', 'celery_supervisor_config', '(', ')', ')', ',', '\\n']
Tokenized   (027): ['[CLS]', 'string', '##io', '(', 'en', '##v', '.', 'hosts', '_', 'data', '.', 'ce', '##ler', '##y', '_', 'supervisor', '_', 'con', '##fi', '##g', '(', ')', ')', ',', '\\', 'n', '[SEP]']
Filtered   (025): ['string', '##io', '(', 'en', '##v', '.', 'hosts', '_', 'data', '.', 'ce', '##ler', '##y', '_', 'supervisor', '_', 'con', '##fi', '##g', '(', ')', ')', ',', '\\', 'n']
Detokenized (012): ['string##io', '(', 'en##v', '.', 'hosts_data', '.', 'ce##ler##y_supervisor_con##fi##g', '(', ')', ')', ',', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "env . hosts_data . celery_supervisor_config_path ( ) , \n"
Original    (009): ['env', '.', 'hosts_data', '.', 'celery_supervisor_config_path', '(', ')', ',', '\\n']
Tokenized   (025): ['[CLS]', 'en', '##v', '.', 'hosts', '_', 'data', '.', 'ce', '##ler', '##y', '_', 'supervisor', '_', 'con', '##fi', '##g', '_', 'path', '(', ')', ',', '\\', 'n', '[SEP]']
Filtered   (023): ['en', '##v', '.', 'hosts', '_', 'data', '.', 'ce', '##ler', '##y', '_', 'supervisor', '_', 'con', '##fi', '##g', '_', 'path', '(', ')', ',', '\\', 'n']
Detokenized (009): ['en##v', '.', 'hosts_data', '.', 'ce##ler##y_supervisor_con##fi##g_path', '(', ')', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "rmdir ( env . hosts_data . celery_supervisor_config_path ( ) , sudo_access = True ) \n"
Original    (015): ['rmdir', '(', 'env', '.', 'hosts_data', '.', 'celery_supervisor_config_path', '(', ')', ',', 'sudo_access', '=', 'True', ')', '\\n']
Tokenized   (036): ['[CLS]', 'rm', '##di', '##r', '(', 'en', '##v', '.', 'hosts', '_', 'data', '.', 'ce', '##ler', '##y', '_', 'supervisor', '_', 'con', '##fi', '##g', '_', 'path', '(', ')', ',', 'sud', '##o', '_', 'access', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (034): ['rm', '##di', '##r', '(', 'en', '##v', '.', 'hosts', '_', 'data', '.', 'ce', '##ler', '##y', '_', 'supervisor', '_', 'con', '##fi', '##g', '_', 'path', '(', ')', ',', 'sud', '##o', '_', 'access', '=', 'true', ')', '\\', 'n']
Detokenized (015): ['rm##di##r', '(', 'en##v', '.', 'hosts_data', '.', 'ce##ler##y_supervisor_con##fi##g_path', '(', ')', ',', 'sud##o_access', '=', 'true', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "sudo ( . format ( env . hosts_data . application_name ( ) ) ) \n"
Original    (015): ['sudo', '(', '.', 'format', '(', 'env', '.', 'hosts_data', '.', 'application_name', '(', ')', ')', ')', '\\n']
Tokenized   (024): ['[CLS]', 'sud', '##o', '(', '.', 'format', '(', 'en', '##v', '.', 'hosts', '_', 'data', '.', 'application', '_', 'name', '(', ')', ')', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['sud', '##o', '(', '.', 'format', '(', 'en', '##v', '.', 'hosts', '_', 'data', '.', 'application', '_', 'name', '(', ')', ')', ')', '\\', 'n']
Detokenized (015): ['sud##o', '(', '.', 'format', '(', 'en##v', '.', 'hosts_data', '.', 'application_name', '(', ')', ')', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "collection_response = SharedCollectionResponse ( json . loads ( self . send ( ) . content ) ) \n"
Original    (018): ['collection_response', '=', 'SharedCollectionResponse', '(', 'json', '.', 'loads', '(', 'self', '.', 'send', '(', ')', '.', 'content', ')', ')', '\\n']
Tokenized   (029): ['[CLS]', 'collection', '_', 'response', '=', 'shared', '##coll', '##ection', '##res', '##pon', '##se', '(', 'j', '##son', '.', 'loads', '(', 'self', '.', 'send', '(', ')', '.', 'content', ')', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['collection', '_', 'response', '=', 'shared', '##coll', '##ection', '##res', '##pon', '##se', '(', 'j', '##son', '.', 'loads', '(', 'self', '.', 'send', '(', ')', '.', 'content', ')', ')', '\\', 'n']
Detokenized (018): ['collection_response', '=', 'shared##coll##ection##res##pon##se', '(', 'j##son', '.', 'loads', '(', 'self', '.', 'send', '(', ')', '.', 'content', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "libraries = [ "sodium" ] , \n"
Original    (007): ['libraries', '=', '[', '"sodium"', ']', ',', '\\n']
Tokenized   (012): ['[CLS]', 'libraries', '=', '[', '"', 'sodium', '"', ']', ',', '\\', 'n', '[SEP]']
Filtered   (010): ['libraries', '=', '[', '"', 'sodium', '"', ']', ',', '\\', 'n']
Detokenized (007): ['libraries', '=', '[', '"sodium"', ']', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "number = 2 , type = 12 , cpp_type = 9 , label = 2 , \n"
Original    (017): ['number', '=', '2', ',', 'type', '=', '12', ',', 'cpp_type', '=', '9', ',', 'label', '=', '2', ',', '\\n']
Tokenized   (023): ['[CLS]', 'number', '=', '2', ',', 'type', '=', '12', ',', 'cp', '##p', '_', 'type', '=', '9', ',', 'label', '=', '2', ',', '\\', 'n', '[SEP]']
Filtered   (021): ['number', '=', '2', ',', 'type', '=', '12', ',', 'cp', '##p', '_', 'type', '=', '9', ',', 'label', '=', '2', ',', '\\', 'n']
Detokenized (017): ['number', '=', '2', ',', 'type', '=', '12', ',', 'cp##p_type', '=', '9', ',', 'label', '=', '2', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "has_default_value = False , default_value = _b ( "" ) , \n"
Original    (012): ['has_default_value', '=', 'False', ',', 'default_value', '=', '_b', '(', '""', ')', ',', '\\n']
Tokenized   (023): ['[CLS]', 'has', '_', 'default', '_', 'value', '=', 'false', ',', 'default', '_', 'value', '=', '_', 'b', '(', '"', '"', ')', ',', '\\', 'n', '[SEP]']
Filtered   (021): ['has', '_', 'default', '_', 'value', '=', 'false', ',', 'default', '_', 'value', '=', '_', 'b', '(', '"', '"', ')', ',', '\\', 'n']
Detokenized (012): ['has_default_value', '=', 'false', ',', 'default_value', '=', '_b', '(', '""', ')', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "PeerSeeds = _reflection . GeneratedProtocolMessageType ( , ( _message . Message , ) , dict ( \n"
Original    (017): ['PeerSeeds', '=', '_reflection', '.', 'GeneratedProtocolMessageType', '(', ',', '(', '_message', '.', 'Message', ',', ')', ',', 'dict', '(', '\\n']
Tokenized   (032): ['[CLS]', 'peers', '##eed', '##s', '=', '_', 'reflection', '.', 'generated', '##pro', '##to', '##col', '##mes', '##sa', '##get', '##ype', '(', ',', '(', '_', 'message', '.', 'message', ',', ')', ',', 'di', '##ct', '(', '\\', 'n', '[SEP]']
Filtered   (030): ['peers', '##eed', '##s', '=', '_', 'reflection', '.', 'generated', '##pro', '##to', '##col', '##mes', '##sa', '##get', '##ype', '(', ',', '(', '_', 'message', '.', 'message', ',', ')', ',', 'di', '##ct', '(', '\\', 'n']
Detokenized (017): ['peers##eed##s', '=', '_reflection', '.', 'generated##pro##to##col##mes##sa##get##ype', '(', ',', '(', '_message', '.', 'message', ',', ')', ',', 'di##ct', '(', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "__module__ = \n"
Original    (003): ['__module__', '=', '\\n']
Tokenized   (010): ['[CLS]', '_', '_', 'module', '_', '_', '=', '\\', 'n', '[SEP]']
Filtered   (008): ['_', '_', 'module', '_', '_', '=', '\\', 'n']
Detokenized (003): ['__module__', '=', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "tstream = BytearrayStream ( istream . read ( self . length ) ) \n"
Original    (014): ['tstream', '=', 'BytearrayStream', '(', 'istream', '.', 'read', '(', 'self', '.', 'length', ')', ')', '\\n']
Tokenized   (023): ['[CLS]', 'ts', '##tream', '=', 'byte', '##ar', '##ray', '##stream', '(', 'ist', '##rea', '##m', '.', 'read', '(', 'self', '.', 'length', ')', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['ts', '##tream', '=', 'byte', '##ar', '##ray', '##stream', '(', 'ist', '##rea', '##m', '.', 'read', '(', 'self', '.', 'length', ')', ')', '\\', 'n']
Detokenized (014): ['ts##tream', '=', 'byte##ar##ray##stream', '(', 'ist##rea##m', '.', 'read', '(', 'self', '.', 'length', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "opts , args = parser . parse_args ( sys . argv [ 1 : ] ) \n"
Original    (017): ['opts', ',', 'args', '=', 'parser', '.', 'parse_args', '(', 'sys', '.', 'argv', '[', '1', ':', ']', ')', '\\n']
Tokenized   (030): ['[CLS]', 'opt', '##s', ',', 'ar', '##gs', '=', 'par', '##ser', '.', 'par', '##se', '_', 'ar', '##gs', '(', 'sy', '##s', '.', 'ar', '##g', '##v', '[', '1', ':', ']', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['opt', '##s', ',', 'ar', '##gs', '=', 'par', '##ser', '.', 'par', '##se', '_', 'ar', '##gs', '(', 'sy', '##s', '.', 'ar', '##g', '##v', '[', '1', ':', ']', ')', '\\', 'n']
Detokenized (017): ['opt##s', ',', 'ar##gs', '=', 'par##ser', '.', 'par##se_ar##gs', '(', 'sy##s', '.', 'ar##g##v', '[', '1', ':', ']', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : ""{0}" . format ( uid ) ) \n"
Original    (008): ['"{0}"', '.', 'format', '(', 'uid', ')', ')', '\\n']
Tokenized   (016): ['[CLS]', '"', '{', '0', '}', '"', '.', 'format', '(', 'ui', '##d', ')', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['"', '{', '0', '}', '"', '.', 'format', '(', 'ui', '##d', ')', ')', '\\', 'n']
Detokenized (008): ['"{0}"', '.', 'format', '(', 'ui##d', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : ""located." . format ( path ) \n"
Original    (007): ['"located."', '.', 'format', '(', 'path', ')', '\\n']
Tokenized   (013): ['[CLS]', '"', 'located', '.', '"', '.', 'format', '(', 'path', ')', '\\', 'n', '[SEP]']
Filtered   (011): ['"', 'located', '.', '"', '.', 'format', '(', 'path', ')', '\\', 'n']
Detokenized (007): ['"located."', '.', 'format', '(', 'path', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "discover_versions . DiscoverVersionsResponsePayload , ** kwargs ) \n"
Original    (008): ['discover_versions', '.', 'DiscoverVersionsResponsePayload', ',', '**', 'kwargs', ')', '\\n']
Tokenized   (025): ['[CLS]', 'discover', '_', 'versions', '.', 'discover', '##version', '##sr', '##es', '##pon', '##se', '##pa', '##yl', '##oa', '##d', ',', '*', '*', 'kw', '##ar', '##gs', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['discover', '_', 'versions', '.', 'discover', '##version', '##sr', '##es', '##pon', '##se', '##pa', '##yl', '##oa', '##d', ',', '*', '*', 'kw', '##ar', '##gs', ')', '\\', 'n']
Detokenized (008): ['discover_versions', '.', 'discover##version##sr##es##pon##se##pa##yl##oa##d', ',', '**', 'kw##ar##gs', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "sqltypes . Base . metadata . create_all ( self . engine ) \n"
Original    (013): ['sqltypes', '.', 'Base', '.', 'metadata', '.', 'create_all', '(', 'self', '.', 'engine', ')', '\\n']
Tokenized   (020): ['[CLS]', 'sql', '##type', '##s', '.', 'base', '.', 'metadata', '.', 'create', '_', 'all', '(', 'self', '.', 'engine', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['sql', '##type', '##s', '.', 'base', '.', 'metadata', '.', 'create', '_', 'all', '(', 'self', '.', 'engine', ')', '\\', 'n']
Detokenized (013): ['sql##type##s', '.', 'base', '.', 'metadata', '.', 'create_all', '(', 'self', '.', 'engine', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "enums . OpaqueDataType . NONE , \n"
Original    (007): ['enums', '.', 'OpaqueDataType', '.', 'NONE', ',', '\\n']
Tokenized   (014): ['[CLS]', 'en', '##ums', '.', 'opaque', '##da', '##tat', '##ype', '.', 'none', ',', '\\', 'n', '[SEP]']
Filtered   (012): ['en', '##ums', '.', 'opaque', '##da', '##tat', '##ype', '.', 'none', ',', '\\', 'n']
Detokenized (007): ['en##ums', '.', 'opaque##da##tat##ype', '.', 'none', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "binascii . hexlify ( self . bytes_a ) , enums . OpaqueDataType . NONE ) \n"
Original    (016): ['binascii', '.', 'hexlify', '(', 'self', '.', 'bytes_a', ')', ',', 'enums', '.', 'OpaqueDataType', '.', 'NONE', ')', '\\n']
Tokenized   (031): ['[CLS]', 'bin', '##as', '##ci', '##i', '.', 'he', '##x', '##li', '##fy', '(', 'self', '.', 'bytes', '_', 'a', ')', ',', 'en', '##ums', '.', 'opaque', '##da', '##tat', '##ype', '.', 'none', ')', '\\', 'n', '[SEP]']
Filtered   (029): ['bin', '##as', '##ci', '##i', '.', 'he', '##x', '##li', '##fy', '(', 'self', '.', 'bytes', '_', 'a', ')', ',', 'en', '##ums', '.', 'opaque', '##da', '##tat', '##ype', '.', 'none', ')', '\\', 'n']
Detokenized (016): ['bin##as##ci##i', '.', 'he##x##li##fy', '(', 'self', '.', 'bytes_a', ')', ',', 'en##ums', '.', 'opaque##da##tat##ype', '.', 'none', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "Session = sessionmaker ( bind = self . engine ) \n"
Original    (011): ['Session', '=', 'sessionmaker', '(', 'bind', '=', 'self', '.', 'engine', ')', '\\n']
Tokenized   (015): ['[CLS]', 'session', '=', 'session', '##maker', '(', 'bind', '=', 'self', '.', 'engine', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['session', '=', 'session', '##maker', '(', 'bind', '=', 'self', '.', 'engine', ')', '\\', 'n']
Detokenized (011): ['session', '=', 'session##maker', '(', 'bind', '=', 'self', '.', 'engine', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "get_obj = session . query ( OpaqueObject ) . filter ( \n"
Original    (012): ['get_obj', '=', 'session', '.', 'query', '(', 'OpaqueObject', ')', '.', 'filter', '(', '\\n']
Tokenized   (020): ['[CLS]', 'get', '_', 'ob', '##j', '=', 'session', '.', 'query', '(', 'opaque', '##ob', '##ject', ')', '.', 'filter', '(', '\\', 'n', '[SEP]']
Filtered   (018): ['get', '_', 'ob', '##j', '=', 'session', '.', 'query', '(', 'opaque', '##ob', '##ject', ')', '.', 'filter', '(', '\\', 'n']
Detokenized (012): ['get_ob##j', '=', 'session', '.', 'query', '(', 'opaque##ob##ject', ')', '.', 'filter', '(', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "ManagedObject . unique_identifier == obj . unique_identifier \n"
Original    (008): ['ManagedObject', '.', 'unique_identifier', '==', 'obj', '.', 'unique_identifier', '\\n']
Tokenized   (023): ['[CLS]', 'managed', '##ob', '##ject', '.', 'unique', '_', 'id', '##ent', '##ifier', '=', '=', 'ob', '##j', '.', 'unique', '_', 'id', '##ent', '##ifier', '\\', 'n', '[SEP]']
Filtered   (021): ['managed', '##ob', '##ject', '.', 'unique', '_', 'id', '##ent', '##ifier', '=', '=', 'ob', '##j', '.', 'unique', '_', 'id', '##ent', '##ifier', '\\', 'n']
Detokenized (008): ['managed##ob##ject', '.', 'unique_id##ent##ifier', '==', 'ob##j', '.', 'unique_id##ent##ifier', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "0 ) ) \n"
Original    (004): ['0', ')', ')', '\\n']
Tokenized   (007): ['[CLS]', '0', ')', ')', '\\', 'n', '[SEP]']
Filtered   (005): ['0', ')', ')', '\\', 'n']
Detokenized (004): ['0', ')', ')', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "expected_mo_names . append ( sqltypes . ManagedObjectName ( expected_names [ 1 ] , \n"
Original    (014): ['expected_mo_names', '.', 'append', '(', 'sqltypes', '.', 'ManagedObjectName', '(', 'expected_names', '[', '1', ']', ',', '\\n']
Tokenized   (029): ['[CLS]', 'expected', '_', 'mo', '_', 'names', '.', 'app', '##end', '(', 'sql', '##type', '##s', '.', 'managed', '##ob', '##ject', '##name', '(', 'expected', '_', 'names', '[', '1', ']', ',', '\\', 'n', '[SEP]']
Filtered   (027): ['expected', '_', 'mo', '_', 'names', '.', 'app', '##end', '(', 'sql', '##type', '##s', '.', 'managed', '##ob', '##ject', '##name', '(', 'expected', '_', 'names', '[', '1', ']', ',', '\\', 'n']
Detokenized (014): ['expected_mo_names', '.', 'app##end', '(', 'sql##type##s', '.', 'managed##ob##ject##name', '(', 'expected_names', '[', '1', ']', ',', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "expected_names = [ first_name , added_name ] \n"
Original    (008): ['expected_names', '=', '[', 'first_name', ',', 'added_name', ']', '\\n']
Tokenized   (017): ['[CLS]', 'expected', '_', 'names', '=', '[', 'first', '_', 'name', ',', 'added', '_', 'name', ']', '\\', 'n', '[SEP]']
Filtered   (015): ['expected', '_', 'names', '=', '[', 'first', '_', 'name', ',', 'added', '_', 'name', ']', '\\', 'n']
Detokenized (008): ['expected_names', '=', '[', 'first_name', ',', 'added_name', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "] } , \n"
Original    (004): [']', '}', ',', '\\n']
Tokenized   (007): ['[CLS]', ']', '}', ',', '\\', 'n', '[SEP]']
Filtered   (005): [']', '}', ',', '\\', 'n']
Detokenized (004): [']', '}', ',', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "types . MethodType ( _lib_dir_option , None , MSVCCompiler ) ) \n"
Original    (012): ['types', '.', 'MethodType', '(', '_lib_dir_option', ',', 'None', ',', 'MSVCCompiler', ')', ')', '\\n']
Tokenized   (026): ['[CLS]', 'types', '.', 'method', '##type', '(', '_', 'li', '##b', '_', 'dir', '_', 'option', ',', 'none', ',', 'ms', '##vc', '##com', '##pile', '##r', ')', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['types', '.', 'method', '##type', '(', '_', 'li', '##b', '_', 'dir', '_', 'option', ',', 'none', ',', 'ms', '##vc', '##com', '##pile', '##r', ')', ')', '\\', 'n']
Detokenized (012): ['types', '.', 'method##type', '(', '_li##b_dir_option', ',', 'none', ',', 'ms##vc##com##pile##r', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "setup ( ** kwds ) \n"
Original    (006): ['setup', '(', '**', 'kwds', ')', '\\n']
Tokenized   (011): ['[CLS]', 'setup', '(', '*', '*', 'kw', '##ds', ')', '\\', 'n', '[SEP]']
Filtered   (009): ['setup', '(', '*', '*', 'kw', '##ds', ')', '\\', 'n']
Detokenized (006): ['setup', '(', '**', 'kw##ds', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "intersphinx_mapping = { : None } \n"
Original    (007): ['intersphinx_mapping', '=', '{', ':', 'None', '}', '\\n']
Tokenized   (015): ['[CLS]', 'inter', '##sp', '##hin', '##x', '_', 'mapping', '=', '{', ':', 'none', '}', '\\', 'n', '[SEP]']
Filtered   (013): ['inter', '##sp', '##hin', '##x', '_', 'mapping', '=', '{', ':', 'none', '}', '\\', 'n']
Detokenized (007): ['inter##sp##hin##x_mapping', '=', '{', ':', 'none', '}', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "new_w = int ( width * wrat ) \n"
Original    (009): ['new_w', '=', 'int', '(', 'width', '*', 'wrat', ')', '\\n']
Tokenized   (015): ['[CLS]', 'new', '_', 'w', '=', 'int', '(', 'width', '*', 'wr', '##at', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['new', '_', 'w', '=', 'int', '(', 'width', '*', 'wr', '##at', ')', '\\', 'n']
Detokenized (009): ['new_w', '=', 'int', '(', 'width', '*', 'wr##at', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "im . getbbox ( ) , Image . BICUBIC ) \n"
Original    (011): ['im', '.', 'getbbox', '(', ')', ',', 'Image', '.', 'BICUBIC', ')', '\\n']
Tokenized   (018): ['[CLS]', 'im', '.', 'get', '##bb', '##ox', '(', ')', ',', 'image', '.', 'bi', '##cu', '##bic', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['im', '.', 'get', '##bb', '##ox', '(', ')', ',', 'image', '.', 'bi', '##cu', '##bic', ')', '\\', 'n']
Detokenized (011): ['im', '.', 'get##bb##ox', '(', ')', ',', 'image', '.', 'bi##cu##bic', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "resize_image ( os . path . abspath ( os . path . join ( , dest + ) ) ) \n"
Original    (021): ['resize_image', '(', 'os', '.', 'path', '.', 'abspath', '(', 'os', '.', 'path', '.', 'join', '(', ',', 'dest', '+', ')', ')', ')', '\\n']
Tokenized   (029): ['[CLS]', 'res', '##ize', '_', 'image', '(', 'os', '.', 'path', '.', 'abs', '##path', '(', 'os', '.', 'path', '.', 'join', '(', ',', 'des', '##t', '+', ')', ')', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['res', '##ize', '_', 'image', '(', 'os', '.', 'path', '.', 'abs', '##path', '(', 'os', '.', 'path', '.', 'join', '(', ',', 'des', '##t', '+', ')', ')', ')', '\\', 'n']
Detokenized (021): ['res##ize_image', '(', 'os', '.', 'path', '.', 'abs##path', '(', 'os', '.', 'path', '.', 'join', '(', ',', 'des##t', '+', ')', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "f_x = Float ( 0.0 , iotype = "out" ) \n"
Original    (011): ['f_x', '=', 'Float', '(', '0.0', ',', 'iotype', '=', '"out"', ')', '\\n']
Tokenized   (021): ['[CLS]', 'f', '_', 'x', '=', 'float', '(', '0', '.', '0', ',', 'io', '##type', '=', '"', 'out', '"', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['f', '_', 'x', '=', 'float', '(', '0', '.', '0', ',', 'io', '##type', '=', '"', 'out', '"', ')', '\\', 'n']
Detokenized (011): ['f_x', '=', 'float', '(', '0.0', ',', 'io##type', '=', '"out"', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "doe_c = [ 0.1 , 0.2 , 0.3 , 0.5 , 0.7 , 0.8 , 0.9 ] + doe_e \n"
Original    (020): ['doe_c', '=', '[', '0.1', ',', '0.2', ',', '0.3', ',', '0.5', ',', '0.7', ',', '0.8', ',', '0.9', ']', '+', 'doe_e', '\\n']
Tokenized   (041): ['[CLS]', 'doe', '_', 'c', '=', '[', '0', '.', '1', ',', '0', '.', '2', ',', '0', '.', '3', ',', '0', '.', '5', ',', '0', '.', '7', ',', '0', '.', '8', ',', '0', '.', '9', ']', '+', 'doe', '_', 'e', '\\', 'n', '[SEP]']
Filtered   (039): ['doe', '_', 'c', '=', '[', '0', '.', '1', ',', '0', '.', '2', ',', '0', '.', '3', ',', '0', '.', '5', ',', '0', '.', '7', ',', '0', '.', '8', ',', '0', '.', '9', ']', '+', 'doe', '_', 'e', '\\', 'n']
Detokenized (020): ['doe_c', '=', '[', '0.1', ',', '0.2', ',', '0.3', ',', '0.5', ',', '0.7', ',', '0.8', ',', '0.9', ']', '+', 'doe_e', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "responses = ( , ) , nfi = self . nfi ) ) \n"
Original    (014): ['responses', '=', '(', ',', ')', ',', 'nfi', '=', 'self', '.', 'nfi', ')', ')', '\\n']
Tokenized   (019): ['[CLS]', 'responses', '=', '(', ',', ')', ',', 'n', '##fi', '=', 'self', '.', 'n', '##fi', ')', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['responses', '=', '(', ',', ')', ',', 'n', '##fi', '=', 'self', '.', 'n', '##fi', ')', ')', '\\', 'n']
Detokenized (014): ['responses', '=', '(', ',', ')', ',', 'n##fi', '=', 'self', '.', 'n##fi', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "sigma_cok = np . array ( [ d . sigma for d in sim_cok . mm_checker . case_outputs . meta_model . f_x ] ) \n"
Original    (025): ['sigma_cok', '=', 'np', '.', 'array', '(', '[', 'd', '.', 'sigma', 'for', 'd', 'in', 'sim_cok', '.', 'mm_checker', '.', 'case_outputs', '.', 'meta_model', '.', 'f_x', ']', ')', '\\n']
Tokenized   (043): ['[CLS]', 'sigma', '_', 'co', '##k', '=', 'np', '.', 'array', '(', '[', 'd', '.', 'sigma', 'for', 'd', 'in', 'sim', '_', 'co', '##k', '.', 'mm', '_', 'check', '##er', '.', 'case', '_', 'outputs', '.', 'meta', '_', 'model', '.', 'f', '_', 'x', ']', ')', '\\', 'n', '[SEP]']
Filtered   (041): ['sigma', '_', 'co', '##k', '=', 'np', '.', 'array', '(', '[', 'd', '.', 'sigma', 'for', 'd', 'in', 'sim', '_', 'co', '##k', '.', 'mm', '_', 'check', '##er', '.', 'case', '_', 'outputs', '.', 'meta', '_', 'model', '.', 'f', '_', 'x', ']', ')', '\\', 'n']
Detokenized (025): ['sigma_co##k', '=', 'np', '.', 'array', '(', '[', 'd', '.', 'sigma', 'for', 'd', 'in', 'sim_co##k', '.', 'mm_check##er', '.', 'case_outputs', '.', 'meta_model', '.', 'f_x', ']', ')', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "actual = sim_k . mm_checker . case_outputs . model . f_x \n"
Original    (012): ['actual', '=', 'sim_k', '.', 'mm_checker', '.', 'case_outputs', '.', 'model', '.', 'f_x', '\\n']
Tokenized   (024): ['[CLS]', 'actual', '=', 'sim', '_', 'k', '.', 'mm', '_', 'check', '##er', '.', 'case', '_', 'outputs', '.', 'model', '.', 'f', '_', 'x', '\\', 'n', '[SEP]']
Filtered   (022): ['actual', '=', 'sim', '_', 'k', '.', 'mm', '_', 'check', '##er', '.', 'case', '_', 'outputs', '.', 'model', '.', 'f', '_', 'x', '\\', 'n']
Detokenized (012): ['actual', '=', 'sim_k', '.', 'mm_check##er', '.', 'case_outputs', '.', 'model', '.', 'f_x', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "predicted_cok - 2 * sigma_cok , facecolor = , alpha = 0.2 ) \n"
Original    (014): ['predicted_cok', '-', '2', '*', 'sigma_cok', ',', 'facecolor', '=', ',', 'alpha', '=', '0.2', ')', '\\n']
Tokenized   (026): ['[CLS]', 'predicted', '_', 'co', '##k', '-', '2', '*', 'sigma', '_', 'co', '##k', ',', 'face', '##color', '=', ',', 'alpha', '=', '0', '.', '2', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['predicted', '_', 'co', '##k', '-', '2', '*', 'sigma', '_', 'co', '##k', ',', 'face', '##color', '=', ',', 'alpha', '=', '0', '.', '2', ')', '\\', 'n']
Detokenized (014): ['predicted_co##k', '-', '2', '*', 'sigma_co##k', ',', 'face##color', '=', ',', 'alpha', '=', '0.2', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "newsetupfile = os . path . join ( os . path . dirname ( setupfile ) , \n"
Original    (018): ['newsetupfile', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dirname', '(', 'setupfile', ')', ',', '\\n']
Tokenized   (028): ['[CLS]', 'news', '##et', '##up', '##fi', '##le', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dir', '##name', '(', 'setup', '##fi', '##le', ')', ',', '\\', 'n', '[SEP]']
Filtered   (026): ['news', '##et', '##up', '##fi', '##le', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dir', '##name', '(', 'setup', '##fi', '##le', ')', ',', '\\', 'n']
Detokenized (018): ['news##et##up##fi##le', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dir##name', '(', 'setup##fi##le', ')', ',', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "srcdir = os . path . abspath ( os . path . expanduser ( srcdir ) ) . replace ( , ) \n"
Original    (023): ['srcdir', '=', 'os', '.', 'path', '.', 'abspath', '(', 'os', '.', 'path', '.', 'expanduser', '(', 'srcdir', ')', ')', '.', 'replace', '(', ',', ')', '\\n']
Tokenized   (032): ['[CLS]', 'sr', '##cd', '##ir', '=', 'os', '.', 'path', '.', 'abs', '##path', '(', 'os', '.', 'path', '.', 'expand', '##user', '(', 'sr', '##cd', '##ir', ')', ')', '.', 'replace', '(', ',', ')', '\\', 'n', '[SEP]']
Filtered   (030): ['sr', '##cd', '##ir', '=', 'os', '.', 'path', '.', 'abs', '##path', '(', 'os', '.', 'path', '.', 'expand', '##user', '(', 'sr', '##cd', '##ir', ')', ')', '.', 'replace', '(', ',', ')', '\\', 'n']
Detokenized (023): ['sr##cd##ir', '=', 'os', '.', 'path', '.', 'abs##path', '(', 'os', '.', 'path', '.', 'expand##user', '(', 'sr##cd##ir', ')', ')', '.', 'replace', '(', ',', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "cmd . extend ( [ , destdir ] ) \n"
Original    (010): ['cmd', '.', 'extend', '(', '[', ',', 'destdir', ']', ')', '\\n']
Tokenized   (017): ['[CLS]', 'cm', '##d', '.', 'extend', '(', '[', ',', 'des', '##t', '##di', '##r', ']', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['cm', '##d', '.', 'extend', '(', '[', ',', 'des', '##t', '##di', '##r', ']', ')', '\\', 'n']
Detokenized (010): ['cm##d', '.', 'extend', '(', '[', ',', 'des##t##di##r', ']', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "list ( newfiles ) ) \n"
Original    (006): ['list', '(', 'newfiles', ')', ')', '\\n']
Tokenized   (011): ['[CLS]', 'list', '(', 'new', '##fi', '##les', ')', ')', '\\', 'n', '[SEP]']
Filtered   (009): ['list', '(', 'new', '##fi', '##les', ')', ')', '\\', 'n']
Detokenized (006): ['list', '(', 'new##fi##les', ')', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "destdir = os . path . abspath ( os . path . expanduser ( options . destdir ) ) \n"
Original    (020): ['destdir', '=', 'os', '.', 'path', '.', 'abspath', '(', 'os', '.', 'path', '.', 'expanduser', '(', 'options', '.', 'destdir', ')', ')', '\\n']
Tokenized   (031): ['[CLS]', 'des', '##t', '##di', '##r', '=', 'os', '.', 'path', '.', 'abs', '##path', '(', 'os', '.', 'path', '.', 'expand', '##user', '(', 'options', '.', 'des', '##t', '##di', '##r', ')', ')', '\\', 'n', '[SEP]']
Filtered   (029): ['des', '##t', '##di', '##r', '=', 'os', '.', 'path', '.', 'abs', '##path', '(', 'os', '.', 'path', '.', 'expand', '##user', '(', 'options', '.', 'des', '##t', '##di', '##r', ')', ')', '\\', 'n']
Detokenized (020): ['des##t##di##r', '=', 'os', '.', 'path', '.', 'abs##path', '(', 'os', '.', 'path', '.', 'expand##user', '(', 'options', '.', 'des##t##di##r', ')', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "all_names . extend ( [ prefix + name \n"
Original    (009): ['all_names', '.', 'extend', '(', '[', 'prefix', '+', 'name', '\\n']
Tokenized   (014): ['[CLS]', 'all', '_', 'names', '.', 'extend', '(', '[', 'prefix', '+', 'name', '\\', 'n', '[SEP]']
Filtered   (012): ['all', '_', 'names', '.', 'extend', '(', '[', 'prefix', '+', 'name', '\\', 'n']
Detokenized (009): ['all_names', '.', 'extend', '(', '[', 'prefix', '+', 'name', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "lnames = [ prefix + rec for rec in driver [ ] ] \n"
Original    (014): ['lnames', '=', '[', 'prefix', '+', 'rec', 'for', 'rec', 'in', 'driver', '[', ']', ']', '\\n']
Tokenized   (019): ['[CLS]', 'l', '##name', '##s', '=', '[', 'prefix', '+', 'rec', 'for', 'rec', 'in', 'driver', '[', ']', ']', '\\', 'n', '[SEP]']
Filtered   (017): ['l', '##name', '##s', '=', '[', 'prefix', '+', 'rec', 'for', 'rec', 'in', 'driver', '[', ']', ']', '\\', 'n']
Detokenized (014): ['l##name##s', '=', '[', 'prefix', '+', 'rec', 'for', 'rec', 'in', 'driver', '[', ']', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "driver_grp = self . _inp [ ] [ driver_name ] \n"
Original    (011): ['driver_grp', '=', 'self', '.', '_inp', '[', ']', '[', 'driver_name', ']', '\\n']
Tokenized   (021): ['[CLS]', 'driver', '_', 'gr', '##p', '=', 'self', '.', '_', 'in', '##p', '[', ']', '[', 'driver', '_', 'name', ']', '\\', 'n', '[SEP]']
Filtered   (019): ['driver', '_', 'gr', '##p', '=', 'self', '.', '_', 'in', '##p', '[', ']', '[', 'driver', '_', 'name', ']', '\\', 'n']
Detokenized (011): ['driver_gr##p', '=', 'self', '.', '_in##p', '[', ']', '[', 'driver_name', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "iteration_grp = self . _inp [ ] [ driver_name ] [ iteration_case_name ] \n"
Original    (014): ['iteration_grp', '=', 'self', '.', '_inp', '[', ']', '[', 'driver_name', ']', '[', 'iteration_case_name', ']', '\\n']
Tokenized   (028): ['[CLS]', 'iteration', '_', 'gr', '##p', '=', 'self', '.', '_', 'in', '##p', '[', ']', '[', 'driver', '_', 'name', ']', '[', 'iteration', '_', 'case', '_', 'name', ']', '\\', 'n', '[SEP]']
Filtered   (026): ['iteration', '_', 'gr', '##p', '=', 'self', '.', '_', 'in', '##p', '[', ']', '[', 'driver', '_', 'name', ']', '[', 'iteration', '_', 'case', '_', 'name', ']', '\\', 'n']
Detokenized (014): ['iteration_gr##p', '=', 'self', '.', '_in##p', '[', ']', '[', 'driver_name', ']', '[', 'iteration_case_name', ']', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "info = self . read_iteration_case_from_hdf5 ( self . _inp , driver_name , iteration_case_name ) yield info \n"
Original    (017): ['info', '=', 'self', '.', 'read_iteration_case_from_hdf5', '(', 'self', '.', '_inp', ',', 'driver_name', ',', 'iteration_case_name', ')', 'yield', 'info', '\\n']
Tokenized   (038): ['[CLS]', 'info', '=', 'self', '.', 'read', '_', 'iteration', '_', 'case', '_', 'from', '_', 'hd', '##f', '##5', '(', 'self', '.', '_', 'in', '##p', ',', 'driver', '_', 'name', ',', 'iteration', '_', 'case', '_', 'name', ')', 'yield', 'info', '\\', 'n', '[SEP]']
Filtered   (036): ['info', '=', 'self', '.', 'read', '_', 'iteration', '_', 'case', '_', 'from', '_', 'hd', '##f', '##5', '(', 'self', '.', '_', 'in', '##p', ',', 'driver', '_', 'name', ',', 'iteration', '_', 'case', '_', 'name', ')', 'yield', 'info', '\\', 'n']
Detokenized (017): ['info', '=', 'self', '.', 'read_iteration_case_from_hd##f##5', '(', 'self', '.', '_in##p', ',', 'driver_name', ',', 'iteration_case_name', ')', 'yield', 'info', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "sleep_time = Float ( 0.0 , iotype = , desc = ) \n"
Original    (013): ['sleep_time', '=', 'Float', '(', '0.0', ',', 'iotype', '=', ',', 'desc', '=', ')', '\\n']
Tokenized   (022): ['[CLS]', 'sleep', '_', 'time', '=', 'float', '(', '0', '.', '0', ',', 'io', '##type', '=', ',', 'des', '##c', '=', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['sleep', '_', 'time', '=', 'float', '(', '0', '.', '0', ',', 'io', '##type', '=', ',', 'des', '##c', '=', ')', '\\', 'n']
Detokenized (013): ['sleep_time', '=', 'float', '(', '0.0', ',', 'io##type', '=', ',', 'des##c', '=', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "accuracy = Float ( 1.0e-6 , iotype = , \n"
Original    (010): ['accuracy', '=', 'Float', '(', '1.0e-6', ',', 'iotype', '=', ',', '\\n']
Tokenized   (019): ['[CLS]', 'accuracy', '=', 'float', '(', '1', '.', '0', '##e', '-', '6', ',', 'io', '##type', '=', ',', '\\', 'n', '[SEP]']
Filtered   (017): ['accuracy', '=', 'float', '(', '1', '.', '0', '##e', '-', '6', ',', 'io', '##type', '=', ',', '\\', 'n']
Detokenized (010): ['accuracy', '=', 'float', '(', '1.0##e-6', ',', 'io##type', '=', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "iprint = Enum ( 0 , [ 0 , 1 , 2 , 3 ] , iotype = , \n"
Original    (020): ['iprint', '=', 'Enum', '(', '0', ',', '[', '0', ',', '1', ',', '2', ',', '3', ']', ',', 'iotype', '=', ',', '\\n']
Tokenized   (027): ['[CLS]', 'ip', '##rin', '##t', '=', 'en', '##um', '(', '0', ',', '[', '0', ',', '1', ',', '2', ',', '3', ']', ',', 'io', '##type', '=', ',', '\\', 'n', '[SEP]']
Filtered   (025): ['ip', '##rin', '##t', '=', 'en', '##um', '(', '0', ',', '[', '0', ',', '1', ',', '2', ',', '3', ']', ',', 'io', '##type', '=', ',', '\\', 'n']
Detokenized (020): ['ip##rin##t', '=', 'en##um', '(', '0', ',', '[', '0', ',', '1', ',', '2', ',', '3', ']', ',', 'io##type', '=', ',', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "output_filename = Str ( , iotype = , \n"
Original    (009): ['output_filename', '=', 'Str', '(', ',', 'iotype', '=', ',', '\\n']
Tokenized   (017): ['[CLS]', 'output', '_', 'file', '##name', '=', 'st', '##r', '(', ',', 'io', '##type', '=', ',', '\\', 'n', '[SEP]']
Filtered   (015): ['output', '_', 'file', '##name', '=', 'st', '##r', '(', ',', 'io', '##type', '=', ',', '\\', 'n']
Detokenized (009): ['output_file##name', '=', 'st##r', '(', ',', 'io##type', '=', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "la = max ( m , 1 ) \n"
Original    (009): ['la', '=', 'max', '(', 'm', ',', '1', ')', '\\n']
Tokenized   (012): ['[CLS]', 'la', '=', 'max', '(', 'm', ',', '1', ')', '\\', 'n', '[SEP]']
Filtered   (010): ['la', '=', 'max', '(', 'm', ',', '1', ')', '\\', 'n']
Detokenized (009): ['la', '=', 'max', '(', 'm', ',', '1', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "gg = zeros ( [ la ] , ) \n"
Original    (010): ['gg', '=', 'zeros', '(', '[', 'la', ']', ',', ')', '\\n']
Tokenized   (015): ['[CLS]', 'g', '##g', '=', 'zero', '##s', '(', '[', 'la', ']', ',', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['g', '##g', '=', 'zero', '##s', '(', '[', 'la', ']', ',', ')', '\\', 'n']
Detokenized (010): ['g##g', '=', 'zero##s', '(', '[', 'la', ']', ',', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "dg = zeros ( [ la , n + 1 ] , ) \n"
Original    (014): ['dg', '=', 'zeros', '(', '[', 'la', ',', 'n', '+', '1', ']', ',', ')', '\\n']
Tokenized   (019): ['[CLS]', 'd', '##g', '=', 'zero', '##s', '(', '[', 'la', ',', 'n', '+', '1', ']', ',', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['d', '##g', '=', 'zero', '##s', '(', '[', 'la', ',', 'n', '+', '1', ']', ',', ')', '\\', 'n']
Detokenized (014): ['d##g', '=', 'zero##s', '(', '[', 'la', ',', 'n', '+', '1', ']', ',', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "mineq = m - meq + 2 * ( n + 1 ) \n"
Original    (014): ['mineq', '=', 'm', '-', 'meq', '+', '2', '*', '(', 'n', '+', '1', ')', '\\n']
Tokenized   (019): ['[CLS]', 'mine', '##q', '=', 'm', '-', 'me', '##q', '+', '2', '*', '(', 'n', '+', '1', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['mine', '##q', '=', 'm', '-', 'me', '##q', '+', '2', '*', '(', 'n', '+', '1', ')', '\\', 'n']
Detokenized (014): ['mine##q', '=', 'm', '-', 'me##q', '+', '2', '*', '(', 'n', '+', '1', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "lsq = ( n + 1 ) * ( ( n + 1 ) + 1 ) + meq * ( ( n + 1 ) + 1 ) + mineq * ( ( n + 1 ) + 1 ) \n"
Original    (042): ['lsq', '=', '(', 'n', '+', '1', ')', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'meq', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'mineq', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '\\n']
Tokenized   (049): ['[CLS]', 'l', '##s', '##q', '=', '(', 'n', '+', '1', ')', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'me', '##q', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'mine', '##q', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '\\', 'n', '[SEP]']
Filtered   (047): ['l', '##s', '##q', '=', '(', 'n', '+', '1', ')', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'me', '##q', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'mine', '##q', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '\\', 'n']
Detokenized (042): ['l##s##q', '=', '(', 'n', '+', '1', ')', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'me##q', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'mine##q', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '\\n']
Counter: 47
===================================================================
Hidden states:  (13, 42, 768)
# Extracted words:  42
Sentence         : "lsi = ( ( n + 1 ) - meq + 1 ) * ( mineq + 2 ) + 2 * mineq \n"
Original    (024): ['lsi', '=', '(', '(', 'n', '+', '1', ')', '-', 'meq', '+', '1', ')', '*', '(', 'mineq', '+', '2', ')', '+', '2', '*', 'mineq', '\\n']
Tokenized   (031): ['[CLS]', 'l', '##si', '=', '(', '(', 'n', '+', '1', ')', '-', 'me', '##q', '+', '1', ')', '*', '(', 'mine', '##q', '+', '2', ')', '+', '2', '*', 'mine', '##q', '\\', 'n', '[SEP]']
Filtered   (029): ['l', '##si', '=', '(', '(', 'n', '+', '1', ')', '-', 'me', '##q', '+', '1', ')', '*', '(', 'mine', '##q', '+', '2', ')', '+', '2', '*', 'mine', '##q', '\\', 'n']
Detokenized (024): ['l##si', '=', '(', '(', 'n', '+', '1', ')', '-', 'me##q', '+', '1', ')', '*', '(', 'mine##q', '+', '2', ')', '+', '2', '*', 'mine##q', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "lsei = ( ( n + 1 ) + mineq ) * ( ( n + 1 ) - meq ) + 2 * meq + ( n + 1 ) \n"
Original    (032): ['lsei', '=', '(', '(', 'n', '+', '1', ')', '+', 'mineq', ')', '*', '(', '(', 'n', '+', '1', ')', '-', 'meq', ')', '+', '2', '*', 'meq', '+', '(', 'n', '+', '1', ')', '\\n']
Tokenized   (039): ['[CLS]', 'l', '##sei', '=', '(', '(', 'n', '+', '1', ')', '+', 'mine', '##q', ')', '*', '(', '(', 'n', '+', '1', ')', '-', 'me', '##q', ')', '+', '2', '*', 'me', '##q', '+', '(', 'n', '+', '1', ')', '\\', 'n', '[SEP]']
Filtered   (037): ['l', '##sei', '=', '(', '(', 'n', '+', '1', ')', '+', 'mine', '##q', ')', '*', '(', '(', 'n', '+', '1', ')', '-', 'me', '##q', ')', '+', '2', '*', 'me', '##q', '+', '(', 'n', '+', '1', ')', '\\', 'n']
Detokenized (032): ['l##sei', '=', '(', '(', 'n', '+', '1', ')', '+', 'mine##q', ')', '*', '(', '(', 'n', '+', '1', ')', '-', 'me##q', ')', '+', '2', '*', 'me##q', '+', '(', 'n', '+', '1', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "slsqpb = ( n + 1 ) * ( n / 2 ) + 2 * m + 3 * n + 3 * ( n + 1 ) + 1 \n"
Original    (032): ['slsqpb', '=', '(', 'n', '+', '1', ')', '*', '(', 'n', '/', '2', ')', '+', '2', '*', 'm', '+', '3', '*', 'n', '+', '3', '*', '(', 'n', '+', '1', ')', '+', '1', '\\n']
Tokenized   (039): ['[CLS]', 'sl', '##s', '##q', '##p', '##b', '=', '(', 'n', '+', '1', ')', '*', '(', 'n', '/', '2', ')', '+', '2', '*', 'm', '+', '3', '*', 'n', '+', '3', '*', '(', 'n', '+', '1', ')', '+', '1', '\\', 'n', '[SEP]']
Filtered   (037): ['sl', '##s', '##q', '##p', '##b', '=', '(', 'n', '+', '1', ')', '*', '(', 'n', '/', '2', ')', '+', '2', '*', 'm', '+', '3', '*', 'n', '+', '3', '*', '(', 'n', '+', '1', ')', '+', '1', '\\', 'n']
Detokenized (032): ['sl##s##q##p##b', '=', '(', 'n', '+', '1', ')', '*', '(', 'n', '/', '2', ')', '+', '2', '*', 'm', '+', '3', '*', 'n', '+', '3', '*', '(', 'n', '+', '1', ')', '+', '1', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "lw = lsq + lsi + lsei + slsqpb + n + m \n"
Original    (014): ['lw', '=', 'lsq', '+', 'lsi', '+', 'lsei', '+', 'slsqpb', '+', 'n', '+', 'm', '\\n']
Tokenized   (026): ['[CLS]', 'l', '##w', '=', 'l', '##s', '##q', '+', 'l', '##si', '+', 'l', '##sei', '+', 'sl', '##s', '##q', '##p', '##b', '+', 'n', '+', 'm', '\\', 'n', '[SEP]']
Filtered   (024): ['l', '##w', '=', 'l', '##s', '##q', '+', 'l', '##si', '+', 'l', '##sei', '+', 'sl', '##s', '##q', '##p', '##b', '+', 'n', '+', 'm', '\\', 'n']
Detokenized (014): ['l##w', '=', 'l##s##q', '+', 'l##si', '+', 'l##sei', '+', 'sl##s##q##p##b', '+', 'n', '+', 'm', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "ljw = max ( mineq , ( n + 1 ) - meq ) \n"
Original    (015): ['ljw', '=', 'max', '(', 'mineq', ',', '(', 'n', '+', '1', ')', '-', 'meq', ')', '\\n']
Tokenized   (022): ['[CLS]', 'l', '##j', '##w', '=', 'max', '(', 'mine', '##q', ',', '(', 'n', '+', '1', ')', '-', 'me', '##q', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['l', '##j', '##w', '=', 'max', '(', 'mine', '##q', ',', '(', 'n', '+', '1', ')', '-', 'me', '##q', ')', '\\', 'n']
Detokenized (015): ['l##j##w', '=', 'max', '(', 'mine##q', ',', '(', 'n', '+', '1', ')', '-', 'me##q', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "_iodict = { : , : } \n"
Original    (008): ['_iodict', '=', '{', ':', ',', ':', '}', '\\n']
Tokenized   (013): ['[CLS]', '_', 'io', '##dict', '=', '{', ':', ',', ':', '}', '\\', 'n', '[SEP]']
Filtered   (011): ['_', 'io', '##dict', '=', '{', ':', ',', ':', '}', '\\', 'n']
Detokenized (008): ['_io##dict', '=', '{', ':', ',', ':', '}', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "state [ ] = { } \n"
Original    (007): ['state', '[', ']', '=', '{', '}', '\\n']
Tokenized   (010): ['[CLS]', 'state', '[', ']', '=', '{', '}', '\\', 'n', '[SEP]']
Filtered   (008): ['state', '[', ']', '=', '{', '}', '\\', 'n']
Detokenized (007): ['state', '[', ']', '=', '{', '}', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "key = ( addr_type , addr , proxy . _authkey ) \n"
Original    (012): ['key', '=', '(', 'addr_type', ',', 'addr', ',', 'proxy', '.', '_authkey', ')', '\\n']
Tokenized   (022): ['[CLS]', 'key', '=', '(', 'add', '##r', '_', 'type', ',', 'add', '##r', ',', 'proxy', '.', '_', 'au', '##th', '##key', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['key', '=', '(', 'add', '##r', '_', 'type', ',', 'add', '##r', ',', 'proxy', '.', '_', 'au', '##th', '##key', ')', '\\', 'n']
Detokenized (012): ['key', '=', '(', 'add##r_type', ',', 'add##r', ',', 'proxy', '.', '_au##th##key', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "address = ( ip_addr , 0 ) \n"
Original    (008): ['address', '=', '(', 'ip_addr', ',', '0', ')', '\\n']
Tokenized   (014): ['[CLS]', 'address', '=', '(', 'ip', '_', 'add', '##r', ',', '0', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['address', '=', '(', 'ip', '_', 'add', '##r', ',', '0', ')', '\\', 'n']
Detokenized (008): ['address', '=', '(', 'ip_add##r', ',', '0', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "access = addr if addr_type == else addr_type \n"
Original    (009): ['access', '=', 'addr', 'if', 'addr_type', '==', 'else', 'addr_type', '\\n']
Tokenized   (020): ['[CLS]', 'access', '=', 'add', '##r', 'if', 'add', '##r', '_', 'type', '=', '=', 'else', 'add', '##r', '_', 'type', '\\', 'n', '[SEP]']
Filtered   (018): ['access', '=', 'add', '##r', 'if', 'add', '##r', '_', 'type', '=', '=', 'else', 'add', '##r', '_', 'type', '\\', 'n']
Detokenized (009): ['access', '=', 'add##r', 'if', 'add##r_type', '==', 'else', 'add##r_type', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "manager = ObjectManager ( self , address , authkey = proxy . _authkey , \n"
Original    (015): ['manager', '=', 'ObjectManager', '(', 'self', ',', 'address', ',', 'authkey', '=', 'proxy', '.', '_authkey', ',', '\\n']
Tokenized   (025): ['[CLS]', 'manager', '=', 'object', '##mana', '##ger', '(', 'self', ',', 'address', ',', 'au', '##th', '##key', '=', 'proxy', '.', '_', 'au', '##th', '##key', ',', '\\', 'n', '[SEP]']
Filtered   (023): ['manager', '=', 'object', '##mana', '##ger', '(', 'self', ',', 'address', ',', 'au', '##th', '##key', '=', 'proxy', '.', '_', 'au', '##th', '##key', ',', '\\', 'n']
Detokenized (015): ['manager', '=', 'object##mana##ger', '(', 'self', ',', 'address', ',', 'au##th##key', '=', 'proxy', '.', '_au##th##key', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "match_dict = self . _alltraits ( ** metadata ) \n"
Original    (010): ['match_dict', '=', 'self', '.', '_alltraits', '(', '**', 'metadata', ')', '\\n']
Tokenized   (020): ['[CLS]', 'match', '_', 'di', '##ct', '=', 'self', '.', '_', 'all', '##tra', '##its', '(', '*', '*', 'metadata', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['match', '_', 'di', '##ct', '=', 'self', '.', '_', 'all', '##tra', '##its', '(', '*', '*', 'metadata', ')', '\\', 'n']
Detokenized (010): ['match_di##ct', '=', 'self', '.', '_all##tra##its', '(', '**', 'metadata', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "childname , _ , restofpath = traitpath . partition ( ) \n"
Original    (012): ['childname', ',', '_', ',', 'restofpath', '=', 'traitpath', '.', 'partition', '(', ')', '\\n']
Tokenized   (019): ['[CLS]', 'child', '##name', ',', '_', ',', 'rest', '##of', '##path', '=', 'trait', '##path', '.', 'partition', '(', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['child', '##name', ',', '_', ',', 'rest', '##of', '##path', '=', 'trait', '##path', '.', 'partition', '(', ')', '\\', 'n']
Detokenized (012): ['child##name', ',', '_', ',', 'rest##of##path', '=', 'trait##path', '.', 'partition', '(', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "mdict . setdefault ( , t . __class__ . __name__ ) \n"
Original    (012): ['mdict', '.', 'setdefault', '(', ',', 't', '.', '__class__', '.', '__name__', ')', '\\n']
Tokenized   (028): ['[CLS]', 'md', '##ic', '##t', '.', 'set', '##de', '##fa', '##ult', '(', ',', 't', '.', '_', '_', 'class', '_', '_', '.', '_', '_', 'name', '_', '_', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['md', '##ic', '##t', '.', 'set', '##de', '##fa', '##ult', '(', ',', 't', '.', '_', '_', 'class', '_', '_', '.', '_', '_', 'name', '_', '_', ')', '\\', 'n']
Detokenized (012): ['md##ic##t', '.', 'set##de##fa##ult', '(', ',', 't', '.', '__class__', '.', '__name__', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "expr = compile ( assign , assign , mode = ) \n"
Original    (012): ['expr', '=', 'compile', '(', 'assign', ',', 'assign', ',', 'mode', '=', ')', '\\n']
Tokenized   (017): ['[CLS]', 'ex', '##pr', '=', 'com', '##pile', '(', 'assign', ',', 'assign', ',', 'mode', '=', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['ex', '##pr', '=', 'com', '##pile', '(', 'assign', ',', 'assign', ',', 'mode', '=', ')', '\\', 'n']
Detokenized (012): ['ex##pr', '=', 'com##pile', '(', 'assign', ',', 'assign', ',', 'mode', '=', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "tstamp = % ( now . year , now . month , now . day , now . hour , now . minute ) \n"
Original    (025): ['tstamp', '=', '%', '(', 'now', '.', 'year', ',', 'now', '.', 'month', ',', 'now', '.', 'day', ',', 'now', '.', 'hour', ',', 'now', '.', 'minute', ')', '\\n']
Tokenized   (030): ['[CLS]', 'ts', '##tam', '##p', '=', '%', '(', 'now', '.', 'year', ',', 'now', '.', 'month', ',', 'now', '.', 'day', ',', 'now', '.', 'hour', ',', 'now', '.', 'minute', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['ts', '##tam', '##p', '=', '%', '(', 'now', '.', 'year', ',', 'now', '.', 'month', ',', 'now', '.', 'day', ',', 'now', '.', 'hour', ',', 'now', '.', 'minute', ')', '\\', 'n']
Detokenized (025): ['ts##tam##p', '=', '%', '(', 'now', '.', 'year', ',', 'now', '.', 'month', ',', 'now', '.', 'day', ',', 'now', '.', 'hour', ',', 'now', '.', 'minute', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "entry_pts = [ ( self , name , _get_entry_group ( self ) ) ] \n"
Original    (015): ['entry_pts', '=', '[', '(', 'self', ',', 'name', ',', '_get_entry_group', '(', 'self', ')', ')', ']', '\\n']
Tokenized   (025): ['[CLS]', 'entry', '_', 'pts', '=', '[', '(', 'self', ',', 'name', ',', '_', 'get', '_', 'entry', '_', 'group', '(', 'self', ')', ')', ']', '\\', 'n', '[SEP]']
Filtered   (023): ['entry', '_', 'pts', '=', '[', '(', 'self', ',', 'name', ',', '_', 'get', '_', 'entry', '_', 'group', '(', 'self', ')', ')', ']', '\\', 'n']
Detokenized (015): ['entry_pts', '=', '[', '(', 'self', ',', 'name', ',', '_get_entry_group', '(', 'self', ')', ')', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "root_start = root_start + 1 if root_start >= 0 else 0 \n"
Original    (012): ['root_start', '=', 'root_start', '+', '1', 'if', 'root_start', '>=', '0', 'else', '0', '\\n']
Tokenized   (022): ['[CLS]', 'root', '_', 'start', '=', 'root', '_', 'start', '+', '1', 'if', 'root', '_', 'start', '>', '=', '0', 'else', '0', '\\', 'n', '[SEP]']
Filtered   (020): ['root', '_', 'start', '=', 'root', '_', 'start', '+', '1', 'if', 'root', '_', 'start', '>', '=', '0', 'else', '0', '\\', 'n']
Detokenized (012): ['root_start', '=', 'root_start', '+', '1', 'if', 'root_start', '>=', '0', 'else', '0', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "root_pathname += \n"
Original    (003): ['root_pathname', '+=', '\\n']
Tokenized   (010): ['[CLS]', 'root', '_', 'path', '##name', '+', '=', '\\', 'n', '[SEP]']
Filtered   (008): ['root', '_', 'path', '##name', '+', '=', '\\', 'n']
Detokenized (003): ['root_path##name', '+=', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "Container . _bases ( type ( obj ) , names ) \n"
Original    (012): ['Container', '.', '_bases', '(', 'type', '(', 'obj', ')', ',', 'names', ')', '\\n']
Tokenized   (017): ['[CLS]', 'container', '.', '_', 'bases', '(', 'type', '(', 'ob', '##j', ')', ',', 'names', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['container', '.', '_', 'bases', '(', 'type', '(', 'ob', '##j', ')', ',', 'names', ')', '\\', 'n']
Detokenized (012): ['container', '.', '_bases', '(', 'type', '(', 'ob##j', ')', ',', 'names', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "names . append ( % ( cls . __module__ , cls . __name__ ) ) \n"
Original    (016): ['names', '.', 'append', '(', '%', '(', 'cls', '.', '__module__', ',', 'cls', '.', '__name__', ')', ')', '\\n']
Tokenized   (030): ['[CLS]', 'names', '.', 'app', '##end', '(', '%', '(', 'cl', '##s', '.', '_', '_', 'module', '_', '_', ',', 'cl', '##s', '.', '_', '_', 'name', '_', '_', ')', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['names', '.', 'app', '##end', '(', '%', '(', 'cl', '##s', '.', '_', '_', 'module', '_', '_', ',', 'cl', '##s', '.', '_', '_', 'name', '_', '_', ')', ')', '\\', 'n']
Detokenized (016): ['names', '.', 'app##end', '(', '%', '(', 'cl##s', '.', '__module__', ',', 'cl##s', '.', '__name__', ')', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "_get_entry_group . group_map = [ \n"
Original    (006): ['_get_entry_group', '.', 'group_map', '=', '[', '\\n']
Tokenized   (016): ['[CLS]', '_', 'get', '_', 'entry', '_', 'group', '.', 'group', '_', 'map', '=', '[', '\\', 'n', '[SEP]']
Filtered   (014): ['_', 'get', '_', 'entry', '_', 'group', '.', 'group', '_', 'map', '=', '[', '\\', 'n']
Detokenized (006): ['_get_entry_group', '.', 'group_map', '=', '[', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "pprint . pprint ( dict ( [ ( n , str ( v ) ) \n"
Original    (016): ['pprint', '.', 'pprint', '(', 'dict', '(', '[', '(', 'n', ',', 'str', '(', 'v', ')', ')', '\\n']
Tokenized   (025): ['[CLS]', 'pp', '##rin', '##t', '.', 'pp', '##rin', '##t', '(', 'di', '##ct', '(', '[', '(', 'n', ',', 'st', '##r', '(', 'v', ')', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['pp', '##rin', '##t', '.', 'pp', '##rin', '##t', '(', 'di', '##ct', '(', '[', '(', 'n', ',', 'st', '##r', '(', 'v', ')', ')', '\\', 'n']
Detokenized (016): ['pp##rin##t', '.', 'pp##rin##t', '(', 'di##ct', '(', '[', '(', 'n', ',', 'st##r', '(', 'v', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "** metadata ) ] ) , \n"
Original    (007): ['**', 'metadata', ')', ']', ')', ',', '\\n']
Tokenized   (011): ['[CLS]', '*', '*', 'metadata', ')', ']', ')', ',', '\\', 'n', '[SEP]']
Filtered   (009): ['*', '*', 'metadata', ')', ']', ')', ',', '\\', 'n']
Detokenized (007): ['**', 'metadata', ')', ']', ')', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "io_attr [ ] = \n"
Original    (005): ['io_attr', '[', ']', '=', '\\n']
Tokenized   (011): ['[CLS]', 'io', '_', 'at', '##tr', '[', ']', '=', '\\', 'n', '[SEP]']
Filtered   (009): ['io', '_', 'at', '##tr', '[', ']', '=', '\\', 'n']
Detokenized (005): ['io_at##tr', '[', ']', '=', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "_redirect_streams ( ofile . fileno ( ) ) \n"
Original    (009): ['_redirect_streams', '(', 'ofile', '.', 'fileno', '(', ')', ')', '\\n']
Tokenized   (019): ['[CLS]', '_', 'red', '##ire', '##ct', '_', 'streams', '(', 'of', '##ile', '.', 'file', '##no', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['_', 'red', '##ire', '##ct', '_', 'streams', '(', 'of', '##ile', '.', 'file', '##no', '(', ')', ')', '\\', 'n']
Detokenized (009): ['_red##ire##ct_streams', '(', 'of##ile', '.', 'file##no', '(', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "leftover = arr_size % num_divisions \n"
Original    (006): ['leftover', '=', 'arr_size', '%', 'num_divisions', '\\n']
Tokenized   (016): ['[CLS]', 'left', '##over', '=', 'ar', '##r', '_', 'size', '%', 'nu', '##m', '_', 'divisions', '\\', 'n', '[SEP]']
Filtered   (014): ['left', '##over', '=', 'ar', '##r', '_', 'size', '%', 'nu', '##m', '_', 'divisions', '\\', 'n']
Detokenized (006): ['left##over', '=', 'ar##r_size', '%', 'nu##m_divisions', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "sizes [ : leftover ] += 1 \n"
Original    (008): ['sizes', '[', ':', 'leftover', ']', '+=', '1', '\\n']
Tokenized   (013): ['[CLS]', 'sizes', '[', ':', 'left', '##over', ']', '+', '=', '1', '\\', 'n', '[SEP]']
Filtered   (011): ['sizes', '[', ':', 'left', '##over', ']', '+', '=', '1', '\\', 'n']
Detokenized (008): ['sizes', '[', ':', 'left##over', ']', '+=', '1', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "offsets [ 1 : ] = numpy . cumsum ( sizes ) [ : - 1 ] \n"
Original    (018): ['offsets', '[', '1', ':', ']', '=', 'numpy', '.', 'cumsum', '(', 'sizes', ')', '[', ':', '-', '1', ']', '\\n']
Tokenized   (025): ['[CLS]', 'offset', '##s', '[', '1', ':', ']', '=', 'nu', '##mp', '##y', '.', 'cum', '##sum', '(', 'sizes', ')', '[', ':', '-', '1', ']', '\\', 'n', '[SEP]']
Filtered   (023): ['offset', '##s', '[', '1', ':', ']', '=', 'nu', '##mp', '##y', '.', 'cum', '##sum', '(', 'sizes', ')', '[', ':', '-', '1', ']', '\\', 'n']
Detokenized (018): ['offset##s', '[', '1', ':', ']', '=', 'nu##mp##y', '.', 'cum##sum', '(', 'sizes', ')', '[', ':', '-', '1', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "z1 = Float ( 0. , iotype = ) \n"
Original    (010): ['z1', '=', 'Float', '(', '0.', ',', 'iotype', '=', ')', '\\n']
Tokenized   (016): ['[CLS]', 'z', '##1', '=', 'float', '(', '0', '.', ',', 'io', '##type', '=', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['z', '##1', '=', 'float', '(', '0', '.', ',', 'io', '##type', '=', ')', '\\', 'n']
Detokenized (010): ['z##1', '=', 'float', '(', '0.', ',', 'io##type', '=', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "z_store = Array ( [ 0. , 0. ] , iotype = ) \n"
Original    (014): ['z_store', '=', 'Array', '(', '[', '0.', ',', '0.', ']', ',', 'iotype', '=', ')', '\\n']
Tokenized   (022): ['[CLS]', 'z', '_', 'store', '=', 'array', '(', '[', '0', '.', ',', '0', '.', ']', ',', 'io', '##type', '=', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['z', '_', 'store', '=', 'array', '(', '[', '0', '.', ',', '0', '.', ']', ',', 'io', '##type', '=', ')', '\\', 'n']
Detokenized (014): ['z_store', '=', 'array', '(', '[', '0.', ',', '0.', ']', ',', 'io##type', '=', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "ssa_F = Array ( [ 0.0 ] , iotype = ) \n"
Original    (012): ['ssa_F', '=', 'Array', '(', '[', '0.0', ']', ',', 'iotype', '=', ')', '\\n']
Tokenized   (021): ['[CLS]', 'ss', '##a', '_', 'f', '=', 'array', '(', '[', '0', '.', '0', ']', ',', 'io', '##type', '=', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['ss', '##a', '_', 'f', '=', 'array', '(', '[', '0', '.', '0', ']', ',', 'io', '##type', '=', ')', '\\', 'n']
Detokenized (012): ['ss##a_f', '=', 'array', '(', '[', '0.0', ']', ',', 'io##type', '=', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "ssa_dG = Array ( [ [ 0.0 , 0.0 ] , [ 0.0 , 0.0 ] ] , iotype = ) \n"
Original    (022): ['ssa_dG', '=', 'Array', '(', '[', '[', '0.0', ',', '0.0', ']', ',', '[', '0.0', ',', '0.0', ']', ']', ',', 'iotype', '=', ')', '\\n']
Tokenized   (038): ['[CLS]', 'ss', '##a', '_', 'd', '##g', '=', 'array', '(', '[', '[', '0', '.', '0', ',', '0', '.', '0', ']', ',', '[', '0', '.', '0', ',', '0', '.', '0', ']', ']', ',', 'io', '##type', '=', ')', '\\', 'n', '[SEP]']
Filtered   (036): ['ss', '##a', '_', 'd', '##g', '=', 'array', '(', '[', '[', '0', '.', '0', ',', '0', '.', '0', ']', ',', '[', '0', '.', '0', ',', '0', '.', '0', ']', ']', ',', 'io', '##type', '=', ')', '\\', 'n']
Detokenized (022): ['ss##a_d##g', '=', 'array', '(', '[', '[', '0.0', ',', '0.0', ']', ',', '[', '0.0', ',', '0.0', ']', ']', ',', 'io##type', '=', ')', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "arr_out = Array ( [ 1. , 2. , 3. ] , iotype = , units = ) \n"
Original    (019): ['arr_out', '=', 'Array', '(', '[', '1.', ',', '2.', ',', '3.', ']', ',', 'iotype', '=', ',', 'units', '=', ')', '\\n']
Tokenized   (029): ['[CLS]', 'ar', '##r', '_', 'out', '=', 'array', '(', '[', '1', '.', ',', '2', '.', ',', '3', '.', ']', ',', 'io', '##type', '=', ',', 'units', '=', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['ar', '##r', '_', 'out', '=', 'array', '(', '[', '1', '.', ',', '2', '.', ',', '3', '.', ']', ',', 'io', '##type', '=', ',', 'units', '=', ')', '\\', 'n']
Detokenized (019): ['ar##r_out', '=', 'array', '(', '[', '1.', ',', '2.', ',', '3.', ']', ',', 'io##type', '=', ',', 'units', '=', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "arg [ ] = np . array ( [ 3.1 ] ) \n"
Original    (013): ['arg', '[', ']', '=', 'np', '.', 'array', '(', '[', '3.1', ']', ')', '\\n']
Tokenized   (019): ['[CLS]', 'ar', '##g', '[', ']', '=', 'np', '.', 'array', '(', '[', '3', '.', '1', ']', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['ar', '##g', '[', ']', '=', 'np', '.', 'array', '(', '[', '3', '.', '1', ']', ')', '\\', 'n']
Detokenized (013): ['ar##g', '[', ']', '=', 'np', '.', 'array', '(', '[', '3.1', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "jacs [ ] = np . array ( [ [ 100.0 , 101 , 102 , 103 ] , \n"
Original    (020): ['jacs', '[', ']', '=', 'np', '.', 'array', '(', '[', '[', '100.0', ',', '101', ',', '102', ',', '103', ']', ',', '\\n']
Tokenized   (026): ['[CLS]', 'ja', '##cs', '[', ']', '=', 'np', '.', 'array', '(', '[', '[', '100', '.', '0', ',', '101', ',', '102', ',', '103', ']', ',', '\\', 'n', '[SEP]']
Filtered   (024): ['ja', '##cs', '[', ']', '=', 'np', '.', 'array', '(', '[', '[', '100', '.', '0', ',', '101', ',', '102', ',', '103', ']', ',', '\\', 'n']
Detokenized (020): ['ja##cs', '[', ']', '=', 'np', '.', 'array', '(', '[', '[', '100.0', ',', '101', ',', '102', ',', '103', ']', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "assert_rel_error ( self , J [ 3 ] [ 0 ] , 3.0 , 1e-5 ) \n"
Original    (017): ['assert_rel_error', '(', 'self', ',', 'J', '[', '3', ']', '[', '0', ']', ',', '3.0', ',', '1e-5', ')', '\\n']
Tokenized   (030): ['[CLS]', 'assert', '_', 're', '##l', '_', 'error', '(', 'self', ',', 'j', '[', '3', ']', '[', '0', ']', ',', '3', '.', '0', ',', '1', '##e', '-', '5', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['assert', '_', 're', '##l', '_', 'error', '(', 'self', ',', 'j', '[', '3', ']', '[', '0', ']', ',', '3', '.', '0', ',', '1', '##e', '-', '5', ')', '\\', 'n']
Detokenized (017): ['assert_re##l_error', '(', 'self', ',', 'j', '[', '3', ']', '[', '0', ']', ',', '3.0', ',', '1##e-5', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "newval = _getformat ( val ) % val \n"
Original    (009): ['newval', '=', '_getformat', '(', 'val', ')', '%', 'val', '\\n']
Tokenized   (016): ['[CLS]', 'new', '##val', '=', '_', 'get', '##form', '##at', '(', 'val', ')', '%', 'val', '\\', 'n', '[SEP]']
Filtered   (014): ['new', '##val', '=', '_', 'get', '##form', '##at', '(', 'val', ')', '%', 'val', '\\', 'n']
Detokenized (009): ['new##val', '=', '_get##form##at', '(', 'val', ')', '%', 'val', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "newline = re . sub ( self . reg , sub . replace_array , line ) \n"
Original    (017): ['newline', '=', 're', '.', 'sub', '(', 'self', '.', 'reg', ',', 'sub', '.', 'replace_array', ',', 'line', ')', '\\n']
Tokenized   (023): ['[CLS]', 'new', '##line', '=', 're', '.', 'sub', '(', 'self', '.', 'reg', ',', 'sub', '.', 'replace', '_', 'array', ',', 'line', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['new', '##line', '=', 're', '.', 'sub', '(', 'self', '.', 'reg', ',', 'sub', '.', 'replace', '_', 'array', ',', 'line', ')', '\\', 'n']
Detokenized (017): ['new##line', '=', 're', '.', 'sub', '(', 'self', '.', 'reg', ',', 'sub', '.', 'replace_array', ',', 'line', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "fields = self . _parse_line ( ) . parseString ( line . replace ( key , "KeyField" ) ) \n"
Original    (020): ['fields', '=', 'self', '.', '_parse_line', '(', ')', '.', 'parseString', '(', 'line', '.', 'replace', '(', 'key', ',', '"KeyField"', ')', ')', '\\n']
Tokenized   (033): ['[CLS]', 'fields', '=', 'self', '.', '_', 'par', '##se', '_', 'line', '(', ')', '.', 'par', '##ses', '##tri', '##ng', '(', 'line', '.', 'replace', '(', 'key', ',', '"', 'key', '##field', '"', ')', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['fields', '=', 'self', '.', '_', 'par', '##se', '_', 'line', '(', ')', '.', 'par', '##ses', '##tri', '##ng', '(', 'line', '.', 'replace', '(', 'key', ',', '"', 'key', '##field', '"', ')', ')', '\\', 'n']
Detokenized (020): ['fields', '=', 'self', '.', '_par##se_line', '(', ')', '.', 'par##ses##tri##ng', '(', 'line', '.', 'replace', '(', 'key', ',', '"key##field"', ')', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "j2 = self . current_row + rowend + 1 \n"
Original    (010): ['j2', '=', 'self', '.', 'current_row', '+', 'rowend', '+', '1', '\\n']
Tokenized   (017): ['[CLS]', 'j', '##2', '=', 'self', '.', 'current', '_', 'row', '+', 'rowe', '##nd', '+', '1', '\\', 'n', '[SEP]']
Filtered   (015): ['j', '##2', '=', 'self', '.', 'current', '_', 'row', '+', 'rowe', '##nd', '+', '1', '\\', 'n']
Detokenized (010): ['j##2', '=', 'self', '.', 'current_row', '+', 'rowe##nd', '+', '1', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "ee = CaselessLiteral ( ) | CaselessLiteral ( ) \n"
Original    (010): ['ee', '=', 'CaselessLiteral', '(', ')', '|', 'CaselessLiteral', '(', ')', '\\n']
Tokenized   (019): ['[CLS]', 'ee', '=', 'case', '##less', '##lite', '##ral', '(', ')', '|', 'case', '##less', '##lite', '##ral', '(', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['ee', '=', 'case', '##less', '##lite', '##ral', '(', ')', '|', 'case', '##less', '##lite', '##ral', '(', ')', '\\', 'n']
Detokenized (010): ['ee', '=', 'case##less##lite##ral', '(', ')', '|', 'case##less##lite##ral', '(', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "num_int = ToInteger ( Combine ( Optional ( sign ) + digits ) ) \n"
Original    (015): ['num_int', '=', 'ToInteger', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\n']
Tokenized   (023): ['[CLS]', 'nu', '##m', '_', 'int', '=', 'to', '##int', '##eger', '(', 'combine', '(', 'optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['nu', '##m', '_', 'int', '=', 'to', '##int', '##eger', '(', 'combine', '(', 'optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\', 'n']
Detokenized (015): ['nu##m_int', '=', 'to##int##eger', '(', 'combine', '(', 'optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "num_float = ToFloat ( Combine ( Optional ( sign ) + \n"
Original    (012): ['num_float', '=', 'ToFloat', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', '\\n']
Tokenized   (020): ['[CLS]', 'nu', '##m', '_', 'float', '=', 'to', '##fl', '##oat', '(', 'combine', '(', 'optional', '(', 'sign', ')', '+', '\\', 'n', '[SEP]']
Filtered   (018): ['nu', '##m', '_', 'float', '=', 'to', '##fl', '##oat', '(', 'combine', '(', 'optional', '(', 'sign', ')', '+', '\\', 'n']
Detokenized (012): ['nu##m_float', '=', 'to##fl##oat', '(', 'combine', '(', 'optional', '(', 'sign', ')', '+', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "Optional ( ee + Optional ( sign ) + digits ) \n"
Original    (012): ['Optional', '(', 'ee', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', '\\n']
Tokenized   (015): ['[CLS]', 'optional', '(', 'ee', '+', 'optional', '(', 'sign', ')', '+', 'digits', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['optional', '(', 'ee', '+', 'optional', '(', 'sign', ')', '+', 'digits', ')', '\\', 'n']
Detokenized (012): ['optional', '(', 'ee', '+', 'optional', '(', 'sign', ')', '+', 'digits', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "mixed_exp = ToFloat ( Combine ( digits + ee + Optional ( sign ) + digits ) ) \n"
Original    (019): ['mixed_exp', '=', 'ToFloat', '(', 'Combine', '(', 'digits', '+', 'ee', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\n']
Tokenized   (027): ['[CLS]', 'mixed', '_', 'ex', '##p', '=', 'to', '##fl', '##oat', '(', 'combine', '(', 'digits', '+', 'ee', '+', 'optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['mixed', '_', 'ex', '##p', '=', 'to', '##fl', '##oat', '(', 'combine', '(', 'digits', '+', 'ee', '+', 'optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\', 'n']
Detokenized (019): ['mixed_ex##p', '=', 'to##fl##oat', '(', 'combine', '(', 'digits', '+', 'ee', '+', 'optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "string_text ) ) ) \n"
Original    (005): ['string_text', ')', ')', ')', '\\n']
Tokenized   (010): ['[CLS]', 'string', '_', 'text', ')', ')', ')', '\\', 'n', '[SEP]']
Filtered   (008): ['string', '_', 'text', ')', ')', ')', '\\', 'n']
Detokenized (005): ['string_text', ')', ')', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "J [ , ] = - 1.0 \n"
Original    (008): ['J', '[', ',', ']', '=', '-', '1.0', '\\n']
Tokenized   (013): ['[CLS]', 'j', '[', ',', ']', '=', '-', '1', '.', '0', '\\', 'n', '[SEP]']
Filtered   (011): ['j', '[', ',', ']', '=', '-', '1', '.', '0', '\\', 'n']
Detokenized (008): ['j', '[', ',', ']', '=', '-', '1.0', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "top [ ] = - 7.0 \n"
Original    (007): ['top', '[', ']', '=', '-', '7.0', '\\n']
Tokenized   (012): ['[CLS]', 'top', '[', ']', '=', '-', '7', '.', '0', '\\', 'n', '[SEP]']
Filtered   (010): ['top', '[', ']', '=', '-', '7', '.', '0', '\\', 'n']
Detokenized (007): ['top', '[', ']', '=', '-', '7.0', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "lhs , op , rhs = _parse_constraint ( expr ) \n"
Original    (011): ['lhs', ',', 'op', ',', 'rhs', '=', '_parse_constraint', '(', 'expr', ')', '\\n']
Tokenized   (021): ['[CLS]', 'l', '##hs', ',', 'op', ',', 'r', '##hs', '=', '_', 'par', '##se', '_', 'constraint', '(', 'ex', '##pr', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['l', '##hs', ',', 'op', ',', 'r', '##hs', '=', '_', 'par', '##se', '_', 'constraint', '(', 'ex', '##pr', ')', '\\', 'n']
Detokenized (011): ['l##hs', ',', 'op', ',', 'r##hs', '=', '_par##se_constraint', '(', 'ex##pr', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "first , second = ( rhs , lhs ) if op . startswith ( ) else ( lhs , rhs ) \n"
Original    (022): ['first', ',', 'second', '=', '(', 'rhs', ',', 'lhs', ')', 'if', 'op', '.', 'startswith', '(', ')', 'else', '(', 'lhs', ',', 'rhs', ')', '\\n']
Tokenized   (030): ['[CLS]', 'first', ',', 'second', '=', '(', 'r', '##hs', ',', 'l', '##hs', ')', 'if', 'op', '.', 'starts', '##with', '(', ')', 'else', '(', 'l', '##hs', ',', 'r', '##hs', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['first', ',', 'second', '=', '(', 'r', '##hs', ',', 'l', '##hs', ')', 'if', 'op', '.', 'starts', '##with', '(', ')', 'else', '(', 'l', '##hs', ',', 'r', '##hs', ')', '\\', 'n']
Detokenized (022): ['first', ',', 'second', '=', '(', 'r##hs', ',', 'l##hs', ')', 'if', 'op', '.', 'starts##with', '(', ')', 'else', '(', 'l##hs', ',', 'r##hs', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "input_graph . add_edges_from ( ( ( start , p ) for p in plist [ 1 : ] ) , \n"
Original    (021): ['input_graph', '.', 'add_edges_from', '(', '(', '(', 'start', ',', 'p', ')', 'for', 'p', 'in', 'plist', '[', '1', ':', ']', ')', ',', '\\n']
Tokenized   (031): ['[CLS]', 'input', '_', 'graph', '.', 'add', '_', 'edges', '_', 'from', '(', '(', '(', 'start', ',', 'p', ')', 'for', 'p', 'in', 'pl', '##ist', '[', '1', ':', ']', ')', ',', '\\', 'n', '[SEP]']
Filtered   (029): ['input', '_', 'graph', '.', 'add', '_', 'edges', '_', 'from', '(', '(', '(', 'start', ',', 'p', ')', 'for', 'p', 'in', 'pl', '##ist', '[', '1', ':', ']', ')', ',', '\\', 'n']
Detokenized (021): ['input_graph', '.', 'add_edges_from', '(', '(', '(', 'start', ',', 'p', ')', 'for', 'p', 'in', 'pl##ist', '[', '1', ':', ']', ')', ',', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "src_idxs = { src : None } \n"
Original    (008): ['src_idxs', '=', '{', 'src', ':', 'None', '}', '\\n']
Tokenized   (017): ['[CLS]', 'sr', '##c', '_', 'id', '##x', '##s', '=', '{', 'sr', '##c', ':', 'none', '}', '\\', 'n', '[SEP]']
Filtered   (015): ['sr', '##c', '_', 'id', '##x', '##s', '=', '{', 'sr', '##c', ':', 'none', '}', '\\', 'n']
Detokenized (008): ['sr##c_id##x##s', '=', '{', 'sr##c', ':', 'none', '}', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "units = [ params_dict [ n ] . get ( ) for n in connected_inputs ] \n"
Original    (017): ['units', '=', '[', 'params_dict', '[', 'n', ']', '.', 'get', '(', ')', 'for', 'n', 'in', 'connected_inputs', ']', '\\n']
Tokenized   (026): ['[CLS]', 'units', '=', '[', 'para', '##ms', '_', 'di', '##ct', '[', 'n', ']', '.', 'get', '(', ')', 'for', 'n', 'in', 'connected', '_', 'inputs', ']', '\\', 'n', '[SEP]']
Filtered   (024): ['units', '=', '[', 'para', '##ms', '_', 'di', '##ct', '[', 'n', ']', '.', 'get', '(', ')', 'for', 'n', 'in', 'connected', '_', 'inputs', ']', '\\', 'n']
Detokenized (017): ['units', '=', '[', 'para##ms_di##ct', '[', 'n', ']', '.', 'get', '(', ')', 'for', 'n', 'in', 'connected_inputs', ']', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "vals = [ params_dict [ n ] [ ] for n in connected_inputs ] \n"
Original    (015): ['vals', '=', '[', 'params_dict', '[', 'n', ']', '[', ']', 'for', 'n', 'in', 'connected_inputs', ']', '\\n']
Tokenized   (025): ['[CLS]', 'val', '##s', '=', '[', 'para', '##ms', '_', 'di', '##ct', '[', 'n', ']', '[', ']', 'for', 'n', 'in', 'connected', '_', 'inputs', ']', '\\', 'n', '[SEP]']
Filtered   (023): ['val', '##s', '=', '[', 'para', '##ms', '_', 'di', '##ct', '[', 'n', ']', '[', ']', 'for', 'n', 'in', 'connected', '_', 'inputs', ']', '\\', 'n']
Detokenized (015): ['val##s', '=', '[', 'para##ms_di##ct', '[', 'n', ']', '[', ']', 'for', 'n', 'in', 'connected_inputs', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "tname , t = connected_inputs [ i ] , u \n"
Original    (011): ['tname', ',', 't', '=', 'connected_inputs', '[', 'i', ']', ',', 'u', '\\n']
Tokenized   (017): ['[CLS]', 'tna', '##me', ',', 't', '=', 'connected', '_', 'inputs', '[', 'i', ']', ',', 'u', '\\', 'n', '[SEP]']
Filtered   (015): ['tna', '##me', ',', 't', '=', 'connected', '_', 'inputs', '[', 'i', ']', ',', 'u', '\\', 'n']
Detokenized (011): ['tna##me', ',', 't', '=', 'connected_inputs', '[', 'i', ']', ',', 'u', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "correct_src = params_dict [ connected_inputs [ 0 ] ] [ ] \n"
Original    (012): ['correct_src', '=', 'params_dict', '[', 'connected_inputs', '[', '0', ']', ']', '[', ']', '\\n']
Tokenized   (024): ['[CLS]', 'correct', '_', 'sr', '##c', '=', 'para', '##ms', '_', 'di', '##ct', '[', 'connected', '_', 'inputs', '[', '0', ']', ']', '[', ']', '\\', 'n', '[SEP]']
Filtered   (022): ['correct', '_', 'sr', '##c', '=', 'para', '##ms', '_', 'di', '##ct', '[', 'connected', '_', 'inputs', '[', '0', ']', ']', '[', ']', '\\', 'n']
Detokenized (012): ['correct_sr##c', '=', 'para##ms_di##ct', '[', 'connected_inputs', '[', '0', ']', ']', '[', ']', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "sorted ( [ ( v , k ) for k , v in forms . items ( ) ] ) ) ) \n"
Original    (023): ['sorted', '(', '[', '(', 'v', ',', 'k', ')', 'for', 'k', ',', 'v', 'in', 'forms', '.', 'items', '(', ')', ']', ')', ')', ')', '\\n']
Tokenized   (026): ['[CLS]', 'sorted', '(', '[', '(', 'v', ',', 'k', ')', 'for', 'k', ',', 'v', 'in', 'forms', '.', 'items', '(', ')', ']', ')', ')', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['sorted', '(', '[', '(', 'v', ',', 'k', ')', 'for', 'k', ',', 'v', 'in', 'forms', '.', 'items', '(', ')', ']', ')', ')', ')', '\\', 'n']
Detokenized (023): ['sorted', '(', '[', '(', 'v', ',', 'k', ')', 'for', 'k', ',', 'v', 'in', 'forms', '.', 'items', '(', ')', ']', ')', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "full_order = { s . pathname : i for i , s in \n"
Original    (014): ['full_order', '=', '{', 's', '.', 'pathname', ':', 'i', 'for', 'i', ',', 's', 'in', '\\n']
Tokenized   (020): ['[CLS]', 'full', '_', 'order', '=', '{', 's', '.', 'path', '##name', ':', 'i', 'for', 'i', ',', 's', 'in', '\\', 'n', '[SEP]']
Filtered   (018): ['full', '_', 'order', '=', '{', 's', '.', 'path', '##name', ':', 'i', 'for', 'i', ',', 's', 'in', '\\', 'n']
Detokenized (014): ['full_order', '=', '{', 's', '.', 'path##name', ':', 'i', 'for', 'i', ',', 's', 'in', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "enumerate ( self . root . subsystems ( recurse = True ) ) } \n"
Original    (015): ['enumerate', '(', 'self', '.', 'root', '.', 'subsystems', '(', 'recurse', '=', 'True', ')', ')', '}', '\\n']
Tokenized   (022): ['[CLS]', 'en', '##ume', '##rate', '(', 'self', '.', 'root', '.', 'sub', '##systems', '(', 'rec', '##urse', '=', 'true', ')', ')', '}', '\\', 'n', '[SEP]']
Filtered   (020): ['en', '##ume', '##rate', '(', 'self', '.', 'root', '.', 'sub', '##systems', '(', 'rec', '##urse', '=', 'true', ')', ')', '}', '\\', 'n']
Detokenized (015): ['en##ume##rate', '(', 'self', '.', 'root', '.', 'sub##systems', '(', 'rec##urse', '=', 'true', ')', ')', '}', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "ssys = srcs [ 0 ] . rsplit ( , 1 ) [ 0 ] \n"
Original    (016): ['ssys', '=', 'srcs', '[', '0', ']', '.', 'rsplit', '(', ',', '1', ')', '[', '0', ']', '\\n']
Tokenized   (023): ['[CLS]', 'ss', '##ys', '=', 'sr', '##cs', '[', '0', ']', '.', 'rs', '##pl', '##it', '(', ',', '1', ')', '[', '0', ']', '\\', 'n', '[SEP]']
Filtered   (021): ['ss', '##ys', '=', 'sr', '##cs', '[', '0', ']', '.', 'rs', '##pl', '##it', '(', ',', '1', ')', '[', '0', ']', '\\', 'n']
Detokenized (016): ['ss##ys', '=', 'sr##cs', '[', '0', ']', '.', 'rs##pl##it', '(', ',', '1', ')', '[', '0', ']', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "params_dict , unknowns_dict = self . root . _setup_variables ( ) \n"
Original    (012): ['params_dict', ',', 'unknowns_dict', '=', 'self', '.', 'root', '.', '_setup_variables', '(', ')', '\\n']
Tokenized   (026): ['[CLS]', 'para', '##ms', '_', 'di', '##ct', ',', 'unknown', '##s', '_', 'di', '##ct', '=', 'self', '.', 'root', '.', '_', 'setup', '_', 'variables', '(', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['para', '##ms', '_', 'di', '##ct', ',', 'unknown', '##s', '_', 'di', '##ct', '=', 'self', '.', 'root', '.', '_', 'setup', '_', 'variables', '(', ')', '\\', 'n']
Detokenized (012): ['para##ms_di##ct', ',', 'unknown##s_di##ct', '=', 'self', '.', 'root', '.', '_setup_variables', '(', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "is not Component . setup_distrib ) ) : \n"
Original    (009): ['is', 'not', 'Component', '.', 'setup_distrib', ')', ')', ':', '\\n']
Tokenized   (017): ['[CLS]', 'is', 'not', 'component', '.', 'setup', '_', 'di', '##st', '##ri', '##b', ')', ')', ':', '\\', 'n', '[SEP]']
Filtered   (015): ['is', 'not', 'component', '.', 'setup', '_', 'di', '##st', '##ri', '##b', ')', ')', ':', '\\', 'n']
Detokenized (009): ['is', 'not', 'component', '.', 'setup_di##st##ri##b', ')', ')', ':', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "alloc_derivs = not self . root . fd_options [ ] \n"
Original    (011): ['alloc_derivs', '=', 'not', 'self', '.', 'root', '.', 'fd_options', '[', ']', '\\n']
Tokenized   (022): ['[CLS]', 'all', '##oc', '_', 'der', '##iv', '##s', '=', 'not', 'self', '.', 'root', '.', 'f', '##d', '_', 'options', '[', ']', '\\', 'n', '[SEP]']
Filtered   (020): ['all', '##oc', '_', 'der', '##iv', '##s', '=', 'not', 'self', '.', 'root', '.', 'f', '##d', '_', 'options', '[', ']', '\\', 'n']
Detokenized (011): ['all##oc_der##iv##s', '=', 'not', 'self', '.', 'root', '.', 'f##d_options', '[', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "dangling_params = sorted ( set ( [ \n"
Original    (008): ['dangling_params', '=', 'sorted', '(', 'set', '(', '[', '\\n']
Tokenized   (014): ['[CLS]', 'dangling', '_', 'para', '##ms', '=', 'sorted', '(', 'set', '(', '[', '\\', 'n', '[SEP]']
Filtered   (012): ['dangling', '_', 'para', '##ms', '=', 'sorted', '(', 'set', '(', '[', '\\', 'n']
Detokenized (008): ['dangling_para##ms', '=', 'sorted', '(', 'set', '(', '[', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "nocomps = sorted ( [ c . pathname for c in self . root . components ( recurse = True , \n"
Original    (022): ['nocomps', '=', 'sorted', '(', '[', 'c', '.', 'pathname', 'for', 'c', 'in', 'self', '.', 'root', '.', 'components', '(', 'recurse', '=', 'True', ',', '\\n']
Tokenized   (029): ['[CLS]', 'no', '##com', '##ps', '=', 'sorted', '(', '[', 'c', '.', 'path', '##name', 'for', 'c', 'in', 'self', '.', 'root', '.', 'components', '(', 'rec', '##urse', '=', 'true', ',', '\\', 'n', '[SEP]']
Filtered   (027): ['no', '##com', '##ps', '=', 'sorted', '(', '[', 'c', '.', 'path', '##name', 'for', 'c', 'in', 'self', '.', 'root', '.', 'components', '(', 'rec', '##urse', '=', 'true', ',', '\\', 'n']
Detokenized (022): ['no##com##ps', '=', 'sorted', '(', '[', 'c', '.', 'path##name', 'for', 'c', 'in', 'self', '.', 'root', '.', 'components', '(', 'rec##urse', '=', 'true', ',', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "local = True ) \n"
Original    (005): ['local', '=', 'True', ')', '\\n']
Tokenized   (008): ['[CLS]', 'local', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (006): ['local', '=', 'true', ')', '\\', 'n']
Detokenized (005): ['local', '=', 'true', ')', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "recorders . extend ( grp . ln_solver . recorders ) \n"
Original    (011): ['recorders', '.', 'extend', '(', 'grp', '.', 'ln_solver', '.', 'recorders', ')', '\\n']
Tokenized   (021): ['[CLS]', 'recorder', '##s', '.', 'extend', '(', 'gr', '##p', '.', 'l', '##n', '_', 'solve', '##r', '.', 'recorder', '##s', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['recorder', '##s', '.', 'extend', '(', 'gr', '##p', '.', 'l', '##n', '_', 'solve', '##r', '.', 'recorder', '##s', ')', '\\', 'n']
Detokenized (011): ['recorder##s', '.', 'extend', '(', 'gr##p', '.', 'l##n_solve##r', '.', 'recorder##s', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "conn_comps . update ( [ s . rsplit ( , 1 ) [ 0 ] \n"
Original    (016): ['conn_comps', '.', 'update', '(', '[', 's', '.', 'rsplit', '(', ',', '1', ')', '[', '0', ']', '\\n']
Tokenized   (025): ['[CLS]', 'con', '##n', '_', 'com', '##ps', '.', 'update', '(', '[', 's', '.', 'rs', '##pl', '##it', '(', ',', '1', ')', '[', '0', ']', '\\', 'n', '[SEP]']
Filtered   (023): ['con', '##n', '_', 'com', '##ps', '.', 'update', '(', '[', 's', '.', 'rs', '##pl', '##it', '(', ',', '1', ')', '[', '0', ']', '\\', 'n']
Detokenized (016): ['con##n_com##ps', '.', 'update', '(', '[', 's', '.', 'rs##pl##it', '(', ',', '1', ')', '[', '0', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "noconn_comps = sorted ( [ c . pathname \n"
Original    (009): ['noconn_comps', '=', 'sorted', '(', '[', 'c', '.', 'pathname', '\\n']
Tokenized   (018): ['[CLS]', 'no', '##con', '##n', '_', 'com', '##ps', '=', 'sorted', '(', '[', 'c', '.', 'path', '##name', '\\', 'n', '[SEP]']
Filtered   (016): ['no', '##con', '##n', '_', 'com', '##ps', '=', 'sorted', '(', '[', 'c', '.', 'path', '##name', '\\', 'n']
Detokenized (009): ['no##con##n_com##ps', '=', 'sorted', '(', '[', 'c', '.', 'path##name', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "strong = [ s for s in nx . strongly_connected_components ( graph ) \n"
Original    (014): ['strong', '=', '[', 's', 'for', 's', 'in', 'nx', '.', 'strongly_connected_components', '(', 'graph', ')', '\\n']
Tokenized   (022): ['[CLS]', 'strong', '=', '[', 's', 'for', 's', 'in', 'n', '##x', '.', 'strongly', '_', 'connected', '_', 'components', '(', 'graph', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['strong', '=', '[', 's', 'for', 's', 'in', 'n', '##x', '.', 'strongly', '_', 'connected', '_', 'components', '(', 'graph', ')', '\\', 'n']
Detokenized (014): ['strong', '=', '[', 's', 'for', 's', 'in', 'n##x', '.', 'strongly_connected_components', '(', 'graph', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "subs = [ s for s in grp . _subsystems ] \n"
Original    (012): ['subs', '=', '[', 's', 'for', 's', 'in', 'grp', '.', '_subsystems', ']', '\\n']
Tokenized   (019): ['[CLS]', 'sub', '##s', '=', '[', 's', 'for', 's', 'in', 'gr', '##p', '.', '_', 'sub', '##systems', ']', '\\', 'n', '[SEP]']
Filtered   (017): ['sub', '##s', '=', '[', 's', 'for', 's', 'in', 'gr', '##p', '.', '_', 'sub', '##systems', ']', '\\', 'n']
Detokenized (012): ['sub##s', '=', '[', 's', 'for', 's', 'in', 'gr##p', '.', '_sub##systems', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "tups = sorted ( [ ( subs . index ( s ) , s ) for s in relstrong [ - 1 ] ] ) \n"
Original    (026): ['tups', '=', 'sorted', '(', '[', '(', 'subs', '.', 'index', '(', 's', ')', ',', 's', ')', 'for', 's', 'in', 'relstrong', '[', '-', '1', ']', ']', ')', '\\n']
Tokenized   (034): ['[CLS]', 'tu', '##ps', '=', 'sorted', '(', '[', '(', 'sub', '##s', '.', 'index', '(', 's', ')', ',', 's', ')', 'for', 's', 'in', 're', '##ls', '##tron', '##g', '[', '-', '1', ']', ']', ')', '\\', 'n', '[SEP]']
Filtered   (032): ['tu', '##ps', '=', 'sorted', '(', '[', '(', 'sub', '##s', '.', 'index', '(', 's', ')', ',', 's', ')', 'for', 's', 'in', 're', '##ls', '##tron', '##g', '[', '-', '1', ']', ']', ')', '\\', 'n']
Detokenized (026): ['tu##ps', '=', 'sorted', '(', '[', '(', 'sub##s', '.', 'index', '(', 's', ')', ',', 's', ')', 'for', 's', 'in', 're##ls##tron##g', '[', '-', '1', ']', ']', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "relstrong [ - 1 ] = [ t [ 1 ] for t in tups ] \n"
Original    (017): ['relstrong', '[', '-', '1', ']', '=', '[', 't', '[', '1', ']', 'for', 't', 'in', 'tups', ']', '\\n']
Tokenized   (024): ['[CLS]', 're', '##ls', '##tron', '##g', '[', '-', '1', ']', '=', '[', 't', '[', '1', ']', 'for', 't', 'in', 'tu', '##ps', ']', '\\', 'n', '[SEP]']
Filtered   (022): ['re', '##ls', '##tron', '##g', '[', '-', '1', ']', '=', '[', 't', '[', '1', ']', 'for', 't', 'in', 'tu', '##ps', ']', '\\', 'n']
Detokenized (017): ['re##ls##tron##g', '[', '-', '1', ']', '=', '[', 't', '[', '1', ']', 'for', 't', 'in', 'tu##ps', ']', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "nearest_child ( grp . pathname , n ) for n in out_of_order [ name ] \n"
Original    (016): ['nearest_child', '(', 'grp', '.', 'pathname', ',', 'n', ')', 'for', 'n', 'in', 'out_of_order', '[', 'name', ']', '\\n']
Tokenized   (027): ['[CLS]', 'nearest', '_', 'child', '(', 'gr', '##p', '.', 'path', '##name', ',', 'n', ')', 'for', 'n', 'in', 'out', '_', 'of', '_', 'order', '[', 'name', ']', '\\', 'n', '[SEP]']
Filtered   (025): ['nearest', '_', 'child', '(', 'gr', '##p', '.', 'path', '##name', ',', 'n', ')', 'for', 'n', 'in', 'out', '_', 'of', '_', 'order', '[', 'name', ']', '\\', 'n']
Detokenized (016): ['nearest_child', '(', 'gr##p', '.', 'path##name', ',', 'n', ')', 'for', 'n', 'in', 'out_of_order', '[', 'name', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "pbos = [ var for var in vec if vec . metadata ( var ) . get ( ) ] \n"
Original    (021): ['pbos', '=', '[', 'var', 'for', 'var', 'in', 'vec', 'if', 'vec', '.', 'metadata', '(', 'var', ')', '.', 'get', '(', ')', ']', '\\n']
Tokenized   (027): ['[CLS]', 'p', '##bos', '=', '[', 'var', 'for', 'var', 'in', 've', '##c', 'if', 've', '##c', '.', 'metadata', '(', 'var', ')', '.', 'get', '(', ')', ']', '\\', 'n', '[SEP]']
Filtered   (025): ['p', '##bos', '=', '[', 'var', 'for', 'var', 'in', 've', '##c', 'if', 've', '##c', '.', 'metadata', '(', 'var', ')', '.', 'get', '(', ')', ']', '\\', 'n']
Detokenized (021): ['p##bos', '=', '[', 'var', 'for', 'var', 'in', 've##c', 'if', 've##c', '.', 'metadata', '(', 'var', ')', '.', 'get', '(', ')', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "iteritems ( self . root . _params_dict ) ) : \n"
Original    (011): ['iteritems', '(', 'self', '.', 'root', '.', '_params_dict', ')', ')', ':', '\\n']
Tokenized   (022): ['[CLS]', 'it', '##eri', '##tem', '##s', '(', 'self', '.', 'root', '.', '_', 'para', '##ms', '_', 'di', '##ct', ')', ')', ':', '\\', 'n', '[SEP]']
Filtered   (020): ['it', '##eri', '##tem', '##s', '(', 'self', '.', 'root', '.', '_', 'para', '##ms', '_', 'di', '##ct', ')', ')', ':', '\\', 'n']
Detokenized (011): ['it##eri##tem##s', '(', 'self', '.', 'root', '.', '_para##ms_di##ct', ')', ')', ':', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "dv_scale = None , cn_scale = None , sparsity = None ) : \n"
Original    (014): ['dv_scale', '=', 'None', ',', 'cn_scale', '=', 'None', ',', 'sparsity', '=', 'None', ')', ':', '\\n']
Tokenized   (024): ['[CLS]', 'd', '##v', '_', 'scale', '=', 'none', ',', 'cn', '_', 'scale', '=', 'none', ',', 'spa', '##rs', '##ity', '=', 'none', ')', ':', '\\', 'n', '[SEP]']
Filtered   (022): ['d', '##v', '_', 'scale', '=', 'none', ',', 'cn', '_', 'scale', '=', 'none', ',', 'spa', '##rs', '##ity', '=', 'none', ')', ':', '\\', 'n']
Detokenized (014): ['d##v_scale', '=', 'none', ',', 'cn_scale', '=', 'none', ',', 'spa##rs##ity', '=', 'none', ')', ':', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "fd_unknowns = [ var for var in unknown_list if var not in indep_list ] \n"
Original    (015): ['fd_unknowns', '=', '[', 'var', 'for', 'var', 'in', 'unknown_list', 'if', 'var', 'not', 'in', 'indep_list', ']', '\\n']
Tokenized   (027): ['[CLS]', 'f', '##d', '_', 'unknown', '##s', '=', '[', 'var', 'for', 'var', 'in', 'unknown', '_', 'list', 'if', 'var', 'not', 'in', 'ind', '##ep', '_', 'list', ']', '\\', 'n', '[SEP]']
Filtered   (025): ['f', '##d', '_', 'unknown', '##s', '=', '[', 'var', 'for', 'var', 'in', 'unknown', '_', 'list', 'if', 'var', 'not', 'in', 'ind', '##ep', '_', 'list', ']', '\\', 'n']
Detokenized (015): ['f##d_unknown##s', '=', '[', 'var', 'for', 'var', 'in', 'unknown_list', 'if', 'var', 'not', 'in', 'ind##ep_list', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "usize += len ( idx ) \n"
Original    (007): ['usize', '+=', 'len', '(', 'idx', ')', '\\n']
Tokenized   (013): ['[CLS]', 'us', '##ize', '+', '=', 'len', '(', 'id', '##x', ')', '\\', 'n', '[SEP]']
Filtered   (011): ['us', '##ize', '+', '=', 'len', '(', 'id', '##x', ')', '\\', 'n']
Detokenized (007): ['us##ize', '+=', 'len', '(', 'id##x', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "fwd = mode == \n"
Original    (005): ['fwd', '=', 'mode', '==', '\\n']
Tokenized   (010): ['[CLS]', 'f', '##wd', '=', 'mode', '=', '=', '\\', 'n', '[SEP]']
Filtered   (008): ['f', '##wd', '=', 'mode', '=', '=', '\\', 'n']
Detokenized (005): ['f##wd', '=', 'mode', '==', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "poi_indices , qoi_indices = self . _poi_indices , self . _qoi_indices \n"
Original    (012): ['poi_indices', ',', 'qoi_indices', '=', 'self', '.', '_poi_indices', ',', 'self', '.', '_qoi_indices', '\\n']
Tokenized   (029): ['[CLS]', 'po', '##i', '_', 'indices', ',', 'q', '##oi', '_', 'indices', '=', 'self', '.', '_', 'po', '##i', '_', 'indices', ',', 'self', '.', '_', 'q', '##oi', '_', 'indices', '\\', 'n', '[SEP]']
Filtered   (027): ['po', '##i', '_', 'indices', ',', 'q', '##oi', '_', 'indices', '=', 'self', '.', '_', 'po', '##i', '_', 'indices', ',', 'self', '.', '_', 'q', '##oi', '_', 'indices', '\\', 'n']
Detokenized (012): ['po##i_indices', ',', 'q##oi_indices', '=', 'self', '.', '_po##i_indices', ',', 'self', '.', '_q##oi_indices', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "in_scale , un_scale = cn_scale , dv_scale \n"
Original    (008): ['in_scale', ',', 'un_scale', '=', 'cn_scale', ',', 'dv_scale', '\\n']
Tokenized   (020): ['[CLS]', 'in', '_', 'scale', ',', 'un', '_', 'scale', '=', 'cn', '_', 'scale', ',', 'd', '##v', '_', 'scale', '\\', 'n', '[SEP]']
Filtered   (018): ['in', '_', 'scale', ',', 'un', '_', 'scale', '=', 'cn', '_', 'scale', ',', 'd', '##v', '_', 'scale', '\\', 'n']
Detokenized (008): ['in_scale', ',', 'un_scale', '=', 'cn_scale', ',', 'd##v_scale', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "duvec = self . root . dumat [ vkey ] \n"
Original    (011): ['duvec', '=', 'self', '.', 'root', '.', 'dumat', '[', 'vkey', ']', '\\n']
Tokenized   (018): ['[CLS]', 'du', '##ve', '##c', '=', 'self', '.', 'root', '.', 'du', '##mat', '[', 'v', '##key', ']', '\\', 'n', '[SEP]']
Filtered   (016): ['du', '##ve', '##c', '=', 'self', '.', 'root', '.', 'du', '##mat', '[', 'v', '##key', ']', '\\', 'n']
Detokenized (011): ['du##ve##c', '=', 'self', '.', 'root', '.', 'du##mat', '[', 'v##key', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "rhs [ vkey ] [ : ] = 0.0 \n"
Original    (010): ['rhs', '[', 'vkey', ']', '[', ':', ']', '=', '0.0', '\\n']
Tokenized   (017): ['[CLS]', 'r', '##hs', '[', 'v', '##key', ']', '[', ':', ']', '=', '0', '.', '0', '\\', 'n', '[SEP]']
Filtered   (015): ['r', '##hs', '[', 'v', '##key', ']', '[', ':', ']', '=', '0', '.', '0', '\\', 'n']
Detokenized (010): ['r##hs', '[', 'v##key', ']', '[', ':', ']', '=', '0.0', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "isinstance ( self . root . ln_solver , LinearGaussSeidel ) ) : \n"
Original    (013): ['isinstance', '(', 'self', '.', 'root', '.', 'ln_solver', ',', 'LinearGaussSeidel', ')', ')', ':', '\\n']
Tokenized   (026): ['[CLS]', 'is', '##ins', '##tance', '(', 'self', '.', 'root', '.', 'l', '##n', '_', 'solve', '##r', ',', 'linear', '##gau', '##ss', '##sei', '##del', ')', ')', ':', '\\', 'n', '[SEP]']
Filtered   (024): ['is', '##ins', '##tance', '(', 'self', '.', 'root', '.', 'l', '##n', '_', 'solve', '##r', ',', 'linear', '##gau', '##ss', '##sei', '##del', ')', ')', ':', '\\', 'n']
Detokenized (013): ['is##ins##tance', '(', 'self', '.', 'root', '.', 'l##n_solve##r', ',', 'linear##gau##ss##sei##del', ')', ')', ':', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "unkn_list = [ item for item in dunknowns if not dunknowns . metadata ( item ) . get ( ) ] \n"
Original    (022): ['unkn_list', '=', '[', 'item', 'for', 'item', 'in', 'dunknowns', 'if', 'not', 'dunknowns', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\n']
Tokenized   (035): ['[CLS]', 'un', '##k', '##n', '_', 'list', '=', '[', 'item', 'for', 'item', 'in', 'dun', '##k', '##now', '##ns', 'if', 'not', 'dun', '##k', '##now', '##ns', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\', 'n', '[SEP]']
Filtered   (033): ['un', '##k', '##n', '_', 'list', '=', '[', 'item', 'for', 'item', 'in', 'dun', '##k', '##now', '##ns', 'if', 'not', 'dun', '##k', '##now', '##ns', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\', 'n']
Detokenized (022): ['un##k##n_list', '=', '[', 'item', 'for', 'item', 'in', 'dun##k##now##ns', 'if', 'not', 'dun##k##now##ns', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "p_size = np . size ( dinputs [ p_name ] ) \n"
Original    (012): ['p_size', '=', 'np', '.', 'size', '(', 'dinputs', '[', 'p_name', ']', ')', '\\n']
Tokenized   (021): ['[CLS]', 'p', '_', 'size', '=', 'np', '.', 'size', '(', 'din', '##put', '##s', '[', 'p', '_', 'name', ']', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['p', '_', 'size', '=', 'np', '.', 'size', '(', 'din', '##put', '##s', '[', 'p', '_', 'name', ']', ')', '\\', 'n']
Detokenized (012): ['p_size', '=', 'np', '.', 'size', '(', 'din##put##s', '[', 'p_name', ']', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "dresids . _dat [ u_name ] . val [ idx ] = 1.0 \n"
Original    (014): ['dresids', '.', '_dat', '[', 'u_name', ']', '.', 'val', '[', 'idx', ']', '=', '1.0', '\\n']
Tokenized   (025): ['[CLS]', 'dr', '##es', '##ids', '.', '_', 'dat', '[', 'u', '_', 'name', ']', '.', 'val', '[', 'id', '##x', ']', '=', '1', '.', '0', '\\', 'n', '[SEP]']
Filtered   (023): ['dr', '##es', '##ids', '.', '_', 'dat', '[', 'u', '_', 'name', ']', '.', 'val', '[', 'id', '##x', ']', '=', '1', '.', '0', '\\', 'n']
Detokenized (014): ['dr##es##ids', '.', '_dat', '[', 'u_name', ']', '.', 'val', '[', 'id##x', ']', '=', '1.0', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "dunknowns , dresids , ) \n"
Original    (006): ['dunknowns', ',', 'dresids', ',', ')', '\\n']
Tokenized   (014): ['[CLS]', 'dun', '##k', '##now', '##ns', ',', 'dr', '##es', '##ids', ',', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['dun', '##k', '##now', '##ns', ',', 'dr', '##es', '##ids', ',', ')', '\\', 'n']
Detokenized (006): ['dun##k##now##ns', ',', 'dr##es##ids', ',', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "jac_rev [ ( u_name , p_name ) ] [ idx , : ] = dinputs . _dat [ p_name ] . val \n"
Original    (023): ['jac_rev', '[', '(', 'u_name', ',', 'p_name', ')', ']', '[', 'idx', ',', ':', ']', '=', 'dinputs', '.', '_dat', '[', 'p_name', ']', '.', 'val', '\\n']
Tokenized   (039): ['[CLS]', 'ja', '##c', '_', 'rev', '[', '(', 'u', '_', 'name', ',', 'p', '_', 'name', ')', ']', '[', 'id', '##x', ',', ':', ']', '=', 'din', '##put', '##s', '.', '_', 'dat', '[', 'p', '_', 'name', ']', '.', 'val', '\\', 'n', '[SEP]']
Filtered   (037): ['ja', '##c', '_', 'rev', '[', '(', 'u', '_', 'name', ',', 'p', '_', 'name', ')', ']', '[', 'id', '##x', ',', ':', ']', '=', 'din', '##put', '##s', '.', '_', 'dat', '[', 'p', '_', 'name', ']', '.', 'val', '\\', 'n']
Detokenized (023): ['ja##c_rev', '[', '(', 'u_name', ',', 'p_name', ')', ']', '[', 'id##x', ',', ':', ']', '=', 'din##put##s', '.', '_dat', '[', 'p_name', ']', '.', 'val', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "c_name = cname , jac_fd2 = jac_fd2 , fd_desc = fd_desc , \n"
Original    (013): ['c_name', '=', 'cname', ',', 'jac_fd2', '=', 'jac_fd2', ',', 'fd_desc', '=', 'fd_desc', ',', '\\n']
Tokenized   (037): ['[CLS]', 'c', '_', 'name', '=', 'cn', '##ame', ',', 'ja', '##c', '_', 'f', '##d', '##2', '=', 'ja', '##c', '_', 'f', '##d', '##2', ',', 'f', '##d', '_', 'des', '##c', '=', 'f', '##d', '_', 'des', '##c', ',', '\\', 'n', '[SEP]']
Filtered   (035): ['c', '_', 'name', '=', 'cn', '##ame', ',', 'ja', '##c', '_', 'f', '##d', '##2', '=', 'ja', '##c', '_', 'f', '##d', '##2', ',', 'f', '##d', '_', 'des', '##c', '=', 'f', '##d', '_', 'des', '##c', ',', '\\', 'n']
Detokenized (013): ['c_name', '=', 'cn##ame', ',', 'ja##c_f##d##2', '=', 'ja##c_f##d##2', ',', 'f##d_des##c', '=', 'f##d_des##c', ',', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "param_srcs = [ root . connections [ p ] for p in abs_indep_list if not root . _params_dict [ p ] . get ( ) ] \n"
Original    (027): ['param_srcs', '=', '[', 'root', '.', 'connections', '[', 'p', ']', 'for', 'p', 'in', 'abs_indep_list', 'if', 'not', 'root', '.', '_params_dict', '[', 'p', ']', '.', 'get', '(', ')', ']', '\\n']
Tokenized   (044): ['[CLS]', 'para', '##m', '_', 'sr', '##cs', '=', '[', 'root', '.', 'connections', '[', 'p', ']', 'for', 'p', 'in', 'abs', '_', 'ind', '##ep', '_', 'list', 'if', 'not', 'root', '.', '_', 'para', '##ms', '_', 'di', '##ct', '[', 'p', ']', '.', 'get', '(', ')', ']', '\\', 'n', '[SEP]']
Filtered   (042): ['para', '##m', '_', 'sr', '##cs', '=', '[', 'root', '.', 'connections', '[', 'p', ']', 'for', 'p', 'in', 'abs', '_', 'ind', '##ep', '_', 'list', 'if', 'not', 'root', '.', '_', 'para', '##ms', '_', 'di', '##ct', '[', 'p', ']', '.', 'get', '(', ')', ']', '\\', 'n']
Detokenized (027): ['para##m_sr##cs', '=', '[', 'root', '.', 'connections', '[', 'p', ']', 'for', 'p', 'in', 'abs_ind##ep_list', 'if', 'not', 'root', '.', '_para##ms_di##ct', '[', 'p', ']', '.', 'get', '(', ')', ']', '\\n']
Counter: 42
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "to_prom_name [ p ] for p , idxs in param_srcs \n"
Original    (011): ['to_prom_name', '[', 'p', ']', 'for', 'p', ',', 'idxs', 'in', 'param_srcs', '\\n']
Tokenized   (024): ['[CLS]', 'to', '_', 'prom', '_', 'name', '[', 'p', ']', 'for', 'p', ',', 'id', '##x', '##s', 'in', 'para', '##m', '_', 'sr', '##cs', '\\', 'n', '[SEP]']
Filtered   (022): ['to', '_', 'prom', '_', 'name', '[', 'p', ']', 'for', 'p', ',', 'id', '##x', '##s', 'in', 'para', '##m', '_', 'sr', '##cs', '\\', 'n']
Detokenized (011): ['to_prom_name', '[', 'p', ']', 'for', 'p', ',', 'id##x##s', 'in', 'para##m_sr##cs', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "unknown_list = [ item for item in unknown_list if not root . unknowns . metadata ( item ) . get ( ) ] \n"
Original    (024): ['unknown_list', '=', '[', 'item', 'for', 'item', 'in', 'unknown_list', 'if', 'not', 'root', '.', 'unknowns', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\n']
Tokenized   (032): ['[CLS]', 'unknown', '_', 'list', '=', '[', 'item', 'for', 'item', 'in', 'unknown', '_', 'list', 'if', 'not', 'root', '.', 'unknown', '##s', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\', 'n', '[SEP]']
Filtered   (030): ['unknown', '_', 'list', '=', '[', 'item', 'for', 'item', 'in', 'unknown', '_', 'list', 'if', 'not', 'root', '.', 'unknown', '##s', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\', 'n']
Detokenized (024): ['unknown_list', '=', '[', 'item', 'for', 'item', 'in', 'unknown_list', 'if', 'not', 'root', '.', 'unknown##s', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "_assemble_deriv_data ( indep_list , unknown_list , data , \n"
Original    (009): ['_assemble_deriv_data', '(', 'indep_list', ',', 'unknown_list', ',', 'data', ',', '\\n']
Tokenized   (023): ['[CLS]', '_', 'assemble', '_', 'der', '##iv', '_', 'data', '(', 'ind', '##ep', '_', 'list', ',', 'unknown', '_', 'list', ',', 'data', ',', '\\', 'n', '[SEP]']
Filtered   (021): ['_', 'assemble', '_', 'der', '##iv', '_', 'data', '(', 'ind', '##ep', '_', 'list', ',', 'unknown', '_', 'list', ',', 'data', ',', '\\', 'n']
Detokenized (009): ['_assemble_der##iv_data', '(', 'ind##ep_list', ',', 'unknown_list', ',', 'data', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "_both_names ( tmeta , to_prom_name ) ) \n"
Original    (008): ['_both_names', '(', 'tmeta', ',', 'to_prom_name', ')', ')', '\\n']
Tokenized   (020): ['[CLS]', '_', 'both', '_', 'names', '(', 't', '##met', '##a', ',', 'to', '_', 'prom', '_', 'name', ')', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['_', 'both', '_', 'names', '(', 't', '##met', '##a', ',', 'to', '_', 'prom', '_', 'name', ')', ')', '\\', 'n']
Detokenized (008): ['_both_names', '(', 't##met##a', ',', 'to_prom_name', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "abs_unames = self . root . _sysdata . to_abs_uname \n"
Original    (010): ['abs_unames', '=', 'self', '.', 'root', '.', '_sysdata', '.', 'to_abs_uname', '\\n']
Tokenized   (024): ['[CLS]', 'abs', '_', 'una', '##mes', '=', 'self', '.', 'root', '.', '_', 'sy', '##sd', '##ata', '.', 'to', '_', 'abs', '_', 'una', '##me', '\\', 'n', '[SEP]']
Filtered   (022): ['abs', '_', 'una', '##mes', '=', 'self', '.', 'root', '.', '_', 'sy', '##sd', '##ata', '.', 'to', '_', 'abs', '_', 'una', '##me', '\\', 'n']
Detokenized (010): ['abs_una##mes', '=', 'self', '.', 'root', '.', '_sy##sd##ata', '.', 'to_abs_una##me', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "out_str = tmp1 . format ( _pad_name ( ) , _pad_name ( ) , \n"
Original    (015): ['out_str', '=', 'tmp1', '.', 'format', '(', '_pad_name', '(', ')', ',', '_pad_name', '(', ')', ',', '\\n']
Tokenized   (029): ['[CLS]', 'out', '_', 'st', '##r', '=', 't', '##mp', '##1', '.', 'format', '(', '_', 'pad', '_', 'name', '(', ')', ',', '_', 'pad', '_', 'name', '(', ')', ',', '\\', 'n', '[SEP]']
Filtered   (027): ['out', '_', 'st', '##r', '=', 't', '##mp', '##1', '.', 'format', '(', '_', 'pad', '_', 'name', '(', ')', ',', '_', 'pad', '_', 'name', '(', ')', ',', '\\', 'n']
Detokenized (015): ['out_st##r', '=', 't##mp##1', '.', 'format', '(', '_pad_name', '(', ')', ',', '_pad_name', '(', ')', ',', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "magfor , magrev , magfd , abs1 , abs2 , \n"
Original    (011): ['magfor', ',', 'magrev', ',', 'magfd', ',', 'abs1', ',', 'abs2', ',', '\\n']
Tokenized   (021): ['[CLS]', 'mag', '##for', ',', 'mag', '##re', '##v', ',', 'mag', '##f', '##d', ',', 'abs', '##1', ',', 'abs', '##2', ',', '\\', 'n', '[SEP]']
Filtered   (019): ['mag', '##for', ',', 'mag', '##re', '##v', ',', 'mag', '##f', '##d', ',', 'abs', '##1', ',', 'abs', '##2', ',', '\\', 'n']
Detokenized (011): ['mag##for', ',', 'mag##re##v', ',', 'mag##f##d', ',', 'abs##1', ',', 'abs##2', ',', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "rel1 , rel2 ) ) \n"
Original    (006): ['rel1', ',', 'rel2', ')', ')', '\\n']
Tokenized   (013): ['[CLS]', 're', '##l', '##1', ',', 're', '##l', '##2', ')', ')', '\\', 'n', '[SEP]']
Filtered   (011): ['re', '##l', '##1', ',', 're', '##l', '##2', ')', ')', '\\', 'n']
Detokenized (006): ['re##l##1', ',', 're##l##2', ')', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "_pad_name ( , 12 , quotes = False ) \n"
Original    (010): ['_pad_name', '(', ',', '12', ',', 'quotes', '=', 'False', ')', '\\n']
Tokenized   (016): ['[CLS]', '_', 'pad', '_', 'name', '(', ',', '12', ',', 'quotes', '=', 'false', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['_', 'pad', '_', 'name', '(', ',', '12', ',', 'quotes', '=', 'false', ')', '\\', 'n']
Detokenized (010): ['_pad_name', '(', ',', '12', ',', 'quotes', '=', 'false', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "magfd , magfd2 , abs4 , rel4 ) ) \n"
Original    (010): ['magfd', ',', 'magfd2', ',', 'abs4', ',', 'rel4', ')', ')', '\\n']
Tokenized   (021): ['[CLS]', 'mag', '##f', '##d', ',', 'mag', '##f', '##d', '##2', ',', 'abs', '##4', ',', 're', '##l', '##4', ')', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['mag', '##f', '##d', ',', 'mag', '##f', '##d', '##2', ',', 'abs', '##4', ',', 're', '##l', '##4', ')', ')', '\\', 'n']
Detokenized (010): ['mag##f##d', ',', 'mag##f##d##2', ',', 'abs##4', ',', 're##l##4', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "out_stream . write ( str ( Jsub_fd2 ) ) \n"
Original    (010): ['out_stream', '.', 'write', '(', 'str', '(', 'Jsub_fd2', ')', ')', '\\n']
Tokenized   (022): ['[CLS]', 'out', '_', 'stream', '.', 'write', '(', 'st', '##r', '(', 'j', '##su', '##b', '_', 'f', '##d', '##2', ')', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['out', '_', 'stream', '.', 'write', '(', 'st', '##r', '(', 'j', '##su', '##b', '_', 'f', '##d', '##2', ')', ')', '\\', 'n']
Detokenized (010): ['out_stream', '.', 'write', '(', 'st##r', '(', 'j##su##b_f##d##2', ')', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sqlite_dict_args . setdefault ( , ) \n"
Original    (007): ['sqlite_dict_args', '.', 'setdefault', '(', ',', ')', '\\n']
Tokenized   (020): ['[CLS]', 'sql', '##ite', '_', 'di', '##ct', '_', 'ar', '##gs', '.', 'set', '##de', '##fa', '##ult', '(', ',', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['sql', '##ite', '_', 'di', '##ct', '_', 'ar', '##gs', '.', 'set', '##de', '##fa', '##ult', '(', ',', ')', '\\', 'n']
Detokenized (007): ['sql##ite_di##ct_ar##gs', '.', 'set##de##fa##ult', '(', ',', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "ll_1 = ll_0 + n_samples - k - 1 \n"
Original    (010): ['ll_1', '=', 'll_0', '+', 'n_samples', '-', 'k', '-', '1', '\\n']
Tokenized   (019): ['[CLS]', 'll', '_', '1', '=', 'll', '_', '0', '+', 'n', '_', 'samples', '-', 'k', '-', '1', '\\', 'n', '[SEP]']
Filtered   (017): ['ll', '_', '1', '=', 'll', '_', '0', '+', 'n', '_', 'samples', '-', 'k', '-', '1', '\\', 'n']
Detokenized (010): ['ll_1', '=', 'll_0', '+', 'n_samples', '-', 'k', '-', '1', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "D = self . D [ lvl ] \n"
Original    (009): ['D', '=', 'self', '.', 'D', '[', 'lvl', ']', '\\n']
Tokenized   (014): ['[CLS]', 'd', '=', 'self', '.', 'd', '[', 'l', '##v', '##l', ']', '\\', 'n', '[SEP]']
Filtered   (012): ['d', '=', 'self', '.', 'd', '[', 'l', '##v', '##l', ']', '\\', 'n']
Detokenized (009): ['d', '=', 'self', '.', 'd', '[', 'l##v##l', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "initial_range = INITIAL_RANGE_DEFAULT , tol = TOLERANCE_DEFAULT ) : \n"
Original    (010): ['initial_range', '=', 'INITIAL_RANGE_DEFAULT', ',', 'tol', '=', 'TOLERANCE_DEFAULT', ')', ':', '\\n']
Tokenized   (022): ['[CLS]', 'initial', '_', 'range', '=', 'initial', '_', 'range', '_', 'default', ',', 'to', '##l', '=', 'tolerance', '_', 'default', ')', ':', '\\', 'n', '[SEP]']
Filtered   (020): ['initial', '_', 'range', '=', 'initial', '_', 'range', '_', 'default', ',', 'to', '##l', '=', 'tolerance', '_', 'default', ')', ':', '\\', 'n']
Detokenized (010): ['initial_range', '=', 'initial_range_default', ',', 'to##l', '=', 'tolerance_default', ')', ':', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "y_best = y [ nlevel - 1 ] \n"
Original    (009): ['y_best', '=', 'y', '[', 'nlevel', '-', '1', ']', '\\n']
Tokenized   (016): ['[CLS]', 'y', '_', 'best', '=', 'y', '[', 'nl', '##eve', '##l', '-', '1', ']', '\\', 'n', '[SEP]']
Filtered   (014): ['y', '_', 'best', '=', 'y', '[', 'nl', '##eve', '##l', '-', '1', ']', '\\', 'n']
Detokenized (009): ['y_best', '=', 'y', '[', 'nl##eve##l', '-', '1', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "+ str ( theta ) ) \n"
Original    (007): ['+', 'str', '(', 'theta', ')', ')', '\\n']
Tokenized   (011): ['[CLS]', '+', 'st', '##r', '(', 'theta', ')', ')', '\\', 'n', '[SEP]']
Filtered   (009): ['+', 'st', '##r', '(', 'theta', ')', ')', '\\', 'n']
Detokenized (007): ['+', 'st##r', '(', 'theta', ')', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "Yt = solve_triangular ( C , y , lower = True ) \n"
Original    (013): ['Yt', '=', 'solve_triangular', '(', 'C', ',', 'y', ',', 'lower', '=', 'True', ')', '\\n']
Tokenized   (019): ['[CLS]', 'y', '##t', '=', 'solve', '_', 'triangular', '(', 'c', ',', 'y', ',', 'lower', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['y', '##t', '=', 'solve', '_', 'triangular', '(', 'c', ',', 'y', ',', 'lower', '=', 'true', ')', '\\', 'n']
Detokenized (013): ['y##t', '=', 'solve_triangular', '(', 'c', ',', 'y', ',', 'lower', '=', 'true', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "err2 = np . dot ( err . T , err ) [ 0 , 0 ] \n"
Original    (018): ['err2', '=', 'np', '.', 'dot', '(', 'err', '.', 'T', ',', 'err', ')', '[', '0', ',', '0', ']', '\\n']
Tokenized   (025): ['[CLS]', 'er', '##r', '##2', '=', 'np', '.', 'dot', '(', 'er', '##r', '.', 't', ',', 'er', '##r', ')', '[', '0', ',', '0', ']', '\\', 'n', '[SEP]']
Filtered   (023): ['er', '##r', '##2', '=', 'np', '.', 'dot', '(', 'er', '##r', '.', 't', ',', 'er', '##r', ')', '[', '0', ',', '0', ']', '\\', 'n']
Detokenized (018): ['er##r##2', '=', 'np', '.', 'dot', '(', 'er##r', '.', 't', ',', 'er##r', ')', '[', '0', ',', '0', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "sigma2 = err2 / ( n_samples - p - q ) \n"
Original    (012): ['sigma2', '=', 'err2', '/', '(', 'n_samples', '-', 'p', '-', 'q', ')', '\\n']
Tokenized   (020): ['[CLS]', 'sigma', '##2', '=', 'er', '##r', '##2', '/', '(', 'n', '_', 'samples', '-', 'p', '-', 'q', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['sigma', '##2', '=', 'er', '##r', '##2', '/', '(', 'n', '_', 'samples', '-', 'p', '-', 'q', ')', '\\', 'n']
Detokenized (012): ['sigma##2', '=', 'er##r##2', '/', '(', 'n_samples', '-', 'p', '-', 'q', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "detR = ( ( np . diag ( C ) ) ** ( 2. / n_samples ) ) . prod ( ) \n"
Original    (023): ['detR', '=', '(', '(', 'np', '.', 'diag', '(', 'C', ')', ')', '**', '(', '2.', '/', 'n_samples', ')', ')', '.', 'prod', '(', ')', '\\n']
Tokenized   (033): ['[CLS]', 'det', '##r', '=', '(', '(', 'np', '.', 'dia', '##g', '(', 'c', ')', ')', '*', '*', '(', '2', '.', '/', 'n', '_', 'samples', ')', ')', '.', 'pro', '##d', '(', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['det', '##r', '=', '(', '(', 'np', '.', 'dia', '##g', '(', 'c', ')', ')', '*', '*', '(', '2', '.', '/', 'n', '_', 'samples', ')', ')', '.', 'pro', '##d', '(', ')', '\\', 'n']
Detokenized (023): ['det##r', '=', '(', '(', 'np', '.', 'dia##g', '(', 'c', ')', ')', '**', '(', '2.', '/', 'n_samples', ')', ')', '.', 'pro##d', '(', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "rlf_value = ( n_samples - p - q ) * np . log10 ( sigma2 ) + n_samples * np . log10 ( detR ) \n"
Original    (026): ['rlf_value', '=', '(', 'n_samples', '-', 'p', '-', 'q', ')', '*', 'np', '.', 'log10', '(', 'sigma2', ')', '+', 'n_samples', '*', 'np', '.', 'log10', '(', 'detR', ')', '\\n']
Tokenized   (040): ['[CLS]', 'r', '##lf', '_', 'value', '=', '(', 'n', '_', 'samples', '-', 'p', '-', 'q', ')', '*', 'np', '.', 'log', '##10', '(', 'sigma', '##2', ')', '+', 'n', '_', 'samples', '*', 'np', '.', 'log', '##10', '(', 'det', '##r', ')', '\\', 'n', '[SEP]']
Filtered   (038): ['r', '##lf', '_', 'value', '=', '(', 'n', '_', 'samples', '-', 'p', '-', 'q', ')', '*', 'np', '.', 'log', '##10', '(', 'sigma', '##2', ')', '+', 'n', '_', 'samples', '*', 'np', '.', 'log', '##10', '(', 'det', '##r', ')', '\\', 'n']
Detokenized (026): ['r##lf_value', '=', '(', 'n_samples', '-', 'p', '-', 'q', ')', '*', 'np', '.', 'log##10', '(', 'sigma##2', ')', '+', 'n_samples', '*', 'np', '.', 'log##10', '(', 'det##r', ')', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "log10t [ i ] - np . log10 ( thetaL [ 0 ] [ i ] ) } ) \n"
Original    (020): ['log10t', '[', 'i', ']', '-', 'np', '.', 'log10', '(', 'thetaL', '[', '0', ']', '[', 'i', ']', ')', '}', ')', '\\n']
Tokenized   (027): ['[CLS]', 'log', '##10', '##t', '[', 'i', ']', '-', 'np', '.', 'log', '##10', '(', 'theta', '##l', '[', '0', ']', '[', 'i', ']', ')', '}', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['log', '##10', '##t', '[', 'i', ']', '-', 'np', '.', 'log', '##10', '(', 'theta', '##l', '[', '0', ']', '[', 'i', ']', ')', '}', ')', '\\', 'n']
Detokenized (020): ['log##10##t', '[', 'i', ']', '-', 'np', '.', 'log##10', '(', 'theta##l', '[', '0', ']', '[', 'i', ']', ')', '}', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "np . log10 ( thetaU [ 0 ] [ i ] ) - log10t [ i ] } ) \n"
Original    (020): ['np', '.', 'log10', '(', 'thetaU', '[', '0', ']', '[', 'i', ']', ')', '-', 'log10t', '[', 'i', ']', '}', ')', '\\n']
Tokenized   (027): ['[CLS]', 'np', '.', 'log', '##10', '(', 'theta', '##u', '[', '0', ']', '[', 'i', ']', ')', '-', 'log', '##10', '##t', '[', 'i', ']', '}', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['np', '.', 'log', '##10', '(', 'theta', '##u', '[', '0', ']', '[', 'i', ']', ')', '-', 'log', '##10', '##t', '[', 'i', ']', '}', ')', '\\', 'n']
Detokenized (020): ['np', '.', 'log##10', '(', 'theta##u', '[', '0', ']', '[', 'i', ']', ')', '-', 'log##10##t', '[', 'i', ']', '}', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "sol = minimize ( rlf_transform , x0 , method = , \n"
Original    (012): ['sol', '=', 'minimize', '(', 'rlf_transform', ',', 'x0', ',', 'method', '=', ',', '\\n']
Tokenized   (019): ['[CLS]', 'sol', '=', 'minimize', '(', 'r', '##lf', '_', 'transform', ',', 'x', '##0', ',', 'method', '=', ',', '\\', 'n', '[SEP]']
Filtered   (017): ['sol', '=', 'minimize', '(', 'r', '##lf', '_', 'transform', ',', 'x', '##0', ',', 'method', '=', ',', '\\', 'n']
Detokenized (012): ['sol', '=', 'minimize', '(', 'r##lf_transform', ',', 'x##0', ',', 'method', '=', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "optimal_theta = 10. ** log10_optimal_x \n"
Original    (006): ['optimal_theta', '=', '10.', '**', 'log10_optimal_x', '\\n']
Tokenized   (018): ['[CLS]', 'optimal', '_', 'theta', '=', '10', '.', '*', '*', 'log', '##10', '_', 'optimal', '_', 'x', '\\', 'n', '[SEP]']
Filtered   (016): ['optimal', '_', 'theta', '=', '10', '.', '*', '*', 'log', '##10', '_', 'optimal', '_', 'x', '\\', 'n']
Detokenized (006): ['optimal_theta', '=', '10.', '**', 'log##10_optimal_x', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "r_t = solve_triangular ( C , r_ . T , lower = True ) \n"
Original    (015): ['r_t', '=', 'solve_triangular', '(', 'C', ',', 'r_', '.', 'T', ',', 'lower', '=', 'True', ')', '\\n']
Tokenized   (023): ['[CLS]', 'r', '_', 't', '=', 'solve', '_', 'triangular', '(', 'c', ',', 'r', '_', '.', 't', ',', 'lower', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['r', '_', 't', '=', 'solve', '_', 'triangular', '(', 'c', ',', 'r', '_', '.', 't', ',', 'lower', '=', 'true', ')', '\\', 'n']
Detokenized (015): ['r_t', '=', 'solve_triangular', '(', 'c', ',', 'r_', '.', 't', ',', 'lower', '=', 'true', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "dx = l1_cross_distances ( X , Y = self . X [ i ] ) \n"
Original    (016): ['dx', '=', 'l1_cross_distances', '(', 'X', ',', 'Y', '=', 'self', '.', 'X', '[', 'i', ']', ')', '\\n']
Tokenized   (025): ['[CLS]', 'd', '##x', '=', 'l', '##1', '_', 'cross', '_', 'distances', '(', 'x', ',', 'y', '=', 'self', '.', 'x', '[', 'i', ']', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['d', '##x', '=', 'l', '##1', '_', 'cross', '_', 'distances', '(', 'x', ',', 'y', '=', 'self', '.', 'x', '[', 'i', ']', ')', '\\', 'n']
Detokenized (016): ['d##x', '=', 'l##1_cross_distances', '(', 'x', ',', 'y', '=', 'self', '.', 'x', '[', 'i', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "r_ = self . corr ( self . theta [ i ] , dx ) . reshape ( n_eval , self . n_samples [ i ] ) \n"
Original    (028): ['r_', '=', 'self', '.', 'corr', '(', 'self', '.', 'theta', '[', 'i', ']', ',', 'dx', ')', '.', 'reshape', '(', 'n_eval', ',', 'self', '.', 'n_samples', '[', 'i', ']', ')', '\\n']
Tokenized   (041): ['[CLS]', 'r', '_', '=', 'self', '.', 'co', '##rr', '(', 'self', '.', 'theta', '[', 'i', ']', ',', 'd', '##x', ')', '.', 'res', '##ha', '##pe', '(', 'n', '_', 'eva', '##l', ',', 'self', '.', 'n', '_', 'samples', '[', 'i', ']', ')', '\\', 'n', '[SEP]']
Filtered   (039): ['r', '_', '=', 'self', '.', 'co', '##rr', '(', 'self', '.', 'theta', '[', 'i', ']', ',', 'd', '##x', ')', '.', 'res', '##ha', '##pe', '(', 'n', '_', 'eva', '##l', ',', 'self', '.', 'n', '_', 'samples', '[', 'i', ']', ')', '\\', 'n']
Detokenized (028): ['r_', '=', 'self', '.', 'co##rr', '(', 'self', '.', 'theta', '[', 'i', ']', ',', 'd##x', ')', '.', 'res##ha##pe', '(', 'n_eva##l', ',', 'self', '.', 'n_samples', '[', 'i', ']', ')', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 28, 768)
# Extracted words:  28
Sentence         : "yt = solve_triangular ( C , self . y [ i ] , lower = True ) \n"
Original    (018): ['yt', '=', 'solve_triangular', '(', 'C', ',', 'self', '.', 'y', '[', 'i', ']', ',', 'lower', '=', 'True', ')', '\\n']
Tokenized   (024): ['[CLS]', 'y', '##t', '=', 'solve', '_', 'triangular', '(', 'c', ',', 'self', '.', 'y', '[', 'i', ']', ',', 'lower', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['y', '##t', '=', 'solve', '_', 'triangular', '(', 'c', ',', 'self', '.', 'y', '[', 'i', ']', ',', 'lower', '=', 'true', ')', '\\', 'n']
Detokenized (018): ['y##t', '=', 'solve_triangular', '(', 'c', ',', 'self', '.', 'y', '[', 'i', ']', ',', 'lower', '=', 'true', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "mu [ : , i ] = ( np . dot ( f . T , beta ) + np . dot ( r_t . T , yt - np . dot ( Ft , beta ) ) ) . ravel ( ) \n"
Original    (044): ['mu', '[', ':', ',', 'i', ']', '=', '(', 'np', '.', 'dot', '(', 'f', '.', 'T', ',', 'beta', ')', '+', 'np', '.', 'dot', '(', 'r_t', '.', 'T', ',', 'yt', '-', 'np', '.', 'dot', '(', 'Ft', ',', 'beta', ')', ')', ')', '.', 'ravel', '(', ')', '\\n']
Tokenized   (051): ['[CLS]', 'mu', '[', ':', ',', 'i', ']', '=', '(', 'np', '.', 'dot', '(', 'f', '.', 't', ',', 'beta', ')', '+', 'np', '.', 'dot', '(', 'r', '_', 't', '.', 't', ',', 'y', '##t', '-', 'np', '.', 'dot', '(', 'ft', ',', 'beta', ')', ')', ')', '.', 'rave', '##l', '(', ')', '\\', 'n', '[SEP]']
Filtered   (049): ['mu', '[', ':', ',', 'i', ']', '=', '(', 'np', '.', 'dot', '(', 'f', '.', 't', ',', 'beta', ')', '+', 'np', '.', 'dot', '(', 'r', '_', 't', '.', 't', ',', 'y', '##t', '-', 'np', '.', 'dot', '(', 'ft', ',', 'beta', ')', ')', ')', '.', 'rave', '##l', '(', ')', '\\', 'n']
Detokenized (044): ['mu', '[', ':', ',', 'i', ']', '=', '(', 'np', '.', 'dot', '(', 'f', '.', 't', ',', 'beta', ')', '+', 'np', '.', 'dot', '(', 'r_t', '.', 't', ',', 'y##t', '-', 'np', '.', 'dot', '(', 'ft', ',', 'beta', ')', ')', ')', '.', 'rave##l', '(', ')', '\\n']
Counter: 49
===================================================================
Hidden states:  (13, 44, 768)
# Extracted words:  44
Sentence         : "u_ = solve_triangular ( G . T , f - np . dot ( Ft . T , r_t ) , lower = True ) \n"
Original    (026): ['u_', '=', 'solve_triangular', '(', 'G', '.', 'T', ',', 'f', '-', 'np', '.', 'dot', '(', 'Ft', '.', 'T', ',', 'r_t', ')', ',', 'lower', '=', 'True', ')', '\\n']
Tokenized   (034): ['[CLS]', 'u', '_', '=', 'solve', '_', 'triangular', '(', 'g', '.', 't', ',', 'f', '-', 'np', '.', 'dot', '(', 'ft', '.', 't', ',', 'r', '_', 't', ')', ',', 'lower', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (032): ['u', '_', '=', 'solve', '_', 'triangular', '(', 'g', '.', 't', ',', 'f', '-', 'np', '.', 'dot', '(', 'ft', '.', 't', ',', 'r', '_', 't', ')', ',', 'lower', '=', 'true', ')', '\\', 'n']
Detokenized (026): ['u_', '=', 'solve_triangular', '(', 'g', '.', 't', ',', 'f', '-', 'np', '.', 'dot', '(', 'ft', '.', 't', ',', 'r_t', ')', ',', 'lower', '=', 'true', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "sigma2_rho = ( sigma2_rho * g ) . sum ( axis = 1 ) \n"
Original    (015): ['sigma2_rho', '=', '(', 'sigma2_rho', '*', 'g', ')', '.', 'sum', '(', 'axis', '=', '1', ')', '\\n']
Tokenized   (026): ['[CLS]', 'sigma', '##2', '_', 'r', '##ho', '=', '(', 'sigma', '##2', '_', 'r', '##ho', '*', 'g', ')', '.', 'sum', '(', 'axis', '=', '1', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['sigma', '##2', '_', 'r', '##ho', '=', '(', 'sigma', '##2', '_', 'r', '##ho', '*', 'g', ')', '.', 'sum', '(', 'axis', '=', '1', ')', '\\', 'n']
Detokenized (015): ['sigma##2_r##ho', '=', '(', 'sigma##2_r##ho', '*', 'g', ')', '.', 'sum', '(', 'axis', '=', '1', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "MSE [ : , i ] = sigma2_rho * MSE [ : , i - 1 ] + Q_ / ( 2 * ( self . n_samples [ i ] - self . p [ i ] - self . q [ i ] ) ) * ( 1 - ( r_t ** 2 ) . sum ( axis = 0 ) ) + self . sigma2 [ i ] * ( u_ ** 2 ) . sum ( axis = 0 ) \n"
Original    (084): ['MSE', '[', ':', ',', 'i', ']', '=', 'sigma2_rho', '*', 'MSE', '[', ':', ',', 'i', '-', '1', ']', '+', 'Q_', '/', '(', '2', '*', '(', 'self', '.', 'n_samples', '[', 'i', ']', '-', 'self', '.', 'p', '[', 'i', ']', '-', 'self', '.', 'q', '[', 'i', ']', ')', ')', '*', '(', '1', '-', '(', 'r_t', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', ')', '+', 'self', '.', 'sigma2', '[', 'i', ']', '*', '(', 'u_', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', '\\n']
Tokenized   (102): ['[CLS]', 'ms', '##e', '[', ':', ',', 'i', ']', '=', 'sigma', '##2', '_', 'r', '##ho', '*', 'ms', '##e', '[', ':', ',', 'i', '-', '1', ']', '+', 'q', '_', '/', '(', '2', '*', '(', 'self', '.', 'n', '_', 'samples', '[', 'i', ']', '-', 'self', '.', 'p', '[', 'i', ']', '-', 'self', '.', 'q', '[', 'i', ']', ')', ')', '*', '(', '1', '-', '(', 'r', '_', 't', '*', '*', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', ')', '+', 'self', '.', 'sigma', '##2', '[', 'i', ']', '*', '(', 'u', '_', '*', '*', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', '\\', 'n', '[SEP]']
Filtered   (100): ['ms', '##e', '[', ':', ',', 'i', ']', '=', 'sigma', '##2', '_', 'r', '##ho', '*', 'ms', '##e', '[', ':', ',', 'i', '-', '1', ']', '+', 'q', '_', '/', '(', '2', '*', '(', 'self', '.', 'n', '_', 'samples', '[', 'i', ']', '-', 'self', '.', 'p', '[', 'i', ']', '-', 'self', '.', 'q', '[', 'i', ']', ')', ')', '*', '(', '1', '-', '(', 'r', '_', 't', '*', '*', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', ')', '+', 'self', '.', 'sigma', '##2', '[', 'i', ']', '*', '(', 'u', '_', '*', '*', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', '\\', 'n']
Detokenized (084): ['ms##e', '[', ':', ',', 'i', ']', '=', 'sigma##2_r##ho', '*', 'ms##e', '[', ':', ',', 'i', '-', '1', ']', '+', 'q_', '/', '(', '2', '*', '(', 'self', '.', 'n_samples', '[', 'i', ']', '-', 'self', '.', 'p', '[', 'i', ']', '-', 'self', '.', 'q', '[', 'i', ']', ')', ')', '*', '(', '1', '-', '(', 'r_t', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', ')', '+', 'self', '.', 'sigma##2', '[', 'i', ']', '*', '(', 'u_', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', '\\n']
Counter: 100
===================================================================
Hidden states:  (13, 84, 768)
# Extracted words:  84
Sentence         : "n_features = np . zeros ( nlevel , dtype = int ) \n"
Original    (013): ['n_features', '=', 'np', '.', 'zeros', '(', 'nlevel', ',', 'dtype', '=', 'int', ')', '\\n']
Tokenized   (022): ['[CLS]', 'n', '_', 'features', '=', 'np', '.', 'zero', '##s', '(', 'nl', '##eve', '##l', ',', 'dt', '##ype', '=', 'int', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['n', '_', 'features', '=', 'np', '.', 'zero', '##s', '(', 'nl', '##eve', '##l', ',', 'dt', '##ype', '=', 'int', ')', '\\', 'n']
Detokenized (013): ['n_features', '=', 'np', '.', 'zero##s', '(', 'nl##eve##l', ',', 'dt##ype', '=', 'int', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "n_samples_y [ i ] = y [ i ] . shape [ 0 ] \n"
Original    (015): ['n_samples_y', '[', 'i', ']', '=', 'y', '[', 'i', ']', '.', 'shape', '[', '0', ']', '\\n']
Tokenized   (022): ['[CLS]', 'n', '_', 'samples', '_', 'y', '[', 'i', ']', '=', 'y', '[', 'i', ']', '.', 'shape', '[', '0', ']', '\\', 'n', '[SEP]']
Filtered   (020): ['n', '_', 'samples', '_', 'y', '[', 'i', ']', '=', 'y', '[', 'i', ']', '.', 'shape', '[', '0', ']', '\\', 'n']
Detokenized (015): ['n_samples_y', '[', 'i', ']', '=', 'y', '[', 'i', ']', '.', 'shape', '[', '0', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "Y_pred , MSE = self . model . predict ( [ new_x ] ) \n"
Original    (015): ['Y_pred', ',', 'MSE', '=', 'self', '.', 'model', '.', 'predict', '(', '[', 'new_x', ']', ')', '\\n']
Tokenized   (024): ['[CLS]', 'y', '_', 'pre', '##d', ',', 'ms', '##e', '=', 'self', '.', 'model', '.', 'predict', '(', '[', 'new', '_', 'x', ']', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['y', '_', 'pre', '##d', ',', 'ms', '##e', '=', 'self', '.', 'model', '.', 'predict', '(', '[', 'new', '_', 'x', ']', ')', '\\', 'n']
Detokenized (015): ['y_pre##d', ',', 'ms##e', '=', 'self', '.', 'model', '.', 'predict', '(', '[', 'new_x', ']', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "X , Y = self . _fit_adapter ( X , Y ) \n"
Original    (013): ['X', ',', 'Y', '=', 'self', '.', '_fit_adapter', '(', 'X', ',', 'Y', ')', '\\n']
Tokenized   (020): ['[CLS]', 'x', ',', 'y', '=', 'self', '.', '_', 'fit', '_', 'adapt', '##er', '(', 'x', ',', 'y', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['x', ',', 'y', '=', 'self', '.', '_', 'fit', '_', 'adapt', '##er', '(', 'x', ',', 'y', ')', '\\', 'n']
Detokenized (013): ['x', ',', 'y', '=', 'self', '.', '_fit_adapt##er', '(', 'x', ',', 'y', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "Y = [ np . array ( y ) for y in reversed ( Y ) ] \n"
Original    (018): ['Y', '=', '[', 'np', '.', 'array', '(', 'y', ')', 'for', 'y', 'in', 'reversed', '(', 'Y', ')', ']', '\\n']
Tokenized   (021): ['[CLS]', 'y', '=', '[', 'np', '.', 'array', '(', 'y', ')', 'for', 'y', 'in', 'reversed', '(', 'y', ')', ']', '\\', 'n', '[SEP]']
Filtered   (019): ['y', '=', '[', 'np', '.', 'array', '(', 'y', ')', 'for', 'y', 'in', 'reversed', '(', 'y', ')', ']', '\\', 'n']
Detokenized (018): ['y', '=', '[', 'np', '.', 'array', '(', 'y', ')', 'for', 'y', 'in', 'reversed', '(', 'y', ')', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "newdata = np . array ( parsed [ : ] ) \n"
Original    (012): ['newdata', '=', 'np', '.', 'array', '(', 'parsed', '[', ':', ']', ')', '\\n']
Tokenized   (018): ['[CLS]', 'new', '##da', '##ta', '=', 'np', '.', 'array', '(', 'par', '##sed', '[', ':', ']', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['new', '##da', '##ta', '=', 'np', '.', 'array', '(', 'par', '##sed', '[', ':', ']', ')', '\\', 'n']
Detokenized (012): ['new##da##ta', '=', 'np', '.', 'array', '(', 'par##sed', '[', ':', ']', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "icc . DB_USER ) , shell = True ) \n"
Original    (010): ['icc', '.', 'DB_USER', ')', ',', 'shell', '=', 'True', ')', '\\n']
Tokenized   (015): ['[CLS]', 'icc', '.', 'db', '_', 'user', ')', ',', 'shell', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['icc', '.', 'db', '_', 'user', ')', ',', 'shell', '=', 'true', ')', '\\', 'n']
Detokenized (010): ['icc', '.', 'db_user', ')', ',', 'shell', '=', 'true', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "instance_db_name , shell = True ) \n"
Original    (007): ['instance_db_name', ',', 'shell', '=', 'True', ')', '\\n']
Tokenized   (014): ['[CLS]', 'instance', '_', 'db', '_', 'name', ',', 'shell', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['instance', '_', 'db', '_', 'name', ',', 'shell', '=', 'true', ')', '\\', 'n']
Detokenized (007): ['instance_db_name', ',', 'shell', '=', 'true', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "customslide = CustomSlide . objects . create ( title = , text = default_projector = Projector . objects . get ( pk = 1 ) \n"
Original    (026): ['customslide', '=', 'CustomSlide', '.', 'objects', '.', 'create', '(', 'title', '=', ',', 'text', '=', 'default_projector', '=', 'Projector', '.', 'objects', '.', 'get', '(', 'pk', '=', '1', ')', '\\n']
Tokenized   (036): ['[CLS]', 'customs', '##lide', '=', 'customs', '##lide', '.', 'objects', '.', 'create', '(', 'title', '=', ',', 'text', '=', 'default', '_', 'project', '##or', '=', 'project', '##or', '.', 'objects', '.', 'get', '(', 'p', '##k', '=', '1', ')', '\\', 'n', '[SEP]']
Filtered   (034): ['customs', '##lide', '=', 'customs', '##lide', '.', 'objects', '.', 'create', '(', 'title', '=', ',', 'text', '=', 'default', '_', 'project', '##or', '=', 'project', '##or', '.', 'objects', '.', 'get', '(', 'p', '##k', '=', '1', ')', '\\', 'n']
Detokenized (026): ['customs##lide', '=', 'customs##lide', '.', 'objects', '.', 'create', '(', 'title', '=', ',', 'text', '=', 'default_project##or', '=', 'project##or', '.', 'objects', '.', 'get', '(', 'p##k', '=', '1', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "default_projector = Projector . objects . get ( pk = 1 ) \n"
Original    (013): ['default_projector', '=', 'Projector', '.', 'objects', '.', 'get', '(', 'pk', '=', '1', ')', '\\n']
Tokenized   (021): ['[CLS]', 'default', '_', 'project', '##or', '=', 'project', '##or', '.', 'objects', '.', 'get', '(', 'p', '##k', '=', '1', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['default', '_', 'project', '##or', '=', 'project', '##or', '.', 'objects', '.', 'get', '(', 'p', '##k', '=', '1', ')', '\\', 'n']
Detokenized (013): ['default_project##or', '=', 'project##or', '.', 'objects', '.', 'get', '(', 'p##k', '=', '1', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "reverse ( , args = [ ] ) ) \n"
Original    (010): ['reverse', '(', ',', 'args', '=', '[', ']', ')', ')', '\\n']
Tokenized   (014): ['[CLS]', 'reverse', '(', ',', 'ar', '##gs', '=', '[', ']', ')', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['reverse', '(', ',', 'ar', '##gs', '=', '[', ']', ')', ')', '\\', 'n']
Detokenized (010): ['reverse', '(', ',', 'ar##gs', '=', '[', ']', ')', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "yield ConfigVariable ( \n"
Original    (004): ['yield', 'ConfigVariable', '(', '\\n']
Tokenized   (011): ['[CLS]', 'yield', 'con', '##fi', '##g', '##var', '##iable', '(', '\\', 'n', '[SEP]']
Filtered   (009): ['yield', 'con', '##fi', '##g', '##var', '##iable', '(', '\\', 'n']
Detokenized (004): ['yield', 'con##fi##g##var##iable', '(', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "{ : , : } , { : , : } ) \n"
Original    (013): ['{', ':', ',', ':', '}', ',', '{', ':', ',', ':', '}', ')', '\\n']
Tokenized   (016): ['[CLS]', '{', ':', ',', ':', '}', ',', '{', ':', ',', ':', '}', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['{', ':', ',', ':', '}', ',', '{', ':', ',', ':', '}', ')', '\\', 'n']
Detokenized (013): ['{', ':', ',', ':', '}', ',', '{', ':', ',', ':', '}', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "validators = ( validator_for_testing , ) ) \n"
Original    (008): ['validators', '=', '(', 'validator_for_testing', ',', ')', ')', '\\n']
Tokenized   (017): ['[CLS]', 'valid', '##ators', '=', '(', 'valid', '##ator', '_', 'for', '_', 'testing', ',', ')', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['valid', '##ators', '=', '(', 'valid', '##ator', '_', 'for', '_', 'testing', ',', ')', ')', '\\', 'n']
Detokenized (008): ['valid##ators', '=', '(', 'valid##ator_for_testing', ',', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "generate_username . return_value = \n"
Original    (005): ['generate_username', '.', 'return_value', '=', '\\n']
Tokenized   (013): ['[CLS]', 'generate', '_', 'user', '##name', '.', 'return', '_', 'value', '=', '\\', 'n', '[SEP]']
Filtered   (011): ['generate', '_', 'user', '##name', '.', 'return', '_', 'value', '=', '\\', 'n']
Detokenized (005): ['generate_user##name', '.', 'return_value', '=', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "serializer = UserFullSerializer ( context = { : view } ) \n"
Original    (012): ['serializer', '=', 'UserFullSerializer', '(', 'context', '=', '{', ':', 'view', '}', ')', '\\n']
Tokenized   (020): ['[CLS]', 'serial', '##izer', '=', 'user', '##ful', '##ls', '##eria', '##lizer', '(', 'context', '=', '{', ':', 'view', '}', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['serial', '##izer', '=', 'user', '##ful', '##ls', '##eria', '##lizer', '(', 'context', '=', '{', ':', 'view', '}', ')', '\\', 'n']
Detokenized (012): ['serial##izer', '=', 'user##ful##ls##eria##lizer', '(', 'context', '=', '{', ':', 'view', '}', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "#domain... #localhost... \n"
Original    (003): ['#domain...', '#localhost...', '\\n']
Tokenized   (016): ['[CLS]', '#', 'domain', '.', '.', '.', '#', 'local', '##hos', '##t', '.', '.', '.', '\\', 'n', '[SEP]']
Filtered   (014): ['#', 'domain', '.', '.', '.', '#', 'local', '##hos', '##t', '.', '.', '.', '\\', 'n']
Detokenized (003): ['#domain...', '#local##hos##t...', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "USERNAME_REGEX = re . compile ( , re . I ) \n"
Original    (012): ['USERNAME_REGEX', '=', 're', '.', 'compile', '(', ',', 're', '.', 'I', ')', '\\n']
Tokenized   (020): ['[CLS]', 'user', '##name', '_', 'reg', '##ex', '=', 're', '.', 'com', '##pile', '(', ',', 're', '.', 'i', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['user', '##name', '_', 'reg', '##ex', '=', 're', '.', 'com', '##pile', '(', ',', 're', '.', 'i', ')', '\\', 'n']
Detokenized (012): ['user##name_reg##ex', '=', 're', '.', 'com##pile', '(', ',', 're', '.', 'i', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "RouteDistinguisher . TYPE_IP_LOC , None , \n"
Original    (007): ['RouteDistinguisher', '.', 'TYPE_IP_LOC', ',', 'None', ',', '\\n']
Tokenized   (019): ['[CLS]', 'routed', '##ist', '##ing', '##uis', '##her', '.', 'type', '_', 'ip', '_', 'lo', '##c', ',', 'none', ',', '\\', 'n', '[SEP]']
Filtered   (017): ['routed', '##ist', '##ing', '##uis', '##her', '.', 'type', '_', 'ip', '_', 'lo', '##c', ',', 'none', ',', '\\', 'n']
Detokenized (007): ['routed##ist##ing##uis##her', '.', 'type_ip_lo##c', ',', 'none', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "10000 + label ) \n"
Original    (005): ['10000', '+', 'label', ')', '\\n']
Tokenized   (009): ['[CLS]', '1000', '##0', '+', 'label', ')', '\\', 'n', '[SEP]']
Filtered   (007): ['1000', '##0', '+', 'label', ')', '\\', 'n']
Detokenized (005): ['1000##0', '+', 'label', ')', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "nh = Inet ( 1 , socket . inet_pton ( socket . AF_INET , \n"
Original    (015): ['nh', '=', 'Inet', '(', '1', ',', 'socket', '.', 'inet_pton', '(', 'socket', '.', 'AF_INET', ',', '\\n']
Tokenized   (026): ['[CLS]', 'nh', '=', 'in', '##et', '(', '1', ',', 'socket', '.', 'in', '##et', '_', 'pt', '##on', '(', 'socket', '.', 'af', '_', 'in', '##et', ',', '\\', 'n', '[SEP]']
Filtered   (024): ['nh', '=', 'in', '##et', '(', '1', ',', 'socket', '.', 'in', '##et', '_', 'pt', '##on', '(', 'socket', '.', 'af', '_', 'in', '##et', ',', '\\', 'n']
Detokenized (015): ['nh', '=', 'in##et', '(', '1', ',', 'socket', '.', 'in##et_pt##on', '(', 'socket', '.', 'af_in##et', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "route . attributes . add ( ECommunities ( self . readvertiseToRTs ) ) \n"
Original    (014): ['route', '.', 'attributes', '.', 'add', '(', 'ECommunities', '(', 'self', '.', 'readvertiseToRTs', ')', ')', '\\n']
Tokenized   (024): ['[CLS]', 'route', '.', 'attributes', '.', 'add', '(', 'eco', '##mm', '##uni', '##ties', '(', 'self', '.', 'read', '##vert', '##ise', '##tort', '##s', ')', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['route', '.', 'attributes', '.', 'add', '(', 'eco', '##mm', '##uni', '##ties', '(', 'self', '.', 'read', '##vert', '##ise', '##tort', '##s', ')', ')', '\\', 'n']
Detokenized (014): ['route', '.', 'attributes', '.', 'add', '(', 'eco##mm##uni##ties', '(', 'self', '.', 'read##vert##ise##tort##s', ')', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "nlri . prefix , label ) \n"
Original    (007): ['nlri', '.', 'prefix', ',', 'label', ')', '\\n']
Tokenized   (011): ['[CLS]', 'nl', '##ri', '.', 'prefix', ',', 'label', ')', '\\', 'n', '[SEP]']
Filtered   (009): ['nl', '##ri', '.', 'prefix', ',', 'label', ')', '\\', 'n']
Detokenized (007): ['nl##ri', '.', 'prefix', ',', 'label', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "set ( self . importRTs ) ) ) > 0 ) \n"
Original    (012): ['set', '(', 'self', '.', 'importRTs', ')', ')', ')', '>', '0', ')', '\\n']
Tokenized   (016): ['[CLS]', 'set', '(', 'self', '.', 'import', '##rts', ')', ')', ')', '>', '0', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['set', '(', 'self', '.', 'import', '##rts', ')', ')', ')', '>', '0', ')', '\\', 'n']
Detokenized (012): ['set', '(', 'self', '.', 'import##rts', ')', ')', ')', '>', '0', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "newRoute . nlri . labelStack [ 0 ] . labelValue , newRoute . nlri , encaps ) \n"
Original    (018): ['newRoute', '.', 'nlri', '.', 'labelStack', '[', '0', ']', '.', 'labelValue', ',', 'newRoute', '.', 'nlri', ',', 'encaps', ')', '\\n']
Tokenized   (033): ['[CLS]', 'new', '##rou', '##te', '.', 'nl', '##ri', '.', 'labels', '##ta', '##ck', '[', '0', ']', '.', 'label', '##val', '##ue', ',', 'new', '##rou', '##te', '.', 'nl', '##ri', ',', 'en', '##cap', '##s', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['new', '##rou', '##te', '.', 'nl', '##ri', '.', 'labels', '##ta', '##ck', '[', '0', ']', '.', 'label', '##val', '##ue', ',', 'new', '##rou', '##te', '.', 'nl', '##ri', ',', 'en', '##cap', '##s', ')', '\\', 'n']
Detokenized (018): ['new##rou##te', '.', 'nl##ri', '.', 'labels##ta##ck', '[', '0', ']', '.', 'label##val##ue', ',', 'new##rou##te', '.', 'nl##ri', ',', 'en##cap##s', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "prefix , oldRoute . attributes . get ( NextHop . ID ) . next_hop , \n"
Original    (016): ['prefix', ',', 'oldRoute', '.', 'attributes', '.', 'get', '(', 'NextHop', '.', 'ID', ')', '.', 'next_hop', ',', '\\n']
Tokenized   (024): ['[CLS]', 'prefix', ',', 'old', '##rou', '##te', '.', 'attributes', '.', 'get', '(', 'next', '##hop', '.', 'id', ')', '.', 'next', '_', 'hop', ',', '\\', 'n', '[SEP]']
Filtered   (022): ['prefix', ',', 'old', '##rou', '##te', '.', 'attributes', '.', 'get', '(', 'next', '##hop', '.', 'id', ')', '.', 'next', '_', 'hop', ',', '\\', 'n']
Detokenized (016): ['prefix', ',', 'old##rou##te', '.', 'attributes', '.', 'get', '(', 'next##hop', '.', 'id', ')', '.', 'next_hop', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "oldRoute . nlri . labelStack [ 0 ] . labelValue , oldRoute . nlri ) \n"
Original    (016): ['oldRoute', '.', 'nlri', '.', 'labelStack', '[', '0', ']', '.', 'labelValue', ',', 'oldRoute', '.', 'nlri', ')', '\\n']
Tokenized   (029): ['[CLS]', 'old', '##rou', '##te', '.', 'nl', '##ri', '.', 'labels', '##ta', '##ck', '[', '0', ']', '.', 'label', '##val', '##ue', ',', 'old', '##rou', '##te', '.', 'nl', '##ri', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['old', '##rou', '##te', '.', 'nl', '##ri', '.', 'labels', '##ta', '##ck', '[', '0', ']', '.', 'label', '##val', '##ue', ',', 'old', '##rou', '##te', '.', 'nl', '##ri', ')', '\\', 'n']
Detokenized (016): ['old##rou##te', '.', 'nl##ri', '.', 'labels##ta##ck', '[', '0', ']', '.', 'label##val##ue', ',', 'old##rou##te', '.', 'nl##ri', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : ""readvertised" : ( LGMap . VALUE , [ repr ( prefix ) for prefix in \n"
Original    (016): ['"readvertised"', ':', '(', 'LGMap', '.', 'VALUE', ',', '[', 'repr', '(', 'prefix', ')', 'for', 'prefix', 'in', '\\n']
Tokenized   (026): ['[CLS]', '"', 'read', '##vert', '##ised', '"', ':', '(', 'l', '##gm', '##ap', '.', 'value', ',', '[', 'rep', '##r', '(', 'prefix', ')', 'for', 'prefix', 'in', '\\', 'n', '[SEP]']
Filtered   (024): ['"', 'read', '##vert', '##ised', '"', ':', '(', 'l', '##gm', '##ap', '.', 'value', ',', '[', 'rep', '##r', '(', 'prefix', ')', 'for', 'prefix', 'in', '\\', 'n']
Detokenized (016): ['"read##vert##ised"', ':', '(', 'l##gm##ap', '.', 'value', ',', '[', 'rep##r', '(', 'prefix', ')', 'for', 'prefix', 'in', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "REACTORNAME = DEFAULT_REACTORS . get ( platform . system ( ) , ) \n"
Original    (014): ['REACTORNAME', '=', 'DEFAULT_REACTORS', '.', 'get', '(', 'platform', '.', 'system', '(', ')', ',', ')', '\\n']
Tokenized   (020): ['[CLS]', 'reactor', '##name', '=', 'default', '_', 'reactors', '.', 'get', '(', 'platform', '.', 'system', '(', ')', ',', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['reactor', '##name', '=', 'default', '_', 'reactors', '.', 'get', '(', 'platform', '.', 'system', '(', ')', ',', ')', '\\', 'n']
Detokenized (014): ['reactor##name', '=', 'default_reactors', '.', 'get', '(', 'platform', '.', 'system', '(', ')', ',', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "set_reactor = lambda : reactor \n"
Original    (006): ['set_reactor', '=', 'lambda', ':', 'reactor', '\\n']
Tokenized   (011): ['[CLS]', 'set', '_', 'reactor', '=', 'lambda', ':', 'reactor', '\\', 'n', '[SEP]']
Filtered   (009): ['set', '_', 'reactor', '=', 'lambda', ':', 'reactor', '\\', 'n']
Detokenized (006): ['set_reactor', '=', 'lambda', ':', 'reactor', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "SIGNALS = dict ( ( k , v ) for v , k in signal . __dict__ . iteritems ( ) if v . startswith ( ) and not v . startswith ( ) ) \n"
Original    (036): ['SIGNALS', '=', 'dict', '(', '(', 'k', ',', 'v', ')', 'for', 'v', ',', 'k', 'in', 'signal', '.', '__dict__', '.', 'iteritems', '(', ')', 'if', 'v', '.', 'startswith', '(', ')', 'and', 'not', 'v', '.', 'startswith', '(', ')', ')', '\\n']
Tokenized   (050): ['[CLS]', 'signals', '=', 'di', '##ct', '(', '(', 'k', ',', 'v', ')', 'for', 'v', ',', 'k', 'in', 'signal', '.', '_', '_', 'di', '##ct', '_', '_', '.', 'it', '##eri', '##tem', '##s', '(', ')', 'if', 'v', '.', 'starts', '##with', '(', ')', 'and', 'not', 'v', '.', 'starts', '##with', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (048): ['signals', '=', 'di', '##ct', '(', '(', 'k', ',', 'v', ')', 'for', 'v', ',', 'k', 'in', 'signal', '.', '_', '_', 'di', '##ct', '_', '_', '.', 'it', '##eri', '##tem', '##s', '(', ')', 'if', 'v', '.', 'starts', '##with', '(', ')', 'and', 'not', 'v', '.', 'starts', '##with', '(', ')', ')', '\\', 'n']
Detokenized (036): ['signals', '=', 'di##ct', '(', '(', 'k', ',', 'v', ')', 'for', 'v', ',', 'k', 'in', 'signal', '.', '__di##ct__', '.', 'it##eri##tem##s', '(', ')', 'if', 'v', '.', 'starts##with', '(', ')', 'and', 'not', 'v', '.', 'starts##with', '(', ')', ')', '\\n']
Counter: 48
===================================================================
Hidden states:  (13, 36, 768)
# Extracted words:  36
Sentence         : "pargs = ( self . name , self . label , self . reactor ) \n"
Original    (016): ['pargs', '=', '(', 'self', '.', 'name', ',', 'self', '.', 'label', ',', 'self', '.', 'reactor', ')', '\\n']
Tokenized   (020): ['[CLS]', 'par', '##gs', '=', '(', 'self', '.', 'name', ',', 'self', '.', 'label', ',', 'self', '.', 'reactor', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['par', '##gs', '=', '(', 'self', '.', 'name', ',', 'self', '.', 'label', ',', 'self', '.', 'reactor', ')', '\\', 'n']
Detokenized (016): ['par##gs', '=', '(', 'self', '.', 'name', ',', 'self', '.', 'label', ',', 'self', '.', 'reactor', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "* pargs , ** pkwargs \n"
Original    (006): ['*', 'pargs', ',', '**', 'pkwargs', '\\n']
Tokenized   (014): ['[CLS]', '*', 'par', '##gs', ',', '*', '*', 'p', '##k', '##war', '##gs', '\\', 'n', '[SEP]']
Filtered   (012): ['*', 'par', '##gs', ',', '*', '*', 'p', '##k', '##war', '##gs', '\\', 'n']
Detokenized (006): ['*', 'par##gs', ',', '**', 'p##k##war##gs', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "logdir = env . pop ( , os . path . join ( os . path . sep , ) ) \n"
Original    (022): ['logdir', '=', 'env', '.', 'pop', '(', ',', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'sep', ',', ')', ')', '\\n']
Tokenized   (028): ['[CLS]', 'log', '##di', '##r', '=', 'en', '##v', '.', 'pop', '(', ',', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'sep', ',', ')', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['log', '##di', '##r', '=', 'en', '##v', '.', 'pop', '(', ',', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'sep', ',', ')', ')', '\\', 'n']
Detokenized (022): ['log##di##r', '=', 'en##v', '.', 'pop', '(', ',', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'sep', ',', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "masksignals = bool ( env . pop ( , True ) ) \n"
Original    (013): ['masksignals', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', 'True', ')', ')', '\\n']
Tokenized   (020): ['[CLS]', 'masks', '##ign', '##als', '=', 'boo', '##l', '(', 'en', '##v', '.', 'pop', '(', ',', 'true', ')', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['masks', '##ign', '##als', '=', 'boo', '##l', '(', 'en', '##v', '.', 'pop', '(', ',', 'true', ')', ')', '\\', 'n']
Detokenized (013): ['masks##ign##als', '=', 'boo##l', '(', 'en##v', '.', 'pop', '(', ',', 'true', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "usetty = bool ( env . pop ( , ) ) \n"
Original    (012): ['usetty', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', ')', ')', '\\n']
Tokenized   (018): ['[CLS]', 'use', '##tty', '=', 'boo', '##l', '(', 'en', '##v', '.', 'pop', '(', ',', ')', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['use', '##tty', '=', 'boo', '##l', '(', 'en', '##v', '.', 'pop', '(', ',', ')', ')', '\\', 'n']
Detokenized (012): ['use##tty', '=', 'boo##l', '(', 'en##v', '.', 'pop', '(', ',', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "maxfd = resource . getrlimit ( resource . RLIMIT_NOFILE ) [ 1 ] \n"
Original    (014): ['maxfd', '=', 'resource', '.', 'getrlimit', '(', 'resource', '.', 'RLIMIT_NOFILE', ')', '[', '1', ']', '\\n']
Tokenized   (028): ['[CLS]', 'max', '##f', '##d', '=', 'resource', '.', 'get', '##rl', '##imi', '##t', '(', 'resource', '.', 'r', '##lim', '##it', '_', 'no', '##fi', '##le', ')', '[', '1', ']', '\\', 'n', '[SEP]']
Filtered   (026): ['max', '##f', '##d', '=', 'resource', '.', 'get', '##rl', '##imi', '##t', '(', 'resource', '.', 'r', '##lim', '##it', '_', 'no', '##fi', '##le', ')', '[', '1', ']', '\\', 'n']
Detokenized (014): ['max##f##d', '=', 'resource', '.', 'get##rl##imi##t', '(', 'resource', '.', 'r##lim##it_no##fi##le', ')', '[', '1', ']', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "hasattr ( os , "devnull" ) and os . devnull or "/dev/null" , \n"
Original    (014): ['hasattr', '(', 'os', ',', '"devnull"', ')', 'and', 'os', '.', 'devnull', 'or', '"/dev/null"', ',', '\\n']
Tokenized   (030): ['[CLS]', 'has', '##att', '##r', '(', 'os', ',', '"', 'dev', '##nu', '##ll', '"', ')', 'and', 'os', '.', 'dev', '##nu', '##ll', 'or', '"', '/', 'dev', '/', 'null', '"', ',', '\\', 'n', '[SEP]']
Filtered   (028): ['has', '##att', '##r', '(', 'os', ',', '"', 'dev', '##nu', '##ll', '"', ')', 'and', 'os', '.', 'dev', '##nu', '##ll', 'or', '"', '/', 'dev', '/', 'null', '"', ',', '\\', 'n']
Detokenized (014): ['has##att##r', '(', 'os', ',', '"dev##nu##ll"', ')', 'and', 'os', '.', 'dev##nu##ll', 'or', '"/dev/null"', ',', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "droned . logging . logToDir ( \n"
Original    (007): ['droned', '.', 'logging', '.', 'logToDir', '(', '\\n']
Tokenized   (014): ['[CLS]', 'drone', '##d', '.', 'logging', '.', 'log', '##to', '##di', '##r', '(', '\\', 'n', '[SEP]']
Filtered   (012): ['drone', '##d', '.', 'logging', '.', 'log', '##to', '##di', '##r', '(', '\\', 'n']
Detokenized (007): ['drone##d', '.', 'logging', '.', 'log##to##di##r', '(', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "conversation . say ( contextSummary , useHTML = False ) \n"
Original    (011): ['conversation', '.', 'say', '(', 'contextSummary', ',', 'useHTML', '=', 'False', ')', '\\n']
Tokenized   (019): ['[CLS]', 'conversation', '.', 'say', '(', 'contexts', '##um', '##mar', '##y', ',', 'use', '##ht', '##ml', '=', 'false', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['conversation', '.', 'say', '(', 'contexts', '##um', '##mar', '##y', ',', 'use', '##ht', '##ml', '=', 'false', ')', '\\', 'n']
Detokenized (011): ['conversation', '.', 'say', '(', 'contexts##um##mar##y', ',', 'use##ht##ml', '=', 'false', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "moduleProvides ( IDroneDService ) #requirement \n"
Original    (006): ['moduleProvides', '(', 'IDroneDService', ')', '#requirement', '\\n']
Tokenized   (018): ['[CLS]', 'module', '##pro', '##vid', '##es', '(', 'id', '##rone', '##ds', '##er', '##vic', '##e', ')', '#', 'requirement', '\\', 'n', '[SEP]']
Filtered   (016): ['module', '##pro', '##vid', '##es', '(', 'id', '##rone', '##ds', '##er', '##vic', '##e', ')', '#', 'requirement', '\\', 'n']
Detokenized (006): ['module##pro##vid##es', '(', 'id##rone##ds##er##vic##e', ')', '#requirement', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "hour = property ( lambda foo : 3600 ) \n"
Original    (010): ['hour', '=', 'property', '(', 'lambda', 'foo', ':', '3600', ')', '\\n']
Tokenized   (014): ['[CLS]', 'hour', '=', 'property', '(', 'lambda', 'foo', ':', '360', '##0', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['hour', '=', 'property', '(', 'lambda', 'foo', ':', '360', '##0', ')', '\\', 'n']
Detokenized (010): ['hour', '=', 'property', '(', 'lambda', 'foo', ':', '360##0', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "watchDict = property ( lambda s : SERVICECONFIG . wrapped . get ( , { } ) ) \n"
Original    (019): ['watchDict', '=', 'property', '(', 'lambda', 's', ':', 'SERVICECONFIG', '.', 'wrapped', '.', 'get', '(', ',', '{', '}', ')', ')', '\\n']
Tokenized   (026): ['[CLS]', 'watch', '##dict', '=', 'property', '(', 'lambda', 's', ':', 'service', '##con', '##fi', '##g', '.', 'wrapped', '.', 'get', '(', ',', '{', '}', ')', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['watch', '##dict', '=', 'property', '(', 'lambda', 's', ':', 'service', '##con', '##fi', '##g', '.', 'wrapped', '.', 'get', '(', ',', '{', '}', ')', ')', '\\', 'n']
Detokenized (019): ['watch##dict', '=', 'property', '(', 'lambda', 's', ':', 'service##con##fi##g', '.', 'wrapped', '.', 'get', '(', ',', '{', '}', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "model = gm . throat_surface_area . cylinder ) \n"
Original    (009): ['model', '=', 'gm', '.', 'throat_surface_area', '.', 'cylinder', ')', '\\n']
Tokenized   (016): ['[CLS]', 'model', '=', 'gm', '.', 'throat', '_', 'surface', '_', 'area', '.', 'cylinder', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['model', '=', 'gm', '.', 'throat', '_', 'surface', '_', 'area', '.', 'cylinder', ')', '\\', 'n']
Detokenized (009): ['model', '=', 'gm', '.', 'throat_surface_area', '.', 'cylinder', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "pores = network . find_connected_pores ( throats , flatten = False ) \n"
Original    (013): ['pores', '=', 'network', '.', 'find_connected_pores', '(', 'throats', ',', 'flatten', '=', 'False', ')', '\\n']
Tokenized   (024): ['[CLS]', 'por', '##es', '=', 'network', '.', 'find', '_', 'connected', '_', 'por', '##es', '(', 'throat', '##s', ',', 'flat', '##ten', '=', 'false', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['por', '##es', '=', 'network', '.', 'find', '_', 'connected', '_', 'por', '##es', '(', 'throat', '##s', ',', 'flat', '##ten', '=', 'false', ')', '\\', 'n']
Detokenized (013): ['por##es', '=', 'network', '.', 'find_connected_por##es', '(', 'throat##s', ',', 'flat##ten', '=', 'false', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "C0 = network [ ] [ pores , 0 ] \n"
Original    (011): ['C0', '=', 'network', '[', ']', '[', 'pores', ',', '0', ']', '\\n']
Tokenized   (016): ['[CLS]', 'c', '##0', '=', 'network', '[', ']', '[', 'por', '##es', ',', '0', ']', '\\', 'n', '[SEP]']
Filtered   (014): ['c', '##0', '=', 'network', '[', ']', '[', 'por', '##es', ',', '0', ']', '\\', 'n']
Detokenized (011): ['c##0', '=', 'network', '[', ']', '[', 'por##es', ',', '0', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "P = phase [ pore_P ] / 100000 \n"
Original    (009): ['P', '=', 'phase', '[', 'pore_P', ']', '/', '100000', '\\n']
Tokenized   (016): ['[CLS]', 'p', '=', 'phase', '[', 'por', '##e', '_', 'p', ']', '/', '1000', '##00', '\\', 'n', '[SEP]']
Filtered   (014): ['p', '=', 'phase', '[', 'por', '##e', '_', 'p', ']', '/', '1000', '##00', '\\', 'n']
Detokenized (009): ['p', '=', 'phase', '[', 'por##e_p', ']', '/', '1000##00', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "a1 = - 1 / b \n"
Original    (007): ['a1', '=', '-', '1', '/', 'b', '\\n']
Tokenized   (010): ['[CLS]', 'a1', '=', '-', '1', '/', 'b', '\\', 'n', '[SEP]']
Filtered   (008): ['a1', '=', '-', '1', '/', 'b', '\\', 'n']
Detokenized (007): ['a1', '=', '-', '1', '/', 'b', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "a2 = ( R * T + b * P ) / ( a * b ) \n"
Original    (018): ['a2', '=', '(', 'R', '*', 'T', '+', 'b', '*', 'P', ')', '/', '(', 'a', '*', 'b', ')', '\\n']
Tokenized   (021): ['[CLS]', 'a2', '=', '(', 'r', '*', 't', '+', 'b', '*', 'p', ')', '/', '(', 'a', '*', 'b', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['a2', '=', '(', 'r', '*', 't', '+', 'b', '*', 'p', ')', '/', '(', 'a', '*', 'b', ')', '\\', 'n']
Detokenized (018): ['a2', '=', '(', 'r', '*', 't', '+', 'b', '*', 'p', ')', '/', '(', 'a', '*', 'b', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "a3 = - P / ( a * b ) \n"
Original    (011): ['a3', '=', '-', 'P', '/', '(', 'a', '*', 'b', ')', '\\n']
Tokenized   (015): ['[CLS]', 'a', '##3', '=', '-', 'p', '/', '(', 'a', '*', 'b', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['a', '##3', '=', '-', 'p', '/', '(', 'a', '*', 'b', ')', '\\', 'n']
Detokenized (011): ['a##3', '=', '-', 'p', '/', '(', 'a', '*', 'b', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "coeffs = sp . vstack ( ( a0 , a1 , a2 , a3 ) ) . T \n"
Original    (019): ['coeffs', '=', 'sp', '.', 'vstack', '(', '(', 'a0', ',', 'a1', ',', 'a2', ',', 'a3', ')', ')', '.', 'T', '\\n']
Tokenized   (027): ['[CLS]', 'coe', '##ffs', '=', 'sp', '.', 'vs', '##ta', '##ck', '(', '(', 'a', '##0', ',', 'a1', ',', 'a2', ',', 'a', '##3', ')', ')', '.', 't', '\\', 'n', '[SEP]']
Filtered   (025): ['coe', '##ffs', '=', 'sp', '.', 'vs', '##ta', '##ck', '(', '(', 'a', '##0', ',', 'a1', ',', 'a2', ',', 'a', '##3', ')', ')', '.', 't', '\\', 'n']
Detokenized (019): ['coe##ffs', '=', 'sp', '.', 'vs##ta##ck', '(', '(', 'a##0', ',', 'a1', ',', 'a2', ',', 'a##3', ')', ')', '.', 't', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "density = sp . array ( [ sp . roots ( C ) for C in coeffs ] ) \n"
Original    (020): ['density', '=', 'sp', '.', 'array', '(', '[', 'sp', '.', 'roots', '(', 'C', ')', 'for', 'C', 'in', 'coeffs', ']', ')', '\\n']
Tokenized   (024): ['[CLS]', 'density', '=', 'sp', '.', 'array', '(', '[', 'sp', '.', 'roots', '(', 'c', ')', 'for', 'c', 'in', 'coe', '##ffs', ']', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['density', '=', 'sp', '.', 'array', '(', '[', 'sp', '.', 'roots', '(', 'c', ')', 'for', 'c', 'in', 'coe', '##ffs', ']', ')', '\\', 'n']
Detokenized (020): ['density', '=', 'sp', '.', 'array', '(', '[', 'sp', '.', 'roots', '(', 'c', ')', 'for', 'c', 'in', 'coe##ffs', ']', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "comp2 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n"
Original    (015): ['comp2', '=', 'OpenPNM', '.', 'Phases', '.', 'GenericPhase', '(', 'network', '=', 'self', '.', 'net', ')', '\\n']
Tokenized   (025): ['[CLS]', 'com', '##p', '##2', '=', 'open', '##p', '##n', '##m', '.', 'phases', '.', 'generic', '##pha', '##se', '(', 'network', '=', 'self', '.', 'net', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['com', '##p', '##2', '=', 'open', '##p', '##n', '##m', '.', 'phases', '.', 'generic', '##pha', '##se', '(', 'network', '=', 'self', '.', 'net', ')', '\\', 'n']
Detokenized (015): ['com##p##2', '=', 'open##p##n##m', '.', 'phases', '.', 'generic##pha##se', '(', 'network', '=', 'self', '.', 'net', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "OpenPNM . Phases . GenericPhase ( network = self . net , \n"
Original    (013): ['OpenPNM', '.', 'Phases', '.', 'GenericPhase', '(', 'network', '=', 'self', '.', 'net', ',', '\\n']
Tokenized   (021): ['[CLS]', 'open', '##p', '##n', '##m', '.', 'phases', '.', 'generic', '##pha', '##se', '(', 'network', '=', 'self', '.', 'net', ',', '\\', 'n', '[SEP]']
Filtered   (019): ['open', '##p', '##n', '##m', '.', 'phases', '.', 'generic', '##pha', '##se', '(', 'network', '=', 'self', '.', 'net', ',', '\\', 'n']
Detokenized (013): ['open##p##n##m', '.', 'phases', '.', 'generic##pha##se', '(', 'network', '=', 'self', '.', 'net', ',', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "components = [ comp1 , comp2 ] ) \n"
Original    (009): ['components', '=', '[', 'comp1', ',', 'comp2', ']', ')', '\\n']
Tokenized   (016): ['[CLS]', 'components', '=', '[', 'com', '##p', '##1', ',', 'com', '##p', '##2', ']', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['components', '=', '[', 'com', '##p', '##1', ',', 'com', '##p', '##2', ']', ')', '\\', 'n']
Detokenized (009): ['components', '=', '[', 'com##p##1', ',', 'com##p##2', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "phase . set_component ( comp2 , mode = ) \n"
Original    (010): ['phase', '.', 'set_component', '(', 'comp2', ',', 'mode', '=', ')', '\\n']
Tokenized   (017): ['[CLS]', 'phase', '.', 'set', '_', 'component', '(', 'com', '##p', '##2', ',', 'mode', '=', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['phase', '.', 'set', '_', 'component', '(', 'com', '##p', '##2', ',', 'mode', '=', ')', '\\', 'n']
Detokenized (010): ['phase', '.', 'set_component', '(', 'com##p##2', ',', 'mode', '=', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "best_seq = fd [ x ] . sequence \n"
Original    (009): ['best_seq', '=', 'fd', '[', 'x', ']', '.', 'sequence', '\\n']
Tokenized   (016): ['[CLS]', 'best', '_', 'se', '##q', '=', 'f', '##d', '[', 'x', ']', '.', 'sequence', '\\', 'n', '[SEP]']
Filtered   (014): ['best', '_', 'se', '##q', '=', 'f', '##d', '[', 'x', ']', '.', 'sequence', '\\', 'n']
Detokenized (009): ['best_se##q', '=', 'f##d', '[', 'x', ']', '.', 'sequence', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "best_id , best_seq , best_qual = rep_info [ pb_id ] \n"
Original    (011): ['best_id', ',', 'best_seq', ',', 'best_qual', '=', 'rep_info', '[', 'pb_id', ']', '\\n']
Tokenized   (027): ['[CLS]', 'best', '_', 'id', ',', 'best', '_', 'se', '##q', ',', 'best', '_', 'qu', '##al', '=', 'rep', '_', 'info', '[', 'p', '##b', '_', 'id', ']', '\\', 'n', '[SEP]']
Filtered   (025): ['best', '_', 'id', ',', 'best', '_', 'se', '##q', ',', 'best', '_', 'qu', '##al', '=', 'rep', '_', 'info', '[', 'p', '##b', '_', 'id', ']', '\\', 'n']
Detokenized (011): ['best_id', ',', 'best_se##q', ',', 'best_qu##al', '=', 'rep_info', '[', 'p##b_id', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "_id_ = "{0}|{1}|{2}" . format ( pb_id , coords [ best_id ] , best_id ) \n"
Original    (016): ['_id_', '=', '"{0}|{1}|{2}"', '.', 'format', '(', 'pb_id', ',', 'coords', '[', 'best_id', ']', ',', 'best_id', ')', '\\n']
Tokenized   (042): ['[CLS]', '_', 'id', '_', '=', '"', '{', '0', '}', '|', '{', '1', '}', '|', '{', '2', '}', '"', '.', 'format', '(', 'p', '##b', '_', 'id', ',', 'co', '##ord', '##s', '[', 'best', '_', 'id', ']', ',', 'best', '_', 'id', ')', '\\', 'n', '[SEP]']
Filtered   (040): ['_', 'id', '_', '=', '"', '{', '0', '}', '|', '{', '1', '}', '|', '{', '2', '}', '"', '.', 'format', '(', 'p', '##b', '_', 'id', ',', 'co', '##ord', '##s', '[', 'best', '_', 'id', ']', ',', 'best', '_', 'id', ')', '\\', 'n']
Detokenized (016): ['_id_', '=', '"{0}|{1}|{2}"', '.', 'format', '(', 'p##b_id', ',', 'co##ord##s', '[', 'best_id', ']', ',', 'best_id', ')', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "iter = BioReaders . GMAPSAMReader ( gmap_sam_filename , True , query_len_dict = transfrag_len_dict ) \n"
Original    (015): ['iter', '=', 'BioReaders', '.', 'GMAPSAMReader', '(', 'gmap_sam_filename', ',', 'True', ',', 'query_len_dict', '=', 'transfrag_len_dict', ')', '\\n']
Tokenized   (043): ['[CLS]', 'it', '##er', '=', 'bio', '##rea', '##ders', '.', 'gma', '##ps', '##am', '##rea', '##der', '(', 'gma', '##p', '_', 'sam', '_', 'file', '##name', ',', 'true', ',', 'query', '_', 'len', '_', 'di', '##ct', '=', 'trans', '##fra', '##g', '_', 'len', '_', 'di', '##ct', ')', '\\', 'n', '[SEP]']
Filtered   (041): ['it', '##er', '=', 'bio', '##rea', '##ders', '.', 'gma', '##ps', '##am', '##rea', '##der', '(', 'gma', '##p', '_', 'sam', '_', 'file', '##name', ',', 'true', ',', 'query', '_', 'len', '_', 'di', '##ct', '=', 'trans', '##fra', '##g', '_', 'len', '_', 'di', '##ct', ')', '\\', 'n']
Detokenized (015): ['it##er', '=', 'bio##rea##ders', '.', 'gma##ps##am##rea##der', '(', 'gma##p_sam_file##name', ',', 'true', ',', 'query_len_di##ct', '=', 'trans##fra##g_len_di##ct', ')', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "TmpRec = namedtuple ( , [ , , , , , , ] ) \n"
Original    (015): ['TmpRec', '=', 'namedtuple', '(', ',', '[', ',', ',', ',', ',', ',', ',', ']', ')', '\\n']
Tokenized   (023): ['[CLS]', 't', '##mp', '##re', '##c', '=', 'named', '##tu', '##ple', '(', ',', '[', ',', ',', ',', ',', ',', ',', ']', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['t', '##mp', '##re', '##c', '=', 'named', '##tu', '##ple', '(', ',', '[', ',', ',', ',', ',', ',', ',', ']', ')', '\\', 'n']
Detokenized (015): ['t##mp##re##c', '=', 'named##tu##ple', '(', ',', '[', ',', ',', ',', ',', ',', ',', ']', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "compressed_records_pointer_dict = defaultdict ( lambda : [ ] ) \n"
Original    (010): ['compressed_records_pointer_dict', '=', 'defaultdict', '(', 'lambda', ':', '[', ']', ')', '\\n']
Tokenized   (021): ['[CLS]', 'compressed', '_', 'records', '_', 'pointer', '_', 'di', '##ct', '=', 'default', '##dict', '(', 'lambda', ':', '[', ']', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['compressed', '_', 'records', '_', 'pointer', '_', 'di', '##ct', '=', 'default', '##dict', '(', 'lambda', ':', '[', ']', ')', '\\', 'n']
Detokenized (010): ['compressed_records_pointer_di##ct', '=', 'default##dict', '(', 'lambda', ':', '[', ']', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "check_ids_unique ( fa_or_fq_filename , is_fq = is_fq ) \n"
Original    (009): ['check_ids_unique', '(', 'fa_or_fq_filename', ',', 'is_fq', '=', 'is_fq', ')', '\\n']
Tokenized   (031): ['[CLS]', 'check', '_', 'id', '##s', '_', 'unique', '(', 'fa', '_', 'or', '_', 'f', '##q', '_', 'file', '##name', ',', 'is', '_', 'f', '##q', '=', 'is', '_', 'f', '##q', ')', '\\', 'n', '[SEP]']
Filtered   (029): ['check', '_', 'id', '##s', '_', 'unique', '(', 'fa', '_', 'or', '_', 'f', '##q', '_', 'file', '##name', ',', 'is', '_', 'f', '##q', '=', 'is', '_', 'f', '##q', ')', '\\', 'n']
Detokenized (009): ['check_id##s_unique', '(', 'fa_or_f##q_file##name', ',', 'is_f##q', '=', 'is_f##q', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "fusion_candidates = find_fusion_candidates ( sam_filename , bs . transfrag_len_dict , min_locus_coverage \n"
Original    (012): ['fusion_candidates', '=', 'find_fusion_candidates', '(', 'sam_filename', ',', 'bs', '.', 'transfrag_len_dict', ',', 'min_locus_coverage', '\\n']
Tokenized   (035): ['[CLS]', 'fusion', '_', 'candidates', '=', 'find', '_', 'fusion', '_', 'candidates', '(', 'sam', '_', 'file', '##name', ',', 'bs', '.', 'trans', '##fra', '##g', '_', 'len', '_', 'di', '##ct', ',', 'min', '_', 'locus', '_', 'coverage', '\\', 'n', '[SEP]']
Filtered   (033): ['fusion', '_', 'candidates', '=', 'find', '_', 'fusion', '_', 'candidates', '(', 'sam', '_', 'file', '##name', ',', 'bs', '.', 'trans', '##fra', '##g', '_', 'len', '_', 'di', '##ct', ',', 'min', '_', 'locus', '_', 'coverage', '\\', 'n']
Detokenized (012): ['fusion_candidates', '=', 'find_fusion_candidates', '(', 'sam_file##name', ',', 'bs', '.', 'trans##fra##g_len_di##ct', ',', 'min_locus_coverage', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "pbid1 , groups1 = line . strip ( ) . split ( ) \n"
Original    (014): ['pbid1', ',', 'groups1', '=', 'line', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Tokenized   (020): ['[CLS]', 'p', '##bid', '##1', ',', 'groups', '##1', '=', 'line', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['p', '##bid', '##1', ',', 'groups', '##1', '=', 'line', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n']
Detokenized (014): ['p##bid##1', ',', 'groups##1', '=', 'line', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "pbid2 , groups2 = f . readline ( ) . strip ( ) . split ( ) \n"
Original    (018): ['pbid2', ',', 'groups2', '=', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Tokenized   (025): ['[CLS]', 'p', '##bid', '##2', ',', 'groups', '##2', '=', 'f', '.', 'read', '##line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['p', '##bid', '##2', ',', 'groups', '##2', '=', 'f', '.', 'read', '##line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n']
Detokenized (018): ['p##bid##2', ',', 'groups##2', '=', 'f', '.', 'read##line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "f_group . write ( "{0}\\t{1}\\n" . format ( pbid1 [ : pbid1 . rfind ( ) ] , "," . join ( group ) ) ) \n"
Original    (027): ['f_group', '.', 'write', '(', '"{0}\\\\t{1}\\\\n"', '.', 'format', '(', 'pbid1', '[', ':', 'pbid1', '.', 'rfind', '(', ')', ']', ',', '","', '.', 'join', '(', 'group', ')', ')', ')', '\\n']
Tokenized   (052): ['[CLS]', 'f', '_', 'group', '.', 'write', '(', '"', '{', '0', '}', '\\', '\\', 't', '{', '1', '}', '\\', '\\', 'n', '"', '.', 'format', '(', 'p', '##bid', '##1', '[', ':', 'p', '##bid', '##1', '.', 'rf', '##ind', '(', ')', ']', ',', '"', ',', '"', '.', 'join', '(', 'group', ')', ')', ')', '\\', 'n', '[SEP]']
Filtered   (050): ['f', '_', 'group', '.', 'write', '(', '"', '{', '0', '}', '\\', '\\', 't', '{', '1', '}', '\\', '\\', 'n', '"', '.', 'format', '(', 'p', '##bid', '##1', '[', ':', 'p', '##bid', '##1', '.', 'rf', '##ind', '(', ')', ']', ',', '"', ',', '"', '.', 'join', '(', 'group', ')', ')', ')', '\\', 'n']
Detokenized (027): ['f_group', '.', 'write', '(', '"{0}\\\\t{1}\\\\n"', '.', 'format', '(', 'p##bid##1', '[', ':', 'p##bid##1', '.', 'rf##ind', '(', ')', ']', ',', '","', '.', 'join', '(', 'group', ')', ')', ')', '\\n']
Counter: 50
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "group_info [ pbid1 [ : pbid1 . rfind ( ) ] ] = list ( group ) \n"
Original    (018): ['group_info', '[', 'pbid1', '[', ':', 'pbid1', '.', 'rfind', '(', ')', ']', ']', '=', 'list', '(', 'group', ')', '\\n']
Tokenized   (028): ['[CLS]', 'group', '_', 'info', '[', 'p', '##bid', '##1', '[', ':', 'p', '##bid', '##1', '.', 'rf', '##ind', '(', ')', ']', ']', '=', 'list', '(', 'group', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['group', '_', 'info', '[', 'p', '##bid', '##1', '[', ':', 'p', '##bid', '##1', '.', 'rf', '##ind', '(', ')', ']', ']', '=', 'list', '(', 'group', ')', '\\', 'n']
Detokenized (018): ['group_info', '[', 'p##bid##1', '[', ':', 'p##bid##1', '.', 'rf##ind', '(', ')', ']', ']', '=', 'list', '(', 'group', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "d1 . update ( d [ ] ) \n"
Original    (009): ['d1', '.', 'update', '(', 'd', '[', ']', ')', '\\n']
Tokenized   (013): ['[CLS]', 'd', '##1', '.', 'update', '(', 'd', '[', ']', ')', '\\', 'n', '[SEP]']
Filtered   (011): ['d', '##1', '.', 'update', '(', 'd', '[', ']', ')', '\\', 'n']
Detokenized (009): ['d##1', '.', 'update', '(', 'd', '[', ']', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "fusion_main ( args . input , args . sam , args . prefix , \n"
Original    (015): ['fusion_main', '(', 'args', '.', 'input', ',', 'args', '.', 'sam', ',', 'args', '.', 'prefix', ',', '\\n']
Tokenized   (023): ['[CLS]', 'fusion', '_', 'main', '(', 'ar', '##gs', '.', 'input', ',', 'ar', '##gs', '.', 'sam', ',', 'ar', '##gs', '.', 'prefix', ',', '\\', 'n', '[SEP]']
Filtered   (021): ['fusion', '_', 'main', '(', 'ar', '##gs', '.', 'input', ',', 'ar', '##gs', '.', 'sam', ',', 'ar', '##gs', '.', 'prefix', ',', '\\', 'n']
Detokenized (015): ['fusion_main', '(', 'ar##gs', '.', 'input', ',', 'ar##gs', '.', 'sam', ',', 'ar##gs', '.', 'prefix', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "is_fq = args . fq , allow_extra_5_exons = args . allow_extra_5exon , \n"
Original    (013): ['is_fq', '=', 'args', '.', 'fq', ',', 'allow_extra_5_exons', '=', 'args', '.', 'allow_extra_5exon', ',', '\\n']
Tokenized   (035): ['[CLS]', 'is', '_', 'f', '##q', '=', 'ar', '##gs', '.', 'f', '##q', ',', 'allow', '_', 'extra', '_', '5', '_', 'ex', '##ons', '=', 'ar', '##gs', '.', 'allow', '_', 'extra', '_', '5', '##ex', '##on', ',', '\\', 'n', '[SEP]']
Filtered   (033): ['is', '_', 'f', '##q', '=', 'ar', '##gs', '.', 'f', '##q', ',', 'allow', '_', 'extra', '_', '5', '_', 'ex', '##ons', '=', 'ar', '##gs', '.', 'allow', '_', 'extra', '_', '5', '##ex', '##on', ',', '\\', 'n']
Detokenized (013): ['is_f##q', '=', 'ar##gs', '.', 'f##q', ',', 'allow_extra_5_ex##ons', '=', 'ar##gs', '.', 'allow_extra_5##ex##on', ',', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "skip_5_exon_alt = False , prefix_dict_pickle_filename = args . prefix_dict_pickle_filename , min_locus_coverage = args . min_locus_coverage , min_locus_coverage_bp = args . min_locus_coverage_bp min_total_coverage = args . min_total_coverage , \n"
Original    (028): ['skip_5_exon_alt', '=', 'False', ',', 'prefix_dict_pickle_filename', '=', 'args', '.', 'prefix_dict_pickle_filename', ',', 'min_locus_coverage', '=', 'args', '.', 'min_locus_coverage', ',', 'min_locus_coverage_bp', '=', 'args', '.', 'min_locus_coverage_bp', 'min_total_coverage', '=', 'args', '.', 'min_total_coverage', ',', '\\n']
Tokenized   (088): ['[CLS]', 'skip', '_', '5', '_', 'ex', '##on', '_', 'alt', '=', 'false', ',', 'prefix', '_', 'di', '##ct', '_', 'pick', '##le', '_', 'file', '##name', '=', 'ar', '##gs', '.', 'prefix', '_', 'di', '##ct', '_', 'pick', '##le', '_', 'file', '##name', ',', 'min', '_', 'locus', '_', 'coverage', '=', 'ar', '##gs', '.', 'min', '_', 'locus', '_', 'coverage', ',', 'min', '_', 'locus', '_', 'coverage', '_', 'bp', '=', 'ar', '##gs', '.', 'min', '_', 'locus', '_', 'coverage', '_', 'bp', 'min', '_', 'total', '_', 'coverage', '=', 'ar', '##gs', '.', 'min', '_', 'total', '_', 'coverage', ',', '\\', 'n', '[SEP]']
Filtered   (086): ['skip', '_', '5', '_', 'ex', '##on', '_', 'alt', '=', 'false', ',', 'prefix', '_', 'di', '##ct', '_', 'pick', '##le', '_', 'file', '##name', '=', 'ar', '##gs', '.', 'prefix', '_', 'di', '##ct', '_', 'pick', '##le', '_', 'file', '##name', ',', 'min', '_', 'locus', '_', 'coverage', '=', 'ar', '##gs', '.', 'min', '_', 'locus', '_', 'coverage', ',', 'min', '_', 'locus', '_', 'coverage', '_', 'bp', '=', 'ar', '##gs', '.', 'min', '_', 'locus', '_', 'coverage', '_', 'bp', 'min', '_', 'total', '_', 'coverage', '=', 'ar', '##gs', '.', 'min', '_', 'total', '_', 'coverage', ',', '\\', 'n']
Detokenized (028): ['skip_5_ex##on_alt', '=', 'false', ',', 'prefix_di##ct_pick##le_file##name', '=', 'ar##gs', '.', 'prefix_di##ct_pick##le_file##name', ',', 'min_locus_coverage', '=', 'ar##gs', '.', 'min_locus_coverage', ',', 'min_locus_coverage_bp', '=', 'ar##gs', '.', 'min_locus_coverage_bp', 'min_total_coverage', '=', 'ar##gs', '.', 'min_total_coverage', ',', '\\n']
Counter: 86
===================================================================
Hidden states:  (13, 28, 768)
# Extracted words:  28
Sentence         : "raw = self . f . readline ( ) . strip ( ) . split ( ) \n"
Original    (018): ['raw', '=', 'self', '.', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Tokenized   (022): ['[CLS]', 'raw', '=', 'self', '.', 'f', '.', 'read', '##line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['raw', '=', 'self', '.', 'f', '.', 'read', '##line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n']
Detokenized (018): ['raw', '=', 'self', '.', 'f', '.', 'read##line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "iden = float ( raw [ 3 ] ) \n"
Original    (010): ['iden', '=', 'float', '(', 'raw', '[', '3', ']', ')', '\\n']
Tokenized   (014): ['[CLS]', 'id', '##en', '=', 'float', '(', 'raw', '[', '3', ']', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['id', '##en', '=', 'float', '(', 'raw', '[', '3', ']', ')', '\\', 'n']
Detokenized (010): ['id##en', '=', 'float', '(', 'raw', '[', '3', ']', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "_qStart , qAln = self . f . readline ( ) . strip ( ) . split ( ) \n"
Original    (020): ['_qStart', ',', 'qAln', '=', 'self', '.', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Tokenized   (029): ['[CLS]', '_', 'q', '##star', '##t', ',', 'q', '##al', '##n', '=', 'self', '.', 'f', '.', 'read', '##line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['_', 'q', '##star', '##t', ',', 'q', '##al', '##n', '=', 'self', '.', 'f', '.', 'read', '##line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n']
Detokenized (020): ['_q##star##t', ',', 'q##al##n', '=', 'self', '.', 'f', '.', 'read##line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "_sStart , sAln = self . f . readline ( ) . strip ( ) . split ( ) [ : 2 ] \n"
Original    (024): ['_sStart', ',', 'sAln', '=', 'self', '.', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '[', ':', '2', ']', '\\n']
Tokenized   (032): ['[CLS]', '_', 'ss', '##tar', '##t', ',', 'sal', '##n', '=', 'self', '.', 'f', '.', 'read', '##line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '[', ':', '2', ']', '\\', 'n', '[SEP]']
Filtered   (030): ['_', 'ss', '##tar', '##t', ',', 'sal', '##n', '=', 'self', '.', 'f', '.', 'read', '##line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '[', ':', '2', ']', '\\', 'n']
Detokenized (024): ['_ss##tar##t', ',', 'sal##n', '=', 'self', '.', 'f', '.', 'read##line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '[', ':', '2', ']', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "missed_q = missed_q * 1. / r . qLength , \n"
Original    (011): ['missed_q', '=', 'missed_q', '*', '1.', '/', 'r', '.', 'qLength', ',', '\\n']
Tokenized   (022): ['[CLS]', 'missed', '_', 'q', '=', 'missed', '_', 'q', '*', '1', '.', '/', 'r', '.', 'q', '##len', '##gt', '##h', ',', '\\', 'n', '[SEP]']
Filtered   (020): ['missed', '_', 'q', '=', 'missed', '_', 'q', '*', '1', '.', '/', 'r', '.', 'q', '##len', '##gt', '##h', ',', '\\', 'n']
Detokenized (011): ['missed_q', '=', 'missed_q', '*', '1.', '/', 'r', '.', 'q##len##gt##h', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "ece_penalty , ece_min_len ) : \n"
Original    (006): ['ece_penalty', ',', 'ece_min_len', ')', ':', '\\n']
Tokenized   (017): ['[CLS]', 'ec', '##e', '_', 'penalty', ',', 'ec', '##e', '_', 'min', '_', 'len', ')', ':', '\\', 'n', '[SEP]']
Filtered   (015): ['ec', '##e', '_', 'penalty', ',', 'ec', '##e', '_', 'min', '_', 'len', ')', ':', '\\', 'n']
Detokenized (006): ['ec##e_penalty', ',', 'ec##e_min_len', ')', ':', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "heading = % ( current_indent , , self . heading ) \n"
Original    (012): ['heading', '=', '%', '(', 'current_indent', ',', ',', 'self', '.', 'heading', ')', '\\n']
Tokenized   (018): ['[CLS]', 'heading', '=', '%', '(', 'current', '_', 'ind', '##ent', ',', ',', 'self', '.', 'heading', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['heading', '=', '%', '(', 'current', '_', 'ind', '##ent', ',', ',', 'self', '.', 'heading', ')', '\\', 'n']
Detokenized (012): ['heading', '=', '%', '(', 'current_ind##ent', ',', ',', 'self', '.', 'heading', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "section = self . _Section ( self , self . _current_section , heading ) \n"
Original    (015): ['section', '=', 'self', '.', '_Section', '(', 'self', ',', 'self', '.', '_current_section', ',', 'heading', ')', '\\n']
Tokenized   (022): ['[CLS]', 'section', '=', 'self', '.', '_', 'section', '(', 'self', ',', 'self', '.', '_', 'current', '_', 'section', ',', 'heading', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['section', '=', 'self', '.', '_', 'section', '(', 'self', ',', 'self', '.', '_', 'current', '_', 'section', ',', 'heading', ')', '\\', 'n']
Detokenized (015): ['section', '=', 'self', '.', '_section', '(', 'self', ',', 'self', '.', '_current_section', ',', 'heading', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "invocations = [ get_invocation ( action ) ] \n"
Original    (009): ['invocations', '=', '[', 'get_invocation', '(', 'action', ')', ']', '\\n']
Tokenized   (017): ['[CLS]', 'in', '##vocation', '##s', '=', '[', 'get', '_', 'in', '##vocation', '(', 'action', ')', ']', '\\', 'n', '[SEP]']
Filtered   (015): ['in', '##vocation', '##s', '=', '[', 'get', '_', 'in', '##vocation', '(', 'action', ')', ']', '\\', 'n']
Detokenized (009): ['in##vocation##s', '=', '[', 'get_in##vocation', '(', 'action', ')', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "action_usage = format ( optionals + positionals , groups ) \n"
Original    (011): ['action_usage', '=', 'format', '(', 'optionals', '+', 'positionals', ',', 'groups', ')', '\\n']
Tokenized   (018): ['[CLS]', 'action', '_', 'usage', '=', 'format', '(', 'optional', '##s', '+', 'position', '##als', ',', 'groups', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['action', '_', 'usage', '=', 'format', '(', 'optional', '##s', '+', 'position', '##als', ',', 'groups', ')', '\\', 'n']
Detokenized (011): ['action_usage', '=', 'format', '(', 'optional##s', '+', 'position##als', ',', 'groups', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "text_width = self . _width - self . _current_indent \n"
Original    (010): ['text_width', '=', 'self', '.', '_width', '-', 'self', '.', '_current_indent', '\\n']
Tokenized   (020): ['[CLS]', 'text', '_', 'width', '=', 'self', '.', '_', 'width', '-', 'self', '.', '_', 'current', '_', 'ind', '##ent', '\\', 'n', '[SEP]']
Filtered   (018): ['text', '_', 'width', '=', 'self', '.', '_', 'width', '-', 'self', '.', '_', 'current', '_', 'ind', '##ent', '\\', 'n']
Detokenized (010): ['text_width', '=', 'self', '.', '_width', '-', 'self', '.', '_current_ind##ent', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "line_len += len ( part ) + 1 \n"
Original    (009): ['line_len', '+=', 'len', '(', 'part', ')', '+', '1', '\\n']
Tokenized   (015): ['[CLS]', 'line', '_', 'len', '+', '=', 'len', '(', 'part', ')', '+', '1', '\\', 'n', '[SEP]']
Filtered   (013): ['line', '_', 'len', '+', '=', 'len', '(', 'part', ')', '+', '1', '\\', 'n']
Detokenized (009): ['line_len', '+=', 'len', '(', 'part', ')', '+', '1', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "part = % ( option_string , args_string ) \n"
Original    (009): ['part', '=', '%', '(', 'option_string', ',', 'args_string', ')', '\\n']
Tokenized   (017): ['[CLS]', 'part', '=', '%', '(', 'option', '_', 'string', ',', 'ar', '##gs', '_', 'string', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['part', '=', '%', '(', 'option', '_', 'string', ',', 'ar', '##gs', '_', 'string', ')', '\\', 'n']
Detokenized (009): ['part', '=', '%', '(', 'option_string', ',', 'ar##gs_string', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "indent = * self . _current_indent \n"
Original    (007): ['indent', '=', '*', 'self', '.', '_current_indent', '\\n']
Tokenized   (015): ['[CLS]', 'ind', '##ent', '=', '*', 'self', '.', '_', 'current', '_', 'ind', '##ent', '\\', 'n', '[SEP]']
Filtered   (013): ['ind', '##ent', '=', '*', 'self', '.', '_', 'current', '_', 'ind', '##ent', '\\', 'n']
Detokenized (007): ['ind##ent', '=', '*', 'self', '.', '_current_ind##ent', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "help_width = self . _width - help_position \n"
Original    (008): ['help_width', '=', 'self', '.', '_width', '-', 'help_position', '\\n']
Tokenized   (016): ['[CLS]', 'help', '_', 'width', '=', 'self', '.', '_', 'width', '-', 'help', '_', 'position', '\\', 'n', '[SEP]']
Filtered   (014): ['help', '_', 'width', '=', 'self', '.', '_', 'width', '-', 'help', '_', 'position', '\\', 'n']
Detokenized (008): ['help_width', '=', 'self', '.', '_width', '-', 'help_position', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "action_width = help_position - self . _current_indent - 2 \n"
Original    (010): ['action_width', '=', 'help_position', '-', 'self', '.', '_current_indent', '-', '2', '\\n']
Tokenized   (021): ['[CLS]', 'action', '_', 'width', '=', 'help', '_', 'position', '-', 'self', '.', '_', 'current', '_', 'ind', '##ent', '-', '2', '\\', 'n', '[SEP]']
Filtered   (019): ['action', '_', 'width', '=', 'help', '_', 'position', '-', 'self', '.', '_', 'current', '_', 'ind', '##ent', '-', '2', '\\', 'n']
Detokenized (010): ['action_width', '=', 'help_position', '-', 'self', '.', '_current_ind##ent', '-', '2', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sup . __init__ ( option_strings = [ ] , dest = name , help = help ) \n"
Original    (018): ['sup', '.', '__init__', '(', 'option_strings', '=', '[', ']', ',', 'dest', '=', 'name', ',', 'help', '=', 'help', ')', '\\n']
Tokenized   (030): ['[CLS]', 'su', '##p', '.', '_', '_', 'in', '##it', '_', '_', '(', 'option', '_', 'strings', '=', '[', ']', ',', 'des', '##t', '=', 'name', ',', 'help', '=', 'help', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['su', '##p', '.', '_', '_', 'in', '##it', '_', '_', '(', 'option', '_', 'strings', '=', '[', ']', ',', 'des', '##t', '=', 'name', ',', 'help', '=', 'help', ')', '\\', 'n']
Detokenized (018): ['su##p', '.', '__in##it__', '(', 'option_strings', '=', '[', ']', ',', 'des##t', '=', 'name', ',', 'help', '=', 'help', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "arg_strings = values [ 1 : ] \n"
Original    (008): ['arg_strings', '=', 'values', '[', '1', ':', ']', '\\n']
Tokenized   (014): ['[CLS]', 'ar', '##g', '_', 'strings', '=', 'values', '[', '1', ':', ']', '\\', 'n', '[SEP]']
Filtered   (012): ['ar', '##g', '_', 'strings', '=', 'values', '[', '1', ':', ']', '\\', 'n']
Detokenized (008): ['ar##g_strings', '=', 'values', '[', '1', ':', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "args_str = . join ( [ repr ( arg ) for arg in args if arg is not None ] ) \n"
Original    (022): ['args_str', '=', '.', 'join', '(', '[', 'repr', '(', 'arg', ')', 'for', 'arg', 'in', 'args', 'if', 'arg', 'is', 'not', 'None', ']', ')', '\\n']
Tokenized   (034): ['[CLS]', 'ar', '##gs', '_', 'st', '##r', '=', '.', 'join', '(', '[', 'rep', '##r', '(', 'ar', '##g', ')', 'for', 'ar', '##g', 'in', 'ar', '##gs', 'if', 'ar', '##g', 'is', 'not', 'none', ']', ')', '\\', 'n', '[SEP]']
Filtered   (032): ['ar', '##gs', '_', 'st', '##r', '=', '.', 'join', '(', '[', 'rep', '##r', '(', 'ar', '##g', ')', 'for', 'ar', '##g', 'in', 'ar', '##gs', 'if', 'ar', '##g', 'is', 'not', 'none', ']', ')', '\\', 'n']
Detokenized (022): ['ar##gs_st##r', '=', '.', 'join', '(', '[', 'rep##r', '(', 'ar##g', ')', 'for', 'ar##g', 'in', 'ar##gs', 'if', 'ar##g', 'is', 'not', 'none', ']', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "type_func = self . _registry_get ( , action . type , action . type ) \n"
Original    (016): ['type_func', '=', 'self', '.', '_registry_get', '(', ',', 'action', '.', 'type', ',', 'action', '.', 'type', ')', '\\n']
Tokenized   (025): ['[CLS]', 'type', '_', 'fun', '##c', '=', 'self', '.', '_', 'registry', '_', 'get', '(', ',', 'action', '.', 'type', ',', 'action', '.', 'type', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['type', '_', 'fun', '##c', '=', 'self', '.', '_', 'registry', '_', 'get', '(', ',', 'action', '.', 'type', ',', 'action', '.', 'type', ')', '\\', 'n']
Detokenized (016): ['type_fun##c', '=', 'self', '.', '_registry_get', '(', ',', 'action', '.', 'type', ',', 'action', '.', 'type', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "conflict_string = . join ( [ option_string \n"
Original    (008): ['conflict_string', '=', '.', 'join', '(', '[', 'option_string', '\\n']
Tokenized   (015): ['[CLS]', 'conflict', '_', 'string', '=', '.', 'join', '(', '[', 'option', '_', 'string', '\\', 'n', '[SEP]']
Filtered   (013): ['conflict', '_', 'string', '=', '.', 'join', '(', '[', 'option', '_', 'string', '\\', 'n']
Detokenized (008): ['conflict_string', '=', '.', 'join', '(', '[', 'option_string', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "in conflicting_actions ] ) \n"
Original    (005): ['in', 'conflicting_actions', ']', ')', '\\n']
Tokenized   (010): ['[CLS]', 'in', 'conflicting', '_', 'actions', ']', ')', '\\', 'n', '[SEP]']
Filtered   (008): ['in', 'conflicting', '_', 'actions', ']', ')', '\\', 'n']
Detokenized (005): ['in', 'conflicting_actions', ']', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "super_init ( description = description , ** kwargs ) \n"
Original    (010): ['super_init', '(', 'description', '=', 'description', ',', '**', 'kwargs', ')', '\\n']
Tokenized   (019): ['[CLS]', 'super', '_', 'in', '##it', '(', 'description', '=', 'description', ',', '*', '*', 'kw', '##ar', '##gs', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['super', '_', 'in', '##it', '(', 'description', '=', 'description', ',', '*', '*', 'kw', '##ar', '##gs', ')', '\\', 'n']
Detokenized (010): ['super_in##it', '(', 'description', '=', 'description', ',', '**', 'kw##ar##gs', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : """"instead""" , DeprecationWarning ) \n"
Original    (005): ['"""instead"""', ',', 'DeprecationWarning', ')', '\\n']
Tokenized   (018): ['[CLS]', '"', '"', '"', 'instead', '"', '"', '"', ',', 'de', '##pre', '##cation', '##war', '##ning', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['"', '"', '"', 'instead', '"', '"', '"', ',', 'de', '##pre', '##cation', '##war', '##ning', ')', '\\', 'n']
Detokenized (005): ['"""instead"""', ',', 'de##pre##cation##war##ning', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "superinit ( description = description , \n"
Original    (007): ['superinit', '(', 'description', '=', 'description', ',', '\\n']
Tokenized   (012): ['[CLS]', 'super', '##ini', '##t', '(', 'description', '=', 'description', ',', '\\', 'n', '[SEP]']
Filtered   (010): ['super', '##ini', '##t', '(', 'description', '=', 'description', ',', '\\', 'n']
Detokenized (007): ['super##ini##t', '(', 'description', '=', 'description', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "default_prefix + , default_prefix * 2 + , \n"
Original    (009): ['default_prefix', '+', ',', 'default_prefix', '*', '2', '+', ',', '\\n']
Tokenized   (016): ['[CLS]', 'default', '_', 'prefix', '+', ',', 'default', '_', 'prefix', '*', '2', '+', ',', '\\', 'n', '[SEP]']
Filtered   (014): ['default', '_', 'prefix', '+', ',', 'default', '_', 'prefix', '*', '2', '+', ',', '\\', 'n']
Detokenized (009): ['default_prefix', '+', ',', 'default_prefix', '*', '2', '+', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "conflicts . extend ( group_actions [ i + 1 : ] ) \n"
Original    (013): ['conflicts', '.', 'extend', '(', 'group_actions', '[', 'i', '+', '1', ':', ']', ')', '\\n']
Tokenized   (018): ['[CLS]', 'conflicts', '.', 'extend', '(', 'group', '_', 'actions', '[', 'i', '+', '1', ':', ']', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['conflicts', '.', 'extend', '(', 'group', '_', 'actions', '[', 'i', '+', '1', ':', ']', ')', '\\', 'n']
Detokenized (013): ['conflicts', '.', 'extend', '(', 'group_actions', '[', 'i', '+', '1', ':', ']', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "action , option_string , explicit_arg = option_tuple \n"
Original    (008): ['action', ',', 'option_string', ',', 'explicit_arg', '=', 'option_tuple', '\\n']
Tokenized   (019): ['[CLS]', 'action', ',', 'option', '_', 'string', ',', 'explicit', '_', 'ar', '##g', '=', 'option', '_', 'tu', '##ple', '\\', 'n', '[SEP]']
Filtered   (017): ['action', ',', 'option', '_', 'string', ',', 'explicit', '_', 'ar', '##g', '=', 'option', '_', 'tu', '##ple', '\\', 'n']
Detokenized (008): ['action', ',', 'option_string', ',', 'explicit_ar##g', '=', 'option_tu##ple', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "option_string = char + explicit_arg [ 0 ] \n"
Original    (009): ['option_string', '=', 'char', '+', 'explicit_arg', '[', '0', ']', '\\n']
Tokenized   (017): ['[CLS]', 'option', '_', 'string', '=', 'char', '+', 'explicit', '_', 'ar', '##g', '[', '0', ']', '\\', 'n', '[SEP]']
Filtered   (015): ['option', '_', 'string', '=', 'char', '+', 'explicit', '_', 'ar', '##g', '[', '0', ']', '\\', 'n']
Detokenized (009): ['option_string', '=', 'char', '+', 'explicit_ar##g', '[', '0', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "new_explicit_arg = explicit_arg [ 1 : ] or None \n"
Original    (010): ['new_explicit_arg', '=', 'explicit_arg', '[', '1', ':', ']', 'or', 'None', '\\n']
Tokenized   (021): ['[CLS]', 'new', '_', 'explicit', '_', 'ar', '##g', '=', 'explicit', '_', 'ar', '##g', '[', '1', ':', ']', 'or', 'none', '\\', 'n', '[SEP]']
Filtered   (019): ['new', '_', 'explicit', '_', 'ar', '##g', '=', 'explicit', '_', 'ar', '##g', '[', '1', ':', ']', 'or', 'none', '\\', 'n']
Detokenized (010): ['new_explicit_ar##g', '=', 'explicit_ar##g', '[', '1', ':', ']', 'or', 'none', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "action_tuples . append ( ( action , args , option_string ) ) \n"
Original    (013): ['action_tuples', '.', 'append', '(', '(', 'action', ',', 'args', ',', 'option_string', ')', ')', '\\n']
Tokenized   (023): ['[CLS]', 'action', '_', 'tu', '##ples', '.', 'app', '##end', '(', '(', 'action', ',', 'ar', '##gs', ',', 'option', '_', 'string', ')', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['action', '_', 'tu', '##ples', '.', 'app', '##end', '(', '(', 'action', ',', 'ar', '##gs', ',', 'option', '_', 'string', ')', ')', '\\', 'n']
Detokenized (013): ['action_tu##ples', '.', 'app##end', '(', '(', 'action', ',', 'ar##gs', ',', 'option_string', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "selected_patterns = arg_strings_pattern [ start : ] \n"
Original    (008): ['selected_patterns', '=', 'arg_strings_pattern', '[', 'start', ':', ']', '\\n']
Tokenized   (018): ['[CLS]', 'selected', '_', 'patterns', '=', 'ar', '##g', '_', 'strings', '_', 'pattern', '[', 'start', ':', ']', '\\', 'n', '[SEP]']
Filtered   (016): ['selected', '_', 'patterns', '=', 'ar', '##g', '_', 'strings', '_', 'pattern', '[', 'start', ':', ']', '\\', 'n']
Detokenized (008): ['selected_patterns', '=', 'ar##g_strings_pattern', '[', 'start', ':', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "extras . extend ( arg_strings [ stop_index : ] ) \n"
Original    (011): ['extras', '.', 'extend', '(', 'arg_strings', '[', 'stop_index', ':', ']', ')', '\\n']
Tokenized   (019): ['[CLS]', 'extras', '.', 'extend', '(', 'ar', '##g', '_', 'strings', '[', 'stop', '_', 'index', ':', ']', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['extras', '.', 'extend', '(', 'ar', '##g', '_', 'strings', '[', 'stop', '_', 'index', ':', ']', ')', '\\', 'n']
Detokenized (011): ['extras', '.', 'extend', '(', 'ar##g_strings', '[', 'stop_index', ':', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "OPTIONAL : _ ( ) , \n"
Original    (007): ['OPTIONAL', ':', '_', '(', ')', ',', '\\n']
Tokenized   (010): ['[CLS]', 'optional', ':', '_', '(', ')', ',', '\\', 'n', '[SEP]']
Filtered   (008): ['optional', ':', '_', '(', ')', ',', '\\', 'n']
Detokenized (007): ['optional', ':', '_', '(', ')', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "pattern = . join ( [ self . _get_nargs_pattern ( action ) \n"
Original    (013): ['pattern', '=', '.', 'join', '(', '[', 'self', '.', '_get_nargs_pattern', '(', 'action', ')', '\\n']
Tokenized   (023): ['[CLS]', 'pattern', '=', '.', 'join', '(', '[', 'self', '.', '_', 'get', '_', 'na', '##rg', '##s', '_', 'pattern', '(', 'action', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['pattern', '=', '.', 'join', '(', '[', 'self', '.', '_', 'get', '_', 'na', '##rg', '##s', '_', 'pattern', '(', 'action', ')', '\\', 'n']
Detokenized (013): ['pattern', '=', '.', 'join', '(', '[', 'self', '.', '_get_na##rg##s_pattern', '(', 'action', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "short_option_prefix = option_string [ : 2 ] \n"
Original    (008): ['short_option_prefix', '=', 'option_string', '[', ':', '2', ']', '\\n']
Tokenized   (017): ['[CLS]', 'short', '_', 'option', '_', 'prefix', '=', 'option', '_', 'string', '[', ':', '2', ']', '\\', 'n', '[SEP]']
Filtered   (015): ['short', '_', 'option', '_', 'prefix', '=', 'option', '_', 'string', '[', ':', '2', ']', '\\', 'n']
Detokenized (008): ['short_option_prefix', '=', 'option_string', '[', ':', '2', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "tup = action , option_string , short_explicit_arg \n"
Original    (008): ['tup', '=', 'action', ',', 'option_string', ',', 'short_explicit_arg', '\\n']
Tokenized   (019): ['[CLS]', 'tu', '##p', '=', 'action', ',', 'option', '_', 'string', ',', 'short', '_', 'explicit', '_', 'ar', '##g', '\\', 'n', '[SEP]']
Filtered   (017): ['tu', '##p', '=', 'action', ',', 'option', '_', 'string', ',', 'short', '_', 'explicit', '_', 'ar', '##g', '\\', 'n']
Detokenized (008): ['tu##p', '=', 'action', ',', 'option_string', ',', 'short_explicit_ar##g', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "not action . option_strings ) : \n"
Original    (007): ['not', 'action', '.', 'option_strings', ')', ':', '\\n']
Tokenized   (012): ['[CLS]', 'not', 'action', '.', 'option', '_', 'strings', ')', ':', '\\', 'n', '[SEP]']
Filtered   (010): ['not', 'action', '.', 'option', '_', 'strings', ')', ':', '\\', 'n']
Detokenized (007): ['not', 'action', '.', 'option_strings', ')', ':', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "vulnerability = obj [ ] [ ] [ ] [ ] [ ] \n"
Original    (014): ['vulnerability', '=', 'obj', '[', ']', '[', ']', '[', ']', '[', ']', '[', ']', '\\n']
Tokenized   (018): ['[CLS]', 'vulnerability', '=', 'ob', '##j', '[', ']', '[', ']', '[', ']', '[', ']', '[', ']', '\\', 'n', '[SEP]']
Filtered   (016): ['vulnerability', '=', 'ob', '##j', '[', ']', '[', ']', '[', ']', '[', ']', '[', ']', '\\', 'n']
Detokenized (014): ['vulnerability', '=', 'ob##j', '[', ']', '[', ']', '[', ']', '[', ']', '[', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "apikey = common . apikey ( sessionKey , args [ 0 ] , debug ) \n"
Original    (016): ['apikey', '=', 'common', '.', 'apikey', '(', 'sessionKey', ',', 'args', '[', '0', ']', ',', 'debug', ')', '\\n']
Tokenized   (025): ['[CLS]', 'api', '##key', '=', 'common', '.', 'api', '##key', '(', 'session', '##key', ',', 'ar', '##gs', '[', '0', ']', ',', 'de', '##bu', '##g', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['api', '##key', '=', 'common', '.', 'api', '##key', '(', 'session', '##key', ',', 'ar', '##gs', '[', '0', ']', ',', 'de', '##bu', '##g', ')', '\\', 'n']
Detokenized (016): ['api##key', '=', 'common', '.', 'api##key', '(', 'session##key', ',', 'ar##gs', '[', '0', ']', ',', 'de##bu##g', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "device = pandevice . base . PanDevice ( args [ 0 ] , api_key = apikey ) \n"
Original    (018): ['device', '=', 'pandevice', '.', 'base', '.', 'PanDevice', '(', 'args', '[', '0', ']', ',', 'api_key', '=', 'apikey', ')', '\\n']
Tokenized   (029): ['[CLS]', 'device', '=', 'pan', '##dev', '##ice', '.', 'base', '.', 'pan', '##dev', '##ice', '(', 'ar', '##gs', '[', '0', ']', ',', 'api', '_', 'key', '=', 'api', '##key', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['device', '=', 'pan', '##dev', '##ice', '.', 'base', '.', 'pan', '##dev', '##ice', '(', 'ar', '##gs', '[', '0', ']', ',', 'api', '_', 'key', '=', 'api', '##key', ')', '\\', 'n']
Detokenized (018): ['device', '=', 'pan##dev##ice', '.', 'base', '.', 'pan##dev##ice', '(', 'ar##gs', '[', '0', ']', ',', 'api_key', '=', 'api##key', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "rebalance_backoff_ms = 2 * 1000 , \n"
Original    (007): ['rebalance_backoff_ms', '=', '2', '*', '1000', ',', '\\n']
Tokenized   (016): ['[CLS]', 're', '##balance', '_', 'back', '##off', '_', 'ms', '=', '2', '*', '1000', ',', '\\', 'n', '[SEP]']
Filtered   (014): ['re', '##balance', '_', 'back', '##off', '_', 'ms', '=', '2', '*', '1000', ',', '\\', 'n']
Detokenized (007): ['re##balance_back##off_ms', '=', '2', '*', '1000', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "uuid = uuid4 ( ) \n"
Original    (006): ['uuid', '=', 'uuid4', '(', ')', '\\n']
Tokenized   (012): ['[CLS]', 'u', '##uid', '=', 'u', '##uid', '##4', '(', ')', '\\', 'n', '[SEP]']
Filtered   (010): ['u', '##uid', '=', 'u', '##uid', '##4', '(', ')', '\\', 'n']
Detokenized (006): ['u##uid', '=', 'u##uid##4', '(', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : """ . join ( traceback . format_tb ( tb ) ) ) \n"
Original    (013): ['""', '.', 'join', '(', 'traceback', '.', 'format_tb', '(', 'tb', ')', ')', ')', '\\n']
Tokenized   (020): ['[CLS]', '"', '"', '.', 'join', '(', 'trace', '##back', '.', 'format', '_', 'tb', '(', 'tb', ')', ')', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['"', '"', '.', 'join', '(', 'trace', '##back', '.', 'format', '_', 'tb', '(', 'tb', ')', ')', ')', '\\', 'n']
Detokenized (013): ['""', '.', 'join', '(', 'trace##back', '.', 'format_tb', '(', 'tb', ')', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "kazoo_kwargs = { : timeout / 1000 } \n"
Original    (009): ['kazoo_kwargs', '=', '{', ':', 'timeout', '/', '1000', '}', '\\n']
Tokenized   (018): ['[CLS]', 'ka', '##zoo', '_', 'kw', '##ar', '##gs', '=', '{', ':', 'time', '##out', '/', '1000', '}', '\\', 'n', '[SEP]']
Filtered   (016): ['ka', '##zoo', '_', 'kw', '##ar', '##gs', '=', '{', ':', 'time', '##out', '/', '1000', '}', '\\', 'n']
Detokenized (009): ['ka##zoo_kw##ar##gs', '=', '{', ':', 'time##out', '/', '1000', '}', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "p_to_str = lambda p : . join ( [ str ( p . topic . name ) , str ( p . leader . id ) , str ( p . id ) ] ) \n"
Original    (036): ['p_to_str', '=', 'lambda', 'p', ':', '.', 'join', '(', '[', 'str', '(', 'p', '.', 'topic', '.', 'name', ')', ',', 'str', '(', 'p', '.', 'leader', '.', 'id', ')', ',', 'str', '(', 'p', '.', 'id', ')', ']', ')', '\\n']
Tokenized   (047): ['[CLS]', 'p', '_', 'to', '_', 'st', '##r', '=', 'lambda', 'p', ':', '.', 'join', '(', '[', 'st', '##r', '(', 'p', '.', 'topic', '.', 'name', ')', ',', 'st', '##r', '(', 'p', '.', 'leader', '.', 'id', ')', ',', 'st', '##r', '(', 'p', '.', 'id', ')', ']', ')', '\\', 'n', '[SEP]']
Filtered   (045): ['p', '_', 'to', '_', 'st', '##r', '=', 'lambda', 'p', ':', '.', 'join', '(', '[', 'st', '##r', '(', 'p', '.', 'topic', '.', 'name', ')', ',', 'st', '##r', '(', 'p', '.', 'leader', '.', 'id', ')', ',', 'st', '##r', '(', 'p', '.', 'id', ')', ']', ')', '\\', 'n']
Detokenized (036): ['p_to_st##r', '=', 'lambda', 'p', ':', '.', 'join', '(', '[', 'st##r', '(', 'p', '.', 'topic', '.', 'name', ')', ',', 'st##r', '(', 'p', '.', 'leader', '.', 'id', ')', ',', 'st##r', '(', 'p', '.', 'id', ')', ']', ')', '\\n']
Counter: 45
===================================================================
Hidden states:  (13, 36, 768)
# Extracted words:  36
Sentence         : "idx = participants . index ( consumer_id or self . _consumer_id ) \n"
Original    (013): ['idx', '=', 'participants', '.', 'index', '(', 'consumer_id', 'or', 'self', '.', '_consumer_id', ')', '\\n']
Tokenized   (022): ['[CLS]', 'id', '##x', '=', 'participants', '.', 'index', '(', 'consumer', '_', 'id', 'or', 'self', '.', '_', 'consumer', '_', 'id', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['id', '##x', '=', 'participants', '.', 'index', '(', 'consumer', '_', 'id', 'or', 'self', '.', '_', 'consumer', '_', 'id', ')', '\\', 'n']
Detokenized (013): ['id##x', '=', 'participants', '.', 'index', '(', 'consumer_id', 'or', 'self', '.', '_consumer_id', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "parts_per_consumer = len ( all_parts ) // len ( participants ) \n"
Original    (012): ['parts_per_consumer', '=', 'len', '(', 'all_parts', ')', '//', 'len', '(', 'participants', ')', '\\n']
Tokenized   (022): ['[CLS]', 'parts', '_', 'per', '_', 'consumer', '=', 'len', '(', 'all', '_', 'parts', ')', '/', '/', 'len', '(', 'participants', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['parts', '_', 'per', '_', 'consumer', '=', 'len', '(', 'all', '_', 'parts', ')', '/', '/', 'len', '(', 'participants', ')', '\\', 'n']
Detokenized (012): ['parts_per_consumer', '=', 'len', '(', 'all_parts', ')', '//', 'len', '(', 'participants', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "remainder_ppc = len ( all_parts ) % len ( participants ) \n"
Original    (012): ['remainder_ppc', '=', 'len', '(', 'all_parts', ')', '%', 'len', '(', 'participants', ')', '\\n']
Tokenized   (020): ['[CLS]', 'remainder', '_', 'pp', '##c', '=', 'len', '(', 'all', '_', 'parts', ')', '%', 'len', '(', 'participants', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['remainder', '_', 'pp', '##c', '=', 'len', '(', 'all', '_', 'parts', ')', '%', 'len', '(', 'participants', ')', '\\', 'n']
Detokenized (012): ['remainder_pp##c', '=', 'len', '(', 'all_parts', ')', '%', 'len', '(', 'participants', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "num_parts = parts_per_consumer + ( 0 if ( idx + 1 > remainder_ppc ) else 1 ) \n"
Original    (018): ['num_parts', '=', 'parts_per_consumer', '+', '(', '0', 'if', '(', 'idx', '+', '1', '>', 'remainder_ppc', ')', 'else', '1', ')', '\\n']
Tokenized   (032): ['[CLS]', 'nu', '##m', '_', 'parts', '=', 'parts', '_', 'per', '_', 'consumer', '+', '(', '0', 'if', '(', 'id', '##x', '+', '1', '>', 'remainder', '_', 'pp', '##c', ')', 'else', '1', ')', '\\', 'n', '[SEP]']
Filtered   (030): ['nu', '##m', '_', 'parts', '=', 'parts', '_', 'per', '_', 'consumer', '+', '(', '0', 'if', '(', 'id', '##x', '+', '1', '>', 'remainder', '_', 'pp', '##c', ')', 'else', '1', ')', '\\', 'n']
Detokenized (018): ['nu##m_parts', '=', 'parts_per_consumer', '+', '(', '0', 'if', '(', 'id##x', '+', '1', '>', 'remainder_pp##c', ')', 'else', '1', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "log . debug ( , [ p_to_str ( p ) for p in new_partitions ] ) \n"
Original    (017): ['log', '.', 'debug', '(', ',', '[', 'p_to_str', '(', 'p', ')', 'for', 'p', 'in', 'new_partitions', ']', ')', '\\n']
Tokenized   (030): ['[CLS]', 'log', '.', 'de', '##bu', '##g', '(', ',', '[', 'p', '_', 'to', '_', 'st', '##r', '(', 'p', ')', 'for', 'p', 'in', 'new', '_', 'partition', '##s', ']', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['log', '.', 'de', '##bu', '##g', '(', ',', '[', 'p', '_', 'to', '_', 'st', '##r', '(', 'p', ')', 'for', 'p', 'in', 'new', '_', 'partition', '##s', ']', ')', '\\', 'n']
Detokenized (017): ['log', '.', 'de##bu##g', '(', ',', '[', 'p_to_st##r', '(', 'p', ')', 'for', 'p', 'in', 'new_partition##s', ']', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "id_ = get_string ( self . _consumer_id ) \n"
Original    (009): ['id_', '=', 'get_string', '(', 'self', '.', '_consumer_id', ')', '\\n']
Tokenized   (018): ['[CLS]', 'id', '_', '=', 'get', '_', 'string', '(', 'self', '.', '_', 'consumer', '_', 'id', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['id', '_', '=', 'get', '_', 'string', '(', 'self', '.', '_', 'consumer', '_', 'id', ')', '\\', 'n']
Detokenized (009): ['id_', '=', 'get_string', '(', 'self', '.', '_consumer_id', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "path = self . _topic_path , slug = partition_slug ) ) \n"
Original    (012): ['path', '=', 'self', '.', '_topic_path', ',', 'slug', '=', 'partition_slug', ')', ')', '\\n']
Tokenized   (020): ['[CLS]', 'path', '=', 'self', '.', '_', 'topic', '_', 'path', ',', 'slug', '=', 'partition', '_', 'slug', ')', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['path', '=', 'self', '.', '_', 'topic', '_', 'path', ',', 'slug', '=', 'partition', '_', 'slug', ')', ')', '\\', 'n']
Detokenized (012): ['path', '=', 'self', '.', '_topic_path', ',', 'slug', '=', 'partition_slug', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "HZCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 ) \n"
Original    (016): ['HZCharLenTable', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Tokenized   (023): ['[CLS]', 'hz', '##cha', '##rle', '##nta', '##ble', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['hz', '##cha', '##rle', '##nta', '##ble', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n']
Detokenized (016): ['hz##cha##rle##nta##ble', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "ISO2022CNCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ) \n"
Original    (022): ['ISO2022CNCharLenTable', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Tokenized   (032): ['[CLS]', 'iso', '##20', '##22', '##c', '##nch', '##ar', '##lent', '##able', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n', '[SEP]']
Filtered   (030): ['iso', '##20', '##22', '##c', '##nch', '##ar', '##lent', '##able', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n']
Detokenized (022): ['iso##20##22##c##nch##ar##lent##able', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "ISO2022JPCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ) \n"
Original    (024): ['ISO2022JPCharLenTable', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Tokenized   (034): ['[CLS]', 'iso', '##20', '##22', '##j', '##pc', '##har', '##lent', '##able', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n', '[SEP]']
Filtered   (032): ['iso', '##20', '##22', '##j', '##pc', '##har', '##lent', '##able', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n']
Detokenized (024): ['iso##20##22##j##pc##har##lent##able', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "logging . basicConfig ( level = logging . INFO ) \n"
Original    (011): ['logging', '.', 'basicConfig', '(', 'level', '=', 'logging', '.', 'INFO', ')', '\\n']
Tokenized   (017): ['[CLS]', 'logging', '.', 'basic', '##con', '##fi', '##g', '(', 'level', '=', 'logging', '.', 'info', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['logging', '.', 'basic', '##con', '##fi', '##g', '(', 'level', '=', 'logging', '.', 'info', ')', '\\', 'n']
Detokenized (011): ['logging', '.', 'basic##con##fi##g', '(', 'level', '=', 'logging', '.', 'info', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "tasa . __version__ , sys . version ) ) \n"
Original    (010): ['tasa', '.', '__version__', ',', 'sys', '.', 'version', ')', ')', '\\n']
Tokenized   (019): ['[CLS]', 'ta', '##sa', '.', '_', '_', 'version', '_', '_', ',', 'sy', '##s', '.', 'version', ')', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['ta', '##sa', '.', '_', '_', 'version', '_', '_', ',', 'sy', '##s', '.', 'version', ')', ')', '\\', 'n']
Detokenized (010): ['ta##sa', '.', '__version__', ',', 'sy##s', '.', 'version', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "type = lambda w : w . partition ( ) [ : : 2 ] , \n"
Original    (017): ['type', '=', 'lambda', 'w', ':', 'w', '.', 'partition', '(', ')', '[', ':', ':', '2', ']', ',', '\\n']
Tokenized   (020): ['[CLS]', 'type', '=', 'lambda', 'w', ':', 'w', '.', 'partition', '(', ')', '[', ':', ':', '2', ']', ',', '\\', 'n', '[SEP]']
Filtered   (018): ['type', '=', 'lambda', 'w', ':', 'w', '.', 'partition', '(', ')', '[', ':', ':', '2', ']', ',', '\\', 'n']
Detokenized (017): ['type', '=', 'lambda', 'w', ':', 'w', '.', 'partition', '(', ')', '[', ':', ':', '2', ']', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "worker_class_name = args . worker [ 1 ] or \n"
Original    (010): ['worker_class_name', '=', 'args', '.', 'worker', '[', '1', ']', 'or', '\\n']
Tokenized   (018): ['[CLS]', 'worker', '_', 'class', '_', 'name', '=', 'ar', '##gs', '.', 'worker', '[', '1', ']', 'or', '\\', 'n', '[SEP]']
Filtered   (016): ['worker', '_', 'class', '_', 'name', '=', 'ar', '##gs', '.', 'worker', '[', '1', ']', 'or', '\\', 'n']
Detokenized (010): ['worker_class_name', '=', 'ar##gs', '.', 'worker', '[', '1', ']', 'or', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "str ( job ) [ : 50 ] ) \n"
Original    (010): ['str', '(', 'job', ')', '[', ':', '50', ']', ')', '\\n']
Tokenized   (014): ['[CLS]', 'st', '##r', '(', 'job', ')', '[', ':', '50', ']', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['st', '##r', '(', 'job', ')', '[', ':', '50', ']', ')', '\\', 'n']
Detokenized (010): ['st##r', '(', 'job', ')', '[', ':', '50', ']', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "processes = [ Process ( target = run , args = ( ) ) for x in range ( count ) ] \n"
Original    (023): ['processes', '=', '[', 'Process', '(', 'target', '=', 'run', ',', 'args', '=', '(', ')', ')', 'for', 'x', 'in', 'range', '(', 'count', ')', ']', '\\n']
Tokenized   (027): ['[CLS]', 'processes', '=', '[', 'process', '(', 'target', '=', 'run', ',', 'ar', '##gs', '=', '(', ')', ')', 'for', 'x', 'in', 'range', '(', 'count', ')', ']', '\\', 'n', '[SEP]']
Filtered   (025): ['processes', '=', '[', 'process', '(', 'target', '=', 'run', ',', 'ar', '##gs', '=', '(', ')', ')', 'for', 'x', 'in', 'range', '(', 'count', ')', ']', '\\', 'n']
Detokenized (023): ['processes', '=', '[', 'process', '(', 'target', '=', 'run', ',', 'ar##gs', '=', '(', ')', ')', 'for', 'x', 'in', 'range', '(', 'count', ')', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "color = models . CharField ( max_length = 6 , validators = [ color_regex ] , help_text = \n"
Original    (019): ['color', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '6', ',', 'validators', '=', '[', 'color_regex', ']', ',', 'help_text', '=', '\\n']
Tokenized   (031): ['[CLS]', 'color', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '6', ',', 'valid', '##ators', '=', '[', 'color', '_', 'reg', '##ex', ']', ',', 'help', '_', 'text', '=', '\\', 'n', '[SEP]']
Filtered   (029): ['color', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '6', ',', 'valid', '##ators', '=', '[', 'color', '_', 'reg', '##ex', ']', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (019): ['color', '=', 'models', '.', 'char##field', '(', 'max_length', '=', '6', ',', 'valid##ators', '=', '[', 'color_reg##ex', ']', ',', 'help_text', '=', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "first_name = models . CharField ( max_length = 64 ) \n"
Original    (011): ['first_name', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '64', ')', '\\n']
Tokenized   (019): ['[CLS]', 'first', '_', 'name', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '64', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['first', '_', 'name', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '64', ')', '\\', 'n']
Detokenized (011): ['first_name', '=', 'models', '.', 'char##field', '(', 'max_length', '=', '64', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "role = models . CharField ( max_length = 17 , choices = ROLE_CHOICES ) \n"
Original    (015): ['role', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '17', ',', 'choices', '=', 'ROLE_CHOICES', ')', '\\n']
Tokenized   (023): ['[CLS]', 'role', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '17', ',', 'choices', '=', 'role', '_', 'choices', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['role', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '17', ',', 'choices', '=', 'role', '_', 'choices', ')', '\\', 'n']
Detokenized (015): ['role', '=', 'models', '.', 'char##field', '(', 'max_length', '=', '17', ',', 'choices', '=', 'role_choices', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "phone_work = models . CharField ( max_length = 15 , validators = [ phone_regex ] , blank = True ) \n"
Original    (021): ['phone_work', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '15', ',', 'validators', '=', '[', 'phone_regex', ']', ',', 'blank', '=', 'True', ')', '\\n']
Tokenized   (033): ['[CLS]', 'phone', '_', 'work', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '15', ',', 'valid', '##ators', '=', '[', 'phone', '_', 'reg', '##ex', ']', ',', 'blank', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['phone', '_', 'work', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '15', ',', 'valid', '##ators', '=', '[', 'phone', '_', 'reg', '##ex', ']', ',', 'blank', '=', 'true', ')', '\\', 'n']
Detokenized (021): ['phone_work', '=', 'models', '.', 'char##field', '(', 'max_length', '=', '15', ',', 'valid##ators', '=', '[', 'phone_reg##ex', ']', ',', 'blank', '=', 'true', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "job_title = models . CharField ( max_length = 128 , blank = True ) \n"
Original    (015): ['job_title', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '128', ',', 'blank', '=', 'True', ')', '\\n']
Tokenized   (023): ['[CLS]', 'job', '_', 'title', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '128', ',', 'blank', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['job', '_', 'title', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '128', ',', 'blank', '=', 'true', ')', '\\', 'n']
Detokenized (015): ['job_title', '=', 'models', '.', 'char##field', '(', 'max_length', '=', '128', ',', 'blank', '=', 'true', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "category = models . CharField ( max_length = 21 , choices = CATEGORY_CHOICES , help_text = description = models . CharField ( max_length = 256 , blank = True , help_text = reference = models . URLField ( blank = True , help_text = ) \n"
Original    (046): ['category', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '21', ',', 'choices', '=', 'CATEGORY_CHOICES', ',', 'help_text', '=', 'description', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '256', ',', 'blank', '=', 'True', ',', 'help_text', '=', 'reference', '=', 'models', '.', 'URLField', '(', 'blank', '=', 'True', ',', 'help_text', '=', ')', '\\n']
Tokenized   (065): ['[CLS]', 'category', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '21', ',', 'choices', '=', 'category', '_', 'choices', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '256', ',', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', 'reference', '=', 'models', '.', 'ur', '##lf', '##ield', '(', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', ')', '\\', 'n', '[SEP]']
Filtered   (063): ['category', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '21', ',', 'choices', '=', 'category', '_', 'choices', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '256', ',', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', 'reference', '=', 'models', '.', 'ur', '##lf', '##ield', '(', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', ')', '\\', 'n']
Detokenized (046): ['category', '=', 'models', '.', 'char##field', '(', 'max_length', '=', '21', ',', 'choices', '=', 'category_choices', ',', 'help_text', '=', 'description', '=', 'models', '.', 'char##field', '(', 'max_length', '=', '256', ',', 'blank', '=', 'true', ',', 'help_text', '=', 'reference', '=', 'models', '.', 'ur##lf##ield', '(', 'blank', '=', 'true', ',', 'help_text', '=', ')', '\\n']
Counter: 63
===================================================================
Hidden states:  (13, 46, 768)
# Extracted words:  46
Sentence         : "acronym = models . CharField ( max_length = 20 , unique = True , help_text = category = models . CharField ( max_length = 9 , choices = CATEGORY_CHOICES , help_text = jurisdiction = models . CharField ( max_length = 64 , help_text = description = models . TextField ( blank = True , help_text = reference = models . URLField ( blank = True , help_text = ) \n"
Original    (070): ['acronym', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '20', ',', 'unique', '=', 'True', ',', 'help_text', '=', 'category', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '9', ',', 'choices', '=', 'CATEGORY_CHOICES', ',', 'help_text', '=', 'jurisdiction', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '64', ',', 'help_text', '=', 'description', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', 'reference', '=', 'models', '.', 'URLField', '(', 'blank', '=', 'True', ',', 'help_text', '=', ')', '\\n']
Tokenized   (097): ['[CLS]', 'acronym', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '20', ',', 'unique', '=', 'true', ',', 'help', '_', 'text', '=', 'category', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '9', ',', 'choices', '=', 'category', '_', 'choices', ',', 'help', '_', 'text', '=', 'jurisdiction', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '64', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'text', '##field', '(', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', 'reference', '=', 'models', '.', 'ur', '##lf', '##ield', '(', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', ')', '\\', 'n', '[SEP]']
Filtered   (095): ['acronym', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '20', ',', 'unique', '=', 'true', ',', 'help', '_', 'text', '=', 'category', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '9', ',', 'choices', '=', 'category', '_', 'choices', ',', 'help', '_', 'text', '=', 'jurisdiction', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '64', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'text', '##field', '(', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', 'reference', '=', 'models', '.', 'ur', '##lf', '##ield', '(', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', ')', '\\', 'n']
Detokenized (070): ['acronym', '=', 'models', '.', 'char##field', '(', 'max_length', '=', '20', ',', 'unique', '=', 'true', ',', 'help_text', '=', 'category', '=', 'models', '.', 'char##field', '(', 'max_length', '=', '9', ',', 'choices', '=', 'category_choices', ',', 'help_text', '=', 'jurisdiction', '=', 'models', '.', 'char##field', '(', 'max_length', '=', '64', ',', 'help_text', '=', 'description', '=', 'models', '.', 'text##field', '(', 'blank', '=', 'true', ',', 'help_text', '=', 'reference', '=', 'models', '.', 'ur##lf##ield', '(', 'blank', '=', 'true', ',', 'help_text', '=', ')', '\\n']
Counter: 95
===================================================================
Hidden states:  (13, 70, 768)
# Extracted words:  70
Sentence         : "description = models . CharField ( max_length = 256 , blank = True , help_text = \n"
Original    (017): ['description', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '256', ',', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (025): ['[CLS]', 'description', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '256', ',', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', '\\', 'n', '[SEP]']
Filtered   (023): ['description', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '256', ',', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (017): ['description', '=', 'models', '.', 'char##field', '(', 'max_length', '=', '256', ',', 'blank', '=', 'true', ',', 'help_text', '=', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "business_criticality = models . CharField ( max_length = 9 , choices = BUSINESS_CRITICALITY_CHOICES , blank platform = models . CharField ( max_length = 11 , choices = PLATFORM_CHOICES , blank = True , null = True ) \n"
Original    (038): ['business_criticality', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '9', ',', 'choices', '=', 'BUSINESS_CRITICALITY_CHOICES', ',', 'blank', 'platform', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '11', ',', 'choices', '=', 'PLATFORM_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\n']
Tokenized   (057): ['[CLS]', 'business', '_', 'critical', '##ity', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '9', ',', 'choices', '=', 'business', '_', 'critical', '##ity', '_', 'choices', ',', 'blank', 'platform', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '11', ',', 'choices', '=', 'platform', '_', 'choices', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (055): ['business', '_', 'critical', '##ity', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '9', ',', 'choices', '=', 'business', '_', 'critical', '##ity', '_', 'choices', ',', 'blank', 'platform', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '11', ',', 'choices', '=', 'platform', '_', 'choices', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ')', '\\', 'n']
Detokenized (038): ['business_critical##ity', '=', 'models', '.', 'char##field', '(', 'max_length', '=', '9', ',', 'choices', '=', 'business_critical##ity_choices', ',', 'blank', 'platform', '=', 'models', '.', 'char##field', '(', 'max_length', '=', '11', ',', 'choices', '=', 'platform_choices', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ')', '\\n']
Counter: 55
===================================================================
Hidden states:  (13, 38, 768)
# Extracted words:  38
Sentence         : "lifecycle = models . CharField ( max_length = 8 , choices = LIFECYCLE_CHOICES , blank = True , null = True ) \n"
Original    (023): ['lifecycle', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '8', ',', 'choices', '=', 'LIFECYCLE_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\n']
Tokenized   (033): ['[CLS]', 'life', '##cycle', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '8', ',', 'choices', '=', 'life', '##cycle', '_', 'choices', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['life', '##cycle', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '8', ',', 'choices', '=', 'life', '##cycle', '_', 'choices', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ')', '\\', 'n']
Detokenized (023): ['life##cycle', '=', 'models', '.', 'char##field', '(', 'max_length', '=', '8', ',', 'choices', '=', 'life##cycle_choices', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "user_records = models . PositiveIntegerField ( blank = True , null = True , help_text = revenue = models . DecimalField ( max_digits = 15 , decimal_places = 2 , blank = True , null = True , help_text = external_audience = models . BooleanField ( default = False , help_text = internet_accessible = models . BooleanField ( default = False , help_text = requestable = models . NullBooleanField ( default = True , help_text = _ ( \n"
Original    (079): ['user_records', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'revenue', '=', 'models', '.', 'DecimalField', '(', 'max_digits', '=', '15', ',', 'decimal_places', '=', '2', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'external_audience', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ',', 'help_text', '=', 'internet_accessible', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ',', 'help_text', '=', 'requestable', '=', 'models', '.', 'NullBooleanField', '(', 'default', '=', 'True', ',', 'help_text', '=', '_', '(', '\\n']
Tokenized   (115): ['[CLS]', 'user', '_', 'records', '=', 'models', '.', 'positive', '##int', '##eger', '##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', 'revenue', '=', 'models', '.', 'decimal', '##field', '(', 'max', '_', 'digits', '=', '15', ',', 'decimal', '_', 'places', '=', '2', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', 'external', '_', 'audience', '=', 'models', '.', 'boo', '##lean', '##field', '(', 'default', '=', 'false', ',', 'help', '_', 'text', '=', 'internet', '_', 'accessible', '=', 'models', '.', 'boo', '##lean', '##field', '(', 'default', '=', 'false', ',', 'help', '_', 'text', '=', 'request', '##able', '=', 'models', '.', 'null', '##bo', '##ole', '##an', '##field', '(', 'default', '=', 'true', ',', 'help', '_', 'text', '=', '_', '(', '\\', 'n', '[SEP]']
Filtered   (113): ['user', '_', 'records', '=', 'models', '.', 'positive', '##int', '##eger', '##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', 'revenue', '=', 'models', '.', 'decimal', '##field', '(', 'max', '_', 'digits', '=', '15', ',', 'decimal', '_', 'places', '=', '2', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', 'external', '_', 'audience', '=', 'models', '.', 'boo', '##lean', '##field', '(', 'default', '=', 'false', ',', 'help', '_', 'text', '=', 'internet', '_', 'accessible', '=', 'models', '.', 'boo', '##lean', '##field', '(', 'default', '=', 'false', ',', 'help', '_', 'text', '=', 'request', '##able', '=', 'models', '.', 'null', '##bo', '##ole', '##an', '##field', '(', 'default', '=', 'true', ',', 'help', '_', 'text', '=', '_', '(', '\\', 'n']
Detokenized (079): ['user_records', '=', 'models', '.', 'positive##int##eger##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help_text', '=', 'revenue', '=', 'models', '.', 'decimal##field', '(', 'max_digits', '=', '15', ',', 'decimal_places', '=', '2', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help_text', '=', 'external_audience', '=', 'models', '.', 'boo##lean##field', '(', 'default', '=', 'false', ',', 'help_text', '=', 'internet_accessible', '=', 'models', '.', 'boo##lean##field', '(', 'default', '=', 'false', ',', 'help_text', '=', 'request##able', '=', 'models', '.', 'null##bo##ole##an##field', '(', 'default', '=', 'true', ',', 'help_text', '=', '_', '(', '\\n']
Counter: 113
===================================================================
Hidden states:  (13, 79, 768)
# Extracted words:  79
Sentence         : "override_dcl = models . IntegerField ( choices = DATA_CLASSIFICATION_CHOICES , blank = True , null = True , help_text override_reason = models . TextField ( blank = True , help_text = \n"
Original    (032): ['override_dcl', '=', 'models', '.', 'IntegerField', '(', 'choices', '=', 'DATA_CLASSIFICATION_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', 'override_reason', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (052): ['[CLS]', 'over', '##ride', '_', 'dc', '##l', '=', 'models', '.', 'integer', '##field', '(', 'choices', '=', 'data', '_', 'classification', '_', 'choices', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', 'over', '##ride', '_', 'reason', '=', 'models', '.', 'text', '##field', '(', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', '\\', 'n', '[SEP]']
Filtered   (050): ['over', '##ride', '_', 'dc', '##l', '=', 'models', '.', 'integer', '##field', '(', 'choices', '=', 'data', '_', 'classification', '_', 'choices', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', 'over', '##ride', '_', 'reason', '=', 'models', '.', 'text', '##field', '(', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (032): ['over##ride_dc##l', '=', 'models', '.', 'integer##field', '(', 'choices', '=', 'data_classification_choices', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help_text', 'over##ride_reason', '=', 'models', '.', 'text##field', '(', 'blank', '=', 'true', ',', 'help_text', '=', '\\n']
Counter: 50
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "threadfix = models . ForeignKey ( ThreadFix , blank = True , null = True , help_text = threadfix_team_id = models . PositiveIntegerField ( blank = True , null = True , help_text = threadfix_application_id = models . PositiveIntegerField ( blank = True , null = True , help_text = \n"
Original    (051): ['threadfix', '=', 'models', '.', 'ForeignKey', '(', 'ThreadFix', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'threadfix_team_id', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'threadfix_application_id', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (083): ['[CLS]', 'thread', '##fi', '##x', '=', 'models', '.', 'foreign', '##key', '(', 'thread', '##fi', '##x', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', 'thread', '##fi', '##x', '_', 'team', '_', 'id', '=', 'models', '.', 'positive', '##int', '##eger', '##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', 'thread', '##fi', '##x', '_', 'application', '_', 'id', '=', 'models', '.', 'positive', '##int', '##eger', '##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', '\\', 'n', '[SEP]']
Filtered   (081): ['thread', '##fi', '##x', '=', 'models', '.', 'foreign', '##key', '(', 'thread', '##fi', '##x', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', 'thread', '##fi', '##x', '_', 'team', '_', 'id', '=', 'models', '.', 'positive', '##int', '##eger', '##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', 'thread', '##fi', '##x', '_', 'application', '_', 'id', '=', 'models', '.', 'positive', '##int', '##eger', '##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (051): ['thread##fi##x', '=', 'models', '.', 'foreign##key', '(', 'thread##fi##x', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help_text', '=', 'thread##fi##x_team_id', '=', 'models', '.', 'positive##int##eger##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help_text', '=', 'thread##fi##x_application_id', '=', 'models', '.', 'positive##int##eger##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help_text', '=', '\\n']
Counter: 81
===================================================================
Hidden states:  (13, 51, 768)
# Extracted words:  51
Sentence         : "asvs_level = models . IntegerField ( choices = ASVS_CHOICES , blank = True , null = True , help_text = asvs_level_percent_achieved = models . PositiveIntegerField ( blank = True , null = True , help_text = asvs_doc_url = models . URLField ( blank = True , help_text = ) \n"
Original    (050): ['asvs_level', '=', 'models', '.', 'IntegerField', '(', 'choices', '=', 'ASVS_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'asvs_level_percent_achieved', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'asvs_doc_url', '=', 'models', '.', 'URLField', '(', 'blank', '=', 'True', ',', 'help_text', '=', ')', '\\n']
Tokenized   (084): ['[CLS]', 'as', '##vs', '_', 'level', '=', 'models', '.', 'integer', '##field', '(', 'choices', '=', 'as', '##vs', '_', 'choices', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', 'as', '##vs', '_', 'level', '_', 'percent', '_', 'achieved', '=', 'models', '.', 'positive', '##int', '##eger', '##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', 'as', '##vs', '_', 'doc', '_', 'ur', '##l', '=', 'models', '.', 'ur', '##lf', '##ield', '(', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', ')', '\\', 'n', '[SEP]']
Filtered   (082): ['as', '##vs', '_', 'level', '=', 'models', '.', 'integer', '##field', '(', 'choices', '=', 'as', '##vs', '_', 'choices', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', 'as', '##vs', '_', 'level', '_', 'percent', '_', 'achieved', '=', 'models', '.', 'positive', '##int', '##eger', '##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', 'as', '##vs', '_', 'doc', '_', 'ur', '##l', '=', 'models', '.', 'ur', '##lf', '##ield', '(', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', ')', '\\', 'n']
Detokenized (050): ['as##vs_level', '=', 'models', '.', 'integer##field', '(', 'choices', '=', 'as##vs_choices', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help_text', '=', 'as##vs_level_percent_achieved', '=', 'models', '.', 'positive##int##eger##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help_text', '=', 'as##vs_doc_ur##l', '=', 'models', '.', 'ur##lf##ield', '(', 'blank', '=', 'true', ',', 'help_text', '=', ')', '\\n']
Counter: 82
===================================================================
Hidden states:  (13, 50, 768)
# Extracted words:  50
Sentence         : "asvs_level_target = models . IntegerField ( choices = ASVS_CHOICES , blank = True , null = True , help_text = \n"
Original    (021): ['asvs_level_target', '=', 'models', '.', 'IntegerField', '(', 'choices', '=', 'ASVS_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (035): ['[CLS]', 'as', '##vs', '_', 'level', '_', 'target', '=', 'models', '.', 'integer', '##field', '(', 'choices', '=', 'as', '##vs', '_', 'choices', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', '\\', 'n', '[SEP]']
Filtered   (033): ['as', '##vs', '_', 'level', '_', 'target', '=', 'models', '.', 'integer', '##field', '(', 'choices', '=', 'as', '##vs', '_', 'choices', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (021): ['as##vs_level_target', '=', 'models', '.', 'integer##field', '(', 'choices', '=', 'as##vs_choices', ',', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help_text', '=', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "organization = models . ForeignKey ( Organization , help_text = people = models . ManyToManyField ( Person , through = , blank = True ) \n"
Original    (026): ['organization', '=', 'models', '.', 'ForeignKey', '(', 'Organization', ',', 'help_text', '=', 'people', '=', 'models', '.', 'ManyToManyField', '(', 'Person', ',', 'through', '=', ',', 'blank', '=', 'True', ')', '\\n']
Tokenized   (035): ['[CLS]', 'organization', '=', 'models', '.', 'foreign', '##key', '(', 'organization', ',', 'help', '_', 'text', '=', 'people', '=', 'models', '.', 'many', '##tom', '##any', '##field', '(', 'person', ',', 'through', '=', ',', 'blank', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (033): ['organization', '=', 'models', '.', 'foreign', '##key', '(', 'organization', ',', 'help', '_', 'text', '=', 'people', '=', 'models', '.', 'many', '##tom', '##any', '##field', '(', 'person', ',', 'through', '=', ',', 'blank', '=', 'true', ')', '\\', 'n']
Detokenized (026): ['organization', '=', 'models', '.', 'foreign##key', '(', 'organization', ',', 'help_text', '=', 'people', '=', 'models', '.', 'many##tom##any##field', '(', 'person', ',', 'through', '=', ',', 'blank', '=', 'true', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "delta = self . created_date - timezone . now ( ) \n"
Original    (012): ['delta', '=', 'self', '.', 'created_date', '-', 'timezone', '.', 'now', '(', ')', '\\n']
Tokenized   (018): ['[CLS]', 'delta', '=', 'self', '.', 'created', '_', 'date', '-', 'time', '##zone', '.', 'now', '(', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['delta', '=', 'self', '.', 'created', '_', 'date', '-', 'time', '##zone', '.', 'now', '(', ')', '\\', 'n']
Detokenized (012): ['delta', '=', 'self', '.', 'created_date', '-', 'time##zone', '.', 'now', '(', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "person = models . ForeignKey ( Person , help_text = ) \n"
Original    (012): ['person', '=', 'models', '.', 'ForeignKey', '(', 'Person', ',', 'help_text', '=', ')', '\\n']
Tokenized   (018): ['[CLS]', 'person', '=', 'models', '.', 'foreign', '##key', '(', 'person', ',', 'help', '_', 'text', '=', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['person', '=', 'models', '.', 'foreign', '##key', '(', 'person', ',', 'help', '_', 'text', '=', ')', '\\', 'n']
Detokenized (012): ['person', '=', 'models', '.', 'foreign##key', '(', 'person', ',', 'help_text', '=', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "environment_type = models . CharField ( max_length = 4 , choices = ENVIRONMENT_CHOICES , help_text = description = models . TextField ( blank = True , help_text = testing_approved = models . BooleanField ( default = False , help_text = \n"
Original    (041): ['environment_type', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '4', ',', 'choices', '=', 'ENVIRONMENT_CHOICES', ',', 'help_text', '=', 'description', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', 'testing_approved', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ',', 'help_text', '=', '\\n']
Tokenized   (062): ['[CLS]', 'environment', '_', 'type', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '4', ',', 'choices', '=', 'environment', '_', 'choices', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'text', '##field', '(', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', 'testing', '_', 'approved', '=', 'models', '.', 'boo', '##lean', '##field', '(', 'default', '=', 'false', ',', 'help', '_', 'text', '=', '\\', 'n', '[SEP]']
Filtered   (060): ['environment', '_', 'type', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '4', ',', 'choices', '=', 'environment', '_', 'choices', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'text', '##field', '(', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', 'testing', '_', 'approved', '=', 'models', '.', 'boo', '##lean', '##field', '(', 'default', '=', 'false', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (041): ['environment_type', '=', 'models', '.', 'char##field', '(', 'max_length', '=', '4', ',', 'choices', '=', 'environment_choices', ',', 'help_text', '=', 'description', '=', 'models', '.', 'text##field', '(', 'blank', '=', 'true', ',', 'help_text', '=', 'testing_approved', '=', 'models', '.', 'boo##lean##field', '(', 'default', '=', 'false', ',', 'help_text', '=', '\\n']
Counter: 60
===================================================================
Hidden states:  (13, 41, 768)
# Extracted words:  41
Sentence         : "location = models . URLField ( help_text = notes = models . TextField ( blank = True , help_text = \n"
Original    (021): ['location', '=', 'models', '.', 'URLField', '(', 'help_text', '=', 'notes', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (031): ['[CLS]', 'location', '=', 'models', '.', 'ur', '##lf', '##ield', '(', 'help', '_', 'text', '=', 'notes', '=', 'models', '.', 'text', '##field', '(', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', '\\', 'n', '[SEP]']
Filtered   (029): ['location', '=', 'models', '.', 'ur', '##lf', '##ield', '(', 'help', '_', 'text', '=', 'notes', '=', 'models', '.', 'text', '##field', '(', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (021): ['location', '=', 'models', '.', 'ur##lf##ield', '(', 'help_text', '=', 'notes', '=', 'models', '.', 'text##field', '(', 'blank', '=', 'true', ',', 'help_text', '=', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "role_description = models . CharField ( max_length = 128 , blank = True , help_text = notes = models . TextField ( blank = True , help_text = \n"
Original    (029): ['role_description', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '128', ',', 'blank', '=', 'True', ',', 'help_text', '=', 'notes', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (042): ['[CLS]', 'role', '_', 'description', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '128', ',', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', 'notes', '=', 'models', '.', 'text', '##field', '(', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', '\\', 'n', '[SEP]']
Filtered   (040): ['role', '_', 'description', '=', 'models', '.', 'char', '##field', '(', 'max', '_', 'length', '=', '128', ',', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', 'notes', '=', 'models', '.', 'text', '##field', '(', 'blank', '=', 'true', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (029): ['role_description', '=', 'models', '.', 'char##field', '(', 'max_length', '=', '128', ',', 'blank', '=', 'true', ',', 'help_text', '=', 'notes', '=', 'models', '.', 'text##field', '(', 'blank', '=', 'true', ',', 'help_text', '=', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "start_date = models . DateField ( help_text = ) \n"
Original    (010): ['start_date', '=', 'models', '.', 'DateField', '(', 'help_text', '=', ')', '\\n']
Tokenized   (018): ['[CLS]', 'start', '_', 'date', '=', 'models', '.', 'date', '##field', '(', 'help', '_', 'text', '=', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['start', '_', 'date', '=', 'models', '.', 'date', '##field', '(', 'help', '_', 'text', '=', ')', '\\', 'n']
Detokenized (010): ['start_date', '=', 'models', '.', 'date##field', '(', 'help_text', '=', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "open_date = models . DateTimeField ( blank = True , null = True , help_text = close_date = models . DateTimeField ( blank = True , null = True , help_text = duration = models . DurationField ( blank = True , null = True ) \n"
Original    (047): ['open_date', '=', 'models', '.', 'DateTimeField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'close_date', '=', 'models', '.', 'DateTimeField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'duration', '=', 'models', '.', 'DurationField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\n']
Tokenized   (063): ['[CLS]', 'open', '_', 'date', '=', 'models', '.', 'date', '##time', '##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', 'close', '_', 'date', '=', 'models', '.', 'date', '##time', '##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', 'duration', '=', 'models', '.', 'duration', '##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (061): ['open', '_', 'date', '=', 'models', '.', 'date', '##time', '##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', 'close', '_', 'date', '=', 'models', '.', 'date', '##time', '##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help', '_', 'text', '=', 'duration', '=', 'models', '.', 'duration', '##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ')', '\\', 'n']
Detokenized (047): ['open_date', '=', 'models', '.', 'date##time##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help_text', '=', 'close_date', '=', 'models', '.', 'date##time##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ',', 'help_text', '=', 'duration', '=', 'models', '.', 'duration##field', '(', 'blank', '=', 'true', ',', 'null', '=', 'true', ')', '\\n']
Counter: 61
===================================================================
Hidden states:  (13, 47, 768)
# Extracted words:  47
Sentence         : "metrics = managers . ActivityTypeMetrics . from_queryset ( managers . ActivityTypeQuerySet ) ( ) \n"
Original    (015): ['metrics', '=', 'managers', '.', 'ActivityTypeMetrics', '.', 'from_queryset', '(', 'managers', '.', 'ActivityTypeQuerySet', ')', '(', ')', '\\n']
Tokenized   (029): ['[CLS]', 'metric', '##s', '=', 'managers', '.', 'activity', '##type', '##metric', '##s', '.', 'from', '_', 'query', '##set', '(', 'managers', '.', 'activity', '##type', '##que', '##rys', '##et', ')', '(', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['metric', '##s', '=', 'managers', '.', 'activity', '##type', '##metric', '##s', '.', 'from', '_', 'query', '##set', '(', 'managers', '.', 'activity', '##type', '##que', '##rys', '##et', ')', '(', ')', '\\', 'n']
Detokenized (015): ['metric##s', '=', 'managers', '.', 'activity##type##metric##s', '.', 'from_query##set', '(', 'managers', '.', 'activity##type##que##rys##et', ')', '(', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "token = models . UUIDField ( default = uuid . uuid4 , editable = False ) \n"
Original    (017): ['token', '=', 'models', '.', 'UUIDField', '(', 'default', '=', 'uuid', '.', 'uuid4', ',', 'editable', '=', 'False', ')', '\\n']
Tokenized   (026): ['[CLS]', 'token', '=', 'models', '.', 'u', '##uid', '##field', '(', 'default', '=', 'u', '##uid', '.', 'u', '##uid', '##4', ',', 'edit', '##able', '=', 'false', ')', '\\', 'n', '[SEP]']
Filtered   (024): ['token', '=', 'models', '.', 'u', '##uid', '##field', '(', 'default', '=', 'u', '##uid', '.', 'u', '##uid', '##4', ',', 'edit', '##able', '=', 'false', ')', '\\', 'n']
Detokenized (017): ['token', '=', 'models', '.', 'u##uid##field', '(', 'default', '=', 'u##uid', '.', 'u##uid##4', ',', 'edit##able', '=', 'false', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "activities = models . ManyToManyField ( ActivityType , limit_choices_to = { : True } ) \n"
Original    (016): ['activities', '=', 'models', '.', 'ManyToManyField', '(', 'ActivityType', ',', 'limit_choices_to', '=', '{', ':', 'True', '}', ')', '\\n']
Tokenized   (027): ['[CLS]', 'activities', '=', 'models', '.', 'many', '##tom', '##any', '##field', '(', 'activity', '##type', ',', 'limit', '_', 'choices', '_', 'to', '=', '{', ':', 'true', '}', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['activities', '=', 'models', '.', 'many', '##tom', '##any', '##field', '(', 'activity', '##type', ',', 'limit', '_', 'choices', '_', 'to', '=', '{', ':', 'true', '}', ')', '\\', 'n']
Detokenized (016): ['activities', '=', 'models', '.', 'many##tom##any##field', '(', 'activity##type', ',', 'limit_choices_to', '=', '{', ':', 'true', '}', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "opener . addheaders = [ ( , ) ] \n"
Original    (010): ['opener', '.', 'addheaders', '=', '[', '(', ',', ')', ']', '\\n']
Tokenized   (015): ['[CLS]', 'opener', '.', 'add', '##head', '##ers', '=', '[', '(', ',', ')', ']', '\\', 'n', '[SEP]']
Filtered   (013): ['opener', '.', 'add', '##head', '##ers', '=', '[', '(', ',', ')', ']', '\\', 'n']
Detokenized (010): ['opener', '.', 'add##head##ers', '=', '[', '(', ',', ')', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "False = 0 \n"
Original    (004): ['False', '=', '0', '\\n']
Tokenized   (007): ['[CLS]', 'false', '=', '0', '\\', 'n', '[SEP]']
Filtered   (005): ['false', '=', '0', '\\', 'n']
Detokenized (004): ['false', '=', '0', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "option_pattern = chr ( 0 ) * 8 \n"
Original    (009): ['option_pattern', '=', 'chr', '(', '0', ')', '*', '8', '\\n']
Tokenized   (015): ['[CLS]', 'option', '_', 'pattern', '=', 'ch', '##r', '(', '0', ')', '*', '8', '\\', 'n', '[SEP]']
Filtered   (013): ['option', '_', 'pattern', '=', 'ch', '##r', '(', '0', ')', '*', '8', '\\', 'n']
Detokenized (009): ['option_pattern', '=', 'ch##r', '(', '0', ')', '*', '8', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "begin = toint ( s [ 5 : 9 ] ) \n"
Original    (012): ['begin', '=', 'toint', '(', 's', '[', '5', ':', '9', ']', ')', '\\n']
Tokenized   (016): ['[CLS]', 'begin', '=', 'to', '##int', '(', 's', '[', '5', ':', '9', ']', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['begin', '=', 'to', '##int', '(', 's', '[', '5', ':', '9', ']', ')', '\\', 'n']
Detokenized (012): ['begin', '=', 'to##int', '(', 's', '[', '5', ':', '9', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "length = len ( s ) - 9 \n"
Original    (009): ['length', '=', 'len', '(', 's', ')', '-', '9', '\\n']
Tokenized   (012): ['[CLS]', 'length', '=', 'len', '(', 's', ')', '-', '9', '\\', 'n', '[SEP]']
Filtered   (010): ['length', '=', 'len', '(', 's', ')', '-', '9', '\\', 'n']
Detokenized (009): ['length', '=', 'len', '(', 's', ')', '-', '9', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "DivergeCommits = namedtuple ( "DivergeCommits" , [ "common_parent" , \n"
Original    (010): ['DivergeCommits', '=', 'namedtuple', '(', '"DivergeCommits"', ',', '[', '"common_parent"', ',', '\\n']
Tokenized   (029): ['[CLS]', 'diver', '##ge', '##com', '##mit', '##s', '=', 'named', '##tu', '##ple', '(', '"', 'diver', '##ge', '##com', '##mit', '##s', '"', ',', '[', '"', 'common', '_', 'parent', '"', ',', '\\', 'n', '[SEP]']
Filtered   (027): ['diver', '##ge', '##com', '##mit', '##s', '=', 'named', '##tu', '##ple', '(', '"', 'diver', '##ge', '##com', '##mit', '##s', '"', ',', '[', '"', 'common', '_', 'parent', '"', ',', '\\', 'n']
Detokenized (010): ['diver##ge##com##mit##s', '=', 'named##tu##ple', '(', '"diver##ge##com##mit##s"', ',', '[', '"common_parent"', ',', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : ""first_commits" , "second_commits" ] ) \n"
Original    (006): ['"first_commits"', ',', '"second_commits"', ']', ')', '\\n']
Tokenized   (017): ['[CLS]', '"', 'first', '_', 'commits', '"', ',', '"', 'second', '_', 'commits', '"', ']', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['"', 'first', '_', 'commits', '"', ',', '"', 'second', '_', 'commits', '"', ']', ')', '\\', 'n']
Detokenized (006): ['"first_commits"', ',', '"second_commits"', ']', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "behind = len ( diverge_commits . second_commits ) > 0 \n"
Original    (011): ['behind', '=', 'len', '(', 'diverge_commits', '.', 'second_commits', ')', '>', '0', '\\n']
Tokenized   (019): ['[CLS]', 'behind', '=', 'len', '(', 'diver', '##ge', '_', 'commits', '.', 'second', '_', 'commits', ')', '>', '0', '\\', 'n', '[SEP]']
Filtered   (017): ['behind', '=', 'len', '(', 'diver', '##ge', '_', 'commits', '.', 'second', '_', 'commits', ')', '>', '0', '\\', 'n']
Detokenized (011): ['behind', '=', 'len', '(', 'diver##ge_commits', '.', 'second_commits', ')', '>', '0', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "onerror = lambda function , fpath , excinfo : log . info ( \n"
Original    (014): ['onerror', '=', 'lambda', 'function', ',', 'fpath', ',', 'excinfo', ':', 'log', '.', 'info', '(', '\\n']
Tokenized   (022): ['[CLS]', 'one', '##rro', '##r', '=', 'lambda', 'function', ',', 'f', '##path', ',', 'ex', '##cin', '##fo', ':', 'log', '.', 'info', '(', '\\', 'n', '[SEP]']
Filtered   (020): ['one', '##rro', '##r', '=', 'lambda', 'function', ',', 'f', '##path', ',', 'ex', '##cin', '##fo', ':', 'log', '.', 'info', '(', '\\', 'n']
Detokenized (014): ['one##rro##r', '=', 'lambda', 'function', ',', 'f##path', ',', 'ex##cin##fo', ':', 'log', '.', 'info', '(', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "commiter = Signature ( commiter [ 0 ] , commiter [ 1 ] ) \n"
Original    (015): ['commiter', '=', 'Signature', '(', 'commiter', '[', '0', ']', ',', 'commiter', '[', '1', ']', ')', '\\n']
Tokenized   (021): ['[CLS]', 'commit', '##er', '=', 'signature', '(', 'commit', '##er', '[', '0', ']', ',', 'commit', '##er', '[', '1', ']', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['commit', '##er', '=', 'signature', '(', 'commit', '##er', '[', '0', ']', ',', 'commit', '##er', '[', '1', ']', ')', '\\', 'n']
Detokenized (015): ['commit##er', '=', 'signature', '(', 'commit##er', '[', '0', ']', ',', 'commit##er', '[', '1', ']', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "len ( path_components ) == 1 and \n"
Original    (008): ['len', '(', 'path_components', ')', '==', '1', 'and', '\\n']
Tokenized   (014): ['[CLS]', 'len', '(', 'path', '_', 'components', ')', '=', '=', '1', 'and', '\\', 'n', '[SEP]']
Filtered   (012): ['len', '(', 'path', '_', 'components', ')', '=', '=', '1', 'and', '\\', 'n']
Detokenized (008): ['len', '(', 'path_components', ')', '==', '1', 'and', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "entry_name == path_components [ 0 ] ) \n"
Original    (008): ['entry_name', '==', 'path_components', '[', '0', ']', ')', '\\n']
Tokenized   (016): ['[CLS]', 'entry', '_', 'name', '=', '=', 'path', '_', 'components', '[', '0', ']', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['entry', '_', 'name', '=', '=', 'path', '_', 'components', '[', '0', ']', ')', '\\', 'n']
Detokenized (008): ['entry_name', '==', 'path_components', '[', '0', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "lambda entry : self . _repo [ entry . id ] ) \n"
Original    (013): ['lambda', 'entry', ':', 'self', '.', '_repo', '[', 'entry', '.', 'id', ']', ')', '\\n']
Tokenized   (018): ['[CLS]', 'lambda', 'entry', ':', 'self', '.', '_', 'rep', '##o', '[', 'entry', '.', 'id', ']', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['lambda', 'entry', ':', 'self', '.', '_', 'rep', '##o', '[', 'entry', '.', 'id', ']', ')', '\\', 'n']
Detokenized (013): ['lambda', 'entry', ':', 'self', '.', '_rep##o', '[', 'entry', '.', 'id', ']', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "GIT_FILEMODE_LINK : { \n"
Original    (004): ['GIT_FILEMODE_LINK', ':', '{', '\\n']
Tokenized   (014): ['[CLS]', 'gi', '##t', '_', 'file', '##mo', '##de', '_', 'link', ':', '{', '\\', 'n', '[SEP]']
Filtered   (012): ['gi', '##t', '_', 'file', '##mo', '##de', '_', 'link', ':', '{', '\\', 'n']
Detokenized (004): ['gi##t_file##mo##de_link', ':', '{', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "iterators = [ self . _repo . walk ( branch . target , sort ) \n"
Original    (016): ['iterators', '=', '[', 'self', '.', '_repo', '.', 'walk', '(', 'branch', '.', 'target', ',', 'sort', ')', '\\n']
Tokenized   (023): ['[CLS]', 'it', '##era', '##tors', '=', '[', 'self', '.', '_', 'rep', '##o', '.', 'walk', '(', 'branch', '.', 'target', ',', 'sort', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['it', '##era', '##tors', '=', '[', 'self', '.', '_', 'rep', '##o', '.', 'walk', '(', 'branch', '.', 'target', ',', 'sort', ')', '\\', 'n']
Detokenized (016): ['it##era##tors', '=', '[', 'self', '.', '_rep##o', '.', 'walk', '(', 'branch', '.', 'target', ',', 'sort', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "stop_iteration = [ False for branch in branches ] \n"
Original    (010): ['stop_iteration', '=', '[', 'False', 'for', 'branch', 'in', 'branches', ']', '\\n']
Tokenized   (015): ['[CLS]', 'stop', '_', 'iteration', '=', '[', 'false', 'for', 'branch', 'in', 'branches', ']', '\\', 'n', '[SEP]']
Filtered   (013): ['stop', '_', 'iteration', '=', '[', 'false', 'for', 'branch', 'in', 'branches', ']', '\\', 'n']
Detokenized (010): ['stop_iteration', '=', '[', 'false', 'for', 'branch', 'in', 'branches', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "second_commit in first_commits ) : \n"
Original    (006): ['second_commit', 'in', 'first_commits', ')', ':', '\\n']
Tokenized   (013): ['[CLS]', 'second', '_', 'commit', 'in', 'first', '_', 'commits', ')', ':', '\\', 'n', '[SEP]']
Filtered   (011): ['second', '_', 'commit', 'in', 'first', '_', 'commits', ')', ':', '\\', 'n']
Detokenized (006): ['second_commit', 'in', 'first_commits', ')', ':', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "new_commit = Commit ( 2 , 2 , "21111111111" ) \n"
Original    (011): ['new_commit', '=', 'Commit', '(', '2', ',', '2', ',', '"21111111111"', ')', '\\n']
Tokenized   (022): ['[CLS]', 'new', '_', 'commit', '=', 'commit', '(', '2', ',', '2', ',', '"', '211', '##11', '##11', '##11', '##11', '"', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['new', '_', 'commit', '=', 'commit', '(', '2', ',', '2', ',', '"', '211', '##11', '##11', '##11', '##11', '"', ')', '\\', 'n']
Detokenized (011): ['new_commit', '=', 'commit', '(', '2', ',', '2', ',', '"211##11##11##11##11"', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "mocked_repo . walk . assert_called_once_with ( "head" , GIT_SORT_TIME ) \n"
Original    (011): ['mocked_repo', '.', 'walk', '.', 'assert_called_once_with', '(', '"head"', ',', 'GIT_SORT_TIME', ')', '\\n']
Tokenized   (030): ['[CLS]', 'mocked', '_', 'rep', '##o', '.', 'walk', '.', 'assert', '_', 'called', '_', 'once', '_', 'with', '(', '"', 'head', '"', ',', 'gi', '##t', '_', 'sort', '_', 'time', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['mocked', '_', 'rep', '##o', '.', 'walk', '.', 'assert', '_', 'called', '_', 'once', '_', 'with', '(', '"', 'head', '"', ',', 'gi', '##t', '_', 'sort', '_', 'time', ')', '\\', 'n']
Detokenized (011): ['mocked_rep##o', '.', 'walk', '.', 'assert_called_once_with', '(', '"head"', ',', 'gi##t_sort_time', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "to_datetime = True ) == datetime \n"
Original    (007): ['to_datetime', '=', 'True', ')', '==', 'datetime', '\\n']
Tokenized   (015): ['[CLS]', 'to', '_', 'date', '##time', '=', 'true', ')', '=', '=', 'date', '##time', '\\', 'n', '[SEP]']
Filtered   (013): ['to', '_', 'date', '##time', '=', 'true', ')', '=', '=', 'date', '##time', '\\', 'n']
Detokenized (007): ['to_date##time', '=', 'true', ')', '==', 'date##time', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "date = dt . date ( 1970 , 1 , 1 ) \n"
Original    (013): ['date', '=', 'dt', '.', 'date', '(', '1970', ',', '1', ',', '1', ')', '\\n']
Tokenized   (016): ['[CLS]', 'date', '=', 'dt', '.', 'date', '(', '1970', ',', '1', ',', '1', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['date', '=', 'dt', '.', 'date', '(', '1970', ',', '1', ',', '1', ')', '\\', 'n']
Detokenized (013): ['date', '=', 'dt', '.', 'date', '(', '1970', ',', '1', ',', '1', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "datetime = dt . datetime ( 1970 , 1 , 1 , 13 , 30 ) \n"
Original    (017): ['datetime', '=', 'dt', '.', 'datetime', '(', '1970', ',', '1', ',', '1', ',', '13', ',', '30', ')', '\\n']
Tokenized   (022): ['[CLS]', 'date', '##time', '=', 'dt', '.', 'date', '##time', '(', '1970', ',', '1', ',', '1', ',', '13', ',', '30', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['date', '##time', '=', 'dt', '.', 'date', '##time', '(', '1970', ',', '1', ',', '1', ',', '13', ',', '30', ')', '\\', 'n']
Detokenized (017): ['date##time', '=', 'dt', '.', 'date##time', '(', '1970', ',', '1', ',', '1', ',', '13', ',', '30', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "internationalizeDocstring = lambda x : x \n"
Original    (007): ['internationalizeDocstring', '=', 'lambda', 'x', ':', 'x', '\\n']
Tokenized   (014): ['[CLS]', 'international', '##ized', '##oc', '##st', '##ring', '=', 'lambda', 'x', ':', 'x', '\\', 'n', '[SEP]']
Filtered   (012): ['international', '##ized', '##oc', '##st', '##ring', '=', 'lambda', 'x', ':', 'x', '\\', 'n']
Detokenized (007): ['international##ized##oc##st##ring', '=', 'lambda', 'x', ':', 'x', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "conf . supybot . drivers . maxReconnectWait ( ) ) \n"
Original    (011): ['conf', '.', 'supybot', '.', 'drivers', '.', 'maxReconnectWait', '(', ')', ')', '\\n']
Tokenized   (023): ['[CLS]', 'con', '##f', '.', 'su', '##py', '##bot', '.', 'drivers', '.', 'max', '##re', '##con', '##ne', '##ct', '##wai', '##t', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['con', '##f', '.', 'su', '##py', '##bot', '.', 'drivers', '.', 'max', '##re', '##con', '##ne', '##ct', '##wai', '##t', '(', ')', ')', '\\', 'n']
Detokenized (011): ['con##f', '.', 'su##py##bot', '.', 'drivers', '.', 'max##re##con##ne##ct##wai##t', '(', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "inst . conn . _sock . __class__ is socket . _closedsocket ) : \n"
Original    (014): ['inst', '.', 'conn', '.', '_sock', '.', '__class__', 'is', 'socket', '.', '_closedsocket', ')', ':', '\\n']
Tokenized   (028): ['[CLS]', 'ins', '##t', '.', 'con', '##n', '.', '_', 'sock', '.', '_', '_', 'class', '_', '_', 'is', 'socket', '.', '_', 'closed', '##so', '##cke', '##t', ')', ':', '\\', 'n', '[SEP]']
Filtered   (026): ['ins', '##t', '.', 'con', '##n', '.', '_', 'sock', '.', '_', '_', 'class', '_', '_', 'is', 'socket', '.', '_', 'closed', '##so', '##cke', '##t', ')', ':', '\\', 'n']
Detokenized (014): ['ins##t', '.', 'con##n', '.', '_sock', '.', '__class__', 'is', 'socket', '.', '_closed##so##cke##t', ')', ':', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "network_config = getattr ( conf . supybot . networks , self . irc . network ) \n"
Original    (017): ['network_config', '=', 'getattr', '(', 'conf', '.', 'supybot', '.', 'networks', ',', 'self', '.', 'irc', '.', 'network', ')', '\\n']
Tokenized   (030): ['[CLS]', 'network', '_', 'con', '##fi', '##g', '=', 'get', '##att', '##r', '(', 'con', '##f', '.', 'su', '##py', '##bot', '.', 'networks', ',', 'self', '.', 'ir', '##c', '.', 'network', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['network', '_', 'con', '##fi', '##g', '=', 'get', '##att', '##r', '(', 'con', '##f', '.', 'su', '##py', '##bot', '.', 'networks', ',', 'self', '.', 'ir', '##c', '.', 'network', ')', '\\', 'n']
Detokenized (017): ['network_con##fi##g', '=', 'get##att##r', '(', 'con##f', '.', 'su##py##bot', '.', 'networks', ',', 'self', '.', 'ir##c', '.', 'network', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "vhost = conf . supybot . protocols . irc . vhost ( ) , \n"
Original    (015): ['vhost', '=', 'conf', '.', 'supybot', '.', 'protocols', '.', 'irc', '.', 'vhost', '(', ')', ',', '\\n']
Tokenized   (026): ['[CLS]', 'v', '##hos', '##t', '=', 'con', '##f', '.', 'su', '##py', '##bot', '.', 'protocols', '.', 'ir', '##c', '.', 'v', '##hos', '##t', '(', ')', ',', '\\', 'n', '[SEP]']
Filtered   (024): ['v', '##hos', '##t', '=', 'con', '##f', '.', 'su', '##py', '##bot', '.', 'protocols', '.', 'ir', '##c', '.', 'v', '##hos', '##t', '(', ')', ',', '\\', 'n']
Detokenized (015): ['v##hos##t', '=', 'con##f', '.', 'su##py##bot', '.', 'protocols', '.', 'ir##c', '.', 'v##hos##t', '(', ')', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "trusted_fingerprints = network_config . ssl . serverFingerprints ( ) , \n"
Original    (011): ['trusted_fingerprints', '=', 'network_config', '.', 'ssl', '.', 'serverFingerprints', '(', ')', ',', '\\n']
Tokenized   (024): ['[CLS]', 'trusted', '_', 'fingerprints', '=', 'network', '_', 'con', '##fi', '##g', '.', 'ss', '##l', '.', 'server', '##finger', '##print', '##s', '(', ')', ',', '\\', 'n', '[SEP]']
Filtered   (022): ['trusted', '_', 'fingerprints', '=', 'network', '_', 'con', '##fi', '##g', '.', 'ss', '##l', '.', 'server', '##finger', '##print', '##s', '(', ')', ',', '\\', 'n']
Detokenized (011): ['trusted_fingerprints', '=', 'network_con##fi##g', '.', 'ss##l', '.', 'server##finger##print##s', '(', ')', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "while tb : \n"
Original    (004): ['while', 'tb', ':', '\\n']
Tokenized   (007): ['[CLS]', 'while', 'tb', ':', '\\', 'n', '[SEP]']
Filtered   (005): ['while', 'tb', ':', '\\', 'n']
Detokenized (004): ['while', 'tb', ':', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "frame . f_lineno ) ) \n"
Original    (006): ['frame', '.', 'f_lineno', ')', ')', '\\n']
Tokenized   (012): ['[CLS]', 'frame', '.', 'f', '_', 'linen', '##o', ')', ')', '\\', 'n', '[SEP]']
Filtered   (010): ['frame', '.', 'f', '_', 'linen', '##o', ')', ')', '\\', 'n']
Detokenized (006): ['frame', '.', 'f_linen##o', ')', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "window = timedelta ( 0 , 0 , 0 , 0 , settings . SESSION_TIMEOUT ) \n"
Original    (017): ['window', '=', 'timedelta', '(', '0', ',', '0', ',', '0', ',', '0', ',', 'settings', '.', 'SESSION_TIMEOUT', ')', '\\n']
Tokenized   (025): ['[CLS]', 'window', '=', 'timed', '##elt', '##a', '(', '0', ',', '0', ',', '0', ',', '0', ',', 'settings', '.', 'session', '_', 'time', '##out', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['window', '=', 'timed', '##elt', '##a', '(', '0', ',', '0', ',', '0', ',', '0', ',', 'settings', '.', 'session', '_', 'time', '##out', ')', '\\', 'n']
Detokenized (017): ['window', '=', 'timed##elt##a', '(', '0', ',', '0', ',', '0', ',', '0', ',', 'settings', '.', 'session_time##out', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "shared = request . POST . get ( "shared" , False ) \n"
Original    (013): ['shared', '=', 'request', '.', 'POST', '.', 'get', '(', '"shared"', ',', 'False', ')', '\\n']
Tokenized   (018): ['[CLS]', 'shared', '=', 'request', '.', 'post', '.', 'get', '(', '"', 'shared', '"', ',', 'false', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['shared', '=', 'request', '.', 'post', '.', 'get', '(', '"', 'shared', '"', ',', 'false', ')', '\\', 'n']
Detokenized (013): ['shared', '=', 'request', '.', 'post', '.', 'get', '(', '"shared"', ',', 'false', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "} , p . score_functions . all ( ) . filter ( selectable_bodies = plan . legislative_body } , report_displays [ 0 ] . scorepanel_set . all ( ) . order_by ( ) ) \n"
Original    (035): ['}', ',', 'p', '.', 'score_functions', '.', 'all', '(', ')', '.', 'filter', '(', 'selectable_bodies', '=', 'plan', '.', 'legislative_body', '}', ',', 'report_displays', '[', '0', ']', '.', 'scorepanel_set', '.', 'all', '(', ')', '.', 'order_by', '(', ')', ')', '\\n']
Tokenized   (053): ['[CLS]', '}', ',', 'p', '.', 'score', '_', 'functions', '.', 'all', '(', ')', '.', 'filter', '(', 'select', '##able', '_', 'bodies', '=', 'plan', '.', 'legislative', '_', 'body', '}', ',', 'report', '_', 'displays', '[', '0', ']', '.', 'score', '##pan', '##el', '_', 'set', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (051): ['}', ',', 'p', '.', 'score', '_', 'functions', '.', 'all', '(', ')', '.', 'filter', '(', 'select', '##able', '_', 'bodies', '=', 'plan', '.', 'legislative', '_', 'body', '}', ',', 'report', '_', 'displays', '[', '0', ']', '.', 'score', '##pan', '##el', '_', 'set', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ')', ')', '\\', 'n']
Detokenized (035): ['}', ',', 'p', '.', 'score_functions', '.', 'all', '(', ')', '.', 'filter', '(', 'select##able_bodies', '=', 'plan', '.', 'legislative_body', '}', ',', 'report_displays', '[', '0', ']', '.', 'score##pan##el_set', '.', 'all', '(', ')', '.', 'order_by', '(', ')', ')', '\\n']
Counter: 51
===================================================================
Hidden states:  (13, 35, 768)
# Extracted words:  35
Sentence         : "body_member_long_label = _ ( ) + \n"
Original    (007): ['body_member_long_label', '=', '_', '(', ')', '+', '\\n']
Tokenized   (016): ['[CLS]', 'body', '_', 'member', '_', 'long', '_', 'label', '=', '_', '(', ')', '+', '\\', 'n', '[SEP]']
Filtered   (014): ['body', '_', 'member', '_', 'long', '_', 'label', '=', '_', '(', ')', '+', '\\', 'n']
Detokenized (007): ['body_member_long_label', '=', '_', '(', ')', '+', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "body_members = _n ( , , 2 ) \n"
Original    (009): ['body_members', '=', '_n', '(', ',', ',', '2', ')', '\\n']
Tokenized   (015): ['[CLS]', 'body', '_', 'members', '=', '_', 'n', '(', ',', ',', '2', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['body', '_', 'members', '=', '_', 'n', '(', ',', ',', '2', ')', '\\', 'n']
Detokenized (009): ['body_members', '=', '_n', '(', ',', ',', '2', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "has_regions = Region . objects . all ( ) . count ( ) > 1 \n"
Original    (016): ['has_regions', '=', 'Region', '.', 'objects', '.', 'all', '(', ')', '.', 'count', '(', ')', '>', '1', '\\n']
Tokenized   (021): ['[CLS]', 'has', '_', 'regions', '=', 'region', '.', 'objects', '.', 'all', '(', ')', '.', 'count', '(', ')', '>', '1', '\\', 'n', '[SEP]']
Filtered   (019): ['has', '_', 'regions', '=', 'region', '.', 'objects', '.', 'all', '(', ')', '.', 'count', '(', ')', '>', '1', '\\', 'n']
Detokenized (016): ['has_regions', '=', 'region', '.', 'objects', '.', 'all', '(', ')', '.', 'count', '(', ')', '>', '1', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "bodies = LegislativeBody . objects . all ( ) . order_by ( , ) \n"
Original    (015): ['bodies', '=', 'LegislativeBody', '.', 'objects', '.', 'all', '(', ')', '.', 'order_by', '(', ',', ')', '\\n']
Tokenized   (021): ['[CLS]', 'bodies', '=', 'legislative', '##body', '.', 'objects', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ',', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['bodies', '=', 'legislative', '##body', '.', 'objects', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ',', ')', '\\', 'n']
Detokenized (015): ['bodies', '=', 'legislative##body', '.', 'objects', '.', 'all', '(', ')', '.', 'order_by', '(', ',', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "l_bodies = [ b for b in bodies if b in [ sd . legislative_body for sd in ScoreDisplay . objects . filter \n"
Original    (024): ['l_bodies', '=', '[', 'b', 'for', 'b', 'in', 'bodies', 'if', 'b', 'in', '[', 'sd', '.', 'legislative_body', 'for', 'sd', 'in', 'ScoreDisplay', '.', 'objects', '.', 'filter', '\\n']
Tokenized   (033): ['[CLS]', 'l', '_', 'bodies', '=', '[', 'b', 'for', 'b', 'in', 'bodies', 'if', 'b', 'in', '[', 'sd', '.', 'legislative', '_', 'body', 'for', 'sd', 'in', 'scored', '##is', '##play', '.', 'objects', '.', 'filter', '\\', 'n', '[SEP]']
Filtered   (031): ['l', '_', 'bodies', '=', '[', 'b', 'for', 'b', 'in', 'bodies', 'if', 'b', 'in', '[', 'sd', '.', 'legislative', '_', 'body', 'for', 'sd', 'in', 'scored', '##is', '##play', '.', 'objects', '.', 'filter', '\\', 'n']
Detokenized (024): ['l_bodies', '=', '[', 'b', 'for', 'b', 'in', 'bodies', 'if', 'b', 'in', '[', 'sd', '.', 'legislative_body', 'for', 'sd', 'in', 'scored##is##play', '.', 'objects', '.', 'filter', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "cfg [ ] = datetime . now ( ) \n"
Original    (010): ['cfg', '[', ']', '=', 'datetime', '.', 'now', '(', ')', '\\n']
Tokenized   (015): ['[CLS]', 'cf', '##g', '[', ']', '=', 'date', '##time', '.', 'now', '(', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['cf', '##g', '[', ']', '=', 'date', '##time', '.', 'now', '(', ')', '\\', 'n']
Detokenized (010): ['cf##g', '[', ']', '=', 'date##time', '.', 'now', '(', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "ll = ModestMaps . Geo . Location ( pt1 . y , pt1 . x ) \n"
Original    (017): ['ll', '=', 'ModestMaps', '.', 'Geo', '.', 'Location', '(', 'pt1', '.', 'y', ',', 'pt1', '.', 'x', ')', '\\n']
Tokenized   (024): ['[CLS]', 'll', '=', 'modest', '##ma', '##ps', '.', 'geo', '.', 'location', '(', 'pt', '##1', '.', 'y', ',', 'pt', '##1', '.', 'x', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['ll', '=', 'modest', '##ma', '##ps', '.', 'geo', '.', 'location', '(', 'pt', '##1', '.', 'y', ',', 'pt', '##1', '.', 'x', ')', '\\', 'n']
Detokenized (017): ['ll', '=', 'modest##ma##ps', '.', 'geo', '.', 'location', '(', 'pt##1', '.', 'y', ',', 'pt##1', '.', 'x', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "provider = ModestMaps . WMS . Provider ( cfg [ ] , { \n"
Original    (014): ['provider', '=', 'ModestMaps', '.', 'WMS', '.', 'Provider', '(', 'cfg', '[', ']', ',', '{', '\\n']
Tokenized   (021): ['[CLS]', 'provider', '=', 'modest', '##ma', '##ps', '.', 'w', '##ms', '.', 'provider', '(', 'cf', '##g', '[', ']', ',', '{', '\\', 'n', '[SEP]']
Filtered   (019): ['provider', '=', 'modest', '##ma', '##ps', '.', 'w', '##ms', '.', 'provider', '(', 'cf', '##g', '[', ']', ',', '{', '\\', 'n']
Detokenized (014): ['provider', '=', 'modest##ma##ps', '.', 'w##ms', '.', 'provider', '(', 'cf##g', '[', ']', ',', '{', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "overlayImg = Image . blend ( overlayImg , ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) , \n"
Original    (026): ['overlayImg', '=', 'Image', '.', 'blend', '(', 'overlayImg', ',', 'ModestMaps', '.', 'mapByExtent', '(', 'provider', ',', 'll', ',', 'ur', ',', 'dims', ')', '.', 'draw', '(', ')', ',', '\\n']
Tokenized   (042): ['[CLS]', 'over', '##lay', '##im', '##g', '=', 'image', '.', 'blend', '(', 'over', '##lay', '##im', '##g', ',', 'modest', '##ma', '##ps', '.', 'map', '##by', '##ex', '##ten', '##t', '(', 'provider', ',', 'll', ',', 'ur', ',', 'dim', '##s', ')', '.', 'draw', '(', ')', ',', '\\', 'n', '[SEP]']
Filtered   (040): ['over', '##lay', '##im', '##g', '=', 'image', '.', 'blend', '(', 'over', '##lay', '##im', '##g', ',', 'modest', '##ma', '##ps', '.', 'map', '##by', '##ex', '##ten', '##t', '(', 'provider', ',', 'll', ',', 'ur', ',', 'dim', '##s', ')', '.', 'draw', '(', ')', ',', '\\', 'n']
Detokenized (026): ['over##lay##im##g', '=', 'image', '.', 'blend', '(', 'over##lay##im##g', ',', 'modest##ma##ps', '.', 'map##by##ex##ten##t', '(', 'provider', ',', 'll', ',', 'ur', ',', 'dim##s', ')', '.', 'draw', '(', ')', ',', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "fullImg . save ( settings . WEB_TEMP + ( % sha . hexdigest ( ) ) , , quality = 100 ) \n"
Original    (023): ['fullImg', '.', 'save', '(', 'settings', '.', 'WEB_TEMP', '+', '(', '%', 'sha', '.', 'hexdigest', '(', ')', ')', ',', ',', 'quality', '=', '100', ')', '\\n']
Tokenized   (035): ['[CLS]', 'full', '##im', '##g', '.', 'save', '(', 'settings', '.', 'web', '_', 'te', '##mp', '+', '(', '%', 'sha', '.', 'he', '##x', '##di', '##ges', '##t', '(', ')', ')', ',', ',', 'quality', '=', '100', ')', '\\', 'n', '[SEP]']
Filtered   (033): ['full', '##im', '##g', '.', 'save', '(', 'settings', '.', 'web', '_', 'te', '##mp', '+', '(', '%', 'sha', '.', 'he', '##x', '##di', '##ges', '##t', '(', ')', ')', ',', ',', 'quality', '=', '100', ')', '\\', 'n']
Detokenized (023): ['full##im##g', '.', 'save', '(', 'settings', '.', 'web_te##mp', '+', '(', '%', 'sha', '.', 'he##x##di##ges##t', '(', ')', ')', ',', ',', 'quality', '=', '100', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "CreatePDF ( page , result , show_error_as_pdf = True ) \n"
Original    (011): ['CreatePDF', '(', 'page', ',', 'result', ',', 'show_error_as_pdf', '=', 'True', ')', '\\n']
Tokenized   (022): ['[CLS]', 'create', '##pd', '##f', '(', 'page', ',', 'result', ',', 'show', '_', 'error', '_', 'as', '_', 'pdf', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['create', '##pd', '##f', '(', 'page', ',', 'result', ',', 'show', '_', 'error', '_', 'as', '_', 'pdf', '=', 'true', ')', '\\', 'n']
Detokenized (011): ['create##pd##f', '(', 'page', ',', 'result', ',', 'show_error_as_pdf', '=', 'true', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "body = LegislativeBody . objects . get ( id = int ( request . POST [ ] ) ) \n"
Original    (020): ['body', '=', 'LegislativeBody', '.', 'objects', '.', 'get', '(', 'id', '=', 'int', '(', 'request', '.', 'POST', '[', ']', ')', ')', '\\n']
Tokenized   (024): ['[CLS]', 'body', '=', 'legislative', '##body', '.', 'objects', '.', 'get', '(', 'id', '=', 'int', '(', 'request', '.', 'post', '[', ']', ')', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['body', '=', 'legislative', '##body', '.', 'objects', '.', 'get', '(', 'id', '=', 'int', '(', 'request', '.', 'post', '[', ']', ')', ')', '\\', 'n']
Detokenized (020): ['body', '=', 'legislative##body', '.', 'objects', '.', 'get', '(', 'id', '=', 'int', '(', 'request', '.', 'post', '[', ']', ')', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "PlanReport . createreport . delay ( planid , stamp , req , language = translation . get_language ( ) ) \n"
Original    (021): ['PlanReport', '.', 'createreport', '.', 'delay', '(', 'planid', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get_language', '(', ')', ')', '\\n']
Tokenized   (032): ['[CLS]', 'plan', '##re', '##port', '.', 'create', '##re', '##port', '.', 'delay', '(', 'plan', '##id', ',', 'stamp', ',', 're', '##q', ',', 'language', '=', 'translation', '.', 'get', '_', 'language', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (030): ['plan', '##re', '##port', '.', 'create', '##re', '##port', '.', 'delay', '(', 'plan', '##id', ',', 'stamp', ',', 're', '##q', ',', 'language', '=', 'translation', '.', 'get', '_', 'language', '(', ')', ')', '\\', 'n']
Detokenized (021): ['plan##re##port', '.', 'create##re##port', '.', 'delay', '(', 'plan##id', ',', 'stamp', ',', 're##q', ',', 'language', '=', 'translation', '.', 'get_language', '(', ')', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "stamp = request . POST . get ( , sha . hexdigest ( ) ) \n"
Original    (016): ['stamp', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'sha', '.', 'hexdigest', '(', ')', ')', '\\n']
Tokenized   (023): ['[CLS]', 'stamp', '=', 'request', '.', 'post', '.', 'get', '(', ',', 'sha', '.', 'he', '##x', '##di', '##ges', '##t', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['stamp', '=', 'request', '.', 'post', '.', 'get', '(', ',', 'sha', '.', 'he', '##x', '##di', '##ges', '##t', '(', ')', ')', '\\', 'n']
Detokenized (016): ['stamp', '=', 'request', '.', 'post', '.', 'get', '(', ',', 'sha', '.', 'he##x##di##ges##t', '(', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "CalculatorReport . createcalculatorreport . delay ( planid , stamp , req , language = translation . get_language ~~ else : \n"
Original    (021): ['CalculatorReport', '.', 'createcalculatorreport', '.', 'delay', '(', 'planid', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get_language', '~~', 'else', ':', '\\n']
Tokenized   (038): ['[CLS]', 'cal', '##cula', '##tor', '##re', '##port', '.', 'create', '##cal', '##cula', '##tor', '##re', '##port', '.', 'delay', '(', 'plan', '##id', ',', 'stamp', ',', 're', '##q', ',', 'language', '=', 'translation', '.', 'get', '_', 'language', '~', '~', 'else', ':', '\\', 'n', '[SEP]']
Filtered   (036): ['cal', '##cula', '##tor', '##re', '##port', '.', 'create', '##cal', '##cula', '##tor', '##re', '##port', '.', 'delay', '(', 'plan', '##id', ',', 'stamp', ',', 're', '##q', ',', 'language', '=', 'translation', '.', 'get', '_', 'language', '~', '~', 'else', ':', '\\', 'n']
Detokenized (021): ['cal##cula##tor##re##port', '.', 'create##cal##cula##tor##re##port', '.', 'delay', '(', 'plan##id', ',', 'stamp', ',', 're##q', ',', 'language', '=', 'translation', '.', 'get_language', '~~', 'else', ':', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "site_id = Site . objects . get_current ( ) . id , \n"
Original    (013): ['site_id', '=', 'Site', '.', 'objects', '.', 'get_current', '(', ')', '.', 'id', ',', '\\n']
Tokenized   (020): ['[CLS]', 'site', '_', 'id', '=', 'site', '.', 'objects', '.', 'get', '_', 'current', '(', ')', '.', 'id', ',', '\\', 'n', '[SEP]']
Filtered   (018): ['site', '_', 'id', '=', 'site', '.', 'objects', '.', 'get', '_', 'current', '(', ')', '.', 'id', ',', '\\', 'n']
Detokenized (013): ['site_id', '=', 'site', '.', 'objects', '.', 'get_current', '(', ')', '.', 'id', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "from_id = int ( request . POST . get ( , - 1 ) ) \n"
Original    (016): ['from_id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '-', '1', ')', ')', '\\n']
Tokenized   (021): ['[CLS]', 'from', '_', 'id', '=', 'int', '(', 'request', '.', 'post', '.', 'get', '(', ',', '-', '1', ')', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['from', '_', 'id', '=', 'int', '(', 'request', '.', 'post', '.', 'get', '(', ',', '-', '1', ')', ')', '\\', 'n']
Detokenized (016): ['from_id', '=', 'int', '(', 'request', '.', 'post', '.', 'get', '(', ',', '-', '1', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "to_id = int ( request . POST . get ( , None ) ) \n"
Original    (015): ['to_id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', 'None', ')', ')', '\\n']
Tokenized   (020): ['[CLS]', 'to', '_', 'id', '=', 'int', '(', 'request', '.', 'post', '.', 'get', '(', ',', 'none', ')', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['to', '_', 'id', '=', 'int', '(', 'request', '.', 'post', '.', 'get', '(', ',', 'none', ')', ')', '\\', 'n']
Detokenized (015): ['to_id', '=', 'int', '(', 'request', '.', 'post', '.', 'get', '(', ',', 'none', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "from_districts = filter ( lambda d : True if d . district_id == from_id else False , all_districts to_district = filter ( lambda d : True if d . district_id == to_id else False , all_districts ) [ 0 ] \n"
Original    (041): ['from_districts', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district_id', '==', 'from_id', 'else', 'False', ',', 'all_districts', 'to_district', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district_id', '==', 'to_id', 'else', 'False', ',', 'all_districts', ')', '[', '0', ']', '\\n']
Tokenized   (062): ['[CLS]', 'from', '_', 'districts', '=', 'filter', '(', 'lambda', 'd', ':', 'true', 'if', 'd', '.', 'district', '_', 'id', '=', '=', 'from', '_', 'id', 'else', 'false', ',', 'all', '_', 'districts', 'to', '_', 'district', '=', 'filter', '(', 'lambda', 'd', ':', 'true', 'if', 'd', '.', 'district', '_', 'id', '=', '=', 'to', '_', 'id', 'else', 'false', ',', 'all', '_', 'districts', ')', '[', '0', ']', '\\', 'n', '[SEP]']
Filtered   (060): ['from', '_', 'districts', '=', 'filter', '(', 'lambda', 'd', ':', 'true', 'if', 'd', '.', 'district', '_', 'id', '=', '=', 'from', '_', 'id', 'else', 'false', ',', 'all', '_', 'districts', 'to', '_', 'district', '=', 'filter', '(', 'lambda', 'd', ':', 'true', 'if', 'd', '.', 'district', '_', 'id', '=', '=', 'to', '_', 'id', 'else', 'false', ',', 'all', '_', 'districts', ')', '[', '0', ']', '\\', 'n']
Detokenized (041): ['from_districts', '=', 'filter', '(', 'lambda', 'd', ':', 'true', 'if', 'd', '.', 'district_id', '==', 'from_id', 'else', 'false', ',', 'all_districts', 'to_district', '=', 'filter', '(', 'lambda', 'd', ':', 'true', 'if', 'd', '.', 'district_id', '==', 'to_id', 'else', 'false', ',', 'all_districts', ')', '[', '0', ']', '\\n']
Counter: 60
===================================================================
Hidden states:  (13, 41, 768)
# Extracted words:  41
Sentence         : "inverse = request . REQUEST [ ] == if in request . REQUEST else False \n"
Original    (016): ['inverse', '=', 'request', '.', 'REQUEST', '[', ']', '==', 'if', 'in', 'request', '.', 'REQUEST', 'else', 'False', '\\n']
Tokenized   (020): ['[CLS]', 'inverse', '=', 'request', '.', 'request', '[', ']', '=', '=', 'if', 'in', 'request', '.', 'request', 'else', 'false', '\\', 'n', '[SEP]']
Filtered   (018): ['inverse', '=', 'request', '.', 'request', '[', ']', '=', '=', 'if', 'in', 'request', '.', 'request', 'else', 'false', '\\', 'n']
Detokenized (016): ['inverse', '=', 'request', '.', 'request', '[', ']', '==', 'if', 'in', 'request', '.', 'request', 'else', 'false', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "my_context . update ( plan . compute_splits ( layer , version = version , inverse = inverse , extended last_item = layer is layers [ - 1 ] \n"
Original    (029): ['my_context', '.', 'update', '(', 'plan', '.', 'compute_splits', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', ',', 'extended', 'last_item', '=', 'layer', 'is', 'layers', '[', '-', '1', ']', '\\n']
Tokenized   (038): ['[CLS]', 'my', '_', 'context', '.', 'update', '(', 'plan', '.', 'compute', '_', 'splits', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', ',', 'extended', 'last', '_', 'item', '=', 'layer', 'is', 'layers', '[', '-', '1', ']', '\\', 'n', '[SEP]']
Filtered   (036): ['my', '_', 'context', '.', 'update', '(', 'plan', '.', 'compute', '_', 'splits', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', ',', 'extended', 'last', '_', 'item', '=', 'layer', 'is', 'layers', '[', '-', '1', ']', '\\', 'n']
Detokenized (029): ['my_context', '.', 'update', '(', 'plan', '.', 'compute_splits', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', ',', 'extended', 'last_item', '=', 'layer', 'is', 'layers', '[', '-', '1', ']', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "community_info = plan . get_community_type_info ( layer , version = version , inverse = inverse if community_info is not None : \n"
Original    (022): ['community_info', '=', 'plan', '.', 'get_community_type_info', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', 'if', 'community_info', 'is', 'not', 'None', ':', '\\n']
Tokenized   (035): ['[CLS]', 'community', '_', 'info', '=', 'plan', '.', 'get', '_', 'community', '_', 'type', '_', 'info', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', 'if', 'community', '_', 'info', 'is', 'not', 'none', ':', '\\', 'n', '[SEP]']
Filtered   (033): ['community', '_', 'info', '=', 'plan', '.', 'get', '_', 'community', '_', 'type', '_', 'info', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', 'if', 'community', '_', 'info', 'is', 'not', 'none', ':', '\\', 'n']
Detokenized (022): ['community_info', '=', 'plan', '.', 'get_community_type_info', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', 'if', 'community_info', 'is', 'not', 'none', ':', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "html += report . render ( calc_context ) \n"
Original    (009): ['html', '+=', 'report', '.', 'render', '(', 'calc_context', ')', '\\n']
Tokenized   (016): ['[CLS]', 'html', '+', '=', 'report', '.', 'render', '(', 'cal', '##c', '_', 'context', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['html', '+', '=', 'report', '.', 'render', '(', 'cal', '##c', '_', 'context', ')', '\\', 'n']
Detokenized (009): ['html', '+=', 'report', '.', 'render', '(', 'cal##c_context', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "geounit_ids = string . split ( request . REQUEST [ "geounits" ] , "|" ) \n"
Original    (016): ['geounit_ids', '=', 'string', '.', 'split', '(', 'request', '.', 'REQUEST', '[', '"geounits"', ']', ',', '"|"', ')', '\\n']
Tokenized   (030): ['[CLS]', 'geo', '##uni', '##t', '_', 'id', '##s', '=', 'string', '.', 'split', '(', 'request', '.', 'request', '[', '"', 'geo', '##uni', '##ts', '"', ']', ',', '"', '|', '"', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['geo', '##uni', '##t', '_', 'id', '##s', '=', 'string', '.', 'split', '(', 'request', '.', 'request', '[', '"', 'geo', '##uni', '##ts', '"', ']', ',', '"', '|', '"', ')', '\\', 'n']
Detokenized (016): ['geo##uni##t_id##s', '=', 'string', '.', 'split', '(', 'request', '.', 'request', '[', '"geo##uni##ts"', ']', ',', '"|"', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "max_version = max ( [ d . version for d in districts ] ) \n"
Original    (015): ['max_version', '=', 'max', '(', '[', 'd', '.', 'version', 'for', 'd', 'in', 'districts', ']', ')', '\\n']
Tokenized   (020): ['[CLS]', 'max', '_', 'version', '=', 'max', '(', '[', 'd', '.', 'version', 'for', 'd', 'in', 'districts', ']', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['max', '_', 'version', '=', 'max', '(', '[', 'd', '.', 'version', 'for', 'd', 'in', 'districts', ']', ')', '\\', 'n']
Detokenized (015): ['max_version', '=', 'max', '(', '[', 'd', '.', 'version', 'for', 'd', 'in', 'districts', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "can_undo = max_version > plan . min_version \n"
Original    (008): ['can_undo', '=', 'max_version', '>', 'plan', '.', 'min_version', '\\n']
Tokenized   (017): ['[CLS]', 'can', '_', 'undo', '=', 'max', '_', 'version', '>', 'plan', '.', 'min', '_', 'version', '\\', 'n', '[SEP]']
Filtered   (015): ['can', '_', 'undo', '=', 'max', '_', 'version', '>', 'plan', '.', 'min', '_', 'version', '\\', 'n']
Detokenized (008): ['can_undo', '=', 'max_version', '>', 'plan', '.', 'min_version', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "bbox = tuple ( map ( lambda x : float ( x ) , bbox . split ( ) ) ) \n"
Original    (022): ['bbox', '=', 'tuple', '(', 'map', '(', 'lambda', 'x', ':', 'float', '(', 'x', ')', ',', 'bbox', '.', 'split', '(', ')', ')', ')', '\\n']
Tokenized   (028): ['[CLS]', 'bb', '##ox', '=', 'tu', '##ple', '(', 'map', '(', 'lambda', 'x', ':', 'float', '(', 'x', ')', ',', 'bb', '##ox', '.', 'split', '(', ')', ')', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['bb', '##ox', '=', 'tu', '##ple', '(', 'map', '(', 'lambda', 'x', ':', 'float', '(', 'x', ')', ',', 'bb', '##ox', '.', 'split', '(', ')', ')', ')', '\\', 'n']
Detokenized (022): ['bb##ox', '=', 'tu##ple', '(', 'map', '(', 'lambda', 'x', ':', 'float', '(', 'x', ')', ',', 'bb##ox', '.', 'split', '(', ')', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "wkt = wkt . replace ( , ) . replace ( , ) \n"
Original    (014): ['wkt', '=', 'wkt', '.', 'replace', '(', ',', ')', '.', 'replace', '(', ',', ')', '\\n']
Tokenized   (019): ['[CLS]', 'w', '##kt', '=', 'w', '##kt', '.', 'replace', '(', ',', ')', '.', 'replace', '(', ',', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['w', '##kt', '=', 'w', '##kt', '.', 'replace', '(', ',', ')', '.', 'replace', '(', ',', ')', '\\', 'n']
Detokenized (014): ['w##kt', '=', 'w##kt', '.', 'replace', '(', ',', ')', '.', 'replace', '(', ',', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "districts = [ d . id for d in plan . get_districts_at_version ( version , include_geom = True ) if locked = District . objects . filter ( id__in = districts ) . collect ( ) \n"
Original    (037): ['districts', '=', '[', 'd', '.', 'id', 'for', 'd', 'in', 'plan', '.', 'get_districts_at_version', '(', 'version', ',', 'include_geom', '=', 'True', ')', 'if', 'locked', '=', 'District', '.', 'objects', '.', 'filter', '(', 'id__in', '=', 'districts', ')', '.', 'collect', '(', ')', '\\n']
Tokenized   (052): ['[CLS]', 'districts', '=', '[', 'd', '.', 'id', 'for', 'd', 'in', 'plan', '.', 'get', '_', 'districts', '_', 'at', '_', 'version', '(', 'version', ',', 'include', '_', 'geo', '##m', '=', 'true', ')', 'if', 'locked', '=', 'district', '.', 'objects', '.', 'filter', '(', 'id', '_', '_', 'in', '=', 'districts', ')', '.', 'collect', '(', ')', '\\', 'n', '[SEP]']
Filtered   (050): ['districts', '=', '[', 'd', '.', 'id', 'for', 'd', 'in', 'plan', '.', 'get', '_', 'districts', '_', 'at', '_', 'version', '(', 'version', ',', 'include', '_', 'geo', '##m', '=', 'true', ')', 'if', 'locked', '=', 'district', '.', 'objects', '.', 'filter', '(', 'id', '_', '_', 'in', '=', 'districts', ')', '.', 'collect', '(', ')', '\\', 'n']
Detokenized (037): ['districts', '=', '[', 'd', '.', 'id', 'for', 'd', 'in', 'plan', '.', 'get_districts_at_version', '(', 'version', ',', 'include_geo##m', '=', 'true', ')', 'if', 'locked', '=', 'district', '.', 'objects', '.', 'filter', '(', 'id__in', '=', 'districts', ')', '.', 'collect', '(', ')', '\\n']
Counter: 50
===================================================================
Hidden states:  (13, 37, 768)
# Extracted words:  37
Sentence         : "locked_buffered = locked . simplify ( 100 , True ) . buffer ( 100 ) if locked else None \n"
Original    (020): ['locked_buffered', '=', 'locked', '.', 'simplify', '(', '100', ',', 'True', ')', '.', 'buffer', '(', '100', ')', 'if', 'locked', 'else', 'None', '\\n']
Tokenized   (027): ['[CLS]', 'locked', '_', 'buffer', '##ed', '=', 'locked', '.', 'sim', '##plify', '(', '100', ',', 'true', ')', '.', 'buffer', '(', '100', ')', 'if', 'locked', 'else', 'none', '\\', 'n', '[SEP]']
Filtered   (025): ['locked', '_', 'buffer', '##ed', '=', 'locked', '.', 'sim', '##plify', '(', '100', ',', 'true', ')', '.', 'buffer', '(', '100', ')', 'if', 'locked', 'else', 'none', '\\', 'n']
Detokenized (020): ['locked_buffer##ed', '=', 'locked', '.', 'sim##plify', '(', '100', ',', 'true', ')', '.', 'buffer', '(', '100', ')', 'if', 'locked', 'else', 'none', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "filtered = Geolevel . objects . get ( id = geolevel ) . geounit_set . filter ( selection ) \n"
Original    (020): ['filtered', '=', 'Geolevel', '.', 'objects', '.', 'get', '(', 'id', '=', 'geolevel', ')', '.', 'geounit_set', '.', 'filter', '(', 'selection', ')', '\\n']
Tokenized   (031): ['[CLS]', 'filtered', '=', 'geo', '##lev', '##el', '.', 'objects', '.', 'get', '(', 'id', '=', 'geo', '##lev', '##el', ')', '.', 'geo', '##uni', '##t', '_', 'set', '.', 'filter', '(', 'selection', ')', '\\', 'n', '[SEP]']
Filtered   (029): ['filtered', '=', 'geo', '##lev', '##el', '.', 'objects', '.', 'get', '(', 'id', '=', 'geo', '##lev', '##el', ')', '.', 'geo', '##uni', '##t', '_', 'set', '.', 'filter', '(', 'selection', ')', '\\', 'n']
Detokenized (020): ['filtered', '=', 'geo##lev##el', '.', 'objects', '.', 'get', '(', 'id', '=', 'geo##lev##el', ')', '.', 'geo##uni##t_set', '.', 'filter', '(', 'selection', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "pfilter = Q ( legislative_body = leg_body ) & Q ( is_valid = True ) \n"
Original    (016): ['pfilter', '=', 'Q', '(', 'legislative_body', '=', 'leg_body', ')', '&', 'Q', '(', 'is_valid', '=', 'True', ')', '\\n']
Tokenized   (027): ['[CLS]', 'p', '##fi', '##lter', '=', 'q', '(', 'legislative', '_', 'body', '=', 'leg', '_', 'body', ')', '&', 'q', '(', 'is', '_', 'valid', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['p', '##fi', '##lter', '=', 'q', '(', 'legislative', '_', 'body', '=', 'leg', '_', 'body', ')', '&', 'q', '(', 'is', '_', 'valid', '=', 'true', ')', '\\', 'n']
Detokenized (016): ['p##fi##lter', '=', 'q', '(', 'legislative_body', '=', 'leg_body', ')', '&', 'q', '(', 'is_valid', '=', 'true', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "panels = display . scorepanel_set . all ( ) . order_by ( ) \n"
Original    (014): ['panels', '=', 'display', '.', 'scorepanel_set', '.', 'all', '(', ')', '.', 'order_by', '(', ')', '\\n']
Tokenized   (023): ['[CLS]', 'panels', '=', 'display', '.', 'score', '##pan', '##el', '_', 'set', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['panels', '=', 'display', '.', 'score', '##pan', '##el', '_', 'set', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ')', '\\', 'n']
Detokenized (014): ['panels', '=', 'display', '.', 'score##pan##el_set', '.', 'all', '(', ')', '.', 'order_by', '(', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "writer . writerow ( [ , , ] + [ p . __unicode__ ( ) for p in panels ] ) \n"
Original    (022): ['writer', '.', 'writerow', '(', '[', ',', ',', ']', '+', '[', 'p', '.', '__unicode__', '(', ')', 'for', 'p', 'in', 'panels', ']', ')', '\\n']
Tokenized   (030): ['[CLS]', 'writer', '.', 'writer', '##ow', '(', '[', ',', ',', ']', '+', '[', 'p', '.', '_', '_', 'unicode', '_', '_', '(', ')', 'for', 'p', 'in', 'panels', ']', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['writer', '.', 'writer', '##ow', '(', '[', ',', ',', ']', '+', '[', 'p', '.', '_', '_', 'unicode', '_', '_', '(', ')', 'for', 'p', 'in', 'panels', ']', ')', '\\', 'n']
Detokenized (022): ['writer', '.', 'writer##ow', '(', '[', ',', ',', ']', '+', '[', 'p', '.', '__unicode__', '(', ')', 'for', 'p', 'in', 'panels', ']', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "rows = int ( request . POST . get ( , 10 ) ) \n"
Original    (015): ['rows', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '10', ')', ')', '\\n']
Tokenized   (018): ['[CLS]', 'rows', '=', 'int', '(', 'request', '.', 'post', '.', 'get', '(', ',', '10', ')', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['rows', '=', 'int', '(', 'request', '.', 'post', '.', 'get', '(', ',', '10', ')', ')', '\\', 'n']
Detokenized (015): ['rows', '=', 'int', '(', 'request', '.', 'post', '.', 'get', '(', ',', '10', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "sidx = request . POST . get ( , ) \n"
Original    (011): ['sidx', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', '\\n']
Tokenized   (015): ['[CLS]', 'sid', '##x', '=', 'request', '.', 'post', '.', 'get', '(', ',', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['sid', '##x', '=', 'request', '.', 'post', '.', 'get', '(', ',', ')', '\\', 'n']
Detokenized (011): ['sid##x', '=', 'request', '.', 'post', '.', 'get', '(', ',', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "owner_filter = request . POST . get ( ) ; \n"
Original    (011): ['owner_filter', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ';', '\\n']
Tokenized   (016): ['[CLS]', 'owner', '_', 'filter', '=', 'request', '.', 'post', '.', 'get', '(', ')', ';', '\\', 'n', '[SEP]']
Filtered   (014): ['owner', '_', 'filter', '=', 'request', '.', 'post', '.', 'get', '(', ')', ';', '\\', 'n']
Detokenized (011): ['owner_filter', '=', 'request', '.', 'post', '.', 'get', '(', ')', ';', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "body_pk = int ( body_pk ) if body_pk else body_pk ; \n"
Original    (012): ['body_pk', '=', 'int', '(', 'body_pk', ')', 'if', 'body_pk', 'else', 'body_pk', ';', '\\n']
Tokenized   (027): ['[CLS]', 'body', '_', 'p', '##k', '=', 'int', '(', 'body', '_', 'p', '##k', ')', 'if', 'body', '_', 'p', '##k', 'else', 'body', '_', 'p', '##k', ';', '\\', 'n', '[SEP]']
Filtered   (025): ['body', '_', 'p', '##k', '=', 'int', '(', 'body', '_', 'p', '##k', ')', 'if', 'body', '_', 'p', '##k', 'else', 'body', '_', 'p', '##k', ';', '\\', 'n']
Detokenized (012): ['body_p##k', '=', 'int', '(', 'body_p##k', ')', 'if', 'body_p##k', 'else', 'body_p##k', ';', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "search = request . POST . get ( , False ) ; \n"
Original    (013): ['search', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', ';', '\\n']
Tokenized   (016): ['[CLS]', 'search', '=', 'request', '.', 'post', '.', 'get', '(', ',', 'false', ')', ';', '\\', 'n', '[SEP]']
Filtered   (014): ['search', '=', 'request', '.', 'post', '.', 'get', '(', ',', 'false', ')', ';', '\\', 'n']
Detokenized (013): ['search', '=', 'request', '.', 'post', '.', 'get', '(', ',', 'false', ')', ';', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "search_string = request . POST . get ( , ) ; \n"
Original    (012): ['search_string', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', ';', '\\n']
Tokenized   (017): ['[CLS]', 'search', '_', 'string', '=', 'request', '.', 'post', '.', 'get', '(', ',', ')', ';', '\\', 'n', '[SEP]']
Filtered   (015): ['search', '_', 'string', '=', 'request', '.', 'post', '.', 'get', '(', ',', ')', ';', '\\', 'n']
Detokenized (012): ['search_string', '=', 'request', '.', 'post', '.', 'get', '(', ',', ')', ';', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "is_community = request . POST . get ( , False ) == ; \n"
Original    (014): ['is_community', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', '==', ';', '\\n']
Tokenized   (020): ['[CLS]', 'is', '_', 'community', '=', 'request', '.', 'post', '.', 'get', '(', ',', 'false', ')', '=', '=', ';', '\\', 'n', '[SEP]']
Filtered   (018): ['is', '_', 'community', '=', 'request', '.', 'post', '.', 'get', '(', ',', 'false', ')', '=', '=', ';', '\\', 'n']
Detokenized (014): ['is_community', '=', 'request', '.', 'post', '.', 'get', '(', ',', 'false', ')', '==', ';', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "all_plans = Plan . objects . filter ( available , not_creating , search_filter , community_filter ) . order_by \n"
Original    (019): ['all_plans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'available', ',', 'not_creating', ',', 'search_filter', ',', 'community_filter', ')', '.', 'order_by', '\\n']
Tokenized   (032): ['[CLS]', 'all', '_', 'plans', '=', 'plan', '.', 'objects', '.', 'filter', '(', 'available', ',', 'not', '_', 'creating', ',', 'search', '_', 'filter', ',', 'community', '_', 'filter', ')', '.', 'order', '_', 'by', '\\', 'n', '[SEP]']
Filtered   (030): ['all', '_', 'plans', '=', 'plan', '.', 'objects', '.', 'filter', '(', 'available', ',', 'not', '_', 'creating', ',', 'search', '_', 'filter', ',', 'community', '_', 'filter', ')', '.', 'order', '_', 'by', '\\', 'n']
Detokenized (019): ['all_plans', '=', 'plan', '.', 'objects', '.', 'filter', '(', 'available', ',', 'not_creating', ',', 'search_filter', ',', 'community_filter', ')', '.', 'order_by', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "all_districts = ( ) \n"
Original    (005): ['all_districts', '=', '(', ')', '\\n']
Tokenized   (010): ['[CLS]', 'all', '_', 'districts', '=', '(', ')', '\\', 'n', '[SEP]']
Filtered   (008): ['all', '_', 'districts', '=', '(', ')', '\\', 'n']
Detokenized (005): ['all_districts', '=', '(', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "_ ( ) ) \n"
Original    (005): ['_', '(', ')', ')', '\\n']
Tokenized   (008): ['[CLS]', '_', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (006): ['_', '(', ')', ')', '\\', 'n']
Detokenized (005): ['_', '(', ')', ')', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "user_functions = ScoreFunction . objects . filter ( selectable_bodies = plan . legislative_body ) . order_by for f in user_functions : \n"
Original    (022): ['user_functions', '=', 'ScoreFunction', '.', 'objects', '.', 'filter', '(', 'selectable_bodies', '=', 'plan', '.', 'legislative_body', ')', '.', 'order_by', 'for', 'f', 'in', 'user_functions', ':', '\\n']
Tokenized   (038): ['[CLS]', 'user', '_', 'functions', '=', 'score', '##fu', '##nction', '.', 'objects', '.', 'filter', '(', 'select', '##able', '_', 'bodies', '=', 'plan', '.', 'legislative', '_', 'body', ')', '.', 'order', '_', 'by', 'for', 'f', 'in', 'user', '_', 'functions', ':', '\\', 'n', '[SEP]']
Filtered   (036): ['user', '_', 'functions', '=', 'score', '##fu', '##nction', '.', 'objects', '.', 'filter', '(', 'select', '##able', '_', 'bodies', '=', 'plan', '.', 'legislative', '_', 'body', ')', '.', 'order', '_', 'by', 'for', 'f', 'in', 'user', '_', 'functions', ':', '\\', 'n']
Detokenized (022): ['user_functions', '=', 'score##fu##nction', '.', 'objects', '.', 'filter', '(', 'select##able_bodies', '=', 'plan', '.', 'legislative_body', ')', '.', 'order_by', 'for', 'f', 'in', 'user_functions', ':', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : ""%s_sidebar_demo" % plan . legislative_body . name , \n"
Original    (009): ['"%s_sidebar_demo"', '%', 'plan', '.', 'legislative_body', '.', 'name', ',', '\\n']
Tokenized   (022): ['[CLS]', '"', '%', 's', '_', 'side', '##bar', '_', 'demo', '"', '%', 'plan', '.', 'legislative', '_', 'body', '.', 'name', ',', '\\', 'n', '[SEP]']
Filtered   (020): ['"', '%', 's', '_', 'side', '##bar', '_', 'demo', '"', '%', 'plan', '.', 'legislative', '_', 'body', '.', 'name', ',', '\\', 'n']
Detokenized (009): ['"%s_side##bar_demo"', '%', 'plan', '.', 'legislative_body', '.', 'name', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "plan . legislative_body . name ) \n"
Original    (007): ['plan', '.', 'legislative_body', '.', 'name', ')', '\\n']
Tokenized   (012): ['[CLS]', 'plan', '.', 'legislative', '_', 'body', '.', 'name', ')', '\\', 'n', '[SEP]']
Filtered   (010): ['plan', '.', 'legislative', '_', 'body', '.', 'name', ')', '\\', 'n']
Detokenized (007): ['plan', '.', 'legislative_body', '.', 'name', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "functions = map ( lambda x : int ( x ) , functions ) \n"
Original    (015): ['functions', '=', 'map', '(', 'lambda', 'x', ':', 'int', '(', 'x', ')', ',', 'functions', ')', '\\n']
Tokenized   (018): ['[CLS]', 'functions', '=', 'map', '(', 'lambda', 'x', ':', 'int', '(', 'x', ')', ',', 'functions', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['functions', '=', 'map', '(', 'lambda', 'x', ':', 'int', '(', 'x', ')', ',', 'functions', ')', '\\', 'n']
Detokenized (015): ['functions', '=', 'map', '(', 'lambda', 'x', ':', 'int', '(', 'x', ')', ',', 'functions', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "display = display . copy_from ( display = demo , title = request . POST . get ( ) , owner = result [ ] = True \n"
Original    (028): ['display', '=', 'display', '.', 'copy_from', '(', 'display', '=', 'demo', ',', 'title', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ',', 'owner', '=', 'result', '[', ']', '=', 'True', '\\n']
Tokenized   (033): ['[CLS]', 'display', '=', 'display', '.', 'copy', '_', 'from', '(', 'display', '=', 'demo', ',', 'title', '=', 'request', '.', 'post', '.', 'get', '(', ')', ',', 'owner', '=', 'result', '[', ']', '=', 'true', '\\', 'n', '[SEP]']
Filtered   (031): ['display', '=', 'display', '.', 'copy', '_', 'from', '(', 'display', '=', 'demo', ',', 'title', '=', 'request', '.', 'post', '.', 'get', '(', ')', ',', 'owner', '=', 'result', '[', ']', '=', 'true', '\\', 'n']
Detokenized (028): ['display', '=', 'display', '.', 'copy_from', '(', 'display', '=', 'demo', ',', 'title', '=', 'request', '.', 'post', '.', 'get', '(', ')', ',', 'owner', '=', 'result', '[', ']', '=', 'true', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 28, 768)
# Extracted words:  28
Sentence         : "version = min ( plan . version , int ( version ) ) \n"
Original    (014): ['version', '=', 'min', '(', 'plan', '.', 'version', ',', 'int', '(', 'version', ')', ')', '\\n']
Tokenized   (017): ['[CLS]', 'version', '=', 'min', '(', 'plan', '.', 'version', ',', 'int', '(', 'version', ')', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['version', '=', 'min', '(', 'plan', '.', 'version', ',', 'int', '(', 'version', ')', ')', '\\', 'n']
Detokenized (014): ['version', '=', 'min', '(', 'plan', '.', 'version', ',', 'int', '(', 'version', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "Comment . objects . filter ( object_pk = district . id , content_type = ct ) . delete ( ) \n"
Original    (021): ['Comment', '.', 'objects', '.', 'filter', '(', 'object_pk', '=', 'district', '.', 'id', ',', 'content_type', '=', 'ct', ')', '.', 'delete', '(', ')', '\\n']
Tokenized   (030): ['[CLS]', 'comment', '.', 'objects', '.', 'filter', '(', 'object', '_', 'p', '##k', '=', 'district', '.', 'id', ',', 'content', '_', 'type', '=', 'ct', ')', '.', 'del', '##ete', '(', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['comment', '.', 'objects', '.', 'filter', '(', 'object', '_', 'p', '##k', '=', 'district', '.', 'id', ',', 'content', '_', 'type', '=', 'ct', ')', '.', 'del', '##ete', '(', ')', '\\', 'n']
Detokenized (021): ['comment', '.', 'objects', '.', 'filter', '(', 'object_p##k', '=', 'district', '.', 'id', ',', 'content_type', '=', 'ct', ')', '.', 'del##ete', '(', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "TaggedItem . objects . filter ( tag__in = tset , object_id = district . id ) . delete ( ) \n"
Original    (021): ['TaggedItem', '.', 'objects', '.', 'filter', '(', 'tag__in', '=', 'tset', ',', 'object_id', '=', 'district', '.', 'id', ')', '.', 'delete', '(', ')', '\\n']
Tokenized   (033): ['[CLS]', 'tagged', '##ite', '##m', '.', 'objects', '.', 'filter', '(', 'tag', '_', '_', 'in', '=', 'ts', '##et', ',', 'object', '_', 'id', '=', 'district', '.', 'id', ')', '.', 'del', '##ete', '(', ')', '\\', 'n', '[SEP]']
Filtered   (031): ['tagged', '##ite', '##m', '.', 'objects', '.', 'filter', '(', 'tag', '_', '_', 'in', '=', 'ts', '##et', ',', 'object', '_', 'id', '=', 'district', '.', 'id', ')', '.', 'del', '##ete', '(', ')', '\\', 'n']
Detokenized (021): ['tagged##ite##m', '.', 'objects', '.', 'filter', '(', 'tag__in', '=', 'ts##et', ',', 'object_id', '=', 'district', '.', 'id', ')', '.', 'del##ete', '(', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "geolevel = plans [ 0 ] . legislative_body . get_geolevels ( ) [ 0 ] \n"
Original    (016): ['geolevel', '=', 'plans', '[', '0', ']', '.', 'legislative_body', '.', 'get_geolevels', '(', ')', '[', '0', ']', '\\n']
Tokenized   (027): ['[CLS]', 'geo', '##lev', '##el', '=', 'plans', '[', '0', ']', '.', 'legislative', '_', 'body', '.', 'get', '_', 'geo', '##lev', '##els', '(', ')', '[', '0', ']', '\\', 'n', '[SEP]']
Filtered   (025): ['geo', '##lev', '##el', '=', 'plans', '[', '0', ']', '.', 'legislative', '_', 'body', '.', 'get', '_', 'geo', '##lev', '##els', '(', ')', '[', '0', ']', '\\', 'n']
Detokenized (016): ['geo##lev##el', '=', 'plans', '[', '0', ']', '.', 'legislative_body', '.', 'get_geo##lev##els', '(', ')', '[', '0', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "plans = Plan . objects . filter ( is_shared = True ) . order_by ( ) [ 0 : 10 ] \n"
Original    (022): ['plans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'is_shared', '=', 'True', ')', '.', 'order_by', '(', ')', '[', '0', ':', '10', ']', '\\n']
Tokenized   (029): ['[CLS]', 'plans', '=', 'plan', '.', 'objects', '.', 'filter', '(', 'is', '_', 'shared', '=', 'true', ')', '.', 'order', '_', 'by', '(', ')', '[', '0', ':', '10', ']', '\\', 'n', '[SEP]']
Filtered   (027): ['plans', '=', 'plan', '.', 'objects', '.', 'filter', '(', 'is', '_', 'shared', '=', 'true', ')', '.', 'order', '_', 'by', '(', ')', '[', '0', ':', '10', ']', '\\', 'n']
Detokenized (022): ['plans', '=', 'plan', '.', 'objects', '.', 'filter', '(', 'is_shared', '=', 'true', ')', '.', 'order_by', '(', ')', '[', '0', ':', '10', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "write_page = 0 if write_page == 1 else write_page + 1 \n"
Original    (012): ['write_page', '=', '0', 'if', 'write_page', '==', '1', 'else', 'write_page', '+', '1', '\\n']
Tokenized   (022): ['[CLS]', 'write', '_', 'page', '=', '0', 'if', 'write', '_', 'page', '=', '=', '1', 'else', 'write', '_', 'page', '+', '1', '\\', 'n', '[SEP]']
Filtered   (020): ['write', '_', 'page', '=', '0', 'if', 'write', '_', 'page', '=', '=', '1', 'else', 'write', '_', 'page', '+', '1', '\\', 'n']
Detokenized (012): ['write_page', '=', '0', 'if', 'write_page', '==', '1', 'else', 'write_page', '+', '1', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "init_sum = 1 if i == 0 else 0 \n"
Original    (010): ['init_sum', '=', '1', 'if', 'i', '==', '0', 'else', '0', '\\n']
Tokenized   (017): ['[CLS]', 'in', '##it', '_', 'sum', '=', '1', 'if', 'i', '=', '=', '0', 'else', '0', '\\', 'n', '[SEP]']
Filtered   (015): ['in', '##it', '_', 'sum', '=', '1', 'if', 'i', '=', '=', '0', 'else', '0', '\\', 'n']
Detokenized (010): ['in##it_sum', '=', '1', 'if', 'i', '==', '0', 'else', '0', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "mem_d0 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n"
Original    (013): ['mem_d0', '.', 'read_nonblocking', '(', '1', ',', 'write_addr', ',', 'mesh_size', '-', '2', ')', '\\n']
Tokenized   (029): ['[CLS]', 'me', '##m', '_', 'd', '##0', '.', 'read', '_', 'non', '##block', '##ing', '(', '1', ',', 'write', '_', 'add', '##r', ',', 'mesh', '_', 'size', '-', '2', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['me', '##m', '_', 'd', '##0', '.', 'read', '_', 'non', '##block', '##ing', '(', '1', ',', 'write', '_', 'add', '##r', ',', 'mesh', '_', 'size', '-', '2', ')', '\\', 'n']
Detokenized (013): ['me##m_d##0', '.', 'read_non##block##ing', '(', '1', ',', 'write_add##r', ',', 'mesh_size', '-', '2', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "write_addr += mesh_size * DSIZE \n"
Original    (006): ['write_addr', '+=', 'mesh_size', '*', 'DSIZE', '\\n']
Tokenized   (016): ['[CLS]', 'write', '_', 'add', '##r', '+', '=', 'mesh', '_', 'size', '*', 'ds', '##ize', '\\', 'n', '[SEP]']
Filtered   (014): ['write', '_', 'add', '##r', '+', '=', 'mesh', '_', 'size', '*', 'ds', '##ize', '\\', 'n']
Detokenized (006): ['write_add##r', '+=', 'mesh_size', '*', 'ds##ize', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "sub_id_base = ( 10 if sub_id_m . group ( 1 ) . count ( "\'d" ) > 0 else \n"
Original    (020): ['sub_id_base', '=', '(', '10', 'if', 'sub_id_m', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\\'d"', ')', '>', '0', 'else', '\\n']
Tokenized   (035): ['[CLS]', 'sub', '_', 'id', '_', 'base', '=', '(', '10', 'if', 'sub', '_', 'id', '_', 'm', '.', 'group', '(', '1', ')', '.', 'count', '(', '"', '\\', "'", 'd', '"', ')', '>', '0', 'else', '\\', 'n', '[SEP]']
Filtered   (033): ['sub', '_', 'id', '_', 'base', '=', '(', '10', 'if', 'sub', '_', 'id', '_', 'm', '.', 'group', '(', '1', ')', '.', 'count', '(', '"', '\\', "'", 'd', '"', ')', '>', '0', 'else', '\\', 'n']
Detokenized (020): ['sub_id_base', '=', '(', '10', 'if', 'sub_id_m', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\\'d"', ')', '>', '0', 'else', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "16 if sub_id_m . group ( 1 ) . count ( "\'h" ) > 0 else \n"
Original    (017): ['16', 'if', 'sub_id_m', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\\'h"', ')', '>', '0', 'else', '\\n']
Tokenized   (028): ['[CLS]', '16', 'if', 'sub', '_', 'id', '_', 'm', '.', 'group', '(', '1', ')', '.', 'count', '(', '"', '\\', "'", 'h', '"', ')', '>', '0', 'else', '\\', 'n', '[SEP]']
Filtered   (026): ['16', 'if', 'sub', '_', 'id', '_', 'm', '.', 'group', '(', '1', ')', '.', 'count', '(', '"', '\\', "'", 'h', '"', ')', '>', '0', 'else', '\\', 'n']
Detokenized (017): ['16', 'if', 'sub_id_m', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\\'h"', ')', '>', '0', 'else', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "10 ) \n"
Original    (003): ['10', ')', '\\n']
Tokenized   (006): ['[CLS]', '10', ')', '\\', 'n', '[SEP]']
Filtered   (004): ['10', ')', '\\', 'n']
Detokenized (003): ['10', ')', '\\n']
Counter: 4
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "optparser . add_option ( "--noreorder" , action = "store_true" , dest = "noreorder" , \n"
Original    (015): ['optparser', '.', 'add_option', '(', '"--noreorder"', ',', 'action', '=', '"store_true"', ',', 'dest', '=', '"noreorder"', ',', '\\n']
Tokenized   (037): ['[CLS]', 'opt', '##par', '##ser', '.', 'add', '_', 'option', '(', '"', '-', '-', 'nor', '##eo', '##rder', '"', ',', 'action', '=', '"', 'store', '_', 'true', '"', ',', 'des', '##t', '=', '"', 'nor', '##eo', '##rder', '"', ',', '\\', 'n', '[SEP]']
Filtered   (035): ['opt', '##par', '##ser', '.', 'add', '_', 'option', '(', '"', '-', '-', 'nor', '##eo', '##rder', '"', ',', 'action', '=', '"', 'store', '_', 'true', '"', ',', 'des', '##t', '=', '"', 'nor', '##eo', '##rder', '"', ',', '\\', 'n']
Detokenized (015): ['opt##par##ser', '.', 'add_option', '(', '"--nor##eo##rder"', ',', 'action', '=', '"store_true"', ',', 'des##t', '=', '"nor##eo##rder"', ',', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "truenode = replaceUndefined ( tree . truenode , termname ) \n"
Original    (011): ['truenode', '=', 'replaceUndefined', '(', 'tree', '.', 'truenode', ',', 'termname', ')', '\\n']
Tokenized   (021): ['[CLS]', 'true', '##no', '##de', '=', 'replace', '##und', '##efined', '(', 'tree', '.', 'true', '##no', '##de', ',', 'term', '##name', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['true', '##no', '##de', '=', 'replace', '##und', '##efined', '(', 'tree', '.', 'true', '##no', '##de', ',', 'term', '##name', ')', '\\', 'n']
Detokenized (011): ['true##no##de', '=', 'replace##und##efined', '(', 'tree', '.', 'true##no##de', ',', 'term##name', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "codedir = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) + \n"
Original    (033): ['codedir', '=', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'abspath', '(', '__file__', ')', ')', ')', ')', '+', '\\n']
Tokenized   (045): ['[CLS]', 'coded', '##ir', '=', 'os', '.', 'path', '.', 'dir', '##name', '(', 'os', '.', 'path', '.', 'dir', '##name', '(', 'os', '.', 'path', '.', 'dir', '##name', '(', 'os', '.', 'path', '.', 'abs', '##path', '(', '_', '_', 'file', '_', '_', ')', ')', ')', ')', '+', '\\', 'n', '[SEP]']
Filtered   (043): ['coded', '##ir', '=', 'os', '.', 'path', '.', 'dir', '##name', '(', 'os', '.', 'path', '.', 'dir', '##name', '(', 'os', '.', 'path', '.', 'dir', '##name', '(', 'os', '.', 'path', '.', 'abs', '##path', '(', '_', '_', 'file', '_', '_', ')', ')', ')', ')', '+', '\\', 'n']
Detokenized (033): ['coded##ir', '=', 'os', '.', 'path', '.', 'dir##name', '(', 'os', '.', 'path', '.', 'dir##name', '(', 'os', '.', 'path', '.', 'dir##name', '(', 'os', '.', 'path', '.', 'abs##path', '(', '__file__', ')', ')', ')', ')', '+', '\\n']
Counter: 43
===================================================================
Hidden states:  (13, 33, 768)
# Extracted words:  33
Sentence         : "analyzer = VerilogDataflowAnalyzer ( filelist , topmodule , \n"
Original    (009): ['analyzer', '=', 'VerilogDataflowAnalyzer', '(', 'filelist', ',', 'topmodule', ',', '\\n']
Tokenized   (025): ['[CLS]', 'analyze', '##r', '=', 've', '##ril', '##og', '##da', '##ta', '##flow', '##anal', '##y', '##zer', '(', 'file', '##list', ',', 'top', '##mo', '##du', '##le', ',', '\\', 'n', '[SEP]']
Filtered   (023): ['analyze', '##r', '=', 've', '##ril', '##og', '##da', '##ta', '##flow', '##anal', '##y', '##zer', '(', 'file', '##list', ',', 'top', '##mo', '##du', '##le', ',', '\\', 'n']
Detokenized (009): ['analyze##r', '=', 've##ril##og##da##ta##flow##anal##y##zer', '(', 'file##list', ',', 'top##mo##du##le', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "constlist = optimizer . getConstlist ( ) \n"
Original    (008): ['constlist', '=', 'optimizer', '.', 'getConstlist', '(', ')', '\\n']
Tokenized   (018): ['[CLS]', 'con', '##st', '##list', '=', 'opt', '##imi', '##zer', '.', 'get', '##con', '##st', '##list', '(', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['con', '##st', '##list', '=', 'opt', '##imi', '##zer', '.', 'get', '##con', '##st', '##list', '(', ')', '\\', 'n']
Detokenized (008): ['con##st##list', '=', 'opt##imi##zer', '.', 'get##con##st##list', '(', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "interval = m . Parameter ( , 16 ) \n"
Original    (010): ['interval', '=', 'm', '.', 'Parameter', '(', ',', '16', ')', '\\n']
Tokenized   (013): ['[CLS]', 'interval', '=', 'm', '.', 'parameter', '(', ',', '16', ')', '\\', 'n', '[SEP]']
Filtered   (011): ['interval', '=', 'm', '.', 'parameter', '(', ',', '16', ')', '\\', 'n']
Detokenized (010): ['interval', '=', 'm', '.', 'parameter', '(', ',', '16', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "led ( led + 1 ) , \n"
Original    (008): ['led', '(', 'led', '+', '1', ')', ',', '\\n']
Tokenized   (011): ['[CLS]', 'led', '(', 'led', '+', '1', ')', ',', '\\', 'n', '[SEP]']
Filtered   (009): ['led', '(', 'led', '+', '1', ')', ',', '\\', 'n']
Detokenized (008): ['led', '(', 'led', '+', '1', ')', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "SingleStatement ( SystemTask ( , , led ) ) \n"
Original    (010): ['SingleStatement', '(', 'SystemTask', '(', ',', ',', 'led', ')', ')', '\\n']
Tokenized   (017): ['[CLS]', 'singles', '##tate', '##ment', '(', 'system', '##tas', '##k', '(', ',', ',', 'led', ')', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['singles', '##tate', '##ment', '(', 'system', '##tas', '##k', '(', ',', ',', 'led', ')', ')', '\\', 'n']
Detokenized (010): ['singles##tate##ment', '(', 'system##tas##k', '(', ',', ',', 'led', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "y = dataflow . Variable ( , valid = , ready = , point = 4 ) \n"
Original    (018): ['y', '=', 'dataflow', '.', 'Variable', '(', ',', 'valid', '=', ',', 'ready', '=', ',', 'point', '=', '4', ')', '\\n']
Tokenized   (022): ['[CLS]', 'y', '=', 'data', '##flow', '.', 'variable', '(', ',', 'valid', '=', ',', 'ready', '=', ',', 'point', '=', '4', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['y', '=', 'data', '##flow', '.', 'variable', '(', ',', 'valid', '=', ',', 'ready', '=', ',', 'point', '=', '4', ')', '\\', 'n']
Detokenized (018): ['y', '=', 'data##flow', '.', 'variable', '(', ',', 'valid', '=', ',', 'ready', '=', ',', 'point', '=', '4', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "z . output ( , valid = , ready = ) \n"
Original    (012): ['z', '.', 'output', '(', ',', 'valid', '=', ',', 'ready', '=', ')', '\\n']
Tokenized   (015): ['[CLS]', 'z', '.', 'output', '(', ',', 'valid', '=', ',', 'ready', '=', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['z', '.', 'output', '(', ',', 'valid', '=', ',', 'ready', '=', ')', '\\', 'n']
Detokenized (012): ['z', '.', 'output', '(', ',', 'valid', '=', ',', 'ready', '=', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "xdata_orig = m . RegLike ( ports [ ] , name = , initval = 0 ) \n"
Original    (018): ['xdata_orig', '=', 'm', '.', 'RegLike', '(', 'ports', '[', ']', ',', 'name', '=', ',', 'initval', '=', '0', ')', '\\n']
Tokenized   (029): ['[CLS]', 'x', '##da', '##ta', '_', 'or', '##ig', '=', 'm', '.', 'reg', '##like', '(', 'ports', '[', ']', ',', 'name', '=', ',', 'in', '##it', '##val', '=', '0', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['x', '##da', '##ta', '_', 'or', '##ig', '=', 'm', '.', 'reg', '##like', '(', 'ports', '[', ']', ',', 'name', '=', ',', 'in', '##it', '##val', '=', '0', ')', '\\', 'n']
Detokenized (018): ['x##da##ta_or##ig', '=', 'm', '.', 'reg##like', '(', 'ports', '[', ']', ',', 'name', '=', ',', 'in##it##val', '=', '0', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "zdata_orig = m . WireLike ( ports [ ] , name = ) \n"
Original    (014): ['zdata_orig', '=', 'm', '.', 'WireLike', '(', 'ports', '[', ']', ',', 'name', '=', ')', '\\n']
Tokenized   (023): ['[CLS]', 'z', '##da', '##ta', '_', 'or', '##ig', '=', 'm', '.', 'wire', '##like', '(', 'ports', '[', ']', ',', 'name', '=', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['z', '##da', '##ta', '_', 'or', '##ig', '=', 'm', '.', 'wire', '##like', '(', 'ports', '[', ']', ',', 'name', '=', ')', '\\', 'n']
Detokenized (014): ['z##da##ta_or##ig', '=', 'm', '.', 'wire##like', '(', 'ports', '[', ']', ',', 'name', '=', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "params = m . connect_params ( main ) , \n"
Original    (010): ['params', '=', 'm', '.', 'connect_params', '(', 'main', ')', ',', '\\n']
Tokenized   (017): ['[CLS]', 'para', '##ms', '=', 'm', '.', 'connect', '_', 'para', '##ms', '(', 'main', ')', ',', '\\', 'n', '[SEP]']
Filtered   (015): ['para', '##ms', '=', 'm', '.', 'connect', '_', 'para', '##ms', '(', 'main', ')', ',', '\\', 'n']
Detokenized (010): ['para##ms', '=', 'm', '.', 'connect_para##ms', '(', 'main', ')', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "reset_stmt . append ( ydata_orig ( 0 ) ) \n"
Original    (010): ['reset_stmt', '.', 'append', '(', 'ydata_orig', '(', '0', ')', ')', '\\n']
Tokenized   (021): ['[CLS]', 'reset', '_', 'st', '##mt', '.', 'app', '##end', '(', 'yd', '##ata', '_', 'or', '##ig', '(', '0', ')', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['reset', '_', 'st', '##mt', '.', 'app', '##end', '(', 'yd', '##ata', '_', 'or', '##ig', '(', '0', ')', ')', '\\', 'n']
Detokenized (010): ['reset_st##mt', '.', 'app##end', '(', 'yd##ata_or##ig', '(', '0', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "nclk ( clk ) , \n"
Original    (006): ['nclk', '(', 'clk', ')', ',', '\\n']
Tokenized   (011): ['[CLS]', 'nc', '##lk', '(', 'cl', '##k', ')', ',', '\\', 'n', '[SEP]']
Filtered   (009): ['nc', '##lk', '(', 'cl', '##k', ')', ',', '\\', 'n']
Detokenized (006): ['nc##lk', '(', 'cl##k', ')', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "send ( , ydata_orig , yvalid , yready , step = 1 , waitnum = 20 ) \n"
Original    (018): ['send', '(', ',', 'ydata_orig', ',', 'yvalid', ',', 'yready', ',', 'step', '=', '1', ',', 'waitnum', '=', '20', ')', '\\n']
Tokenized   (030): ['[CLS]', 'send', '(', ',', 'yd', '##ata', '_', 'or', '##ig', ',', 'y', '##val', '##id', ',', 'y', '##rea', '##dy', ',', 'step', '=', '1', ',', 'wait', '##num', '=', '20', ')', '\\', 'n', '[SEP]']
Filtered   (028): ['send', '(', ',', 'yd', '##ata', '_', 'or', '##ig', ',', 'y', '##val', '##id', ',', 'y', '##rea', '##dy', ',', 'step', '=', '1', ',', 'wait', '##num', '=', '20', ')', '\\', 'n']
Detokenized (018): ['send', '(', ',', 'yd##ata_or##ig', ',', 'y##val##id', ',', 'y##rea##dy', ',', 'step', '=', '1', ',', 'wait##num', '=', '20', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "receive ( , zdata , zvalid , zready , waitnum = 50 ) \n"
Original    (014): ['receive', '(', ',', 'zdata', ',', 'zvalid', ',', 'zready', ',', 'waitnum', '=', '50', ')', '\\n']
Tokenized   (024): ['[CLS]', 'receive', '(', ',', 'z', '##da', '##ta', ',', 'z', '##val', '##id', ',', 'z', '##rea', '##dy', ',', 'wait', '##num', '=', '50', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['receive', '(', ',', 'z', '##da', '##ta', ',', 'z', '##val', '##id', ',', 'z', '##rea', '##dy', ',', 'wait', '##num', '=', '50', ')', '\\', 'n']
Detokenized (014): ['receive', '(', ',', 'z##da##ta', ',', 'z##val##id', ',', 'z##rea##dy', ',', 'wait##num', '=', '50', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "If ( AndList ( zvalid , zready ) ) ( \n"
Original    (011): ['If', '(', 'AndList', '(', 'zvalid', ',', 'zready', ')', ')', '(', '\\n']
Tokenized   (019): ['[CLS]', 'if', '(', 'and', '##list', '(', 'z', '##val', '##id', ',', 'z', '##rea', '##dy', ')', ')', '(', '\\', 'n', '[SEP]']
Filtered   (017): ['if', '(', 'and', '##list', '(', 'z', '##val', '##id', ',', 'z', '##rea', '##dy', ')', ')', '(', '\\', 'n']
Detokenized (011): ['if', '(', 'and##list', '(', 'z##val##id', ',', 'z##rea##dy', ')', ')', '(', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "Systask ( , , zdata_orig ) \n"
Original    (007): ['Systask', '(', ',', ',', 'zdata_orig', ')', '\\n']
Tokenized   (017): ['[CLS]', 'sy', '##sta', '##sk', '(', ',', ',', 'z', '##da', '##ta', '_', 'or', '##ig', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['sy', '##sta', '##sk', '(', ',', ',', 'z', '##da', '##ta', '_', 'or', '##ig', ')', '\\', 'n']
Detokenized (007): ['sy##sta##sk', '(', ',', ',', 'z##da##ta_or##ig', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "count = m . Reg ( , width = 32 , initval = 0 ) \n"
Original    (016): ['count', '=', 'm', '.', 'Reg', '(', ',', 'width', '=', '32', ',', 'initval', '=', '0', ')', '\\n']
Tokenized   (021): ['[CLS]', 'count', '=', 'm', '.', 'reg', '(', ',', 'width', '=', '32', ',', 'in', '##it', '##val', '=', '0', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['count', '=', 'm', '.', 'reg', '(', ',', 'width', '=', '32', ',', 'in', '##it', '##val', '=', '0', ')', '\\', 'n']
Detokenized (016): ['count', '=', 'm', '.', 'reg', '(', ',', 'width', '=', '32', ',', 'in##it##val', '=', '0', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "fsm . add ( valid ( down ) , cond = c , delay = 4 , eager_val = True , lazy_cond = True ) \n"
Original    (026): ['fsm', '.', 'add', '(', 'valid', '(', 'down', ')', ',', 'cond', '=', 'c', ',', 'delay', '=', '4', ',', 'eager_val', '=', 'True', ',', 'lazy_cond', '=', 'True', ')', '\\n']
Tokenized   (036): ['[CLS]', 'f', '##sm', '.', 'add', '(', 'valid', '(', 'down', ')', ',', 'con', '##d', '=', 'c', ',', 'delay', '=', '4', ',', 'eager', '_', 'val', '=', 'true', ',', 'lazy', '_', 'con', '##d', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (034): ['f', '##sm', '.', 'add', '(', 'valid', '(', 'down', ')', ',', 'con', '##d', '=', 'c', ',', 'delay', '=', '4', ',', 'eager', '_', 'val', '=', 'true', ',', 'lazy', '_', 'con', '##d', '=', 'true', ')', '\\', 'n']
Detokenized (026): ['f##sm', '.', 'add', '(', 'valid', '(', 'down', ')', ',', 'con##d', '=', 'c', ',', 'delay', '=', '4', ',', 'eager_val', '=', 'true', ',', 'lazy_con##d', '=', 'true', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "uut = m . Instance ( mkLed ( ) , , \n"
Original    (012): ['uut', '=', 'm', '.', 'Instance', '(', 'mkLed', '(', ')', ',', ',', '\\n']
Tokenized   (017): ['[CLS]', 'u', '##ut', '=', 'm', '.', 'instance', '(', 'mk', '##led', '(', ')', ',', ',', '\\', 'n', '[SEP]']
Filtered   (015): ['u', '##ut', '=', 'm', '.', 'instance', '(', 'mk', '##led', '(', ')', ',', ',', '\\', 'n']
Detokenized (012): ['u##ut', '=', 'm', '.', 'instance', '(', 'mk##led', '(', ')', ',', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "rslt = m . Wire ( , retwidth , signed = True ) \n"
Original    (014): ['rslt', '=', 'm', '.', 'Wire', '(', ',', 'retwidth', ',', 'signed', '=', 'True', ')', '\\n']
Tokenized   (022): ['[CLS]', 'rs', '##lt', '=', 'm', '.', 'wire', '(', ',', 're', '##t', '##wi', '##dt', '##h', ',', 'signed', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['rs', '##lt', '=', 'm', '.', 'wire', '(', ',', 're', '##t', '##wi', '##dt', '##h', ',', 'signed', '=', 'true', ')', '\\', 'n']
Detokenized (014): ['rs##lt', '=', 'm', '.', 'wire', '(', ',', 're##t##wi##dt##h', ',', 'signed', '=', 'true', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "tmpval [ 0 ] ( rslt ) , \n"
Original    (009): ['tmpval', '[', '0', ']', '(', 'rslt', ')', ',', '\\n']
Tokenized   (015): ['[CLS]', 't', '##mp', '##val', '[', '0', ']', '(', 'rs', '##lt', ')', ',', '\\', 'n', '[SEP]']
Filtered   (013): ['t', '##mp', '##val', '[', '0', ']', '(', 'rs', '##lt', ')', ',', '\\', 'n']
Detokenized (009): ['t##mp##val', '[', '0', ']', '(', 'rs##lt', ')', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "vtypes . If ( rst ) ( \n"
Original    (008): ['vtypes', '.', 'If', '(', 'rst', ')', '(', '\\n']
Tokenized   (014): ['[CLS]', 'vt', '##ype', '##s', '.', 'if', '(', 'rs', '##t', ')', '(', '\\', 'n', '[SEP]']
Filtered   (012): ['vt', '##ype', '##s', '.', 'if', '(', 'rs', '##t', ')', '(', '\\', 'n']
Detokenized (008): ['vt##ype##s', '.', 'if', '(', 'rs##t', ')', '(', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "ports = [ ( , clk ) , ( , update ) , ( , a ) , ( , b ) , ( , c ) ] \n"
Original    (029): ['ports', '=', '[', '(', ',', 'clk', ')', ',', '(', ',', 'update', ')', ',', '(', ',', 'a', ')', ',', '(', ',', 'b', ')', ',', '(', ',', 'c', ')', ']', '\\n']
Tokenized   (033): ['[CLS]', 'ports', '=', '[', '(', ',', 'cl', '##k', ')', ',', '(', ',', 'update', ')', ',', '(', ',', 'a', ')', ',', '(', ',', 'b', ')', ',', '(', ',', 'c', ')', ']', '\\', 'n', '[SEP]']
Filtered   (031): ['ports', '=', '[', '(', ',', 'cl', '##k', ')', ',', '(', ',', 'update', ')', ',', '(', ',', 'a', ')', ',', '(', ',', 'b', ')', ',', '(', ',', 'c', ')', ']', '\\', 'n']
Detokenized (029): ['ports', '=', '[', '(', ',', 'cl##k', ')', ',', '(', ',', 'update', ')', ',', '(', ',', 'a', ')', ',', '(', ',', 'b', ')', ',', '(', ',', 'c', ')', ']', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "m . Instance ( mult , , ports = ports ) \n"
Original    (012): ['m', '.', 'Instance', '(', 'mult', ',', ',', 'ports', '=', 'ports', ')', '\\n']
Tokenized   (016): ['[CLS]', 'm', '.', 'instance', '(', 'mu', '##lt', ',', ',', 'ports', '=', 'ports', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['m', '.', 'instance', '(', 'mu', '##lt', ',', ',', 'ports', '=', 'ports', ')', '\\', 'n']
Detokenized (012): ['m', '.', 'instance', '(', 'mu##lt', ',', ',', 'ports', '=', 'ports', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "stdout = subprocess . PIPE ) . stdout \n"
Original    (009): ['stdout', '=', 'subprocess', '.', 'PIPE', ')', '.', 'stdout', '\\n']
Tokenized   (019): ['[CLS]', 'st', '##dou', '##t', '=', 'sub', '##pro', '##ces', '##s', '.', 'pipe', ')', '.', 'st', '##dou', '##t', '\\', 'n', '[SEP]']
Filtered   (017): ['st', '##dou', '##t', '=', 'sub', '##pro', '##ces', '##s', '.', 'pipe', ')', '.', 'st', '##dou', '##t', '\\', 'n']
Detokenized (009): ['st##dou##t', '=', 'sub##pro##ces##s', '.', 'pipe', ')', '.', 'st##dou##t', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "line = [ l for l in sout ] [ 0 ] \n"
Original    (013): ['line', '=', '[', 'l', 'for', 'l', 'in', 'sout', ']', '[', '0', ']', '\\n']
Tokenized   (017): ['[CLS]', 'line', '=', '[', 'l', 'for', 'l', 'in', 'so', '##ut', ']', '[', '0', ']', '\\', 'n', '[SEP]']
Filtered   (015): ['line', '=', '[', 'l', 'for', 'l', 'in', 'so', '##ut', ']', '[', '0', ']', '\\', 'n']
Detokenized (013): ['line', '=', '[', 'l', 'for', 'l', 'in', 'so##ut', ']', '[', '0', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "round ( wmean , prec ) , "+-" , round ( wstd , prec ) ) \n"
Original    (017): ['round', '(', 'wmean', ',', 'prec', ')', ',', '"+-"', ',', 'round', '(', 'wstd', ',', 'prec', ')', ')', '\\n']
Tokenized   (029): ['[CLS]', 'round', '(', 'w', '##me', '##an', ',', 'pre', '##c', ')', ',', '"', '+', '-', '"', ',', 'round', '(', 'w', '##st', '##d', ',', 'pre', '##c', ')', ')', '\\', 'n', '[SEP]']
Filtered   (027): ['round', '(', 'w', '##me', '##an', ',', 'pre', '##c', ')', ',', '"', '+', '-', '"', ',', 'round', '(', 'w', '##st', '##d', ',', 'pre', '##c', ')', ')', '\\', 'n']
Detokenized (017): ['round', '(', 'w##me##an', ',', 'pre##c', ')', ',', '"+-"', ',', 'round', '(', 'w##st##d', ',', 'pre##c', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "size = stop - start ) \n"
Original    (007): ['size', '=', 'stop', '-', 'start', ')', '\\n']
Tokenized   (010): ['[CLS]', 'size', '=', 'stop', '-', 'start', ')', '\\', 'n', '[SEP]']
Filtered   (008): ['size', '=', 'stop', '-', 'start', ')', '\\', 'n']
Detokenized (007): ['size', '=', 'stop', '-', 'start', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "rndbase = numpy . random . randint ( self . nrows , size = niter ) \n"
Original    (017): ['rndbase', '=', 'numpy', '.', 'random', '.', 'randint', '(', 'self', '.', 'nrows', ',', 'size', '=', 'niter', ')', '\\n']
Tokenized   (027): ['[CLS]', 'rn', '##db', '##ase', '=', 'nu', '##mp', '##y', '.', 'random', '.', 'rand', '##int', '(', 'self', '.', 'nr', '##ows', ',', 'size', '=', 'ni', '##ter', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['rn', '##db', '##ase', '=', 'nu', '##mp', '##y', '.', 'random', '.', 'rand', '##int', '(', 'self', '.', 'nr', '##ows', ',', 'size', '=', 'ni', '##ter', ')', '\\', 'n']
Detokenized (017): ['rn##db##ase', '=', 'nu##mp##y', '.', 'random', '.', 'rand##int', '(', 'self', '.', 'nr##ows', ',', 'size', '=', 'ni##ter', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "rng = [ - 1000 , - 1000 ] \n"
Original    (010): ['rng', '=', '[', '-', '1000', ',', '-', '1000', ']', '\\n']
Tokenized   (014): ['[CLS]', 'rn', '##g', '=', '[', '-', '1000', ',', '-', '1000', ']', '\\', 'n', '[SEP]']
Filtered   (012): ['rn', '##g', '=', '[', '-', '1000', ',', '-', '1000', ']', '\\', 'n']
Detokenized (010): ['rn##g', '=', '[', '-', '1000', ',', '-', '1000', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "benchtime , stones = prof . run ( \n"
Original    (009): ['benchtime', ',', 'stones', '=', 'prof', '.', 'run', '(', '\\n']
Tokenized   (013): ['[CLS]', 'bench', '##time', ',', 'stones', '=', 'prof', '.', 'run', '(', '\\', 'n', '[SEP]']
Filtered   (011): ['bench', '##time', ',', 'stones', '=', 'prof', '.', 'run', '(', '\\', 'n']
Detokenized (009): ['bench##time', ',', 'stones', '=', 'prof', '.', 'run', '(', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "db . rng = [ - rng / 2 , rng / 2 ] \n"
Original    (015): ['db', '.', 'rng', '=', '[', '-', 'rng', '/', '2', ',', 'rng', '/', '2', ']', '\\n']
Tokenized   (021): ['[CLS]', 'db', '.', 'rn', '##g', '=', '[', '-', 'rn', '##g', '/', '2', ',', 'rn', '##g', '/', '2', ']', '\\', 'n', '[SEP]']
Filtered   (019): ['db', '.', 'rn', '##g', '=', '[', '-', 'rn', '##g', '/', '2', ',', 'rn', '##g', '/', '2', ']', '\\', 'n']
Detokenized (015): ['db', '.', 'rn##g', '=', '[', '-', 'rn##g', '/', '2', ',', 'rn##g', '/', '2', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "MB_ = 1024 * KB_ \n"
Original    (006): ['MB_', '=', '1024', '*', 'KB_', '\\n']
Tokenized   (012): ['[CLS]', 'mb', '_', '=', '102', '##4', '*', 'kb', '_', '\\', 'n', '[SEP]']
Filtered   (010): ['mb', '_', '=', '102', '##4', '*', 'kb', '_', '\\', 'n']
Detokenized (006): ['mb_', '=', '102##4', '*', 'kb_', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "markers = [ , , , , , , , , , ] \n"
Original    (014): ['markers', '=', '[', ',', ',', ',', ',', ',', ',', ',', ',', ',', ']', '\\n']
Tokenized   (017): ['[CLS]', 'markers', '=', '[', ',', ',', ',', ',', ',', ',', ',', ',', ',', ']', '\\', 'n', '[SEP]']
Filtered   (015): ['markers', '=', '[', ',', ',', ',', ',', ',', ',', ',', ',', ',', ']', '\\', 'n']
Detokenized (014): ['markers', '=', '[', ',', ',', ',', ',', ',', ',', ',', ',', ',', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "memcpyw = float ( tmp . split ( ) [ 1 ] ) \n"
Original    (014): ['memcpyw', '=', 'float', '(', 'tmp', '.', 'split', '(', ')', '[', '1', ']', ')', '\\n']
Tokenized   (021): ['[CLS]', 'me', '##mc', '##py', '##w', '=', 'float', '(', 't', '##mp', '.', 'split', '(', ')', '[', '1', ']', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['me', '##mc', '##py', '##w', '=', 'float', '(', 't', '##mp', '.', 'split', '(', ')', '[', '1', ']', ')', '\\', 'n']
Detokenized (014): ['me##mc##py##w', '=', 'float', '(', 't##mp', '.', 'split', '(', ')', '[', '1', ']', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "values [ "memcpyr" ] . append ( memcpyr ) \n"
Original    (010): ['values', '[', '"memcpyr"', ']', '.', 'append', '(', 'memcpyr', ')', '\\n']
Tokenized   (022): ['[CLS]', 'values', '[', '"', 'me', '##mc', '##py', '##r', '"', ']', '.', 'app', '##end', '(', 'me', '##mc', '##py', '##r', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['values', '[', '"', 'me', '##mc', '##py', '##r', '"', ']', '.', 'app', '##end', '(', 'me', '##mc', '##py', '##r', ')', '\\', 'n']
Detokenized (010): ['values', '[', '"me##mc##py##r"', ']', '.', 'app##end', '(', 'me##mc##py##r', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "ratio = float ( line . split ( ) [ - 1 ] ) \n"
Original    (015): ['ratio', '=', 'float', '(', 'line', '.', 'split', '(', ')', '[', '-', '1', ']', ')', '\\n']
Tokenized   (018): ['[CLS]', 'ratio', '=', 'float', '(', 'line', '.', 'split', '(', ')', '[', '-', '1', ']', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['ratio', '=', 'float', '(', 'line', '.', 'split', '(', ')', '[', '-', '1', ']', ')', '\\', 'n']
Detokenized (015): ['ratio', '=', 'float', '(', 'line', '.', 'split', '(', ')', '[', '-', '1', ']', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "xlim ( 0 , xmax ) \n"
Original    (007): ['xlim', '(', '0', ',', 'xmax', ')', '\\n']
Tokenized   (012): ['[CLS]', 'xl', '##im', '(', '0', ',', 'x', '##max', ')', '\\', 'n', '[SEP]']
Filtered   (010): ['xl', '##im', '(', '0', ',', 'x', '##max', ')', '\\', 'n']
Detokenized (007): ['xl##im', '(', '0', ',', 'x##max', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "ylim ( 0 , None ) \n"
Original    (007): ['ylim', '(', '0', ',', 'None', ')', '\\n']
Tokenized   (011): ['[CLS]', 'y', '##lim', '(', '0', ',', 'none', ')', '\\', 'n', '[SEP]']
Filtered   (009): ['y', '##lim', '(', '0', ',', 'none', ')', '\\', 'n']
Detokenized (007): ['y##lim', '(', '0', ',', 'none', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "grid ( True ) \n"
Original    (005): ['grid', '(', 'True', ')', '\\n']
Tokenized   (008): ['[CLS]', 'grid', '(', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (006): ['grid', '(', 'true', ')', '\\', 'n']
Detokenized (005): ['grid', '(', 'true', ')', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "legend ( [ p [ 0 ] for p in plots \n"
Original    (012): ['legend', '(', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'plots', '\\n']
Tokenized   (015): ['[CLS]', 'legend', '(', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'plots', '\\', 'n', '[SEP]']
Filtered   (013): ['legend', '(', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'plots', '\\', 'n']
Detokenized (012): ['legend', '(', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'plots', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "savefig ( outfile , dpi = 64 ) \n"
Original    (009): ['savefig', '(', 'outfile', ',', 'dpi', '=', '64', ')', '\\n']
Tokenized   (017): ['[CLS]', 'save', '##fi', '##g', '(', 'out', '##fi', '##le', ',', 'd', '##pi', '=', '64', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['save', '##fi', '##g', '(', 'out', '##fi', '##le', ',', 'd', '##pi', '=', '64', ')', '\\', 'n']
Detokenized (009): ['save##fi##g', '(', 'out##fi##le', ',', 'd##pi', '=', '64', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "help = , ) \n"
Original    (005): ['help', '=', ',', ')', '\\n']
Tokenized   (008): ['[CLS]', 'help', '=', ',', ')', '\\', 'n', '[SEP]']
Filtered   (006): ['help', '=', ',', ')', '\\', 'n']
Detokenized (005): ['help', '=', ',', ')', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "parser . add_option ( , , action = , \n"
Original    (010): ['parser', '.', 'add_option', '(', ',', ',', 'action', '=', ',', '\\n']
Tokenized   (016): ['[CLS]', 'par', '##ser', '.', 'add', '_', 'option', '(', ',', ',', 'action', '=', ',', '\\', 'n', '[SEP]']
Filtered   (014): ['par', '##ser', '.', 'add', '_', 'option', '(', ',', ',', 'action', '=', ',', '\\', 'n']
Detokenized (010): ['par##ser', '.', 'add_option', '(', ',', ',', 'action', '=', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "show_plot ( plots , yaxis , legends , gtitle , xmax = int ( options . xmax ) if \n"
Original    (020): ['show_plot', '(', 'plots', ',', 'yaxis', ',', 'legends', ',', 'gtitle', ',', 'xmax', '=', 'int', '(', 'options', '.', 'xmax', ')', 'if', '\\n']
Tokenized   (031): ['[CLS]', 'show', '_', 'plot', '(', 'plots', ',', 'ya', '##xi', '##s', ',', 'legends', ',', 'gt', '##it', '##le', ',', 'x', '##max', '=', 'int', '(', 'options', '.', 'x', '##max', ')', 'if', '\\', 'n', '[SEP]']
Filtered   (029): ['show', '_', 'plot', '(', 'plots', ',', 'ya', '##xi', '##s', ',', 'legends', ',', 'gt', '##it', '##le', ',', 'x', '##max', '=', 'int', '(', 'options', '.', 'x', '##max', ')', 'if', '\\', 'n']
Detokenized (020): ['show_plot', '(', 'plots', ',', 'ya##xi##s', ',', 'legends', ',', 'gt##it##le', ',', 'x##max', '=', 'int', '(', 'options', '.', 'x##max', ')', 'if', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "options . xmax else None ) \n"
Original    (007): ['options', '.', 'xmax', 'else', 'None', ')', '\\n']
Tokenized   (011): ['[CLS]', 'options', '.', 'x', '##max', 'else', 'none', ')', '\\', 'n', '[SEP]']
Filtered   (009): ['options', '.', 'x', '##max', 'else', 'none', ')', '\\', 'n']
Detokenized (007): ['options', '.', 'x##max', 'else', 'none', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "hdfarray . attrs . object = { "a" : 32.1 , "b" : 1 , "c" : [ 1 , 2 ] } \n"
Original    (024): ['hdfarray', '.', 'attrs', '.', 'object', '=', '{', '"a"', ':', '32.1', ',', '"b"', ':', '1', ',', '"c"', ':', '[', '1', ',', '2', ']', '}', '\\n']
Tokenized   (039): ['[CLS]', 'hd', '##far', '##ray', '.', 'at', '##tr', '##s', '.', 'object', '=', '{', '"', 'a', '"', ':', '32', '.', '1', ',', '"', 'b', '"', ':', '1', ',', '"', 'c', '"', ':', '[', '1', ',', '2', ']', '}', '\\', 'n', '[SEP]']
Filtered   (037): ['hd', '##far', '##ray', '.', 'at', '##tr', '##s', '.', 'object', '=', '{', '"', 'a', '"', ':', '32', '.', '1', ',', '"', 'b', '"', ':', '1', ',', '"', 'c', '"', ':', '[', '1', ',', '2', ']', '}', '\\', 'n']
Detokenized (024): ['hd##far##ray', '.', 'at##tr##s', '.', 'object', '=', '{', '"a"', ':', '32.1', ',', '"b"', ':', '1', ',', '"c"', ':', '[', '1', ',', '2', ']', '}', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "addr = hex ( id ( self ) ) \n"
Original    (010): ['addr', '=', 'hex', '(', 'id', '(', 'self', ')', ')', '\\n']
Tokenized   (015): ['[CLS]', 'add', '##r', '=', 'he', '##x', '(', 'id', '(', 'self', ')', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['add', '##r', '=', 'he', '##x', '(', 'id', '(', 'self', ')', ')', '\\', 'n']
Detokenized (010): ['add##r', '=', 'he##x', '(', 'id', '(', 'self', ')', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "node_manager . registry . pop ( pathname , None ) \n"
Original    (011): ['node_manager', '.', 'registry', '.', 'pop', '(', 'pathname', ',', 'None', ')', '\\n']
Tokenized   (017): ['[CLS]', 'node', '_', 'manager', '.', 'registry', '.', 'pop', '(', 'path', '##name', ',', 'none', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['node', '_', 'manager', '.', 'registry', '.', 'pop', '(', 'path', '##name', ',', 'none', ')', '\\', 'n']
Detokenized (011): ['node_manager', '.', 'registry', '.', 'pop', '(', 'path##name', ',', 'none', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "oldpathname , self . _v_pathname ) \n"
Original    (007): ['oldpathname', ',', 'self', '.', '_v_pathname', ')', '\\n']
Tokenized   (016): ['[CLS]', 'old', '##path', '##name', ',', 'self', '.', '_', 'v', '_', 'path', '##name', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['old', '##path', '##name', ',', 'self', '.', '_', 'v', '_', 'path', '##name', ')', '\\', 'n']
Detokenized (007): ['old##path##name', ',', 'self', '.', '_v_path##name', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "recursive = False , _log = False , ** kwargs ) \n"
Original    (012): ['recursive', '=', 'False', ',', '_log', '=', 'False', ',', '**', 'kwargs', ')', '\\n']
Tokenized   (021): ['[CLS]', 'rec', '##urs', '##ive', '=', 'false', ',', '_', 'log', '=', 'false', ',', '*', '*', 'kw', '##ar', '##gs', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['rec', '##urs', '##ive', '=', 'false', ',', '_', 'log', '=', 'false', ',', '*', '*', 'kw', '##ar', '##gs', ')', '\\', 'n']
Detokenized (012): ['rec##urs##ive', '=', 'false', ',', '_log', '=', 'false', ',', '**', 'kw##ar##gs', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "% node . _v_pathname ) \n"
Original    (006): ['%', 'node', '.', '_v_pathname', ')', '\\n']
Tokenized   (013): ['[CLS]', '%', 'node', '.', '_', 'v', '_', 'path', '##name', ')', '\\', 'n', '[SEP]']
Filtered   (011): ['%', 'node', '.', '_', 'v', '_', 'path', '##name', ')', '\\', 'n']
Detokenized (006): ['%', 'node', '.', '_v_path##name', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "or pathname . startswith ( mypathname + ) ) : \n"
Original    (011): ['or', 'pathname', '.', 'startswith', '(', 'mypathname', '+', ')', ')', ':', '\\n']
Tokenized   (018): ['[CLS]', 'or', 'path', '##name', '.', 'starts', '##with', '(', 'my', '##path', '##name', '+', ')', ')', ':', '\\', 'n', '[SEP]']
Filtered   (016): ['or', 'path', '##name', '.', 'starts', '##with', '(', 'my', '##path', '##name', '+', ')', ')', ':', '\\', 'n']
Detokenized (011): ['or', 'path##name', '.', 'starts##with', '(', 'my##path##name', '+', ')', ')', ':', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "newarr = self . h5file . create_array ( , , [ 1 ] ) \n"
Original    (015): ['newarr', '=', 'self', '.', 'h5file', '.', 'create_array', '(', ',', ',', '[', '1', ']', ')', '\\n']
Tokenized   (025): ['[CLS]', 'new', '##ar', '##r', '=', 'self', '.', 'h', '##5', '##fi', '##le', '.', 'create', '_', 'array', '(', ',', ',', '[', '1', ']', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['new', '##ar', '##r', '=', 'self', '.', 'h', '##5', '##fi', '##le', '.', 'create', '_', 'array', '(', ',', ',', '[', '1', ']', ')', '\\', 'n']
Detokenized (015): ['new##ar##r', '=', 'self', '.', 'h##5##fi##le', '.', 'create_array', '(', ',', ',', '[', '1', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "with self . assertRaises ( tables . UndoRedoError ) : \n"
Original    (011): ['with', 'self', '.', 'assertRaises', '(', 'tables', '.', 'UndoRedoError', ')', ':', '\\n']
Tokenized   (019): ['[CLS]', 'with', 'self', '.', 'assert', '##rai', '##ses', '(', 'tables', '.', 'undo', '##redo', '##er', '##ror', ')', ':', '\\', 'n', '[SEP]']
Filtered   (017): ['with', 'self', '.', 'assert', '##rai', '##ses', '(', 'tables', '.', 'undo', '##redo', '##er', '##ror', ')', ':', '\\', 'n']
Detokenized (011): ['with', 'self', '.', 'assert##rai##ses', '(', 'tables', '.', 'undo##redo##er##ror', ')', ':', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""/othergroup1/othergroup2/othergroup3" not in self . h5file ) \n"
Original    (008): ['"/othergroup1/othergroup2/othergroup3"', 'not', 'in', 'self', '.', 'h5file', ')', '\\n']
Tokenized   (027): ['[CLS]', '"', '/', 'other', '##group', '##1', '/', 'other', '##group', '##2', '/', 'other', '##group', '##3', '"', 'not', 'in', 'self', '.', 'h', '##5', '##fi', '##le', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['"', '/', 'other', '##group', '##1', '/', 'other', '##group', '##2', '/', 'other', '##group', '##3', '"', 'not', 'in', 'self', '.', 'h', '##5', '##fi', '##le', ')', '\\', 'n']
Detokenized (008): ['"/other##group##1/other##group##2/other##group##3"', 'not', 'in', 'self', '.', 'h##5##fi##le', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "var2 = BoolCol ( dflt = 0 , pos = 2 ) \n"
Original    (013): ['var2', '=', 'BoolCol', '(', 'dflt', '=', '0', ',', 'pos', '=', '2', ')', '\\n']
Tokenized   (022): ['[CLS]', 'var', '##2', '=', 'boo', '##lco', '##l', '(', 'd', '##fl', '##t', '=', '0', ',', 'po', '##s', '=', '2', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['var', '##2', '=', 'boo', '##lco', '##l', '(', 'd', '##fl', '##t', '=', '0', ',', 'po', '##s', '=', '2', ')', '\\', 'n']
Detokenized (013): ['var##2', '=', 'boo##lco##l', '(', 'd##fl##t', '=', '0', ',', 'po##s', '=', '2', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "None , nrows ) \n"
Original    (005): ['None', ',', 'nrows', ')', '\\n']
Tokenized   (009): ['[CLS]', 'none', ',', 'nr', '##ows', ')', '\\', 'n', '[SEP]']
Filtered   (007): ['none', ',', 'nr', '##ows', ')', '\\', 'n']
Detokenized (005): ['none', ',', 'nr##ows', ')', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "populateTable ( self . h5file . root , ) \n"
Original    (010): ['populateTable', '(', 'self', '.', 'h5file', '.', 'root', ',', ')', '\\n']
Tokenized   (018): ['[CLS]', 'pop', '##ulate', '##table', '(', 'self', '.', 'h', '##5', '##fi', '##le', '.', 'root', ',', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['pop', '##ulate', '##table', '(', 'self', '.', 'h', '##5', '##fi', '##le', '.', 'root', ',', ')', '\\', 'n']
Detokenized (010): ['pop##ulate##table', '(', 'self', '.', 'h##5##fi##le', '.', 'root', ',', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "new_node = self . h5file . copy_children ( \n"
Original    (009): ['new_node', '=', 'self', '.', 'h5file', '.', 'copy_children', '(', '\\n']
Tokenized   (019): ['[CLS]', 'new', '_', 'node', '=', 'self', '.', 'h', '##5', '##fi', '##le', '.', 'copy', '_', 'children', '(', '\\', 'n', '[SEP]']
Filtered   (017): ['new', '_', 'node', '=', 'self', '.', 'h', '##5', '##fi', '##le', '.', 'copy', '_', 'children', '(', '\\', 'n']
Detokenized (009): ['new_node', '=', 'self', '.', 'h##5##fi##le', '.', 'copy_children', '(', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ", , recursive = 1 ) \n"
Original    (007): [',', ',', 'recursive', '=', '1', ')', '\\n']
Tokenized   (012): ['[CLS]', ',', ',', 'rec', '##urs', '##ive', '=', '1', ')', '\\', 'n', '[SEP]']
Filtered   (010): [',', ',', 'rec', '##urs', '##ive', '=', '1', ')', '\\', 'n']
Detokenized (007): [',', ',', 'rec##urs##ive', '=', '1', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "setattr ( attrs , , 11 ) \n"
Original    (008): ['setattr', '(', 'attrs', ',', ',', '11', ')', '\\n']
Tokenized   (015): ['[CLS]', 'set', '##att', '##r', '(', 'at', '##tr', '##s', ',', ',', '11', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['set', '##att', '##r', '(', 'at', '##tr', '##s', ',', ',', '11', ')', '\\', 'n']
Detokenized (008): ['set##att##r', '(', 'at##tr##s', ',', ',', '11', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "delattr ( attrs , ) \n"
Original    (006): ['delattr', '(', 'attrs', ',', ')', '\\n']
Tokenized   (013): ['[CLS]', 'del', '##att', '##r', '(', 'at', '##tr', '##s', ',', ')', '\\', 'n', '[SEP]']
Filtered   (011): ['del', '##att', '##r', '(', 'at', '##tr', '##s', ',', ')', '\\', 'n']
Detokenized (006): ['del##att##r', '(', 'at##tr##s', ',', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "arr . _v_attrs . foo = \n"
Original    (007): ['arr', '.', '_v_attrs', '.', 'foo', '=', '\\n']
Tokenized   (016): ['[CLS]', 'ar', '##r', '.', '_', 'v', '_', 'at', '##tr', '##s', '.', 'foo', '=', '\\', 'n', '[SEP]']
Filtered   (014): ['ar', '##r', '.', '_', 'v', '_', 'at', '##tr', '##s', '.', 'foo', '=', '\\', 'n']
Detokenized (007): ['ar##r', '.', '_v_at##tr##s', '.', 'foo', '=', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "assert response . header ( ) == \n"
Original    (008): ['assert', 'response', '.', 'header', '(', ')', '==', '\\n']
Tokenized   (012): ['[CLS]', 'assert', 'response', '.', 'header', '(', ')', '=', '=', '\\', 'n', '[SEP]']
Filtered   (010): ['assert', 'response', '.', 'header', '(', ')', '=', '=', '\\', 'n']
Detokenized (008): ['assert', 'response', '.', 'header', '(', ')', '==', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "CHANGES = open ( os . path . join ( here , ) , encoding = "utf-8" ) . read ( ) \n"
Original    (023): ['CHANGES', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ',', 'encoding', '=', '"utf-8"', ')', '.', 'read', '(', ')', '\\n']
Tokenized   (031): ['[CLS]', 'changes', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ',', 'encoding', '=', '"', 'ut', '##f', '-', '8', '"', ')', '.', 'read', '(', ')', '\\', 'n', '[SEP]']
Filtered   (029): ['changes', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ',', 'encoding', '=', '"', 'ut', '##f', '-', '8', '"', ')', '.', 'read', '(', ')', '\\', 'n']
Detokenized (023): ['changes', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ',', 'encoding', '=', '"ut##f-8"', ')', '.', 'read', '(', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "tests_require = requires + [ ] , \n"
Original    (008): ['tests_require', '=', 'requires', '+', '[', ']', ',', '\\n']
Tokenized   (013): ['[CLS]', 'tests', '_', 'require', '=', 'requires', '+', '[', ']', ',', '\\', 'n', '[SEP]']
Filtered   (011): ['tests', '_', 'require', '=', 'requires', '+', '[', ']', ',', '\\', 'n']
Detokenized (008): ['tests_require', '=', 'requires', '+', '[', ']', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "user_id = Column ( Integer , primary_key = True ) \n"
Original    (011): ['user_id', '=', 'Column', '(', 'Integer', ',', 'primary_key', '=', 'True', ')', '\\n']
Tokenized   (018): ['[CLS]', 'user', '_', 'id', '=', 'column', '(', 'integer', ',', 'primary', '_', 'key', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['user', '_', 'id', '=', 'column', '(', 'integer', ',', 'primary', '_', 'key', '=', 'true', ')', '\\', 'n']
Detokenized (011): ['user_id', '=', 'column', '(', 'integer', ',', 'primary_key', '=', 'true', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "username = Column ( Unicode ( 20 ) , unique = True ) \n"
Original    (014): ['username', '=', 'Column', '(', 'Unicode', '(', '20', ')', ',', 'unique', '=', 'True', ')', '\\n']
Tokenized   (018): ['[CLS]', 'user', '##name', '=', 'column', '(', 'unicode', '(', '20', ')', ',', 'unique', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['user', '##name', '=', 'column', '(', 'unicode', '(', '20', ')', ',', 'unique', '=', 'true', ')', '\\', 'n']
Detokenized (014): ['user##name', '=', 'column', '(', 'unicode', '(', '20', ')', ',', 'unique', '=', 'true', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "hits = Column ( Integer , default = 0 ) \n"
Original    (011): ['hits', '=', 'Column', '(', 'Integer', ',', 'default', '=', '0', ')', '\\n']
Tokenized   (014): ['[CLS]', 'hits', '=', 'column', '(', 'integer', ',', 'default', '=', '0', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['hits', '=', 'column', '(', 'integer', ',', 'default', '=', '0', ')', '\\', 'n']
Detokenized (011): ['hits', '=', 'column', '(', 'integer', ',', 'default', '=', '0', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "_password = Column ( , Unicode ( 60 ) ) \n"
Original    (011): ['_password', '=', 'Column', '(', ',', 'Unicode', '(', '60', ')', ')', '\\n']
Tokenized   (015): ['[CLS]', '_', 'password', '=', 'column', '(', ',', 'unicode', '(', '60', ')', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['_', 'password', '=', 'column', '(', ',', 'unicode', '(', '60', ')', ')', '\\', 'n']
Detokenized (011): ['_password', '=', 'column', '(', ',', 'unicode', '(', '60', ')', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "Column ( , Integer , ForeignKey ( ) ) \n"
Original    (010): ['Column', '(', ',', 'Integer', ',', 'ForeignKey', '(', ')', ')', '\\n']
Tokenized   (014): ['[CLS]', 'column', '(', ',', 'integer', ',', 'foreign', '##key', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['column', '(', ',', 'integer', ',', 'foreign', '##key', '(', ')', ')', '\\', 'n']
Detokenized (010): ['column', '(', ',', 'integer', ',', 'foreign##key', '(', ')', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "target_id = Column ( Integer , ForeignKey ( ) ) \n"
Original    (011): ['target_id', '=', 'Column', '(', 'Integer', ',', 'ForeignKey', '(', ')', ')', '\\n']
Tokenized   (017): ['[CLS]', 'target', '_', 'id', '=', 'column', '(', 'integer', ',', 'foreign', '##key', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['target', '_', 'id', '=', 'column', '(', 'integer', ',', 'foreign', '##key', '(', ')', ')', '\\', 'n']
Detokenized (011): ['target_id', '=', 'column', '(', 'integer', ',', 'foreign##key', '(', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "comments = relation ( , cascade = "delete" , \n"
Original    (010): ['comments', '=', 'relation', '(', ',', 'cascade', '=', '"delete"', ',', '\\n']
Tokenized   (016): ['[CLS]', 'comments', '=', 'relation', '(', ',', 'cascade', '=', '"', 'del', '##ete', '"', ',', '\\', 'n', '[SEP]']
Filtered   (014): ['comments', '=', 'relation', '(', ',', 'cascade', '=', '"', 'del', '##ete', '"', ',', '\\', 'n']
Detokenized (010): ['comments', '=', 'relation', '(', ',', 'cascade', '=', '"del##ete"', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "author = relation ( User , cascade = "delete" , backref = ) \n"
Original    (014): ['author', '=', 'relation', '(', 'User', ',', 'cascade', '=', '"delete"', ',', 'backref', '=', ')', '\\n']
Tokenized   (022): ['[CLS]', 'author', '=', 'relation', '(', 'user', ',', 'cascade', '=', '"', 'del', '##ete', '"', ',', 'back', '##re', '##f', '=', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['author', '=', 'relation', '(', 'user', ',', 'cascade', '=', '"', 'del', '##ete', '"', ',', 'back', '##re', '##f', '=', ')', '\\', 'n']
Detokenized (014): ['author', '=', 'relation', '(', 'user', ',', 'cascade', '=', '"del##ete"', ',', 'back##re##f', '=', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "tags = relation ( Tag , secondary = ideas_tags , backref = ) \n"
Original    (014): ['tags', '=', 'relation', '(', 'Tag', ',', 'secondary', '=', 'ideas_tags', ',', 'backref', '=', ')', '\\n']
Tokenized   (021): ['[CLS]', 'tags', '=', 'relation', '(', 'tag', ',', 'secondary', '=', 'ideas', '_', 'tags', ',', 'back', '##re', '##f', '=', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['tags', '=', 'relation', '(', 'tag', ',', 'secondary', '=', 'ideas', '_', 'tags', ',', 'back', '##re', '##f', '=', ')', '\\', 'n']
Detokenized (014): ['tags', '=', 'relation', '(', 'tag', ',', 'secondary', '=', 'ideas_tags', ',', 'back##re##f', '=', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "voted_users = relation ( User , secondary = voted_users , lazy = , \n"
Original    (014): ['voted_users', '=', 'relation', '(', 'User', ',', 'secondary', '=', 'voted_users', ',', 'lazy', '=', ',', '\\n']
Tokenized   (021): ['[CLS]', 'voted', '_', 'users', '=', 'relation', '(', 'user', ',', 'secondary', '=', 'voted', '_', 'users', ',', 'lazy', '=', ',', '\\', 'n', '[SEP]']
Filtered   (019): ['voted', '_', 'users', '=', 'relation', '(', 'user', ',', 'secondary', '=', 'voted', '_', 'users', ',', 'lazy', '=', ',', '\\', 'n']
Detokenized (014): ['voted_users', '=', 'relation', '(', 'user', ',', 'secondary', '=', 'voted_users', ',', 'lazy', '=', ',', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "total_votes = column_property ( ( hits + misses ) . label ( ) ) \n"
Original    (015): ['total_votes', '=', 'column_property', '(', '(', 'hits', '+', 'misses', ')', '.', 'label', '(', ')', ')', '\\n']
Tokenized   (022): ['[CLS]', 'total', '_', 'votes', '=', 'column', '_', 'property', '(', '(', 'hits', '+', 'misses', ')', '.', 'label', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['total', '_', 'votes', '=', 'column', '_', 'property', '(', '(', 'hits', '+', 'misses', ')', '.', 'label', '(', ')', ')', '\\', 'n']
Detokenized (015): ['total_votes', '=', 'column_property', '(', '(', 'hits', '+', 'misses', ')', '.', 'label', '(', ')', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "query = query . filter ( cls . target == None ) . order_by ( order_by ) \n"
Original    (018): ['query', '=', 'query', '.', 'filter', '(', 'cls', '.', 'target', '==', 'None', ')', '.', 'order_by', '(', 'order_by', ')', '\\n']
Tokenized   (027): ['[CLS]', 'query', '=', 'query', '.', 'filter', '(', 'cl', '##s', '.', 'target', '=', '=', 'none', ')', '.', 'order', '_', 'by', '(', 'order', '_', 'by', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['query', '=', 'query', '.', 'filter', '(', 'cl', '##s', '.', 'target', '=', '=', 'none', ')', '.', 'order', '_', 'by', '(', 'order', '_', 'by', ')', '\\', 'n']
Detokenized (018): ['query', '=', 'query', '.', 'filter', '(', 'cl##s', '.', 'target', '==', 'none', ')', '.', 'order_by', '(', 'order_by', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "mock_get_auditlog . side_effect = lambda c : auditlog \n"
Original    (009): ['mock_get_auditlog', '.', 'side_effect', '=', 'lambda', 'c', ':', 'auditlog', '\\n']
Tokenized   (020): ['[CLS]', 'mock', '_', 'get', '_', 'audit', '##log', '.', 'side', '_', 'effect', '=', 'lambda', 'c', ':', 'audit', '##log', '\\', 'n', '[SEP]']
Filtered   (018): ['mock', '_', 'get', '_', 'audit', '##log', '.', 'side', '_', 'effect', '=', 'lambda', 'c', ':', 'audit', '##log', '\\', 'n']
Detokenized (009): ['mock_get_audit##log', '.', 'side_effect', '=', 'lambda', 'c', ':', 'audit##log', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "json . loads ( entry [ 2 ] . payload ) , \n"
Original    (013): ['json', '.', 'loads', '(', 'entry', '[', '2', ']', '.', 'payload', ')', ',', '\\n']
Tokenized   (017): ['[CLS]', 'j', '##son', '.', 'loads', '(', 'entry', '[', '2', ']', '.', 'payload', ')', ',', '\\', 'n', '[SEP]']
Filtered   (015): ['j', '##son', '.', 'loads', '(', 'entry', '[', '2', ']', '.', 'payload', ')', ',', '\\', 'n']
Detokenized (013): ['j##son', '.', 'loads', '(', 'entry', '[', '2', ']', '.', 'payload', ')', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : ": 5 \n"
Original    (003): [':', '5', '\\n']
Tokenized   (006): ['[CLS]', ':', '5', '\\', 'n', '[SEP]']
Filtered   (004): [':', '5', '\\', 'n']
Detokenized (003): [':', '5', '\\n']
Counter: 4
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "registry . content = DummyContentRegistry ( ) \n"
Original    (008): ['registry', '.', 'content', '=', 'DummyContentRegistry', '(', ')', '\\n']
Tokenized   (016): ['[CLS]', 'registry', '.', 'content', '=', 'dummy', '##con', '##ten', '##tre', '##gist', '##ry', '(', ')', '\\', 'n', '[SEP]']
Filtered   (014): ['registry', '.', 'content', '=', 'dummy', '##con', '##ten', '##tre', '##gist', '##ry', '(', ')', '\\', 'n']
Detokenized (008): ['registry', '.', 'content', '=', 'dummy##con##ten##tre##gist##ry', '(', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "ep = DummyFunction ( True ) \n"
Original    (007): ['ep', '=', 'DummyFunction', '(', 'True', ')', '\\n']
Tokenized   (012): ['[CLS]', 'ep', '=', 'dummy', '##fu', '##nction', '(', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (010): ['ep', '=', 'dummy', '##fu', '##nction', '(', 'true', ')', '\\', 'n']
Detokenized (007): ['ep', '=', 'dummy##fu##nction', '(', 'true', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "name_node . validator ( node [ ] , filename ) \n"
Original    (011): ['name_node', '.', 'validator', '(', 'node', '[', ']', ',', 'filename', ')', '\\n']
Tokenized   (018): ['[CLS]', 'name', '_', 'node', '.', 'valid', '##ator', '(', 'node', '[', ']', ',', 'file', '##name', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['name', '_', 'node', '.', 'valid', '##ator', '(', 'node', '[', ']', ',', 'file', '##name', ')', '\\', 'n']
Detokenized (011): ['name_node', '.', 'valid##ator', '(', 'node', '[', ']', ',', 'file##name', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "schema [ ] . missing = colander . null \n"
Original    (010): ['schema', '[', ']', '.', 'missing', '=', 'colander', '.', 'null', '\\n']
Tokenized   (015): ['[CLS]', 'sc', '##hema', '[', ']', '.', 'missing', '=', 'cola', '##nder', '.', 'null', '\\', 'n', '[SEP]']
Filtered   (013): ['sc', '##hema', '[', ']', '.', 'missing', '=', 'cola', '##nder', '.', 'null', '\\', 'n']
Detokenized (010): ['sc##hema', '[', ']', '.', 'missing', '=', 'cola##nder', '.', 'null', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "title = appstruct [ ] or None \n"
Original    (008): ['title', '=', 'appstruct', '[', ']', 'or', 'None', '\\n']
Tokenized   (012): ['[CLS]', 'title', '=', 'apps', '##truct', '[', ']', 'or', 'none', '\\', 'n', '[SEP]']
Filtered   (010): ['title', '=', 'apps', '##truct', '[', ']', 'or', 'none', '\\', 'n']
Detokenized (008): ['title', '=', 'apps##truct', '[', ']', 'or', 'none', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "mimetype = appstruct [ ] or USE_MAGIC \n"
Original    (008): ['mimetype', '=', 'appstruct', '[', ']', 'or', 'USE_MAGIC', '\\n']
Tokenized   (016): ['[CLS]', 'mi', '##met', '##ype', '=', 'apps', '##truct', '[', ']', 'or', 'use', '_', 'magic', '\\', 'n', '[SEP]']
Filtered   (014): ['mi', '##met', '##ype', '=', 'apps', '##truct', '[', ']', 'or', 'use', '_', 'magic', '\\', 'n']
Detokenized (008): ['mi##met##ype', '=', 'apps##truct', '[', ']', 'or', 'use_magic', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "filedata = tempstore . get ( uid , { } ) \n"
Original    (012): ['filedata', '=', 'tempstore', '.', 'get', '(', 'uid', ',', '{', '}', ')', '\\n']
Tokenized   (018): ['[CLS]', 'filed', '##ata', '=', 'temps', '##tore', '.', 'get', '(', 'ui', '##d', ',', '{', '}', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['filed', '##ata', '=', 'temps', '##tore', '.', 'get', '(', 'ui', '##d', ',', '{', '}', ')', '\\', 'n']
Detokenized (012): ['filed##ata', '=', 'temps##tore', '.', 'get', '(', 'ui##d', ',', '{', '}', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "resource1 . __acl__ = [ ( None , , None ) , ( None , 1 , None ) ] \n"
Original    (021): ['resource1', '.', '__acl__', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', '\\n']
Tokenized   (030): ['[CLS]', 'resource', '##1', '.', '_', '_', 'ac', '##l', '_', '_', '=', '[', '(', 'none', ',', ',', 'none', ')', ',', '(', 'none', ',', '1', ',', 'none', ')', ']', '\\', 'n', '[SEP]']
Filtered   (028): ['resource', '##1', '.', '_', '_', 'ac', '##l', '_', '_', '=', '[', '(', 'none', ',', ',', 'none', ')', ',', '(', 'none', ',', '1', ',', 'none', ')', ']', '\\', 'n']
Detokenized (021): ['resource##1', '.', '__ac##l__', '=', '[', '(', 'none', ',', ',', 'none', ')', ',', '(', 'none', ',', '1', ',', 'none', ')', ']', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "new_acl = [ ( None , , None ) , ( None , 1 , None ) ] , \n"
Original    (020): ['new_acl', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', ',', '\\n']
Tokenized   (026): ['[CLS]', 'new', '_', 'ac', '##l', '=', '[', '(', 'none', ',', ',', 'none', ')', ',', '(', 'none', ',', '1', ',', 'none', ')', ']', ',', '\\', 'n', '[SEP]']
Filtered   (024): ['new', '_', 'ac', '##l', '=', '[', '(', 'none', ',', ',', 'none', ')', ',', '(', 'none', ',', '1', ',', 'none', ')', ']', ',', '\\', 'n']
Detokenized (020): ['new_ac##l', '=', '[', '(', 'none', ',', ',', 'none', ')', ',', '(', 'none', ',', '1', ',', 'none', ')', ']', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "request . registry . notify ( LoggedIn ( login , user , context , request ) ) \n"
Original    (018): ['request', '.', 'registry', '.', 'notify', '(', 'LoggedIn', '(', 'login', ',', 'user', ',', 'context', ',', 'request', ')', ')', '\\n']
Tokenized   (024): ['[CLS]', 'request', '.', 'registry', '.', 'not', '##ify', '(', 'logged', '##in', '(', 'log', '##in', ',', 'user', ',', 'context', ',', 'request', ')', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['request', '.', 'registry', '.', 'not', '##ify', '(', 'logged', '##in', '(', 'log', '##in', ',', 'user', ',', 'context', ',', 'request', ')', ')', '\\', 'n']
Detokenized (018): ['request', '.', 'registry', '.', 'not##ify', '(', 'logged##in', '(', 'log##in', ',', 'user', ',', 'context', ',', 'request', ')', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "dirname , filename = os . path . split ( context . path ) \n"
Original    (015): ['dirname', ',', 'filename', '=', 'os', '.', 'path', '.', 'split', '(', 'context', '.', 'path', ')', '\\n']
Tokenized   (020): ['[CLS]', 'dir', '##name', ',', 'file', '##name', '=', 'os', '.', 'path', '.', 'split', '(', 'context', '.', 'path', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['dir', '##name', ',', 'file', '##name', '=', 'os', '.', 'path', '.', 'split', '(', 'context', '.', 'path', ')', '\\', 'n']
Detokenized (015): ['dir##name', ',', 'file##name', '=', 'os', '.', 'path', '.', 'split', '(', 'context', '.', 'path', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "response . content_type = mt or \n"
Original    (007): ['response', '.', 'content_type', '=', 'mt', 'or', '\\n']
Tokenized   (012): ['[CLS]', 'response', '.', 'content', '_', 'type', '=', 'mt', 'or', '\\', 'n', '[SEP]']
Filtered   (010): ['response', '.', 'content', '_', 'type', '=', 'mt', 'or', '\\', 'n']
Detokenized (007): ['response', '.', 'content_type', '=', 'mt', 'or', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "getattr ( SkipCase ( ) , ) ) \n"
Original    (009): ['getattr', '(', 'SkipCase', '(', ')', ',', ')', ')', '\\n']
Tokenized   (015): ['[CLS]', 'get', '##att', '##r', '(', 'skip', '##case', '(', ')', ',', ')', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['get', '##att', '##r', '(', 'skip', '##case', '(', ')', ',', ')', ')', '\\', 'n']
Detokenized (009): ['get##att##r', '(', 'skip##case', '(', ')', ',', ')', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "iterator . __class__ . __name__ ) ) \n"
Original    (008): ['iterator', '.', '__class__', '.', '__name__', ')', ')', '\\n']
Tokenized   (021): ['[CLS]', 'it', '##era', '##tor', '.', '_', '_', 'class', '_', '_', '.', '_', '_', 'name', '_', '_', ')', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['it', '##era', '##tor', '.', '_', '_', 'class', '_', '_', '.', '_', '_', 'name', '_', '_', ')', ')', '\\', 'n']
Detokenized (008): ['it##era##tor', '.', '__class__', '.', '__name__', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "parts = super ( newbytes , self ) . splitlines ( keepends ) \n"
Original    (014): ['parts', '=', 'super', '(', 'newbytes', ',', 'self', ')', '.', 'splitlines', '(', 'keepends', ')', '\\n']
Tokenized   (022): ['[CLS]', 'parts', '=', 'super', '(', 'new', '##by', '##tes', ',', 'self', ')', '.', 'split', '##lines', '(', 'keep', '##end', '##s', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['parts', '=', 'super', '(', 'new', '##by', '##tes', ',', 'self', ')', '.', 'split', '##lines', '(', 'keep', '##end', '##s', ')', '\\', 'n']
Detokenized (014): ['parts', '=', 'super', '(', 'new##by##tes', ',', 'self', ')', '.', 'split##lines', '(', 'keep##end##s', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "pos = self . rfind ( sub , * args ) \n"
Original    (012): ['pos', '=', 'self', '.', 'rfind', '(', 'sub', ',', '*', 'args', ')', '\\n']
Tokenized   (018): ['[CLS]', 'po', '##s', '=', 'self', '.', 'rf', '##ind', '(', 'sub', ',', '*', 'ar', '##gs', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['po', '##s', '=', 'self', '.', 'rf', '##ind', '(', 'sub', ',', '*', 'ar', '##gs', ')', '\\', 'n']
Detokenized (012): ['po##s', '=', 'self', '.', 'rf##ind', '(', 'sub', ',', '*', 'ar##gs', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "replaced_builtins = . split ( ) \n"
Original    (007): ['replaced_builtins', '=', '.', 'split', '(', ')', '\\n']
Tokenized   (013): ['[CLS]', 'replaced', '_', 'built', '##ins', '=', '.', 'split', '(', ')', '\\', 'n', '[SEP]']
Filtered   (011): ['replaced', '_', 'built', '##ins', '=', '.', 'split', '(', ')', '\\', 'n']
Detokenized (007): ['replaced_built##ins', '=', '.', 'split', '(', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "expression = . join ( [ "name=\'{0}\'" . format ( name ) for name in replaced_builtins ] ) \n"
Original    (019): ['expression', '=', '.', 'join', '(', '[', '"name=\\\'{0}\\\'"', '.', 'format', '(', 'name', ')', 'for', 'name', 'in', 'replaced_builtins', ']', ')', '\\n']
Tokenized   (035): ['[CLS]', 'expression', '=', '.', 'join', '(', '[', '"', 'name', '=', '\\', "'", '{', '0', '}', '\\', "'", '"', '.', 'format', '(', 'name', ')', 'for', 'name', 'in', 'replaced', '_', 'built', '##ins', ']', ')', '\\', 'n', '[SEP]']
Filtered   (033): ['expression', '=', '.', 'join', '(', '[', '"', 'name', '=', '\\', "'", '{', '0', '}', '\\', "'", '"', '.', 'format', '(', 'name', ')', 'for', 'name', 'in', 'replaced', '_', 'built', '##ins', ']', ')', '\\', 'n']
Detokenized (019): ['expression', '=', '.', 'join', '(', '[', '"name=\\\'{0}\\\'"', '.', 'format', '(', 'name', ')', 'for', 'name', 'in', 'replaced_built##ins', ']', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "touch_import_top ( , name . value , node ) \n"
Original    (010): ['touch_import_top', '(', ',', 'name', '.', 'value', ',', 'node', ')', '\\n']
Tokenized   (017): ['[CLS]', 'touch', '_', 'import', '_', 'top', '(', ',', 'name', '.', 'value', ',', 'node', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['touch', '_', 'import', '_', 'top', '(', ',', 'name', '.', 'value', ',', 'node', ')', '\\', 'n']
Detokenized (010): ['touch_import_top', '(', ',', 'name', '.', 'value', ',', 'node', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "retcode = main ( [ self . textfilename ] ) \n"
Original    (011): ['retcode', '=', 'main', '(', '[', 'self', '.', 'textfilename', ']', ')', '\\n']
Tokenized   (019): ['[CLS]', 're', '##tc', '##ode', '=', 'main', '(', '[', 'self', '.', 'text', '##fi', '##lena', '##me', ']', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['re', '##tc', '##ode', '=', 'main', '(', '[', 'self', '.', 'text', '##fi', '##lena', '##me', ']', ')', '\\', 'n']
Detokenized (011): ['re##tc##ode', '=', 'main', '(', '[', 'self', '.', 'text##fi##lena##me', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "v = self . visit ( node . values [ i ] ) \n"
Original    (014): ['v', '=', 'self', '.', 'visit', '(', 'node', '.', 'values', '[', 'i', ']', ')', '\\n']
Tokenized   (017): ['[CLS]', 'v', '=', 'self', '.', 'visit', '(', 'node', '.', 'values', '[', 'i', ']', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['v', '=', 'self', '.', 'visit', '(', 'node', '.', 'values', '[', 'i', ']', ')', '\\', 'n']
Detokenized (014): ['v', '=', 'self', '.', 'visit', '(', 'node', '.', 'values', '[', 'i', ']', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "props . update ( self . _class_props [ p ] ) \n"
Original    (012): ['props', '.', 'update', '(', 'self', '.', '_class_props', '[', 'p', ']', ')', '\\n']
Tokenized   (018): ['[CLS]', 'props', '.', 'update', '(', 'self', '.', '_', 'class', '_', 'props', '[', 'p', ']', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['props', '.', 'update', '(', 'self', '.', '_', 'class', '_', 'props', '[', 'p', ']', ')', '\\', 'n']
Detokenized (012): ['props', '.', 'update', '(', 'self', '.', '_class_props', '[', 'p', ']', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "kwargs_init = [ % ( x . split ( ) [ 0 ] , x . split ( ) [ 0 ] ) for x in kwargs ] \n"
Original    (029): ['kwargs_init', '=', '[', '%', '(', 'x', '.', 'split', '(', ')', '[', '0', ']', ',', 'x', '.', 'split', '(', ')', '[', '0', ']', ')', 'for', 'x', 'in', 'kwargs', ']', '\\n']
Tokenized   (039): ['[CLS]', 'kw', '##ar', '##gs', '_', 'in', '##it', '=', '[', '%', '(', 'x', '.', 'split', '(', ')', '[', '0', ']', ',', 'x', '.', 'split', '(', ')', '[', '0', ']', ')', 'for', 'x', 'in', 'kw', '##ar', '##gs', ']', '\\', 'n', '[SEP]']
Filtered   (037): ['kw', '##ar', '##gs', '_', 'in', '##it', '=', '[', '%', '(', 'x', '.', 'split', '(', ')', '[', '0', ']', ',', 'x', '.', 'split', '(', ')', '[', '0', ']', ')', 'for', 'x', 'in', 'kw', '##ar', '##gs', ']', '\\', 'n']
Detokenized (029): ['kw##ar##gs_in##it', '=', '[', '%', '(', 'x', '.', 'split', '(', ')', '[', '0', ']', ',', 'x', '.', 'split', '(', ')', '[', '0', ']', ')', 'for', 'x', 'in', 'kw##ar##gs', ']', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "else : nargs = node . args . args \n"
Original    (010): ['else', ':', 'nargs', '=', 'node', '.', 'args', '.', 'args', '\\n']
Tokenized   (017): ['[CLS]', 'else', ':', 'na', '##rg', '##s', '=', 'node', '.', 'ar', '##gs', '.', 'ar', '##gs', '\\', 'n', '[SEP]']
Filtered   (015): ['else', ':', 'na', '##rg', '##s', '=', 'node', '.', 'ar', '##gs', '.', 'ar', '##gs', '\\', 'n']
Detokenized (010): ['else', ':', 'na##rg##s', '=', 'node', '.', 'ar##gs', '.', 'ar##gs', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "kwargs . append ( % ( a , default_value ) ) \n"
Original    (012): ['kwargs', '.', 'append', '(', '%', '(', 'a', ',', 'default_value', ')', ')', '\\n']
Tokenized   (020): ['[CLS]', 'kw', '##ar', '##gs', '.', 'app', '##end', '(', '%', '(', 'a', ',', 'default', '_', 'value', ')', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['kw', '##ar', '##gs', '.', 'app', '##end', '(', '%', '(', 'a', ',', 'default', '_', 'value', ')', ')', '\\', 'n']
Detokenized (012): ['kw##ar##gs', '.', 'app##end', '(', '%', '(', 'a', ',', 'default_value', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "offset = len ( node . args . args ) - len ( node . args . defaults ) \n"
Original    (020): ['offset', '=', 'len', '(', 'node', '.', 'args', '.', 'args', ')', '-', 'len', '(', 'node', '.', 'args', '.', 'defaults', ')', '\\n']
Tokenized   (027): ['[CLS]', 'offset', '=', 'len', '(', 'node', '.', 'ar', '##gs', '.', 'ar', '##gs', ')', '-', 'len', '(', 'node', '.', 'ar', '##gs', '.', 'default', '##s', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['offset', '=', 'len', '(', 'node', '.', 'ar', '##gs', '.', 'ar', '##gs', ')', '-', 'len', '(', 'node', '.', 'ar', '##gs', '.', 'default', '##s', ')', '\\', 'n']
Detokenized (020): ['offset', '=', 'len', '(', 'node', '.', 'ar##gs', '.', 'ar##gs', ')', '-', 'len', '(', 'node', '.', 'ar##gs', '.', 'default##s', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "varargs = [ % n for n in range ( 16 ) ] \n"
Original    (014): ['varargs', '=', '[', '%', 'n', 'for', 'n', 'in', 'range', '(', '16', ')', ']', '\\n']
Tokenized   (019): ['[CLS]', 'var', '##ar', '##gs', '=', '[', '%', 'n', 'for', 'n', 'in', 'range', '(', '16', ')', ']', '\\', 'n', '[SEP]']
Filtered   (017): ['var', '##ar', '##gs', '=', '[', '%', 'n', 'for', 'n', 'in', 'range', '(', '16', ')', ']', '\\', 'n']
Detokenized (014): ['var##ar##gs', '=', '[', '%', 'n', 'for', 'n', 'in', 'range', '(', '16', ')', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "buffer += % self . indent ( ) \n"
Original    (009): ['buffer', '+=', '%', 'self', '.', 'indent', '(', ')', '\\n']
Tokenized   (014): ['[CLS]', 'buffer', '+', '=', '%', 'self', '.', 'ind', '##ent', '(', ')', '\\', 'n', '[SEP]']
Filtered   (012): ['buffer', '+', '=', '%', 'self', '.', 'ind', '##ent', '(', ')', '\\', 'n']
Detokenized (009): ['buffer', '+=', '%', 'self', '.', 'ind##ent', '(', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "arg_name = args = None \n"
Original    (006): ['arg_name', '=', 'args', '=', 'None', '\\n']
Tokenized   (013): ['[CLS]', 'ar', '##g', '_', 'name', '=', 'ar', '##gs', '=', 'none', '\\', 'n', '[SEP]']
Filtered   (011): ['ar', '##g', '_', 'name', '=', 'ar', '##gs', '=', 'none', '\\', 'n']
Detokenized (006): ['ar##g_name', '=', 'ar##gs', '=', 'none', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "comp . append ( self . visit ( node . comparators [ i ] ) ) \n"
Original    (017): ['comp', '.', 'append', '(', 'self', '.', 'visit', '(', 'node', '.', 'comparators', '[', 'i', ']', ')', ')', '\\n']
Tokenized   (024): ['[CLS]', 'com', '##p', '.', 'app', '##end', '(', 'self', '.', 'visit', '(', 'node', '.', 'com', '##para', '##tors', '[', 'i', ']', ')', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['com', '##p', '.', 'app', '##end', '(', 'self', '.', 'visit', '(', 'node', '.', 'com', '##para', '##tors', '[', 'i', ']', ')', ')', '\\', 'n']
Detokenized (017): ['com##p', '.', 'app##end', '(', 'self', '.', 'visit', '(', 'node', '.', 'com##para##tors', '[', 'i', ']', ')', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "testtime = time ( ) - starttime \n"
Original    (008): ['testtime', '=', 'time', '(', ')', '-', 'starttime', '\\n']
Tokenized   (013): ['[CLS]', 'test', '##time', '=', 'time', '(', ')', '-', 'start', '##time', '\\', 'n', '[SEP]']
Filtered   (011): ['test', '##time', '=', 'time', '(', ')', '-', 'start', '##time', '\\', 'n']
Detokenized (008): ['test##time', '=', 'time', '(', ')', '-', 'start##time', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "primes_per_sec = len ( seq ) * ( 1.0 / testtime ) \n"
Original    (013): ['primes_per_sec', '=', 'len', '(', 'seq', ')', '*', '(', '1.0', '/', 'testtime', ')', '\\n']
Tokenized   (025): ['[CLS]', 'prime', '##s', '_', 'per', '_', 'sec', '=', 'len', '(', 'se', '##q', ')', '*', '(', '1', '.', '0', '/', 'test', '##time', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['prime', '##s', '_', 'per', '_', 'sec', '=', 'len', '(', 'se', '##q', ')', '*', '(', '1', '.', '0', '/', 'test', '##time', ')', '\\', 'n']
Detokenized (013): ['prime##s_per_sec', '=', 'len', '(', 'se##q', ')', '*', '(', '1.0', '/', 'test##time', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "b = range ( 1 , 10 ) \n"
Original    (009): ['b', '=', 'range', '(', '1', ',', '10', ')', '\\n']
Tokenized   (012): ['[CLS]', 'b', '=', 'range', '(', '1', ',', '10', ')', '\\', 'n', '[SEP]']
Filtered   (010): ['b', '=', 'range', '(', '1', ',', '10', ')', '\\', 'n']
Detokenized (009): ['b', '=', 'range', '(', '1', ',', '10', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "w1 = threading . start_webworker ( worker , ( seq , , ) ) \n"
Original    (015): ['w1', '=', 'threading', '.', 'start_webworker', '(', 'worker', ',', '(', 'seq', ',', ',', ')', ')', '\\n']
Tokenized   (025): ['[CLS]', 'w', '##1', '=', 'thread', '##ing', '.', 'start', '_', 'web', '##work', '##er', '(', 'worker', ',', '(', 'se', '##q', ',', ',', ')', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['w', '##1', '=', 'thread', '##ing', '.', 'start', '_', 'web', '##work', '##er', '(', 'worker', ',', '(', 'se', '##q', ',', ',', ')', ')', '\\', 'n']
Detokenized (015): ['w##1', '=', 'thread##ing', '.', 'start_web##work##er', '(', 'worker', ',', '(', 'se##q', ',', ',', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "TestError ( in seq ) \n"
Original    (006): ['TestError', '(', 'in', 'seq', ')', '\\n']
Tokenized   (012): ['[CLS]', 'test', '##er', '##ror', '(', 'in', 'se', '##q', ')', '\\', 'n', '[SEP]']
Filtered   (010): ['test', '##er', '##ror', '(', 'in', 'se', '##q', ')', '\\', 'n']
Detokenized (006): ['test##er##ror', '(', 'in', 'se##q', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "del self . face_groups [ : ] \n"
Original    (008): ['del', 'self', '.', 'face_groups', '[', ':', ']', '\\n']
Tokenized   (013): ['[CLS]', 'del', 'self', '.', 'face', '_', 'groups', '[', ':', ']', '\\', 'n', '[SEP]']
Filtered   (011): ['del', 'self', '.', 'face', '_', 'groups', '[', ':', ']', '\\', 'n']
Detokenized (008): ['del', 'self', '.', 'face_groups', '[', ':', ']', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "mtllib_path = os . path . join ( model_path , data [ 0 ] ) \n"
Original    (016): ['mtllib_path', '=', 'os', '.', 'path', '.', 'join', '(', 'model_path', ',', 'data', '[', '0', ']', ')', '\\n']
Tokenized   (025): ['[CLS]', 'mt', '##lli', '##b', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'model', '_', 'path', ',', 'data', '[', '0', ']', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['mt', '##lli', '##b', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'model', '_', 'path', ',', 'data', '[', '0', ']', ')', '\\', 'n']
Detokenized (016): ['mt##lli##b_path', '=', 'os', '.', 'path', '.', 'join', '(', 'model_path', ',', 'data', '[', '0', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "vertex = ( float ( x ) , float ( y ) , float ( z ) ) \n"
Original    (019): ['vertex', '=', '(', 'float', '(', 'x', ')', ',', 'float', '(', 'y', ')', ',', 'float', '(', 'z', ')', ')', '\\n']
Tokenized   (022): ['[CLS]', 'vertex', '=', '(', 'float', '(', 'x', ')', ',', 'float', '(', 'y', ')', ',', 'float', '(', 'z', ')', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['vertex', '=', '(', 'float', '(', 'x', ')', ',', 'float', '(', 'y', ')', ',', 'float', '(', 'z', ')', ')', '\\', 'n']
Detokenized (019): ['vertex', '=', '(', 'float', '(', 'x', ')', ',', 'float', '(', 'y', ')', ',', 'float', '(', 'z', ')', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "tex_coord = ( float ( s ) , float ( t ) ) \n"
Original    (014): ['tex_coord', '=', '(', 'float', '(', 's', ')', ',', 'float', '(', 't', ')', ')', '\\n']
Tokenized   (020): ['[CLS]', 'tex', '_', 'co', '##ord', '=', '(', 'float', '(', 's', ')', ',', 'float', '(', 't', ')', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['tex', '_', 'co', '##ord', '=', '(', 'float', '(', 's', ')', ',', 'float', '(', 't', ')', ')', '\\', 'n']
Detokenized (014): ['tex_co##ord', '=', '(', 'float', '(', 's', ')', ',', 'float', '(', 't', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "indices = ( int ( vi ) - 1 , int ( ti ) - 1 , int ( ni ) - 1 ) \n"
Original    (025): ['indices', '=', '(', 'int', '(', 'vi', ')', '-', '1', ',', 'int', '(', 'ti', ')', '-', '1', ',', 'int', '(', 'ni', ')', '-', '1', ')', '\\n']
Tokenized   (028): ['[CLS]', 'indices', '=', '(', 'int', '(', 'vi', ')', '-', '1', ',', 'int', '(', 'ti', ')', '-', '1', ',', 'int', '(', 'ni', ')', '-', '1', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['indices', '=', '(', 'int', '(', 'vi', ')', '-', '1', ',', 'int', '(', 'ti', ')', '-', '1', ',', 'int', '(', 'ni', ')', '-', '1', ')', '\\', 'n']
Detokenized (025): ['indices', '=', '(', 'int', '(', 'vi', ')', '-', '1', ',', 'int', '(', 'ti', ')', '-', '1', ',', 'int', '(', 'ni', ')', '-', '1', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "glBindTexture ( GL_TEXTURE_2D , material . texture_id ) \n"
Original    (009): ['glBindTexture', '(', 'GL_TEXTURE_2D', ',', 'material', '.', 'texture_id', ')', '\\n']
Tokenized   (023): ['[CLS]', 'g', '##lb', '##ind', '##text', '##ure', '(', 'g', '##l', '_', 'texture', '_', '2d', ',', 'material', '.', 'texture', '_', 'id', ')', '\\', 'n', '[SEP]']
Filtered   (021): ['g', '##lb', '##ind', '##text', '##ure', '(', 'g', '##l', '_', 'texture', '_', '2d', ',', 'material', '.', 'texture', '_', 'id', ')', '\\', 'n']
Detokenized (009): ['g##lb##ind##text##ure', '(', 'g##l_texture_2d', ',', 'material', '.', 'texture_id', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "glPixelStorei ( GL_UNPACK_ALIGNMENT , 1 ) \n"
Original    (007): ['glPixelStorei', '(', 'GL_UNPACK_ALIGNMENT', ',', '1', ')', '\\n']
Tokenized   (021): ['[CLS]', 'g', '##lp', '##ix', '##els', '##tore', '##i', '(', 'g', '##l', '_', 'un', '##pack', '_', 'alignment', ',', '1', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['g', '##lp', '##ix', '##els', '##tore', '##i', '(', 'g', '##l', '_', 'un', '##pack', '_', 'alignment', ',', '1', ')', '\\', 'n']
Detokenized (007): ['g##lp##ix##els##tore##i', '(', 'g##l_un##pack_alignment', ',', '1', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "glNormal3fv ( normals [ ni ] ) \n"
Original    (008): ['glNormal3fv', '(', 'normals', '[', 'ni', ']', ')', '\\n']
Tokenized   (018): ['[CLS]', 'g', '##ln', '##or', '##mal', '##3', '##f', '##v', '(', 'normal', '##s', '[', 'ni', ']', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['g', '##ln', '##or', '##mal', '##3', '##f', '##v', '(', 'normal', '##s', '[', 'ni', ']', ')', '\\', 'n']
Detokenized (008): ['g##ln##or##mal##3##f##v', '(', 'normal##s', '[', 'ni', ']', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "picture = pygame . image . load ( picture_file ) . convert ( ) \n"
Original    (015): ['picture', '=', 'pygame', '.', 'image', '.', 'load', '(', 'picture_file', ')', '.', 'convert', '(', ')', '\\n']
Tokenized   (022): ['[CLS]', 'picture', '=', 'p', '##y', '##game', '.', 'image', '.', 'load', '(', 'picture', '_', 'file', ')', '.', 'convert', '(', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['picture', '=', 'p', '##y', '##game', '.', 'image', '.', 'load', '(', 'picture', '_', 'file', ')', '.', 'convert', '(', ')', '\\', 'n']
Detokenized (015): ['picture', '=', 'p##y##game', '.', 'image', '.', 'load', '(', 'picture_file', ')', '.', 'convert', '(', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "screen . blit ( picture , ( - picture_pos . x , picture_pos . y ) ) \n"
Original    (018): ['screen', '.', 'blit', '(', 'picture', ',', '(', '-', 'picture_pos', '.', 'x', ',', 'picture_pos', '.', 'y', ')', ')', '\\n']
Tokenized   (028): ['[CLS]', 'screen', '.', 'b', '##lit', '(', 'picture', ',', '(', '-', 'picture', '_', 'po', '##s', '.', 'x', ',', 'picture', '_', 'po', '##s', '.', 'y', ')', ')', '\\', 'n', '[SEP]']
Filtered   (026): ['screen', '.', 'b', '##lit', '(', 'picture', ',', '(', '-', 'picture', '_', 'po', '##s', '.', 'x', ',', 'picture', '_', 'po', '##s', '.', 'y', ')', ')', '\\', 'n']
Detokenized (018): ['screen', '.', 'b##lit', '(', 'picture', ',', '(', '-', 'picture_po##s', '.', 'x', ',', 'picture_po##s', '.', 'y', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "time_passed_seconds = time_passed / 1000.0 \n"
Original    (006): ['time_passed_seconds', '=', 'time_passed', '/', '1000.0', '\\n']
Tokenized   (017): ['[CLS]', 'time', '_', 'passed', '_', 'seconds', '=', 'time', '_', 'passed', '/', '1000', '.', '0', '\\', 'n', '[SEP]']
Filtered   (015): ['time', '_', 'passed', '_', 'seconds', '=', 'time', '_', 'passed', '/', '1000', '.', '0', '\\', 'n']
Detokenized (006): ['time_passed_seconds', '=', 'time_passed', '/', '1000.0', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "picture_pos += scroll_direction * scroll_speed * time_passed_seconds \n"
Original    (008): ['picture_pos', '+=', 'scroll_direction', '*', 'scroll_speed', '*', 'time_passed_seconds', '\\n']
Tokenized   (023): ['[CLS]', 'picture', '_', 'po', '##s', '+', '=', 'scroll', '_', 'direction', '*', 'scroll', '_', 'speed', '*', 'time', '_', 'passed', '_', 'seconds', '\\', 'n', '[SEP]']
Filtered   (021): ['picture', '_', 'po', '##s', '+', '=', 'scroll', '_', 'direction', '*', 'scroll', '_', 'speed', '*', 'time', '_', 'passed', '_', 'seconds', '\\', 'n']
Detokenized (008): ['picture_po##s', '+=', 'scroll_direction', '*', 'scroll_speed', '*', 'time_passed_seconds', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "VERSION = open ( "version.txt" ) . readline ( ) . strip ( ) \n"
Original    (015): ['VERSION', '=', 'open', '(', '"version.txt"', ')', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '\\n']
Tokenized   (024): ['[CLS]', 'version', '=', 'open', '(', '"', 'version', '.', 'tx', '##t', '"', ')', '.', 'read', '##line', '(', ')', '.', 'strip', '(', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['version', '=', 'open', '(', '"', 'version', '.', 'tx', '##t', '"', ')', '.', 'read', '##line', '(', ')', '.', 'strip', '(', ')', '\\', 'n']
Detokenized (015): ['version', '=', 'open', '(', '"version.tx##t"', ')', '.', 'read##line', '(', ')', '.', 'strip', '(', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "DOWNLOAD_URL = DOWNLOAD_BASEURL + "dubbo-client-%s-py2.7.egg" % VERSION \n"
Original    (008): ['DOWNLOAD_URL', '=', 'DOWNLOAD_BASEURL', '+', '"dubbo-client-%s-py2.7.egg"', '%', 'VERSION', '\\n']
Tokenized   (034): ['[CLS]', 'download', '_', 'ur', '##l', '=', 'download', '_', 'base', '##ur', '##l', '+', '"', 'dub', '##bo', '-', 'client', '-', '%', 's', '-', 'p', '##y', '##2', '.', '7', '.', 'egg', '"', '%', 'version', '\\', 'n', '[SEP]']
Filtered   (032): ['download', '_', 'ur', '##l', '=', 'download', '_', 'base', '##ur', '##l', '+', '"', 'dub', '##bo', '-', 'client', '-', '%', 's', '-', 'p', '##y', '##2', '.', '7', '.', 'egg', '"', '%', 'version', '\\', 'n']
Detokenized (008): ['download_ur##l', '=', 'download_base##ur##l', '+', '"dub##bo-client-%s-p##y##2.7.egg"', '%', 'version', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "long_description = open ( "README.md" ) . read ( ) , \n"
Original    (012): ['long_description', '=', 'open', '(', '"README.md"', ')', '.', 'read', '(', ')', ',', '\\n']
Tokenized   (022): ['[CLS]', 'long', '_', 'description', '=', 'open', '(', '"', 'read', '##me', '.', 'md', '"', ')', '.', 'read', '(', ')', ',', '\\', 'n', '[SEP]']
Filtered   (020): ['long', '_', 'description', '=', 'open', '(', '"', 'read', '##me', '.', 'md', '"', ')', '.', 'read', '(', ')', ',', '\\', 'n']
Detokenized (012): ['long_description', '=', 'open', '(', '"read##me.md"', ')', '.', 'read', '(', ')', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "install_requires = [ "kazoo>=2.0" , "python-jsonrpc>=0.7.3" ] , \n"
Original    (009): ['install_requires', '=', '[', '"kazoo>=2.0"', ',', '"python-jsonrpc>=0.7.3"', ']', ',', '\\n']
Tokenized   (036): ['[CLS]', 'install', '_', 'requires', '=', '[', '"', 'ka', '##zoo', '>', '=', '2', '.', '0', '"', ',', '"', 'python', '-', 'j', '##son', '##rp', '##c', '>', '=', '0', '.', '7', '.', '3', '"', ']', ',', '\\', 'n', '[SEP]']
Filtered   (034): ['install', '_', 'requires', '=', '[', '"', 'ka', '##zoo', '>', '=', '2', '.', '0', '"', ',', '"', 'python', '-', 'j', '##son', '##rp', '##c', '>', '=', '0', '.', '7', '.', '3', '"', ']', ',', '\\', 'n']
Detokenized (009): ['install_requires', '=', '[', '"ka##zoo>=2.0"', ',', '"python-j##son##rp##c>=0.7.3"', ']', ',', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "field = models . BooleanField ( default = False ) , \n"
Original    (012): ['field', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ')', ',', '\\n']
Tokenized   (017): ['[CLS]', 'field', '=', 'models', '.', 'boo', '##lean', '##field', '(', 'default', '=', 'false', ')', ',', '\\', 'n', '[SEP]']
Filtered   (015): ['field', '=', 'models', '.', 'boo', '##lean', '##field', '(', 'default', '=', 'false', ')', ',', '\\', 'n']
Detokenized (012): ['field', '=', 'models', '.', 'boo##lean##field', '(', 'default', '=', 'false', ')', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "args = [ in_path , user_out_path ] , env = [ "PATH=" + os . environ . get ( "PATH" , "" ) use_sandbox = True , use_nobody = True ) \n"
Original    (032): ['args', '=', '[', 'in_path', ',', 'user_out_path', ']', ',', 'env', '=', '[', '"PATH="', '+', 'os', '.', 'environ', '.', 'get', '(', '"PATH"', ',', '""', ')', 'use_sandbox', '=', 'True', ',', 'use_nobody', '=', 'True', ')', '\\n']
Tokenized   (056): ['[CLS]', 'ar', '##gs', '=', '[', 'in', '_', 'path', ',', 'user', '_', 'out', '_', 'path', ']', ',', 'en', '##v', '=', '[', '"', 'path', '=', '"', '+', 'os', '.', 'en', '##vir', '##on', '.', 'get', '(', '"', 'path', '"', ',', '"', '"', ')', 'use', '_', 'sand', '##box', '=', 'true', ',', 'use', '_', 'nobody', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (054): ['ar', '##gs', '=', '[', 'in', '_', 'path', ',', 'user', '_', 'out', '_', 'path', ']', ',', 'en', '##v', '=', '[', '"', 'path', '=', '"', '+', 'os', '.', 'en', '##vir', '##on', '.', 'get', '(', '"', 'path', '"', ',', '"', '"', ')', 'use', '_', 'sand', '##box', '=', 'true', ',', 'use', '_', 'nobody', '=', 'true', ')', '\\', 'n']
Detokenized (032): ['ar##gs', '=', '[', 'in_path', ',', 'user_out_path', ']', ',', 'en##v', '=', '[', '"path="', '+', 'os', '.', 'en##vir##on', '.', 'get', '(', '"path"', ',', '""', ')', 'use_sand##box', '=', 'true', ',', 'use_nobody', '=', 'true', ')', '\\n']
Counter: 54
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "print_skip = 5 , * args , ** kwargs ) : \n"
Original    (012): ['print_skip', '=', '5', ',', '*', 'args', ',', '**', 'kwargs', ')', ':', '\\n']
Tokenized   (021): ['[CLS]', 'print', '_', 'skip', '=', '5', ',', '*', 'ar', '##gs', ',', '*', '*', 'kw', '##ar', '##gs', ')', ':', '\\', 'n', '[SEP]']
Filtered   (019): ['print', '_', 'skip', '=', '5', ',', '*', 'ar', '##gs', ',', '*', '*', 'kw', '##ar', '##gs', ')', ':', '\\', 'n']
Detokenized (012): ['print_skip', '=', '5', ',', '*', 'ar##gs', ',', '**', 'kw##ar##gs', ')', ':', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "error = np . max ( np . abs ( new_v - v ) ) \n"
Original    (016): ['error', '=', 'np', '.', 'max', '(', 'np', '.', 'abs', '(', 'new_v', '-', 'v', ')', ')', '\\n']
Tokenized   (021): ['[CLS]', 'error', '=', 'np', '.', 'max', '(', 'np', '.', 'abs', '(', 'new', '_', 'v', '-', 'v', ')', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['error', '=', 'np', '.', 'max', '(', 'np', '.', 'abs', '(', 'new', '_', 'v', '-', 'v', ')', ')', '\\', 'n']
Detokenized (016): ['error', '=', 'np', '.', 'max', '(', 'np', '.', 'abs', '(', 'new_v', '-', 'v', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "ac = ( a_0 - c ) / 2.0 \n"
Original    (010): ['ac', '=', '(', 'a_0', '-', 'c', ')', '/', '2.0', '\\n']
Tokenized   (017): ['[CLS]', 'ac', '=', '(', 'a', '_', '0', '-', 'c', ')', '/', '2', '.', '0', '\\', 'n', '[SEP]']
Filtered   (015): ['ac', '=', '(', 'a', '_', '0', '-', 'c', ')', '/', '2', '.', '0', '\\', 'n']
Detokenized (010): ['ac', '=', '(', 'a_0', '-', 'c', ')', '/', '2.0', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "R = - R \n"
Original    (005): ['R', '=', '-', 'R', '\\n']
Tokenized   (008): ['[CLS]', 'r', '=', '-', 'r', '\\', 'n', '[SEP]']
Filtered   (006): ['r', '=', '-', 'r', '\\', 'n']
Detokenized (005): ['r', '=', '-', 'r', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "B = np . array ( [ [ 0. ] , \n"
Original    (012): ['B', '=', 'np', '.', 'array', '(', '[', '[', '0.', ']', ',', '\\n']
Tokenized   (016): ['[CLS]', 'b', '=', 'np', '.', 'array', '(', '[', '[', '0', '.', ']', ',', '\\', 'n', '[SEP]']
Filtered   (014): ['b', '=', 'np', '.', 'array', '(', '[', '[', '0', '.', ']', ',', '\\', 'n']
Detokenized (012): ['b', '=', 'np', '.', 'array', '(', '[', '[', '0.', ']', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "Fr , Kr , Pr = self . Fr , self . Kr , self . Pr \n"
Original    (018): ['Fr', ',', 'Kr', ',', 'Pr', '=', 'self', '.', 'Fr', ',', 'self', '.', 'Kr', ',', 'self', '.', 'Pr', '\\n']
Tokenized   (023): ['[CLS]', 'fr', ',', 'k', '##r', ',', 'pr', '=', 'self', '.', 'fr', ',', 'self', '.', 'k', '##r', ',', 'self', '.', 'pr', '\\', 'n', '[SEP]']
Filtered   (021): ['fr', ',', 'k', '##r', ',', 'pr', '=', 'self', '.', 'fr', ',', 'self', '.', 'k', '##r', ',', 'self', '.', 'pr', '\\', 'n']
Detokenized (018): ['fr', ',', 'k##r', ',', 'pr', '=', 'self', '.', 'fr', ',', 'self', '.', 'k##r', ',', 'self', '.', 'pr', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "Fs , Ks , Ps = rblq . robust_rule_simple ( P_init = Pr , tol = 1e-12 ) \n"
Original    (019): ['Fs', ',', 'Ks', ',', 'Ps', '=', 'rblq', '.', 'robust_rule_simple', '(', 'P_init', '=', 'Pr', ',', 'tol', '=', '1e-12', ')', '\\n']
Tokenized   (036): ['[CLS]', 'f', '##s', ',', 'ks', ',', 'ps', '=', 'rb', '##l', '##q', '.', 'robust', '_', 'rule', '_', 'simple', '(', 'p', '_', 'in', '##it', '=', 'pr', ',', 'to', '##l', '=', '1', '##e', '-', '12', ')', '\\', 'n', '[SEP]']
Filtered   (034): ['f', '##s', ',', 'ks', ',', 'ps', '=', 'rb', '##l', '##q', '.', 'robust', '_', 'rule', '_', 'simple', '(', 'p', '_', 'in', '##it', '=', 'pr', ',', 'to', '##l', '=', '1', '##e', '-', '12', ')', '\\', 'n']
Detokenized (019): ['f##s', ',', 'ks', ',', 'ps', '=', 'rb##l##q', '.', 'robust_rule_simple', '(', 'p_in##it', '=', 'pr', ',', 'to##l', '=', '1##e-12', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "Kf , Pf , df , Of , of = rblq . evaluate_F ( Fr ) \n"
Original    (017): ['Kf', ',', 'Pf', ',', 'df', ',', 'Of', ',', 'of', '=', 'rblq', '.', 'evaluate_F', '(', 'Fr', ')', '\\n']
Tokenized   (027): ['[CLS]', 'k', '##f', ',', 'p', '##f', ',', 'd', '##f', ',', 'of', ',', 'of', '=', 'rb', '##l', '##q', '.', 'evaluate', '_', 'f', '(', 'fr', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['k', '##f', ',', 'p', '##f', ',', 'd', '##f', ',', 'of', ',', 'of', '=', 'rb', '##l', '##q', '.', 'evaluate', '_', 'f', '(', 'fr', ')', '\\', 'n']
Detokenized (017): ['k##f', ',', 'p##f', ',', 'd##f', ',', 'of', ',', 'of', '=', 'rb##l##q', '.', 'evaluate_f', '(', 'fr', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "group = h5f . createGroup ( "/" , ) \n"
Original    (010): ['group', '=', 'h5f', '.', 'createGroup', '(', '"/"', ',', ')', '\\n']
Tokenized   (018): ['[CLS]', 'group', '=', 'h', '##5', '##f', '.', 'create', '##group', '(', '"', '/', '"', ',', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['group', '=', 'h', '##5', '##f', '.', 'create', '##group', '(', '"', '/', '"', ',', ')', '\\', 'n']
Detokenized (010): ['group', '=', 'h##5##f', '.', 'create##group', '(', '"/"', ',', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "global ctr \n"
Original    (003): ['global', 'ctr', '\\n']
Tokenized   (007): ['[CLS]', 'global', 'ct', '##r', '\\', 'n', '[SEP]']
Filtered   (005): ['global', 'ct', '##r', '\\', 'n']
Detokenized (003): ['global', 'ct##r', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "listOfInputPaths . append ( rootdir + "/Raw/Yahoo/US/NYSE/" ) \n"
Original    (009): ['listOfInputPaths', '.', 'append', '(', 'rootdir', '+', '"/Raw/Yahoo/US/NYSE/"', ')', '\\n']
Tokenized   (031): ['[CLS]', 'list', '##of', '##in', '##put', '##path', '##s', '.', 'app', '##end', '(', 'root', '##di', '##r', '+', '"', '/', 'raw', '/', 'yahoo', '/', 'us', '/', 'ny', '##se', '/', '"', ')', '\\', 'n', '[SEP]']
Filtered   (029): ['list', '##of', '##in', '##put', '##path', '##s', '.', 'app', '##end', '(', 'root', '##di', '##r', '+', '"', '/', 'raw', '/', 'yahoo', '/', 'us', '/', 'ny', '##se', '/', '"', ')', '\\', 'n']
Detokenized (009): ['list##of##in##put##path##s', '.', 'app##end', '(', 'root##di##r', '+', '"/raw/yahoo/us/ny##se/"', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "filtered_names = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , filtered_names ) \n"
Original    (025): ['filtered_names', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'fileExtensionToRemove', ')', ')', '[', '0', ']', ')', ',', 'filtered_names', ')', '\\n']
Tokenized   (038): ['[CLS]', 'filtered', '_', 'names', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'st', '##r', '(', 'file', '##ex', '##tension', '##tore', '##mo', '##ve', ')', ')', '[', '0', ']', ')', ',', 'filtered', '_', 'names', ')', '\\', 'n', '[SEP]']
Filtered   (036): ['filtered', '_', 'names', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'st', '##r', '(', 'file', '##ex', '##tension', '##tore', '##mo', '##ve', ')', ')', '[', '0', ']', ')', ',', 'filtered', '_', 'names', ')', '\\', 'n']
Detokenized (025): ['filtered_names', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'st##r', '(', 'file##ex##tension##tore##mo##ve', ')', ')', '[', '0', ']', ')', ',', 'filtered_names', ')', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "stock_data = np . loadtxt ( path + stock + ".csv" , np . float , None , "," , None , 1 , use_cols ) \n"
Original    (027): ['stock_data', '=', 'np', '.', 'loadtxt', '(', 'path', '+', 'stock', '+', '".csv"', ',', 'np', '.', 'float', ',', 'None', ',', '","', ',', 'None', ',', '1', ',', 'use_cols', ')', '\\n']
Tokenized   (043): ['[CLS]', 'stock', '_', 'data', '=', 'np', '.', 'load', '##t', '##xt', '(', 'path', '+', 'stock', '+', '"', '.', 'cs', '##v', '"', ',', 'np', '.', 'float', ',', 'none', ',', '"', ',', '"', ',', 'none', ',', '1', ',', 'use', '_', 'col', '##s', ')', '\\', 'n', '[SEP]']
Filtered   (041): ['stock', '_', 'data', '=', 'np', '.', 'load', '##t', '##xt', '(', 'path', '+', 'stock', '+', '"', '.', 'cs', '##v', '"', ',', 'np', '.', 'float', ',', 'none', ',', '"', ',', '"', ',', 'none', ',', '1', ',', 'use', '_', 'col', '##s', ')', '\\', 'n']
Detokenized (027): ['stock_data', '=', 'np', '.', 'load##t##xt', '(', 'path', '+', 'stock', '+', '".cs##v"', ',', 'np', '.', 'float', ',', 'none', ',', '","', ',', 'none', ',', '1', ',', 'use_col##s', ')', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "pkl . dump ( stock_data , f , - 1 ) \n"
Original    (012): ['pkl', '.', 'dump', '(', 'stock_data', ',', 'f', ',', '-', '1', ')', '\\n']
Tokenized   (019): ['[CLS]', 'p', '##k', '##l', '.', 'dump', '(', 'stock', '_', 'data', ',', 'f', ',', '-', '1', ')', '\\', 'n', '[SEP]']
Filtered   (017): ['p', '##k', '##l', '.', 'dump', '(', 'stock', '_', 'data', ',', 'f', ',', '-', '1', ')', '\\', 'n']
Detokenized (012): ['p##k##l', '.', 'dump', '(', 'stock_data', ',', 'f', ',', '-', '1', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "startday = dt . datetime ( t [ 2 ] , t [ 0 ] , t [ 1 ] ) \n"
Original    (022): ['startday', '=', 'dt', '.', 'datetime', '(', 't', '[', '2', ']', ',', 't', '[', '0', ']', ',', 't', '[', '1', ']', ')', '\\n']
Tokenized   (027): ['[CLS]', 'start', '##day', '=', 'dt', '.', 'date', '##time', '(', 't', '[', '2', ']', ',', 't', '[', '0', ']', ',', 't', '[', '1', ']', ')', '\\', 'n', '[SEP]']
Filtered   (025): ['start', '##day', '=', 'dt', '.', 'date', '##time', '(', 't', '[', '2', ']', ',', 't', '[', '0', ']', ',', 't', '[', '1', ']', ')', '\\', 'n']
Detokenized (022): ['start##day', '=', 'dt', '.', 'date##time', '(', 't', '[', '2', ']', ',', 't', '[', '0', ']', ',', 't', '[', '1', ']', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "t = map ( int , sys . argv [ 2 ] . split ( ) ) \n"
Original    (018): ['t', '=', 'map', '(', 'int', ',', 'sys', '.', 'argv', '[', '2', ']', '.', 'split', '(', ')', ')', '\\n']
Tokenized   (024): ['[CLS]', 't', '=', 'map', '(', 'int', ',', 'sy', '##s', '.', 'ar', '##g', '##v', '[', '2', ']', '.', 'split', '(', ')', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['t', '=', 'map', '(', 'int', ',', 'sy', '##s', '.', 'ar', '##g', '##v', '[', '2', ']', '.', 'split', '(', ')', ')', '\\', 'n']
Detokenized (018): ['t', '=', 'map', '(', 'int', ',', 'sy##s', '.', 'ar##g##v', '[', '2', ']', '.', 'split', '(', ')', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "historic = dataobj . get_data ( timestamps , symbols , "close" ) \n"
Original    (013): ['historic', '=', 'dataobj', '.', 'get_data', '(', 'timestamps', ',', 'symbols', ',', '"close"', ')', '\\n']
Tokenized   (024): ['[CLS]', 'historic', '=', 'data', '##ob', '##j', '.', 'get', '_', 'data', '(', 'times', '##tam', '##ps', ',', 'symbols', ',', '"', 'close', '"', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['historic', '=', 'data', '##ob', '##j', '.', 'get', '_', 'data', '(', 'times', '##tam', '##ps', ',', 'symbols', ',', '"', 'close', '"', ')', '\\', 'n']
Detokenized (013): ['historic', '=', 'data##ob##j', '.', 'get_data', '(', 'times##tam##ps', ',', 'symbols', ',', '"close"', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "alloc = alloc . append ( DataMatrix ( index = [ historic . index [ date ] ] , data = [ alloc_val ] , columns = [ symbols ~~ alloc [ ] = 1 - alloc [ symbols [ 0 ] ] \n"
Original    (044): ['alloc', '=', 'alloc', '.', 'append', '(', 'DataMatrix', '(', 'index', '=', '[', 'historic', '.', 'index', '[', 'date', ']', ']', ',', 'data', '=', '[', 'alloc_val', ']', ',', 'columns', '=', '[', 'symbols', '~~', 'alloc', '[', ']', '=', '1', '-', 'alloc', '[', 'symbols', '[', '0', ']', ']', '\\n']
Tokenized   (058): ['[CLS]', 'all', '##oc', '=', 'all', '##oc', '.', 'app', '##end', '(', 'data', '##mat', '##rix', '(', 'index', '=', '[', 'historic', '.', 'index', '[', 'date', ']', ']', ',', 'data', '=', '[', 'all', '##oc', '_', 'val', ']', ',', 'columns', '=', '[', 'symbols', '~', '~', 'all', '##oc', '[', ']', '=', '1', '-', 'all', '##oc', '[', 'symbols', '[', '0', ']', ']', '\\', 'n', '[SEP]']
Filtered   (056): ['all', '##oc', '=', 'all', '##oc', '.', 'app', '##end', '(', 'data', '##mat', '##rix', '(', 'index', '=', '[', 'historic', '.', 'index', '[', 'date', ']', ']', ',', 'data', '=', '[', 'all', '##oc', '_', 'val', ']', ',', 'columns', '=', '[', 'symbols', '~', '~', 'all', '##oc', '[', ']', '=', '1', '-', 'all', '##oc', '[', 'symbols', '[', '0', ']', ']', '\\', 'n']
Detokenized (044): ['all##oc', '=', 'all##oc', '.', 'app##end', '(', 'data##mat##rix', '(', 'index', '=', '[', 'historic', '.', 'index', '[', 'date', ']', ']', ',', 'data', '=', '[', 'all##oc_val', ']', ',', 'columns', '=', '[', 'symbols', '~~', 'all##oc', '[', ']', '=', '1', '-', 'all##oc', '[', 'symbols', '[', '0', ']', ']', '\\n']
Counter: 56
===================================================================
Hidden states:  (13, 44, 768)
# Extracted words:  44
Sentence         : "output = open ( sys . argv [ 3 ] , "wb" ) \n"
Original    (014): ['output', '=', 'open', '(', 'sys', '.', 'argv', '[', '3', ']', ',', '"wb"', ')', '\\n']
Tokenized   (022): ['[CLS]', 'output', '=', 'open', '(', 'sy', '##s', '.', 'ar', '##g', '##v', '[', '3', ']', ',', '"', 'wb', '"', ')', '\\', 'n', '[SEP]']
Filtered   (020): ['output', '=', 'open', '(', 'sy', '##s', '.', 'ar', '##g', '##v', '[', '3', ']', ',', '"', 'wb', '"', ')', '\\', 'n']
Detokenized (014): ['output', '=', 'open', '(', 'sy##s', '.', 'ar##g##v', '[', '3', ']', ',', '"wb"', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "stocksAtThisPath = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , stocksAtThisPath \n"
Original    (024): ['stocksAtThisPath', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'fileExtensionToRemove', ')', ')', '[', '0', ']', ')', ',', 'stocksAtThisPath', '\\n']
Tokenized   (039): ['[CLS]', 'stocks', '##att', '##his', '##path', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'st', '##r', '(', 'file', '##ex', '##tension', '##tore', '##mo', '##ve', ')', ')', '[', '0', ']', ')', ',', 'stocks', '##att', '##his', '##path', '\\', 'n', '[SEP]']
Filtered   (037): ['stocks', '##att', '##his', '##path', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'st', '##r', '(', 'file', '##ex', '##tension', '##tore', '##mo', '##ve', ')', ')', '[', '0', ']', ')', ',', 'stocks', '##att', '##his', '##path', '\\', 'n']
Detokenized (024): ['stocks##att##his##path', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'st##r', '(', 'file##ex##tension##tore##mo##ve', ')', ')', '[', '0', ']', ')', ',', 'stocks##att##his##path', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "@ memoize_default ( None , evaluator_is_first_arg = True ) \n"
Original    (010): ['@', 'memoize_default', '(', 'None', ',', 'evaluator_is_first_arg', '=', 'True', ')', '\\n']
Tokenized   (025): ['[CLS]', '@', 'memo', '##ize', '_', 'default', '(', 'none', ',', 'eva', '##lu', '##ator', '_', 'is', '_', 'first', '_', 'ar', '##g', '=', 'true', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['@', 'memo', '##ize', '_', 'default', '(', 'none', ',', 'eva', '##lu', '##ator', '_', 'is', '_', 'first', '_', 'ar', '##g', '=', 'true', ')', '\\', 'n']
Detokenized (010): ['@', 'memo##ize_default', '(', 'none', ',', 'eva##lu##ator_is_first_ar##g', '=', 'true', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "param_str = _search_param_in_docstr ( func . raw_doc , str ( param . get_name ( ) ) ) \n"
Original    (018): ['param_str', '=', '_search_param_in_docstr', '(', 'func', '.', 'raw_doc', ',', 'str', '(', 'param', '.', 'get_name', '(', ')', ')', ')', '\\n']
Tokenized   (042): ['[CLS]', 'para', '##m', '_', 'st', '##r', '=', '_', 'search', '_', 'para', '##m', '_', 'in', '_', 'doc', '##st', '##r', '(', 'fun', '##c', '.', 'raw', '_', 'doc', ',', 'st', '##r', '(', 'para', '##m', '.', 'get', '_', 'name', '(', ')', ')', ')', '\\', 'n', '[SEP]']
Filtered   (040): ['para', '##m', '_', 'st', '##r', '=', '_', 'search', '_', 'para', '##m', '_', 'in', '_', 'doc', '##st', '##r', '(', 'fun', '##c', '.', 'raw', '_', 'doc', ',', 'st', '##r', '(', 'para', '##m', '.', 'get', '_', 'name', '(', ')', ')', ')', '\\', 'n']
Detokenized (018): ['para##m_st##r', '=', '_search_para##m_in_doc##st##r', '(', 'fun##c', '.', 'raw_doc', ',', 'st##r', '(', 'para##m', '.', 'get_name', '(', ')', ')', ')', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "patterns = [ re . compile ( p % re . escape ( param_str ) ) \n"
Original    (017): ['patterns', '=', '[', 're', '.', 'compile', '(', 'p', '%', 're', '.', 'escape', '(', 'param_str', ')', ')', '\\n']
Tokenized   (025): ['[CLS]', 'patterns', '=', '[', 're', '.', 'com', '##pile', '(', 'p', '%', 're', '.', 'escape', '(', 'para', '##m', '_', 'st', '##r', ')', ')', '\\', 'n', '[SEP]']
Filtered   (023): ['patterns', '=', '[', 're', '.', 'com', '##pile', '(', 'p', '%', 're', '.', 'escape', '(', 'para', '##m', '_', 'st', '##r', ')', ')', '\\', 'n']
Detokenized (017): ['patterns', '=', '[', 're', '.', 'com##pile', '(', 'p', '%', 're', '.', 'escape', '(', 'para##m_st##r', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "it = ( evaluator . execute ( d ) for d in definitions ) \n"
Original    (015): ['it', '=', '(', 'evaluator', '.', 'execute', '(', 'd', ')', 'for', 'd', 'in', 'definitions', ')', '\\n']
Tokenized   (020): ['[CLS]', 'it', '=', '(', 'eva', '##lu', '##ator', '.', 'execute', '(', 'd', ')', 'for', 'd', 'in', 'definitions', ')', '\\', 'n', '[SEP]']
Filtered   (018): ['it', '=', '(', 'eva', '##lu', '##ator', '.', 'execute', '(', 'd', ')', 'for', 'd', 'in', 'definitions', ')', '\\', 'n']
Detokenized (015): ['it', '=', '(', 'eva##lu##ator', '.', 'execute', '(', 'd', ')', 'for', 'd', 'in', 'definitions', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "tok = parsed . module . subscopes [ 0 ] . statements [ 0 ] . _token_list [ 2 ] \n"
Original    (021): ['tok', '=', 'parsed', '.', 'module', '.', 'subscopes', '[', '0', ']', '.', 'statements', '[', '0', ']', '.', '_token_list', '[', '2', ']', '\\n']
Tokenized   (031): ['[CLS]', 'to', '##k', '=', 'par', '##sed', '.', 'module', '.', 'sub', '##scope', '##s', '[', '0', ']', '.', 'statements', '[', '0', ']', '.', '_', 'token', '_', 'list', '[', '2', ']', '\\', 'n', '[SEP]']
Filtered   (029): ['to', '##k', '=', 'par', '##sed', '.', 'module', '.', 'sub', '##scope', '##s', '[', '0', ']', '.', 'statements', '[', '0', ']', '.', '_', 'token', '_', 'list', '[', '2', ']', '\\', 'n']
Detokenized (021): ['to##k', '=', 'par##sed', '.', 'module', '.', 'sub##scope##s', '[', '0', ']', '.', 'statements', '[', '0', ']', '.', '_token_list', '[', '2', ']', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "__slots__ = ( "graphVariable" , \n"
Original    (006): ['__slots__', '=', '(', '"graphVariable"', ',', '\\n']
Tokenized   (017): ['[CLS]', '_', '_', 'slots', '_', '_', '=', '(', '"', 'graph', '##var', '##iable', '"', ',', '\\', 'n', '[SEP]']
Filtered   (015): ['_', '_', 'slots', '_', '_', '=', '(', '"', 'graph', '##var', '##iable', '"', ',', '\\', 'n']
Detokenized (006): ['__slots__', '=', '(', '"graph##var##iable"', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "( None , \n"
Original    (004): ['(', 'None', ',', '\\n']
Tokenized   (007): ['[CLS]', '(', 'none', ',', '\\', 'n', '[SEP]']
Filtered   (005): ['(', 'none', ',', '\\', 'n']
Detokenized (004): ['(', 'none', ',', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "def __init__ ( self , patterns = [ ] , prolog = None ) : \n"
Original    (016): ['def', '__init__', '(', 'self', ',', 'patterns', '=', '[', ']', ',', 'prolog', '=', 'None', ')', ':', '\\n']
Tokenized   (025): ['[CLS]', 'def', '_', '_', 'in', '##it', '_', '_', '(', 'self', ',', 'patterns', '=', '[', ']', ',', 'pro', '##log', '=', 'none', ')', ':', '\\', 'n', '[SEP]']
Filtered   (023): ['def', '_', '_', 'in', '##it', '_', '_', '(', 'self', ',', 'patterns', '=', '[', ']', ',', 'pro', '##log', '=', 'none', ')', ':', '\\', 'n']
Detokenized (016): ['def', '__in##it__', '(', 'self', ',', 'patterns', '=', '[', ']', ',', 'pro##log', '=', 'none', ')', ':', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "return term . n3 ( ) \n"
Original    (007): ['return', 'term', '.', 'n3', '(', ')', '\\n']
Tokenized   (011): ['[CLS]', 'return', 'term', '.', 'n', '##3', '(', ')', '\\', 'n', '[SEP]']
Filtered   (009): ['return', 'term', '.', 'n', '##3', '(', ')', '\\', 'n']
Detokenized (007): ['return', 'term', '.', 'n##3', '(', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ". join ( [ + . join ( [ \n"
Original    (010): ['.', 'join', '(', '[', '+', '.', 'join', '(', '[', '\\n']
Tokenized   (013): ['[CLS]', '.', 'join', '(', '[', '+', '.', 'join', '(', '[', '\\', 'n', '[SEP]']
Filtered   (011): ['.', 'join', '(', '[', '+', '.', 'join', '(', '[', '\\', 'n']
Detokenized (010): ['.', 'join', '(', '[', '+', '.', 'join', '(', '[', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "[ ( "a" , "?b" , 24 ) , ( "?r" , "?c" , 12345 ) , ( v1 , "?c" , 3333 ) , ( u1 , "?c" , 9999 ) ] ) \n"
Original    (035): ['[', '(', '"a"', ',', '"?b"', ',', '24', ')', ',', '(', '"?r"', ',', '"?c"', ',', '12345', ')', ',', '(', 'v1', ',', '"?c"', ',', '3333', ')', ',', '(', 'u1', ',', '"?c"', ',', '9999', ')', ']', ')', '\\n']
Tokenized   (060): ['[CLS]', '[', '(', '"', 'a', '"', ',', '"', '?', 'b', '"', ',', '24', ')', ',', '(', '"', '?', 'r', '"', ',', '"', '?', 'c', '"', ',', '123', '##45', ')', ',', '(', 'v', '##1', ',', '"', '?', 'c', '"', ',', '333', '##3', ')', ',', '(', 'u', '##1', ',', '"', '?', 'c', '"', ',', '999', '##9', ')', ']', ')', '\\', 'n', '[SEP]']
Filtered   (058): ['[', '(', '"', 'a', '"', ',', '"', '?', 'b', '"', ',', '24', ')', ',', '(', '"', '?', 'r', '"', ',', '"', '?', 'c', '"', ',', '123', '##45', ')', ',', '(', 'v', '##1', ',', '"', '?', 'c', '"', ',', '333', '##3', ')', ',', '(', 'u', '##1', ',', '"', '?', 'c', '"', ',', '999', '##9', ')', ']', ')', '\\', 'n']
Detokenized (035): ['[', '(', '"a"', ',', '"?b"', ',', '24', ')', ',', '(', '"?r"', ',', '"?c"', ',', '123##45', ')', ',', '(', 'v##1', ',', '"?c"', ',', '333##3', ')', ',', '(', 'u##1', ',', '"?c"', ',', '999##9', ')', ']', ')', '\\n']
Counter: 58
===================================================================
Hidden states:  (13, 35, 768)
# Extracted words:  35
Sentence         : "unittest . TextTestRunner ( verbosity = 3 ) . run ( suite ) \n"
Original    (014): ['unittest', '.', 'TextTestRunner', '(', 'verbosity', '=', '3', ')', '.', 'run', '(', 'suite', ')', '\\n']
Tokenized   (021): ['[CLS]', 'unit', '##test', '.', 'text', '##test', '##runner', '(', 'verb', '##osity', '=', '3', ')', '.', 'run', '(', 'suite', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['unit', '##test', '.', 'text', '##test', '##runner', '(', 'verb', '##osity', '=', '3', ')', '.', 'run', '(', 'suite', ')', '\\', 'n']
Detokenized (014): ['unit##test', '.', 'text##test##runner', '(', 'verb##osity', '=', '3', ')', '.', 'run', '(', 'suite', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : ") . parse ( "http://www.w3.org/People/Berners-Lee/card.rdf" ) \n"
Original    (007): [')', '.', 'parse', '(', '"http://www.w3.org/People/Berners-Lee/card.rdf"', ')', '\\n']
Tokenized   (034): ['[CLS]', ')', '.', 'par', '##se', '(', '"', 'http', ':', '/', '/', 'www', '.', 'w', '##3', '.', 'org', '/', 'people', '/', 'bern', '##ers', '-', 'lee', '/', 'card', '.', 'rd', '##f', '"', ')', '\\', 'n', '[SEP]']
Filtered   (032): [')', '.', 'par', '##se', '(', '"', 'http', ':', '/', '/', 'www', '.', 'w', '##3', '.', 'org', '/', 'people', '/', 'bern', '##ers', '-', 'lee', '/', 'card', '.', 'rd', '##f', '"', ')', '\\', 'n']
Detokenized (007): [')', '.', 'par##se', '(', '"http://www.w##3.org/people/bern##ers-lee/card.rd##f"', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "graph . get_context ( URIRef ( ) \n"
Original    (008): ['graph', '.', 'get_context', '(', 'URIRef', '(', ')', '\\n']
Tokenized   (015): ['[CLS]', 'graph', '.', 'get', '_', 'context', '(', 'ur', '##ire', '##f', '(', ')', '\\', 'n', '[SEP]']
Filtered   (013): ['graph', '.', 'get', '_', 'context', '(', 'ur', '##ire', '##f', '(', ')', '\\', 'n']
Detokenized (008): ['graph', '.', 'get_context', '(', 'ur##ire##f', '(', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "bob . set ( FOAF . name , Literal ( "Bob" ) ) \n"
Original    (014): ['bob', '.', 'set', '(', 'FOAF', '.', 'name', ',', 'Literal', '(', '"Bob"', ')', ')', '\\n']
Tokenized   (021): ['[CLS]', 'bob', '.', 'set', '(', 'f', '##oa', '##f', '.', 'name', ',', 'literal', '(', '"', 'bob', '"', ')', ')', '\\', 'n', '[SEP]']
Filtered   (019): ['bob', '.', 'set', '(', 'f', '##oa', '##f', '.', 'name', ',', 'literal', '(', '"', 'bob', '"', ')', ')', '\\', 'n']
Detokenized (014): ['bob', '.', 'set', '(', 'f##oa##f', '.', 'name', ',', 'literal', '(', '"bob"', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "print g . serialize ( format = ) \n"
Original    (009): ['print', 'g', '.', 'serialize', '(', 'format', '=', ')', '\\n']
Tokenized   (013): ['[CLS]', 'print', 'g', '.', 'serial', '##ize', '(', 'format', '=', ')', '\\', 'n', '[SEP]']
Filtered   (011): ['print', 'g', '.', 'serial', '##ize', '(', 'format', '=', ')', '\\', 'n']
Detokenized (009): ['print', 'g', '.', 'serial##ize', '(', 'format', '=', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "context = self . uriref ( ) or self . nodeid ( ) or self . sink . identifier \n"
Original    (020): ['context', '=', 'self', '.', 'uriref', '(', ')', 'or', 'self', '.', 'nodeid', '(', ')', 'or', 'self', '.', 'sink', '.', 'identifier', '\\n']
Tokenized   (028): ['[CLS]', 'context', '=', 'self', '.', 'ur', '##ire', '##f', '(', ')', 'or', 'self', '.', 'node', '##id', '(', ')', 'or', 'self', '.', 'sink', '.', 'id', '##ent', '##ifier', '\\', 'n', '[SEP]']
Filtered   (026): ['context', '=', 'self', '.', 'ur', '##ire', '##f', '(', ')', 'or', 'self', '.', 'node', '##id', '(', ')', 'or', 'self', '.', 'sink', '.', 'id', '##ent', '##ifier', '\\', 'n']
Detokenized (020): ['context', '=', 'self', '.', 'ur##ire##f', '(', ')', 'or', 'self', '.', 'node##id', '(', ')', 'or', 'self', '.', 'sink', '.', 'id##ent##ifier', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "from rdflib . query import Result , ResultSerializer , ResultParser \n"
Original    (011): ['from', 'rdflib', '.', 'query', 'import', 'Result', ',', 'ResultSerializer', ',', 'ResultParser', '\\n']
Tokenized   (020): ['[CLS]', 'from', 'rd', '##fl', '##ib', '.', 'query', 'import', 'result', ',', 'results', '##eria', '##lizer', ',', 'result', '##par', '##ser', '\\', 'n', '[SEP]']
Filtered   (018): ['from', 'rd', '##fl', '##ib', '.', 'query', 'import', 'result', ',', 'results', '##eria', '##lizer', ',', 'result', '##par', '##ser', '\\', 'n']
Detokenized (011): ['from', 'rd##fl##ib', '.', 'query', 'import', 'result', ',', 'results##eria##lizer', ',', 'result##par##ser', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "class CSVResultParser ( ResultParser ) : \n"
Original    (007): ['class', 'CSVResultParser', '(', 'ResultParser', ')', ':', '\\n']
Tokenized   (016): ['[CLS]', 'class', 'cs', '##vres', '##ult', '##par', '##ser', '(', 'result', '##par', '##ser', ')', ':', '\\', 'n', '[SEP]']
Filtered   (014): ['class', 'cs', '##vres', '##ult', '##par', '##ser', '(', 'result', '##par', '##ser', ')', ':', '\\', 'n']
Detokenized (007): ['class', 'cs##vres##ult##par##ser', '(', 'result##par##ser', ')', ':', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "r . bindings = [ ] \n"
Original    (007): ['r', '.', 'bindings', '=', '[', ']', '\\n']
Tokenized   (011): ['[CLS]', 'r', '.', 'binding', '##s', '=', '[', ']', '\\', 'n', '[SEP]']
Filtered   (009): ['r', '.', 'binding', '##s', '=', '[', ']', '\\', 'n']
Detokenized (007): ['r', '.', 'binding##s', '=', '[', ']', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "if result . type != "SELECT" : \n"
Original    (008): ['if', 'result', '.', 'type', '!=', '"SELECT"', ':', '\\n']
Tokenized   (014): ['[CLS]', 'if', 'result', '.', 'type', '!', '=', '"', 'select', '"', ':', '\\', 'n', '[SEP]']
Filtered   (012): ['if', 'result', '.', 'type', '!', '=', '"', 'select', '"', ':', '\\', 'n']
Detokenized (008): ['if', 'result', '.', 'type', '!=', '"select"', ':', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "stream = codecs . getwriter ( encoding ) ( stream ) \n"
Original    (012): ['stream', '=', 'codecs', '.', 'getwriter', '(', 'encoding', ')', '(', 'stream', ')', '\\n']
Tokenized   (017): ['[CLS]', 'stream', '=', 'code', '##cs', '.', 'get', '##writer', '(', 'encoding', ')', '(', 'stream', ')', '\\', 'n', '[SEP]']
Filtered   (015): ['stream', '=', 'code', '##cs', '.', 'get', '##writer', '(', 'encoding', ')', '(', 'stream', ')', '\\', 'n']
Detokenized (012): ['stream', '=', 'code##cs', '.', 'get##writer', '(', 'encoding', ')', '(', 'stream', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "vs = [ self . serializeTerm ( v , encoding ) for v in self . result . vars ] \n"
Original    (021): ['vs', '=', '[', 'self', '.', 'serializeTerm', '(', 'v', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'vars', ']', '\\n']
Tokenized   (028): ['[CLS]', 'vs', '=', '[', 'self', '.', 'serial', '##ize', '##ter', '##m', '(', 'v', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'var', '##s', ']', '\\', 'n', '[SEP]']
Filtered   (026): ['vs', '=', '[', 'self', '.', 'serial', '##ize', '##ter', '##m', '(', 'v', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'var', '##s', ']', '\\', 'n']
Detokenized (021): ['vs', '=', '[', 'self', '.', 'serial##ize##ter##m', '(', 'v', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'var##s', ']', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "for row in self . result . bindings : \n"
Original    (010): ['for', 'row', 'in', 'self', '.', 'result', '.', 'bindings', ':', '\\n']
Tokenized   (014): ['[CLS]', 'for', 'row', 'in', 'self', '.', 'result', '.', 'binding', '##s', ':', '\\', 'n', '[SEP]']
Filtered   (012): ['for', 'row', 'in', 'self', '.', 'result', '.', 'binding', '##s', ':', '\\', 'n']
Detokenized (010): ['for', 'row', 'in', 'self', '.', 'result', '.', 'binding##s', ':', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "row . get ( v ) , encoding ) for v in self . result . vars ] ) \n"
Original    (020): ['row', '.', 'get', '(', 'v', ')', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'vars', ']', ')', '\\n']
Tokenized   (024): ['[CLS]', 'row', '.', 'get', '(', 'v', ')', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'var', '##s', ']', ')', '\\', 'n', '[SEP]']
Filtered   (022): ['row', '.', 'get', '(', 'v', ')', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'var', '##s', ']', ')', '\\', 'n']
Detokenized (020): ['row', '.', 'get', '(', 'v', ')', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'var##s', ']', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "try : import nose \n"
Original    (005): ['try', ':', 'import', 'nose', '\\n']
Tokenized   (008): ['[CLS]', 'try', ':', 'import', 'nose', '\\', 'n', '[SEP]']
Filtered   (006): ['try', ':', 'import', 'nose', '\\', 'n']
Detokenized (005): ['try', ':', 'import', 'nose', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "~~~ argv += DEFAULT_DIRS \n"
Original    (005): ['~~~', 'argv', '+=', 'DEFAULT_DIRS', '\\n']
Tokenized   (016): ['[CLS]', '~', '~', '~', 'ar', '##g', '##v', '+', '=', 'default', '_', 'dir', '##s', '\\', 'n', '[SEP]']
Filtered   (014): ['~', '~', '~', 'ar', '##g', '##v', '+', '=', 'default', '_', 'dir', '##s', '\\', 'n']
Detokenized (005): ['~~~', 'ar##g##v', '+=', 'default_dir##s', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "nose . run_exit ( argv = finalArgs ) \n"
Original    (009): ['nose', '.', 'run_exit', '(', 'argv', '=', 'finalArgs', ')', '\\n']
Tokenized   (018): ['[CLS]', 'nose', '.', 'run', '_', 'exit', '(', 'ar', '##g', '##v', '=', 'final', '##ar', '##gs', ')', '\\', 'n', '[SEP]']
Filtered   (016): ['nose', '.', 'run', '_', 'exit', '(', 'ar', '##g', '##v', '=', 'final', '##ar', '##gs', ')', '\\', 'n']
Detokenized (009): ['nose', '.', 'run_exit', '(', 'ar##g##v', '=', 'final##ar##gs', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Loading model: microsoft/codebert-base
Reading input corpus
Preparing output file
Extracting representations from model
Sentence         : "\n"
Original    (001): ['\\n']
Tokenized   (004): ['<s>', '\\', 'n', '</s>']
Filtered   (002): ['\\', 'n']
Detokenized (001): ['\\n']
Counter: 2
===================================================================
Hidden states:  (13, 1, 768)
# Extracted words:  1
Sentence         : "# \n"
Original    (002): ['#', '\\n']
Tokenized   (005): ['<s>', '#', '\\', 'n', '</s>']
Filtered   (003): ['#', '\\', 'n']
Detokenized (002): ['#', '\\n']
Counter: 3
===================================================================
Hidden states:  (13, 2, 768)
# Extracted words:  2
Sentence         : "template_name = \n"
Original    (003): ['template_name', '=', '\\n']
Tokenized   (008): ['<s>', 'template', '_', 'name', '=', '\\', 'n', '</s>']
Filtered   (006): ['template', '_', 'name', '=', '\\', 'n']
Detokenized (003): ['template_name', '=', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "slug = "policy_profile" \n"
Original    (004): ['slug', '=', '"policy_profile"', '\\n']
Tokenized   (012): ['<s>', 'sl', 'ug', '=', '"', 'policy', '_', 'profile', '"', '\\', 'n', '</s>']
Filtered   (010): ['sl', 'ug', '=', '"', 'policy', '_', 'profile', '"', '\\', 'n']
Detokenized (004): ['slug', '=', '"policy_profile"', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "preload = False \n"
Original    (004): ['preload', '=', 'False', '\\n']
Tokenized   (008): ['<s>', 'pre', 'load', '=', 'False', '\\', 'n', '</s>']
Filtered   (006): ['pre', 'load', '=', 'False', '\\', 'n']
Detokenized (004): ['preload', '=', 'False', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "tabs = ( NetworkProfileTab , PolicyProfileTab ) \n"
Original    (008): ['tabs', '=', '(', 'NetworkProfileTab', ',', 'PolicyProfileTab', ')', '\\n']
Tokenized   (016): ['<s>', 'tab', 's', '=', '(', 'Network', 'Profile', 'Tab', ',', 'Policy', 'Profile', 'Tab', ')', '\\', 'n', '</s>']
Filtered   (014): ['tab', 's', '=', '(', 'Network', 'Profile', 'Tab', ',', 'Policy', 'Profile', 'Tab', ')', '\\', 'n']
Detokenized (008): ['tabs', '=', '(', 'NetworkProfileTab', ',', 'PolicyProfileTab', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "weak_store = WeakLocal ( ) \n"
Original    (006): ['weak_store', '=', 'WeakLocal', '(', ')', '\\n']
Tokenized   (012): ['<s>', 'weak', '_', 'store', '=', 'Weak', 'Local', '(', ')', '\\', 'n', '</s>']
Filtered   (010): ['weak', '_', 'store', '=', 'Weak', 'Local', '(', ')', '\\', 'n']
Detokenized (006): ['weak_store', '=', 'WeakLocal', '(', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "strong_store = corolocal . local \n"
Original    (006): ['strong_store', '=', 'corolocal', '.', 'local', '\\n']
Tokenized   (013): ['<s>', 'strong', '_', 'store', '=', 'cor', 'ol', 'ocal', '.', 'local', '\\', 'n', '</s>']
Filtered   (011): ['strong', '_', 'store', '=', 'cor', 'ol', 'ocal', '.', 'local', '\\', 'n']
Detokenized (006): ['strong_store', '=', 'corolocal', '.', 'local', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "eventlet . monkey_patch ( ) \n"
Original    (006): ['eventlet', '.', 'monkey_patch', '(', ')', '\\n']
Tokenized   (012): ['<s>', 'event', 'let', '.', 'monkey', '_', 'patch', '(', ')', '\\', 'n', '</s>']
Filtered   (010): ['event', 'let', '.', 'monkey', '_', 'patch', '(', ')', '\\', 'n']
Detokenized (006): ['eventlet', '.', 'monkey_patch', '(', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "CONF . register_opts ( impl_zmq . zmq_opts ) \n"
Original    (009): ['CONF', '.', 'register_opts', '(', 'impl_zmq', '.', 'zmq_opts', ')', '\\n']
Tokenized   (025): ['<s>', 'CON', 'F', '.', 'register', '_', 'op', 'ts', '(', 'impl', '_', 'z', 'm', 'q', '.', 'z', 'm', 'q', '_', 'op', 'ts', ')', '\\', 'n', '</s>']
Filtered   (023): ['CON', 'F', '.', 'register', '_', 'op', 'ts', '(', 'impl', '_', 'z', 'm', 'q', '.', 'z', 'm', 'q', '_', 'op', 'ts', ')', '\\', 'n']
Detokenized (009): ['CONF', '.', 'register_opts', '(', 'impl_zmq', '.', 'zmq_opts', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "vpnservices_dict = { : self . api_vpnservices . list ( ) } \n"
Original    (013): ['vpnservices_dict', '=', '{', ':', 'self', '.', 'api_vpnservices', '.', 'list', '(', ')', '}', '\\n']
Tokenized   (024): ['<s>', 'v', 'pn', 'services', '_', 'dict', '=', '{', ':', 'self', '.', 'api', '_', 'v', 'pn', 'services', '.', 'list', '(', ')', '}', '\\', 'n', '</s>']
Filtered   (022): ['v', 'pn', 'services', '_', 'dict', '=', '{', ':', 'self', '.', 'api', '_', 'v', 'pn', 'services', '.', 'list', '(', ')', '}', '\\', 'n']
Detokenized (013): ['vpnservices_dict', '=', '{', ':', 'self', '.', 'api_vpnservices', '.', 'list', '(', ')', '}', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "vpnservice [ ] [ ] ) \n"
Original    (007): ['vpnservice', '[', ']', '[', ']', ')', '\\n']
Tokenized   (012): ['<s>', 'v', 'pn', 'service', '[', ']', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (010): ['v', 'pn', 'service', '[', ']', '[', ']', ')', '\\', 'n']
Detokenized (007): ['vpnservice', '[', ']', '[', ']', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "form_data } ) . AndReturn ( ipsecsiteconnection ) \n"
Original    (009): ['form_data', '}', ')', '.', 'AndReturn', '(', 'ipsecsiteconnection', ')', '\\n']
Tokenized   (018): ['<s>', 'form', '_', 'data', '}', ')', '.', 'And', 'Return', '(', 'ip', 'sec', 'site', 'connection', ')', '\\', 'n', '</s>']
Filtered   (016): ['form', '_', 'data', '}', ')', '.', 'And', 'Return', '(', 'ip', 'sec', 'site', 'connection', ')', '\\', 'n']
Detokenized (009): ['form_data', '}', ')', '.', 'AndReturn', '(', 'ipsecsiteconnection', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "ipsecsiteconnections_dict ) \n"
Original    (003): ['ipsecsiteconnections_dict', ')', '\\n']
Tokenized   (012): ['<s>', 'ip', 'sec', 'site', 'connect', 'ions', '_', 'dict', ')', '\\', 'n', '</s>']
Filtered   (010): ['ip', 'sec', 'site', 'connect', 'ions', '_', 'dict', ')', '\\', 'n']
Detokenized (003): ['ipsecsiteconnections_dict', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "ipsecsiteconnections [ ] ) : \n"
Original    (006): ['ipsecsiteconnections', '[', ']', ')', ':', '\\n']
Tokenized   (013): ['<s>', 'ip', 'sec', 'site', 'connect', 'ions', '[', ']', ')', ':', '\\', 'n', '</s>']
Filtered   (011): ['ip', 'sec', 'site', 'connect', 'ions', '[', ']', ')', ':', '\\', 'n']
Detokenized (006): ['ipsecsiteconnections', '[', ']', ')', ':', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "neutronclient . show_ipsec_site_connection ( \n"
Original    (005): ['neutronclient', '.', 'show_ipsec_site_connection', '(', '\\n']
Tokenized   (018): ['<s>', 'ne', 'ut', 'ron', 'client', '.', 'show', '_', 'ip', 'sec', '_', 'site', '_', 'connection', '(', '\\', 'n', '</s>']
Filtered   (016): ['ne', 'ut', 'ron', 'client', '.', 'show', '_', 'ip', 'sec', '_', 'site', '_', 'connection', '(', '\\', 'n']
Detokenized (005): ['neutronclient', '.', 'show_ipsec_site_connection', '(', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "ret_val = api . vpn . ipsecsiteconnection_get ( self . request , \n"
Original    (013): ['ret_val', '=', 'api', '.', 'vpn', '.', 'ipsecsiteconnection_get', '(', 'self', '.', 'request', ',', '\\n']
Tokenized   (024): ['<s>', 'ret', '_', 'val', '=', 'api', '.', 'v', 'pn', '.', 'ip', 'sec', 'site', 'connection', '_', 'get', '(', 'self', '.', 'request', ',', '\\', 'n', '</s>']
Filtered   (022): ['ret', '_', 'val', '=', 'api', '.', 'v', 'pn', '.', 'ip', 'sec', 'site', 'connection', '_', 'get', '(', 'self', '.', 'request', ',', '\\', 'n']
Detokenized (013): ['ret_val', '=', 'api', '.', 'vpn', '.', 'ipsecsiteconnection_get', '(', 'self', '.', 'request', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "response_kwargs . setdefault ( "filename" , "usage.csv" ) \n"
Original    (009): ['response_kwargs', '.', 'setdefault', '(', '"filename"', ',', '"usage.csv"', ')', '\\n']
Tokenized   (022): ['<s>', 'response', '_', 'kw', 'args', '.', 'set', 'default', '(', '"', 'filename', '"', ',', '"', 'usage', '.', 'csv', '"', ')', '\\', 'n', '</s>']
Filtered   (020): ['response', '_', 'kw', 'args', '.', 'set', 'default', '(', '"', 'filename', '"', ',', '"', 'usage', '.', 'csv', '"', ')', '\\', 'n']
Detokenized (009): ['response_kwargs', '.', 'setdefault', '(', '"filename"', ',', '"usage.csv"', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "BlendProbes = 1 \n"
Original    (004): ['BlendProbes', '=', '1', '\\n']
Tokenized   (010): ['<s>', 'Bl', 'end', 'Pro', 'bes', '=', '1', '\\', 'n', '</s>']
Filtered   (008): ['Bl', 'end', 'Pro', 'bes', '=', '1', '\\', 'n']
Detokenized (004): ['BlendProbes', '=', '1', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "lightmap_index = field ( "m_LightmapIndex" ) \n"
Original    (007): ['lightmap_index', '=', 'field', '(', '"m_LightmapIndex"', ')', '\\n']
Tokenized   (019): ['<s>', 'light', 'map', '_', 'index', '=', 'field', '(', '"', 'm', '_', 'Light', 'map', 'Index', '"', ')', '\\', 'n', '</s>']
Filtered   (017): ['light', 'map', '_', 'index', '=', 'field', '(', '"', 'm', '_', 'Light', 'map', 'Index', '"', ')', '\\', 'n']
Detokenized (007): ['lightmap_index', '=', 'field', '(', '"m_LightmapIndex"', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "receive_shadows = field ( "m_ReceiveShadows" , bool ) \n"
Original    (009): ['receive_shadows', '=', 'field', '(', '"m_ReceiveShadows"', ',', 'bool', ')', '\\n']
Tokenized   (023): ['<s>', 're', 'ceive', '_', 'sh', 'adows', '=', 'field', '(', '"', 'm', '_', 'Re', 'ceive', 'Sh', 'adows', '"', ',', 'bool', ')', '\\', 'n', '</s>']
Filtered   (021): ['re', 'ceive', '_', 'sh', 'adows', '=', 'field', '(', '"', 'm', '_', 'Re', 'ceive', 'Sh', 'adows', '"', ',', 'bool', ')', '\\', 'n']
Detokenized (009): ['receive_shadows', '=', 'field', '(', '"m_ReceiveShadows"', ',', 'bool', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "Config . parser . readfp ( sconf ) \n"
Original    (009): ['Config', '.', 'parser', '.', 'readfp', '(', 'sconf', ')', '\\n']
Tokenized   (015): ['<s>', 'Config', '.', 'parser', '.', 'read', 'fp', '(', 'sc', 'on', 'f', ')', '\\', 'n', '</s>']
Filtered   (013): ['Config', '.', 'parser', '.', 'read', 'fp', '(', 'sc', 'on', 'f', ')', '\\', 'n']
Detokenized (009): ['Config', '.', 'parser', '.', 'readfp', '(', 'sconf', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "BBS_XMPP_CERT_FILE = BBS_ROOT + "xmpp.crt" \n"
Original    (006): ['BBS_XMPP_CERT_FILE', '=', 'BBS_ROOT', '+', '"xmpp.crt"', '\\n']
Tokenized   (030): ['<s>', 'B', 'BS', '_', 'X', 'MP', 'P', '_', 'C', 'ERT', '_', 'FILE', '=', 'B', 'BS', '_', 'RO', 'OT', '+', '"', 'x', 'm', 'pp', '.', 'cr', 't', '"', '\\', 'n', '</s>']
Filtered   (028): ['B', 'BS', '_', 'X', 'MP', 'P', '_', 'C', 'ERT', '_', 'FILE', '=', 'B', 'BS', '_', 'RO', 'OT', '+', '"', 'x', 'm', 'pp', '.', 'cr', 't', '"', '\\', 'n']
Detokenized (006): ['BBS_XMPP_CERT_FILE', '=', 'BBS_ROOT', '+', '"xmpp.crt"', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "BOARDS_FILE = BBS_ROOT + \n"
Original    (005): ['BOARDS_FILE', '=', 'BBS_ROOT', '+', '\\n']
Tokenized   (015): ['<s>', 'BO', 'ARDS', '_', 'FILE', '=', 'B', 'BS', '_', 'RO', 'OT', '+', '\\', 'n', '</s>']
Filtered   (013): ['BO', 'ARDS', '_', 'FILE', '=', 'B', 'BS', '_', 'RO', 'OT', '+', '\\', 'n']
Detokenized (005): ['BOARDS_FILE', '=', 'BBS_ROOT', '+', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "USHM_SIZE = MAXACTIVE + 10 \n"
Original    (006): ['USHM_SIZE', '=', 'MAXACTIVE', '+', '10', '\\n']
Tokenized   (014): ['<s>', 'USH', 'M', '_', 'SIZE', '=', 'MAX', 'ACT', 'IVE', '+', '10', '\\', 'n', '</s>']
Filtered   (012): ['USH', 'M', '_', 'SIZE', '=', 'MAX', 'ACT', 'IVE', '+', '10', '\\', 'n']
Detokenized (006): ['USHM_SIZE', '=', 'MAXACTIVE', '+', '10', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "UTMP_HASHSIZE = USHM_SIZE * 4 \n"
Original    (006): ['UTMP_HASHSIZE', '=', 'USHM_SIZE', '*', '4', '\\n']
Tokenized   (018): ['<s>', 'UT', 'MP', '_', 'H', 'AS', 'HS', 'IZE', '=', 'US', 'HM', '_', 'SIZE', '*', '4', '\\', 'n', '</s>']
Filtered   (016): ['UT', 'MP', '_', 'H', 'AS', 'HS', 'IZE', '=', 'US', 'HM', '_', 'SIZE', '*', '4', '\\', 'n']
Detokenized (006): ['UTMP_HASHSIZE', '=', 'USHM_SIZE', '*', '4', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "SESSION_TIMEOUT = datetime . timedelta ( 30 ) \n"
Original    (009): ['SESSION_TIMEOUT', '=', 'datetime', '.', 'timedelta', '(', '30', ')', '\\n']
Tokenized   (018): ['<s>', 'S', 'ESSION', '_', 'TIME', 'OUT', '=', 'dat', 'etime', '.', 'timed', 'elta', '(', '30', ')', '\\', 'n', '</s>']
Filtered   (016): ['S', 'ESSION', '_', 'TIME', 'OUT', '=', 'dat', 'etime', '.', 'timed', 'elta', '(', '30', ')', '\\', 'n']
Detokenized (009): ['SESSION_TIMEOUT', '=', 'datetime', '.', 'timedelta', '(', '30', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "SESSION_TIMEOUT_SECONDS = 86400 * 30 \n"
Original    (006): ['SESSION_TIMEOUT_SECONDS', '=', '86400', '*', '30', '\\n']
Tokenized   (019): ['<s>', 'S', 'ESSION', '_', 'TIME', 'OUT', '_', 'SEC', 'ON', 'DS', '=', '8', '64', '00', '*', '30', '\\', 'n', '</s>']
Filtered   (017): ['S', 'ESSION', '_', 'TIME', 'OUT', '_', 'SEC', 'ON', 'DS', '=', '8', '64', '00', '*', '30', '\\', 'n']
Detokenized (006): ['SESSION_TIMEOUT_SECONDS', '=', '86400', '*', '30', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "MAX_ATTACHSIZE = 20 * 1024 * 1024 \n"
Original    (008): ['MAX_ATTACHSIZE', '=', '20', '*', '1024', '*', '1024', '\\n']
Tokenized   (016): ['<s>', 'MAX', '_', 'ATT', 'AC', 'HS', 'IZE', '=', '20', '*', '1024', '*', '1024', '\\', 'n', '</s>']
Filtered   (014): ['MAX', '_', 'ATT', 'AC', 'HS', 'IZE', '=', '20', '*', '1024', '*', '1024', '\\', 'n']
Detokenized (008): ['MAX_ATTACHSIZE', '=', '20', '*', '1024', '*', '1024', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "MAIL_SIZE_LIMIT = - 1 \n"
Original    (005): ['MAIL_SIZE_LIMIT', '=', '-', '1', '\\n']
Tokenized   (015): ['<s>', 'MA', 'IL', '_', 'SIZE', '_', 'L', 'IM', 'IT', '=', '-', '1', '\\', 'n', '</s>']
Filtered   (013): ['MA', 'IL', '_', 'SIZE', '_', 'L', 'IM', 'IT', '=', '-', '1', '\\', 'n']
Detokenized (005): ['MAIL_SIZE_LIMIT', '=', '-', '1', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "newparts = [ ] \n"
Original    (005): ['newparts', '=', '[', ']', '\\n']
Tokenized   (009): ['<s>', 'new', 'parts', '=', '[', ']', '\\', 'n', '</s>']
Filtered   (007): ['new', 'parts', '=', '[', ']', '\\', 'n']
Detokenized (005): ['newparts', '=', '[', ']', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "firstitem = self . GetItem ( user , route + [ start ] , has_perm , need_perm ) \n"
Original    (019): ['firstitem', '=', 'self', '.', 'GetItem', '(', 'user', ',', 'route', '+', '[', 'start', ']', ',', 'has_perm', ',', 'need_perm', ')', '\\n']
Tokenized   (028): ['<s>', 'first', 'item', '=', 'self', '.', 'Get', 'Item', '(', 'user', ',', 'route', '+', '[', 'start', ']', ',', 'has', '_', 'perm', ',', 'need', '_', 'perm', ')', '\\', 'n', '</s>']
Filtered   (026): ['first', 'item', '=', 'self', '.', 'Get', 'Item', '(', 'user', ',', 'route', '+', '[', 'start', ']', ',', 'has', '_', 'perm', ',', 'need', '_', 'perm', ')', '\\', 'n']
Detokenized (019): ['firstitem', '=', 'self', '.', 'GetItem', '(', 'user', ',', 'route', '+', '[', 'start', ']', ',', 'has_perm', ',', 'need_perm', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "_id = start - 1 \n"
Original    (006): ['_id', '=', 'start', '-', '1', '\\n']
Tokenized   (010): ['<s>', '_', 'id', '=', 'start', '-', '1', '\\', 'n', '</s>']
Filtered   (008): ['_', 'id', '=', 'start', '-', '1', '\\', 'n']
Detokenized (006): ['_id', '=', 'start', '-', '1', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "linkfile = "%s/boards/xattach/%s" % ( Config . BBS_ROOT , filename ) \n"
Original    (012): ['linkfile', '=', '"%s/boards/xattach/%s"', '%', '(', 'Config', '.', 'BBS_ROOT', ',', 'filename', ')', '\\n']
Tokenized   (030): ['<s>', 'link', 'file', '=', '"%', 's', '/', 'boards', '/', 'x', 'attach', '/', '%', 's', '"', '%', '(', 'Config', '.', 'B', 'BS', '_', 'RO', 'OT', ',', 'filename', ')', '\\', 'n', '</s>']
Filtered   (028): ['link', 'file', '=', '"%', 's', '/', 'boards', '/', 'x', 'attach', '/', '%', 's', '"', '%', '(', 'Config', '.', 'B', 'BS', '_', 'RO', 'OT', ',', 'filename', ')', '\\', 'n']
Detokenized (012): ['linkfile', '=', '"%s/boards/xattach/%s"', '%', '(', 'Config', '.', 'BBS_ROOT', ',', 'filename', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "boardname = svc . get_str ( params , , ) \n"
Original    (011): ['boardname', '=', 'svc', '.', 'get_str', '(', 'params', ',', ',', ')', '\\n']
Tokenized   (018): ['<s>', 'board', 'name', '=', 's', 'vc', '.', 'get', '_', 'str', '(', 'params', ',', ',', ')', '\\', 'n', '</s>']
Filtered   (016): ['board', 'name', '=', 's', 'vc', '.', 'get', '_', 'str', '(', 'params', ',', ',', ')', '\\', 'n']
Detokenized (011): ['boardname', '=', 'svc', '.', 'get_str', '(', 'params', ',', ',', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "has_perm = user . IsDigestMgr ( ) \n"
Original    (008): ['has_perm', '=', 'user', '.', 'IsDigestMgr', '(', ')', '\\n']
Tokenized   (017): ['<s>', 'has', '_', 'perm', '=', 'user', '.', 'Is', 'Dig', 'est', 'M', 'gr', '(', ')', '\\', 'n', '</s>']
Filtered   (015): ['has', '_', 'perm', '=', 'user', '.', 'Is', 'Dig', 'est', 'M', 'gr', '(', ')', '\\', 'n']
Detokenized (008): ['has_perm', '=', 'user', '.', 'IsDigestMgr', '(', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "Digest . View ( svc , basenode , route , session , has_perm , start , count ) \n"
Original    (019): ['Digest', '.', 'View', '(', 'svc', ',', 'basenode', ',', 'route', ',', 'session', ',', 'has_perm', ',', 'start', ',', 'count', ')', '\\n']
Tokenized   (028): ['<s>', 'Dig', 'est', '.', 'View', '(', 's', 'vc', ',', 'bas', 'en', 'ode', ',', 'route', ',', 'session', ',', 'has', '_', 'perm', ',', 'start', ',', 'count', ')', '\\', 'n', '</s>']
Filtered   (026): ['Dig', 'est', '.', 'View', '(', 's', 'vc', ',', 'bas', 'en', 'ode', ',', 'route', ',', 'session', ',', 'has', '_', 'perm', ',', 'start', ',', 'count', ')', '\\', 'n']
Detokenized (019): ['Digest', '.', 'View', '(', 'svc', ',', 'basenode', ',', 'route', ',', 'session', ',', 'has_perm', ',', 'start', ',', 'count', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "svc . writedata ( json . dumps ( result ) ) \n"
Original    (012): ['svc', '.', 'writedata', '(', 'json', '.', 'dumps', '(', 'result', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'sv', 'c', '.', 'writ', 'ed', 'ata', '(', 'json', '.', 'dumps', '(', 'result', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['sv', 'c', '.', 'writ', 'ed', 'ata', '(', 'json', '.', 'dumps', '(', 'result', ')', ')', '\\', 'n']
Detokenized (012): ['svc', '.', 'writedata', '(', 'json', '.', 'dumps', '(', 'result', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "postinfo = Post . Post ( item . realpath ( ) , None ) \n"
Original    (015): ['postinfo', '=', 'Post', '.', 'Post', '(', 'item', '.', 'realpath', '(', ')', ',', 'None', ')', '\\n']
Tokenized   (020): ['<s>', 'post', 'info', '=', 'Post', '.', 'Post', '(', 'item', '.', 'real', 'path', '(', ')', ',', 'None', ')', '\\', 'n', '</s>']
Filtered   (018): ['post', 'info', '=', 'Post', '.', 'Post', '(', 'item', '.', 'real', 'path', '(', ')', ',', 'None', ')', '\\', 'n']
Detokenized (015): ['postinfo', '=', 'Post', '.', 'Post', '(', 'item', '.', 'realpath', '(', ')', ',', 'None', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "msg_count = msgbox . GetMsgCount ( all = False ) \n"
Original    (011): ['msg_count', '=', 'msgbox', '.', 'GetMsgCount', '(', 'all', '=', 'False', ')', '\\n']
Tokenized   (019): ['<s>', 'msg', '_', 'count', '=', 'msg', 'box', '.', 'Get', 'Msg', 'Count', '(', 'all', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (017): ['msg', '_', 'count', '=', 'msg', 'box', '.', 'Get', 'Msg', 'Count', '(', 'all', '=', 'False', ')', '\\', 'n']
Detokenized (011): ['msg_count', '=', 'msgbox', '.', 'GetMsgCount', '(', 'all', '=', 'False', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "xmpp_read = self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) \n"
Original    (017): ['xmpp_read', '=', 'self', '.', 'rosters', '.', 'get_xmpp_read', '(', 'self', '.', '_user', '.', 'GetUID', '(', ')', ')', '\\n']
Tokenized   (032): ['<s>', 'x', 'm', 'pp', '_', 'read', '=', 'self', '.', 'rosters', '.', 'get', '_', 'x', 'm', 'pp', '_', 'read', '(', 'self', '.', '_', 'user', '.', 'Get', 'UID', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (030): ['x', 'm', 'pp', '_', 'read', '=', 'self', '.', 'rosters', '.', 'get', '_', 'x', 'm', 'pp', '_', 'read', '(', 'self', '.', '_', 'user', '.', 'Get', 'UID', '(', ')', ')', '\\', 'n']
Detokenized (017): ['xmpp_read', '=', 'self', '.', 'rosters', '.', 'get_xmpp_read', '(', 'self', '.', '_user', '.', 'GetUID', '(', ')', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "read_count = msg_count - msg_unread \n"
Original    (006): ['read_count', '=', 'msg_count', '-', 'msg_unread', '\\n']
Tokenized   (016): ['<s>', 'read', '_', 'count', '=', 'msg', '_', 'count', '-', 'msg', '_', 'un', 'read', '\\', 'n', '</s>']
Filtered   (014): ['read', '_', 'count', '=', 'msg', '_', 'count', '-', 'msg', '_', 'un', 'read', '\\', 'n']
Detokenized (006): ['read_count', '=', 'msg_count', '-', 'msg_unread', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "term_read = self . rosters . get_term_read ( self . get_uid ( ) ) \n"
Original    (015): ['term_read', '=', 'self', '.', 'rosters', '.', 'get_term_read', '(', 'self', '.', 'get_uid', '(', ')', ')', '\\n']
Tokenized   (026): ['<s>', 'term', '_', 'read', '=', 'self', '.', 'rosters', '.', 'get', '_', 'term', '_', 'read', '(', 'self', '.', 'get', '_', 'uid', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['term', '_', 'read', '=', 'self', '.', 'rosters', '.', 'get', '_', 'term', '_', 'read', '(', 'self', '.', 'get', '_', 'uid', '(', ')', ')', '\\', 'n']
Detokenized (015): ['term_read', '=', 'self', '.', 'rosters', '.', 'get_term_read', '(', 'self', '.', 'get_uid', '(', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "new_unread [ msghead . topid ] = i \n"
Original    (009): ['new_unread', '[', 'msghead', '.', 'topid', ']', '=', 'i', '\\n']
Tokenized   (017): ['<s>', 'new', '_', 'un', 'read', '[', 'msg', 'head', '.', 'top', 'id', ']', '=', 'i', '\\', 'n', '</s>']
Filtered   (015): ['new', '_', 'un', 'read', '[', 'msg', 'head', '.', 'top', 'id', ']', '=', 'i', '\\', 'n']
Detokenized (009): ['new_unread', '[', 'msghead', '.', 'topid', ']', '=', 'i', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "to_steal = { } \n"
Original    (005): ['to_steal', '=', '{', '}', '\\n']
Tokenized   (011): ['<s>', 'to', '_', 'st', 'eal', '=', '{', '}', '\\', 'n', '</s>']
Filtered   (009): ['to', '_', 'st', 'eal', '=', '{', '}', '\\', 'n']
Detokenized (005): ['to_steal', '=', '{', '}', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "to_steal_begin = msg_count \n"
Original    (004): ['to_steal_begin', '=', 'msg_count', '\\n']
Tokenized   (014): ['<s>', 'to', '_', 'st', 'eal', '_', 'begin', '=', 'msg', '_', 'count', '\\', 'n', '</s>']
Filtered   (012): ['to', '_', 'st', 'eal', '_', 'begin', '=', 'msg', '_', 'count', '\\', 'n']
Detokenized (004): ['to_steal_begin', '=', 'msg_count', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "pass \n"
Original    (002): ['pass', '\\n']
Tokenized   (005): ['<s>', 'pass', '\\', 'n', '</s>']
Filtered   (003): ['pass', '\\', 'n']
Detokenized (002): ['pass', '\\n']
Counter: 3
===================================================================
Hidden states:  (13, 2, 768)
# Extracted words:  2
Sentence         : "final_unread [ pid ] = ( new_unread [ pid ] , 1 ) \n"
Original    (014): ['final_unread', '[', 'pid', ']', '=', '(', 'new_unread', '[', 'pid', ']', ',', '1', ')', '\\n']
Tokenized   (023): ['<s>', 'final', '_', 'un', 'read', '[', 'pid', ']', '=', '(', 'new', '_', 'un', 'read', '[', 'pid', ']', ',', '1', ')', '\\', 'n', '</s>']
Filtered   (021): ['final', '_', 'un', 'read', '[', 'pid', ']', '=', '(', 'new', '_', 'un', 'read', '[', 'pid', ']', ',', '1', ')', '\\', 'n']
Detokenized (014): ['final_unread', '[', 'pid', ']', '=', '(', 'new_unread', '[', 'pid', ']', ',', '1', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "msgtext = msgbox . LoadMsgText ( msghead ) \n"
Original    (009): ['msgtext', '=', 'msgbox', '.', 'LoadMsgText', '(', 'msghead', ')', '\\n']
Tokenized   (017): ['<s>', 'msg', 'text', '=', 'msg', 'box', '.', 'Load', 'Msg', 'Text', '(', 'msg', 'head', ')', '\\', 'n', '</s>']
Filtered   (015): ['msg', 'text', '=', 'msg', 'box', '.', 'Load', 'Msg', 'Text', '(', 'msg', 'head', ')', '\\', 'n']
Detokenized (009): ['msgtext', '=', 'msgbox', '.', 'LoadMsgText', '(', 'msghead', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "roster = self . rosters . get ( self ) \n"
Original    (011): ['roster', '=', 'self', '.', 'rosters', '.', 'get', '(', 'self', ')', '\\n']
Tokenized   (015): ['<s>', 'ro', 'ster', '=', 'self', '.', 'rosters', '.', 'get', '(', 'self', ')', '\\', 'n', '</s>']
Filtered   (013): ['ro', 'ster', '=', 'self', '.', 'rosters', '.', 'get', '(', 'self', ')', '\\', 'n']
Detokenized (011): ['roster', '=', 'self', '.', 'rosters', '.', 'get', '(', 'self', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "PYTHON_VERSION = sys . version_info [ : 3 ] \n"
Original    (010): ['PYTHON_VERSION', '=', 'sys', '.', 'version_info', '[', ':', '3', ']', '\\n']
Tokenized   (020): ['<s>', 'P', 'Y', 'TH', 'ON', '_', 'VERSION', '=', 'sys', '.', 'version', '_', 'info', '[', ':', '3', ']', '\\', 'n', '</s>']
Filtered   (018): ['P', 'Y', 'TH', 'ON', '_', 'VERSION', '=', 'sys', '.', 'version', '_', 'info', '[', ':', '3', ']', '\\', 'n']
Detokenized (010): ['PYTHON_VERSION', '=', 'sys', '.', 'version_info', '[', ':', '3', ']', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "PY2 = ( PYTHON_VERSION [ 0 ] == 2 ) \n"
Original    (011): ['PY2', '=', '(', 'PYTHON_VERSION', '[', '0', ']', '==', '2', ')', '\\n']
Tokenized   (021): ['<s>', 'P', 'Y', '2', '=', '(', 'P', 'Y', 'TH', 'ON', '_', 'VERSION', '[', '0', ']', '==', '2', ')', '\\', 'n', '</s>']
Filtered   (019): ['P', 'Y', '2', '=', '(', 'P', 'Y', 'TH', 'ON', '_', 'VERSION', '[', '0', ']', '==', '2', ')', '\\', 'n']
Detokenized (011): ['PY2', '=', '(', 'PYTHON_VERSION', '[', '0', ']', '==', '2', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "sp_desc , \n"
Original    (003): ['sp_desc', ',', '\\n']
Tokenized   (008): ['<s>', 'sp', '_', 'desc', ',', '\\', 'n', '</s>']
Filtered   (006): ['sp', '_', 'desc', ',', '\\', 'n']
Detokenized (003): ['sp_desc', ',', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "con = hpov . connection ( args . host ) \n"
Original    (011): ['con', '=', 'hpov', '.', 'connection', '(', 'args', '.', 'host', ')', '\\n']
Tokenized   (015): ['<s>', 'con', '=', 'hp', 'ov', '.', 'connection', '(', 'args', '.', 'host', ')', '\\', 'n', '</s>']
Filtered   (013): ['con', '=', 'hp', 'ov', '.', 'connection', '(', 'args', '.', 'host', ')', '\\', 'n']
Detokenized (011): ['con', '=', 'hpov', '.', 'connection', '(', 'args', '.', 'host', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "acceptEULA ( con ) \n"
Original    (005): ['acceptEULA', '(', 'con', ')', '\\n']
Tokenized   (011): ['<s>', 'accept', 'E', 'UL', 'A', '(', 'con', ')', '\\', 'n', '</s>']
Filtered   (009): ['accept', 'E', 'UL', 'A', '(', 'con', ')', '\\', 'n']
Detokenized (005): ['acceptEULA', '(', 'con', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "fw_settings = profile . make_firmware_dict ( sts , args . baseline ) \n"
Original    (013): ['fw_settings', '=', 'profile', '.', 'make_firmware_dict', '(', 'sts', ',', 'args', '.', 'baseline', ')', '\\n']
Tokenized   (024): ['<s>', 'fw', '_', 'settings', '=', 'profile', '.', 'make', '_', 'f', 'irm', 'ware', '_', 'dict', '(', 'sts', ',', 'args', '.', 'baseline', ')', '\\', 'n', '</s>']
Filtered   (022): ['fw', '_', 'settings', '=', 'profile', '.', 'make', '_', 'f', 'irm', 'ware', '_', 'dict', '(', 'sts', ',', 'args', '.', 'baseline', ')', '\\', 'n']
Detokenized (013): ['fw_settings', '=', 'profile', '.', 'make_firmware_dict', '(', 'sts', ',', 'args', '.', 'baseline', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "boot , bootmode = profile . make_boot_settings_dict ( srv , sht , args . disable_manage_boot , \n"
Original    (017): ['boot', ',', 'bootmode', '=', 'profile', '.', 'make_boot_settings_dict', '(', 'srv', ',', 'sht', ',', 'args', '.', 'disable_manage_boot', ',', '\\n']
Tokenized   (034): ['<s>', 'boot', ',', 'boot', 'mode', '=', 'profile', '.', 'make', '_', 'boot', '_', 'settings', '_', 'dict', '(', 'sr', 'v', ',', 'sh', 't', ',', 'args', '.', 'disable', '_', 'man', 'age', '_', 'boot', ',', '\\', 'n', '</s>']
Filtered   (032): ['boot', ',', 'boot', 'mode', '=', 'profile', '.', 'make', '_', 'boot', '_', 'settings', '_', 'dict', '(', 'sr', 'v', ',', 'sh', 't', ',', 'args', '.', 'disable', '_', 'man', 'age', '_', 'boot', ',', '\\', 'n']
Detokenized (017): ['boot', ',', 'bootmode', '=', 'profile', '.', 'make_boot_settings_dict', '(', 'srv', ',', 'sht', ',', 'args', '.', 'disable_manage_boot', ',', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "define_profile_template ( srv , \n"
Original    (005): ['define_profile_template', '(', 'srv', ',', '\\n']
Tokenized   (013): ['<s>', 'define', '_', 'profile', '_', 'template', '(', 'sr', 'v', ',', '\\', 'n', '</s>']
Filtered   (011): ['define', '_', 'profile', '_', 'template', '(', 'sr', 'v', ',', '\\', 'n']
Detokenized (005): ['define_profile_template', '(', 'srv', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "sht [ ] , \n"
Original    (005): ['sht', '[', ']', ',', '\\n']
Tokenized   (009): ['<s>', 'sh', 't', '[', ']', ',', '\\', 'n', '</s>']
Filtered   (007): ['sh', 't', '[', ']', ',', '\\', 'n']
Detokenized (005): ['sht', '[', ']', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "credential = { : args . domain . upper ( ) , : args . user , : args . passwd } \n"
Original    (023): ['credential', '=', '{', ':', 'args', '.', 'domain', '.', 'upper', '(', ')', ',', ':', 'args', '.', 'user', ',', ':', 'args', '.', 'passwd', '}', '\\n']
Tokenized   (029): ['<s>', 'c', 'red', 'ential', '=', '{', ':', 'args', '.', 'domain', '.', 'upper', '(', ')', ',', ':', 'args', '.', 'user', ',', ':', 'args', '.', 'pass', 'wd', '}', '\\', 'n', '</s>']
Filtered   (027): ['c', 'red', 'ential', '=', '{', ':', 'args', '.', 'domain', '.', 'upper', '(', ')', ',', ':', 'args', '.', 'user', ',', ':', 'args', '.', 'pass', 'wd', '}', '\\', 'n']
Detokenized (023): ['credential', '=', '{', ':', 'args', '.', 'domain', '.', 'upper', '(', ')', ',', ':', 'args', '.', 'user', ',', ':', 'args', '.', 'passwd', '}', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "get_address_pools ( con , srv , args . types ) \n"
Original    (011): ['get_address_pools', '(', 'con', ',', 'srv', ',', 'args', '.', 'types', ')', '\\n']
Tokenized   (020): ['<s>', 'get', '_', 'address', '_', 'pool', 's', '(', 'con', ',', 'sr', 'v', ',', 'args', '.', 'types', ')', '\\', 'n', '</s>']
Filtered   (018): ['get', '_', 'address', '_', 'pool', 's', '(', 'con', ',', 'sr', 'v', ',', 'args', '.', 'types', ')', '\\', 'n']
Detokenized (011): ['get_address_pools', '(', 'con', ',', 'srv', ',', 'args', '.', 'types', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "enclosure_group = None , server_profile = None ) : \n"
Original    (010): ['enclosure_group', '=', 'None', ',', 'server_profile', '=', 'None', ')', ':', '\\n']
Tokenized   (018): ['<s>', 'en', 'closure', '_', 'group', '=', 'None', ',', 'server', '_', 'profile', '=', 'None', ')', ':', '\\', 'n', '</s>']
Filtered   (016): ['en', 'closure', '_', 'group', '=', 'None', ',', 'server', '_', 'profile', '=', 'None', ')', ':', '\\', 'n']
Detokenized (010): ['enclosure_group', '=', 'None', ',', 'server_profile', '=', 'None', ')', ':', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "biosSettings = None , \n"
Original    (005): ['biosSettings', '=', 'None', ',', '\\n']
Tokenized   (010): ['<s>', 'b', 'ios', 'Settings', '=', 'None', ',', '\\', 'n', '</s>']
Filtered   (008): ['b', 'ios', 'Settings', '=', 'None', ',', '\\', 'n']
Detokenized (005): ['biosSettings', '=', 'None', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "macType = , \n"
Original    (004): ['macType', '=', ',', '\\n']
Tokenized   (008): ['<s>', 'mac', 'Type', '=', ',', '\\', 'n', '</s>']
Filtered   (006): ['mac', 'Type', '=', ',', '\\', 'n']
Detokenized (004): ['macType', '=', ',', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "localStorageSettingsV3 , macType , name , \n"
Original    (007): ['localStorageSettingsV3', ',', 'macType', ',', 'name', ',', '\\n']
Tokenized   (015): ['<s>', 'local', 'Storage', 'Settings', 'V', '3', ',', 'mac', 'Type', ',', 'name', ',', '\\', 'n', '</s>']
Filtered   (013): ['local', 'Storage', 'Settings', 'V', '3', ',', 'mac', 'Type', ',', 'name', ',', '\\', 'n']
Detokenized (007): ['localStorageSettingsV3', ',', 'macType', ',', 'name', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "sanStorageV3 , serialNumber , \n"
Original    (005): ['sanStorageV3', ',', 'serialNumber', ',', '\\n']
Tokenized   (012): ['<s>', 'san', 'Storage', 'V', '3', ',', 'serial', 'Number', ',', '\\', 'n', '</s>']
Filtered   (010): ['san', 'Storage', 'V', '3', ',', 'serial', 'Number', ',', '\\', 'n']
Detokenized (005): ['sanStorageV3', ',', 'serialNumber', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "serverProfileTemplateUri , uuid , wwnType ) \n"
Original    (007): ['serverProfileTemplateUri', ',', 'uuid', ',', 'wwnType', ')', '\\n']
Tokenized   (017): ['<s>', 'server', 'Profile', 'Template', 'U', 'ri', ',', 'u', 'uid', ',', 'w', 'wn', 'Type', ')', '\\', 'n', '</s>']
Filtered   (015): ['server', 'Profile', 'Template', 'U', 'ri', ',', 'u', 'uid', ',', 'w', 'wn', 'Type', ')', '\\', 'n']
Detokenized (007): ['serverProfileTemplateUri', ',', 'uuid', ',', 'wwnType', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "profile_template = self . _con . get ( entity [ ] ) \n"
Original    (013): ['profile_template', '=', 'self', '.', '_con', '.', 'get', '(', 'entity', '[', ']', ')', '\\n']
Tokenized   (019): ['<s>', 'profile', '_', 'template', '=', 'self', '.', '_', 'con', '.', 'get', '(', 'entity', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (017): ['profile', '_', 'template', '=', 'self', '.', '_', 'con', '.', 'get', '(', 'entity', '[', ']', ')', '\\', 'n']
Detokenized (013): ['profile_template', '=', 'self', '.', '_con', '.', 'get', '(', 'entity', '[', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "powerMode = ) : \n"
Original    (005): ['powerMode', '=', ')', ':', '\\n']
Tokenized   (009): ['<s>', 'power', 'Mode', '=', ')', ':', '\\', 'n', '</s>']
Filtered   (007): ['power', 'Mode', '=', ')', ':', '\\', 'n']
Detokenized (005): ['powerMode', '=', ')', ':', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "egroup = make_EnclosureGroupV200 ( associatedLIGs , name , powerMode ) \n"
Original    (011): ['egroup', '=', 'make_EnclosureGroupV200', '(', 'associatedLIGs', ',', 'name', ',', 'powerMode', ')', '\\n']
Tokenized   (025): ['<s>', 'eg', 'roup', '=', 'make', '_', 'En', 'closure', 'Group', 'V', '200', '(', 'associated', 'L', 'IG', 's', ',', 'name', ',', 'power', 'Mode', ')', '\\', 'n', '</s>']
Filtered   (023): ['eg', 'roup', '=', 'make', '_', 'En', 'closure', 'Group', 'V', '200', '(', 'associated', 'L', 'IG', 's', ',', 'name', ',', 'power', 'Mode', ')', '\\', 'n']
Detokenized (011): ['egroup', '=', 'make_EnclosureGroupV200', '(', 'associatedLIGs', ',', 'name', ',', 'powerMode', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "allocatorBody = { : count } \n"
Original    (007): ['allocatorBody', '=', '{', ':', 'count', '}', '\\n']
Tokenized   (012): ['<s>', 'alloc', 'ator', 'Body', '=', '{', ':', 'count', '}', '\\', 'n', '</s>']
Filtered   (010): ['alloc', 'ator', 'Body', '=', '{', ':', 'count', '}', '\\', 'n']
Detokenized (007): ['allocatorBody', '=', '{', ':', 'count', '}', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "prange [ ] = False \n"
Original    (006): ['prange', '[', ']', '=', 'False', '\\n']
Tokenized   (010): ['<s>', 'pr', 'ange', '[', ']', '=', 'False', '\\', 'n', '</s>']
Filtered   (008): ['pr', 'ange', '[', ']', '=', 'False', '\\', 'n']
Detokenized (006): ['prange', '[', ']', '=', 'False', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "tempstr = "hp-rest-classes-bios-" + romfamily + "-" + biosversion \n"
Original    (010): ['tempstr', '=', '"hp-rest-classes-bios-"', '+', 'romfamily', '+', '"-"', '+', 'biosversion', '\\n']
Tokenized   (026): ['<s>', 'temp', 'str', '=', '"', 'hp', '-', 'rest', '-', 'classes', '-', 'b', 'ios', '-"', '+', 'rom', 'family', '+', '"', '-"', '+', 'bios', 'version', '\\', 'n', '</s>']
Filtered   (024): ['temp', 'str', '=', '"', 'hp', '-', 'rest', '-', 'classes', '-', 'b', 'ios', '-"', '+', 'rom', 'family', '+', '"', '-"', '+', 'bios', 'version', '\\', 'n']
Detokenized (010): ['tempstr', '=', '"hp-rest-classes-bios-"', '+', 'romfamily', '+', '"-"', '+', 'biosversion', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "monolith = None ) : \n"
Original    (006): ['monolith', '=', 'None', ')', ':', '\\n']
Tokenized   (010): ['<s>', 'mon', 'olith', '=', 'None', ')', ':', '\\', 'n', '</s>']
Filtered   (008): ['mon', 'olith', '=', 'None', ')', ':', '\\', 'n']
Detokenized (006): ['monolith', '=', 'None', ')', ':', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "pathjoinstr ) ) : \n"
Original    (005): ['pathjoinstr', ')', ')', ':', '\\n']
Tokenized   (010): ['<s>', 'path', 'join', 'str', ')', ')', ':', '\\', 'n', '</s>']
Filtered   (008): ['path', 'join', 'str', ')', ')', ':', '\\', 'n']
Detokenized (005): ['pathjoinstr', ')', ')', ':', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "newclass . set_root ( root ) \n"
Original    (007): ['newclass', '.', 'set_root', '(', 'root', ')', '\\n']
Tokenized   (013): ['<s>', 'new', 'class', '.', 'set', '_', 'root', '(', 'root', ')', '\\', 'n', '</s>']
Filtered   (011): ['new', 'class', '.', 'set', '_', 'root', '(', 'root', ')', '\\', 'n']
Detokenized (007): ['newclass', '.', 'set_root', '(', 'root', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "folderentries = data [ "links" ] \n"
Original    (007): ['folderentries', '=', 'data', '[', '"links"', ']', '\\n']
Tokenized   (014): ['<s>', 'fold', 'erent', 'ries', '=', 'data', '[', '"', 'links', '"', ']', '\\', 'n', '</s>']
Filtered   (012): ['fold', 'erent', 'ries', '=', 'data', '[', '"', 'links', '"', ']', '\\', 'n']
Detokenized (007): ['folderentries', '=', 'data', '[', '"links"', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "datareturn . append ( self . load_file ( fqpath , root = root , biossection = True , registries = True , datareturn = True ) ) \n"
Original    (028): ['datareturn', '.', 'append', '(', 'self', '.', 'load_file', '(', 'fqpath', ',', 'root', '=', 'root', ',', 'biossection', '=', 'True', ',', 'registries', '=', 'True', ',', 'datareturn', '=', 'True', ')', ')', '\\n']
Tokenized   (041): ['<s>', 'dat', 'aret', 'urn', '.', 'append', '(', 'self', '.', 'load', '_', 'file', '(', 'f', 'q', 'path', ',', 'root', '=', 'root', ',', 'bios', 'section', '=', 'True', ',', 'regist', 'ries', '=', 'True', ',', 'dat', 'aret', 'urn', '=', 'True', ')', ')', '\\', 'n', '</s>']
Filtered   (039): ['dat', 'aret', 'urn', '.', 'append', '(', 'self', '.', 'load', '_', 'file', '(', 'f', 'q', 'path', ',', 'root', '=', 'root', ',', 'bios', 'section', '=', 'True', ',', 'regist', 'ries', '=', 'True', ',', 'dat', 'aret', 'urn', '=', 'True', ')', ')', '\\', 'n']
Detokenized (028): ['datareturn', '.', 'append', '(', 'self', '.', 'load_file', '(', 'fqpath', ',', 'root', '=', 'root', ',', 'biossection', '=', 'True', ',', 'registries', '=', 'True', ',', 'datareturn', '=', 'True', ')', ')', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 28, 768)
# Extracted words:  28
Sentence         : "currdict = currdict , monolith = monolith , \n"
Original    (009): ['currdict', '=', 'currdict', ',', 'monolith', '=', 'monolith', ',', '\\n']
Tokenized   (018): ['<s>', 'cur', 'rd', 'ict', '=', 'cur', 'rd', 'ict', ',', 'mon', 'olith', '=', 'mon', 'olith', ',', '\\', 'n', '</s>']
Filtered   (016): ['cur', 'rd', 'ict', '=', 'cur', 'rd', 'ict', ',', 'mon', 'olith', '=', 'mon', 'olith', ',', '\\', 'n']
Detokenized (009): ['currdict', '=', 'currdict', ',', 'monolith', '=', 'monolith', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "newarg = newarg , checkall = checkall ) \n"
Original    (009): ['newarg', '=', 'newarg', ',', 'checkall', '=', 'checkall', ')', '\\n']
Tokenized   (016): ['<s>', 'new', 'arg', '=', 'new', 'arg', ',', 'check', 'all', '=', 'check', 'all', ')', '\\', 'n', '</s>']
Filtered   (014): ['new', 'arg', '=', 'new', 'arg', ',', 'check', 'all', '=', 'check', 'all', ')', '\\', 'n']
Detokenized (009): ['newarg', '=', 'newarg', ',', 'checkall', '=', 'checkall', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "attrreg = self . find_bios_registry ( regname = regname ) \n"
Original    (011): ['attrreg', '=', 'self', '.', 'find_bios_registry', '(', 'regname', '=', 'regname', ')', '\\n']
Tokenized   (023): ['<s>', 'attr', 'reg', '=', 'self', '.', 'find', '_', 'b', 'ios', '_', 'reg', 'istry', '(', 'reg', 'name', '=', 'reg', 'name', ')', '\\', 'n', '</s>']
Filtered   (021): ['attr', 'reg', '=', 'self', '.', 'find', '_', 'b', 'ios', '_', 'reg', 'istry', '(', 'reg', 'name', '=', 'reg', 'name', ')', '\\', 'n']
Detokenized (011): ['attrreg', '=', 'self', '.', 'find_bios_registry', '(', 'regname', '=', 'regname', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "schlink = schlink [ len ( schlink ) - 2 ] \n"
Original    (012): ['schlink', '=', 'schlink', '[', 'len', '(', 'schlink', ')', '-', '2', ']', '\\n']
Tokenized   (018): ['<s>', 'sch', 'link', '=', 'sch', 'link', '[', 'len', '(', 'sch', 'link', ')', '-', '2', ']', '\\', 'n', '</s>']
Filtered   (016): ['sch', 'link', '=', 'sch', 'link', '[', 'len', '(', 'sch', 'link', ')', '-', '2', ']', '\\', 'n']
Detokenized (012): ['schlink', '=', 'schlink', '[', 'len', '(', 'schlink', ')', '-', '2', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "schname . lower ( ) ) : \n"
Original    (008): ['schname', '.', 'lower', '(', ')', ')', ':', '\\n']
Tokenized   (013): ['<s>', 's', 'chn', 'ame', '.', 'lower', '(', ')', ')', ':', '\\', 'n', '</s>']
Filtered   (011): ['s', 'chn', 'ame', '.', 'lower', '(', ')', ')', ':', '\\', 'n']
Detokenized (008): ['schname', '.', 'lower', '(', ')', ')', ':', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "xref = os . path . normpath ( currloc . Uri . extref ) . lstrip ( os . path . sep ) \n"
Original    (024): ['xref', '=', 'os', '.', 'path', '.', 'normpath', '(', 'currloc', '.', 'Uri', '.', 'extref', ')', '.', 'lstrip', '(', 'os', '.', 'path', '.', 'sep', ')', '\\n']
Tokenized   (033): ['<s>', 'x', 'ref', '=', 'os', '.', 'path', '.', 'norm', 'path', '(', 'cur', 'r', 'loc', '.', 'Uri', '.', 'ext', 'ref', ')', '.', 'l', 'strip', '(', 'os', '.', 'path', '.', 'sep', ')', '\\', 'n', '</s>']
Filtered   (031): ['x', 'ref', '=', 'os', '.', 'path', '.', 'norm', 'path', '(', 'cur', 'r', 'loc', '.', 'Uri', '.', 'ext', 'ref', ')', '.', 'l', 'strip', '(', 'os', '.', 'path', '.', 'sep', ')', '\\', 'n']
Detokenized (024): ['xref', '=', 'os', '.', 'path', '.', 'normpath', '(', 'currloc', '.', 'Uri', '.', 'extref', ')', '.', 'lstrip', '(', 'os', '.', 'path', '.', 'sep', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "fqpath = os . path . join ( root , xref ) \n"
Original    (013): ['fqpath', '=', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'xref', ')', '\\n']
Tokenized   (019): ['<s>', 'f', 'q', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'x', 'ref', ')', '\\', 'n', '</s>']
Filtered   (017): ['f', 'q', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'x', 'ref', ')', '\\', 'n']
Detokenized (013): ['fqpath', '=', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'xref', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "langcode = list ( locale . getdefaultlocale ( ) ) \n"
Original    (011): ['langcode', '=', 'list', '(', 'locale', '.', 'getdefaultlocale', '(', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'lang', 'code', '=', 'list', '(', 'locale', '.', 'get', 'default', 'loc', 'ale', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['lang', 'code', '=', 'list', '(', 'locale', '.', 'get', 'default', 'loc', 'ale', '(', ')', ')', '\\', 'n']
Detokenized (011): ['langcode', '=', 'list', '(', 'locale', '.', 'getdefaultlocale', '(', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "locationlanguage = locationlanguage . replace ( "-" , "_" ) \n"
Original    (011): ['locationlanguage', '=', 'locationlanguage', '.', 'replace', '(', '"-"', ',', '"_"', ')', '\\n']
Tokenized   (018): ['<s>', 'location', 'language', '=', 'location', 'language', '.', 'replace', '(', '"', '-"', ',', '"_', '"', ')', '\\', 'n', '</s>']
Filtered   (016): ['location', 'language', '=', 'location', 'language', '.', 'replace', '(', '"', '-"', ',', '"_', '"', ')', '\\', 'n']
Detokenized (011): ['locationlanguage', '=', 'locationlanguage', '.', 'replace', '(', '"-"', ',', '"_"', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "currtype = currtype . split ( ) [ 0 ] + \n"
Original    (012): ['currtype', '=', 'currtype', '.', 'split', '(', ')', '[', '0', ']', '+', '\\n']
Tokenized   (019): ['<s>', 'cur', 'r', 'type', '=', 'cur', 'r', 'type', '.', 'split', '(', ')', '[', '0', ']', '+', '\\', 'n', '</s>']
Filtered   (017): ['cur', 'r', 'type', '=', 'cur', 'r', 'type', '.', 'split', '(', ')', '[', '0', ']', '+', '\\', 'n']
Detokenized (012): ['currtype', '=', 'currtype', '.', 'split', '(', ')', '[', '0', ']', '+', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "insttype = instance . resp . dict [ "title" ] . split ( ) [ : 1 ] \n"
Original    (019): ['insttype', '=', 'instance', '.', 'resp', '.', 'dict', '[', '"title"', ']', '.', 'split', '(', ')', '[', ':', '1', ']', '\\n']
Tokenized   (025): ['<s>', 'inst', 'type', '=', 'instance', '.', 'resp', '.', 'dict', '[', '"', 'title', '"', ']', '.', 'split', '(', ')', '[', ':', '1', ']', '\\', 'n', '</s>']
Filtered   (023): ['inst', 'type', '=', 'instance', '.', 'resp', '.', 'dict', '[', '"', 'title', '"', ']', '.', 'split', '(', ')', '[', ':', '1', ']', '\\', 'n']
Detokenized (019): ['insttype', '=', 'instance', '.', 'resp', '.', 'dict', '[', '"title"', ']', '.', 'split', '(', ')', '[', ':', '1', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "nextarg = newarg [ newarg . index ( arg ) + 1 ] \n"
Original    (014): ['nextarg', '=', 'newarg', '[', 'newarg', '.', 'index', '(', 'arg', ')', '+', '1', ']', '\\n']
Tokenized   (020): ['<s>', 'next', 'arg', '=', 'new', 'arg', '[', 'new', 'arg', '.', 'index', '(', 'arg', ')', '+', '1', ']', '\\', 'n', '</s>']
Filtered   (018): ['next', 'arg', '=', 'new', 'arg', '[', 'new', 'arg', '.', 'index', '(', 'arg', ')', '+', '1', ']', '\\', 'n']
Detokenized (014): ['nextarg', '=', 'newarg', '[', 'newarg', '.', 'index', '(', 'arg', ')', '+', '1', ']', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "regcopy [ nextarg ] = patterninfo \n"
Original    (007): ['regcopy', '[', 'nextarg', ']', '=', 'patterninfo', '\\n']
Tokenized   (013): ['<s>', 'reg', 'copy', '[', 'next', 'arg', ']', '=', 'pattern', 'info', '\\', 'n', '</s>']
Filtered   (011): ['reg', 'copy', '[', 'next', 'arg', ']', '=', 'pattern', 'info', '\\', 'n']
Detokenized (007): ['regcopy', '[', 'nextarg', ']', '=', 'patterninfo', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "validictory . validate ( tdict , jsonsch ) \n"
Original    (009): ['validictory', '.', 'validate', '(', 'tdict', ',', 'jsonsch', ')', '\\n']
Tokenized   (017): ['<s>', 'valid', 'ict', 'ory', '.', 'validate', '(', 't', 'dict', ',', 'js', 'ons', 'ch', ')', '\\', 'n', '</s>']
Filtered   (015): ['valid', 'ict', 'ory', '.', 'validate', '(', 't', 'dict', ',', 'js', 'ons', 'ch', ')', '\\', 'n']
Detokenized (009): ['validictory', '.', 'validate', '(', 'tdict', ',', 'jsonsch', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "wrapper . subsequent_indent = * 4 \n"
Original    (007): ['wrapper', '.', 'subsequent_indent', '=', '*', '4', '\\n']
Tokenized   (013): ['<s>', 'wrapper', '.', 'subsequent', '_', 'ind', 'ent', '=', '*', '4', '\\', 'n', '</s>']
Filtered   (011): ['wrapper', '.', 'subsequent', '_', 'ind', 'ent', '=', '*', '4', '\\', 'n']
Detokenized (007): ['wrapper', '.', 'subsequent_indent', '=', '*', '4', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "RegistryValidationError ( \n"
Original    (003): ['RegistryValidationError', '(', '\\n']
Tokenized   (010): ['<s>', 'Reg', 'istry', 'Val', 'idation', 'Error', '(', '\\', 'n', '</s>']
Filtered   (008): ['Reg', 'istry', 'Val', 'idation', 'Error', '(', '\\', 'n']
Detokenized (003): ['RegistryValidationError', '(', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "regentry = self \n"
Original    (004): ['regentry', '=', 'self', '\\n']
Tokenized   (008): ['<s>', 'reg', 'entry', '=', 'self', '\\', 'n', '</s>']
Filtered   (006): ['reg', 'entry', '=', 'self', '\\', 'n']
Detokenized (004): ['regentry', '=', 'self', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : ""\'%(ValueExpression)s\'" % ( self ) , regentry = self ) ) \n"
Original    (012): ['"\\\'%(ValueExpression)s\\\'"', '%', '(', 'self', ')', ',', 'regentry', '=', 'self', ')', ')', '\\n']
Tokenized   (026): ['<s>', '"', "\\'", '%', '(', 'Value', 'Exp', 'ression', ')', 's', '\\', '\'"', '%', '(', 'self', ')', ',', 'reg', 'entry', '=', 'self', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['"', "\\'", '%', '(', 'Value', 'Exp', 'ression', ')', 's', '\\', '\'"', '%', '(', 'self', ')', ',', 'reg', 'entry', '=', 'self', ')', ')', '\\', 'n']
Detokenized (012): ['"\\\'%(ValueExpression)s\\\'"', '%', '(', 'self', ')', ',', 'regentry', '=', 'self', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "intval = int ( newval ) \n"
Original    (007): ['intval', '=', 'int', '(', 'newval', ')', '\\n']
Tokenized   (012): ['<s>', 'int', 'val', '=', 'int', '(', 'new', 'val', ')', '\\', 'n', '</s>']
Filtered   (010): ['int', 'val', '=', 'int', '(', 'new', 'val', ')', '\\', 'n']
Detokenized (007): ['intval', '=', 'int', '(', 'newval', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "MICROS_TRANSLATIONS = ( \n"
Original    (004): ['MICROS_TRANSLATIONS', '=', '(', '\\n']
Tokenized   (014): ['<s>', 'MIC', 'R', 'OS', '_', 'TR', 'AN', 'SL', 'ATIONS', '=', '(', '\\', 'n', '</s>']
Filtered   (012): ['MIC', 'R', 'OS', '_', 'TR', 'AN', 'SL', 'ATIONS', '=', '(', '\\', 'n']
Detokenized (004): ['MICROS_TRANSLATIONS', '=', '(', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "MICROS_TRANSLATION_HASH = dict ( ( alt , v ) for k , v in MICROS_TRANSLATIONS for alt in k ) \n"
Original    (021): ['MICROS_TRANSLATION_HASH', '=', 'dict', '(', '(', 'alt', ',', 'v', ')', 'for', 'k', ',', 'v', 'in', 'MICROS_TRANSLATIONS', 'for', 'alt', 'in', 'k', ')', '\\n']
Tokenized   (041): ['<s>', 'MIC', 'R', 'OS', '_', 'TR', 'AN', 'SL', 'ATION', '_', 'H', 'ASH', '=', 'dict', '(', '(', 'alt', ',', 'v', ')', 'for', 'k', ',', 'v', 'in', 'MIC', 'R', 'OS', '_', 'TR', 'AN', 'SL', 'ATIONS', 'for', 'alt', 'in', 'k', ')', '\\', 'n', '</s>']
Filtered   (039): ['MIC', 'R', 'OS', '_', 'TR', 'AN', 'SL', 'ATION', '_', 'H', 'ASH', '=', 'dict', '(', '(', 'alt', ',', 'v', ')', 'for', 'k', ',', 'v', 'in', 'MIC', 'R', 'OS', '_', 'TR', 'AN', 'SL', 'ATIONS', 'for', 'alt', 'in', 'k', ')', '\\', 'n']
Detokenized (021): ['MICROS_TRANSLATION_HASH', '=', 'dict', '(', '(', 'alt', ',', 'v', ')', 'for', 'k', ',', 'v', 'in', 'MICROS_TRANSLATIONS', 'for', 'alt', 'in', 'k', ')', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "epoch_milliseconds = epoch_millis = milliseconds = millis = ms \n"
Original    (010): ['epoch_milliseconds', '=', 'epoch_millis', '=', 'milliseconds', '=', 'millis', '=', 'ms', '\\n']
Tokenized   (022): ['<s>', 'ep', 'och', '_', 'mill', 'isec', 'onds', '=', 'epoch', '_', 'mill', 'is', '=', 'milliseconds', '=', 'mill', 'is', '=', 'ms', '\\', 'n', '</s>']
Filtered   (020): ['ep', 'och', '_', 'mill', 'isec', 'onds', '=', 'epoch', '_', 'mill', 'is', '=', 'milliseconds', '=', 'mill', 'is', '=', 'ms', '\\', 'n']
Detokenized (010): ['epoch_milliseconds', '=', 'epoch_millis', '=', 'milliseconds', '=', 'millis', '=', 'ms', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "epoch_microseconds = epoch_micros = microseconds = micros \n"
Original    (008): ['epoch_microseconds', '=', 'epoch_micros', '=', 'microseconds', '=', 'micros', '\\n']
Tokenized   (020): ['<s>', 'ep', 'och', '_', 'micro', 'seconds', '=', 'epoch', '_', 'micro', 's', '=', 'micro', 'seconds', '=', 'micro', 's', '\\', 'n', '</s>']
Filtered   (018): ['ep', 'och', '_', 'micro', 'seconds', '=', 'epoch', '_', 'micro', 's', '=', 'micro', 'seconds', '=', 'micro', 's', '\\', 'n']
Detokenized (008): ['epoch_microseconds', '=', 'epoch_micros', '=', 'microseconds', '=', 'micros', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "micros = u".%06d" % dt . microsecond if dt . microsecond else \n"
Original    (013): ['micros', '=', 'u".%06d"', '%', 'dt', '.', 'microsecond', 'if', 'dt', '.', 'microsecond', 'else', '\\n']
Tokenized   (026): ['<s>', 'micro', 's', '=', 'u', '".', '%', '06', 'd', '"', '%', 'd', 't', '.', 'micro', 'second', 'if', 'd', 't', '.', 'micro', 'second', 'else', '\\', 'n', '</s>']
Filtered   (024): ['micro', 's', '=', 'u', '".', '%', '06', 'd', '"', '%', 'd', 't', '.', 'micro', 'second', 'if', 'd', 't', '.', 'micro', 'second', 'else', '\\', 'n']
Detokenized (013): ['micros', '=', 'u".%06d"', '%', 'dt', '.', 'microsecond', 'if', 'dt', '.', 'microsecond', 'else', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "datastore_owner_uuid = request . REQUEST [ "datastore_owner__uuid" ] \n"
Original    (009): ['datastore_owner_uuid', '=', 'request', '.', 'REQUEST', '[', '"datastore_owner__uuid"', ']', '\\n']
Tokenized   (029): ['<s>', 'dat', 'ast', 'ore', '_', 'owner', '_', 'uu', 'id', '=', 'request', '.', 'RE', 'QUEST', '[', '"', 'dat', 'ast', 'ore', '_', 'owner', '__', 'uu', 'id', '"', ']', '\\', 'n', '</s>']
Filtered   (027): ['dat', 'ast', 'ore', '_', 'owner', '_', 'uu', 'id', '=', 'request', '.', 'RE', 'QUEST', '[', '"', 'dat', 'ast', 'ore', '_', 'owner', '__', 'uu', 'id', '"', ']', '\\', 'n']
Detokenized (009): ['datastore_owner_uuid', '=', 'request', '.', 'REQUEST', '[', '"datastore_owner__uuid"', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "datastore_owner , ds_owner_created = Profile . objects . get_or_create ( uuid = datastore_owner_uuid ) \n"
Original    (015): ['datastore_owner', ',', 'ds_owner_created', '=', 'Profile', '.', 'objects', '.', 'get_or_create', '(', 'uuid', '=', 'datastore_owner_uuid', ')', '\\n']
Tokenized   (039): ['<s>', 'dat', 'ast', 'ore', '_', 'owner', ',', 'd', 's', '_', 'owner', '_', 'created', '=', 'Profile', '.', 'objects', '.', 'get', '_', 'or', '_', 'create', '(', 'u', 'uid', '=', 'dat', 'ast', 'ore', '_', 'owner', '_', 'uu', 'id', ')', '\\', 'n', '</s>']
Filtered   (037): ['dat', 'ast', 'ore', '_', 'owner', ',', 'd', 's', '_', 'owner', '_', 'created', '=', 'Profile', '.', 'objects', '.', 'get', '_', 'or', '_', 'create', '(', 'u', 'uid', '=', 'dat', 'ast', 'ore', '_', 'owner', '_', 'uu', 'id', ')', '\\', 'n']
Detokenized (015): ['datastore_owner', ',', 'ds_owner_created', '=', 'Profile', '.', 'objects', '.', 'get_or_create', '(', 'uuid', '=', 'datastore_owner_uuid', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "port = db . Column ( db . Integer , nullable = False ) \n"
Original    (015): ['port', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'nullable', '=', 'False', ')', '\\n']
Tokenized   (019): ['<s>', 'port', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'null', 'able', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (017): ['port', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'null', 'able', '=', 'False', ')', '\\', 'n']
Detokenized (015): ['port', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'nullable', '=', 'False', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "eru_container_id = db . Column ( db . String ( 64 ) , index = True ) \n"
Original    (018): ['eru_container_id', '=', 'db', '.', 'Column', '(', 'db', '.', 'String', '(', '64', ')', ',', 'index', '=', 'True', ')', '\\n']
Tokenized   (026): ['<s>', 'er', 'u', '_', 'container', '_', 'id', '=', 'db', '.', 'Column', '(', 'db', '.', 'String', '(', '64', ')', ',', 'index', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (024): ['er', 'u', '_', 'container', '_', 'id', '=', 'db', '.', 'Column', '(', 'db', '.', 'String', '(', '64', ')', ',', 'index', '=', 'True', ')', '\\', 'n']
Detokenized (018): ['eru_container_id', '=', 'db', '.', 'Column', '(', 'db', '.', 'String', '(', '64', ')', ',', 'index', '=', 'True', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "suppress_alert = db . Column ( db . Integer , nullable = False , default = 1 ) \n"
Original    (019): ['suppress_alert', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'nullable', '=', 'False', ',', 'default', '=', '1', ')', '\\n']
Tokenized   (026): ['<s>', 'supp', 'ress', '_', 'alert', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'null', 'able', '=', 'False', ',', 'default', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (024): ['supp', 'ress', '_', 'alert', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'null', 'able', '=', 'False', ',', 'default', '=', '1', ')', '\\', 'n']
Detokenized (019): ['suppress_alert', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'nullable', '=', 'False', ',', 'default', '=', '1', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "__table_args__ = ( db . Index ( , , , unique = True ) , ) \n"
Original    (017): ['__table_args__', '=', '(', 'db', '.', 'Index', '(', ',', ',', ',', 'unique', '=', 'True', ')', ',', ')', '\\n']
Tokenized   (024): ['<s>', '__', 'table', '_', 'args', '__', '=', '(', 'db', '.', 'Index', '(', ',', ',', ',', 'unique', '=', 'True', ')', ',', ')', '\\', 'n', '</s>']
Filtered   (022): ['__', 'table', '_', 'args', '__', '=', '(', 'db', '.', 'Index', '(', ',', ',', ',', 'unique', '=', 'True', ')', ',', ')', '\\', 'n']
Detokenized (017): ['__table_args__', '=', '(', 'db', '.', 'Index', '(', ',', ',', ',', 'unique', '=', 'True', ')', ',', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "cluster_id = cluster_id ) \n"
Original    (005): ['cluster_id', '=', 'cluster_id', ')', '\\n']
Tokenized   (013): ['<s>', 'cl', 'uster', '_', 'id', '=', 'cluster', '_', 'id', ')', '\\', 'n', '</s>']
Filtered   (011): ['cl', 'uster', '_', 'id', '=', 'cluster', '_', 'id', ')', '\\', 'n']
Detokenized (005): ['cluster_id', '=', 'cluster_id', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "Proxy . id . desc ( ) ) . offset ( offset ) . limit ( limit ) . all ( ) \n"
Original    (023): ['Proxy', '.', 'id', '.', 'desc', '(', ')', ')', '.', 'offset', '(', 'offset', ')', '.', 'limit', '(', 'limit', ')', '.', 'all', '(', ')', '\\n']
Tokenized   (026): ['<s>', 'Proxy', '.', 'id', '.', 'desc', '(', ')', ')', '.', 'offset', '(', 'offset', ')', '.', 'limit', '(', 'limit', ')', '.', 'all', '(', ')', '\\', 'n', '</s>']
Filtered   (024): ['Proxy', '.', 'id', '.', 'desc', '(', ')', ')', '.', 'offset', '(', 'offset', ')', '.', 'limit', '(', 'limit', ')', '.', 'all', '(', ')', '\\', 'n']
Detokenized (023): ['Proxy', '.', 'id', '.', 'desc', '(', ')', ')', '.', 'offset', '(', 'offset', ')', '.', 'limit', '(', 'limit', ')', '.', 'all', '(', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "iou_as_issuer . issue_funds ( amount_issued , ) \n"
Original    (008): ['iou_as_issuer', '.', 'issue_funds', '(', 'amount_issued', ',', ')', '\\n']
Tokenized   (022): ['<s>', 'i', 'ou', '_', 'as', '_', 'iss', 'uer', '.', 'issue', '_', 'fund', 's', '(', 'amount', '_', 'issued', ',', ')', '\\', 'n', '</s>']
Filtered   (020): ['i', 'ou', '_', 'as', '_', 'iss', 'uer', '.', 'issue', '_', 'fund', 's', '(', 'amount', '_', 'issued', ',', ')', '\\', 'n']
Detokenized (008): ['iou_as_issuer', '.', 'issue_funds', '(', 'amount_issued', ',', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "github_info_json = urllib2 . urlopen ( latest ) . read ( ) \n"
Original    (013): ['github_info_json', '=', 'urllib2', '.', 'urlopen', '(', 'latest', ')', '.', 'read', '(', ')', '\\n']
Tokenized   (024): ['<s>', 'github', '_', 'info', '_', 'json', '=', 'ur', 'll', 'ib', '2', '.', 'url', 'open', '(', 'latest', ')', '.', 'read', '(', ')', '\\', 'n', '</s>']
Filtered   (022): ['github', '_', 'info', '_', 'json', '=', 'ur', 'll', 'ib', '2', '.', 'url', 'open', '(', 'latest', ')', '.', 'read', '(', ')', '\\', 'n']
Detokenized (013): ['github_info_json', '=', 'urllib2', '.', 'urlopen', '(', 'latest', ')', '.', 'read', '(', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "drawing_tool . set_window_title ( update_notifier , watching_player = twitch_username , updates_queued = len ( new_states_queue ) , read_delay = opt . read_delay ) \n"
Original    (024): ['drawing_tool', '.', 'set_window_title', '(', 'update_notifier', ',', 'watching_player', '=', 'twitch_username', ',', 'updates_queued', '=', 'len', '(', 'new_states_queue', ')', ',', 'read_delay', '=', 'opt', '.', 'read_delay', ')', '\\n']
Tokenized   (052): ['<s>', 'draw', 'ing', '_', 'tool', '.', 'set', '_', 'window', '_', 'title', '(', 'update', '_', 'not', 'ifier', ',', 'watching', '_', 'player', '=', 'twitch', '_', 'username', ',', 'updates', '_', 'que', 'ued', '=', 'len', '(', 'new', '_', 'states', '_', 'queue', ')', ',', 'read', '_', 'delay', '=', 'opt', '.', 'read', '_', 'delay', ')', '\\', 'n', '</s>']
Filtered   (050): ['draw', 'ing', '_', 'tool', '.', 'set', '_', 'window', '_', 'title', '(', 'update', '_', 'not', 'ifier', ',', 'watching', '_', 'player', '=', 'twitch', '_', 'username', ',', 'updates', '_', 'que', 'ued', '=', 'len', '(', 'new', '_', 'states', '_', 'queue', ')', ',', 'read', '_', 'delay', '=', 'opt', '.', 'read', '_', 'delay', ')', '\\', 'n']
Detokenized (024): ['drawing_tool', '.', 'set_window_title', '(', 'update_notifier', ',', 'watching_player', '=', 'twitch_username', ',', 'updates_queued', '=', 'len', '(', 'new_states_queue', ')', ',', 'read_delay', '=', 'opt', '.', 'read_delay', ')', '\\n']
Counter: 50
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "put_url = opt . trackerserver_url + "/tracker/api/update/" + opt . trackerserver_authkey \n"
Original    (012): ['put_url', '=', 'opt', '.', 'trackerserver_url', '+', '"/tracker/api/update/"', '+', 'opt', '.', 'trackerserver_authkey', '\\n']
Tokenized   (033): ['<s>', 'put', '_', 'url', '=', 'opt', '.', 'track', 'ers', 'erver', '_', 'url', '+', '"/', 'tr', 'acker', '/', 'api', '/', 'update', '/"', '+', 'opt', '.', 'track', 'ers', 'erver', '_', 'auth', 'key', '\\', 'n', '</s>']
Filtered   (031): ['put', '_', 'url', '=', 'opt', '.', 'track', 'ers', 'erver', '_', 'url', '+', '"/', 'tr', 'acker', '/', 'api', '/', 'update', '/"', '+', 'opt', '.', 'track', 'ers', 'erver', '_', 'auth', 'key', '\\', 'n']
Detokenized (012): ['put_url', '=', 'opt', '.', 'trackerserver_url', '+', '"/tracker/api/update/"', '+', 'opt', '.', 'trackerserver_authkey', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "json_string = json . dumps ( state , cls = TrackerStateEncoder , sort_keys = True ) \n"
Original    (017): ['json_string', '=', 'json', '.', 'dumps', '(', 'state', ',', 'cls', '=', 'TrackerStateEncoder', ',', 'sort_keys', '=', 'True', ')', '\\n']
Tokenized   (028): ['<s>', 'json', '_', 'string', '=', 'json', '.', 'dumps', '(', 'state', ',', 'cl', 's', '=', 'Tracker', 'State', 'Enc', 'oder', ',', 'sort', '_', 'keys', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (026): ['json', '_', 'string', '=', 'json', '.', 'dumps', '(', 'state', ',', 'cl', 's', '=', 'Tracker', 'State', 'Enc', 'oder', ',', 'sort', '_', 'keys', '=', 'True', ')', '\\', 'n']
Detokenized (017): ['json_string', '=', 'json', '.', 'dumps', '(', 'state', ',', 'cls', '=', 'TrackerStateEncoder', ',', 'sort_keys', '=', 'True', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "new_states_queue . pop ( 0 ) \n"
Original    (007): ['new_states_queue', '.', 'pop', '(', '0', ')', '\\n']
Tokenized   (014): ['<s>', 'new', '_', 'states', '_', 'queue', '.', 'pop', '(', '0', ')', '\\', 'n', '</s>']
Filtered   (012): ['new', '_', 'states', '_', 'queue', '.', 'pop', '(', '0', ')', '\\', 'n']
Detokenized (007): ['new_states_queue', '.', 'pop', '(', '0', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "framecount += 1 \n"
Original    (004): ['framecount', '+=', '1', '\\n']
Tokenized   (008): ['<s>', 'frame', 'count', '+=', '1', '\\', 'n', '</s>']
Filtered   (006): ['frame', 'count', '+=', '1', '\\', 'n']
Detokenized (004): ['framecount', '+=', '1', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "decay = decay , \n"
Original    (005): ['decay', '=', 'decay', ',', '\\n']
Tokenized   (009): ['<s>', 'dec', 'ay', '=', 'decay', ',', '\\', 'n', '</s>']
Filtered   (007): ['dec', 'ay', '=', 'decay', ',', '\\', 'n']
Detokenized (005): ['decay', '=', 'decay', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "expected_kwargs = { , } \n"
Original    (006): ['expected_kwargs', '=', '{', ',', '}', '\\n']
Tokenized   (012): ['<s>', 'expected', '_', 'kw', 'args', '=', '{', ',', '}', '\\', 'n', '</s>']
Filtered   (010): ['expected', '_', 'kw', 'args', '=', '{', ',', '}', '\\', 'n']
Detokenized (006): ['expected_kwargs', '=', '{', ',', '}', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "outputs [ ] = in_shapes [ ] \n"
Original    (008): ['outputs', '[', ']', '=', 'in_shapes', '[', ']', '\\n']
Tokenized   (015): ['<s>', 'output', 's', '[', ']', '=', 'in', '_', 'sh', 'apes', '[', ']', '\\', 'n', '</s>']
Filtered   (013): ['output', 's', '[', ']', '=', 'in', '_', 'sh', 'apes', '[', ']', '\\', 'n']
Detokenized (008): ['outputs', '[', ']', '=', 'in_shapes', '[', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "buf = BufferStructure ( self . in_shapes [ ] . feature_shape [ - 1 ] ) \n"
Original    (017): ['buf', '=', 'BufferStructure', '(', 'self', '.', 'in_shapes', '[', ']', '.', 'feature_shape', '[', '-', '1', ']', ')', '\\n']
Tokenized   (027): ['<s>', 'buf', '=', 'Buffer', 'St', 'ructure', '(', 'self', '.', 'in', '_', 'sh', 'apes', '[', ']', '.', 'feature', '_', 'shape', '[', '-', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (025): ['buf', '=', 'Buffer', 'St', 'ructure', '(', 'self', '.', 'in', '_', 'sh', 'apes', '[', ']', '.', 'feature', '_', 'shape', '[', '-', '1', ']', ')', '\\', 'n']
Detokenized (017): ['buf', '=', 'BufferStructure', '(', 'self', '.', 'in_shapes', '[', ']', '.', 'feature_shape', '[', '-', '1', ']', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "internals [ ] = self . in_shapes [ ] \n"
Original    (010): ['internals', '[', ']', '=', 'self', '.', 'in_shapes', '[', ']', '\\n']
Tokenized   (017): ['<s>', 'intern', 'als', '[', ']', '=', 'self', '.', 'in', '_', 'sh', 'apes', '[', ']', '\\', 'n', '</s>']
Filtered   (015): ['intern', 'als', '[', ']', '=', 'self', '.', 'in', '_', 'sh', 'apes', '[', ']', '\\', 'n']
Detokenized (010): ['internals', '[', ']', '=', 'self', '.', 'in_shapes', '[', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sigma_b , centered , x_hat = buffers . internals \n"
Original    (010): ['sigma_b', ',', 'centered', ',', 'x_hat', '=', 'buffers', '.', 'internals', '\\n']
Tokenized   (019): ['<s>', 's', 'igma', '_', 'b', ',', 'centered', ',', 'x', '_', 'hat', '=', 'buffers', '.', 'intern', 'als', '\\', 'n', '</s>']
Filtered   (017): ['s', 'igma', '_', 'b', ',', 'centered', ',', 'x', '_', 'hat', '=', 'buffers', '.', 'intern', 'als', '\\', 'n']
Detokenized (010): ['sigma_b', ',', 'centered', ',', 'x_hat', '=', 'buffers', '.', 'internals', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "dgamma = buffers . gradients . gamma \n"
Original    (008): ['dgamma', '=', 'buffers', '.', 'gradients', '.', 'gamma', '\\n']
Tokenized   (014): ['<s>', 'd', 'gam', 'ma', '=', 'buffers', '.', 'grad', 'ients', '.', 'gamma', '\\', 'n', '</s>']
Filtered   (012): ['d', 'gam', 'ma', '=', 'buffers', '.', 'grad', 'ients', '.', 'gamma', '\\', 'n']
Detokenized (008): ['dgamma', '=', 'buffers', '.', 'gradients', '.', 'gamma', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "outdeltas = flatten_all_but_last ( buffers . output_deltas . default ) \n"
Original    (011): ['outdeltas', '=', 'flatten_all_but_last', '(', 'buffers', '.', 'output_deltas', '.', 'default', ')', '\\n']
Tokenized   (028): ['<s>', 'out', 'd', 'elt', 'as', '=', 'flatt', 'en', '_', 'all', '_', 'but', '_', 'last', '(', 'buffers', '.', 'output', '_', 'd', 'elt', 'as', '.', 'default', ')', '\\', 'n', '</s>']
Filtered   (026): ['out', 'd', 'elt', 'as', '=', 'flatt', 'en', '_', 'all', '_', 'but', '_', 'last', '(', 'buffers', '.', 'output', '_', 'd', 'elt', 'as', '.', 'default', ')', '\\', 'n']
Detokenized (011): ['outdeltas', '=', 'flatten_all_but_last', '(', 'buffers', '.', 'output_deltas', '.', 'default', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "_h . add_tt ( term4 , indeltas , indeltas ) \n"
Original    (011): ['_h', '.', 'add_tt', '(', 'term4', ',', 'indeltas', ',', 'indeltas', ')', '\\n']
Tokenized   (022): ['<s>', '_', 'h', '.', 'add', '_', 'tt', '(', 'term', '4', ',', 'ind', 'elt', 'as', ',', 'ind', 'elt', 'as', ')', '\\', 'n', '</s>']
Filtered   (020): ['_', 'h', '.', 'add', '_', 'tt', '(', 'term', '4', ',', 'ind', 'elt', 'as', ',', 'ind', 'elt', 'as', ')', '\\', 'n']
Detokenized (011): ['_h', '.', 'add_tt', '(', 'term4', ',', 'indeltas', ',', 'indeltas', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "targets_name = , mask_name = ) : \n"
Original    (008): ['targets_name', '=', ',', 'mask_name', '=', ')', ':', '\\n']
Tokenized   (017): ['<s>', 't', 'arg', 'ets', '_', 'name', '=', ',', 'mask', '_', 'name', '=', ')', ':', '\\', 'n', '</s>']
Filtered   (015): ['t', 'arg', 'ets', '_', 'name', '=', ',', 'mask', '_', 'name', '=', ')', ':', '\\', 'n']
Detokenized (008): ['targets_name', '=', ',', 'mask_name', '=', ')', ':', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "mask_name = , name = None ) : \n"
Original    (009): ['mask_name', '=', ',', 'name', '=', 'None', ')', ':', '\\n']
Tokenized   (014): ['<s>', 'mask', '_', 'name', '=', ',', 'name', '=', 'None', ')', ':', '\\', 'n', '</s>']
Filtered   (012): ['mask', '_', 'name', '=', ',', 'name', '=', 'None', ')', ':', '\\', 'n']
Detokenized (009): ['mask_name', '=', ',', 'name', '=', 'None', ')', ':', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "true_labels ) . astype ( np . float ) \n"
Original    (010): ['true_labels', ')', '.', 'astype', '(', 'np', '.', 'float', ')', '\\n']
Tokenized   (017): ['<s>', 'true', '_', 'lab', 'els', ')', '.', 'ast', 'ype', '(', 'np', '.', 'float', ')', '\\', 'n', '</s>']
Filtered   (015): ['true', '_', 'lab', 'els', ')', '.', 'ast', 'ype', '(', 'np', '.', 'float', ')', '\\', 'n']
Detokenized (010): ['true_labels', ')', '.', 'astype', '(', 'np', '.', 'float', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "epochs = [ 0 ] * 4 + [ 1 ] * 4 + [ 2 ] * 4 \n"
Original    (020): ['epochs', '=', '[', '0', ']', '*', '4', '+', '[', '1', ']', '*', '4', '+', '[', '2', ']', '*', '4', '\\n']
Tokenized   (025): ['<s>', 'ep', 'och', 's', '=', '[', '0', ']', '*', '4', '+', '[', '1', ']', '*', '4', '+', '[', '2', ']', '*', '4', '\\', 'n', '</s>']
Filtered   (023): ['ep', 'och', 's', '=', '[', '0', ']', '*', '4', '+', '[', '1', ']', '*', '4', '+', '[', '2', ']', '*', '4', '\\', 'n']
Detokenized (020): ['epochs', '=', '[', '0', ']', '*', '4', '+', '[', '1', ']', '*', '4', '+', '[', '2', ']', '*', '4', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "on_rtd = os . environ . get ( , None ) == \n"
Original    (013): ['on_rtd', '=', 'os', '.', 'environ', '.', 'get', '(', ',', 'None', ')', '==', '\\n']
Tokenized   (020): ['<s>', 'on', '_', 'r', 'td', '=', 'os', '.', 'en', 'viron', '.', 'get', '(', ',', 'None', ')', '==', '\\', 'n', '</s>']
Filtered   (018): ['on', '_', 'r', 'td', '=', 'os', '.', 'en', 'viron', '.', 'get', '(', ',', 'None', ')', '==', '\\', 'n']
Detokenized (013): ['on_rtd', '=', 'os', '.', 'environ', '.', 'get', '(', ',', 'None', ')', '==', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "html_theme_path = [ sphinx_rtd_theme . get_html_theme_path ( ) ] \n"
Original    (010): ['html_theme_path', '=', '[', 'sphinx_rtd_theme', '.', 'get_html_theme_path', '(', ')', ']', '\\n']
Tokenized   (030): ['<s>', 'html', '_', 'theme', '_', 'path', '=', '[', 'sp', 'hin', 'x', '_', 'r', 'td', '_', 'theme', '.', 'get', '_', 'html', '_', 'theme', '_', 'path', '(', ')', ']', '\\', 'n', '</s>']
Filtered   (028): ['html', '_', 'theme', '_', 'path', '=', '[', 'sp', 'hin', 'x', '_', 'r', 'td', '_', 'theme', '.', 'get', '_', 'html', '_', 'theme', '_', 'path', '(', ')', ']', '\\', 'n']
Detokenized (010): ['html_theme_path', '=', '[', 'sphinx_rtd_theme', '.', 'get_html_theme_path', '(', ')', ']', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "latex_elements = { \n"
Original    (004): ['latex_elements', '=', '{', '\\n']
Tokenized   (011): ['<s>', 'late', 'x', '_', 'e', 'lements', '=', '{', '\\', 'n', '</s>']
Filtered   (009): ['late', 'x', '_', 'e', 'lements', '=', '{', '\\', 'n']
Detokenized (004): ['latex_elements', '=', '{', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "latex_documents = [ \n"
Original    (004): ['latex_documents', '=', '[', '\\n']
Tokenized   (011): ['<s>', 'late', 'x', '_', 'doc', 'uments', '=', '[', '\\', 'n', '</s>']
Filtered   (009): ['late', 'x', '_', 'doc', 'uments', '=', '[', '\\', 'n']
Detokenized (004): ['latex_documents', '=', '[', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "ignored_fallbacks = ( ) ) : \n"
Original    (007): ['ignored_fallbacks', '=', '(', ')', ')', ':', '\\n']
Tokenized   (014): ['<s>', 'ign', 'ored', '_', 'fall', 'backs', '=', '(', ')', ')', ':', '\\', 'n', '</s>']
Filtered   (012): ['ign', 'ored', '_', 'fall', 'backs', '=', '(', ')', ')', ':', '\\', 'n']
Detokenized (007): ['ignored_fallbacks', '=', '(', ')', ')', ':', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""b" : 2.0 , \n"
Original    (005): ['"b"', ':', '2.0', ',', '\\n']
Tokenized   (012): ['<s>', '"', 'b', '"', ':', '2', '.', '0', ',', '\\', 'n', '</s>']
Filtered   (010): ['"', 'b', '"', ':', '2', '.', '0', ',', '\\', 'n']
Detokenized (005): ['"b"', ':', '2.0', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""c" : True , \n"
Original    (005): ['"c"', ':', 'True', ',', '\\n']
Tokenized   (010): ['<s>', '"', 'c', '"', ':', 'True', ',', '\\', 'n', '</s>']
Filtered   (008): ['"', 'c', '"', ':', 'True', ',', '\\', 'n']
Detokenized (005): ['"c"', ':', 'True', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""d" : , \n"
Original    (004): ['"d"', ':', ',', '\\n']
Tokenized   (009): ['<s>', '"', 'd', '"', ':', ',', '\\', 'n', '</s>']
Filtered   (007): ['"', 'd', '"', ':', ',', '\\', 'n']
Detokenized (004): ['"d"', ':', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : ""e" : [ 1 , 2 , 3 ] , \n"
Original    (011): ['"e"', ':', '[', '1', ',', '2', ',', '3', ']', ',', '\\n']
Tokenized   (016): ['<s>', '"', 'e', '"', ':', '[', '1', ',', '2', ',', '3', ']', ',', '\\', 'n', '</s>']
Filtered   (014): ['"', 'e', '"', ':', '[', '1', ',', '2', ',', '3', ']', ',', '\\', 'n']
Detokenized (011): ['"e"', ':', '[', '1', ',', '2', ',', '3', ']', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""f" : { : , : } , \n"
Original    (009): ['"f"', ':', '{', ':', ',', ':', '}', ',', '\\n']
Tokenized   (014): ['<s>', '"', 'f', '"', ':', '{', ':', ',', ':', '}', ',', '\\', 'n', '</s>']
Filtered   (012): ['"', 'f', '"', ':', '{', ':', ',', ':', '}', ',', '\\', 'n']
Detokenized (009): ['"f"', ':', '{', ':', ',', ':', '}', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ""answer" : 42 \n"
Original    (004): ['"answer"', ':', '42', '\\n']
Tokenized   (009): ['<s>', '"', 'answer', '"', ':', '42', '\\', 'n', '</s>']
Filtered   (007): ['"', 'answer', '"', ':', '42', '\\', 'n']
Detokenized (004): ['"answer"', ':', '42', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "p_error = self . kp * current_error \n"
Original    (008): ['p_error', '=', 'self', '.', 'kp', '*', 'current_error', '\\n']
Tokenized   (016): ['<s>', 'p', '_', 'error', '=', 'self', '.', 'k', 'p', '*', 'current', '_', 'error', '\\', 'n', '</s>']
Filtered   (014): ['p', '_', 'error', '=', 'self', '.', 'k', 'p', '*', 'current', '_', 'error', '\\', 'n']
Detokenized (008): ['p_error', '=', 'self', '.', 'kp', '*', 'current_error', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "d_error = self . kd * ( current_error - self . previous_error ) / timestep \n"
Original    (016): ['d_error', '=', 'self', '.', 'kd', '*', '(', 'current_error', '-', 'self', '.', 'previous_error', ')', '/', 'timestep', '\\n']
Tokenized   (028): ['<s>', 'd', '_', 'error', '=', 'self', '.', 'k', 'd', '*', '(', 'current', '_', 'error', '-', 'self', '.', 'previous', '_', 'error', ')', '/', 'tim', 'est', 'ep', '\\', 'n', '</s>']
Filtered   (026): ['d', '_', 'error', '=', 'self', '.', 'k', 'd', '*', '(', 'current', '_', 'error', '-', 'self', '.', 'previous', '_', 'error', ')', '/', 'tim', 'est', 'ep', '\\', 'n']
Detokenized (016): ['d_error', '=', 'self', '.', 'kd', '*', '(', 'current_error', '-', 'self', '.', 'previous_error', ')', '/', 'timestep', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "current_error + self . previous_error ) / 2 + self . integral_error \n"
Original    (013): ['current_error', '+', 'self', '.', 'previous_error', ')', '/', '2', '+', 'self', '.', 'integral_error', '\\n']
Tokenized   (022): ['<s>', 'current', '_', 'error', '+', 'self', '.', 'previous', '_', 'error', ')', '/', '2', '+', 'self', '.', 'integral', '_', 'error', '\\', 'n', '</s>']
Filtered   (020): ['current', '_', 'error', '+', 'self', '.', 'previous', '_', 'error', ')', '/', '2', '+', 'self', '.', 'integral', '_', 'error', '\\', 'n']
Detokenized (013): ['current_error', '+', 'self', '.', 'previous_error', ')', '/', '2', '+', 'self', '.', 'integral_error', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "i_error = self . ki * self . integral_error \n"
Original    (010): ['i_error', '=', 'self', '.', 'ki', '*', 'self', '.', 'integral_error', '\\n']
Tokenized   (017): ['<s>', 'i', '_', 'error', '=', 'self', '.', 'ki', '*', 'self', '.', 'integral', '_', 'error', '\\', 'n', '</s>']
Filtered   (015): ['i', '_', 'error', '=', 'self', '.', 'ki', '*', 'self', '.', 'integral', '_', 'error', '\\', 'n']
Detokenized (010): ['i_error', '=', 'self', '.', 'ki', '*', 'self', '.', 'integral_error', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "total_error = p_error + d_error + i_error \n"
Original    (008): ['total_error', '=', 'p_error', '+', 'd_error', '+', 'i_error', '\\n']
Tokenized   (019): ['<s>', 'total', '_', 'error', '=', 'p', '_', 'error', '+', 'd', '_', 'error', '+', 'i', '_', 'error', '\\', 'n', '</s>']
Filtered   (017): ['total', '_', 'error', '=', 'p', '_', 'error', '+', 'd', '_', 'error', '+', 'i', '_', 'error', '\\', 'n']
Detokenized (008): ['total_error', '=', 'p_error', '+', 'd_error', '+', 'i_error', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "cmd_match_names = cmd . Cmd . completenames ( self , text , * ignored ) \n"
Original    (016): ['cmd_match_names', '=', 'cmd', '.', 'Cmd', '.', 'completenames', '(', 'self', ',', 'text', ',', '*', 'ignored', ')', '\\n']
Tokenized   (026): ['<s>', 'cmd', '_', 'match', '_', 'names', '=', 'cmd', '.', 'C', 'md', '.', 'comple', 'ten', 'ames', '(', 'self', ',', 'text', ',', '*', 'ignored', ')', '\\', 'n', '</s>']
Filtered   (024): ['cmd', '_', 'match', '_', 'names', '=', 'cmd', '.', 'C', 'md', '.', 'comple', 'ten', 'ames', '(', 'self', ',', 'text', ',', '*', 'ignored', ')', '\\', 'n']
Detokenized (016): ['cmd_match_names', '=', 'cmd', '.', 'Cmd', '.', 'completenames', '(', 'self', ',', 'text', ',', '*', 'ignored', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "obj_names = self . ctrl_client . objects . keys ( ) \n"
Original    (012): ['obj_names', '=', 'self', '.', 'ctrl_client', '.', 'objects', '.', 'keys', '(', ')', '\\n']
Tokenized   (020): ['<s>', 'obj', '_', 'names', '=', 'self', '.', 'c', 'trl', '_', 'client', '.', 'objects', '.', 'keys', '(', ')', '\\', 'n', '</s>']
Filtered   (018): ['obj', '_', 'names', '=', 'self', '.', 'c', 'trl', '_', 'client', '.', 'objects', '.', 'keys', '(', ')', '\\', 'n']
Detokenized (012): ['obj_names', '=', 'self', '.', 'ctrl_client', '.', 'objects', '.', 'keys', '(', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "api_match_names = [ x for x in obj_names if x . startswith ( text ) ] \n"
Original    (017): ['api_match_names', '=', '[', 'x', 'for', 'x', 'in', 'obj_names', 'if', 'x', '.', 'startswith', '(', 'text', ')', ']', '\\n']
Tokenized   (028): ['<s>', 'api', '_', 'match', '_', 'names', '=', '[', 'x', 'for', 'x', 'in', 'obj', '_', 'names', 'if', 'x', '.', 'start', 'sw', 'ith', '(', 'text', ')', ']', '\\', 'n', '</s>']
Filtered   (026): ['api', '_', 'match', '_', 'names', '=', '[', 'x', 'for', 'x', 'in', 'obj', '_', 'names', 'if', 'x', '.', 'start', 'sw', 'ith', '(', 'text', ')', ']', '\\', 'n']
Detokenized (017): ['api_match_names', '=', '[', 'x', 'for', 'x', 'in', 'obj_names', 'if', 'x', '.', 'startswith', '(', 'text', ')', ']', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "reply_time = self . ctrl_client . ping ( ) \n"
Original    (010): ['reply_time', '=', 'self', '.', 'ctrl_client', '.', 'ping', '(', ')', '\\n']
Tokenized   (018): ['<s>', 'reply', '_', 'time', '=', 'self', '.', 'c', 'trl', '_', 'client', '.', 'ping', '(', ')', '\\', 'n', '</s>']
Filtered   (016): ['reply', '_', 'time', '=', 'self', '.', 'c', 'trl', '_', 'client', '.', 'ping', '(', ')', '\\', 'n']
Detokenized (010): ['reply_time', '=', 'self', '.', 'ctrl_client', '.', 'ping', '(', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sub_addr = sys . argv [ 2 ] \n"
Original    (009): ['sub_addr', '=', 'sys', '.', 'argv', '[', '2', ']', '\\n']
Tokenized   (015): ['<s>', 'sub', '_', 'addr', '=', 'sys', '.', 'arg', 'v', '[', '2', ']', '\\', 'n', '</s>']
Filtered   (013): ['sub', '_', 'addr', '=', 'sys', '.', 'arg', 'v', '[', '2', ']', '\\', 'n']
Detokenized (009): ['sub_addr', '=', 'sys', '.', 'argv', '[', '2', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "CLI ( ctrl_addr , sub_addr ) . cmdloop ( ) \n"
Original    (011): ['CLI', '(', 'ctrl_addr', ',', 'sub_addr', ')', '.', 'cmdloop', '(', ')', '\\n']
Tokenized   (021): ['<s>', 'CL', 'I', '(', 'c', 'trl', '_', 'addr', ',', 'sub', '_', 'addr', ')', '.', 'cmd', 'loop', '(', ')', '\\', 'n', '</s>']
Filtered   (019): ['CL', 'I', '(', 'c', 'trl', '_', 'addr', ',', 'sub', '_', 'addr', ')', '.', 'cmd', 'loop', '(', ')', '\\', 'n']
Detokenized (011): ['CLI', '(', 'ctrl_addr', ',', 'sub_addr', ')', '.', 'cmdloop', '(', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "duty = int ( cur_pwm [ "duty_ns" ] ) \n"
Original    (010): ['duty', '=', 'int', '(', 'cur_pwm', '[', '"duty_ns"', ']', ')', '\\n']
Tokenized   (020): ['<s>', 'duty', '=', 'int', '(', 'cur', '_', 'p', 'wm', '[', '"', 'duty', '_', 'ns', '"', ']', ')', '\\', 'n', '</s>']
Filtered   (018): ['duty', '=', 'int', '(', 'cur', '_', 'p', 'wm', '[', '"', 'duty', '_', 'ns', '"', ']', ')', '\\', 'n']
Detokenized (010): ['duty', '=', 'int', '(', 'cur_pwm', '[', '"duty_ns"', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "read_pos = int ( round ( ( ( duty - 580000 ) / 2320000. ) * 180 ) ) \n"
Original    (020): ['read_pos', '=', 'int', '(', 'round', '(', '(', '(', 'duty', '-', '580000', ')', '/', '2320000.', ')', '*', '180', ')', ')', '\\n']
Tokenized   (029): ['<s>', 'read', '_', 'pos', '=', 'int', '(', 'round', '(', '(', '(', 'duty', '-', '58', '0000', ')', '/', '23', '200', '00', '.', ')', '*', '180', ')', ')', '\\', 'n', '</s>']
Filtered   (027): ['read', '_', 'pos', '=', 'int', '(', 'round', '(', '(', '(', 'duty', '-', '58', '0000', ')', '/', '23', '200', '00', '.', ')', '*', '180', ')', ')', '\\', 'n']
Detokenized (020): ['read_pos', '=', 'int', '(', 'round', '(', '(', '(', 'duty', '-', '580000', ')', '/', '2320000.', ')', '*', '180', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "create_login_url , create_logout_url \n"
Original    (004): ['create_login_url', ',', 'create_logout_url', '\\n']
Tokenized   (016): ['<s>', 'create', '_', 'login', '_', 'url', ',', 'create', '_', 'log', 'out', '_', 'url', '\\', 'n', '</s>']
Filtered   (014): ['create', '_', 'login', '_', 'url', ',', 'create', '_', 'log', 'out', '_', 'url', '\\', 'n']
Detokenized (004): ['create_login_url', ',', 'create_logout_url', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "create_logout_url ( request . url ) \n"
Original    (007): ['create_logout_url', '(', 'request', '.', 'url', ')', '\\n']
Tokenized   (015): ['<s>', 'create', '_', 'log', 'out', '_', 'url', '(', 'request', '.', 'url', ')', '\\', 'n', '</s>']
Filtered   (013): ['create', '_', 'log', 'out', '_', 'url', '(', 'request', '.', 'url', ')', '\\', 'n']
Detokenized (007): ['create_logout_url', '(', 'request', '.', 'url', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "_value = _options_header_vkw ( _value , kw ) \n"
Original    (009): ['_value', '=', '_options_header_vkw', '(', '_value', ',', 'kw', ')', '\\n']
Tokenized   (021): ['<s>', '_', 'value', '=', '_', 'options', '_', 'header', '_', 'v', 'kw', '(', '_', 'value', ',', 'k', 'w', ')', '\\', 'n', '</s>']
Filtered   (019): ['_', 'value', '=', '_', 'options', '_', 'header', '_', 'v', 'kw', '(', '_', 'value', ',', 'k', 'w', ')', '\\', 'n']
Detokenized (009): ['_value', '=', '_options_header_vkw', '(', '_value', ',', 'kw', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "value_type == value_subtype == ) or \n"
Original    (007): ['value_type', '==', 'value_subtype', '==', ')', 'or', '\\n']
Tokenized   (015): ['<s>', 'value', '_', 'type', '==', 'value', '_', 'sub', 'type', '==', ')', 'or', '\\', 'n', '</s>']
Filtered   (013): ['value', '_', 'type', '==', 'value', '_', 'sub', 'type', '==', ')', 'or', '\\', 'n']
Detokenized (007): ['value_type', '==', 'value_subtype', '==', ')', 'or', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "value_subtype == or \n"
Original    (004): ['value_subtype', '==', 'or', '\\n']
Tokenized   (010): ['<s>', 'value', '_', 'sub', 'type', '==', 'or', '\\', 'n', '</s>']
Filtered   (008): ['value', '_', 'sub', 'type', '==', 'or', '\\', 'n']
Detokenized (004): ['value_subtype', '==', 'or', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "item_subtype == value_subtype ) ) \n"
Original    (006): ['item_subtype', '==', 'value_subtype', ')', ')', '\\n']
Tokenized   (015): ['<s>', 'item', '_', 'sub', 'type', '==', 'value', '_', 'sub', 'type', ')', ')', '\\', 'n', '</s>']
Filtered   (013): ['item', '_', 'sub', 'type', '==', 'value', '_', 'sub', 'type', ')', ')', '\\', 'n']
Detokenized (006): ['item_subtype', '==', 'value_subtype', ')', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "no_cache = cache_property ( , , None ) \n"
Original    (009): ['no_cache', '=', 'cache_property', '(', ',', ',', 'None', ')', '\\n']
Tokenized   (016): ['<s>', 'no', '_', 'cache', '=', 'cache', '_', 'property', '(', ',', ',', 'None', ')', '\\', 'n', '</s>']
Filtered   (014): ['no', '_', 'cache', '=', 'cache', '_', 'property', '(', ',', ',', 'None', ')', '\\', 'n']
Detokenized (009): ['no_cache', '=', 'cache_property', '(', ',', ',', 'None', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "no_store = cache_property ( , None , bool ) \n"
Original    (010): ['no_store', '=', 'cache_property', '(', ',', 'None', ',', 'bool', ')', '\\n']
Tokenized   (017): ['<s>', 'no', '_', 'store', '=', 'cache', '_', 'property', '(', ',', 'None', ',', 'bool', ')', '\\', 'n', '</s>']
Filtered   (015): ['no', '_', 'store', '=', 'cache', '_', 'property', '(', ',', 'None', ',', 'bool', ')', '\\', 'n']
Detokenized (010): ['no_store', '=', 'cache_property', '(', ',', 'None', ',', 'bool', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "max_age = cache_property ( , - 1 , int ) \n"
Original    (011): ['max_age', '=', 'cache_property', '(', ',', '-', '1', ',', 'int', ')', '\\n']
Tokenized   (018): ['<s>', 'max', '_', 'age', '=', 'cache', '_', 'property', '(', ',', '-', '1', ',', 'int', ')', '\\', 'n', '</s>']
Filtered   (016): ['max', '_', 'age', '=', 'cache', '_', 'property', '(', ',', '-', '1', ',', 'int', ')', '\\', 'n']
Detokenized (011): ['max_age', '=', 'cache_property', '(', ',', '-', '1', ',', 'int', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "no_transform = cache_property ( , None , None ) \n"
Original    (010): ['no_transform', '=', 'cache_property', '(', ',', 'None', ',', 'None', ')', '\\n']
Tokenized   (017): ['<s>', 'no', '_', 'transform', '=', 'cache', '_', 'property', '(', ',', 'None', ',', 'None', ')', '\\', 'n', '</s>']
Filtered   (015): ['no', '_', 'transform', '=', 'cache', '_', 'property', '(', ',', 'None', ',', 'None', ')', '\\', 'n']
Detokenized (010): ['no_transform', '=', 'cache_property', '(', ',', 'None', ',', 'None', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "max_stale = cache_property ( , , int ) \n"
Original    (009): ['max_stale', '=', 'cache_property', '(', ',', ',', 'int', ')', '\\n']
Tokenized   (017): ['<s>', 'max', '_', 'st', 'ale', '=', 'cache', '_', 'property', '(', ',', ',', 'int', ')', '\\', 'n', '</s>']
Filtered   (015): ['max', '_', 'st', 'ale', '=', 'cache', '_', 'property', '(', ',', ',', 'int', ')', '\\', 'n']
Detokenized (009): ['max_stale', '=', 'cache_property', '(', ',', ',', 'int', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "etag , weak = unquote_etag ( etag ) \n"
Original    (009): ['etag', ',', 'weak', '=', 'unquote_etag', '(', 'etag', ')', '\\n']
Tokenized   (018): ['<s>', 'et', 'ag', ',', 'weak', '=', 'un', 'quote', '_', 'et', 'ag', '(', 'et', 'ag', ')', '\\', 'n', '</s>']
Filtered   (016): ['et', 'ag', ',', 'weak', '=', 'un', 'quote', '_', 'et', 'ag', '(', 'et', 'ag', ')', '\\', 'n']
Detokenized (009): ['etag', ',', 'weak', '=', 'unquote_etag', '(', 'etag', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "uri = property ( lambda x : x . get ( ) , doc = ) \n"
Original    (017): ['uri', '=', 'property', '(', 'lambda', 'x', ':', 'x', '.', 'get', '(', ')', ',', 'doc', '=', ')', '\\n']
Tokenized   (020): ['<s>', 'uri', '=', 'property', '(', 'lambda', 'x', ':', 'x', '.', 'get', '(', ')', ',', 'doc', '=', ')', '\\', 'n', '</s>']
Filtered   (018): ['uri', '=', 'property', '(', 'lambda', 'x', ':', 'x', '.', 'get', '(', ')', ',', 'doc', '=', ')', '\\', 'n']
Detokenized (017): ['uri', '=', 'property', '(', 'lambda', 'x', ':', 'x', '.', 'get', '(', ')', ',', 'doc', '=', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "_require_quoting = frozenset ( [ , , , ] ) \n"
Original    (011): ['_require_quoting', '=', 'frozenset', '(', '[', ',', ',', ',', ']', ')', '\\n']
Tokenized   (020): ['<s>', '_', 'require', '_', 'qu', 'oting', '=', 'fro', 'zens', 'et', '(', '[', ',', ',', ',', ']', ')', '\\', 'n', '</s>']
Filtered   (018): ['_', 'require', '_', 'qu', 'oting', '=', 'fro', 'zens', 'et', '(', '[', ',', ',', ',', ']', ')', '\\', 'n']
Detokenized (011): ['_require_quoting', '=', 'frozenset', '(', '[', ',', ',', ',', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "auth_type = d . pop ( , None ) or \n"
Original    (011): ['auth_type', '=', 'd', '.', 'pop', '(', ',', 'None', ')', 'or', '\\n']
Tokenized   (016): ['<s>', 'auth', '_', 'type', '=', 'd', '.', 'pop', '(', ',', 'None', ')', 'or', '\\', 'n', '</s>']
Filtered   (014): ['auth', '_', 'type', '=', 'd', '.', 'pop', '(', ',', 'None', ')', 'or', '\\', 'n']
Detokenized (011): ['auth_type', '=', 'd', '.', 'pop', '(', ',', 'None', ')', 'or', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "allow_token = key not in self . _require_quoting ) ) \n"
Original    (011): ['allow_token', '=', 'key', 'not', 'in', 'self', '.', '_require_quoting', ')', ')', '\\n']
Tokenized   (020): ['<s>', 'allow', '_', 'token', '=', 'key', 'not', 'in', 'self', '.', '_', 'require', '_', 'qu', 'oting', ')', ')', '\\', 'n', '</s>']
Filtered   (018): ['allow', '_', 'token', '=', 'key', 'not', 'in', 'self', '.', '_', 'require', '_', 'qu', 'oting', ')', ')', '\\', 'n']
Detokenized (011): ['allow_token', '=', 'key', 'not', 'in', 'self', '.', '_require_quoting', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "realm = auth_property ( , doc = ) \n"
Original    (009): ['realm', '=', 'auth_property', '(', ',', 'doc', '=', ')', '\\n']
Tokenized   (015): ['<s>', 'real', 'm', '=', 'auth', '_', 'property', '(', ',', 'doc', '=', ')', '\\', 'n', '</s>']
Filtered   (013): ['real', 'm', '=', 'auth', '_', 'property', '(', ',', 'doc', '=', ')', '\\', 'n']
Detokenized (009): ['realm', '=', 'auth_property', '(', ',', 'doc', '=', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "rshell , shell , clear_datastore , create_user , \n"
Original    (009): ['rshell', ',', 'shell', ',', 'clear_datastore', ',', 'create_user', ',', '\\n']
Tokenized   (019): ['<s>', 'rs', 'hell', ',', 'shell', ',', 'clear', '_', 'dat', 'ast', 'ore', ',', 'create', '_', 'user', ',', '\\', 'n', '</s>']
Filtered   (017): ['rs', 'hell', ',', 'shell', ',', 'clear', '_', 'dat', 'ast', 'ore', ',', 'create', '_', 'user', ',', '\\', 'n']
Detokenized (009): ['rshell', ',', 'shell', ',', 'clear_datastore', ',', 'create_user', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "Rule ( , endpoint = , \n"
Original    (007): ['Rule', '(', ',', 'endpoint', '=', ',', '\\n']
Tokenized   (010): ['<s>', 'Rule', '(', ',', 'endpoint', '=', ',', '\\', 'n', '</s>']
Filtered   (008): ['Rule', '(', ',', 'endpoint', '=', ',', '\\', 'n']
Detokenized (007): ['Rule', '(', ',', 'endpoint', '=', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "data_field = db . StringProperty ( required = True , \n"
Original    (011): ['data_field', '=', 'db', '.', 'StringProperty', '(', 'required', '=', 'True', ',', '\\n']
Tokenized   (017): ['<s>', 'data', '_', 'field', '=', 'db', '.', 'String', 'Property', '(', 'required', '=', 'True', ',', '\\', 'n', '</s>']
Filtered   (015): ['data', '_', 'field', '=', 'db', '.', 'String', 'Property', '(', 'required', '=', 'True', ',', '\\', 'n']
Detokenized (011): ['data_field', '=', 'db', '.', 'StringProperty', '(', 'required', '=', 'True', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "dst_name = path . join ( dst_path , filename ) \n"
Original    (011): ['dst_name', '=', 'path', '.', 'join', '(', 'dst_path', ',', 'filename', ')', '\\n']
Tokenized   (019): ['<s>', 'd', 'st', '_', 'name', '=', 'path', '.', 'join', '(', 'dst', '_', 'path', ',', 'filename', ')', '\\', 'n', '</s>']
Filtered   (017): ['d', 'st', '_', 'name', '=', 'path', '.', 'join', '(', 'dst', '_', 'path', ',', 'filename', ')', '\\', 'n']
Detokenized (011): ['dst_name', '=', 'path', '.', 'join', '(', 'dst_path', ',', 'filename', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "modifiable_problem_fields = [ "description" ] \n"
Original    (006): ['modifiable_problem_fields', '=', '[', '"description"', ']', '\\n']
Tokenized   (016): ['<s>', 'mod', 'ifiable', '_', 'problem', '_', 'fields', '=', '[', '"', 'description', '"', ']', '\\', 'n', '</s>']
Filtered   (014): ['mod', 'ifiable', '_', 'problem', '_', 'fields', '=', '[', '"', 'description', '"', ']', '\\', 'n']
Detokenized (006): ['modifiable_problem_fields', '=', '[', '"description"', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "problem = api . problem . get_problem ( pid = pid ) \n"
Original    (013): ['problem', '=', 'api', '.', 'problem', '.', 'get_problem', '(', 'pid', '=', 'pid', ')', '\\n']
Tokenized   (018): ['<s>', 'problem', '=', 'api', '.', 'problem', '.', 'get', '_', 'problem', '(', 'pid', '=', 'pid', ')', '\\', 'n', '</s>']
Filtered   (016): ['problem', '=', 'api', '.', 'problem', '.', 'get', '_', 'problem', '(', 'pid', '=', 'pid', ')', '\\', 'n']
Detokenized (013): ['problem', '=', 'api', '.', 'problem', '.', 'get_problem', '(', 'pid', '=', 'pid', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "build = get_generator ( pid ) . generate ( random , pid , api . autogen_tools , n ) \n"
Original    (020): ['build', '=', 'get_generator', '(', 'pid', ')', '.', 'generate', '(', 'random', ',', 'pid', ',', 'api', '.', 'autogen_tools', ',', 'n', ')', '\\n']
Tokenized   (029): ['<s>', 'build', '=', 'get', '_', 'gener', 'ator', '(', 'pid', ')', '.', 'generate', '(', 'random', ',', 'pid', ',', 'api', '.', 'aut', 'ogen', '_', 'tools', ',', 'n', ')', '\\', 'n', '</s>']
Filtered   (027): ['build', '=', 'get', '_', 'gener', 'ator', '(', 'pid', ')', '.', 'generate', '(', 'random', ',', 'pid', ',', 'api', '.', 'aut', 'ogen', '_', 'tools', ',', 'n', ')', '\\', 'n']
Detokenized (020): ['build', '=', 'get_generator', '(', 'pid', ')', '.', 'generate', '(', 'random', ',', 'pid', ',', 'api', '.', 'autogen_tools', ',', 'n', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "autogen_instance_path = get_instance_path ( pid , n = n ) \n"
Original    (011): ['autogen_instance_path', '=', 'get_instance_path', '(', 'pid', ',', 'n', '=', 'n', ')', '\\n']
Tokenized   (023): ['<s>', 'aut', 'ogen', '_', 'instance', '_', 'path', '=', 'get', '_', 'instance', '_', 'path', '(', 'pid', ',', 'n', '=', 'n', ')', '\\', 'n', '</s>']
Filtered   (021): ['aut', 'ogen', '_', 'instance', '_', 'path', '=', 'get', '_', 'instance', '_', 'path', '(', 'pid', ',', 'n', '=', 'n', ')', '\\', 'n']
Detokenized (011): ['autogen_instance_path', '=', 'get_instance_path', '(', 'pid', ',', 'n', '=', 'n', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""resource_files" : { \n"
Original    (004): ['"resource_files"', ':', '{', '\\n']
Tokenized   (011): ['<s>', '"', 'resource', '_', 'files', '"', ':', '{', '\\', 'n', '</s>']
Filtered   (009): ['"', 'resource', '_', 'files', '"', ':', '{', '\\', 'n']
Detokenized (004): ['"resource_files"', ':', '{', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "instance_path = path . join ( path . dirname ( generator_path ) , "instances" , name , str ( n ) ) \n"
Original    (023): ['instance_path', '=', 'path', '.', 'join', '(', 'path', '.', 'dirname', '(', 'generator_path', ')', ',', '"instances"', ',', 'name', ',', 'str', '(', 'n', ')', ')', '\\n']
Tokenized   (034): ['<s>', 'instance', '_', 'path', '=', 'path', '.', 'join', '(', 'path', '.', 'dir', 'name', '(', 'generator', '_', 'path', ')', ',', '"', 'inst', 'ances', '"', ',', 'name', ',', 'str', '(', 'n', ')', ')', '\\', 'n', '</s>']
Filtered   (032): ['instance', '_', 'path', '=', 'path', '.', 'join', '(', 'path', '.', 'dir', 'name', '(', 'generator', '_', 'path', ')', ',', '"', 'inst', 'ances', '"', ',', 'name', ',', 'str', '(', 'n', ')', ')', '\\', 'n']
Detokenized (023): ['instance_path', '=', 'path', '.', 'join', '(', 'path', '.', 'dirname', '(', 'generator_path', ')', ',', '"instances"', ',', 'name', ',', 'str', '(', 'n', ')', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : ""correct" : correct , \n"
Original    (005): ['"correct"', ':', 'correct', ',', '\\n']
Tokenized   (010): ['<s>', '"', 'correct', '"', ':', 'correct', ',', '\\', 'n', '</s>']
Filtered   (008): ['"', 'correct', '"', ':', 'correct', ',', '\\', 'n']
Detokenized (005): ['"correct"', ':', 'correct', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""points" : problem [ "score" ] , \n"
Original    (008): ['"points"', ':', 'problem', '[', '"score"', ']', ',', '\\n']
Tokenized   (015): ['<s>', '"', 'points', '"', ':', 'problem', '[', '"', 'score', '"', ']', ',', '\\', 'n', '</s>']
Filtered   (013): ['"', 'points', '"', ':', 'problem', '[', '"', 'score', '"', ']', ',', '\\', 'n']
Detokenized (008): ['"points"', ':', 'problem', '[', '"score"', ']', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : ""message" : message \n"
Original    (004): ['"message"', ':', 'message', '\\n']
Tokenized   (009): ['<s>', '"', 'message', '"', ':', 'message', '\\', 'n', '</s>']
Filtered   (007): ['"', 'message', '"', ':', 'message', '\\', 'n']
Detokenized (004): ['"message"', ':', 'message', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "k = str ( random . randint ( 0 , 1000 ) ) \n"
Original    (014): ['k', '=', 'str', '(', 'random', '.', 'randint', '(', '0', ',', '1000', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'k', '=', 'str', '(', 'random', '.', 'rand', 'int', '(', '0', ',', '1000', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['k', '=', 'str', '(', 'random', '.', 'rand', 'int', '(', '0', ',', '1000', ')', ')', '\\', 'n']
Detokenized (014): ['k', '=', 'str', '(', 'random', '.', 'randint', '(', '0', ',', '1000', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : ""public" : [ ( "/tmp/key" , "public_static" ) ] , \n"
Original    (011): ['"public"', ':', '[', '(', '"/tmp/key"', ',', '"public_static"', ')', ']', ',', '\\n']
Tokenized   (024): ['<s>', '"', 'public', '"', ':', '[', '(', '"/', 'tmp', '/', 'key', '"', ',', '"', 'public', '_', 'static', '"', ')', ']', ',', '\\', 'n', '</s>']
Filtered   (022): ['"', 'public', '"', ':', '[', '(', '"/', 'tmp', '/', 'key', '"', ',', '"', 'public', '_', 'static', '"', ')', ']', ',', '\\', 'n']
Detokenized (011): ['"public"', ':', '[', '(', '"/tmp/key"', ',', '"public_static"', ')', ']', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""private" : [ ( "/tmp/key" , "private_static" ) ] \n"
Original    (010): ['"private"', ':', '[', '(', '"/tmp/key"', ',', '"private_static"', ')', ']', '\\n']
Tokenized   (023): ['<s>', '"', 'private', '"', ':', '[', '(', '"/', 'tmp', '/', 'key', '"', ',', '"', 'private', '_', 'static', '"', ')', ']', '\\', 'n', '</s>']
Filtered   (021): ['"', 'private', '"', ':', '[', '(', '"/', 'tmp', '/', 'key', '"', ',', '"', 'private', '_', 'static', '"', ')', ']', '\\', 'n']
Detokenized (010): ['"private"', ':', '[', '(', '"/tmp/key"', ',', '"private_static"', ')', ']', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "layout = eval ( scriptWindow . setLayout ( layout ) \n"
Original    (011): ['layout', '=', 'eval', '(', 'scriptWindow', '.', 'setLayout', '(', 'layout', ')', '\\n']
Tokenized   (016): ['<s>', 'layout', '=', 'eval', '(', 'script', 'Window', '.', 'set', 'Layout', '(', 'layout', ')', '\\', 'n', '</s>']
Filtered   (014): ['layout', '=', 'eval', '(', 'script', 'Window', '.', 'set', 'Layout', '(', 'layout', ')', '\\', 'n']
Detokenized (011): ['layout', '=', 'eval', '(', 'scriptWindow', '.', 'setLayout', '(', 'layout', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "scriptWindow . _Widget__qtWidget . resize ( 995 , 500 ) \n"
Original    (011): ['scriptWindow', '.', '_Widget__qtWidget', '.', 'resize', '(', '995', ',', '500', ')', '\\n']
Tokenized   (020): ['<s>', 'script', 'Window', '.', '_', 'Widget', '__', 'qt', 'Widget', '.', 'resize', '(', '9', '95', ',', '500', ')', '\\', 'n', '</s>']
Filtered   (018): ['script', 'Window', '.', '_', 'Widget', '__', 'qt', 'Widget', '.', 'resize', '(', '9', '95', ',', '500', ')', '\\', 'n']
Detokenized (011): ['scriptWindow', '.', '_Widget__qtWidget', '.', 'resize', '(', '995', ',', '500', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""inputSequence" , \n"
Original    (003): ['"inputSequence"', ',', '\\n']
Tokenized   (010): ['<s>', '"', 'input', 'Sequ', 'ence', '"', ',', '\\', 'n', '</s>']
Filtered   (008): ['"', 'input', 'Sequ', 'ence', '"', ',', '\\', 'n']
Detokenized (003): ['"inputSequence"', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "defaultValue = "" , \n"
Original    (005): ['defaultValue', '=', '""', ',', '\\n']
Tokenized   (009): ['<s>', 'default', 'Value', '=', '""', ',', '\\', 'n', '</s>']
Filtered   (007): ['default', 'Value', '=', '""', ',', '\\', 'n']
Detokenized (005): ['defaultValue', '=', '""', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "dc . write ( "-" + objectName , "bound" , b ) \n"
Original    (013): ['dc', '.', 'write', '(', '"-"', '+', 'objectName', ',', '"bound"', ',', 'b', ')', '\\n']
Tokenized   (020): ['<s>', 'dc', '.', 'write', '(', '"', '-"', '+', 'object', 'Name', ',', '"', 'bound', '"', ',', 'b', ')', '\\', 'n', '</s>']
Filtered   (018): ['dc', '.', 'write', '(', '"', '-"', '+', 'object', 'Name', ',', '"', 'bound', '"', ',', 'b', ')', '\\', 'n']
Detokenized (013): ['dc', '.', 'write', '(', '"-"', '+', 'objectName', ',', '"bound"', ',', 'b', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "additionalTerminalPlugTypes = ( GafferScene . ScenePlug , ) \n"
Original    (009): ['additionalTerminalPlugTypes', '=', '(', 'GafferScene', '.', 'ScenePlug', ',', ')', '\\n']
Tokenized   (020): ['<s>', 'add', 'itional', 'Termin', 'al', 'Plug', 'Types', '=', '(', 'G', 'affer', 'Scene', '.', 'Scene', 'Plug', ',', ')', '\\', 'n', '</s>']
Filtered   (018): ['add', 'itional', 'Termin', 'al', 'Plug', 'Types', '=', '(', 'G', 'affer', 'Scene', '.', 'Scene', 'Plug', ',', ')', '\\', 'n']
Detokenized (009): ['additionalTerminalPlugTypes', '=', '(', 'GafferScene', '.', 'ScenePlug', ',', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "replace = context . get ( "textWriter:replace" , IECore . StringVectorData ( ) ) \n"
Original    (015): ['replace', '=', 'context', '.', 'get', '(', '"textWriter:replace"', ',', 'IECore', '.', 'StringVectorData', '(', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'replace', '=', 'context', '.', 'get', '(', '"', 'text', 'Writer', ':', 'replace', '"', ',', 'I', 'EC', 'ore', '.', 'String', 'Vector', 'Data', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['replace', '=', 'context', '.', 'get', '(', '"', 'text', 'Writer', ':', 'replace', '"', ',', 'I', 'EC', 'ore', '.', 'String', 'Vector', 'Data', '(', ')', ')', '\\', 'n']
Detokenized (015): ['replace', '=', 'context', '.', 'get', '(', '"textWriter:replace"', ',', 'IECore', '.', 'StringVectorData', '(', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "inMetadata = r [ "out" ] [ "metadata" ] . getValue ( ) \n"
Original    (014): ['inMetadata', '=', 'r', '[', '"out"', ']', '[', '"metadata"', ']', '.', 'getValue', '(', ')', '\\n']
Tokenized   (024): ['<s>', 'in', 'Met', 'adata', '=', 'r', '[', '"', 'out', '"', ']', '[', '"', 'metadata', '"', ']', '.', 'get', 'Value', '(', ')', '\\', 'n', '</s>']
Filtered   (022): ['in', 'Met', 'adata', '=', 'r', '[', '"', 'out', '"', ']', '[', '"', 'metadata', '"', ']', '.', 'get', 'Value', '(', ')', '\\', 'n']
Detokenized (014): ['inMetadata', '=', 'r', '[', '"out"', ']', '[', '"metadata"', ']', '.', 'getValue', '(', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "negFileName = os . path . expandvars ( "$GAFFER_ROOT/python/GafferImageTest/images/checkerWithNegativeDataWindow.200x150.exr" \n"
Original    (010): ['negFileName', '=', 'os', '.', 'path', '.', 'expandvars', '(', '"$GAFFER_ROOT/python/GafferImageTest/images/checkerWithNegativeDataWindow.200x150.exr"', '\\n']
Tokenized   (047): ['<s>', 'neg', 'File', 'Name', '=', 'os', '.', 'path', '.', 'expand', 'v', 'ars', '(', '"$', 'GA', 'FFER', '_', 'RO', 'OT', '/', 'python', '/', 'G', 'affer', 'Image', 'Test', '/', 'images', '/', 'check', 'er', 'With', 'Neg', 'ative', 'Data', 'Window', '.', '200', 'x', '150', '.', 'ex', 'r', '"', '\\', 'n', '</s>']
Filtered   (045): ['neg', 'File', 'Name', '=', 'os', '.', 'path', '.', 'expand', 'v', 'ars', '(', '"$', 'GA', 'FFER', '_', 'RO', 'OT', '/', 'python', '/', 'G', 'affer', 'Image', 'Test', '/', 'images', '/', 'check', 'er', 'With', 'Neg', 'ative', 'Data', 'Window', '.', '200', 'x', '150', '.', 'ex', 'r', '"', '\\', 'n']
Detokenized (010): ['negFileName', '=', 'os', '.', 'path', '.', 'expandvars', '(', '"$GAFFER_ROOT/python/GafferImageTest/images/checkerWithNegativeDataWindow.200x150.exr"', '\\n']
Counter: 45
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "GafferImage . Display , \n"
Original    (005): ['GafferImage', '.', 'Display', ',', '\\n']
Tokenized   (010): ['<s>', 'G', 'affer', 'Image', '.', 'Display', ',', '\\', 'n', '</s>']
Filtered   (008): ['G', 'affer', 'Image', '.', 'Display', ',', '\\', 'n']
Detokenized (005): ['GafferImage', '.', 'Display', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""port" : [ \n"
Original    (004): ['"port"', ':', '[', '\\n']
Tokenized   (009): ['<s>', '"', 'port', '"', ':', '[', '\\', 'n', '</s>']
Filtered   (007): ['"', 'port', '"', ':', '[', '\\', 'n']
Detokenized (004): ['"port"', ':', '[', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "updateCountPlug . setValue ( updateCountPlug . getValue ( ) + 1 ) \n"
Original    (013): ['updateCountPlug', '.', 'setValue', '(', 'updateCountPlug', '.', 'getValue', '(', ')', '+', '1', ')', '\\n']
Tokenized   (022): ['<s>', 'update', 'Count', 'Plug', '.', 'set', 'Value', '(', 'update', 'Count', 'Plug', '.', 'get', 'Value', '(', ')', '+', '1', ')', '\\', 'n', '</s>']
Filtered   (020): ['update', 'Count', 'Plug', '.', 'set', 'Value', '(', 'update', 'Count', 'Plug', '.', 'get', 'Value', '(', ')', '+', '1', ')', '\\', 'n']
Detokenized (013): ['updateCountPlug', '.', 'setValue', '(', 'updateCountPlug', '.', 'getValue', '(', ')', '+', '1', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "__import__ ( "IECore" ) . loadConfig ( "GAFFER_STARTUP_PATHS" , { } , subdirectory = "GafferImageUI" ) \n"
Original    (017): ['__import__', '(', '"IECore"', ')', '.', 'loadConfig', '(', '"GAFFER_STARTUP_PATHS"', ',', '{', '}', ',', 'subdirectory', '=', '"GafferImageUI"', ')', '\\n']
Tokenized   (044): ['<s>', '__', 'import', '__', '(', '"', 'I', 'EC', 'ore', '"', ')', '.', 'load', 'Config', '(', '"', 'GA', 'FFER', '_', 'ST', 'ART', 'UP', '_', 'P', 'AT', 'HS', '"', ',', '{', '}', ',', 'sub', 'directory', '=', '"', 'G', 'affer', 'Image', 'UI', '"', ')', '\\', 'n', '</s>']
Filtered   (042): ['__', 'import', '__', '(', '"', 'I', 'EC', 'ore', '"', ')', '.', 'load', 'Config', '(', '"', 'GA', 'FFER', '_', 'ST', 'ART', 'UP', '_', 'P', 'AT', 'HS', '"', ',', '{', '}', ',', 'sub', 'directory', '=', '"', 'G', 'affer', 'Image', 'UI', '"', ')', '\\', 'n']
Detokenized (017): ['__import__', '(', '"IECore"', ')', '.', 'loadConfig', '(', '"GAFFER_STARTUP_PATHS"', ',', '{', '}', ',', 'subdirectory', '=', '"GafferImageUI"', ')', '\\n']
Counter: 42
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "GafferRenderMan . RenderManShader . shaderLoader ( ) . clear ( ) \n"
Original    (012): ['GafferRenderMan', '.', 'RenderManShader', '.', 'shaderLoader', '(', ')', '.', 'clear', '(', ')', '\\n']
Tokenized   (022): ['<s>', 'G', 'affer', 'Render', 'Man', '.', 'Render', 'Man', 'Sh', 'ader', '.', 'shader', 'Loader', '(', ')', '.', 'clear', '(', ')', '\\', 'n', '</s>']
Filtered   (020): ['G', 'affer', 'Render', 'Man', '.', 'Render', 'Man', 'Sh', 'ader', '.', 'shader', 'Loader', '(', ')', '.', 'clear', '(', ')', '\\', 'n']
Detokenized (012): ['GafferRenderMan', '.', 'RenderManShader', '.', 'shaderLoader', '(', ')', '.', 'clear', '(', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n"
Original    (018): ['coshader', '=', 'self', '.', 'compileShader', '(', 'os', '.', 'path', '.', 'dirname', '(', '__file__', ')', '+', '"/shaders/coshader.sl"', ')', '\\n']
Tokenized   (037): ['<s>', 'c', 'osh', 'ader', '=', 'self', '.', 'compile', 'Sh', 'ader', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', '__', 'file', '__', ')', '+', '"/', 'sh', 'aders', '/', 'c', 'osh', 'ader', '.', 'sl', '"', ')', '\\', 'n', '</s>']
Filtered   (035): ['c', 'osh', 'ader', '=', 'self', '.', 'compile', 'Sh', 'ader', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', '__', 'file', '__', ')', '+', '"/', 'sh', 'aders', '/', 'c', 'osh', 'ader', '.', 'sl', '"', ')', '\\', 'n']
Detokenized (018): ['coshader', '=', 'self', '.', 'compileShader', '(', 'os', '.', 'path', '.', 'dirname', '(', '__file__', ')', '+', '"/shaders/coshader.sl"', ')', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "nn [ "outString" ] = Gaffer . StringPlug ( direction = Gaffer . Plug . Direction . Out ) \n"
Original    (020): ['nn', '[', '"outString"', ']', '=', 'Gaffer', '.', 'StringPlug', '(', 'direction', '=', 'Gaffer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\n']
Tokenized   (029): ['<s>', 'nn', '[', '"', 'out', 'String', '"', ']', '=', 'G', 'affer', '.', 'String', 'Plug', '(', 'direction', '=', 'G', 'affer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\', 'n', '</s>']
Filtered   (027): ['nn', '[', '"', 'out', 'String', '"', ']', '=', 'G', 'affer', '.', 'String', 'Plug', '(', 'direction', '=', 'G', 'affer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\', 'n']
Detokenized (020): ['nn', '[', '"outString"', ']', '=', 'Gaffer', '.', 'StringPlug', '(', 'direction', '=', 'Gaffer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "shader2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version2.sl" , shaderName = "unversioned" \n"
Original    (021): ['shader2', '=', 'self', '.', 'compileShader', '(', 'os', '.', 'path', '.', 'dirname', '(', '__file__', ')', '+', '"/shaders/version2.sl"', ',', 'shaderName', '=', '"unversioned"', '\\n']
Tokenized   (044): ['<s>', 'sh', 'ader', '2', '=', 'self', '.', 'compile', 'Sh', 'ader', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', '__', 'file', '__', ')', '+', '"/', 'sh', 'aders', '/', 'version', '2', '.', 'sl', '"', ',', 'shader', 'Name', '=', '"', 'un', 'version', 'ed', '"', '\\', 'n', '</s>']
Filtered   (042): ['sh', 'ader', '2', '=', 'self', '.', 'compile', 'Sh', 'ader', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', '__', 'file', '__', ')', '+', '"/', 'sh', 'aders', '/', 'version', '2', '.', 'sl', '"', ',', 'shader', 'Name', '=', '"', 'un', 'version', 'ed', '"', '\\', 'n']
Detokenized (021): ['shader2', '=', 'self', '.', 'compileShader', '(', 'os', '.', 'path', '.', 'dirname', '(', '__file__', ')', '+', '"/shaders/version2.sl"', ',', 'shaderName', '=', '"unversioned"', '\\n']
Counter: 42
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "assignment [ "shader" ] . setInput ( shaderNode [ "out" ] ) \n"
Original    (013): ['assignment', '[', '"shader"', ']', '.', 'setInput', '(', 'shaderNode', '[', '"out"', ']', ')', '\\n']
Tokenized   (024): ['<s>', 'ass', 'ignment', '[', '"', 'sh', 'ader', '"', ']', '.', 'set', 'Input', '(', 'shader', 'Node', '[', '"', 'out', '"', ']', ')', '\\', 'n', '</s>']
Filtered   (022): ['ass', 'ignment', '[', '"', 'sh', 'ader', '"', ']', '.', 'set', 'Input', '(', 'shader', 'Node', '[', '"', 'out', '"', ']', ')', '\\', 'n']
Detokenized (013): ['assignment', '[', '"shader"', ']', '.', 'setInput', '(', 'shaderNode', '[', '"out"', ']', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "dirtiedNames = [ x [ 0 ] . fullName ( ) for x in cs ] \n"
Original    (017): ['dirtiedNames', '=', '[', 'x', '[', '0', ']', '.', 'fullName', '(', ')', 'for', 'x', 'in', 'cs', ']', '\\n']
Tokenized   (024): ['<s>', 'd', 'irt', 'ied', 'Names', '=', '[', 'x', '[', '0', ']', '.', 'full', 'Name', '(', ')', 'for', 'x', 'in', 'cs', ']', '\\', 'n', '</s>']
Filtered   (022): ['d', 'irt', 'ied', 'Names', '=', '[', 'x', '[', '0', ']', '.', 'full', 'Name', '(', ')', 'for', 'x', 'in', 'cs', ']', '\\', 'n']
Detokenized (017): ['dirtiedNames', '=', '[', 'x', '[', '0', ']', '.', 'fullName', '(', ')', 'for', 'x', 'in', 'cs', ']', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : ""dynamicFloatArray" : IECore . FloatVectorData ( [ ] ) , \n"
Original    (011): ['"dynamicFloatArray"', ':', 'IECore', '.', 'FloatVectorData', '(', '[', ']', ')', ',', '\\n']
Tokenized   (023): ['<s>', '"', 'd', 'ynamic', 'Float', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'Float', 'Vector', 'Data', '(', '[', ']', ')', ',', '\\', 'n', '</s>']
Filtered   (021): ['"', 'd', 'ynamic', 'Float', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'Float', 'Vector', 'Data', '(', '[', ']', ')', ',', '\\', 'n']
Detokenized (011): ['"dynamicFloatArray"', ':', 'IECore', '.', 'FloatVectorData', '(', '[', ']', ')', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""fixedFloatArray" : IECore . FloatVectorData ( [ 1 , 2 , 3 , 4 ] ) , \n"
Original    (018): ['"fixedFloatArray"', ':', 'IECore', '.', 'FloatVectorData', '(', '[', '1', ',', '2', ',', '3', ',', '4', ']', ')', ',', '\\n']
Tokenized   (029): ['<s>', '"', 'fixed', 'Float', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'Float', 'Vector', 'Data', '(', '[', '1', ',', '2', ',', '3', ',', '4', ']', ')', ',', '\\', 'n', '</s>']
Filtered   (027): ['"', 'fixed', 'Float', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'Float', 'Vector', 'Data', '(', '[', '1', ',', '2', ',', '3', ',', '4', ']', ')', ',', '\\', 'n']
Detokenized (018): ['"fixedFloatArray"', ':', 'IECore', '.', 'FloatVectorData', '(', '[', '1', ',', '2', ',', '3', ',', '4', ']', ')', ',', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : ""dynamicStringArray" : IECore . StringVectorData ( [ "dynamic" , "arrays" , "can" , "still" , "have" , "defaults" "fixedStringArray" : IECore . StringVectorData ( [ "hello" , "goodbye" ] ) , \n"
Original    (032): ['"dynamicStringArray"', ':', 'IECore', '.', 'StringVectorData', '(', '[', '"dynamic"', ',', '"arrays"', ',', '"can"', ',', '"still"', ',', '"have"', ',', '"defaults"', '"fixedStringArray"', ':', 'IECore', '.', 'StringVectorData', '(', '[', '"hello"', ',', '"goodbye"', ']', ')', ',', '\\n']
Tokenized   (072): ['<s>', '"', 'd', 'ynamic', 'String', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'String', 'Vector', 'Data', '(', '[', '"', 'd', 'ynamic', '"', ',', '"', 'arr', 'ays', '"', ',', '"', 'can', '"', ',', '"', 'still', '"', ',', '"', 'have', '"', ',', '"', 'default', 's', '"', '"', 'fixed', 'String', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'String', 'Vector', 'Data', '(', '[', '"', 'hello', '"', ',', '"', 'good', 'bye', '"', ']', ')', ',', '\\', 'n', '</s>']
Filtered   (070): ['"', 'd', 'ynamic', 'String', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'String', 'Vector', 'Data', '(', '[', '"', 'd', 'ynamic', '"', ',', '"', 'arr', 'ays', '"', ',', '"', 'can', '"', ',', '"', 'still', '"', ',', '"', 'have', '"', ',', '"', 'default', 's', '"', '"', 'fixed', 'String', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'String', 'Vector', 'Data', '(', '[', '"', 'hello', '"', ',', '"', 'good', 'bye', '"', ']', ')', ',', '\\', 'n']
Detokenized (032): ['"dynamicStringArray"', ':', 'IECore', '.', 'StringVectorData', '(', '[', '"dynamic"', ',', '"arrays"', ',', '"can"', ',', '"still"', ',', '"have"', ',', '"defaults"', '"fixedStringArray"', ':', 'IECore', '.', 'StringVectorData', '(', '[', '"hello"', ',', '"goodbye"', ']', ')', ',', '\\n']
Counter: 70
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : ""dynamicColorArray" : IECore . Color3fVectorData ( [ IECore . Color3f ( 1 ) , IECore . Color3f ( 2 ) ] ) , \n"
Original    (024): ['"dynamicColorArray"', ':', 'IECore', '.', 'Color3fVectorData', '(', '[', 'IECore', '.', 'Color3f', '(', '1', ')', ',', 'IECore', '.', 'Color3f', '(', '2', ')', ']', ')', ',', '\\n']
Tokenized   (046): ['<s>', '"', 'd', 'ynamic', 'Color', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'Color', '3', 'f', 'Vector', 'Data', '(', '[', 'I', 'EC', 'ore', '.', 'Color', '3', 'f', '(', '1', ')', ',', 'I', 'EC', 'ore', '.', 'Color', '3', 'f', '(', '2', ')', ']', ')', ',', '\\', 'n', '</s>']
Filtered   (044): ['"', 'd', 'ynamic', 'Color', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'Color', '3', 'f', 'Vector', 'Data', '(', '[', 'I', 'EC', 'ore', '.', 'Color', '3', 'f', '(', '1', ')', ',', 'I', 'EC', 'ore', '.', 'Color', '3', 'f', '(', '2', ')', ']', ')', ',', '\\', 'n']
Detokenized (024): ['"dynamicColorArray"', ':', 'IECore', '.', 'Color3fVectorData', '(', '[', 'IECore', '.', 'Color3f', '(', '1', ')', ',', 'IECore', '.', 'Color3f', '(', '2', ')', ']', ')', ',', '\\n']
Counter: 44
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : ""dynamicVectorArray" : IECore . V3fVectorData ( [ ] , IECore . GeometricData . Interpretation . Vector ) , \n"
Original    (019): ['"dynamicVectorArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', ']', ',', 'IECore', '.', 'GeometricData', '.', 'Interpretation', '.', 'Vector', ')', ',', '\\n']
Tokenized   (038): ['<s>', '"', 'd', 'ynamic', 'Vector', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'V', '3', 'f', 'Vector', 'Data', '(', '[', ']', ',', 'I', 'EC', 'ore', '.', 'Ge', 'ometric', 'Data', '.', 'Interpret', 'ation', '.', 'Vector', ')', ',', '\\', 'n', '</s>']
Filtered   (036): ['"', 'd', 'ynamic', 'Vector', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'V', '3', 'f', 'Vector', 'Data', '(', '[', ']', ',', 'I', 'EC', 'ore', '.', 'Ge', 'ometric', 'Data', '.', 'Interpret', 'ation', '.', 'Vector', ')', ',', '\\', 'n']
Detokenized (019): ['"dynamicVectorArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', ']', ',', 'IECore', '.', 'GeometricData', '.', 'Interpretation', '.', 'Vector', ')', ',', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : ""fixedVectorArray" : IECore . V3fVectorData ( [ IECore . V3f ( x ) for x in range ( 1 , 6 ) ] , IECore . GeometricData "dynamicPointArray" : IECore . V3fVectorData ( [ ] , IECore . GeometricData . Interpretation . Point ) , \n"
Original    (046): ['"fixedVectorArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', 'IECore', '.', 'V3f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'IECore', '.', 'GeometricData', '"dynamicPointArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', ']', ',', 'IECore', '.', 'GeometricData', '.', 'Interpretation', '.', 'Point', ')', ',', '\\n']
Tokenized   (083): ['<s>', '"', 'fixed', 'Vector', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'V', '3', 'f', 'Vector', 'Data', '(', '[', 'I', 'EC', 'ore', '.', 'V', '3', 'f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'I', 'EC', 'ore', '.', 'Ge', 'ometric', 'Data', '"', 'd', 'ynamic', 'Point', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'V', '3', 'f', 'Vector', 'Data', '(', '[', ']', ',', 'I', 'EC', 'ore', '.', 'Ge', 'ometric', 'Data', '.', 'Interpret', 'ation', '.', 'Point', ')', ',', '\\', 'n', '</s>']
Filtered   (081): ['"', 'fixed', 'Vector', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'V', '3', 'f', 'Vector', 'Data', '(', '[', 'I', 'EC', 'ore', '.', 'V', '3', 'f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'I', 'EC', 'ore', '.', 'Ge', 'ometric', 'Data', '"', 'd', 'ynamic', 'Point', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'V', '3', 'f', 'Vector', 'Data', '(', '[', ']', ',', 'I', 'EC', 'ore', '.', 'Ge', 'ometric', 'Data', '.', 'Interpret', 'ation', '.', 'Point', ')', ',', '\\', 'n']
Detokenized (046): ['"fixedVectorArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', 'IECore', '.', 'V3f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'IECore', '.', 'GeometricData', '"dynamicPointArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', ']', ',', 'IECore', '.', 'GeometricData', '.', 'Interpretation', '.', 'Point', ')', ',', '\\n']
Counter: 81
===================================================================
Hidden states:  (13, 46, 768)
# Extracted words:  46
Sentence         : ""fixedNormalArray" : IECore . V3fVectorData ( [ IECore . V3f ( x ) for x in range ( 1 , 6 ) ] , IECore . GeometricData } \n"
Original    (029): ['"fixedNormalArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', 'IECore', '.', 'V3f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'IECore', '.', 'GeometricData', '}', '\\n']
Tokenized   (050): ['<s>', '"', 'fixed', 'Normal', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'V', '3', 'f', 'Vector', 'Data', '(', '[', 'I', 'EC', 'ore', '.', 'V', '3', 'f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'I', 'EC', 'ore', '.', 'Ge', 'ometric', 'Data', '}', '\\', 'n', '</s>']
Filtered   (048): ['"', 'fixed', 'Normal', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'V', '3', 'f', 'Vector', 'Data', '(', '[', 'I', 'EC', 'ore', '.', 'V', '3', 'f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'I', 'EC', 'ore', '.', 'Ge', 'ometric', 'Data', '}', '\\', 'n']
Detokenized (029): ['"fixedNormalArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', 'IECore', '.', 'V3f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'IECore', '.', 'GeometricData', '}', '\\n']
Counter: 48
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "arrayShader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" n4 = GafferRenderMan . RenderManShader ( ) \n"
Original    (024): ['arrayShader', '=', 'self', '.', 'compileShader', '(', 'os', '.', 'path', '.', 'dirname', '(', '__file__', ')', '+', '"/shaders/coshaderArrayParameters.sl"', 'n4', '=', 'GafferRenderMan', '.', 'RenderManShader', '(', ')', '\\n']
Tokenized   (052): ['<s>', 'array', 'Sh', 'ader', '=', 'self', '.', 'compile', 'Sh', 'ader', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', '__', 'file', '__', ')', '+', '"/', 'sh', 'aders', '/', 'c', 'osh', 'ader', 'Array', 'Parameters', '.', 'sl', '"', 'n', '4', '=', 'G', 'affer', 'Render', 'Man', '.', 'Render', 'Man', 'Sh', 'ader', '(', ')', '\\', 'n', '</s>']
Filtered   (050): ['array', 'Sh', 'ader', '=', 'self', '.', 'compile', 'Sh', 'ader', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', '__', 'file', '__', ')', '+', '"/', 'sh', 'aders', '/', 'c', 'osh', 'ader', 'Array', 'Parameters', '.', 'sl', '"', 'n', '4', '=', 'G', 'affer', 'Render', 'Man', '.', 'Render', 'Man', 'Sh', 'ader', '(', ')', '\\', 'n']
Detokenized (024): ['arrayShader', '=', 'self', '.', 'compileShader', '(', 'os', '.', 'path', '.', 'dirname', '(', '__file__', ')', '+', '"/shaders/coshaderArrayParameters.sl"', 'n4', '=', 'GafferRenderMan', '.', 'RenderManShader', '(', ')', '\\n']
Counter: 50
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "coshaderNode [ "enabled" ] . setValue ( False ) \n"
Original    (010): ['coshaderNode', '[', '"enabled"', ']', '.', 'setValue', '(', 'False', ')', '\\n']
Tokenized   (019): ['<s>', 'c', 'osh', 'ader', 'Node', '[', '"', 'enabled', '"', ']', '.', 'set', 'Value', '(', 'False', ')', '\\', 'n', '</s>']
Filtered   (017): ['c', 'osh', 'ader', 'Node', '[', '"', 'enabled', '"', ']', '.', 'set', 'Value', '(', 'False', ')', '\\', 'n']
Detokenized (010): ['coshaderNode', '[', '"enabled"', ']', '.', 'setValue', '(', 'False', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "floatValue = IECore . Splineff ( \n"
Original    (007): ['floatValue', '=', 'IECore', '.', 'Splineff', '(', '\\n']
Tokenized   (015): ['<s>', 'float', 'Value', '=', 'I', 'EC', 'ore', '.', 'Spl', 'ine', 'ff', '(', '\\', 'n', '</s>']
Filtered   (013): ['float', 'Value', '=', 'I', 'EC', 'ore', '.', 'Spl', 'ine', 'ff', '(', '\\', 'n']
Detokenized (007): ['floatValue', '=', 'IECore', '.', 'Splineff', '(', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "S [ "parameters" ] [ "coshaderParameter" ] . setInput ( D2 [ "out" ] ) \n"
Original    (016): ['S', '[', '"parameters"', ']', '[', '"coshaderParameter"', ']', '.', 'setInput', '(', 'D2', '[', '"out"', ']', ')', '\\n']
Tokenized   (031): ['<s>', 'S', '[', '"', 'param', 'eters', '"', ']', '[', '"', 'c', 'osh', 'ader', 'Parameter', '"', ']', '.', 'set', 'Input', '(', 'D', '2', '[', '"', 'out', '"', ']', ')', '\\', 'n', '</s>']
Filtered   (029): ['S', '[', '"', 'param', 'eters', '"', ']', '[', '"', 'c', 'osh', 'ader', 'Parameter', '"', ']', '.', 'set', 'Input', '(', 'D', '2', '[', '"', 'out', '"', ']', ')', '\\', 'n']
Detokenized (016): ['S', '[', '"parameters"', ']', '[', '"coshaderParameter"', ']', '.', 'setInput', '(', 'D2', '[', '"out"', ']', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "coshaderNode0 [ "parameters" ] [ "floatParameter" ] . setValue ( 0 ) \n"
Original    (013): ['coshaderNode0', '[', '"parameters"', ']', '[', '"floatParameter"', ']', '.', 'setValue', '(', '0', ')', '\\n']
Tokenized   (027): ['<s>', 'c', 'osh', 'ader', 'Node', '0', '[', '"', 'param', 'eters', '"', ']', '[', '"', 'float', 'Parameter', '"', ']', '.', 'set', 'Value', '(', '0', ')', '\\', 'n', '</s>']
Filtered   (025): ['c', 'osh', 'ader', 'Node', '0', '[', '"', 'param', 'eters', '"', ']', '[', '"', 'float', 'Parameter', '"', ']', '.', 'set', 'Value', '(', '0', ')', '\\', 'n']
Detokenized (013): ['coshaderNode0', '[', '"parameters"', ']', '[', '"floatParameter"', ']', '.', 'setValue', '(', '0', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "sn1 = GafferRenderMan . RenderManShader ( "Shader1" ) \n"
Original    (009): ['sn1', '=', 'GafferRenderMan', '.', 'RenderManShader', '(', '"Shader1"', ')', '\\n']
Tokenized   (023): ['<s>', 'sn', '1', '=', 'G', 'affer', 'Render', 'Man', '.', 'Render', 'Man', 'Sh', 'ader', '(', '"', 'Sh', 'ader', '1', '"', ')', '\\', 'n', '</s>']
Filtered   (021): ['sn', '1', '=', 'G', 'affer', 'Render', 'Man', '.', 'Render', 'Man', 'Sh', 'ader', '(', '"', 'Sh', 'ader', '1', '"', ')', '\\', 'n']
Detokenized (009): ['sn1', '=', 'GafferRenderMan', '.', 'RenderManShader', '(', '"Shader1"', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "script [ "assignment" ] [ "shader" ] . setInput ( script [ "shader" ] [ "out" ] ) \n"
Original    (019): ['script', '[', '"assignment"', ']', '[', '"shader"', ']', '.', 'setInput', '(', 'script', '[', '"shader"', ']', '[', '"out"', ']', ')', '\\n']
Tokenized   (034): ['<s>', 'script', '[', '"', 'ass', 'ignment', '"', ']', '[', '"', 'sh', 'ader', '"', ']', '.', 'set', 'Input', '(', 'script', '[', '"', 'sh', 'ader', '"', ']', '[', '"', 'out', '"', ']', ')', '\\', 'n', '</s>']
Filtered   (032): ['script', '[', '"', 'ass', 'ignment', '"', ']', '[', '"', 'sh', 'ader', '"', ']', '.', 'set', 'Input', '(', 'script', '[', '"', 'sh', 'ader', '"', ']', '[', '"', 'out', '"', ']', ')', '\\', 'n']
Detokenized (019): ['script', '[', '"assignment"', ']', '[', '"shader"', ']', '.', 'setInput', '(', 'script', '[', '"shader"', ']', '[', '"out"', ']', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "traverseConnection = Gaffer . ScopedConnection ( GafferSceneTest . connectTraverseSceneToPlugDirtiedSignal script [ "shader" ] . loadShader ( "matte" ) \n"
Original    (019): ['traverseConnection', '=', 'Gaffer', '.', 'ScopedConnection', '(', 'GafferSceneTest', '.', 'connectTraverseSceneToPlugDirtiedSignal', 'script', '[', '"shader"', ']', '.', 'loadShader', '(', '"matte"', ')', '\\n']
Tokenized   (048): ['<s>', 'tra', 'verse', 'Connection', '=', 'G', 'affer', '.', 'Sc', 'oped', 'Connection', '(', 'G', 'affer', 'Scene', 'Test', '.', 'connect', 'Tra', 'verse', 'Scene', 'To', 'Plug', 'D', 'irt', 'ied', 'Sign', 'al', 'script', '[', '"', 'sh', 'ader', '"', ']', '.', 'load', 'Sh', 'ader', '(', '"', 'mat', 'te', '"', ')', '\\', 'n', '</s>']
Filtered   (046): ['tra', 'verse', 'Connection', '=', 'G', 'affer', '.', 'Sc', 'oped', 'Connection', '(', 'G', 'affer', 'Scene', 'Test', '.', 'connect', 'Tra', 'verse', 'Scene', 'To', 'Plug', 'D', 'irt', 'ied', 'Sign', 'al', 'script', '[', '"', 'sh', 'ader', '"', ']', '.', 'load', 'Sh', 'ader', '(', '"', 'mat', 'te', '"', ')', '\\', 'n']
Detokenized (019): ['traverseConnection', '=', 'Gaffer', '.', 'ScopedConnection', '(', 'GafferSceneTest', '.', 'connectTraverseSceneToPlugDirtiedSignal', 'script', '[', '"shader"', ']', '.', 'loadShader', '(', '"matte"', ')', '\\n']
Counter: 46
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "current = s [ "render" ] . hash ( c ) \n"
Original    (012): ['current', '=', 's', '[', '"render"', ']', '.', 'hash', '(', 'c', ')', '\\n']
Tokenized   (017): ['<s>', 'current', '=', 's', '[', '"', 'render', '"', ']', '.', 'hash', '(', 'c', ')', '\\', 'n', '</s>']
Filtered   (015): ['current', '=', 's', '[', '"', 'render', '"', ']', '.', 'hash', '(', 'c', ')', '\\', 'n']
Detokenized (012): ['current', '=', 's', '[', '"render"', ']', '.', 'hash', '(', 'c', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : ""layout:section" , "Transform" , \n"
Original    (005): ['"layout:section"', ',', '"Transform"', ',', '\\n']
Tokenized   (014): ['<s>', '"', 'layout', ':', 'section', '"', ',', '"', 'Transform', '"', ',', '\\', 'n', '</s>']
Filtered   (012): ['"', 'layout', ':', 'section', '"', ',', '"', 'Transform', '"', ',', '\\', 'n']
Detokenized (005): ['"layout:section"', ',', '"Transform"', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""toolbarLayout:index" , 2 , \n"
Original    (005): ['"toolbarLayout:index"', ',', '2', ',', '\\n']
Tokenized   (014): ['<s>', '"', 'tool', 'bar', 'Layout', ':', 'index', '"', ',', '2', ',', '\\', 'n', '</s>']
Filtered   (012): ['"', 'tool', 'bar', 'Layout', ':', 'index', '"', ',', '2', ',', '\\', 'n']
Detokenized (005): ['"toolbarLayout:index"', ',', '2', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""toolbarLayout:divider" , True , \n"
Original    (005): ['"toolbarLayout:divider"', ',', 'True', ',', '\\n']
Tokenized   (015): ['<s>', '"', 'tool', 'bar', 'Layout', ':', 'div', 'ider', '"', ',', 'True', ',', '\\', 'n', '</s>']
Filtered   (013): ['"', 'tool', 'bar', 'Layout', ':', 'div', 'ider', '"', ',', 'True', ',', '\\', 'n']
Detokenized (005): ['"toolbarLayout:divider"', ',', 'True', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "currentName = self . getPlug ( ) . getValue ( ) \n"
Original    (012): ['currentName', '=', 'self', '.', 'getPlug', '(', ')', '.', 'getValue', '(', ')', '\\n']
Tokenized   (018): ['<s>', 'current', 'Name', '=', 'self', '.', 'get', 'Plug', '(', ')', '.', 'get', 'Value', '(', ')', '\\', 'n', '</s>']
Filtered   (016): ['current', 'Name', '=', 'self', '.', 'get', 'Plug', '(', ')', '.', 'get', 'Value', '(', ')', '\\', 'n']
Detokenized (012): ['currentName', '=', 'self', '.', 'getPlug', '(', ')', '.', 'getValue', '(', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "menuButton = GafferUI . MenuButton ( menu = menu , image = "grid.png" , hasFrame = False ) \n"
Original    (019): ['menuButton', '=', 'GafferUI', '.', 'MenuButton', '(', 'menu', '=', 'menu', ',', 'image', '=', '"grid.png"', ',', 'hasFrame', '=', 'False', ')', '\\n']
Tokenized   (031): ['<s>', 'menu', 'Button', '=', 'G', 'affer', 'UI', '.', 'Menu', 'Button', '(', 'menu', '=', 'menu', ',', 'image', '=', '"', 'grid', '.', 'png', '"', ',', 'has', 'Frame', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (029): ['menu', 'Button', '=', 'G', 'affer', 'UI', '.', 'Menu', 'Button', '(', 'menu', '=', 'menu', ',', 'image', '=', '"', 'grid', '.', 'png', '"', ',', 'has', 'Frame', '=', 'False', ')', '\\', 'n']
Detokenized (019): ['menuButton', '=', 'GafferUI', '.', 'MenuButton', '(', 'menu', '=', 'menu', ',', 'image', '=', '"grid.png"', ',', 'hasFrame', '=', 'False', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "p3 = Gaffer . IntPlug ( "sum" , Gaffer . Plug . Direction . Out ) \n"
Original    (017): ['p3', '=', 'Gaffer', '.', 'IntPlug', '(', '"sum"', ',', 'Gaffer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\n']
Tokenized   (026): ['<s>', 'p', '3', '=', 'G', 'affer', '.', 'Int', 'Plug', '(', '"', 'sum', '"', ',', 'G', 'affer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\', 'n', '</s>']
Filtered   (024): ['p', '3', '=', 'G', 'affer', '.', 'Int', 'Plug', '(', '"', 'sum', '"', ',', 'G', 'affer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\', 'n']
Detokenized (017): ['p3', '=', 'Gaffer', '.', 'IntPlug', '(', '"sum"', ',', 'Gaffer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "childrenStrings = [ str ( c ) for c in children ] \n"
Original    (013): ['childrenStrings', '=', '[', 'str', '(', 'c', ')', 'for', 'c', 'in', 'children', ']', '\\n']
Tokenized   (018): ['<s>', 'children', 'Str', 'ings', '=', '[', 'str', '(', 'c', ')', 'for', 'c', 'in', 'children', ']', '\\', 'n', '</s>']
Filtered   (016): ['children', 'Str', 'ings', '=', '[', 'str', '(', 'c', ')', 'for', 'c', 'in', 'children', ']', '\\', 'n']
Detokenized (013): ['childrenStrings', '=', '[', 'str', '(', 'c', ')', 'for', 'c', 'in', 'children', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "c2 = [ str ( p ) for p in path2 . children ( ) ] \n"
Original    (017): ['c2', '=', '[', 'str', '(', 'p', ')', 'for', 'p', 'in', 'path2', '.', 'children', '(', ')', ']', '\\n']
Tokenized   (022): ['<s>', 'c', '2', '=', '[', 'str', '(', 'p', ')', 'for', 'p', 'in', 'path', '2', '.', 'children', '(', ')', ']', '\\', 'n', '</s>']
Filtered   (020): ['c', '2', '=', '[', 'str', '(', 'p', ')', 'for', 'p', 'in', 'path', '2', '.', 'children', '(', ')', ']', '\\', 'n']
Detokenized (017): ['c2', '=', '[', 'str', '(', 'p', ')', 'for', 'p', 'in', 'path2', '.', 'children', '(', ')', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "horizontalAlignment = GafferUI . Label . HorizontalAlignment . Right , \n"
Original    (011): ['horizontalAlignment', '=', 'GafferUI', '.', 'Label', '.', 'HorizontalAlignment', '.', 'Right', ',', '\\n']
Tokenized   (022): ['<s>', 'hor', 'izontal', 'Al', 'ignment', '=', 'G', 'affer', 'UI', '.', 'Label', '.', 'Hor', 'izontal', 'Al', 'ignment', '.', 'Right', ',', '\\', 'n', '</s>']
Filtered   (020): ['hor', 'izontal', 'Al', 'ignment', '=', 'G', 'affer', 'UI', '.', 'Label', '.', 'Hor', 'izontal', 'Al', 'ignment', '.', 'Right', ',', '\\', 'n']
Detokenized (011): ['horizontalAlignment', '=', 'GafferUI', '.', 'Label', '.', 'HorizontalAlignment', '.', 'Right', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "nameWidget . textWidget ( ) . _qtWidget ( ) . setFixedWidth ( GafferUI . PlugWidget . labelWidth ( ) ) \n"
Original    (021): ['nameWidget', '.', 'textWidget', '(', ')', '.', '_qtWidget', '(', ')', '.', 'setFixedWidth', '(', 'GafferUI', '.', 'PlugWidget', '.', 'labelWidth', '(', ')', ')', '\\n']
Tokenized   (034): ['<s>', 'name', 'Widget', '.', 'text', 'Widget', '(', ')', '.', '_', 'qt', 'Widget', '(', ')', '.', 'set', 'Fixed', 'Width', '(', 'G', 'affer', 'UI', '.', 'Plug', 'Widget', '.', 'label', 'Width', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (032): ['name', 'Widget', '.', 'text', 'Widget', '(', ')', '.', '_', 'qt', 'Widget', '(', ')', '.', 'set', 'Fixed', 'Width', '(', 'G', 'affer', 'UI', '.', 'Plug', 'Widget', '.', 'label', 'Width', '(', ')', ')', '\\', 'n']
Detokenized (021): ['nameWidget', '.', 'textWidget', '(', ')', '.', '_qtWidget', '(', ')', '.', 'setFixedWidth', '(', 'GafferUI', '.', 'PlugWidget', '.', 'labelWidth', '(', ')', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "childPlug [ "enabled" ] , \n"
Original    (006): ['childPlug', '[', '"enabled"', ']', ',', '\\n']
Tokenized   (012): ['<s>', 'child', 'Plug', '[', '"', 'enabled', '"', ']', ',', '\\', 'n', '</s>']
Filtered   (010): ['child', 'Plug', '[', '"', 'enabled', '"', ']', ',', '\\', 'n']
Detokenized (006): ['childPlug', '[', '"enabled"', ']', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "memberPlug = memberPlug if memberPlug is not None else plug . ancestor ( Gaffer . CompoundDataPlug . MemberPlug if memberPlug is None : \n"
Original    (024): ['memberPlug', '=', 'memberPlug', 'if', 'memberPlug', 'is', 'not', 'None', 'else', 'plug', '.', 'ancestor', '(', 'Gaffer', '.', 'CompoundDataPlug', '.', 'MemberPlug', 'if', 'memberPlug', 'is', 'None', ':', '\\n']
Tokenized   (036): ['<s>', 'member', 'Plug', '=', 'member', 'Plug', 'if', 'member', 'Plug', 'is', 'not', 'None', 'else', 'plug', '.', 'ancestor', '(', 'G', 'affer', '.', 'Comp', 'ound', 'Data', 'Plug', '.', 'Member', 'Plug', 'if', 'member', 'Plug', 'is', 'None', ':', '\\', 'n', '</s>']
Filtered   (034): ['member', 'Plug', '=', 'member', 'Plug', 'if', 'member', 'Plug', 'is', 'not', 'None', 'else', 'plug', '.', 'ancestor', '(', 'G', 'affer', '.', 'Comp', 'ound', 'Data', 'Plug', '.', 'Member', 'Plug', 'if', 'member', 'Plug', 'is', 'None', ':', '\\', 'n']
Detokenized (024): ['memberPlug', '=', 'memberPlug', 'if', 'memberPlug', 'is', 'not', 'None', 'else', 'plug', '.', 'ancestor', '(', 'Gaffer', '.', 'CompoundDataPlug', '.', 'MemberPlug', 'if', 'memberPlug', 'is', 'None', ':', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "menuDefinition . append ( "/Delete" , { "command" : IECore . curry ( __deletePlug , memberPlug ) , "active" \n"
Original    (020): ['menuDefinition', '.', 'append', '(', '"/Delete"', ',', '{', '"command"', ':', 'IECore', '.', 'curry', '(', '__deletePlug', ',', 'memberPlug', ')', ',', '"active"', '\\n']
Tokenized   (035): ['<s>', 'menu', 'Definition', '.', 'append', '(', '"/', 'Delete', '"', ',', '{', '"', 'command', '"', ':', 'I', 'EC', 'ore', '.', 'curry', '(', '__', 'delete', 'Plug', ',', 'member', 'Plug', ')', ',', '"', 'active', '"', '\\', 'n', '</s>']
Filtered   (033): ['menu', 'Definition', '.', 'append', '(', '"/', 'Delete', '"', ',', '{', '"', 'command', '"', ':', 'I', 'EC', 'ore', '.', 'curry', '(', '__', 'delete', 'Plug', ',', 'member', 'Plug', ')', ',', '"', 'active', '"', '\\', 'n']
Detokenized (020): ['menuDefinition', '.', 'append', '(', '"/Delete"', ',', '{', '"command"', ':', 'IECore', '.', 'curry', '(', '__deletePlug', ',', 'memberPlug', ')', ',', '"active"', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "includeSequences = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "fileSystemPathPlugValueWidget:includeSequences" \n"
Original    (016): ['includeSequences', '=', 'Gaffer', '.', 'Metadata', '.', 'plugValue', '(', 'self', '.', 'getPlug', '(', ')', ',', '"fileSystemPathPlugValueWidget:includeSequences"', '\\n']
Tokenized   (036): ['<s>', 'include', 'Sequ', 'ences', '=', 'G', 'affer', '.', 'Met', 'adata', '.', 'plug', 'Value', '(', 'self', '.', 'get', 'Plug', '(', ')', ',', '"', 'file', 'System', 'Path', 'Plug', 'Value', 'Widget', ':', 'include', 'Sequ', 'ences', '"', '\\', 'n', '</s>']
Filtered   (034): ['include', 'Sequ', 'ences', '=', 'G', 'affer', '.', 'Met', 'adata', '.', 'plug', 'Value', '(', 'self', '.', 'get', 'Plug', '(', ')', ',', '"', 'file', 'System', 'Path', 'Plug', 'Value', 'Widget', ':', 'include', 'Sequ', 'ences', '"', '\\', 'n']
Detokenized (016): ['includeSequences', '=', 'Gaffer', '.', 'Metadata', '.', 'plugValue', '(', 'self', '.', 'getPlug', '(', ')', ',', '"fileSystemPathPlugValueWidget:includeSequences"', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "reuse = reuseUntil is not None \n"
Original    (007): ['reuse', '=', 'reuseUntil', 'is', 'not', 'None', '\\n']
Tokenized   (012): ['<s>', 're', 'use', '=', 'reuse', 'Until', 'is', 'not', 'None', '\\', 'n', '</s>']
Filtered   (010): ['re', 'use', '=', 'reuse', 'Until', 'is', 'not', 'None', '\\', 'n']
Detokenized (007): ['reuse', '=', 'reuseUntil', 'is', 'not', 'None', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "_MultiLineStringMetadataWidget ( key = "description" ) \n"
Original    (007): ['_MultiLineStringMetadataWidget', '(', 'key', '=', '"description"', ')', '\\n']
Tokenized   (018): ['<s>', '_', 'Multi', 'Line', 'String', 'Met', 'adata', 'Widget', '(', 'key', '=', '"', 'description', '"', ')', '\\', 'n', '</s>']
Filtered   (016): ['_', 'Multi', 'Line', 'String', 'Met', 'adata', 'Widget', '(', 'key', '=', '"', 'description', '"', ')', '\\', 'n']
Detokenized (007): ['_MultiLineStringMetadataWidget', '(', 'key', '=', '"description"', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""active" : isinstance ( node , Gaffer . Box ) or nodeEditor . nodeUI ( ) . plugValueWidget ( node [ "user" ] ) } \n"
Original    (026): ['"active"', ':', 'isinstance', '(', 'node', ',', 'Gaffer', '.', 'Box', ')', 'or', 'nodeEditor', '.', 'nodeUI', '(', ')', '.', 'plugValueWidget', '(', 'node', '[', '"user"', ']', ')', '}', '\\n']
Tokenized   (039): ['<s>', '"', 'active', '"', ':', 'is', 'instance', '(', 'node', ',', 'G', 'affer', '.', 'Box', ')', 'or', 'node', 'Editor', '.', 'node', 'UI', '(', ')', '.', 'plug', 'Value', 'Widget', '(', 'node', '[', '"', 'user', '"', ']', ')', '}', '\\', 'n', '</s>']
Filtered   (037): ['"', 'active', '"', ':', 'is', 'instance', '(', 'node', ',', 'G', 'affer', '.', 'Box', ')', 'or', 'node', 'Editor', '.', 'node', 'UI', '(', ')', '.', 'plug', 'Value', 'Widget', '(', 'node', '[', '"', 'user', '"', ']', ')', '}', '\\', 'n']
Detokenized (026): ['"active"', ':', 'isinstance', '(', 'node', ',', 'Gaffer', '.', 'Box', ')', 'or', 'nodeEditor', '.', 'nodeUI', '(', ')', '.', 'plugValueWidget', '(', 'node', '[', '"user"', ']', ')', '}', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "dialogue = GafferUI . ColorChooserDialogue ( color = color , useDisplayTransform = False ) \n"
Original    (015): ['dialogue', '=', 'GafferUI', '.', 'ColorChooserDialogue', '(', 'color', '=', 'color', ',', 'useDisplayTransform', '=', 'False', ')', '\\n']
Tokenized   (026): ['<s>', 'dial', 'ogue', '=', 'G', 'affer', 'UI', '.', 'Color', 'Cho', 'oser', 'Dialogue', '(', 'color', '=', 'color', ',', 'use', 'Display', 'Transform', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (024): ['dial', 'ogue', '=', 'G', 'affer', 'UI', '.', 'Color', 'Cho', 'oser', 'Dialogue', '(', 'color', '=', 'color', ',', 'use', 'Display', 'Transform', '=', 'False', ')', '\\', 'n']
Detokenized (015): ['dialogue', '=', 'GafferUI', '.', 'ColorChooserDialogue', '(', 'color', '=', 'color', ',', 'useDisplayTransform', '=', 'False', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "editor . plugEditor ( ) . reveal ( ) \n"
Original    (010): ['editor', '.', 'plugEditor', '(', ')', '.', 'reveal', '(', ')', '\\n']
Tokenized   (014): ['<s>', 'editor', '.', 'plug', 'Editor', '(', ')', '.', 'reveal', '(', ')', '\\', 'n', '</s>']
Filtered   (012): ['editor', '.', 'plug', 'Editor', '(', ')', '.', 'reveal', '(', ')', '\\', 'n']
Detokenized (010): ['editor', '.', 'plugEditor', '(', ')', '.', 'reveal', '(', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "_MetadataWidget . __init__ ( self , self . __menuButton , key , target , parenting = parenting ) \n"
Original    (019): ['_MetadataWidget', '.', '__init__', '(', 'self', ',', 'self', '.', '__menuButton', ',', 'key', ',', 'target', ',', 'parenting', '=', 'parenting', ')', '\\n']
Tokenized   (029): ['<s>', '_', 'Met', 'adata', 'Widget', '.', '__', 'init', '__', '(', 'self', ',', 'self', '.', '__', 'menu', 'Button', ',', 'key', ',', 'target', ',', 'parenting', '=', 'parenting', ')', '\\', 'n', '</s>']
Filtered   (027): ['_', 'Met', 'adata', 'Widget', '.', '__', 'init', '__', '(', 'self', ',', 'self', '.', '__', 'menu', 'Button', ',', 'key', ',', 'target', ',', 'parenting', '=', 'parenting', ')', '\\', 'n']
Detokenized (019): ['_MetadataWidget', '.', '__init__', '(', 'self', ',', 'self', '.', '__menuButton', ',', 'key', ',', 'target', ',', 'parenting', '=', 'parenting', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : ""checkBox" : value == self . __currentValue \n"
Original    (008): ['"checkBox"', ':', 'value', '==', 'self', '.', '__currentValue', '\\n']
Tokenized   (016): ['<s>', '"', 'check', 'Box', '"', ':', 'value', '==', 'self', '.', '__', 'current', 'Value', '\\', 'n', '</s>']
Filtered   (014): ['"', 'check', 'Box', '"', ':', 'value', '==', 'self', '.', '__', 'current', 'Value', '\\', 'n']
Detokenized (008): ['"checkBox"', ':', 'value', '==', 'self', '.', '__currentValue', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "child . __parent = None \n"
Original    (006): ['child', '.', '__parent', '=', 'None', '\\n']
Tokenized   (010): ['<s>', 'child', '.', '__', 'parent', '=', 'None', '\\', 'n', '</s>']
Filtered   (008): ['child', '.', '__', 'parent', '=', 'None', '\\', 'n']
Detokenized (006): ['child', '.', '__parent', '=', 'None', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "columns = ( GafferUI . PathListingWidget . defaultNameColumn , ) , \n"
Original    (012): ['columns', '=', '(', 'GafferUI', '.', 'PathListingWidget', '.', 'defaultNameColumn', ',', ')', ',', '\\n']
Tokenized   (023): ['<s>', 'column', 's', '=', '(', 'G', 'affer', 'UI', '.', 'Path', 'List', 'ing', 'Widget', '.', 'default', 'Name', 'Column', ',', ')', ',', '\\', 'n', '</s>']
Filtered   (021): ['column', 's', '=', '(', 'G', 'affer', 'UI', '.', 'Path', 'List', 'ing', 'Widget', '.', 'default', 'Name', 'Column', ',', ')', ',', '\\', 'n']
Detokenized (012): ['columns', '=', '(', 'GafferUI', '.', 'PathListingWidget', '.', 'defaultNameColumn', ',', ')', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "definition = Gaffer . WeakMethod ( self . __addMenuDefinition ) \n"
Original    (011): ['definition', '=', 'Gaffer', '.', 'WeakMethod', '(', 'self', '.', '__addMenuDefinition', ')', '\\n']
Tokenized   (019): ['<s>', 'definition', '=', 'G', 'affer', '.', 'Weak', 'Method', '(', 'self', '.', '__', 'add', 'Menu', 'Definition', ')', '\\', 'n', '</s>']
Filtered   (017): ['definition', '=', 'G', 'affer', '.', 'Weak', 'Method', '(', 'self', '.', '__', 'add', 'Menu', 'Definition', ')', '\\', 'n']
Detokenized (011): ['definition', '=', 'Gaffer', '.', 'WeakMethod', '(', 'self', '.', '__addMenuDefinition', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "newIndex = 0 if event . line . p0 . y < 1 else len ( newParent ) \n"
Original    (019): ['newIndex', '=', '0', 'if', 'event', '.', 'line', '.', 'p0', '.', 'y', '<', '1', 'else', 'len', '(', 'newParent', ')', '\\n']
Tokenized   (025): ['<s>', 'new', 'Index', '=', '0', 'if', 'event', '.', 'line', '.', 'p', '0', '.', 'y', '<', '1', 'else', 'len', '(', 'new', 'Parent', ')', '\\', 'n', '</s>']
Filtered   (023): ['new', 'Index', '=', '0', 'if', 'event', '.', 'line', '.', 'p', '0', '.', 'y', '<', '1', 'else', 'len', '(', 'new', 'Parent', ')', '\\', 'n']
Detokenized (019): ['newIndex', '=', '0', 'if', 'event', '.', 'line', '.', 'p0', '.', 'y', '<', '1', 'else', 'len', '(', 'newParent', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "newParent . insert ( newIndex , self . __dragItem ) \n"
Original    (011): ['newParent', '.', 'insert', '(', 'newIndex', ',', 'self', '.', '__dragItem', ')', '\\n']
Tokenized   (019): ['<s>', 'new', 'Parent', '.', 'insert', '(', 'new', 'Index', ',', 'self', '.', '__', 'dr', 'ag', 'Item', ')', '\\', 'n', '</s>']
Filtered   (017): ['new', 'Parent', '.', 'insert', '(', 'new', 'Index', ',', 'self', '.', '__', 'dr', 'ag', 'Item', ')', '\\', 'n']
Detokenized (011): ['newParent', '.', 'insert', '(', 'newIndex', ',', 'self', '.', '__dragItem', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "selection [ : ] = self . __dragItem . fullName ( ) . split ( "." ) \n"
Original    (018): ['selection', '[', ':', ']', '=', 'self', '.', '__dragItem', '.', 'fullName', '(', ')', '.', 'split', '(', '"."', ')', '\\n']
Tokenized   (026): ['<s>', 'selection', '[', ':', ']', '=', 'self', '.', '__', 'dr', 'ag', 'Item', '.', 'full', 'Name', '(', ')', '.', 'split', '(', '"', '."', ')', '\\', 'n', '</s>']
Filtered   (024): ['selection', '[', ':', ']', '=', 'self', '.', '__', 'dr', 'ag', 'Item', '.', 'full', 'Name', '(', ')', '.', 'split', '(', '"', '."', ')', '\\', 'n']
Detokenized (018): ['selection', '[', ':', ']', '=', 'self', '.', '__dragItem', '.', 'fullName', '(', ')', '.', 'split', '(', '"."', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "_registerMetadata ( plug , "nodule:type" , "" ) \n"
Original    (009): ['_registerMetadata', '(', 'plug', ',', '"nodule:type"', ',', '""', ')', '\\n']
Tokenized   (021): ['<s>', '_', 'register', 'Met', 'adata', '(', 'plug', ',', '"', 'n', 'od', 'ule', ':', 'type', '"', ',', '""', ')', '\\', 'n', '</s>']
Filtered   (019): ['_', 'register', 'Met', 'adata', '(', 'plug', ',', '"', 'n', 'od', 'ule', ':', 'type', '"', ',', '""', ')', '\\', 'n']
Detokenized (009): ['_registerMetadata', '(', 'plug', ',', '"nodule:type"', ',', '""', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "parentItem \n"
Original    (002): ['parentItem', '\\n']
Tokenized   (006): ['<s>', 'parent', 'Item', '\\', 'n', '</s>']
Filtered   (004): ['parent', 'Item', '\\', 'n']
Detokenized (002): ['parentItem', '\\n']
Counter: 4
===================================================================
Hidden states:  (13, 2, 768)
# Extracted words:  2
Sentence         : "existingSectionNames = set ( c . name ( ) for c in rootItem if isinstance ( c , _SectionLayoutItem ) ) \n"
Original    (022): ['existingSectionNames', '=', 'set', '(', 'c', '.', 'name', '(', ')', 'for', 'c', 'in', 'rootItem', 'if', 'isinstance', '(', 'c', ',', '_SectionLayoutItem', ')', ')', '\\n']
Tokenized   (032): ['<s>', 'existing', 'Section', 'Names', '=', 'set', '(', 'c', '.', 'name', '(', ')', 'for', 'c', 'in', 'root', 'Item', 'if', 'is', 'instance', '(', 'c', ',', '_', 'Section', 'Layout', 'Item', ')', ')', '\\', 'n', '</s>']
Filtered   (030): ['existing', 'Section', 'Names', '=', 'set', '(', 'c', '.', 'name', '(', ')', 'for', 'c', 'in', 'root', 'Item', 'if', 'is', 'instance', '(', 'c', ',', '_', 'Section', 'Layout', 'Item', ')', ')', '\\', 'n']
Detokenized (022): ['existingSectionNames', '=', 'set', '(', 'c', '.', 'name', '(', ')', 'for', 'c', 'in', 'rootItem', 'if', 'isinstance', '(', 'c', ',', '_SectionLayoutItem', ')', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "Gaffer . Metadata . plugValue ( self . getPlug ( ) , "preset:" + selectedPaths [ 0 ] [ 0 ] ) \n"
Original    (023): ['Gaffer', '.', 'Metadata', '.', 'plugValue', '(', 'self', '.', 'getPlug', '(', ')', ',', '"preset:"', '+', 'selectedPaths', '[', '0', ']', '[', '0', ']', ')', '\\n']
Tokenized   (035): ['<s>', 'G', 'affer', '.', 'Met', 'adata', '.', 'plug', 'Value', '(', 'self', '.', 'get', 'Plug', '(', ')', ',', '"', 'pres', 'et', ':"', '+', 'selected', 'Path', 's', '[', '0', ']', '[', '0', ']', ')', '\\', 'n', '</s>']
Filtered   (033): ['G', 'affer', '.', 'Met', 'adata', '.', 'plug', 'Value', '(', 'self', '.', 'get', 'Plug', '(', ')', ',', '"', 'pres', 'et', ':"', '+', 'selected', 'Path', 's', '[', '0', ']', '[', '0', ']', ')', '\\', 'n']
Detokenized (023): ['Gaffer', '.', 'Metadata', '.', 'plugValue', '(', 'self', '.', 'getPlug', '(', ')', ',', '"preset:"', '+', 'selectedPaths', '[', '0', ']', '[', '0', ']', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "srcPath = self . __pathListing . getPath ( ) . copy ( ) . setFromString ( event . data [ 0 ] ) \n"
Original    (024): ['srcPath', '=', 'self', '.', '__pathListing', '.', 'getPath', '(', ')', '.', 'copy', '(', ')', '.', 'setFromString', '(', 'event', '.', 'data', '[', '0', ']', ')', '\\n']
Tokenized   (034): ['<s>', 'src', 'Path', '=', 'self', '.', '__', 'path', 'List', 'ing', '.', 'get', 'Path', '(', ')', '.', 'copy', '(', ')', '.', 'set', 'From', 'String', '(', 'event', '.', 'data', '[', '0', ']', ')', '\\', 'n', '</s>']
Filtered   (032): ['src', 'Path', '=', 'self', '.', '__', 'path', 'List', 'ing', '.', 'get', 'Path', '(', ')', '.', 'copy', '(', ')', '.', 'set', 'From', 'String', '(', 'event', '.', 'data', '[', '0', ']', ')', '\\', 'n']
Detokenized (024): ['srcPath', '=', 'self', '.', '__pathListing', '.', 'getPath', '(', ')', '.', 'copy', '(', ')', '.', 'setFromString', '(', 'event', '.', 'data', '[', '0', ']', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "srcIndex = d . keys ( ) . index ( srcPath [ 0 ] ) \n"
Original    (016): ['srcIndex', '=', 'd', '.', 'keys', '(', ')', '.', 'index', '(', 'srcPath', '[', '0', ']', ')', '\\n']
Tokenized   (021): ['<s>', 'src', 'Index', '=', 'd', '.', 'keys', '(', ')', '.', 'index', '(', 'src', 'Path', '[', '0', ']', ')', '\\', 'n', '</s>']
Filtered   (019): ['src', 'Index', '=', 'd', '.', 'keys', '(', ')', '.', 'index', '(', 'src', 'Path', '[', '0', ']', ')', '\\', 'n']
Detokenized (016): ['srcIndex', '=', 'd', '.', 'keys', '(', ')', '.', 'index', '(', 'srcPath', '[', '0', ']', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "targetPath = self . __pathListing . pathAt ( event . line . p0 ) \n"
Original    (015): ['targetPath', '=', 'self', '.', '__pathListing', '.', 'pathAt', '(', 'event', '.', 'line', '.', 'p0', ')', '\\n']
Tokenized   (024): ['<s>', 'target', 'Path', '=', 'self', '.', '__', 'path', 'List', 'ing', '.', 'path', 'At', '(', 'event', '.', 'line', '.', 'p', '0', ')', '\\', 'n', '</s>']
Filtered   (022): ['target', 'Path', '=', 'self', '.', '__', 'path', 'List', 'ing', '.', 'path', 'At', '(', 'event', '.', 'line', '.', 'p', '0', ')', '\\', 'n']
Detokenized (015): ['targetPath', '=', 'self', '.', '__pathListing', '.', 'pathAt', '(', 'event', '.', 'line', '.', 'p0', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "item = items [ srcIndex ] \n"
Original    (007): ['item', '=', 'items', '[', 'srcIndex', ']', '\\n']
Tokenized   (011): ['<s>', 'item', '=', 'items', '[', 'src', 'Index', ']', '\\', 'n', '</s>']
Filtered   (009): ['item', '=', 'items', '[', 'src', 'Index', ']', '\\', 'n']
Detokenized (007): ['item', '=', 'items', '[', 'srcIndex', ']', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "selectedPreset = self . __pathListing . getSelectedPaths ( ) [ 0 ] [ 0 ] \n"
Original    (016): ['selectedPreset', '=', 'self', '.', '__pathListing', '.', 'getSelectedPaths', '(', ')', '[', '0', ']', '[', '0', ']', '\\n']
Tokenized   (028): ['<s>', 'selected', 'Pres', 'et', '=', 'self', '.', '__', 'path', 'List', 'ing', '.', 'get', 'Se', 'lected', 'Path', 's', '(', ')', '[', '0', ']', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (026): ['selected', 'Pres', 'et', '=', 'self', '.', '__', 'path', 'List', 'ing', '.', 'get', 'Se', 'lected', 'Path', 's', '(', ')', '[', '0', ']', '[', '0', ']', '\\', 'n']
Detokenized (016): ['selectedPreset', '=', 'self', '.', '__pathListing', '.', 'getSelectedPaths', '(', ')', '[', '0', ']', '[', '0', ']', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "selectedIndex = [ p [ 0 ] for p in paths ] . index ( selectedPreset ) \n"
Original    (018): ['selectedIndex', '=', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'paths', ']', '.', 'index', '(', 'selectedPreset', ')', '\\n']
Tokenized   (024): ['<s>', 'selected', 'Index', '=', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'paths', ']', '.', 'index', '(', 'selected', 'Pres', 'et', ')', '\\', 'n', '</s>']
Filtered   (022): ['selected', 'Index', '=', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'paths', ']', '.', 'index', '(', 'selected', 'Pres', 'et', ')', '\\', 'n']
Detokenized (018): ['selectedIndex', '=', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'paths', ']', '.', 'index', '(', 'selectedPreset', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "preset = selectedPaths [ 0 ] [ 0 ] \n"
Original    (010): ['preset', '=', 'selectedPaths', '[', '0', ']', '[', '0', ']', '\\n']
Tokenized   (016): ['<s>', 'pres', 'et', '=', 'selected', 'Path', 's', '[', '0', ']', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (014): ['pres', 'et', '=', 'selected', 'Path', 's', '[', '0', ']', '[', '0', ']', '\\', 'n']
Detokenized (010): ['preset', '=', 'selectedPaths', '[', '0', ']', '[', '0', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "scrolledContainer . setChild ( GafferUI . ListContainer ( spacing = 4 ) ) \n"
Original    (014): ['scrolledContainer', '.', 'setChild', '(', 'GafferUI', '.', 'ListContainer', '(', 'spacing', '=', '4', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'sc', 'rolled', 'Container', '.', 'set', 'Child', '(', 'G', 'affer', 'UI', '.', 'List', 'Container', '(', 'spacing', '=', '4', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['sc', 'rolled', 'Container', '.', 'set', 'Child', '(', 'G', 'affer', 'UI', '.', 'List', 'Container', '(', 'spacing', '=', '4', ')', ')', '\\', 'n']
Detokenized (014): ['scrolledContainer', '.', 'setChild', '(', 'GafferUI', '.', 'ListContainer', '(', 'spacing', '=', '4', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __gadgetMenuDefinition ) ) \n"
Original    (016): ['menu', '=', 'GafferUI', '.', 'Menu', '(', 'Gaffer', '.', 'WeakMethod', '(', 'self', '.', '__gadgetMenuDefinition', ')', ')', '\\n']
Tokenized   (028): ['<s>', 'menu', '=', 'G', 'affer', 'UI', '.', 'Menu', '(', 'G', 'affer', '.', 'Weak', 'Method', '(', 'self', '.', '__', 'g', 'ad', 'get', 'Menu', 'Definition', ')', ')', '\\', 'n', '</s>']
Filtered   (026): ['menu', '=', 'G', 'affer', 'UI', '.', 'Menu', '(', 'G', 'affer', '.', 'Weak', 'Method', '(', 'self', '.', '__', 'g', 'ad', 'get', 'Menu', 'Definition', ')', ')', '\\', 'n']
Detokenized (016): ['menu', '=', 'GafferUI', '.', 'Menu', '(', 'Gaffer', '.', 'WeakMethod', '(', 'self', '.', '__gadgetMenuDefinition', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : ""/" + g . label , \n"
Original    (007): ['"/"', '+', 'g', '.', 'label', ',', '\\n']
Tokenized   (011): ['<s>', '"', '/"', '+', 'g', '.', 'label', ',', '\\', 'n', '</s>']
Filtered   (009): ['"', '/"', '+', 'g', '.', 'label', ',', '\\', 'n']
Detokenized (007): ['"/"', '+', 'g', '.', 'label', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""command" : functools . partial ( Gaffer . WeakMethod ( self . __registerOrDeregisterMetadata ) , key = "checkBox" : metadata == g . metadata , \n"
Original    (026): ['"command"', ':', 'functools', '.', 'partial', '(', 'Gaffer', '.', 'WeakMethod', '(', 'self', '.', '__registerOrDeregisterMetadata', ')', ',', 'key', '=', '"checkBox"', ':', 'metadata', '==', 'g', '.', 'metadata', ',', '\\n']
Tokenized   (046): ['<s>', '"', 'command', '"', ':', 'fun', 'ct', 'ools', '.', 'partial', '(', 'G', 'affer', '.', 'Weak', 'Method', '(', 'self', '.', '__', 'register', 'Or', 'D', 'ere', 'g', 'ister', 'Met', 'adata', ')', ',', 'key', '=', '"', 'check', 'Box', '"', ':', 'metadata', '==', 'g', '.', 'metadata', ',', '\\', 'n', '</s>']
Filtered   (044): ['"', 'command', '"', ':', 'fun', 'ct', 'ools', '.', 'partial', '(', 'G', 'affer', '.', 'Weak', 'Method', '(', 'self', '.', '__', 'register', 'Or', 'D', 'ere', 'g', 'ister', 'Met', 'adata', ')', ',', 'key', '=', '"', 'check', 'Box', '"', ':', 'metadata', '==', 'g', '.', 'metadata', ',', '\\', 'n']
Detokenized (026): ['"command"', ':', 'functools', '.', 'partial', '(', 'Gaffer', '.', 'WeakMethod', '(', 'self', '.', '__registerOrDeregisterMetadata', ')', ',', 'key', '=', '"checkBox"', ':', 'metadata', '==', 'g', '.', 'metadata', ',', '\\n']
Counter: 44
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "__WidgetDefinition ( "None" , Gaffer . Plug , "" ) , \n"
Original    (012): ['__WidgetDefinition', '(', '"None"', ',', 'Gaffer', '.', 'Plug', ',', '""', ')', ',', '\\n']
Tokenized   (020): ['<s>', '__', 'Widget', 'Definition', '(', '"', 'None', '"', ',', 'G', 'affer', '.', 'Plug', ',', '""', ')', ',', '\\', 'n', '</s>']
Filtered   (018): ['__', 'Widget', 'Definition', '(', '"', 'None', '"', ',', 'G', 'affer', '.', 'Plug', ',', '""', ')', ',', '\\', 'n']
Detokenized (012): ['__WidgetDefinition', '(', '"None"', ',', 'Gaffer', '.', 'Plug', ',', '""', ')', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "__MetadataDefinition = collections . namedtuple ( "MetadataDefinition" , ( "key" , "label" , "metadataWidgetType" __metadataDefinitions = ( \n"
Original    (018): ['__MetadataDefinition', '=', 'collections', '.', 'namedtuple', '(', '"MetadataDefinition"', ',', '(', '"key"', ',', '"label"', ',', '"metadataWidgetType"', '__metadataDefinitions', '=', '(', '\\n']
Tokenized   (041): ['<s>', '__', 'Met', 'adata', 'Definition', '=', 'collections', '.', 'named', 't', 'uple', '(', '"', 'Met', 'adata', 'Definition', '"', ',', '(', '"', 'key', '"', ',', '"', 'label', '"', ',', '"', 'metadata', 'Widget', 'Type', '"', '__', 'metadata', 'Def', 'initions', '=', '(', '\\', 'n', '</s>']
Filtered   (039): ['__', 'Met', 'adata', 'Definition', '=', 'collections', '.', 'named', 't', 'uple', '(', '"', 'Met', 'adata', 'Definition', '"', ',', '(', '"', 'key', '"', ',', '"', 'label', '"', ',', '"', 'metadata', 'Widget', 'Type', '"', '__', 'metadata', 'Def', 'initions', '=', '(', '\\', 'n']
Detokenized (018): ['__MetadataDefinition', '=', 'collections', '.', 'namedtuple', '(', '"MetadataDefinition"', ',', '(', '"key"', ',', '"label"', ',', '"metadataWidgetType"', '__metadataDefinitions', '=', '(', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "newSectionPath [ - 1 ] = nameWidget . getText ( ) . replace ( "." , "" ) \n"
Original    (019): ['newSectionPath', '[', '-', '1', ']', '=', 'nameWidget', '.', 'getText', '(', ')', '.', 'replace', '(', '"."', ',', '""', ')', '\\n']
Tokenized   (027): ['<s>', 'new', 'Section', 'Path', '[', '-', '1', ']', '=', 'name', 'Widget', '.', 'get', 'Text', '(', ')', '.', 'replace', '(', '"', '."', ',', '""', ')', '\\', 'n', '</s>']
Filtered   (025): ['new', 'Section', 'Path', '[', '-', '1', ']', '=', 'name', 'Widget', '.', 'get', 'Text', '(', ')', '.', 'replace', '(', '"', '."', ',', '""', ')', '\\', 'n']
Detokenized (019): ['newSectionPath', '[', '-', '1', ']', '=', 'nameWidget', '.', 'getText', '(', ')', '.', 'replace', '(', '"."', ',', '""', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "_metadata ( self . getPlugParent ( ) , name ) \n"
Original    (011): ['_metadata', '(', 'self', '.', 'getPlugParent', '(', ')', ',', 'name', ')', '\\n']
Tokenized   (017): ['<s>', '_', 'metadata', '(', 'self', '.', 'get', 'Plug', 'Parent', '(', ')', ',', 'name', ')', '\\', 'n', '</s>']
Filtered   (015): ['_', 'metadata', '(', 'self', '.', 'get', 'Plug', 'Parent', '(', ')', ',', 'name', ')', '\\', 'n']
Detokenized (011): ['_metadata', '(', 'self', '.', 'getPlugParent', '(', ')', ',', 'name', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "_deregisterMetadata ( self . getPlugParent ( ) , name ) \n"
Original    (011): ['_deregisterMetadata', '(', 'self', '.', 'getPlugParent', '(', ')', ',', 'name', ')', '\\n']
Tokenized   (022): ['<s>', '_', 'd', 'ere', 'g', 'ister', 'Met', 'adata', '(', 'self', '.', 'get', 'Plug', 'Parent', '(', ')', ',', 'name', ')', '\\', 'n', '</s>']
Filtered   (020): ['_', 'd', 'ere', 'g', 'ister', 'Met', 'adata', '(', 'self', '.', 'get', 'Plug', 'Parent', '(', ')', ',', 'name', ')', '\\', 'n']
Detokenized (011): ['_deregisterMetadata', '(', 'self', '.', 'getPlugParent', '(', ')', ',', 'name', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "wr2 = weakref . ref ( w . _qtWidget ( ) ) \n"
Original    (013): ['wr2', '=', 'weakref', '.', 'ref', '(', 'w', '.', '_qtWidget', '(', ')', ')', '\\n']
Tokenized   (020): ['<s>', 'wr', '2', '=', 'weak', 'ref', '.', 'ref', '(', 'w', '.', '_', 'qt', 'Widget', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (018): ['wr', '2', '=', 'weak', 'ref', '.', 'ref', '(', 'w', '.', '_', 'qt', 'Widget', '(', ')', ')', '\\', 'n']
Detokenized (013): ['wr2', '=', 'weakref', '.', 'ref', '(', 'w', '.', '_qtWidget', '(', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "WidgetTest . signalsEmitted = 0 \n"
Original    (006): ['WidgetTest', '.', 'signalsEmitted', '=', '0', '\\n']
Tokenized   (012): ['<s>', 'Widget', 'Test', '.', 'signals', 'E', 'mitted', '=', '0', '\\', 'n', '</s>']
Filtered   (010): ['Widget', 'Test', '.', 'signals', 'E', 'mitted', '=', '0', '\\', 'n']
Detokenized (006): ['WidgetTest', '.', 'signalsEmitted', '=', '0', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "QtGui . QApplication . instance ( ) . sendEvent ( w . _qtWidget ( ) , event ) \n"
Original    (019): ['QtGui', '.', 'QApplication', '.', 'instance', '(', ')', '.', 'sendEvent', '(', 'w', '.', '_qtWidget', '(', ')', ',', 'event', ')', '\\n']
Tokenized   (029): ['<s>', 'Q', 't', 'Gu', 'i', '.', 'Q', 'Application', '.', 'instance', '(', ')', '.', 'send', 'Event', '(', 'w', '.', '_', 'qt', 'Widget', '(', ')', ',', 'event', ')', '\\', 'n', '</s>']
Filtered   (027): ['Q', 't', 'Gu', 'i', '.', 'Q', 'Application', '.', 'instance', '(', ')', '.', 'send', 'Event', '(', 'w', '.', '_', 'qt', 'Widget', '(', ')', ',', 'event', ')', '\\', 'n']
Detokenized (019): ['QtGui', '.', 'QApplication', '.', 'instance', '(', ')', '.', 'sendEvent', '(', 'w', '.', '_qtWidget', '(', ')', ',', 'event', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "GafferUI . BoxUI . appendNodeEditorToolMenuDefinitions ( nodeEditor , node , menuDefinition ) \n"
Original    (013): ['GafferUI', '.', 'BoxUI', '.', 'appendNodeEditorToolMenuDefinitions', '(', 'nodeEditor', ',', 'node', ',', 'menuDefinition', ')', '\\n']
Tokenized   (027): ['<s>', 'G', 'affer', 'UI', '.', 'Box', 'UI', '.', 'append', 'Node', 'Editor', 'Tool', 'Menu', 'Def', 'initions', '(', 'node', 'Editor', ',', 'node', ',', 'menu', 'Definition', ')', '\\', 'n', '</s>']
Filtered   (025): ['G', 'affer', 'UI', '.', 'Box', 'UI', '.', 'append', 'Node', 'Editor', 'Tool', 'Menu', 'Def', 'initions', '(', 'node', 'Editor', ',', 'node', ',', 'menu', 'Definition', ')', '\\', 'n']
Detokenized (013): ['GafferUI', '.', 'BoxUI', '.', 'appendNodeEditorToolMenuDefinitions', '(', 'nodeEditor', ',', 'node', ',', 'menuDefinition', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "GafferSceneUI . FilteredSceneProcessorUI . appendNodeEditorToolMenuDefinitions ( nodeEditor , node , menuDefinition \n"
Original    (012): ['GafferSceneUI', '.', 'FilteredSceneProcessorUI', '.', 'appendNodeEditorToolMenuDefinitions', '(', 'nodeEditor', ',', 'node', ',', 'menuDefinition', '\\n']
Tokenized   (031): ['<s>', 'G', 'affer', 'Scene', 'UI', '.', 'Fil', 'tered', 'Scene', 'Process', 'or', 'UI', '.', 'append', 'Node', 'Editor', 'Tool', 'Menu', 'Def', 'initions', '(', 'node', 'Editor', ',', 'node', ',', 'menu', 'Definition', '\\', 'n', '</s>']
Filtered   (029): ['G', 'affer', 'Scene', 'UI', '.', 'Fil', 'tered', 'Scene', 'Process', 'or', 'UI', '.', 'append', 'Node', 'Editor', 'Tool', 'Menu', 'Def', 'initions', '(', 'node', 'Editor', ',', 'node', ',', 'menu', 'Definition', '\\', 'n']
Detokenized (012): ['GafferSceneUI', '.', 'FilteredSceneProcessorUI', '.', 'appendNodeEditorToolMenuDefinitions', '(', 'nodeEditor', ',', 'node', ',', 'menuDefinition', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "yappi . print_stats ( sort_type = yappi . SORTTYPE_TTOT , limit = 30 , thread_stats_on = False ) \n"
Original    (019): ['yappi', '.', 'print_stats', '(', 'sort_type', '=', 'yappi', '.', 'SORTTYPE_TTOT', ',', 'limit', '=', '30', ',', 'thread_stats_on', '=', 'False', ')', '\\n']
Tokenized   (039): ['<s>', 'y', 'app', 'i', '.', 'print', '_', 'stats', '(', 'sort', '_', 'type', '=', 'y', 'app', 'i', '.', 'S', 'ORT', 'TYPE', '_', 'TT', 'OT', ',', 'limit', '=', '30', ',', 'thread', '_', 'stats', '_', 'on', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (037): ['y', 'app', 'i', '.', 'print', '_', 'stats', '(', 'sort', '_', 'type', '=', 'y', 'app', 'i', '.', 'S', 'ORT', 'TYPE', '_', 'TT', 'OT', ',', 'limit', '=', '30', ',', 'thread', '_', 'stats', '_', 'on', '=', 'False', ')', '\\', 'n']
Detokenized (019): ['yappi', '.', 'print_stats', '(', 'sort_type', '=', 'yappi', '.', 'SORTTYPE_TTOT', ',', 'limit', '=', '30', ',', 'thread_stats_on', '=', 'False', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "SAMPLE_EXTRACT_METRICS_PAGE = os . path . join ( datadir , "monthly_download" ) \n"
Original    (013): ['SAMPLE_EXTRACT_METRICS_PAGE', '=', 'os', '.', 'path', '.', 'join', '(', 'datadir', ',', '"monthly_download"', ')', '\\n']
Tokenized   (035): ['<s>', 'SAM', 'PLE', '_', 'EX', 'TR', 'ACT', '_', 'MET', 'R', 'ICS', '_', 'PA', 'GE', '=', 'os', '.', 'path', '.', 'join', '(', 'dat', 'ad', 'ir', ',', '"', 'month', 'ly', '_', 'download', '"', ')', '\\', 'n', '</s>']
Filtered   (033): ['SAM', 'PLE', '_', 'EX', 'TR', 'ACT', '_', 'MET', 'R', 'ICS', '_', 'PA', 'GE', '=', 'os', '.', 'path', '.', 'join', '(', 'dat', 'ad', 'ir', ',', '"', 'month', 'ly', '_', 'download', '"', ')', '\\', 'n']
Detokenized (013): ['SAMPLE_EXTRACT_METRICS_PAGE', '=', 'os', '.', 'path', '.', 'join', '(', 'datadir', ',', '"monthly_download"', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH = os . path . join ( datadir , "monthly_download_different_month" \n"
Original    (012): ['SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH', '=', 'os', '.', 'path', '.', 'join', '(', 'datadir', ',', '"monthly_download_different_month"', '\\n']
Tokenized   (046): ['<s>', 'SAM', 'PLE', '_', 'EX', 'TR', 'ACT', '_', 'MET', 'R', 'ICS', '_', 'PA', 'GE', '_', 'D', 'IF', 'FER', 'ENT', '_', 'MON', 'TH', '=', 'os', '.', 'path', '.', 'join', '(', 'dat', 'ad', 'ir', ',', '"', 'month', 'ly', '_', 'download', '_', 'different', '_', 'month', '"', '\\', 'n', '</s>']
Filtered   (044): ['SAM', 'PLE', '_', 'EX', 'TR', 'ACT', '_', 'MET', 'R', 'ICS', '_', 'PA', 'GE', '_', 'D', 'IF', 'FER', 'ENT', '_', 'MON', 'TH', '=', 'os', '.', 'path', '.', 'join', '(', 'dat', 'ad', 'ir', ',', '"', 'month', 'ly', '_', 'download', '_', 'different', '_', 'month', '"', '\\', 'n']
Detokenized (012): ['SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH', '=', 'os', '.', 'path', '.', 'join', '(', 'datadir', ',', '"monthly_download_different_month"', '\\n']
Counter: 44
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "testitem_aliases = ( "pmid" , TEST_PMID ) \n"
Original    (008): ['testitem_aliases', '=', '(', '"pmid"', ',', 'TEST_PMID', ')', '\\n']
Tokenized   (021): ['<s>', 'test', 'item', '_', 'ali', 'ases', '=', '(', '"', 'pm', 'id', '"', ',', 'TEST', '_', 'PM', 'ID', ')', '\\', 'n', '</s>']
Filtered   (019): ['test', 'item', '_', 'ali', 'ases', '=', '(', '"', 'pm', 'id', '"', ',', 'TEST', '_', 'PM', 'ID', ')', '\\', 'n']
Detokenized (008): ['testitem_aliases', '=', '(', '"pmid"', ',', 'TEST_PMID', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "sample_data_dump = open ( SAMPLE_EXTRACT_METRICS_PAGE , "r" ) . read ( ) \n"
Original    (013): ['sample_data_dump', '=', 'open', '(', 'SAMPLE_EXTRACT_METRICS_PAGE', ',', '"r"', ')', '.', 'read', '(', ')', '\\n']
Tokenized   (034): ['<s>', 'sample', '_', 'data', '_', 'dump', '=', 'open', '(', 'SAM', 'PLE', '_', 'EX', 'TR', 'ACT', '_', 'MET', 'R', 'ICS', '_', 'PA', 'GE', ',', '"', 'r', '"', ')', '.', 'read', '(', ')', '\\', 'n', '</s>']
Filtered   (032): ['sample', '_', 'data', '_', 'dump', '=', 'open', '(', 'SAM', 'PLE', '_', 'EX', 'TR', 'ACT', '_', 'MET', 'R', 'ICS', '_', 'PA', 'GE', ',', '"', 'r', '"', ')', '.', 'read', '(', ')', '\\', 'n']
Detokenized (013): ['sample_data_dump', '=', 'open', '(', 'SAMPLE_EXTRACT_METRICS_PAGE', ',', '"r"', ')', '.', 'read', '(', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "sample_data_dump_different_month = open ( SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH , "r" ) . read \n"
Original    (011): ['sample_data_dump_different_month', '=', 'open', '(', 'SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH', ',', '"r"', ')', '.', 'read', '\\n']
Tokenized   (044): ['<s>', 'sample', '_', 'data', '_', 'dump', '_', 'different', '_', 'month', '=', 'open', '(', 'SAM', 'PLE', '_', 'EX', 'TR', 'ACT', '_', 'MET', 'R', 'ICS', '_', 'PA', 'GE', '_', 'D', 'IF', 'FER', 'ENT', '_', 'MON', 'TH', ',', '"', 'r', '"', ')', '.', 'read', '\\', 'n', '</s>']
Filtered   (042): ['sample', '_', 'data', '_', 'dump', '_', 'different', '_', 'month', '=', 'open', '(', 'SAM', 'PLE', '_', 'EX', 'TR', 'ACT', '_', 'MET', 'R', 'ICS', '_', 'PA', 'GE', '_', 'D', 'IF', 'FER', 'ENT', '_', 'MON', 'TH', ',', '"', 'r', '"', ')', '.', 'read', '\\', 'n']
Detokenized (011): ['sample_data_dump_different_month', '=', 'open', '(', 'SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH', ',', '"r"', ')', '.', 'read', '\\n']
Counter: 42
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""max_event_date" : "2012-01-31T07:34:01.126892" \n"
Original    (004): ['"max_event_date"', ':', '"2012-01-31T07:34:01.126892"', '\\n']
Tokenized   (029): ['<s>', '"', 'max', '_', 'event', '_', 'date', '"', ':', '"', '2012', '-', '01', '-', '31', 'T', '07', ':', '34', ':', '01', '.', '12', '68', '92', '"', '\\', 'n', '</s>']
Filtered   (027): ['"', 'max', '_', 'event', '_', 'date', '"', ':', '"', '2012', '-', '01', '-', '31', 'T', '07', ':', '34', ':', '01', '.', '12', '68', '92', '"', '\\', 'n']
Detokenized (004): ['"max_event_date"', ':', '"2012-01-31T07:34:01.126892"', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : ""_id" : "abc123" , \n"
Original    (005): ['"_id"', ':', '"abc123"', ',', '\\n']
Tokenized   (014): ['<s>', '"', '_', 'id', '"', ':', '"', 'abc', '123', '"', ',', '\\', 'n', '</s>']
Filtered   (012): ['"', '_', 'id', '"', ':', '"', 'abc', '123', '"', ',', '\\', 'n']
Detokenized (005): ['"_id"', ':', '"abc123"', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""raw" : "max_event_date" : "2012-10-31T07:34:01.126892" , \n"
Original    (007): ['"raw"', ':', '"max_event_date"', ':', '"2012-10-31T07:34:01.126892"', ',', '\\n']
Tokenized   (034): ['<s>', '"', 'raw', '"', ':', '"', 'max', '_', 'event', '_', 'date', '"', ':', '"', '2012', '-', '10', '-', '31', 'T', '07', ':', '34', ':', '01', '.', '12', '68', '92', '"', ',', '\\', 'n', '</s>']
Filtered   (032): ['"', 'raw', '"', ':', '"', 'max', '_', 'event', '_', 'date', '"', ':', '"', '2012', '-', '10', '-', '31', 'T', '07', ':', '34', ':', '01', '.', '12', '68', '92', '"', ',', '\\', 'n']
Detokenized (007): ['"raw"', ':', '"max_event_date"', ':', '"2012-10-31T07:34:01.126892"', ',', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""23110252" \n"
Original    (002): ['"23110252"', '\\n']
Tokenized   (009): ['<s>', '"', '23', '110', '252', '"', '\\', 'n', '</s>']
Filtered   (007): ['"', '23', '110', '252', '"', '\\', 'n']
Detokenized (002): ['"23110252"', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 2, 768)
# Extracted words:  2
Sentence         : "cache_client = redis . from_url ( os . getenv ( "REDIS_URL" ) , REDIS_CACHE_DATABASE_NUMBER ) \n"
Original    (016): ['cache_client', '=', 'redis', '.', 'from_url', '(', 'os', '.', 'getenv', '(', '"REDIS_URL"', ')', ',', 'REDIS_CACHE_DATABASE_NUMBER', ')', '\\n']
Tokenized   (043): ['<s>', 'cache', '_', 'client', '=', 'red', 'is', '.', 'from', '_', 'url', '(', 'os', '.', 'get', 'env', '(', '"', 'RED', 'IS', '_', 'URL', '"', ')', ',', 'RED', 'IS', '_', 'C', 'AC', 'HE', '_', 'D', 'AT', 'AB', 'ASE', '_', 'NUM', 'BER', ')', '\\', 'n', '</s>']
Filtered   (041): ['cache', '_', 'client', '=', 'red', 'is', '.', 'from', '_', 'url', '(', 'os', '.', 'get', 'env', '(', '"', 'RED', 'IS', '_', 'URL', '"', ')', ',', 'RED', 'IS', '_', 'C', 'AC', 'HE', '_', 'D', 'AT', 'AB', 'ASE', '_', 'NUM', 'BER', ')', '\\', 'n']
Detokenized (016): ['cache_client', '=', 'redis', '.', 'from_url', '(', 'os', '.', 'getenv', '(', '"REDIS_URL"', ')', ',', 'REDIS_CACHE_DATABASE_NUMBER', ')', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "MAX_CACHE_SIZE_BYTES = 100 * 1000 * 1000 #100mb \n"
Original    (009): ['MAX_CACHE_SIZE_BYTES', '=', '100', '*', '1000', '*', '1000', '#100mb', '\\n']
Tokenized   (024): ['<s>', 'MAX', '_', 'C', 'AC', 'HE', '_', 'SIZE', '_', 'BY', 'T', 'ES', '=', '100', '*', '1000', '*', '1000', '#', '100', 'mb', '\\', 'n', '</s>']
Filtered   (022): ['MAX', '_', 'C', 'AC', 'HE', '_', 'SIZE', '_', 'BY', 'T', 'ES', '=', '100', '*', '1000', '*', '1000', '#', '100', 'mb', '\\', 'n']
Detokenized (009): ['MAX_CACHE_SIZE_BYTES', '=', '100', '*', '1000', '*', '1000', '#100mb', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "set_response = mc . set ( hash_key , json . dumps ( data ) ) \n"
Original    (016): ['set_response', '=', 'mc', '.', 'set', '(', 'hash_key', ',', 'json', '.', 'dumps', '(', 'data', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'set', '_', 'response', '=', 'mc', '.', 'set', '(', 'hash', '_', 'key', ',', 'json', '.', 'dumps', '(', 'data', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['set', '_', 'response', '=', 'mc', '.', 'set', '(', 'hash', '_', 'key', ',', 'json', '.', 'dumps', '(', 'data', ')', ')', '\\', 'n']
Detokenized (016): ['set_response', '=', 'mc', '.', 'set', '(', 'hash_key', ',', 'json', '.', 'dumps', '(', 'data', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "metrics_url_template = "http://alm.plos.org/api/v3/articles?ids=%s&source=citations,counter&api_key=" provenance_url_template = "http://dx.doi.org/%s" \n"
Original    (007): ['metrics_url_template', '=', '"http://alm.plos.org/api/v3/articles?ids=%s&source=citations,counter&api_key="', 'provenance_url_template', '=', '"http://dx.doi.org/%s"', '\\n']
Tokenized   (063): ['<s>', 'met', 'rics', '_', 'url', '_', 'template', '=', '"', 'http', '://', 'alm', '.', 'pl', 'os', '.', 'org', '/', 'api', '/', 'v', '3', '/', 'articles', '?', 'ids', '=', '%', 's', '&', 'source', '=', 'c', 'itations', ',', 'counter', '&', 'api', '_', 'key', '="', 'proven', 'ance', '_', 'url', '_', 'template', '=', '"', 'http', '://', 'dx', '.', 'doi', '.', 'org', '/', '%', 's', '"', '\\', 'n', '</s>']
Filtered   (061): ['met', 'rics', '_', 'url', '_', 'template', '=', '"', 'http', '://', 'alm', '.', 'pl', 'os', '.', 'org', '/', 'api', '/', 'v', '3', '/', 'articles', '?', 'ids', '=', '%', 's', '&', 'source', '=', 'c', 'itations', ',', 'counter', '&', 'api', '_', 'key', '="', 'proven', 'ance', '_', 'url', '_', 'template', '=', '"', 'http', '://', 'dx', '.', 'doi', '.', 'org', '/', '%', 's', '"', '\\', 'n']
Detokenized (007): ['metrics_url_template', '=', '"http://alm.plos.org/api/v3/articles?ids=%s&source=citations,counter&api_key="', 'provenance_url_template', '=', '"http://dx.doi.org/%s"', '\\n']
Counter: 61
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "relevant = ( ( "doi" == namespace ) and ( "10.1371/" in nid ) ) \n"
Original    (016): ['relevant', '=', '(', '(', '"doi"', '==', 'namespace', ')', 'and', '(', '"10.1371/"', 'in', 'nid', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'relevant', '=', '(', '(', '"', 'doi', '"', '==', 'namespace', ')', 'and', '(', '"', '10', '.', '13', '71', '/"', 'in', 'n', 'id', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['relevant', '=', '(', '(', '"', 'doi', '"', '==', 'namespace', ')', 'and', '(', '"', '10', '.', '13', '71', '/"', 'in', 'n', 'id', ')', ')', '\\', 'n']
Detokenized (016): ['relevant', '=', '(', '(', '"doi"', '==', 'namespace', ')', 'and', '(', '"10.1371/"', 'in', 'nid', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "this_article = json_response [ 0 ] [ "sources" ] [ 0 ] [ "metrics" ] \n"
Original    (016): ['this_article', '=', 'json_response', '[', '0', ']', '[', '"sources"', ']', '[', '0', ']', '[', '"metrics"', ']', '\\n']
Tokenized   (029): ['<s>', 'this', '_', 'article', '=', 'json', '_', 'response', '[', '0', ']', '[', '"', 's', 'ources', '"', ']', '[', '0', ']', '[', '"', 'met', 'rics', '"', ']', '\\', 'n', '</s>']
Filtered   (027): ['this', '_', 'article', '=', 'json', '_', 'response', '[', '0', ']', '[', '"', 's', 'ources', '"', ']', '[', '0', ']', '[', '"', 'met', 'rics', '"', ']', '\\', 'n']
Detokenized (016): ['this_article', '=', 'json_response', '[', '0', ']', '[', '"sources"', ']', '[', '0', ']', '[', '"metrics"', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "redis_url = os . environ . get ( , "redis://127.0.0.1:6379/" ) \n"
Original    (012): ['redis_url', '=', 'os', '.', 'environ', '.', 'get', '(', ',', '"redis://127.0.0.1:6379/"', ')', '\\n']
Tokenized   (033): ['<s>', 'red', 'is', '_', 'url', '=', 'os', '.', 'en', 'viron', '.', 'get', '(', ',', '"', 'red', 'is', '://', '127', '.', '0', '.', '0', '.', '1', ':', '6', '379', '/"', ')', '\\', 'n', '</s>']
Filtered   (031): ['red', 'is', '_', 'url', '=', 'os', '.', 'en', 'viron', '.', 'get', '(', ',', '"', 'red', 'is', '://', '127', '.', '0', '.', '0', '.', '1', ':', '6', '379', '/"', ')', '\\', 'n']
Detokenized (012): ['redis_url', '=', 'os', '.', 'environ', '.', 'get', '(', ',', '"redis://127.0.0.1:6379/"', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "Queue ( , routing_key = ) \n"
Original    (007): ['Queue', '(', ',', 'routing_key', '=', ')', '\\n']
Tokenized   (012): ['<s>', 'Queue', '(', ',', 'routing', '_', 'key', '=', ')', '\\', 'n', '</s>']
Filtered   (010): ['Queue', '(', ',', 'routing', '_', 'key', '=', ')', '\\', 'n']
Detokenized (007): ['Queue', '(', ',', 'routing_key', '=', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "CELERY_ACCEPT_CONTENT = [ , ] \n"
Original    (006): ['CELERY_ACCEPT_CONTENT', '=', '[', ',', ']', '\\n']
Tokenized   (017): ['<s>', 'C', 'EL', 'ERY', '_', 'AC', 'CEPT', '_', 'CONT', 'ENT', '=', '[', ',', ']', '\\', 'n', '</s>']
Filtered   (015): ['C', 'EL', 'ERY', '_', 'AC', 'CEPT', '_', 'CONT', 'ENT', '=', '[', ',', ']', '\\', 'n']
Detokenized (006): ['CELERY_ACCEPT_CONTENT', '=', '[', ',', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "CELERY_IMPORTS = ( "core_tasks" , ) \n"
Original    (007): ['CELERY_IMPORTS', '=', '(', '"core_tasks"', ',', ')', '\\n']
Tokenized   (021): ['<s>', 'C', 'EL', 'ERY', '_', 'IM', 'P', 'ORTS', '=', '(', '"', 'core', '_', 't', 'asks', '"', ',', ')', '\\', 'n', '</s>']
Filtered   (019): ['C', 'EL', 'ERY', '_', 'IM', 'P', 'ORTS', '=', '(', '"', 'core', '_', 't', 'asks', '"', ',', ')', '\\', 'n']
Detokenized (007): ['CELERY_IMPORTS', '=', '(', '"core_tasks"', ',', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "sampledir = os . path . join ( os . path . split ( __file__ ) [ 0 ] , "../../../extras/sample_provider_pages/" ) \n"
Original    (023): ['sampledir', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'split', '(', '__file__', ')', '[', '0', ']', ',', '"../../../extras/sample_provider_pages/"', ')', '\\n']
Tokenized   (043): ['<s>', 'sam', 'pled', 'ir', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'split', '(', '__', 'file', '__', ')', '[', '0', ']', ',', '"', '../', '../', '../', 'ext', 'ras', '/', 'sample', '_', 'prov', 'ider', '_', 'pages', '/"', ')', '\\', 'n', '</s>']
Filtered   (041): ['sam', 'pled', 'ir', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'split', '(', '__', 'file', '__', ')', '[', '0', ']', ',', '"', '../', '../', '../', 'ext', 'ras', '/', 'sample', '_', 'prov', 'ider', '_', 'pages', '/"', ')', '\\', 'n']
Detokenized (023): ['sampledir', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'split', '(', '__file__', ')', '[', '0', ']', ',', '"../../../extras/sample_provider_pages/"', ')', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "TEST_XML = open ( os . path . join ( sampledir , "facebook" , "metrics" ) ) . read ( ) \n"
Original    (022): ['TEST_XML', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'sampledir', ',', '"facebook"', ',', '"metrics"', ')', ')', '.', 'read', '(', ')', '\\n']
Tokenized   (035): ['<s>', 'T', 'EST', '_', 'X', 'ML', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'sampled', 'ir', ',', '"', 'facebook', '"', ',', '"', 'met', 'rics', '"', ')', ')', '.', 'read', '(', ')', '\\', 'n', '</s>']
Filtered   (033): ['T', 'EST', '_', 'X', 'ML', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'sampled', 'ir', ',', '"', 'facebook', '"', ',', '"', 'met', 'rics', '"', ')', ')', '.', 'read', '(', ')', '\\', 'n']
Detokenized (022): ['TEST_XML', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'sampledir', ',', '"facebook"', ',', '"metrics"', ')', ')', '.', 'read', '(', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "provider_names = [ provider . __class__ . __name__ for provider in providers ] \n"
Original    (014): ['provider_names', '=', '[', 'provider', '.', '__class__', '.', '__name__', 'for', 'provider', 'in', 'providers', ']', '\\n']
Tokenized   (024): ['<s>', 'prov', 'ider', '_', 'names', '=', '[', 'provider', '.', '__', 'class', '__', '.', '__', 'name', '__', 'for', 'provider', 'in', 'providers', ']', '\\', 'n', '</s>']
Filtered   (022): ['prov', 'ider', '_', 'names', '=', '[', 'provider', '.', '__', 'class', '__', '.', '__', 'name', '__', 'for', 'provider', 'in', 'providers', ']', '\\', 'n']
Detokenized (014): ['provider_names', '=', '[', 'provider', '.', '__class__', '.', '__name__', 'for', 'provider', 'in', 'providers', ']', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "assert_equals ( md [ "pubmed" ] [ ] , ) \n"
Original    (011): ['assert_equals', '(', 'md', '[', '"pubmed"', ']', '[', ']', ',', ')', '\\n']
Tokenized   (020): ['<s>', 'assert', '_', 'equ', 'als', '(', 'md', '[', '"', 'pub', 'med', '"', ']', '[', ']', ',', ')', '\\', 'n', '</s>']
Filtered   (018): ['assert', '_', 'equ', 'als', '(', 'md', '[', '"', 'pub', 'med', '"', ']', '[', ']', ',', ')', '\\', 'n']
Detokenized (011): ['assert_equals', '(', 'md', '[', '"pubmed"', ']', '[', ']', ',', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "tiid = db . Column ( db . Text , db . ForeignKey ( ) , primary_key = True ) \n"
Original    (021): ['tiid', '=', 'db', '.', 'Column', '(', 'db', '.', 'Text', ',', 'db', '.', 'ForeignKey', '(', ')', ',', 'primary_key', '=', 'True', ')', '\\n']
Tokenized   (028): ['<s>', 'ti', 'id', '=', 'db', '.', 'Column', '(', 'db', '.', 'Text', ',', 'db', '.', 'Foreign', 'Key', '(', ')', ',', 'primary', '_', 'key', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (026): ['ti', 'id', '=', 'db', '.', 'Column', '(', 'db', '.', 'Text', ',', 'db', '.', 'Foreign', 'Key', '(', ')', ',', 'primary', '_', 'key', '=', 'True', ')', '\\', 'n']
Detokenized (021): ['tiid', '=', 'db', '.', 'Column', '(', 'db', '.', 'Text', ',', 'db', '.', 'ForeignKey', '(', ')', ',', 'primary_key', '=', 'True', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "tweet_ids_with_response = [ tweet [ "id_str" ] for tweet in data ] \n"
Original    (013): ['tweet_ids_with_response', '=', '[', 'tweet', '[', '"id_str"', ']', 'for', 'tweet', 'in', 'data', ']', '\\n']
Tokenized   (027): ['<s>', 't', 'weet', '_', 'ids', '_', 'with', '_', 'response', '=', '[', 'tweet', '[', '"', 'id', '_', 'str', '"', ']', 'for', 'tweet', 'in', 'data', ']', '\\', 'n', '</s>']
Filtered   (025): ['t', 'weet', '_', 'ids', '_', 'with', '_', 'response', '=', '[', 'tweet', '[', '"', 'id', '_', 'str', '"', ']', 'for', 'tweet', 'in', 'data', ']', '\\', 'n']
Detokenized (013): ['tweet_ids_with_response', '=', '[', 'tweet', '[', '"id_str"', ']', 'for', 'tweet', 'in', 'data', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "tweet_ids_without_response = [ tweet for tweet in tweet_ids if tweet not in tweet_ids_with_response flag_deleted_tweets ( tweet_ids_without_response ) \n"
Original    (018): ['tweet_ids_without_response', '=', '[', 'tweet', 'for', 'tweet', 'in', 'tweet_ids', 'if', 'tweet', 'not', 'in', 'tweet_ids_with_response', 'flag_deleted_tweets', '(', 'tweet_ids_without_response', ')', '\\n']
Tokenized   (049): ['<s>', 't', 'weet', '_', 'ids', '_', 'without', '_', 'response', '=', '[', 'tweet', 'for', 'tweet', 'in', 'tweet', '_', 'ids', 'if', 'tweet', 'not', 'in', 'tweet', '_', 'ids', '_', 'with', '_', 'response', 'flag', '_', 'de', 'leted', '_', 't', 'we', 'ets', '(', 'tweet', '_', 'ids', '_', 'without', '_', 'response', ')', '\\', 'n', '</s>']
Filtered   (047): ['t', 'weet', '_', 'ids', '_', 'without', '_', 'response', '=', '[', 'tweet', 'for', 'tweet', 'in', 'tweet', '_', 'ids', 'if', 'tweet', 'not', 'in', 'tweet', '_', 'ids', '_', 'with', '_', 'response', 'flag', '_', 'de', 'leted', '_', 't', 'we', 'ets', '(', 'tweet', '_', 'ids', '_', 'without', '_', 'response', ')', '\\', 'n']
Detokenized (018): ['tweet_ids_without_response', '=', '[', 'tweet', 'for', 'tweet', 'in', 'tweet_ids', 'if', 'tweet', 'not', 'in', 'tweet_ids_with_response', 'flag_deleted_tweets', '(', 'tweet_ids_without_response', ')', '\\n']
Counter: 47
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "access_token = os . getenv ( "TWITTER_ACCESS_TOKEN" ) \n"
Original    (009): ['access_token', '=', 'os', '.', 'getenv', '(', '"TWITTER_ACCESS_TOKEN"', ')', '\\n']
Tokenized   (025): ['<s>', 'access', '_', 'token', '=', 'os', '.', 'get', 'env', '(', '"', 'TW', 'IT', 'TER', '_', 'ACC', 'ESS', '_', 'TO', 'KEN', '"', ')', '\\', 'n', '</s>']
Filtered   (023): ['access', '_', 'token', '=', 'os', '.', 'get', 'env', '(', '"', 'TW', 'IT', 'TER', '_', 'ACC', 'ESS', '_', 'TO', 'KEN', '"', ')', '\\', 'n']
Detokenized (009): ['access_token', '=', 'os', '.', 'getenv', '(', '"TWITTER_ACCESS_TOKEN"', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "num = len ( tweets ) ) ) \n"
Original    (009): ['num', '=', 'len', '(', 'tweets', ')', ')', ')', '\\n']
Tokenized   (012): ['<s>', 'num', '=', 'len', '(', 'tweets', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (010): ['num', '=', 'len', '(', 'tweets', ')', ')', ')', '\\', 'n']
Detokenized (009): ['num', '=', 'len', '(', 'tweets', ')', ')', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "list_of_groups = [ tweets [ i : i + group_size ] for i in range ( 0 , len ( tweets ) , group_size ) ] \n"
Original    (027): ['list_of_groups', '=', '[', 'tweets', '[', 'i', ':', 'i', '+', 'group_size', ']', 'for', 'i', 'in', 'range', '(', '0', ',', 'len', '(', 'tweets', ')', ',', 'group_size', ')', ']', '\\n']
Tokenized   (038): ['<s>', 'list', '_', 'of', '_', 'groups', '=', '[', 'tweets', '[', 'i', ':', 'i', '+', 'group', '_', 'size', ']', 'for', 'i', 'in', 'range', '(', '0', ',', 'len', '(', 'tweets', ')', ',', 'group', '_', 'size', ')', ']', '\\', 'n', '</s>']
Filtered   (036): ['list', '_', 'of', '_', 'groups', '=', '[', 'tweets', '[', 'i', ':', 'i', '+', 'group', '_', 'size', ']', 'for', 'i', 'in', 'range', '(', '0', ',', 'len', '(', 'tweets', ')', ',', 'group', '_', 'size', ')', ']', '\\', 'n']
Detokenized (027): ['list_of_groups', '=', '[', 'tweets', '[', 'i', ':', 'i', '+', 'group_size', ']', 'for', 'i', 'in', 'range', '(', '0', ',', 'len', '(', 'tweets', ')', ',', 'group_size', ')', ']', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "handle_all_tweets ( response . data , tweet_subset ) \n"
Original    (009): ['handle_all_tweets', '(', 'response', '.', 'data', ',', 'tweet_subset', ')', '\\n']
Tokenized   (021): ['<s>', 'handle', '_', 'all', '_', 't', 'we', 'ets', '(', 'response', '.', 'data', ',', 'tweet', '_', 'sub', 'set', ')', '\\', 'n', '</s>']
Filtered   (019): ['handle', '_', 'all', '_', 't', 'we', 'ets', '(', 'response', '.', 'data', ',', 'tweet', '_', 'sub', 'set', ')', '\\', 'n']
Detokenized (009): ['handle_all_tweets', '(', 'response', '.', 'data', ',', 'tweet_subset', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "tweets = Tweet . query . filter ( Tweet . profile_id == profile_id ) \n"
Original    (015): ['tweets', '=', 'Tweet', '.', 'query', '.', 'filter', '(', 'Tweet', '.', 'profile_id', '==', 'profile_id', ')', '\\n']
Tokenized   (024): ['<s>', 't', 'we', 'ets', '=', 'Tweet', '.', 'query', '.', 'filter', '(', 'Tweet', '.', 'profile', '_', 'id', '==', 'profile', '_', 'id', ')', '\\', 'n', '</s>']
Filtered   (022): ['t', 'we', 'ets', '=', 'Tweet', '.', 'query', '.', 'filter', '(', 'Tweet', '.', 'profile', '_', 'id', '==', 'profile', '_', 'id', ')', '\\', 'n']
Detokenized (015): ['tweets', '=', 'Tweet', '.', 'query', '.', 'filter', '(', 'Tweet', '.', 'profile_id', '==', 'profile_id', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "tweet_dict = dict ( [ ( ( tweet . tweet_id , tweet . tiid ) , tweet ) for tweet in tweets ] ) \n"
Original    (025): ['tweet_dict', '=', 'dict', '(', '[', '(', '(', 'tweet', '.', 'tweet_id', ',', 'tweet', '.', 'tiid', ')', ',', 'tweet', ')', 'for', 'tweet', 'in', 'tweets', ']', ')', '\\n']
Tokenized   (034): ['<s>', 't', 'weet', '_', 'dict', '=', 'dict', '(', '[', '(', '(', 'tweet', '.', 'tweet', '_', 'id', ',', 'tweet', '.', 'ti', 'id', ')', ',', 'tweet', ')', 'for', 'tweet', 'in', 'tweets', ']', ')', '\\', 'n', '</s>']
Filtered   (032): ['t', 'weet', '_', 'dict', '=', 'dict', '(', '[', '(', '(', 'tweet', '.', 'tweet', '_', 'id', ',', 'tweet', '.', 'ti', 'id', ')', ',', 'tweet', ')', 'for', 'tweet', 'in', 'tweets', ']', ')', '\\', 'n']
Detokenized (025): ['tweet_dict', '=', 'dict', '(', '[', '(', '(', 'tweet', '.', 'tweet_id', ',', 'tweet', '.', 'tiid', ')', ',', 'tweet', ')', 'for', 'tweet', 'in', 'tweets', ']', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "tweet . profile_id = profile_id \n"
Original    (006): ['tweet', '.', 'profile_id', '=', 'profile_id', '\\n']
Tokenized   (014): ['<s>', 't', 'weet', '.', 'profile', '_', 'id', '=', 'profile', '_', 'id', '\\', 'n', '</s>']
Filtered   (012): ['t', 'weet', '.', 'profile', '_', 'id', '=', 'profile', '_', 'id', '\\', 'n']
Detokenized (006): ['tweet', '.', 'profile_id', '=', 'profile_id', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "tweet_ids = [ tweet . tweet_id for tweet in tweets_to_hydrate_from_twitter ] \n"
Original    (012): ['tweet_ids', '=', '[', 'tweet', '.', 'tweet_id', 'for', 'tweet', 'in', 'tweets_to_hydrate_from_twitter', ']', '\\n']
Tokenized   (029): ['<s>', 't', 'weet', '_', 'ids', '=', '[', 'tweet', '.', 'tweet', '_', 'id', 'for', 'tweet', 'in', 'tweets', '_', 'to', '_', 'hyd', 'rate', '_', 'from', '_', 'twitter', ']', '\\', 'n', '</s>']
Filtered   (027): ['t', 'weet', '_', 'ids', '=', '[', 'tweet', '.', 'tweet', '_', 'id', 'for', 'tweet', 'in', 'tweets', '_', 'to', '_', 'hyd', 'rate', '_', 'from', '_', 'twitter', ']', '\\', 'n']
Detokenized (012): ['tweet_ids', '=', '[', 'tweet', '.', 'tweet_id', 'for', 'tweet', 'in', 'tweets_to_hydrate_from_twitter', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "display_url = url_info [ "display_url" ] \n"
Original    (007): ['display_url', '=', 'url_info', '[', '"display_url"', ']', '\\n']
Tokenized   (018): ['<s>', 'display', '_', 'url', '=', 'url', '_', 'info', '[', '"', 'display', '_', 'url', '"', ']', '\\', 'n', '</s>']
Filtered   (016): ['display', '_', 'url', '=', 'url', '_', 'info', '[', '"', 'display', '_', 'url', '"', ']', '\\', 'n']
Detokenized (007): ['display_url', '=', 'url_info', '[', '"display_url"', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "tweet_id = self . tweet_id , \n"
Original    (007): ['tweet_id', '=', 'self', '.', 'tweet_id', ',', '\\n']
Tokenized   (015): ['<s>', 't', 'weet', '_', 'id', '=', 'self', '.', 'tweet', '_', 'id', ',', '\\', 'n', '</s>']
Filtered   (013): ['t', 'weet', '_', 'id', '=', 'self', '.', 'tweet', '_', 'id', ',', '\\', 'n']
Detokenized (007): ['tweet_id', '=', 'self', '.', 'tweet_id', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "file_loc = os . path . dirname ( os . path . realpath ( __file__ ) ) \n"
Original    (018): ['file_loc', '=', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'realpath', '(', '__file__', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'file', '_', 'loc', '=', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'real', 'path', '(', '__', 'file', '__', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['file', '_', 'loc', '=', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'real', 'path', '(', '__', 'file', '__', ')', ')', '\\', 'n']
Detokenized (018): ['file_loc', '=', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'realpath', '(', '__file__', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "urllib . urlretrieve ( url + fname , fname ) \n"
Original    (011): ['urllib', '.', 'urlretrieve', '(', 'url', '+', 'fname', ',', 'fname', ')', '\\n']
Tokenized   (020): ['<s>', 'ur', 'll', 'ib', '.', 'url', 'ret', 'rieve', '(', 'url', '+', 'f', 'name', ',', 'f', 'name', ')', '\\', 'n', '</s>']
Filtered   (018): ['ur', 'll', 'ib', '.', 'url', 'ret', 'rieve', '(', 'url', '+', 'f', 'name', ',', 'f', 'name', ')', '\\', 'n']
Detokenized (011): ['urllib', '.', 'urlretrieve', '(', 'url', '+', 'fname', ',', 'fname', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "loaded = np . fromstring ( fd . read ( ) , dtype = np . uint8 ) \n"
Original    (019): ['loaded', '=', 'np', '.', 'fromstring', '(', 'fd', '.', 'read', '(', ')', ',', 'dtype', '=', 'np', '.', 'uint8', ')', '\\n']
Tokenized   (026): ['<s>', 'loaded', '=', 'np', '.', 'from', 'string', '(', 'f', 'd', '.', 'read', '(', ')', ',', 'd', 'type', '=', 'np', '.', 'uint', '8', ')', '\\', 'n', '</s>']
Filtered   (024): ['loaded', '=', 'np', '.', 'from', 'string', '(', 'f', 'd', '.', 'read', '(', ')', ',', 'd', 'type', '=', 'np', '.', 'uint', '8', ')', '\\', 'n']
Detokenized (019): ['loaded', '=', 'np', '.', 'fromstring', '(', 'fd', '.', 'read', '(', ')', ',', 'dtype', '=', 'np', '.', 'uint8', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "fd = gzip . open ( os . path . join ( data_dir , ) ) \n"
Original    (017): ['fd', '=', 'gzip', '.', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'data_dir', ',', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'fd', '=', 'g', 'zip', '.', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'data', '_', 'dir', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['fd', '=', 'g', 'zip', '.', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'data', '_', 'dir', ',', ')', ')', '\\', 'n']
Detokenized (017): ['fd', '=', 'gzip', '.', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'data_dir', ',', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "trY = loaded [ 8 : ] . reshape ( ( 60000 ) ) \n"
Original    (015): ['trY', '=', 'loaded', '[', '8', ':', ']', '.', 'reshape', '(', '(', '60000', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'tr', 'Y', '=', 'loaded', '[', '8', ':', ']', '.', 'resh', 'ape', '(', '(', '6', '0000', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['tr', 'Y', '=', 'loaded', '[', '8', ':', ']', '.', 'resh', 'ape', '(', '(', '6', '0000', ')', ')', '\\', 'n']
Detokenized (015): ['trY', '=', 'loaded', '[', '8', ':', ']', '.', 'reshape', '(', '(', '60000', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "trX = trX . reshape ( - 1 , 28 , 28 ) \n"
Original    (014): ['trX', '=', 'trX', '.', 'reshape', '(', '-', '1', ',', '28', ',', '28', ')', '\\n']
Tokenized   (020): ['<s>', 'tr', 'X', '=', 'tr', 'X', '.', 'resh', 'ape', '(', '-', '1', ',', '28', ',', '28', ')', '\\', 'n', '</s>']
Filtered   (018): ['tr', 'X', '=', 'tr', 'X', '.', 'resh', 'ape', '(', '-', '1', ',', '28', ',', '28', ')', '\\', 'n']
Detokenized (014): ['trX', '=', 'trX', '.', 'reshape', '(', '-', '1', ',', '28', ',', '28', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "dirpath = os . path . join ( self . repo . path , "unused_directory" ) \n"
Original    (017): ['dirpath', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"unused_directory"', ')', '\\n']
Tokenized   (026): ['<s>', 'dir', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"', 'un', 'used', '_', 'directory', '"', ')', '\\', 'n', '</s>']
Filtered   (024): ['dir', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"', 'un', 'used', '_', 'directory', '"', ')', '\\', 'n']
Detokenized (017): ['dirpath', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"unused_directory"', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "subpath = os . path . join ( self . repo . path , "a" , "b" , "c" ) \n"
Original    (021): ['subpath', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"a"', ',', '"b"', ',', '"c"', ')', '\\n']
Tokenized   (031): ['<s>', 'sub', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"', 'a', '"', ',', '"', 'b', '"', ',', '"', 'c', '"', ')', '\\', 'n', '</s>']
Filtered   (029): ['sub', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"', 'a', '"', ',', '"', 'b', '"', ',', '"', 'c', '"', ')', '\\', 'n']
Detokenized (021): ['subpath', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"a"', ',', '"b"', ',', '"c"', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "handle = self . profile . username , \n"
Original    (009): ['handle', '=', 'self', '.', 'profile', '.', 'username', ',', '\\n']
Tokenized   (012): ['<s>', 'handle', '=', 'self', '.', 'profile', '.', 'username', ',', '\\', 'n', '</s>']
Filtered   (010): ['handle', '=', 'self', '.', 'profile', '.', 'username', ',', '\\', 'n']
Detokenized (009): ['handle', '=', 'self', '.', 'profile', '.', 'username', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "id_key = ) \n"
Original    (004): ['id_key', '=', ')', '\\n']
Tokenized   (009): ['<s>', 'id', '_', 'key', '=', ')', '\\', 'n', '</s>']
Filtered   (007): ['id', '_', 'key', '=', ')', '\\', 'n']
Detokenized (004): ['id_key', '=', ')', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "message_thread = model . MessageThread ( okc_id = self . thread . id , \n"
Original    (015): ['message_thread', '=', 'model', '.', 'MessageThread', '(', 'okc_id', '=', 'self', '.', 'thread', '.', 'id', ',', '\\n']
Tokenized   (024): ['<s>', 'message', '_', 'thread', '=', 'model', '.', 'Message', 'Thread', '(', 'ok', 'c', '_', 'id', '=', 'self', '.', 'thread', '.', 'id', ',', '\\', 'n', '</s>']
Filtered   (022): ['message', '_', 'thread', '=', 'model', '.', 'Message', 'Thread', '(', 'ok', 'c', '_', 'id', '=', 'self', '.', 'thread', '.', 'id', ',', '\\', 'n']
Detokenized (015): ['message_thread', '=', 'model', '.', 'MessageThread', '(', 'okc_id', '=', 'self', '.', 'thread', '.', 'id', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "new_messages = [ message for message in self . thread . messages \n"
Original    (013): ['new_messages', '=', '[', 'message', 'for', 'message', 'in', 'self', '.', 'thread', '.', 'messages', '\\n']
Tokenized   (019): ['<s>', 'new', '_', 'mess', 'ages', '=', '[', 'message', 'for', 'message', 'in', 'self', '.', 'thread', '.', 'messages', '\\', 'n', '</s>']
Filtered   (017): ['new', '_', 'mess', 'ages', '=', '[', 'message', 'for', 'message', 'in', 'self', '.', 'thread', '.', 'messages', '\\', 'n']
Detokenized (013): ['new_messages', '=', '[', 'message', 'for', 'message', 'in', 'self', '.', 'thread', '.', 'messages', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "new_message_model = model . Message ( okc_id = new_message . id , \n"
Original    (013): ['new_message_model', '=', 'model', '.', 'Message', '(', 'okc_id', '=', 'new_message', '.', 'id', ',', '\\n']
Tokenized   (025): ['<s>', 'new', '_', 'message', '_', 'model', '=', 'model', '.', 'Message', '(', 'ok', 'c', '_', 'id', '=', 'new', '_', 'message', '.', 'id', ',', '\\', 'n', '</s>']
Filtered   (023): ['new', '_', 'message', '_', 'model', '=', 'model', '.', 'Message', '(', 'ok', 'c', '_', 'id', '=', 'new', '_', 'message', '.', 'id', ',', '\\', 'n']
Detokenized (013): ['new_message_model', '=', 'model', '.', 'Message', '(', 'okc_id', '=', 'new_message', '.', 'id', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "time_sent = new_message . time_sent ) \n"
Original    (007): ['time_sent', '=', 'new_message', '.', 'time_sent', ')', '\\n']
Tokenized   (016): ['<s>', 'time', '_', 'sent', '=', 'new', '_', 'message', '.', 'time', '_', 'sent', ')', '\\', 'n', '</s>']
Filtered   (014): ['time', '_', 'sent', '=', 'new', '_', 'message', '.', 'time', '_', 'sent', ')', '\\', 'n']
Detokenized (007): ['time_sent', '=', 'new_message', '.', 'time_sent', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "mailbox . Sync ( user ) . all ( ) \n"
Original    (011): ['mailbox', '.', 'Sync', '(', 'user', ')', '.', 'all', '(', ')', '\\n']
Tokenized   (015): ['<s>', 'mail', 'box', '.', 'Sync', '(', 'user', ')', '.', 'all', '(', ')', '\\', 'n', '</s>']
Filtered   (013): ['mail', 'box', '.', 'Sync', '(', 'user', ')', '.', 'all', '(', ')', '\\', 'n']
Detokenized (011): ['mailbox', '.', 'Sync', '(', 'user', ')', '.', 'all', '(', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "user_model . upsert_model ( id_key = ) \n"
Original    (008): ['user_model', '.', 'upsert_model', '(', 'id_key', '=', ')', '\\n']
Tokenized   (018): ['<s>', 'user', '_', 'model', '.', 'ups', 'ert', '_', 'model', '(', 'id', '_', 'key', '=', ')', '\\', 'n', '</s>']
Filtered   (016): ['user', '_', 'model', '.', 'ups', 'ert', '_', 'model', '(', 'id', '_', 'key', '=', ')', '\\', 'n']
Detokenized (008): ['user_model', '.', 'upsert_model', '(', 'id_key', '=', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "response_dict = user . photo . upload_and_confirm ( user . quickmatch ( ) . photo_infos [ 0 ] ) \n"
Original    (020): ['response_dict', '=', 'user', '.', 'photo', '.', 'upload_and_confirm', '(', 'user', '.', 'quickmatch', '(', ')', '.', 'photo_infos', '[', '0', ']', ')', '\\n']
Tokenized   (034): ['<s>', 'response', '_', 'dict', '=', 'user', '.', 'photo', '.', 'upload', '_', 'and', '_', 'conf', 'irm', '(', 'user', '.', 'quick', 'match', '(', ')', '.', 'photo', '_', 'inf', 'os', '[', '0', ']', ')', '\\', 'n', '</s>']
Filtered   (032): ['response', '_', 'dict', '=', 'user', '.', 'photo', '.', 'upload', '_', 'and', '_', 'conf', 'irm', '(', 'user', '.', 'quick', 'match', '(', ')', '.', 'photo', '_', 'inf', 'os', '[', '0', ']', ')', '\\', 'n']
Detokenized (020): ['response_dict', '=', 'user', '.', 'photo', '.', 'upload_and_confirm', '(', 'user', '.', 'quickmatch', '(', ')', '.', 'photo_infos', '[', '0', ']', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "vcr_live_sleep ( 2 ) \n"
Original    (005): ['vcr_live_sleep', '(', '2', ')', '\\n']
Tokenized   (013): ['<s>', 'v', 'cr', '_', 'live', '_', 'sleep', '(', '2', ')', '\\', 'n', '</s>']
Filtered   (011): ['v', 'cr', '_', 'live', '_', 'sleep', '(', '2', ')', '\\', 'n']
Detokenized (005): ['vcr_live_sleep', '(', '2', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "b2_h = shared_zeros ( ( self . hp . batch_size , n_h ) ) \n"
Original    (015): ['b2_h', '=', 'shared_zeros', '(', '(', 'self', '.', 'hp', '.', 'batch_size', ',', 'n_h', ')', ')', '\\n']
Tokenized   (028): ['<s>', 'b', '2', '_', 'h', '=', 'shared', '_', 'zer', 'os', '(', '(', 'self', '.', 'hp', '.', 'batch', '_', 'size', ',', 'n', '_', 'h', ')', ')', '\\', 'n', '</s>']
Filtered   (026): ['b', '2', '_', 'h', '=', 'shared', '_', 'zer', 'os', '(', '(', 'self', '.', 'hp', '.', 'batch', '_', 'size', ',', 'n', '_', 'h', ')', ')', '\\', 'n']
Detokenized (015): ['b2_h', '=', 'shared_zeros', '(', '(', 'self', '.', 'hp', '.', 'batch_size', ',', 'n_h', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "W1 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n"
Original    (019): ['W1', '=', 'shared_normal', '(', '(', 'n_h', ',', 'n_h', '*', 'gates', ')', ',', 'scale', '=', 'scale', '*', '1.5', ')', '\\n']
Tokenized   (031): ['<s>', 'W', '1', '=', 'shared', '_', 'normal', '(', '(', 'n', '_', 'h', ',', 'n', '_', 'h', '*', 'gates', ')', ',', 'scale', '=', 'scale', '*', '1', '.', '5', ')', '\\', 'n', '</s>']
Filtered   (029): ['W', '1', '=', 'shared', '_', 'normal', '(', '(', 'n', '_', 'h', ',', 'n', '_', 'h', '*', 'gates', ')', ',', 'scale', '=', 'scale', '*', '1', '.', '5', ')', '\\', 'n']
Detokenized (019): ['W1', '=', 'shared_normal', '(', '(', 'n_h', ',', 'n_h', '*', 'gates', ')', ',', 'scale', '=', 'scale', '*', '1.5', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "b1 = shared_zeros ( ( n_h * gates ) ) \n"
Original    (011): ['b1', '=', 'shared_zeros', '(', '(', 'n_h', '*', 'gates', ')', ')', '\\n']
Tokenized   (020): ['<s>', 'b', '1', '=', 'shared', '_', 'zer', 'os', '(', '(', 'n', '_', 'h', '*', 'gates', ')', ')', '\\', 'n', '</s>']
Filtered   (018): ['b', '1', '=', 'shared', '_', 'zer', 'os', '(', '(', 'n', '_', 'h', '*', 'gates', ')', ')', '\\', 'n']
Detokenized (011): ['b1', '=', 'shared_zeros', '(', '(', 'n_h', '*', 'gates', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "b2 = shared_zeros ( ( n_h * gates , ) ) \n"
Original    (012): ['b2', '=', 'shared_zeros', '(', '(', 'n_h', '*', 'gates', ',', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'b', '2', '=', 'shared', '_', 'zer', 'os', '(', '(', 'n', '_', 'h', '*', 'gates', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['b', '2', '=', 'shared', '_', 'zer', 'os', '(', '(', 'n', '_', 'h', '*', 'gates', ',', ')', ')', '\\', 'n']
Detokenized (012): ['b2', '=', 'shared_zeros', '(', '(', 'n_h', '*', 'gates', ',', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "i_on = T . nnet . sigmoid ( g_on [ : , : n_h ] ) \n"
Original    (017): ['i_on', '=', 'T', '.', 'nnet', '.', 'sigmoid', '(', 'g_on', '[', ':', ',', ':', 'n_h', ']', ')', '\\n']
Tokenized   (029): ['<s>', 'i', '_', 'on', '=', 'T', '.', 'n', 'net', '.', 's', 'igm', 'oid', '(', 'g', '_', 'on', '[', ':', ',', ':', 'n', '_', 'h', ']', ')', '\\', 'n', '</s>']
Filtered   (027): ['i', '_', 'on', '=', 'T', '.', 'n', 'net', '.', 's', 'igm', 'oid', '(', 'g', '_', 'on', '[', ':', ',', ':', 'n', '_', 'h', ']', ')', '\\', 'n']
Detokenized (017): ['i_on', '=', 'T', '.', 'nnet', '.', 'sigmoid', '(', 'g_on', '[', ':', ',', ':', 'n_h', ']', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "f_on = T . nnet . sigmoid ( g_on [ : , n_h : 2 * n_h ] ) \n"
Original    (020): ['f_on', '=', 'T', '.', 'nnet', '.', 'sigmoid', '(', 'g_on', '[', ':', ',', 'n_h', ':', '2', '*', 'n_h', ']', ')', '\\n']
Tokenized   (034): ['<s>', 'f', '_', 'on', '=', 'T', '.', 'n', 'net', '.', 's', 'igm', 'oid', '(', 'g', '_', 'on', '[', ':', ',', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '\\', 'n', '</s>']
Filtered   (032): ['f', '_', 'on', '=', 'T', '.', 'n', 'net', '.', 's', 'igm', 'oid', '(', 'g', '_', 'on', '[', ':', ',', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '\\', 'n']
Detokenized (020): ['f_on', '=', 'T', '.', 'nnet', '.', 'sigmoid', '(', 'g_on', '[', ':', ',', 'n_h', ':', '2', '*', 'n_h', ']', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "o_on = T . nnet . sigmoid ( g_on [ : , 2 * n_h : 3 * n_h ] ) \n"
Original    (022): ['o_on', '=', 'T', '.', 'nnet', '.', 'sigmoid', '(', 'g_on', '[', ':', ',', '2', '*', 'n_h', ':', '3', '*', 'n_h', ']', ')', '\\n']
Tokenized   (036): ['<s>', 'o', '_', 'on', '=', 'T', '.', 'n', 'net', '.', 's', 'igm', 'oid', '(', 'g', '_', 'on', '[', ':', ',', '2', '*', 'n', '_', 'h', ':', '3', '*', 'n', '_', 'h', ']', ')', '\\', 'n', '</s>']
Filtered   (034): ['o', '_', 'on', '=', 'T', '.', 'n', 'net', '.', 's', 'igm', 'oid', '(', 'g', '_', 'on', '[', ':', ',', '2', '*', 'n', '_', 'h', ':', '3', '*', 'n', '_', 'h', ']', ')', '\\', 'n']
Detokenized (022): ['o_on', '=', 'T', '.', 'nnet', '.', 'sigmoid', '(', 'g_on', '[', ':', ',', '2', '*', 'n_h', ':', '3', '*', 'n_h', ']', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "h_t = T . tanh ( T . dot ( X , W [ : , 1 * n_h : 2 * n_h ] ) + T . dot ( h , U [ : , 1 * n_h : 2 * n_h ] ) + b [ 1 * n_h : 2 * n_h ] ) \n"
Original    (058): ['h_t', '=', 'T', '.', 'tanh', '(', 'T', '.', 'dot', '(', 'X', ',', 'W', '[', ':', ',', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '+', 'T', '.', 'dot', '(', 'h', ',', 'U', '[', ':', ',', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '+', 'b', '[', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '\\n']
Tokenized   (076): ['<s>', 'h', '_', 't', '=', 'T', '.', 'tan', 'h', '(', 'T', '.', 'dot', '(', 'X', ',', 'W', '[', ':', ',', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '+', 'T', '.', 'dot', '(', 'h', ',', 'U', '[', ':', ',', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '+', 'b', '[', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '\\', 'n', '</s>']
Filtered   (074): ['h', '_', 't', '=', 'T', '.', 'tan', 'h', '(', 'T', '.', 'dot', '(', 'X', ',', 'W', '[', ':', ',', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '+', 'T', '.', 'dot', '(', 'h', ',', 'U', '[', ':', ',', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '+', 'b', '[', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '\\', 'n']
Detokenized (058): ['h_t', '=', 'T', '.', 'tanh', '(', 'T', '.', 'dot', '(', 'X', ',', 'W', '[', ':', ',', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '+', 'T', '.', 'dot', '(', 'h', ',', 'U', '[', ':', ',', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '+', 'b', '[', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '\\n']
Counter: 74
===================================================================
Hidden states:  (13, 58, 768)
# Extracted words:  58
Sentence         : "te_cost , te_h_updates = model ( self . X , self . params , 0. ) \n"
Original    (017): ['te_cost', ',', 'te_h_updates', '=', 'model', '(', 'self', '.', 'X', ',', 'self', '.', 'params', ',', '0.', ')', '\\n']
Tokenized   (028): ['<s>', 'te', '_', 'cost', ',', 'te', '_', 'h', '_', 'up', 'dates', '=', 'model', '(', 'self', '.', 'X', ',', 'self', '.', 'params', ',', '0', '.', ')', '\\', 'n', '</s>']
Filtered   (026): ['te', '_', 'cost', ',', 'te', '_', 'h', '_', 'up', 'dates', '=', 'model', '(', 'self', '.', 'X', ',', 'self', '.', 'params', ',', '0', '.', ')', '\\', 'n']
Detokenized (017): ['te_cost', ',', 'te_h_updates', '=', 'model', '(', 'self', '.', 'X', ',', 'self', '.', 'params', ',', '0.', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "csvWriter = csv . writer ( sys . stdout , delimiter = separator , quotechar = quote , \n"
Original    (019): ['csvWriter', '=', 'csv', '.', 'writer', '(', 'sys', '.', 'stdout', ',', 'delimiter', '=', 'separator', ',', 'quotechar', '=', 'quote', ',', '\\n']
Tokenized   (028): ['<s>', 'csv', 'Writer', '=', 'c', 'sv', '.', 'writer', '(', 'sys', '.', 'std', 'out', ',', 'delim', 'iter', '=', 'separ', 'ator', ',', 'quote', 'char', '=', 'quote', ',', '\\', 'n', '</s>']
Filtered   (026): ['csv', 'Writer', '=', 'c', 'sv', '.', 'writer', '(', 'sys', '.', 'std', 'out', ',', 'delim', 'iter', '=', 'separ', 'ator', ',', 'quote', 'char', '=', 'quote', ',', '\\', 'n']
Detokenized (019): ['csvWriter', '=', 'csv', '.', 'writer', '(', 'sys', '.', 'stdout', ',', 'delimiter', '=', 'separator', ',', 'quotechar', '=', 'quote', ',', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "IColumnProvider_Methods = IPersist_Methods + [ "Initialize" , "GetColumnInfo" , "GetItemData" ] \n"
Original    (012): ['IColumnProvider_Methods', '=', 'IPersist_Methods', '+', '[', '"Initialize"', ',', '"GetColumnInfo"', ',', '"GetItemData"', ']', '\\n']
Tokenized   (035): ['<s>', 'IC', 'ol', 'umn', 'Provider', '_', 'Methods', '=', 'IP', 'ers', 'ist', '_', 'Methods', '+', '[', '"', 'Initial', 'ize', '"', ',', '"', 'Get', 'Column', 'Info', '"', ',', '"', 'Get', 'Item', 'Data', '"', ']', '\\', 'n', '</s>']
Filtered   (033): ['IC', 'ol', 'umn', 'Provider', '_', 'Methods', '=', 'IP', 'ers', 'ist', '_', 'Methods', '+', '[', '"', 'Initial', 'ize', '"', ',', '"', 'Get', 'Column', 'Info', '"', ',', '"', 'Get', 'Item', 'Data', '"', ']', '\\', 'n']
Detokenized (012): ['IColumnProvider_Methods', '=', 'IPersist_Methods', '+', '[', '"Initialize"', ',', '"GetColumnInfo"', ',', '"GetItemData"', ']', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "_com_interfaces_ = [ pythoncom . IID_IPersist , \n"
Original    (008): ['_com_interfaces_', '=', '[', 'pythoncom', '.', 'IID_IPersist', ',', '\\n']
Tokenized   (022): ['<s>', '_', 'com', '_', 'inter', 'faces', '_', '=', '[', 'python', 'com', '.', 'I', 'ID', '_', 'IP', 'ers', 'ist', ',', '\\', 'n', '</s>']
Filtered   (020): ['_', 'com', '_', 'inter', 'faces', '_', '=', '[', 'python', 'com', '.', 'I', 'ID', '_', 'IP', 'ers', 'ist', ',', '\\', 'n']
Detokenized (008): ['_com_interfaces_', '=', '[', 'pythoncom', '.', 'IID_IPersist', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "20 , #cChars \n"
Original    (004): ['20', ',', '#cChars', '\\n']
Tokenized   (010): ['<s>', '20', ',', '#', 'c', 'Ch', 'ars', '\\', 'n', '</s>']
Filtered   (008): ['20', ',', '#', 'c', 'Ch', 'ars', '\\', 'n']
Detokenized (004): ['20', ',', '#cChars', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "fmt_id == self . _reg_clsid_ \n"
Original    (006): ['fmt_id', '==', 'self', '.', '_reg_clsid_', '\\n']
Tokenized   (017): ['<s>', 'f', 'mt', '_', 'id', '==', 'self', '.', '_', 'reg', '_', 'cl', 'sid', '_', '\\', 'n', '</s>']
Filtered   (015): ['f', 'mt', '_', 'id', '==', 'self', '.', '_', 'reg', '_', 'cl', 'sid', '_', '\\', 'n']
Detokenized (006): ['fmt_id', '==', 'self', '.', '_reg_clsid_', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : ""Folder\\\\ShellEx\\\\ColumnHandlers\\\\" + str ( ColumnProvider . _reg_clsid_ ) ) \n"
Original    (010): ['"Folder\\\\\\\\ShellEx\\\\\\\\ColumnHandlers\\\\\\\\"', '+', 'str', '(', 'ColumnProvider', '.', '_reg_clsid_', ')', ')', '\\n']
Tokenized   (029): ['<s>', '"', 'Folder', '\\\\\\\\', 'Shell', 'Ex', '\\\\\\\\', 'Column', 'Hand', 'lers', '\\\\\\\\', '"', '+', 'str', '(', 'Column', 'Provider', '.', '_', 'reg', '_', 'cl', 'sid', '_', ')', ')', '\\', 'n', '</s>']
Filtered   (027): ['"', 'Folder', '\\\\\\\\', 'Shell', 'Ex', '\\\\\\\\', 'Column', 'Hand', 'lers', '\\\\\\\\', '"', '+', 'str', '(', 'Column', 'Provider', '.', '_', 'reg', '_', 'cl', 'sid', '_', ')', ')', '\\', 'n']
Detokenized (010): ['"Folder\\\\\\\\ShellEx\\\\\\\\ColumnHandlers\\\\\\\\"', '+', 'str', '(', 'ColumnProvider', '.', '_reg_clsid_', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "_winreg . SetValueEx ( key , None , 0 , _winreg . REG_SZ , ColumnProvider . _reg_desc_ ) \n"
Original    (019): ['_winreg', '.', 'SetValueEx', '(', 'key', ',', 'None', ',', '0', ',', '_winreg', '.', 'REG_SZ', ',', 'ColumnProvider', '.', '_reg_desc_', ')', '\\n']
Tokenized   (036): ['<s>', '_', 'win', 'reg', '.', 'Set', 'Value', 'Ex', '(', 'key', ',', 'None', ',', '0', ',', '_', 'win', 'reg', '.', 'REG', '_', 'S', 'Z', ',', 'Column', 'Provider', '.', '_', 'reg', '_', 'desc', '_', ')', '\\', 'n', '</s>']
Filtered   (034): ['_', 'win', 'reg', '.', 'Set', 'Value', 'Ex', '(', 'key', ',', 'None', ',', '0', ',', '_', 'win', 'reg', '.', 'REG', '_', 'S', 'Z', ',', 'Column', 'Provider', '.', '_', 'reg', '_', 'desc', '_', ')', '\\', 'n']
Detokenized (019): ['_winreg', '.', 'SetValueEx', '(', 'key', ',', 'None', ',', '0', ',', '_winreg', '.', 'REG_SZ', ',', 'ColumnProvider', '.', '_reg_desc_', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "register . UseCommandLine ( ColumnProvider , \n"
Original    (007): ['register', '.', 'UseCommandLine', '(', 'ColumnProvider', ',', '\\n']
Tokenized   (013): ['<s>', 'register', '.', 'Use', 'Command', 'Line', '(', 'Column', 'Provider', ',', '\\', 'n', '</s>']
Filtered   (011): ['register', '.', 'Use', 'Command', 'Line', '(', 'Column', 'Provider', ',', '\\', 'n']
Detokenized (007): ['register', '.', 'UseCommandLine', '(', 'ColumnProvider', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "aliases = MultipleValueField ( required = False ) \n"
Original    (009): ['aliases', '=', 'MultipleValueField', '(', 'required', '=', 'False', ')', '\\n']
Tokenized   (015): ['<s>', 'ali', 'ases', '=', 'Multiple', 'Value', 'Field', '(', 'required', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (013): ['ali', 'ases', '=', 'Multiple', 'Value', 'Field', '(', 'required', '=', 'False', ')', '\\', 'n']
Detokenized (009): ['aliases', '=', 'MultipleValueField', '(', 'required', '=', 'False', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "StoredQueryParameter = namedtuple ( "StoredQueryParameter" , ( , , , , \n"
Original    (012): ['StoredQueryParameter', '=', 'namedtuple', '(', '"StoredQueryParameter"', ',', '(', ',', ',', ',', ',', '\\n']
Tokenized   (025): ['<s>', 'St', 'ored', 'Query', 'Parameter', '=', 'named', 't', 'uple', '(', '"', 'St', 'ored', 'Query', 'Parameter', '"', ',', '(', ',', ',', ',', ',', '\\', 'n', '</s>']
Filtered   (023): ['St', 'ored', 'Query', 'Parameter', '=', 'named', 't', 'uple', '(', '"', 'St', 'ored', 'Query', 'Parameter', '"', ',', '(', ',', ',', ',', ',', '\\', 'n']
Detokenized (012): ['StoredQueryParameter', '=', 'namedtuple', '(', '"StoredQueryParameter"', ',', '(', ',', ',', ',', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "fts = list ( self . models . keys ( ) ) \n"
Original    (013): ['fts', '=', 'list', '(', 'self', '.', 'models', '.', 'keys', '(', ')', ')', '\\n']
Tokenized   (016): ['<s>', 'fts', '=', 'list', '(', 'self', '.', 'models', '.', 'keys', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (014): ['fts', '=', 'list', '(', 'self', '.', 'models', '.', 'keys', '(', ')', ')', '\\', 'n']
Detokenized (013): ['fts', '=', 'list', '(', 'self', '.', 'models', '.', 'keys', '(', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "sort_by = parms . cleaned_data [ ] \n"
Original    (008): ['sort_by', '=', 'parms', '.', 'cleaned_data', '[', ']', '\\n']
Tokenized   (016): ['<s>', 'sort', '_', 'by', '=', 'par', 'ms', '.', 'cleaned', '_', 'data', '[', ']', '\\', 'n', '</s>']
Filtered   (014): ['sort', '_', 'by', '=', 'par', 'ms', '.', 'cleaned', '_', 'data', '[', ']', '\\', 'n']
Detokenized (008): ['sort_by', '=', 'parms', '.', 'cleaned_data', '[', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "geometry_field = self . geometries [ type_names [ 0 ] ] \n"
Original    (012): ['geometry_field', '=', 'self', '.', 'geometries', '[', 'type_names', '[', '0', ']', ']', '\\n']
Tokenized   (022): ['<s>', 'ge', 'ometry', '_', 'field', '=', 'self', '.', 'ge', 'omet', 'ries', '[', 'type', '_', 'names', '[', '0', ']', ']', '\\', 'n', '</s>']
Filtered   (020): ['ge', 'ometry', '_', 'field', '=', 'self', '.', 'ge', 'omet', 'ries', '[', 'type', '_', 'names', '[', '0', ']', ']', '\\', 'n']
Detokenized (012): ['geometry_field', '=', 'self', '.', 'geometries', '[', 'type_names', '[', '0', ']', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "mxy = mxy ) \n"
Original    (005): ['mxy', '=', 'mxy', ')', '\\n']
Tokenized   (010): ['<s>', 'm', 'xy', '=', 'm', 'xy', ')', '\\', 'n', '</s>']
Filtered   (008): ['m', 'xy', '=', 'm', 'xy', ')', '\\', 'n']
Detokenized (005): ['mxy', '=', 'mxy', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "query_set = query_set . order_by ( * sort_by ) \n"
Original    (010): ['query_set', '=', 'query_set', '.', 'order_by', '(', '*', 'sort_by', ')', '\\n']
Tokenized   (021): ['<s>', 'query', '_', 'set', '=', 'query', '_', 'set', '.', 'order', '_', 'by', '(', '*', 'sort', '_', 'by', ')', '\\', 'n', '</s>']
Filtered   (019): ['query', '_', 'set', '=', 'query', '_', 'set', '.', 'order', '_', 'by', '(', '*', 'sort', '_', 'by', ')', '\\', 'n']
Detokenized (010): ['query_set', '=', 'query_set', '.', 'order_by', '(', '*', 'sort_by', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "output_format = root . get ( , ) \n"
Original    (009): ['output_format', '=', 'root', '.', 'get', '(', ',', ')', '\\n']
Tokenized   (014): ['<s>', 'output', '_', 'format', '=', 'root', '.', 'get', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (012): ['output', '_', 'format', '=', 'root', '.', 'get', '(', ',', ')', '\\', 'n']
Detokenized (009): ['output_format', '=', 'root', '.', 'get', '(', ',', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "type_names . append ( ( namespace , name ) ) \n"
Original    (011): ['type_names', '.', 'append', '(', '(', 'namespace', ',', 'name', ')', ')', '\\n']
Tokenized   (016): ['<s>', 'type', '_', 'names', '.', 'append', '(', '(', 'namespace', ',', 'name', ')', ')', '\\', 'n', '</s>']
Filtered   (014): ['type', '_', 'names', '.', 'append', '(', '(', 'namespace', ',', 'name', ')', ')', '\\', 'n']
Detokenized (011): ['type_names', '.', 'append', '(', '(', 'namespace', ',', 'name', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""schema" : feature_type . schema , \n"
Original    (007): ['"schema"', ':', 'feature_type', '.', 'schema', ',', '\\n']
Tokenized   (015): ['<s>', '"', 'sche', 'ma', '"', ':', 'feature', '_', 'type', '.', 'schema', ',', '\\', 'n', '</s>']
Filtered   (013): ['"', 'sche', 'ma', '"', ':', 'feature', '_', 'type', '.', 'schema', ',', '\\', 'n']
Detokenized (007): ['"schema"', ':', 'feature_type', '.', 'schema', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""ns_name" : feature_type . ns_name \n"
Original    (006): ['"ns_name"', ':', 'feature_type', '.', 'ns_name', '\\n']
Tokenized   (017): ['<s>', '"', 'ns', '_', 'name', '"', ':', 'feature', '_', 'type', '.', 'ns', '_', 'name', '\\', 'n', '</s>']
Filtered   (015): ['"', 'ns', '_', 'name', '"', ':', 'feature', '_', 'type', '.', 'ns', '_', 'name', '\\', 'n']
Detokenized (006): ['"ns_name"', ':', 'feature_type', '.', 'ns_name', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "db_params = settings . DATABASES [ response . db ] \n"
Original    (011): ['db_params', '=', 'settings', '.', 'DATABASES', '[', 'response', '.', 'db', ']', '\\n']
Tokenized   (020): ['<s>', 'db', '_', 'params', '=', 'settings', '.', 'D', 'AT', 'AB', 'AS', 'ES', '[', 'response', '.', 'db', ']', '\\', 'n', '</s>']
Filtered   (018): ['db', '_', 'params', '=', 'settings', '.', 'D', 'AT', 'AB', 'AS', 'ES', '[', 'response', '.', 'db', ']', '\\', 'n']
Detokenized (011): ['db_params', '=', 'settings', '.', 'DATABASES', '[', 'response', '.', 'db', ']', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "parameters = tuple ( [ adapt ( p ) for p in parameters ] ) \n"
Original    (016): ['parameters', '=', 'tuple', '(', '[', 'adapt', '(', 'p', ')', 'for', 'p', 'in', 'parameters', ']', ')', '\\n']
Tokenized   (020): ['<s>', 'param', 'eters', '=', 'tuple', '(', '[', 'adapt', '(', 'p', ')', 'for', 'p', 'in', 'parameters', ']', ')', '\\', 'n', '</s>']
Filtered   (018): ['param', 'eters', '=', 'tuple', '(', '[', 'adapt', '(', 'p', ')', 'for', 'p', 'in', 'parameters', ']', ')', '\\', 'n']
Detokenized (016): ['parameters', '=', 'tuple', '(', '[', 'adapt', '(', 'p', ')', 'for', 'p', 'in', 'parameters', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "connection_string = "PG:dbname=\'{db}\'" . format ( db = db_params [ ] ) \n"
Original    (013): ['connection_string', '=', '"PG:dbname=\\\'{db}\\\'"', '.', 'format', '(', 'db', '=', 'db_params', '[', ']', ')', '\\n']
Tokenized   (030): ['<s>', 'connection', '_', 'string', '=', '"', 'PG', ':', 'db', 'name', '=', "\\'", '{', 'db', '}\\', '\'"', '.', 'format', '(', 'db', '=', 'db', '_', 'params', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (028): ['connection', '_', 'string', '=', '"', 'PG', ':', 'db', 'name', '=', "\\'", '{', 'db', '}\\', '\'"', '.', 'format', '(', 'db', '=', 'db', '_', 'params', '[', ']', ')', '\\', 'n']
Detokenized (013): ['connection_string', '=', '"PG:dbname=\\\'{db}\\\'"', '.', 'format', '(', 'db', '=', 'db_params', '[', ']', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "etree . SubElement ( p , ) . text = parameter . abstractS \n"
Original    (014): ['etree', '.', 'SubElement', '(', 'p', ',', ')', '.', 'text', '=', 'parameter', '.', 'abstractS', '\\n']
Tokenized   (020): ['<s>', 'et', 'ree', '.', 'Sub', 'Element', '(', 'p', ',', ')', '.', 'text', '=', 'parameter', '.', 'abstract', 'S', '\\', 'n', '</s>']
Filtered   (018): ['et', 'ree', '.', 'Sub', 'Element', '(', 'p', ',', ')', '.', 'text', '=', 'parameter', '.', 'abstract', 'S', '\\', 'n']
Detokenized (014): ['etree', '.', 'SubElement', '(', 'p', ',', ')', '.', 'text', '=', 'parameter', '.', 'abstractS', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : ""isPrivate" : parameter . query_expression . private == True , \n"
Original    (011): ['"isPrivate"', ':', 'parameter', '.', 'query_expression', '.', 'private', '==', 'True', ',', '\\n']
Tokenized   (019): ['<s>', '"', 'is', 'Private', '"', ':', 'parameter', '.', 'query', '_', 'expression', '.', 'private', '==', 'True', ',', '\\', 'n', '</s>']
Filtered   (017): ['"', 'is', 'Private', '"', ':', 'parameter', '.', 'query', '_', 'expression', '.', 'private', '==', 'True', ',', '\\', 'n']
Detokenized (011): ['"isPrivate"', ':', 'parameter', '.', 'query_expression', '.', 'private', '==', 'True', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""language" : parameter . query_expression . language , \n"
Original    (009): ['"language"', ':', 'parameter', '.', 'query_expression', '.', 'language', ',', '\\n']
Tokenized   (016): ['<s>', '"', 'language', '"', ':', 'parameter', '.', 'query', '_', 'expression', '.', 'language', ',', '\\', 'n', '</s>']
Filtered   (014): ['"', 'language', '"', ':', 'parameter', '.', 'query', '_', 'expression', '.', 'language', ',', '\\', 'n']
Detokenized (009): ['"language"', ':', 'parameter', '.', 'query_expression', '.', 'language', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ""returnFeatureTypes" : . join ( parameter . query_expression . return_feature_types } ) . text = parameter . query_expression . text \n"
Original    (021): ['"returnFeatureTypes"', ':', '.', 'join', '(', 'parameter', '.', 'query_expression', '.', 'return_feature_types', '}', ')', '.', 'text', '=', 'parameter', '.', 'query_expression', '.', 'text', '\\n']
Tokenized   (036): ['<s>', '"', 'return', 'Feature', 'Types', '"', ':', '.', 'join', '(', 'parameter', '.', 'query', '_', 'expression', '.', 'return', '_', 'feature', '_', 'types', '}', ')', '.', 'text', '=', 'parameter', '.', 'query', '_', 'expression', '.', 'text', '\\', 'n', '</s>']
Filtered   (034): ['"', 'return', 'Feature', 'Types', '"', ':', '.', 'join', '(', 'parameter', '.', 'query', '_', 'expression', '.', 'return', '_', 'feature', '_', 'types', '}', ')', '.', 'text', '=', 'parameter', '.', 'query', '_', 'expression', '.', 'text', '\\', 'n']
Detokenized (021): ['"returnFeatureTypes"', ':', '.', 'join', '(', 'parameter', '.', 'query_expression', '.', 'return_feature_types', '}', ')', '.', 'text', '=', 'parameter', '.', 'query_expression', '.', 'text', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : ""endpoint" : request . build_absolute_uri ( ) . split ( ) [ 0 ] , \n"
Original    (016): ['"endpoint"', ':', 'request', '.', 'build_absolute_uri', '(', ')', '.', 'split', '(', ')', '[', '0', ']', ',', '\\n']
Tokenized   (026): ['<s>', '"', 'end', 'point', '"', ':', 'request', '.', 'build', '_', 'absolute', '_', 'uri', '(', ')', '.', 'split', '(', ')', '[', '0', ']', ',', '\\', 'n', '</s>']
Filtered   (024): ['"', 'end', 'point', '"', ':', 'request', '.', 'build', '_', 'absolute', '_', 'uri', '(', ')', '.', 'split', '(', ')', '[', '0', ']', ',', '\\', 'n']
Detokenized (016): ['"endpoint"', ':', 'request', '.', 'build_absolute_uri', '(', ')', '.', 'split', '(', ')', '[', '0', ']', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : ""output_formats" : [ ogr . GetDriver ( drv ) . GetName ( ) for drv in range ( ogr . GetDriverCount ( ) ) "addr_street" : self . addr_street , \n"
Original    (031): ['"output_formats"', ':', '[', 'ogr', '.', 'GetDriver', '(', 'drv', ')', '.', 'GetName', '(', ')', 'for', 'drv', 'in', 'range', '(', 'ogr', '.', 'GetDriverCount', '(', ')', ')', '"addr_street"', ':', 'self', '.', 'addr_street', ',', '\\n']
Tokenized   (053): ['<s>', '"', 'output', '_', 'form', 'ats', '"', ':', '[', 'o', 'gr', '.', 'Get', 'Driver', '(', 'dr', 'v', ')', '.', 'Get', 'Name', '(', ')', 'for', 'dr', 'v', 'in', 'range', '(', 'o', 'gr', '.', 'Get', 'Driver', 'Count', '(', ')', ')', '"', 'addr', '_', 'street', '"', ':', 'self', '.', 'addr', '_', 'street', ',', '\\', 'n', '</s>']
Filtered   (051): ['"', 'output', '_', 'form', 'ats', '"', ':', '[', 'o', 'gr', '.', 'Get', 'Driver', '(', 'dr', 'v', ')', '.', 'Get', 'Name', '(', ')', 'for', 'dr', 'v', 'in', 'range', '(', 'o', 'gr', '.', 'Get', 'Driver', 'Count', '(', ')', ')', '"', 'addr', '_', 'street', '"', ':', 'self', '.', 'addr', '_', 'street', ',', '\\', 'n']
Detokenized (031): ['"output_formats"', ':', '[', 'ogr', '.', 'GetDriver', '(', 'drv', ')', '.', 'GetName', '(', ')', 'for', 'drv', 'in', 'range', '(', 'ogr', '.', 'GetDriverCount', '(', ')', ')', '"addr_street"', ':', 'self', '.', 'addr_street', ',', '\\n']
Counter: 51
===================================================================
Hidden states:  (13, 31, 768)
# Extracted words:  31
Sentence         : ""feature_versioning" : self . adapter . supports_feature_versioning ( ) , \n"
Original    (011): ['"feature_versioning"', ':', 'self', '.', 'adapter', '.', 'supports_feature_versioning', '(', ')', ',', '\\n']
Tokenized   (024): ['<s>', '"', 'feature', '_', 'version', 'ing', '"', ':', 'self', '.', 'adapter', '.', 'supports', '_', 'feature', '_', 'version', 'ing', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (022): ['"', 'feature', '_', 'version', 'ing', '"', ':', 'self', '.', 'adapter', '.', 'supports', '_', 'feature', '_', 'version', 'ing', '(', ')', ',', '\\', 'n']
Detokenized (011): ['"feature_versioning"', ':', 'self', '.', 'adapter', '.', 'supports_feature_versioning', '(', ')', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""date" : datetime . now ( ) , \n"
Original    (009): ['"date"', ':', 'datetime', '.', 'now', '(', ')', ',', '\\n']
Tokenized   (015): ['<s>', '"', 'date', '"', ':', 'dat', 'etime', '.', 'now', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (013): ['"', 'date', '"', ':', 'dat', 'etime', '.', 'now', '(', ')', ',', '\\', 'n']
Detokenized (009): ['"date"', ':', 'datetime', '.', 'now', '(', ')', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "matchItem . setText ( 3 , unicode ( self . data [ "Matches" ] ) ) \n"
Original    (017): ['matchItem', '.', 'setText', '(', '3', ',', 'unicode', '(', 'self', '.', 'data', '[', '"Matches"', ']', ')', ')', '\\n']
Tokenized   (026): ['<s>', 'match', 'Item', '.', 'set', 'Text', '(', '3', ',', 'unic', 'ode', '(', 'self', '.', 'data', '[', '"', 'Mat', 'ches', '"', ']', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['match', 'Item', '.', 'set', 'Text', '(', '3', ',', 'unic', 'ode', '(', 'self', '.', 'data', '[', '"', 'Mat', 'ches', '"', ']', ')', ')', '\\', 'n']
Detokenized (017): ['matchItem', '.', 'setText', '(', '3', ',', 'unicode', '(', 'self', '.', 'data', '[', '"Matches"', ']', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "roundItem . setText ( 3 , unicode ( opponent [ 2 ] ) ) \n"
Original    (015): ['roundItem', '.', 'setText', '(', '3', ',', 'unicode', '(', 'opponent', '[', '2', ']', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'round', 'Item', '.', 'set', 'Text', '(', '3', ',', 'unic', 'ode', '(', 'opponent', '[', '2', ']', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['round', 'Item', '.', 'set', 'Text', '(', '3', ',', 'unic', 'ode', '(', 'opponent', '[', '2', ']', ')', ')', '\\', 'n']
Detokenized (015): ['roundItem', '.', 'setText', '(', '3', ',', 'unicode', '(', 'opponent', '[', '2', ']', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "opponent [ 3 ] = roundItem \n"
Original    (007): ['opponent', '[', '3', ']', '=', 'roundItem', '\\n']
Tokenized   (012): ['<s>', 'opp', 'onent', '[', '3', ']', '=', 'round', 'Item', '\\', 'n', '</s>']
Filtered   (010): ['opp', 'onent', '[', '3', ']', '=', 'round', 'Item', '\\', 'n']
Detokenized (007): ['opponent', '[', '3', ']', '=', 'roundItem', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "M = np . matrix ( [ [ 2 , 3 , 4 ] , \n"
Original    (016): ['M', '=', 'np', '.', 'matrix', '(', '[', '[', '2', ',', '3', ',', '4', ']', ',', '\\n']
Tokenized   (019): ['<s>', 'M', '=', 'np', '.', 'matrix', '(', '[', '[', '2', ',', '3', ',', '4', ']', ',', '\\', 'n', '</s>']
Filtered   (017): ['M', '=', 'np', '.', 'matrix', '(', '[', '[', '2', ',', '3', ',', '4', ']', ',', '\\', 'n']
Detokenized (016): ['M', '=', 'np', '.', 'matrix', '(', '[', '[', '2', ',', '3', ',', '4', ']', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "matrix = Matrix ( M , mtype = ) \n"
Original    (010): ['matrix', '=', 'Matrix', '(', 'M', ',', 'mtype', '=', ')', '\\n']
Tokenized   (015): ['<s>', 'mat', 'rix', '=', 'Matrix', '(', 'M', ',', 'm', 'type', '=', ')', '\\', 'n', '</s>']
Filtered   (013): ['mat', 'rix', '=', 'Matrix', '(', 'M', ',', 'm', 'type', '=', ')', '\\', 'n']
Detokenized (010): ['matrix', '=', 'Matrix', '(', 'M', ',', 'mtype', '=', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "math = Math ( data = [ , vec_name , , Matrix ( M * a ) ] ) \n"
Original    (020): ['math', '=', 'Math', '(', 'data', '=', '[', ',', 'vec_name', ',', ',', 'Matrix', '(', 'M', '*', 'a', ')', ']', ')', '\\n']
Tokenized   (025): ['<s>', 'math', '=', 'Math', '(', 'data', '=', '[', ',', 'vec', '_', 'name', ',', ',', 'Matrix', '(', 'M', '*', 'a', ')', ']', ')', '\\', 'n', '</s>']
Filtered   (023): ['math', '=', 'Math', '(', 'data', '=', '[', ',', 'vec', '_', 'name', ',', ',', 'Matrix', '(', 'M', '*', 'a', ')', ']', ')', '\\', 'n']
Detokenized (020): ['math', '=', 'Math', '(', 'data', '=', '[', ',', 'vec_name', ',', ',', 'Matrix', '(', 'M', '*', 'a', ')', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "q2 = Quantity ( v , format_cb = lambda x : str ( int ( x ) ) ) \n"
Original    (020): ['q2', '=', 'Quantity', '(', 'v', ',', 'format_cb', '=', 'lambda', 'x', ':', 'str', '(', 'int', '(', 'x', ')', ')', ')', '\\n']
Tokenized   (026): ['<s>', 'q', '2', '=', 'Quantity', '(', 'v', ',', 'format', '_', 'cb', '=', 'lambda', 'x', ':', 'str', '(', 'int', '(', 'x', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['q', '2', '=', 'Quantity', '(', 'v', ',', 'format', '_', 'cb', '=', 'lambda', 'x', ':', 'str', '(', 'int', '(', 'x', ')', ')', ')', '\\', 'n']
Detokenized (020): ['q2', '=', 'Quantity', '(', 'v', ',', 'format_cb', '=', 'lambda', 'x', ':', 'str', '(', 'int', '(', 'x', ')', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "q3 = Quantity ( v , options = { : } ) \n"
Original    (013): ['q3', '=', 'Quantity', '(', 'v', ',', 'options', '=', '{', ':', '}', ')', '\\n']
Tokenized   (017): ['<s>', 'q', '3', '=', 'Quantity', '(', 'v', ',', 'options', '=', '{', ':', '}', ')', '\\', 'n', '</s>']
Filtered   (015): ['q', '3', '=', 'Quantity', '(', 'v', ',', 'options', '=', '{', ':', '}', ')', '\\', 'n']
Detokenized (013): ['q3', '=', 'Quantity', '(', 'v', ',', 'options', '=', '{', ':', '}', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "test_dimensionality_to_siunitx ( ) \n"
Original    (004): ['test_dimensionality_to_siunitx', '(', ')', '\\n']
Tokenized   (016): ['<s>', 'test', '_', 'dimension', 'ality', '_', 'to', '_', 'si', 'unit', 'x', '(', ')', '\\', 'n', '</s>']
Filtered   (014): ['test', '_', 'dimension', 'ality', '_', 'to', '_', 'si', 'unit', 'x', '(', ')', '\\', 'n']
Detokenized (004): ['test_dimensionality_to_siunitx', '(', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "ph = put_handler . put_handler ( fs , ) \n"
Original    (010): ['ph', '=', 'put_handler', '.', 'put_handler', '(', 'fs', ',', ')', '\\n']
Tokenized   (017): ['<s>', 'ph', '=', 'put', '_', 'handler', '.', 'put', '_', 'handler', '(', 'fs', ',', ')', '\\', 'n', '</s>']
Filtered   (015): ['ph', '=', 'put', '_', 'handler', '.', 'put', '_', 'handler', '(', 'fs', ',', ')', '\\', 'n']
Detokenized (010): ['ph', '=', 'put_handler', '.', 'put_handler', '(', 'fs', ',', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "hs = http_server . http_server ( ip = , port = 8080 ) \n"
Original    (014): ['hs', '=', 'http_server', '.', 'http_server', '(', 'ip', '=', ',', 'port', '=', '8080', ')', '\\n']
Tokenized   (022): ['<s>', 'hs', '=', 'http', '_', 'server', '.', 'http', '_', 'server', '(', 'ip', '=', ',', 'port', '=', '80', '80', ')', '\\', 'n', '</s>']
Filtered   (020): ['hs', '=', 'http', '_', 'server', '.', 'http', '_', 'server', '(', 'ip', '=', ',', 'port', '=', '80', '80', ')', '\\', 'n']
Detokenized (014): ['hs', '=', 'http_server', '.', 'http_server', '(', 'ip', '=', ',', 'port', '=', '8080', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "num_trans = num_requests * num_conns \n"
Original    (006): ['num_trans', '=', 'num_requests', '*', 'num_conns', '\\n']
Tokenized   (017): ['<s>', 'num', '_', 'trans', '=', 'num', '_', 'requ', 'ests', '*', 'num', '_', 'con', 'ns', '\\', 'n', '</s>']
Filtered   (015): ['num', '_', 'trans', '=', 'num', '_', 'requ', 'ests', '*', 'num', '_', 'con', 'ns', '\\', 'n']
Detokenized (006): ['num_trans', '=', 'num_requests', '*', 'num_conns', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "trans_per_sec = num_trans / total_time \n"
Original    (006): ['trans_per_sec', '=', 'num_trans', '/', 'total_time', '\\n']
Tokenized   (017): ['<s>', 'trans', '_', 'per', '_', 'sec', '=', 'num', '_', 'trans', '/', 'total', '_', 'time', '\\', 'n', '</s>']
Filtered   (015): ['trans', '_', 'per', '_', 'sec', '=', 'num', '_', 'trans', '/', 'total', '_', 'time', '\\', 'n']
Detokenized (006): ['trans_per_sec', '=', 'num_trans', '/', 'total_time', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "map ( str , ( num_conns , num_requests , request_size , throughput , trans_per_sec ) \n"
Original    (016): ['map', '(', 'str', ',', '(', 'num_conns', ',', 'num_requests', ',', 'request_size', ',', 'throughput', ',', 'trans_per_sec', ')', '\\n']
Tokenized   (031): ['<s>', 'map', '(', 'str', ',', '(', 'num', '_', 'con', 'ns', ',', 'num', '_', 'requ', 'ests', ',', 'request', '_', 'size', ',', 'throughput', ',', 'trans', '_', 'per', '_', 'sec', ')', '\\', 'n', '</s>']
Filtered   (029): ['map', '(', 'str', ',', '(', 'num', '_', 'con', 'ns', ',', 'num', '_', 'requ', 'ests', ',', 'request', '_', 'size', ',', 'throughput', ',', 'trans', '_', 'per', '_', 'sec', ')', '\\', 'n']
Detokenized (016): ['map', '(', 'str', ',', '(', 'num_conns', ',', 'num_requests', ',', 'request_size', ',', 'throughput', ',', 'trans_per_sec', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "queue . add_task ( task , 3 ) \n"
Original    (009): ['queue', '.', 'add_task', '(', 'task', ',', '3', ')', '\\n']
Tokenized   (014): ['<s>', 'queue', '.', 'add', '_', 'task', '(', 'task', ',', '3', ')', '\\', 'n', '</s>']
Filtered   (012): ['queue', '.', 'add', '_', 'task', '(', 'task', ',', '3', ')', '\\', 'n']
Detokenized (009): ['queue', '.', 'add_task', '(', 'task', ',', '3', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "futures . append ( queue . yield_task ( task , 3 ) ) \n"
Original    (014): ['futures', '.', 'append', '(', 'queue', '.', 'yield_task', '(', 'task', ',', '3', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'f', 'ut', 'ures', '.', 'append', '(', 'queue', '.', 'yield', '_', 'task', '(', 'task', ',', '3', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['f', 'ut', 'ures', '.', 'append', '(', 'queue', '.', 'yield', '_', 'task', '(', 'task', ',', '3', ')', ')', '\\', 'n']
Detokenized (014): ['futures', '.', 'append', '(', 'queue', '.', 'yield_task', '(', 'task', ',', '3', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "task_results [ : ] = res \n"
Original    (007): ['task_results', '[', ':', ']', '=', 'res', '\\n']
Tokenized   (012): ['<s>', 'task', '_', 'results', '[', ':', ']', '=', 'res', '\\', 'n', '</s>']
Filtered   (010): ['task', '_', 'results', '[', ':', ']', '=', 'res', '\\', 'n']
Detokenized (007): ['task_results', '[', ':', ']', '=', 'res', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "shuffle ( self . __queued_servers ) \n"
Original    (007): ['shuffle', '(', 'self', '.', '__queued_servers', ')', '\\n']
Tokenized   (016): ['<s>', 'sh', 'uffle', '(', 'self', '.', '__', 'que', 'ued', '_', 'ser', 'vers', ')', '\\', 'n', '</s>']
Filtered   (014): ['sh', 'uffle', '(', 'self', '.', '__', 'que', 'ued', '_', 'ser', 'vers', ')', '\\', 'n']
Detokenized (007): ['shuffle', '(', 'self', '.', '__queued_servers', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "event_name = event [ ] \n"
Original    (006): ['event_name', '=', 'event', '[', ']', '\\n']
Tokenized   (011): ['<s>', 'event', '_', 'name', '=', 'event', '[', ']', '\\', 'n', '</s>']
Filtered   (009): ['event', '_', 'name', '=', 'event', '[', ']', '\\', 'n']
Detokenized (006): ['event_name', '=', 'event', '[', ']', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "event_data = zlib . compress ( pickle . dumps ( event ) ) \n"
Original    (014): ['event_data', '=', 'zlib', '.', 'compress', '(', 'pickle', '.', 'dumps', '(', 'event', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'event', '_', 'data', '=', 'z', 'lib', '.', 'compress', '(', 'pick', 'le', '.', 'dumps', '(', 'event', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['event', '_', 'data', '=', 'z', 'lib', '.', 'compress', '(', 'pick', 'le', '.', 'dumps', '(', 'event', ')', ')', '\\', 'n']
Detokenized (014): ['event_data', '=', 'zlib', '.', 'compress', '(', 'pickle', '.', 'dumps', '(', 'event', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "path_only , query = self . _split_path ( path ) \n"
Original    (011): ['path_only', ',', 'query', '=', 'self', '.', '_split_path', '(', 'path', ')', '\\n']
Tokenized   (019): ['<s>', 'path', '_', 'only', ',', 'query', '=', 'self', '.', '_', 'split', '_', 'path', '(', 'path', ')', '\\', 'n', '</s>']
Filtered   (017): ['path', '_', 'only', ',', 'query', '=', 'self', '.', '_', 'split', '_', 'path', '(', 'path', ')', '\\', 'n']
Detokenized (011): ['path_only', ',', 'query', '=', 'self', '.', '_split_path', '(', 'path', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "break ; \n"
Original    (003): ['break', ';', '\\n']
Tokenized   (006): ['<s>', 'break', ';', '\\', 'n', '</s>']
Filtered   (004): ['break', ';', '\\', 'n']
Detokenized (003): ['break', ';', '\\n']
Counter: 4
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "u . email = user [ 2 ] \n"
Original    (009): ['u', '.', 'email', '=', 'user', '[', '2', ']', '\\n']
Tokenized   (012): ['<s>', 'u', '.', 'email', '=', 'user', '[', '2', ']', '\\', 'n', '</s>']
Filtered   (010): ['u', '.', 'email', '=', 'user', '[', '2', ']', '\\', 'n']
Detokenized (009): ['u', '.', 'email', '=', 'user', '[', '2', ']', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "trac_components = list ( [ ] ) \n"
Original    (008): ['trac_components', '=', 'list', '(', '[', ']', ')', '\\n']
Tokenized   (015): ['<s>', 'tr', 'ac', '_', 'comp', 'onents', '=', 'list', '(', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (013): ['tr', 'ac', '_', 'comp', 'onents', '=', 'list', '(', '[', ']', ')', '\\', 'n']
Detokenized (008): ['trac_components', '=', 'list', '(', '[', ']', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "component . owner = self . _get_user_login ( component . owner ) \n"
Original    (013): ['component', '.', 'owner', '=', 'self', '.', '_get_user_login', '(', 'component', '.', 'owner', ')', '\\n']
Tokenized   (021): ['<s>', 'component', '.', 'owner', '=', 'self', '.', '_', 'get', '_', 'user', '_', 'login', '(', 'component', '.', 'owner', ')', '\\', 'n', '</s>']
Filtered   (019): ['component', '.', 'owner', '=', 'self', '.', '_', 'get', '_', 'user', '_', 'login', '(', 'component', '.', 'owner', ')', '\\', 'n']
Detokenized (013): ['component', '.', 'owner', '=', 'self', '.', '_get_user_login', '(', 'component', '.', 'owner', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "networks [ pkt . pduSource ] . append ( pkt . wirtnNetwork ) \n"
Original    (014): ['networks', '[', 'pkt', '.', 'pduSource', ']', '.', 'append', '(', 'pkt', '.', 'wirtnNetwork', ')', '\\n']
Tokenized   (025): ['<s>', 'net', 'works', '[', 'p', 'kt', '.', 'p', 'du', 'Source', ']', '.', 'append', '(', 'p', 'kt', '.', 'w', 'irt', 'n', 'Network', ')', '\\', 'n', '</s>']
Filtered   (023): ['net', 'works', '[', 'p', 'kt', '.', 'p', 'du', 'Source', ']', '.', 'append', '(', 'p', 'kt', '.', 'w', 'irt', 'n', 'Network', ')', '\\', 'n']
Detokenized (014): ['networks', '[', 'pkt', '.', 'pduSource', ']', '.', 'append', '(', 'pkt', '.', 'wirtnNetwork', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "filterSource = Address ( sys . argv [ i + 1 ] ) \n"
Original    (014): ['filterSource', '=', 'Address', '(', 'sys', '.', 'argv', '[', 'i', '+', '1', ']', ')', '\\n']
Tokenized   (019): ['<s>', 'filter', 'Source', '=', 'Address', '(', 'sys', '.', 'arg', 'v', '[', 'i', '+', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (017): ['filter', 'Source', '=', 'Address', '(', 'sys', '.', 'arg', 'v', '[', 'i', '+', '1', ']', ')', '\\', 'n']
Detokenized (014): ['filterSource', '=', 'Address', '(', 'sys', '.', 'argv', '[', 'i', '+', '1', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "net_count . sort ( lambda x , y : cmp ( y [ 1 ] , x [ 1 ] ) ) \n"
Original    (023): ['net_count', '.', 'sort', '(', 'lambda', 'x', ',', 'y', ':', 'cmp', '(', 'y', '[', '1', ']', ',', 'x', '[', '1', ']', ')', ')', '\\n']
Tokenized   (029): ['<s>', 'net', '_', 'count', '.', 'sort', '(', 'lambda', 'x', ',', 'y', ':', 'c', 'mp', '(', 'y', '[', '1', ']', ',', 'x', '[', '1', ']', ')', ')', '\\', 'n', '</s>']
Filtered   (027): ['net', '_', 'count', '.', 'sort', '(', 'lambda', 'x', ',', 'y', ':', 'c', 'mp', '(', 'y', '[', '1', ']', ',', 'x', '[', '1', ']', ')', ')', '\\', 'n']
Detokenized (023): ['net_count', '.', 'sort', '(', 'lambda', 'x', ',', 'y', ':', 'cmp', '(', 'y', '[', '1', ']', ',', 'x', '[', '1', ']', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "strm = StringIO ( self . pickleBuffer ) \n"
Original    (009): ['strm', '=', 'StringIO', '(', 'self', '.', 'pickleBuffer', ')', '\\n']
Tokenized   (016): ['<s>', 'str', 'm', '=', 'String', 'IO', '(', 'self', '.', 'pick', 'le', 'Buffer', ')', '\\', 'n', '</s>']
Filtered   (014): ['str', 'm', '=', 'String', 'IO', '(', 'self', '.', 'pick', 'le', 'Buffer', ')', '\\', 'n']
Detokenized (009): ['strm', '=', 'StringIO', '(', 'self', '.', 'pickleBuffer', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "pdu . pduSource = self . peer \n"
Original    (008): ['pdu', '.', 'pduSource', '=', 'self', '.', 'peer', '\\n']
Tokenized   (014): ['<s>', 'p', 'du', '.', 'p', 'du', 'Source', '=', 'self', '.', 'peer', '\\', 'n', '</s>']
Filtered   (012): ['p', 'du', '.', 'p', 'du', 'Source', '=', 'self', '.', 'peer', '\\', 'n']
Detokenized (008): ['pdu', '.', 'pduSource', '=', 'self', '.', 'peer', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "connect_task . install_task ( _time ( ) + self . reconnect [ actor . peer ] ) \n"
Original    (018): ['connect_task', '.', 'install_task', '(', '_time', '(', ')', '+', 'self', '.', 'reconnect', '[', 'actor', '.', 'peer', ']', ')', '\\n']
Tokenized   (026): ['<s>', 'connect', '_', 'task', '.', 'install', '_', 'task', '(', '_', 'time', '(', ')', '+', 'self', '.', 'reconnect', '[', 'actor', '.', 'peer', ']', ')', '\\', 'n', '</s>']
Filtered   (024): ['connect', '_', 'task', '.', 'install', '_', 'task', '(', '_', 'time', '(', ')', '+', 'self', '.', 'reconnect', '[', 'actor', '.', 'peer', ']', ')', '\\', 'n']
Detokenized (018): ['connect_task', '.', 'install_task', '(', '_time', '(', ')', '+', 'self', '.', 'reconnect', '[', 'actor', '.', 'peer', ']', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "asyncore . dispatcher . __init__ ( self , sock ) \n"
Original    (011): ['asyncore', '.', 'dispatcher', '.', '__init__', '(', 'self', ',', 'sock', ')', '\\n']
Tokenized   (018): ['<s>', 'as', 'yn', 'core', '.', 'dispatcher', '.', '__', 'init', '__', '(', 'self', ',', 'sock', ')', '\\', 'n', '</s>']
Filtered   (016): ['as', 'yn', 'core', '.', 'dispatcher', '.', '__', 'init', '__', '(', 'self', ',', 'sock', ')', '\\', 'n']
Detokenized (011): ['asyncore', '.', 'dispatcher', '.', '__init__', '(', 'self', ',', 'sock', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "TCPServerDirector . _warning ( , err ) \n"
Original    (008): ['TCPServerDirector', '.', '_warning', '(', ',', 'err', ')', '\\n']
Tokenized   (015): ['<s>', 'TC', 'PS', 'erver', 'Director', '.', '_', 'warning', '(', ',', 'err', ')', '\\', 'n', '</s>']
Filtered   (013): ['TC', 'PS', 'erver', 'Director', '.', '_', 'warning', '(', ',', 'err', ')', '\\', 'n']
Detokenized (008): ['TCPServerDirector', '.', '_warning', '(', ',', 'err', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "buff = packet [ 1 ] \n"
Original    (007): ['buff', '=', 'packet', '[', '1', ']', '\\n']
Tokenized   (010): ['<s>', 'buff', '=', 'packet', '[', '1', ']', '\\', 'n', '</s>']
Filtered   (008): ['buff', '=', 'packet', '[', '1', ']', '\\', 'n']
Detokenized (007): ['buff', '=', 'packet', '[', '1', ']', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "fileIdentifier = ( obj_type , obj_inst ) , \n"
Original    (009): ['fileIdentifier', '=', '(', 'obj_type', ',', 'obj_inst', ')', ',', '\\n']
Tokenized   (018): ['<s>', 'file', 'Ident', 'ifier', '=', '(', 'obj', '_', 'type', ',', 'obj', '_', 'inst', ')', ',', '\\', 'n', '</s>']
Filtered   (016): ['file', 'Ident', 'ifier', '=', '(', 'obj', '_', 'type', ',', 'obj', '_', 'inst', ')', ',', '\\', 'n']
Detokenized (009): ['fileIdentifier', '=', '(', 'obj_type', ',', 'obj_inst', ')', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "record_data = list ( args [ 4 : ] ) \n"
Original    (011): ['record_data', '=', 'list', '(', 'args', '[', '4', ':', ']', ')', '\\n']
Tokenized   (016): ['<s>', 'record', '_', 'data', '=', 'list', '(', 'args', '[', '4', ':', ']', ')', '\\', 'n', '</s>']
Filtered   (014): ['record', '_', 'data', '=', 'list', '(', 'args', '[', '4', ':', ']', ')', '\\', 'n']
Detokenized (011): ['record_data', '=', 'list', '(', 'args', '[', '4', ':', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "accessMethod = AtomicWriteFileRequestAccessMethodChoice ( \n"
Original    (005): ['accessMethod', '=', 'AtomicWriteFileRequestAccessMethodChoice', '(', '\\n']
Tokenized   (015): ['<s>', 'access', 'Method', '=', 'Atomic', 'Write', 'File', 'Request', 'Access', 'Method', 'Choice', '(', '\\', 'n', '</s>']
Filtered   (013): ['access', 'Method', '=', 'Atomic', 'Write', 'File', 'Request', 'Access', 'Method', 'Choice', '(', '\\', 'n']
Detokenized (005): ['accessMethod', '=', 'AtomicWriteFileRequestAccessMethodChoice', '(', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "objectIdentifier = int ( args . ini . objectidentifier ) , \n"
Original    (012): ['objectIdentifier', '=', 'int', '(', 'args', '.', 'ini', '.', 'objectidentifier', ')', ',', '\\n']
Tokenized   (020): ['<s>', 'object', 'Ident', 'ifier', '=', 'int', '(', 'args', '.', 'in', 'i', '.', 'object', 'ident', 'ifier', ')', ',', '\\', 'n', '</s>']
Filtered   (018): ['object', 'Ident', 'ifier', '=', 'int', '(', 'args', '.', 'in', 'i', '.', 'object', 'ident', 'ifier', ')', ',', '\\', 'n']
Detokenized (012): ['objectIdentifier', '=', 'int', '(', 'args', '.', 'ini', '.', 'objectidentifier', ')', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "this_application = TestApplication ( this_device , args . ini . address ) \n"
Original    (013): ['this_application', '=', 'TestApplication', '(', 'this_device', ',', 'args', '.', 'ini', '.', 'address', ')', '\\n']
Tokenized   (022): ['<s>', 'this', '_', 'application', '=', 'Test', 'Application', '(', 'this', '_', 'device', ',', 'args', '.', 'in', 'i', '.', 'address', ')', '\\', 'n', '</s>']
Filtered   (020): ['this', '_', 'application', '=', 'Test', 'Application', '(', 'this', '_', 'device', ',', 'args', '.', 'in', 'i', '.', 'address', ')', '\\', 'n']
Detokenized (013): ['this_application', '=', 'TestApplication', '(', 'this_device', ',', 'args', '.', 'ini', '.', 'address', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "_log . debug ( "running" ) \n"
Original    (007): ['_log', '.', 'debug', '(', '"running"', ')', '\\n']
Tokenized   (013): ['<s>', '_', 'log', '.', 'debug', '(', '"', 'running', '"', ')', '\\', 'n', '</s>']
Filtered   (011): ['_', 'log', '.', 'debug', '(', '"', 'running', '"', ')', '\\', 'n']
Detokenized (007): ['_log', '.', 'debug', '(', '"running"', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "Status . FAILED_TO_ADD_TO_CLIENT : % ( COLOR_FAILED_TO_ADD_TO_CLIENT , Color . ENDC ) , \n"
Original    (014): ['Status', '.', 'FAILED_TO_ADD_TO_CLIENT', ':', '%', '(', 'COLOR_FAILED_TO_ADD_TO_CLIENT', ',', 'Color', '.', 'ENDC', ')', ',', '\\n']
Tokenized   (043): ['<s>', 'Status', '.', 'FA', 'IL', 'ED', '_', 'TO', '_', 'ADD', '_', 'TO', '_', 'CL', 'IENT', ':', '%', '(', 'COL', 'OR', '_', 'FA', 'IL', 'ED', '_', 'TO', '_', 'ADD', '_', 'TO', '_', 'CL', 'IENT', ',', 'Color', '.', 'END', 'C', ')', ',', '\\', 'n', '</s>']
Filtered   (041): ['Status', '.', 'FA', 'IL', 'ED', '_', 'TO', '_', 'ADD', '_', 'TO', '_', 'CL', 'IENT', ':', '%', '(', 'COL', 'OR', '_', 'FA', 'IL', 'ED', '_', 'TO', '_', 'ADD', '_', 'TO', '_', 'CL', 'IENT', ',', 'Color', '.', 'END', 'C', ')', ',', '\\', 'n']
Detokenized (014): ['Status', '.', 'FAILED_TO_ADD_TO_CLIENT', ':', '%', '(', 'COLOR_FAILED_TO_ADD_TO_CLIENT', ',', 'Color', '.', 'ENDC', ')', ',', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "end_size += f [ ] \n"
Original    (006): ['end_size', '+=', 'f', '[', ']', '\\n']
Tokenized   (011): ['<s>', 'end', '_', 'size', '+=', 'f', '[', ']', '\\', 'n', '</s>']
Filtered   (009): ['end', '_', 'size', '+=', 'f', '[', ']', '\\', 'n']
Detokenized (006): ['end_size', '+=', 'f', '[', ']', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "files_to_check += self . db . find_hash_varying_size ( f [ ] ) \n"
Original    (013): ['files_to_check', '+=', 'self', '.', 'db', '.', 'find_hash_varying_size', '(', 'f', '[', ']', ')', '\\n']
Tokenized   (028): ['<s>', 'files', '_', 'to', '_', 'check', '+=', 'self', '.', 'db', '.', 'find', '_', 'hash', '_', 'v', 'ary', 'ing', '_', 'size', '(', 'f', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (026): ['files', '_', 'to', '_', 'check', '+=', 'self', '.', 'db', '.', 'find', '_', 'hash', '_', 'v', 'ary', 'ing', '_', 'size', '(', 'f', '[', ']', ')', '\\', 'n']
Detokenized (013): ['files_to_check', '+=', 'self', '.', 'db', '.', 'find_hash_varying_size', '(', 'f', '[', ']', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "match_start , match_end = pieces . match_file ( db_file , start_size , end_size ) \n"
Original    (015): ['match_start', ',', 'match_end', '=', 'pieces', '.', 'match_file', '(', 'db_file', ',', 'start_size', ',', 'end_size', ')', '\\n']
Tokenized   (030): ['<s>', 'match', '_', 'start', ',', 'match', '_', 'end', '=', 'pieces', '.', 'match', '_', 'file', '(', 'db', '_', 'file', ',', 'start', '_', 'size', ',', 'end', '_', 'size', ')', '\\', 'n', '</s>']
Filtered   (028): ['match', '_', 'start', ',', 'match', '_', 'end', '=', 'pieces', '.', 'match', '_', 'file', '(', 'db', '_', 'file', ',', 'start', '_', 'size', ',', 'end', '_', 'size', ')', '\\', 'n']
Detokenized (015): ['match_start', ',', 'match_end', '=', 'pieces', '.', 'match_file', '(', 'db_file', ',', 'start_size', ',', 'end_size', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "path_files [ os . path . join ( * path ) ] . append ( { \n"
Original    (017): ['path_files', '[', 'os', '.', 'path', '.', 'join', '(', '*', 'path', ')', ']', '.', 'append', '(', '{', '\\n']
Tokenized   (022): ['<s>', 'path', '_', 'files', '[', 'os', '.', 'path', '.', 'join', '(', '*', 'path', ')', ']', '.', 'append', '(', '{', '\\', 'n', '</s>']
Filtered   (020): ['path', '_', 'files', '[', 'os', '.', 'path', '.', 'join', '(', '*', 'path', ')', ']', '.', 'append', '(', '{', '\\', 'n']
Detokenized (017): ['path_files', '[', 'os', '.', 'path', '.', 'join', '(', '*', 'path', ')', ']', '.', 'append', '(', '{', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "files_sorted [ . join ( orig_path ) ] = i \n"
Original    (011): ['files_sorted', '[', '.', 'join', '(', 'orig_path', ')', ']', '=', 'i', '\\n']
Tokenized   (019): ['<s>', 'files', '_', 's', 'orted', '[', '.', 'join', '(', 'orig', '_', 'path', ')', ']', '=', 'i', '\\', 'n', '</s>']
Filtered   (017): ['files', '_', 's', 'orted', '[', '.', 'join', '(', 'orig', '_', 'path', ')', ']', '=', 'i', '\\', 'n']
Detokenized (011): ['files_sorted', '[', '.', 'join', '(', 'orig_path', ')', ']', '=', 'i', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "found_size , missing_size = 0 , 0 \n"
Original    (008): ['found_size', ',', 'missing_size', '=', '0', ',', '0', '\\n']
Tokenized   (015): ['<s>', 'found', '_', 'size', ',', 'missing', '_', 'size', '=', '0', ',', '0', '\\', 'n', '</s>']
Filtered   (013): ['found', '_', 'size', ',', 'missing', '_', 'size', '=', '0', ',', '0', '\\', 'n']
Detokenized (008): ['found_size', ',', 'missing_size', '=', '0', ',', '0', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "output_fp . write ( * write_bytes ) \n"
Original    (008): ['output_fp', '.', 'write', '(', '*', 'write_bytes', ')', '\\n']
Tokenized   (015): ['<s>', 'output', '_', 'fp', '.', 'write', '(', '*', 'write', '_', 'bytes', ')', '\\', 'n', '</s>']
Filtered   (013): ['output', '_', 'fp', '.', 'write', '(', '*', 'write', '_', 'bytes', ')', '\\', 'n']
Detokenized (008): ['output_fp', '.', 'write', '(', '*', 'write_bytes', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "bytes_written += read_bytes \n"
Original    (004): ['bytes_written', '+=', 'read_bytes', '\\n']
Tokenized   (011): ['<s>', 'bytes', '_', 'written', '+=', 'read', '_', 'bytes', '\\', 'n', '</s>']
Filtered   (009): ['bytes', '_', 'written', '+=', 'read', '_', 'bytes', '\\', 'n']
Detokenized (004): ['bytes_written', '+=', 'read_bytes', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "missing_percent = ( missing_size / ( found_size + missing_size ) ) * 100 \n"
Original    (014): ['missing_percent', '=', '(', 'missing_size', '/', '(', 'found_size', '+', 'missing_size', ')', ')', '*', '100', '\\n']
Tokenized   (025): ['<s>', 'missing', '_', 'percent', '=', '(', 'missing', '_', 'size', '/', '(', 'found', '_', 'size', '+', 'missing', '_', 'size', ')', ')', '*', '100', '\\', 'n', '</s>']
Filtered   (023): ['missing', '_', 'percent', '=', '(', 'missing', '_', 'size', '/', '(', 'found', '_', 'size', '+', 'missing', '_', 'size', ')', ')', '*', '100', '\\', 'n']
Detokenized (014): ['missing_percent', '=', '(', 'missing_size', '/', '(', 'found_size', '+', 'missing_size', ')', ')', '*', '100', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "found_percent = 100 - missing_percent \n"
Original    (006): ['found_percent', '=', '100', '-', 'missing_percent', '\\n']
Tokenized   (013): ['<s>', 'found', '_', 'percent', '=', '100', '-', 'missing', '_', 'percent', '\\', 'n', '</s>']
Filtered   (011): ['found', '_', 'percent', '=', '100', '-', 'missing', '_', 'percent', '\\', 'n']
Detokenized (006): ['found_percent', '=', '100', '-', 'missing_percent', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "would_not_add = missing_size and missing_percent > self . add_limit_percent or missing_size > \n"
Original    (013): ['would_not_add', '=', 'missing_size', 'and', 'missing_percent', '>', 'self', '.', 'add_limit_percent', 'or', 'missing_size', '>', '\\n']
Tokenized   (030): ['<s>', 'would', '_', 'not', '_', 'add', '=', 'missing', '_', 'size', 'and', 'missing', '_', 'percent', '>', 'self', '.', 'add', '_', 'limit', '_', 'percent', 'or', 'missing', '_', 'size', '>', '\\', 'n', '</s>']
Filtered   (028): ['would', '_', 'not', '_', 'add', '=', 'missing', '_', 'size', 'and', 'missing', '_', 'percent', '>', 'self', '.', 'add', '_', 'limit', '_', 'percent', 'or', 'missing', '_', 'size', '>', '\\', 'n']
Detokenized (013): ['would_not_add', '=', 'missing_size', 'and', 'missing_percent', '>', 'self', '.', 'add_limit_percent', 'or', 'missing_size', '>', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "LEGO_PALETTE = ( , , , , , , ) \n"
Original    (011): ['LEGO_PALETTE', '=', '(', ',', ',', ',', ',', ',', ',', ')', '\\n']
Tokenized   (020): ['<s>', 'LE', 'GO', '_', 'P', 'AL', 'ET', 'TE', '=', '(', ',', ',', ',', ',', ',', ',', ')', '\\', 'n', '</s>']
Filtered   (018): ['LE', 'GO', '_', 'P', 'AL', 'ET', 'TE', '=', '(', ',', ',', ',', ',', ',', ',', ')', '\\', 'n']
Detokenized (011): ['LEGO_PALETTE', '=', '(', ',', ',', ',', ',', ',', ',', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "Draft4Validator , RefResolver , create , extend , validator_for , validate , \n"
Original    (013): ['Draft4Validator', ',', 'RefResolver', ',', 'create', ',', 'extend', ',', 'validator_for', ',', 'validate', ',', '\\n']
Tokenized   (024): ['<s>', 'Draft', '4', 'Valid', 'ator', ',', 'Ref', 'Res', 'olver', ',', 'create', ',', 'extend', ',', 'valid', 'ator', '_', 'for', ',', 'validate', ',', '\\', 'n', '</s>']
Filtered   (022): ['Draft', '4', 'Valid', 'ator', ',', 'Ref', 'Res', 'olver', ',', 'create', ',', 'extend', ',', 'valid', 'ator', '_', 'for', ',', 'validate', ',', '\\', 'n']
Detokenized (013): ['Draft4Validator', ',', 'RefResolver', ',', 'create', ',', 'extend', ',', 'validator_for', ',', 'validate', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "u"enum" : [ [ "a" , "b" , "c" ] , [ "d" , "e" , "f" ] ] , \n"
Original    (021): ['u"enum"', ':', '[', '[', '"a"', ',', '"b"', ',', '"c"', ']', ',', '[', '"d"', ',', '"e"', ',', '"f"', ']', ']', ',', '\\n']
Tokenized   (039): ['<s>', 'u', '"', 'enum', '"', ':', '[', '[', '"', 'a', '"', ',', '"', 'b', '"', ',', '"', 'c', '"', ']', ',', '[', '"', 'd', '"', ',', '"', 'e', '"', ',', '"', 'f', '"', ']', ']', ',', '\\', 'n', '</s>']
Filtered   (037): ['u', '"', 'enum', '"', ':', '[', '[', '"', 'a', '"', ',', '"', 'b', '"', ',', '"', 'c', '"', ']', ',', '[', '"', 'd', '"', ',', '"', 'e', '"', ',', '"', 'f', '"', ']', ']', ',', '\\', 'n']
Detokenized (021): ['u"enum"', ':', '[', '[', '"a"', ',', '"b"', ',', '"c"', ']', ',', '[', '"d"', ',', '"e"', ',', '"f"', ']', ']', ',', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "got = ( e . message for e in self . validator . iter_errors ( instance , schema ) ) \n"
Original    (021): ['got', '=', '(', 'e', '.', 'message', 'for', 'e', 'in', 'self', '.', 'validator', '.', 'iter_errors', '(', 'instance', ',', 'schema', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'got', '=', '(', 'e', '.', 'message', 'for', 'e', 'in', 'self', '.', 'valid', 'ator', '.', 'iter', '_', 'errors', '(', 'instance', ',', 'schema', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['got', '=', '(', 'e', '.', 'message', 'for', 'e', 'in', 'self', '.', 'valid', 'ator', '.', 'iter', '_', 'errors', '(', 'instance', ',', 'schema', ')', ')', '\\', 'n']
Detokenized (021): ['got', '=', '(', 'e', '.', 'message', 'for', 'e', 'in', 'self', '.', 'validator', '.', 'iter_errors', '(', 'instance', ',', 'schema', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "checker . checks ( u"thing" ) ( check_fn ) \n"
Original    (010): ['checker', '.', 'checks', '(', 'u"thing"', ')', '(', 'check_fn', ')', '\\n']
Tokenized   (019): ['<s>', 'check', 'er', '.', 'checks', '(', 'u', '"', 'thing', '"', ')', '(', 'check', '_', 'fn', ')', '\\', 'n', '</s>']
Filtered   (017): ['check', 'er', '.', 'checks', '(', 'u', '"', 'thing', '"', ')', '(', 'check', '_', 'fn', ')', '\\', 'n']
Detokenized (010): ['checker', '.', 'checks', '(', 'u"thing"', ')', '(', 'check_fn', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "deque ( [ "type" , 1 , "properties" , "foo" , "enum" ] ) , \n"
Original    (016): ['deque', '(', '[', '"type"', ',', '1', ',', '"properties"', ',', '"foo"', ',', '"enum"', ']', ')', ',', '\\n']
Tokenized   (028): ['<s>', 'de', 'que', '(', '[', '"', 'type', '"', ',', '1', ',', '"', 'properties', '"', ',', '"', 'foo', '"', ',', '"', 'enum', '"', ']', ')', ',', '\\', 'n', '</s>']
Filtered   (026): ['de', 'que', '(', '[', '"', 'type', '"', ',', '1', ',', '"', 'properties', '"', ',', '"', 'foo', '"', ',', '"', 'enum', '"', ']', ')', ',', '\\', 'n']
Detokenized (016): ['deque', '(', '[', '"type"', ',', '1', ',', '"properties"', ',', '"foo"', ',', '"enum"', ']', ')', ',', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : ""baz" : { "minItems" : 2 } , \n"
Original    (009): ['"baz"', ':', '{', '"minItems"', ':', '2', '}', ',', '\\n']
Tokenized   (018): ['<s>', '"', 'b', 'az', '"', ':', '{', '"', 'min', 'Items', '"', ':', '2', '}', ',', '\\', 'n', '</s>']
Filtered   (016): ['"', 'b', 'az', '"', ':', '{', '"', 'min', 'Items', '"', ':', '2', '}', ',', '\\', 'n']
Detokenized (009): ['"baz"', ':', '{', '"minItems"', ':', '2', '}', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ""required" : [ "root" ] , \n"
Original    (007): ['"required"', ':', '[', '"root"', ']', ',', '\\n']
Tokenized   (014): ['<s>', '"', 'required', '"', ':', '[', '"', 'root', '"', ']', ',', '\\', 'n', '</s>']
Filtered   (012): ['"', 'required', '"', ':', '[', '"', 'root', '"', ']', ',', '\\', 'n']
Detokenized (007): ['"required"', ':', '[', '"root"', ']', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "e2 . absolute_schema_path , deque ( \n"
Original    (007): ['e2', '.', 'absolute_schema_path', ',', 'deque', '(', '\\n']
Tokenized   (017): ['<s>', 'e', '2', '.', 'absolute', '_', 'sche', 'ma', '_', 'path', ',', 'de', 'que', '(', '\\', 'n', '</s>']
Filtered   (015): ['e', '2', '.', 'absolute', '_', 'sche', 'ma', '_', 'path', ',', 'de', 'que', '(', '\\', 'n']
Detokenized (007): ['e2', '.', 'absolute_schema_path', ',', 'deque', '(', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""additionalProperties" : { "type" : "integer" , "minimum" : 5 } \n"
Original    (012): ['"additionalProperties"', ':', '{', '"type"', ':', '"integer"', ',', '"minimum"', ':', '5', '}', '\\n']
Tokenized   (026): ['<s>', '"', 'add', 'itional', 'Pro', 'perties', '"', ':', '{', '"', 'type', '"', ':', '"', 'integer', '"', ',', '"', 'minimum', '"', ':', '5', '}', '\\', 'n', '</s>']
Filtered   (024): ['"', 'add', 'itional', 'Pro', 'perties', '"', ':', '{', '"', 'type', '"', ':', '"', 'integer', '"', ',', '"', 'minimum', '"', ':', '5', '}', '\\', 'n']
Detokenized (012): ['"additionalProperties"', ':', '{', '"type"', ':', '"integer"', ',', '"minimum"', ':', '5', '}', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : ""bar" : { "type" : "string" } , \n"
Original    (009): ['"bar"', ':', '{', '"type"', ':', '"string"', '}', ',', '\\n']
Tokenized   (018): ['<s>', '"', 'bar', '"', ':', '{', '"', 'type', '"', ':', '"', 'string', '"', '}', ',', '\\', 'n', '</s>']
Filtered   (016): ['"', 'bar', '"', ':', '{', '"', 'type', '"', ':', '"', 'string', '"', '}', ',', '\\', 'n']
Detokenized (009): ['"bar"', ':', '{', '"type"', ':', '"string"', '}', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ""foo" : { "minimum" : 5 } \n"
Original    (008): ['"foo"', ':', '{', '"minimum"', ':', '5', '}', '\\n']
Tokenized   (015): ['<s>', '"', 'foo', '"', ':', '{', '"', 'minimum', '"', ':', '5', '}', '\\', 'n', '</s>']
Filtered   (013): ['"', 'foo', '"', ':', '{', '"', 'minimum', '"', ':', '5', '}', '\\', 'n']
Detokenized (008): ['"foo"', ':', '{', '"minimum"', ':', '5', '}', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : ""items" : [ { } ] , \n"
Original    (008): ['"items"', ':', '[', '{', '}', ']', ',', '\\n']
Tokenized   (013): ['<s>', '"', 'items', '"', ':', '[', '{', '}', ']', ',', '\\', 'n', '</s>']
Filtered   (011): ['"', 'items', '"', ':', '[', '{', '}', ']', ',', '\\', 'n']
Detokenized (008): ['"items"', ':', '[', '{', '}', ']', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "validate ( instance = instance , schema = { my_property : my_value } ) \n"
Original    (015): ['validate', '(', 'instance', '=', 'instance', ',', 'schema', '=', '{', 'my_property', ':', 'my_value', '}', ')', '\\n']
Tokenized   (023): ['<s>', 'valid', 'ate', '(', 'instance', '=', 'instance', ',', 'schema', '=', '{', 'my', '_', 'property', ':', 'my', '_', 'value', '}', ')', '\\', 'n', '</s>']
Filtered   (021): ['valid', 'ate', '(', 'instance', '=', 'instance', ',', 'schema', '=', '{', 'my', '_', 'property', ':', 'my', '_', 'value', '}', ')', '\\', 'n']
Detokenized (015): ['validate', '(', 'instance', '=', 'instance', ',', 'schema', '=', '{', 'my_property', ':', 'my_value', '}', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "chk_schema . assert_called_once_with ( { } ) \n"
Original    (008): ['chk_schema', '.', 'assert_called_once_with', '(', '{', '}', ')', '\\n']
Tokenized   (021): ['<s>', 'ch', 'k', '_', 'sche', 'ma', '.', 'assert', '_', 'called', '_', 'once', '_', 'with', '(', '{', '}', ')', '\\', 'n', '</s>']
Filtered   (019): ['ch', 'k', '_', 'sche', 'ma', '.', 'assert', '_', 'called', '_', 'once', '_', 'with', '(', '{', '}', ')', '\\', 'n']
Detokenized (008): ['chk_schema', '.', 'assert_called_once_with', '(', '{', '}', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "stored_schema = { "stored" : "schema" } \n"
Original    (008): ['stored_schema', '=', '{', '"stored"', ':', '"schema"', '}', '\\n']
Tokenized   (021): ['<s>', 'st', 'ored', '_', 'sche', 'ma', '=', '{', '"', 'st', 'ored', '"', ':', '"', 'sche', 'ma', '"', '}', '\\', 'n', '</s>']
Filtered   (019): ['st', 'ored', '_', 'sche', 'ma', '=', '{', '"', 'st', 'ored', '"', ':', '"', 'sche', 'ma', '"', '}', '\\', 'n']
Detokenized (008): ['stored_schema', '=', '{', '"stored"', ':', '"schema"', '}', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : ""ports" : \n"
Original    (003): ['"ports"', ':', '\\n']
Tokenized   (008): ['<s>', '"', 'ports', '"', ':', '\\', 'n', '</s>']
Filtered   (006): ['"', 'ports', '"', ':', '\\', 'n']
Detokenized (003): ['"ports"', ':', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "l2Report . generateReport ( pod . id , True , False ) \n"
Original    (013): ['l2Report', '.', 'generateReport', '(', 'pod', '.', 'id', ',', 'True', ',', 'False', ')', '\\n']
Tokenized   (019): ['<s>', 'l', '2', 'Report', '.', 'generate', 'Report', '(', 'pod', '.', 'id', ',', 'True', ',', 'False', ')', '\\', 'n', '</s>']
Filtered   (017): ['l', '2', 'Report', '.', 'generate', 'Report', '(', 'pod', '.', 'id', ',', 'True', ',', 'False', ')', '\\', 'n']
Detokenized (013): ['l2Report', '.', 'generateReport', '(', 'pod', '.', 'id', ',', 'True', ',', 'False', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "_YAML_ = splitext ( __file__ ) [ 0 ] + \n"
Original    (011): ['_YAML_', '=', 'splitext', '(', '__file__', ')', '[', '0', ']', '+', '\\n']
Tokenized   (022): ['<s>', '_', 'Y', 'AM', 'L', '_', '=', 'spl', 'ite', 'xt', '(', '__', 'file', '__', ')', '[', '0', ']', '+', '\\', 'n', '</s>']
Filtered   (020): ['_', 'Y', 'AM', 'L', '_', '=', 'spl', 'ite', 'xt', '(', '__', 'file', '__', ')', '[', '0', ']', '+', '\\', 'n']
Detokenized (011): ['_YAML_', '=', 'splitext', '(', '__file__', ')', '[', '0', ']', '+', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "globals ( ) . update ( loadyaml ( _YAML_ ) ) \n"
Original    (012): ['globals', '(', ')', '.', 'update', '(', 'loadyaml', '(', '_YAML_', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'gl', 'ob', 'als', '(', ')', '.', 'update', '(', 'load', 'y', 'aml', '(', '_', 'Y', 'AM', 'L', '_', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['gl', 'ob', 'als', '(', ')', '.', 'update', '(', 'load', 'y', 'aml', '(', '_', 'Y', 'AM', 'L', '_', ')', ')', '\\', 'n']
Detokenized (012): ['globals', '(', ')', '.', 'update', '(', 'loadyaml', '(', '_YAML_', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "gather_facts = False ) \n"
Original    (005): ['gather_facts', '=', 'False', ')', '\\n']
Tokenized   (011): ['<s>', 'g', 'ather', '_', 'facts', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (009): ['g', 'ather', '_', 'facts', '=', 'False', ')', '\\', 'n']
Detokenized (005): ['gather_facts', '=', 'False', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "DEFAULT_API_URLS = ( , \n"
Original    (005): ['DEFAULT_API_URLS', '=', '(', ',', '\\n']
Tokenized   (014): ['<s>', 'DE', 'FAULT', '_', 'API', '_', 'UR', 'LS', '=', '(', ',', '\\', 'n', '</s>']
Filtered   (012): ['DE', 'FAULT', '_', 'API', '_', 'UR', 'LS', '=', '(', ',', '\\', 'n']
Detokenized (005): ['DEFAULT_API_URLS', '=', '(', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "BAD_STATUS_CODES = [ , , , \n"
Original    (007): ['BAD_STATUS_CODES', '=', '[', ',', ',', ',', '\\n']
Tokenized   (018): ['<s>', 'B', 'AD', '_', 'STAT', 'US', '_', 'C', 'OD', 'ES', '=', '[', ',', ',', ',', '\\', 'n', '</s>']
Filtered   (016): ['B', 'AD', '_', 'STAT', 'US', '_', 'C', 'OD', 'ES', '=', '[', ',', ',', ',', '\\', 'n']
Detokenized (007): ['BAD_STATUS_CODES', '=', '[', ',', ',', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "translate_otp = True , api_urls = DEFAULT_API_URLS , \n"
Original    (009): ['translate_otp', '=', 'True', ',', 'api_urls', '=', 'DEFAULT_API_URLS', ',', '\\n']
Tokenized   (025): ['<s>', 'trans', 'late', '_', 'ot', 'p', '=', 'True', ',', 'api', '_', 'url', 's', '=', 'DE', 'FAULT', '_', 'API', '_', 'UR', 'LS', ',', '\\', 'n', '</s>']
Filtered   (023): ['trans', 'late', '_', 'ot', 'p', '=', 'True', ',', 'api', '_', 'url', 's', '=', 'DE', 'FAULT', '_', 'API', '_', 'UR', 'LS', ',', '\\', 'n']
Detokenized (009): ['translate_otp', '=', 'True', ',', 'api_urls', '=', 'DEFAULT_API_URLS', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "rand_str = b ( os . urandom ( 30 ) ) \n"
Original    (012): ['rand_str', '=', 'b', '(', 'os', '.', 'urandom', '(', '30', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'rand', '_', 'str', '=', 'b', '(', 'os', '.', 'ur', 'andom', '(', '30', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['rand', '_', 'str', '=', 'b', '(', 'os', '.', 'ur', 'andom', '(', '30', ')', ')', '\\', 'n']
Detokenized (012): ['rand_str', '=', 'b', '(', 'os', '.', 'urandom', '(', '30', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "nonce = base64 . b64encode ( rand_str , b ( ) ) [ : 25 ] . decode ( ) \n"
Original    (021): ['nonce', '=', 'base64', '.', 'b64encode', '(', 'rand_str', ',', 'b', '(', ')', ')', '[', ':', '25', ']', '.', 'decode', '(', ')', '\\n']
Tokenized   (031): ['<s>', 'non', 'ce', '=', 'base', '64', '.', 'b', '64', 'en', 'code', '(', 'rand', '_', 'str', ',', 'b', '(', ')', ')', '[', ':', '25', ']', '.', 'decode', '(', ')', '\\', 'n', '</s>']
Filtered   (029): ['non', 'ce', '=', 'base', '64', '.', 'b', '64', 'en', 'code', '(', 'rand', '_', 'str', ',', 'b', '(', ')', ')', '[', ':', '25', ']', '.', 'decode', '(', ')', '\\', 'n']
Detokenized (021): ['nonce', '=', 'base64', '.', 'b64encode', '(', 'rand_str', ',', 'b', '(', ')', ')', '[', ':', '25', ']', '.', 'decode', '(', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "otp . otp , nonce , \n"
Original    (007): ['otp', '.', 'otp', ',', 'nonce', ',', '\\n']
Tokenized   (013): ['<s>', 'ot', 'p', '.', 'ot', 'p', ',', 'non', 'ce', ',', '\\', 'n', '</s>']
Filtered   (011): ['ot', 'p', '.', 'ot', 'p', ',', 'non', 'ce', ',', '\\', 'n']
Detokenized (007): ['otp', '.', 'otp', ',', 'nonce', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "pairs_string = . join ( [ . join ( pair ) for pair in pairs_sorted ] ) \n"
Original    (018): ['pairs_string', '=', '.', 'join', '(', '[', '.', 'join', '(', 'pair', ')', 'for', 'pair', 'in', 'pairs_sorted', ']', ')', '\\n']
Tokenized   (027): ['<s>', 'p', 'airs', '_', 'string', '=', '.', 'join', '(', '[', '.', 'join', '(', 'pair', ')', 'for', 'pair', 'in', 'pairs', '_', 's', 'orted', ']', ')', '\\', 'n', '</s>']
Filtered   (025): ['p', 'airs', '_', 'string', '=', '.', 'join', '(', '[', '.', 'join', '(', 'pair', ')', 'for', 'pair', 'in', 'pairs', '_', 's', 'orted', ']', ')', '\\', 'n']
Detokenized (018): ['pairs_string', '=', '.', 'join', '(', '[', '.', 'join', '(', 'pair', ')', 'for', 'pair', 'in', 'pairs_sorted', ']', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "digest = hmac . new ( self . key , b ( pairs_string ) , hashlib . sha1 ) . digest ( ) \n"
Original    (024): ['digest', '=', 'hmac', '.', 'new', '(', 'self', '.', 'key', ',', 'b', '(', 'pairs_string', ')', ',', 'hashlib', '.', 'sha1', ')', '.', 'digest', '(', ')', '\\n']
Tokenized   (034): ['<s>', 'dig', 'est', '=', 'h', 'mac', '.', 'new', '(', 'self', '.', 'key', ',', 'b', '(', 'pairs', '_', 'string', ')', ',', 'hash', 'lib', '.', 'sh', 'a', '1', ')', '.', 'digest', '(', ')', '\\', 'n', '</s>']
Filtered   (032): ['dig', 'est', '=', 'h', 'mac', '.', 'new', '(', 'self', '.', 'key', ',', 'b', '(', 'pairs', '_', 'string', ')', ',', 'hash', 'lib', '.', 'sh', 'a', '1', ')', '.', 'digest', '(', ')', '\\', 'n']
Detokenized (024): ['digest', '=', 'hmac', '.', 'new', '(', 'self', '.', 'key', ',', 'b', '(', 'pairs_string', ')', ',', 'hashlib', '.', 'sha1', ')', '.', 'digest', '(', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "signature = ( [ unquote ( v ) for k , v in pairs if k == ] or [ None ] ) [ 0 ] \n"
Original    (027): ['signature', '=', '(', '[', 'unquote', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '==', ']', 'or', '[', 'None', ']', ')', '[', '0', ']', '\\n']
Tokenized   (032): ['<s>', 'sign', 'ature', '=', '(', '[', 'un', 'quote', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '==', ']', 'or', '[', 'None', ']', ')', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (030): ['sign', 'ature', '=', '(', '[', 'un', 'quote', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '==', ']', 'or', '[', 'None', ']', ')', '[', '0', ']', '\\', 'n']
Detokenized (027): ['signature', '=', '(', '[', 'unquote', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '==', ']', 'or', '[', 'None', ']', ')', '[', '0', ']', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "query_string = . join ( [ k + + v for k , v in pairs if k != ] ) \n"
Original    (022): ['query_string', '=', '.', 'join', '(', '[', 'k', '+', '+', 'v', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '!=', ']', ')', '\\n']
Tokenized   (027): ['<s>', 'query', '_', 'string', '=', '.', 'join', '(', '[', 'k', '+', '+', 'v', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '!=', ']', ')', '\\', 'n', '</s>']
Filtered   (025): ['query', '_', 'string', '=', '.', 'join', '(', '[', 'k', '+', '+', 'v', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '!=', ']', ')', '\\', 'n']
Detokenized (022): ['query_string', '=', '.', 'join', '(', '[', 'k', '+', '+', 'v', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '!=', ']', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "pairs = ( x . split ( , 1 ) for x in query_string . split ( ) ) \n"
Original    (020): ['pairs', '=', '(', 'x', '.', 'split', '(', ',', '1', ')', 'for', 'x', 'in', 'query_string', '.', 'split', '(', ')', ')', '\\n']
Tokenized   (026): ['<s>', 'p', 'airs', '=', '(', 'x', '.', 'split', '(', ',', '1', ')', 'for', 'x', 'in', 'query', '_', 'string', '.', 'split', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['p', 'airs', '=', '(', 'x', '.', 'split', '(', ',', '1', ')', 'for', 'x', 'in', 'query', '_', 'string', '.', 'split', '(', ')', ')', '\\', 'n']
Detokenized (020): ['pairs', '=', '(', 'x', '.', 'split', '(', ',', '1', ')', 'for', 'x', 'in', 'query_string', '.', 'split', '(', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "py_modules = [ ] , \n"
Original    (006): ['py_modules', '=', '[', ']', ',', '\\n']
Tokenized   (011): ['<s>', 'py', '_', 'modules', '=', '[', ']', ',', '\\', 'n', '</s>']
Filtered   (009): ['py', '_', 'modules', '=', '[', ']', ',', '\\', 'n']
Detokenized (006): ['py_modules', '=', '[', ']', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "submitter , msg = result [ 0 ] \n"
Original    (009): ['submitter', ',', 'msg', '=', 'result', '[', '0', ']', '\\n']
Tokenized   (013): ['<s>', 'sub', 'mitter', ',', 'msg', '=', 'result', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (011): ['sub', 'mitter', ',', 'msg', '=', 'result', '[', '0', ']', '\\', 'n']
Detokenized (009): ['submitter', ',', 'msg', '=', 'result', '[', '0', ']', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "contact = self . line_interface . _get_contact_by_id ( me . id ) \n"
Original    (013): ['contact', '=', 'self', '.', 'line_interface', '.', '_get_contact_by_id', '(', 'me', '.', 'id', ')', '\\n']
Tokenized   (025): ['<s>', 'contact', '=', 'self', '.', 'line', '_', 'interface', '.', '_', 'get', '_', 'contact', '_', 'by', '_', 'id', '(', 'me', '.', 'id', ')', '\\', 'n', '</s>']
Filtered   (023): ['contact', '=', 'self', '.', 'line', '_', 'interface', '.', '_', 'get', '_', 'contact', '_', 'by', '_', 'id', '(', 'me', '.', 'id', ')', '\\', 'n']
Detokenized (013): ['contact', '=', 'self', '.', 'line_interface', '.', '_get_contact_by_id', '(', 'me', '.', 'id', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "ok_ ( me_display_name == me . name ) \n"
Original    (009): ['ok_', '(', 'me_display_name', '==', 'me', '.', 'name', ')', '\\n']
Tokenized   (017): ['<s>', 'ok', '_', '(', 'me', '_', 'display', '_', 'name', '==', 'me', '.', 'name', ')', '\\', 'n', '</s>']
Filtered   (015): ['ok', '_', '(', 'me', '_', 'display', '_', 'name', '==', 'me', '.', 'name', ')', '\\', 'n']
Detokenized (009): ['ok_', '(', 'me_display_name', '==', 'me', '.', 'name', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "transport . get_extra_info . return_value = None \n"
Original    (008): ['transport', '.', 'get_extra_info', '.', 'return_value', '=', 'None', '\\n']
Tokenized   (018): ['<s>', 'trans', 'port', '.', 'get', '_', 'extra', '_', 'info', '.', 'return', '_', 'value', '=', 'None', '\\', 'n', '</s>']
Filtered   (016): ['trans', 'port', '.', 'get', '_', 'extra', '_', 'info', '.', 'return', '_', 'value', '=', 'None', '\\', 'n']
Detokenized (008): ['transport', '.', 'get_extra_info', '.', 'return_value', '=', 'None', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "ShortenerSettings = namedtuple ( , [ \n"
Original    (007): ['ShortenerSettings', '=', 'namedtuple', '(', ',', '[', '\\n']
Tokenized   (014): ['<s>', 'Short', 'ener', 'Settings', '=', 'named', 't', 'uple', '(', ',', '[', '\\', 'n', '</s>']
Filtered   (012): ['Short', 'ener', 'Settings', '=', 'named', 't', 'uple', '(', ',', '[', '\\', 'n']
Detokenized (007): ['ShortenerSettings', '=', 'namedtuple', '(', ',', '[', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "right_to_left = [ , ] , \n"
Original    (007): ['right_to_left', '=', '[', ',', ']', ',', '\\n']
Tokenized   (014): ['<s>', 'right', '_', 'to', '_', 'left', '=', '[', ',', ']', ',', '\\', 'n', '</s>']
Filtered   (012): ['right', '_', 'to', '_', 'left', '=', '[', ',', ']', ',', '\\', 'n']
Detokenized (007): ['right_to_left', '=', '[', ',', ']', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "shortener = { } , \n"
Original    (006): ['shortener', '=', '{', '}', ',', '\\n']
Tokenized   (010): ['<s>', 'short', 'ener', '=', '{', '}', ',', '\\', 'n', '</s>']
Filtered   (008): ['short', 'ener', '=', '{', '}', ',', '\\', 'n']
Detokenized (006): ['shortener', '=', '{', '}', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "workers_pool = 10 , \n"
Original    (005): ['workers_pool', '=', '10', ',', '\\n']
Tokenized   (010): ['<s>', 'workers', '_', 'pool', '=', '10', ',', '\\', 'n', '</s>']
Filtered   (008): ['workers', '_', 'pool', '=', '10', ',', '\\', 'n']
Detokenized (005): ['workers_pool', '=', '10', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "cms_service_host = "http://localhost:5001" \n"
Original    (004): ['cms_service_host', '=', '"http://localhost:5001"', '\\n']
Tokenized   (018): ['<s>', 'cms', '_', 'service', '_', 'host', '=', '"', 'http', '://', 'localhost', ':', '500', '1', '"', '\\', 'n', '</s>']
Filtered   (016): ['cms', '_', 'service', '_', 'host', '=', '"', 'http', '://', 'localhost', ':', '500', '1', '"', '\\', 'n']
Detokenized (004): ['cms_service_host', '=', '"http://localhost:5001"', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "subparsers = args . add_subparsers ( help = , dest = ) \n"
Original    (013): ['subparsers', '=', 'args', '.', 'add_subparsers', '(', 'help', '=', ',', 'dest', '=', ')', '\\n']
Tokenized   (024): ['<s>', 'sub', 'p', 'ars', 'ers', '=', 'args', '.', 'add', '_', 'sub', 'p', 'ars', 'ers', '(', 'help', '=', ',', 'dest', '=', ')', '\\', 'n', '</s>']
Filtered   (022): ['sub', 'p', 'ars', 'ers', '=', 'args', '.', 'add', '_', 'sub', 'p', 'ars', 'ers', '(', 'help', '=', ',', 'dest', '=', ')', '\\', 'n']
Detokenized (013): ['subparsers', '=', 'args', '.', 'add_subparsers', '(', 'help', '=', ',', 'dest', '=', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "template_parser . add_argument ( , \n"
Original    (006): ['template_parser', '.', 'add_argument', '(', ',', '\\n']
Tokenized   (013): ['<s>', 'template', '_', 'parser', '.', 'add', '_', 'argument', '(', ',', '\\', 'n', '</s>']
Filtered   (011): ['template', '_', 'parser', '.', 'add', '_', 'argument', '(', ',', '\\', 'n']
Detokenized (006): ['template_parser', '.', 'add_argument', '(', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "config_parser . add_argument ( , help = ) \n"
Original    (009): ['config_parser', '.', 'add_argument', '(', ',', 'help', '=', ')', '\\n']
Tokenized   (016): ['<s>', 'config', '_', 'parser', '.', 'add', '_', 'argument', '(', ',', 'help', '=', ')', '\\', 'n', '</s>']
Filtered   (014): ['config', '_', 'parser', '.', 'add', '_', 'argument', '(', ',', 'help', '=', ')', '\\', 'n']
Detokenized (009): ['config_parser', '.', 'add_argument', '(', ',', 'help', '=', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "gui_parser . add_argument ( , , type = str , help = ) \n"
Original    (014): ['gui_parser', '.', 'add_argument', '(', ',', ',', 'type', '=', 'str', ',', 'help', '=', ')', '\\n']
Tokenized   (021): ['<s>', 'gui', '_', 'parser', '.', 'add', '_', 'argument', '(', ',', ',', 'type', '=', 'str', ',', 'help', '=', ')', '\\', 'n', '</s>']
Filtered   (019): ['gui', '_', 'parser', '.', 'add', '_', 'argument', '(', ',', ',', 'type', '=', 'str', ',', 'help', '=', ')', '\\', 'n']
Detokenized (014): ['gui_parser', '.', 'add_argument', '(', ',', ',', 'type', '=', 'str', ',', 'help', '=', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "changePwdResult = conn . extend . microsoft . modify_password ( user_dn , newpassword ) \n"
Original    (015): ['changePwdResult', '=', 'conn', '.', 'extend', '.', 'microsoft', '.', 'modify_password', '(', 'user_dn', ',', 'newpassword', ')', '\\n']
Tokenized   (027): ['<s>', 'change', 'P', 'wd', 'Result', '=', 'conn', '.', 'extend', '.', 'micro', 'soft', '.', 'modify', '_', 'password', '(', 'user', '_', 'dn', ',', 'new', 'password', ')', '\\', 'n', '</s>']
Filtered   (025): ['change', 'P', 'wd', 'Result', '=', 'conn', '.', 'extend', '.', 'micro', 'soft', '.', 'modify', '_', 'password', '(', 'user', '_', 'dn', ',', 'new', 'password', ')', '\\', 'n']
Detokenized (015): ['changePwdResult', '=', 'conn', '.', 'extend', '.', 'microsoft', '.', 'modify_password', '(', 'user_dn', ',', 'newpassword', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "cap_path = os . path . join ( caps_directory , ) \n"
Original    (012): ['cap_path', '=', 'os', '.', 'path', '.', 'join', '(', 'caps_directory', ',', ')', '\\n']
Tokenized   (019): ['<s>', 'cap', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'caps', '_', 'directory', ',', ')', '\\', 'n', '</s>']
Filtered   (017): ['cap', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'caps', '_', 'directory', ',', ')', '\\', 'n']
Detokenized (012): ['cap_path', '=', 'os', '.', 'path', '.', 'join', '(', 'caps_directory', ',', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "cap . eventloop . stop ( ) \n"
Original    (008): ['cap', '.', 'eventloop', '.', 'stop', '(', ')', '\\n']
Tokenized   (012): ['<s>', 'cap', '.', 'event', 'loop', '.', 'stop', '(', ')', '\\', 'n', '</s>']
Filtered   (010): ['cap', '.', 'event', 'loop', '.', 'stop', '(', ')', '\\', 'n']
Detokenized (008): ['cap', '.', 'eventloop', '.', 'stop', '(', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "flush = Service ( name = , \n"
Original    (008): ['flush', '=', 'Service', '(', 'name', '=', ',', '\\n']
Tokenized   (011): ['<s>', 'flush', '=', 'Service', '(', 'name', '=', ',', '\\', 'n', '</s>']
Filtered   (009): ['flush', '=', 'Service', '(', 'name', '=', ',', '\\', 'n']
Detokenized (008): ['flush', '=', 'Service', '(', 'name', '=', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "sourceIds = [ d [ ] for d in response . json ] \n"
Original    (014): ['sourceIds', '=', '[', 'd', '[', ']', 'for', 'd', 'in', 'response', '.', 'json', ']', '\\n']
Tokenized   (019): ['<s>', 'source', 'Id', 's', '=', '[', 'd', '[', ']', 'for', 'd', 'in', 'response', '.', 'json', ']', '\\', 'n', '</s>']
Filtered   (017): ['source', 'Id', 's', '=', '[', 'd', '[', ']', 'for', 'd', 'in', 'response', '.', 'json', ']', '\\', 'n']
Detokenized (014): ['sourceIds', '=', '[', 'd', '[', ']', 'for', 'd', 'in', 'response', '.', 'json', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : ""folder." ) \n"
Original    (003): ['"folder."', ')', '\\n']
Tokenized   (008): ['<s>', '"', 'folder', '."', ')', '\\', 'n', '</s>']
Filtered   (006): ['"', 'folder', '."', ')', '\\', 'n']
Detokenized (003): ['"folder."', ')', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "minerva_metadata [ ] = { \n"
Original    (006): ['minerva_metadata', '[', ']', '=', '{', '\\n']
Tokenized   (012): ['<s>', 'min', 'erva', '_', 'metadata', '[', ']', '=', '{', '\\', 'n', '</s>']
Filtered   (010): ['min', 'erva', '_', 'metadata', '[', ']', '=', '{', '\\', 'n']
Detokenized (006): ['minerva_metadata', '[', ']', '=', '{', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "Description ( ) \n"
Original    (004): ['Description', '(', ')', '\\n']
Tokenized   (007): ['<s>', 'Description', '(', ')', '\\', 'n', '</s>']
Filtered   (005): ['Description', '(', ')', '\\', 'n']
Detokenized (004): ['Description', '(', ')', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "matches = re . findall ( "(\'|\\")(\\S+)(\'|\\")" , text ) \n"
Original    (011): ['matches', '=', 're', '.', 'findall', '(', '"(\\\'|\\\\")(\\\\S+)(\\\'|\\\\")"', ',', 'text', ')', '\\n']
Tokenized   (030): ['<s>', 'mat', 'ches', '=', 're', '.', 'find', 'all', '(', '"(', "\\'", '|', '\\\\', '")', '(', '\\\\', 'S', '+', ')(', "\\'", '|', '\\\\', '")', '"', ',', 'text', ')', '\\', 'n', '</s>']
Filtered   (028): ['mat', 'ches', '=', 're', '.', 'find', 'all', '(', '"(', "\\'", '|', '\\\\', '")', '(', '\\\\', 'S', '+', ')(', "\\'", '|', '\\\\', '")', '"', ',', 'text', ')', '\\', 'n']
Detokenized (011): ['matches', '=', 're', '.', 'findall', '(', '"(\\\'|\\\\")(\\\\S+)(\\\'|\\\\")"', ',', 'text', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "package_data = { : [ ] } , \n"
Original    (009): ['package_data', '=', '{', ':', '[', ']', '}', ',', '\\n']
Tokenized   (014): ['<s>', 'package', '_', 'data', '=', '{', ':', '[', ']', '}', ',', '\\', 'n', '</s>']
Filtered   (012): ['package', '_', 'data', '=', '{', ':', '[', ']', '}', ',', '\\', 'n']
Detokenized (009): ['package_data', '=', '{', ':', '[', ']', '}', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "list_permissions = [ , , , ] \n"
Original    (008): ['list_permissions', '=', '[', ',', ',', ',', ']', '\\n']
Tokenized   (014): ['<s>', 'list', '_', 'per', 'missions', '=', '[', ',', ',', ',', ']', '\\', 'n', '</s>']
Filtered   (012): ['list', '_', 'per', 'missions', '=', '[', ',', ',', ',', ']', '\\', 'n']
Detokenized (008): ['list_permissions', '=', '[', ',', ',', ',', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "option_list = BaseCommand . option_list + ( \n"
Original    (008): ['option_list', '=', 'BaseCommand', '.', 'option_list', '+', '(', '\\n']
Tokenized   (016): ['<s>', 'option', '_', 'list', '=', 'Base', 'Command', '.', 'option', '_', 'list', '+', '(', '\\', 'n', '</s>']
Filtered   (014): ['option', '_', 'list', '=', 'Base', 'Command', '.', 'option', '_', 'list', '+', '(', '\\', 'n']
Detokenized (008): ['option_list', '=', 'BaseCommand', '.', 'option_list', '+', '(', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "make_option ( , \n"
Original    (004): ['make_option', '(', ',', '\\n']
Tokenized   (009): ['<s>', 'make', '_', 'option', '(', ',', '\\', 'n', '</s>']
Filtered   (007): ['make', '_', 'option', '(', ',', '\\', 'n']
Detokenized (004): ['make_option', '(', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "confirm_token = Column ( Unicode ( 100 ) ) \n"
Original    (010): ['confirm_token', '=', 'Column', '(', 'Unicode', '(', '100', ')', ')', '\\n']
Tokenized   (016): ['<s>', 'conf', 'irm', '_', 'token', '=', 'Column', '(', 'Unicode', '(', '100', ')', ')', '\\', 'n', '</s>']
Filtered   (014): ['conf', 'irm', '_', 'token', '=', 'Column', '(', 'Unicode', '(', '100', ')', ')', '\\', 'n']
Detokenized (010): ['confirm_token', '=', 'Column', '(', 'Unicode', '(', '100', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "creation_date = Column ( DateTime ( ) , nullable = False ) \n"
Original    (013): ['creation_date', '=', 'Column', '(', 'DateTime', '(', ')', ',', 'nullable', '=', 'False', ')', '\\n']
Tokenized   (020): ['<s>', 'creation', '_', 'date', '=', 'Column', '(', 'Date', 'Time', '(', ')', ',', 'null', 'able', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (018): ['creation', '_', 'date', '=', 'Column', '(', 'Date', 'Time', '(', ')', ',', 'null', 'able', '=', 'False', ')', '\\', 'n']
Detokenized (013): ['creation_date', '=', 'Column', '(', 'DateTime', '(', ')', ',', 'nullable', '=', 'False', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "last_login_date = Column ( DateTime ( ) ) \n"
Original    (009): ['last_login_date', '=', 'Column', '(', 'DateTime', '(', ')', ')', '\\n']
Tokenized   (017): ['<s>', 'last', '_', 'login', '_', 'date', '=', 'Column', '(', 'Date', 'Time', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (015): ['last', '_', 'login', '_', 'date', '=', 'Column', '(', 'Date', 'Time', '(', ')', ')', '\\', 'n']
Detokenized (009): ['last_login_date', '=', 'Column', '(', 'DateTime', '(', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "SHARING_ROLES = [ , , ] \n"
Original    (007): ['SHARING_ROLES', '=', '[', ',', ',', ']', '\\n']
Tokenized   (016): ['<s>', 'SH', 'AR', 'ING', '_', 'R', 'OL', 'ES', '=', '[', ',', ',', ']', '\\', 'n', '</s>']
Filtered   (014): ['SH', 'AR', 'ING', '_', 'R', 'OL', 'ES', '=', '[', ',', ',', ']', '\\', 'n']
Detokenized (007): ['SHARING_ROLES', '=', '[', ',', ',', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "USER_MANAGEMENT_ROLES = SHARING_ROLES + [ ] \n"
Original    (007): ['USER_MANAGEMENT_ROLES', '=', 'SHARING_ROLES', '+', '[', ']', '\\n']
Tokenized   (023): ['<s>', 'USER', '_', 'MAN', 'AG', 'EMENT', '_', 'R', 'OL', 'ES', '=', 'SHAR', 'ING', '_', 'R', 'OL', 'ES', '+', '[', ']', '\\', 'n', '</s>']
Filtered   (021): ['USER', '_', 'MAN', 'AG', 'EMENT', '_', 'R', 'OL', 'ES', '=', 'SHAR', 'ING', '_', 'R', 'OL', 'ES', '+', '[', ']', '\\', 'n']
Detokenized (007): ['USER_MANAGEMENT_ROLES', '=', 'SHARING_ROLES', '+', '[', ']', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "_DEFAULT_SHARING_ROLES = SHARING_ROLES [ : ] \n"
Original    (007): ['_DEFAULT_SHARING_ROLES', '=', 'SHARING_ROLES', '[', ':', ']', '\\n']
Tokenized   (025): ['<s>', '_', 'DE', 'FAULT', '_', 'SH', 'AR', 'ING', '_', 'R', 'OL', 'ES', '=', 'SHAR', 'ING', '_', 'R', 'OL', 'ES', '[', ':', ']', '\\', 'n', '</s>']
Filtered   (023): ['_', 'DE', 'FAULT', '_', 'SH', 'AR', 'ING', '_', 'R', 'OL', 'ES', '=', 'SHAR', 'ING', '_', 'R', 'OL', 'ES', '[', ':', ']', '\\', 'n']
Detokenized (007): ['_DEFAULT_SHARING_ROLES', '=', 'SHARING_ROLES', '[', ':', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "principal = get_principals ( ) . get ( name ) \n"
Original    (011): ['principal', '=', 'get_principals', '(', ')', '.', 'get', '(', 'name', ')', '\\n']
Tokenized   (021): ['<s>', 'pr', 'inc', 'ipal', '=', 'get', '_', 'pr', 'inc', 'ip', 'als', '(', ')', '.', 'get', '(', 'name', ')', '\\', 'n', '</s>']
Filtered   (019): ['pr', 'inc', 'ipal', '=', 'get', '_', 'pr', 'inc', 'ip', 'als', '(', ')', '.', 'get', '(', 'name', ')', '\\', 'n']
Detokenized (011): ['principal', '=', 'get_principals', '(', ')', '.', 'get', '(', 'name', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "lg for lg in context . local_groups \n"
Original    (008): ['lg', 'for', 'lg', 'in', 'context', '.', 'local_groups', '\\n']
Tokenized   (015): ['<s>', 'l', 'g', 'for', 'l', 'g', 'in', 'context', '.', 'local', '_', 'groups', '\\', 'n', '</s>']
Filtered   (013): ['l', 'g', 'for', 'l', 'g', 'in', 'context', '.', 'local', '_', 'groups', '\\', 'n']
Detokenized (008): ['lg', 'for', 'lg', 'in', 'context', '.', 'local_groups', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "LocalGroup ( context , name , unicode ( group_name ) ) \n"
Original    (012): ['LocalGroup', '(', 'context', ',', 'name', ',', 'unicode', '(', 'group_name', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'Local', 'Group', '(', 'context', ',', 'name', ',', 'unic', 'ode', '(', 'group', '_', 'name', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['Local', 'Group', '(', 'context', ',', 'name', ',', 'unic', 'ode', '(', 'group', '_', 'name', ')', ')', '\\', 'n']
Detokenized (012): ['LocalGroup', '(', 'context', ',', 'name', ',', 'unicode', '(', 'group_name', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "filters . append ( func . lower ( col ) . like ( value ) ) \n"
Original    (017): ['filters', '.', 'append', '(', 'func', '.', 'lower', '(', 'col', ')', '.', 'like', '(', 'value', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'fil', 'ters', '.', 'append', '(', 'func', '.', 'lower', '(', 'col', ')', '.', 'like', '(', 'value', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['fil', 'ters', '.', 'append', '(', 'func', '.', 'lower', '(', 'col', ')', '.', 'like', '(', 'value', ')', ')', '\\', 'n']
Detokenized (017): ['filters', '.', 'append', '(', 'func', '.', 'lower', '(', 'col', ')', '.', 'like', '(', 'value', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "bcrypt . hashpw ( password . encode ( ) , hashed . encode ( ) ) ) \n"
Original    (018): ['bcrypt', '.', 'hashpw', '(', 'password', '.', 'encode', '(', ')', ',', 'hashed', '.', 'encode', '(', ')', ')', ')', '\\n']
Tokenized   (025): ['<s>', 'bc', 'rypt', '.', 'hash', 'p', 'w', '(', 'password', '.', 'encode', '(', ')', ',', 'has', 'hed', '.', 'encode', '(', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (023): ['bc', 'rypt', '.', 'hash', 'p', 'w', '(', 'password', '.', 'encode', '(', ')', ',', 'has', 'hed', '.', 'encode', '(', ')', ')', ')', '\\', 'n']
Detokenized (018): ['bcrypt', '.', 'hashpw', '(', 'password', '.', 'encode', '(', ')', ',', 'hashed', '.', 'encode', '(', ')', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "browser . open ( . format ( BASE_URL ) ) \n"
Original    (011): ['browser', '.', 'open', '(', '.', 'format', '(', 'BASE_URL', ')', ')', '\\n']
Tokenized   (016): ['<s>', 'browser', '.', 'open', '(', '.', 'format', '(', 'BASE', '_', 'URL', ')', ')', '\\', 'n', '</s>']
Filtered   (014): ['browser', '.', 'open', '(', '.', 'format', '(', 'BASE', '_', 'URL', ')', ')', '\\', 'n']
Detokenized (011): ['browser', '.', 'open', '(', '.', 'format', '(', 'BASE_URL', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "AUTHORS = open ( os . path . join ( here , ) ) . read ( ) \n"
Original    (019): ['AUTHORS', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ')', '.', 'read', '(', ')', '\\n']
Tokenized   (024): ['<s>', 'A', 'UTH', 'ORS', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ')', '.', 'read', '(', ')', '\\', 'n', '</s>']
Filtered   (022): ['A', 'UTH', 'ORS', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ')', '.', 'read', '(', ')', '\\', 'n']
Detokenized (019): ['AUTHORS', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ')', '.', 'read', '(', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "fixed_boxes ) : \n"
Original    (004): ['fixed_boxes', ')', ':', '\\n']
Tokenized   (009): ['<s>', 'fixed', '_', 'boxes', ')', ':', '\\', 'n', '</s>']
Filtered   (007): ['fixed', '_', 'boxes', ')', ':', '\\', 'n']
Detokenized (004): ['fixed_boxes', ')', ':', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "resolve_percentages ( box , ( containing_block . width , containing_block . height ) ) \n"
Original    (015): ['resolve_percentages', '(', 'box', ',', '(', 'containing_block', '.', 'width', ',', 'containing_block', '.', 'height', ')', ')', '\\n']
Tokenized   (026): ['<s>', 'res', 'olve', '_', 'percent', 'ages', '(', 'box', ',', '(', 'containing', '_', 'block', '.', 'width', ',', 'containing', '_', 'block', '.', 'height', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['res', 'olve', '_', 'percent', 'ages', '(', 'box', ',', '(', 'containing', '_', 'block', '.', 'width', ',', 'containing', '_', 'block', '.', 'height', ')', ')', '\\', 'n']
Detokenized (015): ['resolve_percentages', '(', 'box', ',', '(', 'containing_block', '.', 'width', ',', 'containing_block', '.', 'height', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "box , _ , _ , _ , _ = block_container_layout ( \n"
Original    (013): ['box', ',', '_', ',', '_', ',', '_', ',', '_', '=', 'block_container_layout', '(', '\\n']
Tokenized   (020): ['<s>', 'box', ',', '_', ',', '_', ',', '_', ',', '_', '=', 'block', '_', 'container', '_', 'layout', '(', '\\', 'n', '</s>']
Filtered   (018): ['box', ',', '_', ',', '_', ',', '_', ',', '_', '=', 'block', '_', 'container', '_', 'layout', '(', '\\', 'n']
Detokenized (013): ['box', ',', '_', ',', '_', ',', '_', ',', '_', '=', 'block_container_layout', '(', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "skip_stack = None , device_size = device_size , page_is_empty = False , \n"
Original    (013): ['skip_stack', '=', 'None', ',', 'device_size', '=', 'device_size', ',', 'page_is_empty', '=', 'False', ',', '\\n']
Tokenized   (026): ['<s>', 'skip', '_', 'stack', '=', 'None', ',', 'device', '_', 'size', '=', 'device', '_', 'size', ',', 'page', '_', 'is', '_', 'empty', '=', 'False', ',', '\\', 'n', '</s>']
Filtered   (024): ['skip', '_', 'stack', '=', 'None', ',', 'device', '_', 'size', '=', 'device', '_', 'size', ',', 'page', '_', 'is', '_', 'empty', '=', 'False', ',', '\\', 'n']
Detokenized (013): ['skip_stack', '=', 'None', ',', 'device_size', '=', 'device_size', ',', 'page_is_empty', '=', 'False', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "list_marker_layout ( context , box ) \n"
Original    (007): ['list_marker_layout', '(', 'context', ',', 'box', ')', '\\n']
Tokenized   (015): ['<s>', 'list', '_', 'mark', 'er', '_', 'layout', '(', 'context', ',', 'box', ')', '\\', 'n', '</s>']
Filtered   (013): ['list', '_', 'mark', 'er', '_', 'layout', '(', 'context', ',', 'box', ')', '\\', 'n']
Detokenized (007): ['list_marker_layout', '(', 'context', ',', 'box', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "hypothetical_position = box . position_y + collapsed_margin \n"
Original    (008): ['hypothetical_position', '=', 'box', '.', 'position_y', '+', 'collapsed_margin', '\\n']
Tokenized   (019): ['<s>', 'hyp', 'ot', 'hetical', '_', 'position', '=', 'box', '.', 'position', '_', 'y', '+', 'collapsed', '_', 'margin', '\\', 'n', '</s>']
Filtered   (017): ['hyp', 'ot', 'hetical', '_', 'position', '=', 'box', '.', 'position', '_', 'y', '+', 'collapsed', '_', 'margin', '\\', 'n']
Detokenized (008): ['hypothetical_position', '=', 'box', '.', 'position_y', '+', 'collapsed_margin', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "box_width = box . margin_width ( ) if outer else box . border_width ( ) \n"
Original    (016): ['box_width', '=', 'box', '.', 'margin_width', '(', ')', 'if', 'outer', 'else', 'box', '.', 'border_width', '(', ')', '\\n']
Tokenized   (025): ['<s>', 'box', '_', 'width', '=', 'box', '.', 'margin', '_', 'width', '(', ')', 'if', 'outer', 'else', 'box', '.', 'border', '_', 'width', '(', ')', '\\', 'n', '</s>']
Filtered   (023): ['box', '_', 'width', '=', 'box', '.', 'margin', '_', 'width', '(', ')', 'if', 'outer', 'else', 'box', '.', 'border', '_', 'width', '(', ')', '\\', 'n']
Detokenized (016): ['box_width', '=', 'box', '.', 'margin_width', '(', ')', 'if', 'outer', 'else', 'box', '.', 'border_width', '(', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "max_right_bound -= box . margin_right \n"
Original    (006): ['max_right_bound', '-=', 'box', '.', 'margin_right', '\\n']
Tokenized   (015): ['<s>', 'max', '_', 'right', '_', 'bound', '-=', 'box', '.', 'margin', '_', 'right', '\\', 'n', '</s>']
Filtered   (013): ['max', '_', 'right', '_', 'bound', '-=', 'box', '.', 'margin', '_', 'right', '\\', 'n']
Detokenized (006): ['max_right_bound', '-=', 'box', '.', 'margin_right', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "shape . position_y + shape . margin_height ( ) \n"
Original    (010): ['shape', '.', 'position_y', '+', 'shape', '.', 'margin_height', '(', ')', '\\n']
Tokenized   (017): ['<s>', 'shape', '.', 'position', '_', 'y', '+', 'shape', '.', 'margin', '_', 'height', '(', ')', '\\', 'n', '</s>']
Filtered   (015): ['shape', '.', 'position', '_', 'y', '+', 'shape', '.', 'margin', '_', 'height', '(', ')', '\\', 'n']
Detokenized (010): ['shape', '.', 'position_y', '+', 'shape', '.', 'margin_height', '(', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "urlpatterns = patterns ( , \n"
Original    (006): ['urlpatterns', '=', 'patterns', '(', ',', '\\n']
Tokenized   (011): ['<s>', 'url', 'pattern', 's', '=', 'patterns', '(', ',', '\\', 'n', '</s>']
Filtered   (009): ['url', 'pattern', 's', '=', 'patterns', '(', ',', '\\', 'n']
Detokenized (006): ['urlpatterns', '=', 'patterns', '(', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "obj1 , obj2 = qs \n"
Original    (006): ['obj1', ',', 'obj2', '=', 'qs', '\\n']
Tokenized   (012): ['<s>', 'obj', '1', ',', 'obj', '2', '=', 'q', 's', '\\', 'n', '</s>']
Filtered   (010): ['obj', '1', ',', 'obj', '2', '=', 'q', 's', '\\', 'n']
Detokenized (006): ['obj1', ',', 'obj2', '=', 'qs', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "n1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n"
Original    (022): ['n1', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'pk', '=', 'self', '.', 'normal_id', '[', '1', ']', ')', '\\n']
Tokenized   (029): ['<s>', 'n', '1', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'p', 'k', '=', 'self', '.', 'normal', '_', 'id', '[', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (027): ['n', '1', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'p', 'k', '=', 'self', '.', 'normal', '_', 'id', '[', '1', ']', ')', '\\', 'n']
Detokenized (022): ['n1', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'pk', '=', 'self', '.', 'normal_id', '[', '1', ']', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "shared_field = NEW_SHARED , translated_field = NEW_TRANSLATED \n"
Original    (008): ['shared_field', '=', 'NEW_SHARED', ',', 'translated_field', '=', 'NEW_TRANSLATED', '\\n']
Tokenized   (024): ['<s>', 'shared', '_', 'field', '=', 'NEW', '_', 'SH', 'AR', 'ED', ',', 'translated', '_', 'field', '=', 'NEW', '_', 'TR', 'AN', 'SL', 'ATED', '\\', 'n', '</s>']
Filtered   (022): ['shared', '_', 'field', '=', 'NEW', '_', 'SH', 'AR', 'ED', ',', 'translated', '_', 'field', '=', 'NEW', '_', 'TR', 'AN', 'SL', 'ATED', '\\', 'n']
Detokenized (008): ['shared_field', '=', 'NEW_SHARED', ',', 'translated_field', '=', 'NEW_TRANSLATED', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "NORMAL [ 2 ] . shared_field ] ) \n"
Original    (009): ['NORMAL', '[', '2', ']', '.', 'shared_field', ']', ')', '\\n']
Tokenized   (015): ['<s>', 'NOR', 'MAL', '[', '2', ']', '.', 'shared', '_', 'field', ']', ')', '\\', 'n', '</s>']
Filtered   (013): ['NOR', 'MAL', '[', '2', ']', '.', 'shared', '_', 'field', ']', ')', '\\', 'n']
Detokenized (009): ['NORMAL', '[', '2', ']', '.', 'shared_field', ']', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "ja = Normal . objects . language ( ) . get ( pk = en . pk ) \n"
Original    (019): ['ja', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'pk', '=', 'en', '.', 'pk', ')', '\\n']
Tokenized   (024): ['<s>', 'ja', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'p', 'k', '=', 'en', '.', 'p', 'k', ')', '\\', 'n', '</s>']
Filtered   (022): ['ja', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'p', 'k', '=', 'en', '.', 'p', 'k', ')', '\\', 'n']
Detokenized (019): ['ja', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'pk', '=', 'en', '.', 'pk', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "AggregateModel . objects . language ( "en" ) . create ( number = 0 , translated_number = 0 ) \n"
Original    (020): ['AggregateModel', '.', 'objects', '.', 'language', '(', '"en"', ')', '.', 'create', '(', 'number', '=', '0', ',', 'translated_number', '=', '0', ')', '\\n']
Tokenized   (029): ['<s>', 'Agg', 'regate', 'Model', '.', 'objects', '.', 'language', '(', '"', 'en', '"', ')', '.', 'create', '(', 'number', '=', '0', ',', 'translated', '_', 'number', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (027): ['Agg', 'regate', 'Model', '.', 'objects', '.', 'language', '(', '"', 'en', '"', ')', '.', 'create', '(', 'number', '=', '0', ',', 'translated', '_', 'number', '=', '0', ')', '\\', 'n']
Detokenized (020): ['AggregateModel', '.', 'objects', '.', 'language', '(', '"en"', ')', '.', 'create', '(', 'number', '=', '0', ',', 'translated_number', '=', '0', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "shared_contains_two = Q ( shared_field__contains = ) \n"
Original    (008): ['shared_contains_two', '=', 'Q', '(', 'shared_field__contains', '=', ')', '\\n']
Tokenized   (021): ['<s>', 'shared', '_', 'cont', 'ains', '_', 'two', '=', 'Q', '(', 'shared', '_', 'field', '__', 'cont', 'ains', '=', ')', '\\', 'n', '</s>']
Filtered   (019): ['shared', '_', 'cont', 'ains', '_', 'two', '=', 'Q', '(', 'shared', '_', 'field', '__', 'cont', 'ains', '=', ')', '\\', 'n']
Detokenized (008): ['shared_contains_two', '=', 'Q', '(', 'shared_field__contains', '=', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "normal_one = Q ( normal_field = STANDARD [ 1 ] . normal_field ) \n"
Original    (014): ['normal_one', '=', 'Q', '(', 'normal_field', '=', 'STANDARD', '[', '1', ']', '.', 'normal_field', ')', '\\n']
Tokenized   (024): ['<s>', 'normal', '_', 'one', '=', 'Q', '(', 'normal', '_', 'field', '=', 'STAND', 'ARD', '[', '1', ']', '.', 'normal', '_', 'field', ')', '\\', 'n', '</s>']
Filtered   (022): ['normal', '_', 'one', '=', 'Q', '(', 'normal', '_', 'field', '=', 'STAND', 'ARD', '[', '1', ']', '.', 'normal', '_', 'field', ')', '\\', 'n']
Detokenized (014): ['normal_one', '=', 'Q', '(', 'normal_field', '=', 'STANDARD', '[', '1', ']', '.', 'normal_field', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "shared_one = Q ( normal__shared_field = NORMAL [ STANDARD [ 1 ] . normal ] . shared_field ) \n"
Original    (019): ['shared_one', '=', 'Q', '(', 'normal__shared_field', '=', 'NORMAL', '[', 'STANDARD', '[', '1', ']', '.', 'normal', ']', '.', 'shared_field', ')', '\\n']
Tokenized   (032): ['<s>', 'shared', '_', 'one', '=', 'Q', '(', 'normal', '__', 'shared', '_', 'field', '=', 'NOR', 'MAL', '[', 'STAND', 'ARD', '[', '1', ']', '.', 'normal', ']', '.', 'shared', '_', 'field', ')', '\\', 'n', '</s>']
Filtered   (030): ['shared', '_', 'one', '=', 'Q', '(', 'normal', '__', 'shared', '_', 'field', '=', 'NOR', 'MAL', '[', 'STAND', 'ARD', '[', '1', ']', '.', 'normal', ']', '.', 'shared', '_', 'field', ')', '\\', 'n']
Detokenized (019): ['shared_one', '=', 'Q', '(', 'normal__shared_field', '=', 'NORMAL', '[', 'STANDARD', '[', '1', ']', '.', 'normal', ']', '.', 'shared_field', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "translated_one_en = Q ( normal__translated_field = NORMAL [ STANDARD [ 1 ] . normal ] . translated_field [ translated_two_en = Q ( normal__translated_field = NORMAL [ STANDARD [ 2 ] . normal ] . translated_field [ \n"
Original    (037): ['translated_one_en', '=', 'Q', '(', 'normal__translated_field', '=', 'NORMAL', '[', 'STANDARD', '[', '1', ']', '.', 'normal', ']', '.', 'translated_field', '[', 'translated_two_en', '=', 'Q', '(', 'normal__translated_field', '=', 'NORMAL', '[', 'STANDARD', '[', '2', ']', '.', 'normal', ']', '.', 'translated_field', '[', '\\n']
Tokenized   (067): ['<s>', 'trans', 'lated', '_', 'one', '_', 'en', '=', 'Q', '(', 'normal', '__', 'trans', 'lated', '_', 'field', '=', 'NOR', 'MAL', '[', 'STAND', 'ARD', '[', '1', ']', '.', 'normal', ']', '.', 'translated', '_', 'field', '[', 'translated', '_', 'two', '_', 'en', '=', 'Q', '(', 'normal', '__', 'trans', 'lated', '_', 'field', '=', 'NOR', 'MAL', '[', 'STAND', 'ARD', '[', '2', ']', '.', 'normal', ']', '.', 'translated', '_', 'field', '[', '\\', 'n', '</s>']
Filtered   (065): ['trans', 'lated', '_', 'one', '_', 'en', '=', 'Q', '(', 'normal', '__', 'trans', 'lated', '_', 'field', '=', 'NOR', 'MAL', '[', 'STAND', 'ARD', '[', '1', ']', '.', 'normal', ']', '.', 'translated', '_', 'field', '[', 'translated', '_', 'two', '_', 'en', '=', 'Q', '(', 'normal', '__', 'trans', 'lated', '_', 'field', '=', 'NOR', 'MAL', '[', 'STAND', 'ARD', '[', '2', ']', '.', 'normal', ']', '.', 'translated', '_', 'field', '[', '\\', 'n']
Detokenized (037): ['translated_one_en', '=', 'Q', '(', 'normal__translated_field', '=', 'NORMAL', '[', 'STANDARD', '[', '1', ']', '.', 'normal', ']', '.', 'translated_field', '[', 'translated_two_en', '=', 'Q', '(', 'normal__translated_field', '=', 'NORMAL', '[', 'STANDARD', '[', '2', ']', '.', 'normal', ']', '.', 'translated_field', '[', '\\n']
Counter: 65
===================================================================
Hidden states:  (13, 37, 768)
# Extracted words:  37
Sentence         : "qs = manager . filter ( shared_one & ~ translated_two_en ) \n"
Original    (012): ['qs', '=', 'manager', '.', 'filter', '(', 'shared_one', '&', '~', 'translated_two_en', ')', '\\n']
Tokenized   (021): ['<s>', 'qs', '=', 'manager', '.', 'filter', '(', 'shared', '_', 'one', '&', '~', 'translated', '_', 'two', '_', 'en', ')', '\\', 'n', '</s>']
Filtered   (019): ['qs', '=', 'manager', '.', 'filter', '(', 'shared', '_', 'one', '&', '~', 'translated', '_', 'two', '_', 'en', ')', '\\', 'n']
Detokenized (012): ['qs', '=', 'manager', '.', 'filter', '(', 'shared_one', '&', '~', 'translated_two_en', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "Normal . objects . language ( ) . complex_filter , \n"
Original    (011): ['Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'complex_filter', ',', '\\n']
Tokenized   (016): ['<s>', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'complex', '_', 'filter', ',', '\\', 'n', '</s>']
Filtered   (014): ['Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'complex', '_', 'filter', ',', '\\', 'n']
Detokenized (011): ['Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'complex_filter', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "analytics . track ( user_id , "Activate" , { \n"
Original    (010): ['analytics', '.', 'track', '(', 'user_id', ',', '"Activate"', ',', '{', '\\n']
Tokenized   (019): ['<s>', 'analy', 'tics', '.', 'track', '(', 'user', '_', 'id', ',', '"', 'Activ', 'ate', '"', ',', '{', '\\', 'n', '</s>']
Filtered   (017): ['analy', 'tics', '.', 'track', '(', 'user', '_', 'id', ',', '"', 'Activ', 'ate', '"', ',', '{', '\\', 'n']
Detokenized (010): ['analytics', '.', 'track', '(', 'user_id', ',', '"Activate"', ',', '{', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sublime_plugin . WindowCommand . __init__ ( self , * args , ** kwargs ) \n"
Original    (015): ['sublime_plugin', '.', 'WindowCommand', '.', '__init__', '(', 'self', ',', '*', 'args', ',', '**', 'kwargs', ')', '\\n']
Tokenized   (026): ['<s>', 'sub', 'lime', '_', 'plugin', '.', 'Window', 'Command', '.', '__', 'init', '__', '(', 'self', ',', '*', 'args', ',', '**', 'k', 'w', 'args', ')', '\\', 'n', '</s>']
Filtered   (024): ['sub', 'lime', '_', 'plugin', '.', 'Window', 'Command', '.', '__', 'init', '__', '(', 'self', ',', '*', 'args', ',', '**', 'k', 'w', 'args', ')', '\\', 'n']
Detokenized (015): ['sublime_plugin', '.', 'WindowCommand', '.', '__init__', '(', 'self', ',', '*', 'args', ',', '**', 'kwargs', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : ""show_response" , { "title" : title , "text" : text } ) \n"
Original    (013): ['"show_response"', ',', '{', '"title"', ':', 'title', ',', '"text"', ':', 'text', '}', ')', '\\n']
Tokenized   (024): ['<s>', '"', 'show', '_', 'response', '"', ',', '{', '"', 'title', '"', ':', 'title', ',', '"', 'text', '"', ':', 'text', '}', ')', '\\', 'n', '</s>']
Filtered   (022): ['"', 'show', '_', 'response', '"', ',', '{', '"', 'title', '"', ':', 'title', ',', '"', 'text', '"', ':', 'text', '}', ')', '\\', 'n']
Detokenized (013): ['"show_response"', ',', '{', '"title"', ':', 'title', ',', '"text"', ':', 'text', '}', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "ensure_ascii = False \n"
Original    (004): ['ensure_ascii', '=', 'False', '\\n']
Tokenized   (012): ['<s>', 'ens', 'ure', '_', 'as', 'ci', 'i', '=', 'False', '\\', 'n', '</s>']
Filtered   (010): ['ens', 'ure', '_', 'as', 'ci', 'i', '=', 'False', '\\', 'n']
Detokenized (004): ['ensure_ascii', '=', 'False', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "syntax = "Packages/JavaScript/JSON.tmLanguage" ) \n"
Original    (005): ['syntax', '=', '"Packages/JavaScript/JSON.tmLanguage"', ')', '\\n']
Tokenized   (020): ['<s>', 'sy', 'ntax', '=', '"', 'Pack', 'ages', '/', 'Java', 'Script', '/', 'JSON', '.', 'tm', 'Language', '"', ')', '\\', 'n', '</s>']
Filtered   (018): ['sy', 'ntax', '=', '"', 'Pack', 'ages', '/', 'Java', 'Script', '/', 'JSON', '.', 'tm', 'Language', '"', ')', '\\', 'n']
Detokenized (005): ['syntax', '=', '"Packages/JavaScript/JSON.tmLanguage"', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "scroll = self . settings . scroll_size \n"
Original    (008): ['scroll', '=', 'self', '.', 'settings', '.', 'scroll_size', '\\n']
Tokenized   (013): ['<s>', 'scroll', '=', 'self', '.', 'settings', '.', 'scroll', '_', 'size', '\\', 'n', '</s>']
Filtered   (011): ['scroll', '=', 'self', '.', 'settings', '.', 'scroll', '_', 'size', '\\', 'n']
Detokenized (008): ['scroll', '=', 'self', '.', 'settings', '.', 'scroll_size', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : ""show_panel" , { "panel" : "output.elasticsearch" } ) \n"
Original    (009): ['"show_panel"', ',', '{', '"panel"', ':', '"output.elasticsearch"', '}', ')', '\\n']
Tokenized   (024): ['<s>', '"', 'show', '_', 'panel', '"', ',', '{', '"', 'panel', '"', ':', '"', 'output', '.', 'el', 'astic', 'search', '"', '}', ')', '\\', 'n', '</s>']
Filtered   (022): ['"', 'show', '_', 'panel', '"', ',', '{', '"', 'panel', '"', ':', '"', 'output', '.', 'el', 'astic', 'search', '"', '}', ')', '\\', 'n']
Detokenized (009): ['"show_panel"', ',', '{', '"panel"', ':', '"output.elasticsearch"', '}', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "panel . set_read_only ( True ) \n"
Original    (007): ['panel', '.', 'set_read_only', '(', 'True', ')', '\\n']
Tokenized   (014): ['<s>', 'panel', '.', 'set', '_', 'read', '_', 'only', '(', 'True', ')', '\\', 'n', '</s>']
Filtered   (012): ['panel', '.', 'set', '_', 'read', '_', 'only', '(', 'True', ')', '\\', 'n']
Detokenized (007): ['panel', '.', 'set_read_only', '(', 'True', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "400 : RequestError , \n"
Original    (005): ['400', ':', 'RequestError', ',', '\\n']
Tokenized   (009): ['<s>', '400', ':', 'Request', 'Error', ',', '\\', 'n', '</s>']
Filtered   (007): ['400', ':', 'Request', 'Error', ',', '\\', 'n']
Detokenized (005): ['400', ':', 'RequestError', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "TestConfigFileSource . ConcreteConfigFileSource ) \n"
Original    (005): ['TestConfigFileSource', '.', 'ConcreteConfigFileSource', ')', '\\n']
Tokenized   (015): ['<s>', 'Test', 'Config', 'File', 'Source', '.', 'Con', 'crete', 'Config', 'File', 'Source', ')', '\\', 'n', '</s>']
Filtered   (013): ['Test', 'Config', 'File', 'Source', '.', 'Con', 'crete', 'Config', 'File', 'Source', ')', '\\', 'n']
Detokenized (005): ['TestConfigFileSource', '.', 'ConcreteConfigFileSource', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "ReferenceReachabilityTester . TwoWayScopeReferenceAttacher . attach ( self . _scope_tree ) \n"
Original    (011): ['ReferenceReachabilityTester', '.', 'TwoWayScopeReferenceAttacher', '.', 'attach', '(', 'self', '.', '_scope_tree', ')', '\\n']
Tokenized   (027): ['<s>', 'Reference', 'Re', 'ach', 'ability', 'T', 'ester', '.', 'Two', 'Way', 'Scope', 'Reference', 'Att', 'acher', '.', 'attach', '(', 'self', '.', '_', 'scope', '_', 'tree', ')', '\\', 'n', '</s>']
Filtered   (025): ['Reference', 'Re', 'ach', 'ability', 'T', 'ester', '.', 'Two', 'Way', 'Scope', 'Reference', 'Att', 'acher', '.', 'attach', '(', 'self', '.', '_', 'scope', '_', 'tree', ')', '\\', 'n']
Detokenized (011): ['ReferenceReachabilityTester', '.', 'TwoWayScopeReferenceAttacher', '.', 'attach', '(', 'self', '.', '_scope_tree', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "declaring_id_node [ REFERECED_FLAG ] = True \n"
Original    (007): ['declaring_id_node', '[', 'REFERECED_FLAG', ']', '=', 'True', '\\n']
Tokenized   (020): ['<s>', 'decl', 'aring', '_', 'id', '_', 'node', '[', 'RE', 'FER', 'EC', 'ED', '_', 'FLAG', ']', '=', 'True', '\\', 'n', '</s>']
Filtered   (018): ['decl', 'aring', '_', 'id', '_', 'node', '[', 'RE', 'FER', 'EC', 'ED', '_', 'FLAG', ']', '=', 'True', '\\', 'n']
Detokenized (007): ['declaring_id_node', '[', 'REFERECED_FLAG', ']', '=', 'True', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "node_type = NodeType ( node [ ] ) \n"
Original    (009): ['node_type', '=', 'NodeType', '(', 'node', '[', ']', ')', '\\n']
Tokenized   (015): ['<s>', 'node', '_', 'type', '=', 'Node', 'Type', '(', 'node', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (013): ['node', '_', 'type', '=', 'Node', 'Type', '(', 'node', '[', ']', ')', '\\', 'n']
Detokenized (009): ['node_type', '=', 'NodeType', '(', 'node', '[', ']', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "is_set_cmd = excmd_node [ ] [ ] . get ( ) in SetCommandFamily \n"
Original    (014): ['is_set_cmd', '=', 'excmd_node', '[', ']', '[', ']', '.', 'get', '(', ')', 'in', 'SetCommandFamily', '\\n']
Tokenized   (026): ['<s>', 'is', '_', 'set', '_', 'cmd', '=', 'exc', 'md', '_', 'node', '[', ']', '[', ']', '.', 'get', '(', ')', 'in', 'Set', 'Command', 'Family', '\\', 'n', '</s>']
Filtered   (024): ['is', '_', 'set', '_', 'cmd', '=', 'exc', 'md', '_', 'node', '[', ']', '[', ']', '.', 'get', '(', ')', 'in', 'Set', 'Command', 'Family', '\\', 'n']
Detokenized (014): ['is_set_cmd', '=', 'excmd_node', '[', ']', '[', ']', '.', 'get', '(', ')', 'in', 'SetCommandFamily', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "option_name = re . match ( , option_expr ) . group ( 0 ) \n"
Original    (015): ['option_name', '=', 're', '.', 'match', '(', ',', 'option_expr', ')', '.', 'group', '(', '0', ')', '\\n']
Tokenized   (022): ['<s>', 'option', '_', 'name', '=', 're', '.', 'match', '(', ',', 'option', '_', 'expr', ')', '.', 'group', '(', '0', ')', '\\', 'n', '</s>']
Filtered   (020): ['option', '_', 'name', '=', 're', '.', 'match', '(', ',', 'option', '_', 'expr', ')', '.', 'group', '(', '0', ')', '\\', 'n']
Detokenized (015): ['option_name', '=', 're', '.', 'match', '(', ',', 'option_expr', ')', '.', 'group', '(', '0', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "is_valid = option_name not in AbbreviationsIncludingInvertPrefix \n"
Original    (007): ['is_valid', '=', 'option_name', 'not', 'in', 'AbbreviationsIncludingInvertPrefix', '\\n']
Tokenized   (023): ['<s>', 'is', '_', 'valid', '=', 'option', '_', 'name', 'not', 'in', 'Ab', 'bre', 'vi', 'ations', 'In', 'cluding', 'In', 'vert', 'Pref', 'ix', '\\', 'n', '</s>']
Filtered   (021): ['is', '_', 'valid', '=', 'option', '_', 'name', 'not', 'in', 'Ab', 'bre', 'vi', 'ations', 'In', 'cluding', 'In', 'vert', 'Pref', 'ix', '\\', 'n']
Detokenized (007): ['is_valid', '=', 'option_name', 'not', 'in', 'AbbreviationsIncludingInvertPrefix', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "stderr . setFormatter ( logging . Formatter ( \n"
Original    (009): ['stderr', '.', 'setFormatter', '(', 'logging', '.', 'Formatter', '(', '\\n']
Tokenized   (017): ['<s>', 'st', 'der', 'r', '.', 'set', 'Form', 'atter', '(', 'logging', '.', 'Form', 'atter', '(', '\\', 'n', '</s>']
Filtered   (015): ['st', 'der', 'r', '.', 'set', 'Form', 'atter', '(', 'logging', '.', 'Form', 'atter', '(', '\\', 'n']
Detokenized (009): ['stderr', '.', 'setFormatter', '(', 'logging', '.', 'Formatter', '(', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "level = level if level else os . environ . get ( , ) \n"
Original    (015): ['level', '=', 'level', 'if', 'level', 'else', 'os', '.', 'environ', '.', 'get', '(', ',', ')', '\\n']
Tokenized   (019): ['<s>', 'level', '=', 'level', 'if', 'level', 'else', 'os', '.', 'en', 'viron', '.', 'get', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (017): ['level', '=', 'level', 'if', 'level', 'else', 'os', '.', 'en', 'viron', '.', 'get', '(', ',', ')', '\\', 'n']
Detokenized (015): ['level', '=', 'level', 'if', 'level', 'else', 'os', '.', 'environ', '.', 'get', '(', ',', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "g_s = g0 * g_c * g_R * g_D * g_T * g_M \n"
Original    (014): ['g_s', '=', 'g0', '*', 'g_c', '*', 'g_R', '*', 'g_D', '*', 'g_T', '*', 'g_M', '\\n']
Tokenized   (030): ['<s>', 'g', '_', 's', '=', 'g', '0', '*', 'g', '_', 'c', '*', 'g', '_', 'R', '*', 'g', '_', 'D', '*', 'g', '_', 'T', '*', 'g', '_', 'M', '\\', 'n', '</s>']
Filtered   (028): ['g', '_', 's', '=', 'g', '0', '*', 'g', '_', 'c', '*', 'g', '_', 'R', '*', 'g', '_', 'D', '*', 'g', '_', 'T', '*', 'g', '_', 'M', '\\', 'n']
Detokenized (014): ['g_s', '=', 'g0', '*', 'g_c', '*', 'g_R', '*', 'g_D', '*', 'g_T', '*', 'g_M', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "g_T = ( ( TK - TL ) * ( TH - TK ) ** alpha_T ) / ( ( T0 - TL ) * ( TH - T0 ) ** alpha_T ) \n"
Original    (034): ['g_T', '=', '(', '(', 'TK', '-', 'TL', ')', '*', '(', 'TH', '-', 'TK', ')', '**', 'alpha_T', ')', '/', '(', '(', 'T0', '-', 'TL', ')', '*', '(', 'TH', '-', 'T0', ')', '**', 'alpha_T', ')', '\\n']
Tokenized   (047): ['<s>', 'g', '_', 'T', '=', '(', '(', 'T', 'K', '-', 'TL', ')', '*', '(', 'TH', '-', 'T', 'K', ')', '**', 'alpha', '_', 'T', ')', '/', '(', '(', 'T', '0', '-', 'TL', ')', '*', '(', 'TH', '-', 'T', '0', ')', '**', 'alpha', '_', 'T', ')', '\\', 'n', '</s>']
Filtered   (045): ['g', '_', 'T', '=', '(', '(', 'T', 'K', '-', 'TL', ')', '*', '(', 'TH', '-', 'T', 'K', ')', '**', 'alpha', '_', 'T', ')', '/', '(', '(', 'T', '0', '-', 'TL', ')', '*', '(', 'TH', '-', 'T', '0', ')', '**', 'alpha', '_', 'T', ')', '\\', 'n']
Detokenized (034): ['g_T', '=', '(', '(', 'TK', '-', 'TL', ')', '*', '(', 'TH', '-', 'TK', ')', '**', 'alpha_T', ')', '/', '(', '(', 'T0', '-', 'TL', ')', '*', '(', 'TH', '-', 'T0', ')', '**', 'alpha_T', ')', '\\n']
Counter: 45
===================================================================
Hidden states:  (13, 34, 768)
# Extracted words:  34
Sentence         : "r_a = AeroReist ( um , zm , z0 , d ) \n"
Original    (013): ['r_a', '=', 'AeroReist', '(', 'um', ',', 'zm', ',', 'z0', ',', 'd', ')', '\\n']
Tokenized   (022): ['<s>', 'r', '_', 'a', '=', 'Aero', 'Re', 'ist', '(', 'um', ',', 'z', 'm', ',', 'z', '0', ',', 'd', ')', '\\', 'n', '</s>']
Filtered   (020): ['r', '_', 'a', '=', 'Aero', 'Re', 'ist', '(', 'um', ',', 'z', 'm', ',', 'z', '0', ',', 'd', ')', '\\', 'n']
Detokenized (013): ['r_a', '=', 'AeroReist', '(', 'um', ',', 'zm', ',', 'z0', ',', 'd', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "r_s = SurfResist ( g0 , S , D , Tc , SM , SM0 ) \n"
Original    (017): ['r_s', '=', 'SurfResist', '(', 'g0', ',', 'S', ',', 'D', ',', 'Tc', ',', 'SM', ',', 'SM0', ')', '\\n']
Tokenized   (027): ['<s>', 'r', '_', 's', '=', 'Surf', 'Res', 'ist', '(', 'g', '0', ',', 'S', ',', 'D', ',', 'T', 'c', ',', 'SM', ',', 'SM', '0', ')', '\\', 'n', '</s>']
Filtered   (025): ['r', '_', 's', '=', 'Surf', 'Res', 'ist', '(', 'g', '0', ',', 'S', ',', 'D', ',', 'T', 'c', ',', 'SM', ',', 'SM', '0', ')', '\\', 'n']
Detokenized (017): ['r_s', '=', 'SurfResist', '(', 'g0', ',', 'S', ',', 'D', ',', 'Tc', ',', 'SM', ',', 'SM0', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "LE = ( delta * Rn + ( rho_a * cP * D ) / r_a ) / ( delta + gamma * ( 1.0 + r_s / r_a ) ) \n"
Original    (032): ['LE', '=', '(', 'delta', '*', 'Rn', '+', '(', 'rho_a', '*', 'cP', '*', 'D', ')', '/', 'r_a', ')', '/', '(', 'delta', '+', 'gamma', '*', '(', '1.0', '+', 'r_s', '/', 'r_a', ')', ')', '\\n']
Tokenized   (048): ['<s>', 'LE', '=', '(', 'delta', '*', 'R', 'n', '+', '(', 'r', 'ho', '_', 'a', '*', 'c', 'P', '*', 'D', ')', '/', 'r', '_', 'a', ')', '/', '(', 'delta', '+', 'gamma', '*', '(', '1', '.', '0', '+', 'r', '_', 's', '/', 'r', '_', 'a', ')', ')', '\\', 'n', '</s>']
Filtered   (046): ['LE', '=', '(', 'delta', '*', 'R', 'n', '+', '(', 'r', 'ho', '_', 'a', '*', 'c', 'P', '*', 'D', ')', '/', 'r', '_', 'a', ')', '/', '(', 'delta', '+', 'gamma', '*', '(', '1', '.', '0', '+', 'r', '_', 's', '/', 'r', '_', 'a', ')', ')', '\\', 'n']
Detokenized (032): ['LE', '=', '(', 'delta', '*', 'Rn', '+', '(', 'rho_a', '*', 'cP', '*', 'D', ')', '/', 'r_a', ')', '/', '(', 'delta', '+', 'gamma', '*', '(', '1.0', '+', 'r_s', '/', 'r_a', ')', ')', '\\n']
Counter: 46
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "glClearColor ( * background_color ) \n"
Original    (006): ['glClearColor', '(', '*', 'background_color', ')', '\\n']
Tokenized   (013): ['<s>', 'gl', 'Clear', 'Color', '(', '*', 'background', '_', 'color', ')', '\\', 'n', '</s>']
Filtered   (011): ['gl', 'Clear', 'Color', '(', '*', 'background', '_', 'color', ')', '\\', 'n']
Detokenized (006): ['glClearColor', '(', '*', 'background_color', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "glScissor ( x , y , width , height ) \n"
Original    (011): ['glScissor', '(', 'x', ',', 'y', ',', 'width', ',', 'height', ')', '\\n']
Tokenized   (017): ['<s>', 'gl', 'Sc', 'iss', 'or', '(', 'x', ',', 'y', ',', 'width', ',', 'height', ')', '\\', 'n', '</s>']
Filtered   (015): ['gl', 'Sc', 'iss', 'or', '(', 'x', ',', 'y', ',', 'width', ',', 'height', ')', '\\', 'n']
Detokenized (011): ['glScissor', '(', 'x', ',', 'y', ',', 'width', ',', 'height', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "glOrtho ( x , x + width , y , y + height , - 1 , 1 ) \n"
Original    (020): ['glOrtho', '(', 'x', ',', 'x', '+', 'width', ',', 'y', ',', 'y', '+', 'height', ',', '-', '1', ',', '1', ')', '\\n']
Tokenized   (026): ['<s>', 'gl', 'Or', 'th', 'o', '(', 'x', ',', 'x', '+', 'width', ',', 'y', ',', 'y', '+', 'height', ',', '-', '1', ',', '1', ')', '\\', 'n', '</s>']
Filtered   (024): ['gl', 'Or', 'th', 'o', '(', 'x', ',', 'x', '+', 'width', ',', 'y', ',', 'y', '+', 'height', ',', '-', '1', ',', '1', ')', '\\', 'n']
Detokenized (020): ['glOrtho', '(', 'x', ',', 'x', '+', 'width', ',', 'y', ',', 'y', '+', 'height', ',', '-', '1', ',', '1', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "glNormal3f ( 0 , 1. , 0 ) \n"
Original    (009): ['glNormal3f', '(', '0', ',', '1.', ',', '0', ')', '\\n']
Tokenized   (016): ['<s>', 'gl', 'Normal', '3', 'f', '(', '0', ',', '1', '.', ',', '0', ')', '\\', 'n', '</s>']
Filtered   (014): ['gl', 'Normal', '3', 'f', '(', '0', ',', '1', '.', ',', '0', ')', '\\', 'n']
Detokenized (009): ['glNormal3f', '(', '0', ',', '1.', ',', '0', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "glVertex ( n , n , p ) \n"
Original    (009): ['glVertex', '(', 'n', ',', 'n', ',', 'p', ')', '\\n']
Tokenized   (014): ['<s>', 'gl', 'Ver', 'tex', '(', 'n', ',', 'n', ',', 'p', ')', '\\', 'n', '</s>']
Filtered   (012): ['gl', 'Ver', 'tex', '(', 'n', ',', 'n', ',', 'p', ')', '\\', 'n']
Detokenized (009): ['glVertex', '(', 'n', ',', 'n', ',', 'p', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "companies = [ path for path in paths \n"
Original    (009): ['companies', '=', '[', 'path', 'for', 'path', 'in', 'paths', '\\n']
Tokenized   (013): ['<s>', 'compan', 'ies', '=', '[', 'path', 'for', 'path', 'in', 'paths', '\\', 'n', '</s>']
Filtered   (011): ['compan', 'ies', '=', '[', 'path', 'for', 'path', 'in', 'paths', '\\', 'n']
Detokenized (009): ['companies', '=', '[', 'path', 'for', 'path', 'in', 'paths', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "and os . path . exists ( os . path . join ( folder , path , ) ) ] \n"
Original    (021): ['and', 'os', '.', 'path', '.', 'exists', '(', 'os', '.', 'path', '.', 'join', '(', 'folder', ',', 'path', ',', ')', ')', ']', '\\n']
Tokenized   (024): ['<s>', 'and', 'os', '.', 'path', '.', 'exists', '(', 'os', '.', 'path', '.', 'join', '(', 'folder', ',', 'path', ',', ')', ')', ']', '\\', 'n', '</s>']
Filtered   (022): ['and', 'os', '.', 'path', '.', 'exists', '(', 'os', '.', 'path', '.', 'join', '(', 'folder', ',', 'path', ',', ')', ')', ']', '\\', 'n']
Detokenized (021): ['and', 'os', '.', 'path', '.', 'exists', '(', 'os', '.', 'path', '.', 'join', '(', 'folder', ',', 'path', ',', ')', ')', ']', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "folder = os . path . join ( root_folder , , , ) \n"
Original    (014): ['folder', '=', 'os', '.', 'path', '.', 'join', '(', 'root_folder', ',', ',', ',', ')', '\\n']
Tokenized   (019): ['<s>', 'folder', '=', 'os', '.', 'path', '.', 'join', '(', 'root', '_', 'folder', ',', ',', ',', ')', '\\', 'n', '</s>']
Filtered   (017): ['folder', '=', 'os', '.', 'path', '.', 'join', '(', 'root', '_', 'folder', ',', ',', ',', ')', '\\', 'n']
Detokenized (014): ['folder', '=', 'os', '.', 'path', '.', 'join', '(', 'root_folder', ',', ',', ',', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "scripts = [ , \n"
Original    (005): ['scripts', '=', '[', ',', '\\n']
Tokenized   (008): ['<s>', 'scripts', '=', '[', ',', '\\', 'n', '</s>']
Filtered   (006): ['scripts', '=', '[', ',', '\\', 'n']
Detokenized (005): ['scripts', '=', '[', ',', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "pro2 . predict ( ) from collections import OrderedDict \n"
Original    (010): ['pro2', '.', 'predict', '(', ')', 'from', 'collections', 'import', 'OrderedDict', '\\n']
Tokenized   (017): ['<s>', 'pro', '2', '.', 'predict', '(', ')', 'from', 'collections', 'import', 'Ord', 'ered', 'D', 'ict', '\\', 'n', '</s>']
Filtered   (015): ['pro', '2', '.', 'predict', '(', ')', 'from', 'collections', 'import', 'Ord', 'ered', 'D', 'ict', '\\', 'n']
Detokenized (010): ['pro2', '.', 'predict', '(', ')', 'from', 'collections', 'import', 'OrderedDict', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : ""dimensions." % ( self . __class__ . __name__ , shape ) ) \n"
Original    (013): ['"dimensions."', '%', '(', 'self', '.', '__class__', '.', '__name__', ',', 'shape', ')', ')', '\\n']
Tokenized   (023): ['<s>', '"', 'dim', 'ensions', '."', '%', '(', 'self', '.', '__', 'class', '__', '.', '__', 'name', '__', ',', 'shape', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['"', 'dim', 'ensions', '."', '%', '(', 'self', '.', '__', 'class', '__', '.', '__', 'name', '__', ',', 'shape', ')', ')', '\\', 'n']
Detokenized (013): ['"dimensions."', '%', '(', 'self', '.', '__class__', '.', '__name__', ',', 'shape', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "only = set ( tag for tag , value in tags . items ( ) if value ) \n"
Original    (019): ['only', '=', 'set', '(', 'tag', 'for', 'tag', ',', 'value', 'in', 'tags', '.', 'items', '(', ')', 'if', 'value', ')', '\\n']
Tokenized   (022): ['<s>', 'only', '=', 'set', '(', 'tag', 'for', 'tag', ',', 'value', 'in', 'tags', '.', 'items', '(', ')', 'if', 'value', ')', '\\', 'n', '</s>']
Filtered   (020): ['only', '=', 'set', '(', 'tag', 'for', 'tag', ',', 'value', 'in', 'tags', '.', 'items', '(', ')', 'if', 'value', ')', '\\', 'n']
Detokenized (019): ['only', '=', 'set', '(', 'tag', 'for', 'tag', ',', 'value', 'in', 'tags', '.', 'items', '(', ')', 'if', 'value', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "crop1 = [ None , , , ] \n"
Original    (009): ['crop1', '=', '[', 'None', ',', ',', ',', ']', '\\n']
Tokenized   (013): ['<s>', 'crop', '1', '=', '[', 'None', ',', ',', ',', ']', '\\', 'n', '</s>']
Filtered   (011): ['crop', '1', '=', '[', 'None', ',', ',', ',', ']', '\\', 'n']
Detokenized (009): ['crop1', '=', '[', 'None', ',', ',', ',', ']', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "outs = [ o . eval ( ) for o in outs ] \n"
Original    (014): ['outs', '=', '[', 'o', '.', 'eval', '(', ')', 'for', 'o', 'in', 'outs', ']', '\\n']
Tokenized   (017): ['<s>', 'outs', '=', '[', 'o', '.', 'eval', '(', ')', 'for', 'o', 'in', 'outs', ']', '\\', 'n', '</s>']
Filtered   (015): ['outs', '=', '[', 'o', '.', 'eval', '(', ')', 'for', 'o', 'in', 'outs', ']', '\\', 'n']
Detokenized (014): ['outs', '=', '[', 'o', '.', 'eval', '(', ')', 'for', 'o', 'in', 'outs', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "crop_test ( crop_x , [ x0 , x1 , x2 , x0 , x1 , x2 ] , \n"
Original    (019): ['crop_test', '(', 'crop_x', ',', '[', 'x0', ',', 'x1', ',', 'x2', ',', 'x0', ',', 'x1', ',', 'x2', ']', ',', '\\n']
Tokenized   (032): ['<s>', 'crop', '_', 'test', '(', 'crop', '_', 'x', ',', '[', 'x', '0', ',', 'x', '1', ',', 'x', '2', ',', 'x', '0', ',', 'x', '1', ',', 'x', '2', ']', ',', '\\', 'n', '</s>']
Filtered   (030): ['crop', '_', 'test', '(', 'crop', '_', 'x', ',', '[', 'x', '0', ',', 'x', '1', ',', 'x', '2', ',', 'x', '0', ',', 'x', '1', ',', 'x', '2', ']', ',', '\\', 'n']
Detokenized (019): ['crop_test', '(', 'crop_x', ',', '[', 'x0', ',', 'x1', ',', 'x2', ',', 'x0', ',', 'x1', ',', 'x2', ']', ',', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "cropping = [ ] * 2 ) \n"
Original    (008): ['cropping', '=', '[', ']', '*', '2', ')', '\\n']
Tokenized   (012): ['<s>', 'cro', 'pping', '=', '[', ']', '*', '2', ')', '\\', 'n', '</s>']
Filtered   (010): ['cro', 'pping', '=', '[', ']', '*', '2', ')', '\\', 'n']
Detokenized (008): ['cropping', '=', '[', ']', '*', '2', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "desired_result_0 = numpy . concatenate ( [ x0 [ : , : 2 ] , x1 [ : , : 2 ] ] , axis = 0 ) \n"
Original    (029): ['desired_result_0', '=', 'numpy', '.', 'concatenate', '(', '[', 'x0', '[', ':', ',', ':', '2', ']', ',', 'x1', '[', ':', ',', ':', '2', ']', ']', ',', 'axis', '=', '0', ')', '\\n']
Tokenized   (042): ['<s>', 'des', 'ired', '_', 'result', '_', '0', '=', 'n', 'umpy', '.', 'conc', 'aten', 'ate', '(', '[', 'x', '0', '[', ':', ',', ':', '2', ']', ',', 'x', '1', '[', ':', ',', ':', '2', ']', ']', ',', 'axis', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (040): ['des', 'ired', '_', 'result', '_', '0', '=', 'n', 'umpy', '.', 'conc', 'aten', 'ate', '(', '[', 'x', '0', '[', ':', ',', ':', '2', ']', ',', 'x', '1', '[', ':', ',', ':', '2', ']', ']', ',', 'axis', '=', '0', ')', '\\', 'n']
Detokenized (029): ['desired_result_0', '=', 'numpy', '.', 'concatenate', '(', '[', 'x0', '[', ':', ',', ':', '2', ']', ',', 'x1', '[', ':', ',', ':', '2', ']', ']', ',', 'axis', '=', '0', ')', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "desired_result_1 = numpy . concatenate ( [ x0 [ : 4 , : ] , x1 [ : 4 , : ] ] , axis = 1 ) \n"
Original    (029): ['desired_result_1', '=', 'numpy', '.', 'concatenate', '(', '[', 'x0', '[', ':', '4', ',', ':', ']', ',', 'x1', '[', ':', '4', ',', ':', ']', ']', ',', 'axis', '=', '1', ')', '\\n']
Tokenized   (042): ['<s>', 'des', 'ired', '_', 'result', '_', '1', '=', 'n', 'umpy', '.', 'conc', 'aten', 'ate', '(', '[', 'x', '0', '[', ':', '4', ',', ':', ']', ',', 'x', '1', '[', ':', '4', ',', ':', ']', ']', ',', 'axis', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (040): ['des', 'ired', '_', 'result', '_', '1', '=', 'n', 'umpy', '.', 'conc', 'aten', 'ate', '(', '[', 'x', '0', '[', ':', '4', ',', ':', ']', ',', 'x', '1', '[', ':', '4', ',', ':', ']', ']', ',', 'axis', '=', '1', ')', '\\', 'n']
Detokenized (029): ['desired_result_1', '=', 'numpy', '.', 'concatenate', '(', '[', 'x0', '[', ':', '4', ',', ':', ']', ',', 'x1', '[', ':', '4', ',', ':', ']', ']', ',', 'axis', '=', '1', ')', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "inputs = [ theano . shared ( a ) , \n"
Original    (011): ['inputs', '=', '[', 'theano', '.', 'shared', '(', 'a', ')', ',', '\\n']
Tokenized   (016): ['<s>', 'input', 's', '=', '[', 'the', 'ano', '.', 'shared', '(', 'a', ')', ',', '\\', 'n', '</s>']
Filtered   (014): ['input', 's', '=', '[', 'the', 'ano', '.', 'shared', '(', 'a', ')', ',', '\\', 'n']
Detokenized (011): ['inputs', '=', '[', 'theano', '.', 'shared', '(', 'a', ')', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "theano . shared ( b ) ] \n"
Original    (008): ['theano', '.', 'shared', '(', 'b', ')', ']', '\\n']
Tokenized   (012): ['<s>', 'the', 'ano', '.', 'shared', '(', 'b', ')', ']', '\\', 'n', '</s>']
Filtered   (010): ['the', 'ano', '.', 'shared', '(', 'b', ')', ']', '\\', 'n']
Detokenized (008): ['theano', '.', 'shared', '(', 'b', ')', ']', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "mask = os . urandom ( 4 ) if mask else None \n"
Original    (013): ['mask', '=', 'os', '.', 'urandom', '(', '4', ')', 'if', 'mask', 'else', 'None', '\\n']
Tokenized   (017): ['<s>', 'mask', '=', 'os', '.', 'ur', 'andom', '(', '4', ')', 'if', 'mask', 'else', 'None', '\\', 'n', '</s>']
Filtered   (015): ['mask', '=', 'os', '.', 'ur', 'andom', '(', '4', ')', 'if', 'mask', 'else', 'None', '\\', 'n']
Detokenized (013): ['mask', '=', 'os', '.', 'urandom', '(', '4', ')', 'if', 'mask', 'else', 'None', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "masking_key = mask , fin = 1 ) . build ( ) \n"
Original    (013): ['masking_key', '=', 'mask', ',', 'fin', '=', '1', ')', '.', 'build', '(', ')', '\\n']
Tokenized   (019): ['<s>', 'mask', 'ing', '_', 'key', '=', 'mask', ',', 'fin', '=', '1', ')', '.', 'build', '(', ')', '\\', 'n', '</s>']
Filtered   (017): ['mask', 'ing', '_', 'key', '=', 'mask', ',', 'fin', '=', '1', ')', '.', 'build', '(', ')', '\\', 'n']
Detokenized (013): ['masking_key', '=', 'mask', ',', 'fin', '=', '1', ')', '.', 'build', '(', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "fin = fin ) . build ( ) \n"
Original    (009): ['fin', '=', 'fin', ')', '.', 'build', '(', ')', '\\n']
Tokenized   (012): ['<s>', 'fin', '=', 'fin', ')', '.', 'build', '(', ')', '\\', 'n', '</s>']
Filtered   (010): ['fin', '=', 'fin', ')', '.', 'build', '(', ')', '\\', 'n']
Detokenized (009): ['fin', '=', 'fin', ')', '.', 'build', '(', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "flags = unpack ( , msg [ idx : idx + 4 ] ) [ 0 ] \n"
Original    (018): ['flags', '=', 'unpack', '(', ',', 'msg', '[', 'idx', ':', 'idx', '+', '4', ']', ')', '[', '0', ']', '\\n']
Tokenized   (024): ['<s>', 'flags', '=', 'un', 'pack', '(', ',', 'msg', '[', 'id', 'x', ':', 'id', 'x', '+', '4', ']', ')', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (022): ['flags', '=', 'un', 'pack', '(', ',', 'msg', '[', 'id', 'x', ':', 'id', 'x', '+', '4', ']', ')', '[', '0', ']', '\\', 'n']
Detokenized (018): ['flags', '=', 'unpack', '(', ',', 'msg', '[', 'idx', ':', 'idx', '+', '4', ']', ')', '[', '0', ']', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "pdc = req . get_options ( ) [ ] \n"
Original    (010): ['pdc', '=', 'req', '.', 'get_options', '(', ')', '[', ']', '\\n']
Tokenized   (016): ['<s>', 'p', 'dc', '=', 'req', '.', 'get', '_', 'options', '(', ')', '[', ']', '\\', 'n', '</s>']
Filtered   (014): ['p', 'dc', '=', 'req', '.', 'get', '_', 'options', '(', ')', '[', ']', '\\', 'n']
Detokenized (010): ['pdc', '=', 'req', '.', 'get_options', '(', ')', '[', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "bdc = req . get_options ( ) . get ( , False ) \n"
Original    (014): ['bdc', '=', 'req', '.', 'get_options', '(', ')', '.', 'get', '(', ',', 'False', ')', '\\n']
Tokenized   (020): ['<s>', 'bd', 'c', '=', 'req', '.', 'get', '_', 'options', '(', ')', '.', 'get', '(', ',', 'False', ')', '\\', 'n', '</s>']
Filtered   (018): ['bd', 'c', '=', 'req', '.', 'get', '_', 'options', '(', ')', '.', 'get', '(', ',', 'False', ')', '\\', 'n']
Detokenized (014): ['bdc', '=', 'req', '.', 'get_options', '(', ')', '.', 'get', '(', ',', 'False', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "decoded_path = urllib . unquote ( url . path ) [ 1 : ] \n"
Original    (015): ['decoded_path', '=', 'urllib', '.', 'unquote', '(', 'url', '.', 'path', ')', '[', '1', ':', ']', '\\n']
Tokenized   (024): ['<s>', 'dec', 'oded', '_', 'path', '=', 'ur', 'll', 'ib', '.', 'un', 'quote', '(', 'url', '.', 'path', ')', '[', '1', ':', ']', '\\', 'n', '</s>']
Filtered   (022): ['dec', 'oded', '_', 'path', '=', 'ur', 'll', 'ib', '.', 'un', 'quote', '(', 'url', '.', 'path', ')', '[', '1', ':', ']', '\\', 'n']
Detokenized (015): ['decoded_path', '=', 'urllib', '.', 'unquote', '(', 'url', '.', 'path', ')', '[', '1', ':', ']', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "rules = . join ( req . requires ( ) ) . strip ( ) \n"
Original    (016): ['rules', '=', '.', 'join', '(', 'req', '.', 'requires', '(', ')', ')', '.', 'strip', '(', ')', '\\n']
Tokenized   (019): ['<s>', 'rules', '=', '.', 'join', '(', 'req', '.', 'requires', '(', ')', ')', '.', 'strip', '(', ')', '\\', 'n', '</s>']
Filtered   (017): ['rules', '=', '.', 'join', '(', 'req', '.', 'requires', '(', ')', ')', '.', 'strip', '(', ')', '\\', 'n']
Detokenized (016): ['rules', '=', '.', 'join', '(', 'req', '.', 'requires', '(', ')', ')', '.', 'strip', '(', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n"
Original    (018): ['domain', '=', 'req', '.', 'get_options', '(', ')', '.', 'get', '(', ',', 'req', '.', 'auth_name', '(', ')', ')', '\\n']
Tokenized   (025): ['<s>', 'domain', '=', 'req', '.', 'get', '_', 'options', '(', ')', '.', 'get', '(', ',', 'req', '.', 'auth', '_', 'name', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (023): ['domain', '=', 'req', '.', 'get', '_', 'options', '(', ')', '.', 'get', '(', ',', 'req', '.', 'auth', '_', 'name', '(', ')', ')', '\\', 'n']
Detokenized (018): ['domain', '=', 'req', '.', 'get_options', '(', ')', '.', 'get', '(', ',', 'req', '.', 'auth_name', '(', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "auth_headers = req . headers_in . get ( , [ ] ) \n"
Original    (013): ['auth_headers', '=', 'req', '.', 'headers_in', '.', 'get', '(', ',', '[', ']', ')', '\\n']
Tokenized   (020): ['<s>', 'auth', '_', 'headers', '=', 'req', '.', 'headers', '_', 'in', '.', 'get', '(', ',', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (018): ['auth', '_', 'headers', '=', 'req', '.', 'headers', '_', 'in', '.', 'get', '(', ',', '[', ']', ')', '\\', 'n']
Detokenized (013): ['auth_headers', '=', 'req', '.', 'headers_in', '.', 'get', '(', ',', '[', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "set_remote_user ( req , ah_data [ 1 ] , domain ) \n"
Original    (012): ['set_remote_user', '(', 'req', ',', 'ah_data', '[', '1', ']', ',', 'domain', ')', '\\n']
Tokenized   (021): ['<s>', 'set', '_', 'remote', '_', 'user', '(', 'req', ',', 'ah', '_', 'data', '[', '1', ']', ',', 'domain', ')', '\\', 'n', '</s>']
Filtered   (019): ['set', '_', 'remote', '_', 'user', '(', 'req', ',', 'ah', '_', 'data', '[', '1', ']', ',', 'domain', ')', '\\', 'n']
Detokenized (012): ['set_remote_user', '(', 'req', ',', 'ah_data', '[', '1', ']', ',', 'domain', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "dict = json . loads ( request . data . decode ( ) ) \n"
Original    (015): ['dict', '=', 'json', '.', 'loads', '(', 'request', '.', 'data', '.', 'decode', '(', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'dict', '=', 'json', '.', 'loads', '(', 'request', '.', 'data', '.', 'decode', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['dict', '=', 'json', '.', 'loads', '(', 'request', '.', 'data', '.', 'decode', '(', ')', ')', '\\', 'n']
Detokenized (015): ['dict', '=', 'json', '.', 'loads', '(', 'request', '.', 'data', '.', 'decode', '(', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "rv = self . app . delete ( . format ( id ) ) \n"
Original    (015): ['rv', '=', 'self', '.', 'app', '.', 'delete', '(', '.', 'format', '(', 'id', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'r', 'v', '=', 'self', '.', 'app', '.', 'delete', '(', '.', 'format', '(', 'id', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['r', 'v', '=', 'self', '.', 'app', '.', 'delete', '(', '.', 'format', '(', 'id', ')', ')', '\\', 'n']
Detokenized (015): ['rv', '=', 'self', '.', 'app', '.', 'delete', '(', '.', 'format', '(', 'id', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "configure_logging ( "index_pages_logging.config" , "index_pages.log" ) \n"
Original    (007): ['configure_logging', '(', '"index_pages_logging.config"', ',', '"index_pages.log"', ')', '\\n']
Tokenized   (029): ['<s>', 'config', 'ure', '_', 'log', 'ging', '(', '"', 'index', '_', 'pages', '_', 'log', 'ging', '.', 'config', '"', ',', '"', 'index', '_', 'pages', '.', 'log', '"', ')', '\\', 'n', '</s>']
Filtered   (027): ['config', 'ure', '_', 'log', 'ging', '(', '"', 'index', '_', 'pages', '_', 'log', 'ging', '.', 'config', '"', ',', '"', 'index', '_', 'pages', '.', 'log', '"', ')', '\\', 'n']
Detokenized (007): ['configure_logging', '(', '"index_pages_logging.config"', ',', '"index_pages.log"', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "ocr_file = join ( dir , ) \n"
Original    (008): ['ocr_file', '=', 'join', '(', 'dir', ',', ')', '\\n']
Tokenized   (013): ['<s>', 'ocr', '_', 'file', '=', 'join', '(', 'dir', ',', ')', '\\', 'n', '</s>']
Filtered   (011): ['ocr', '_', 'file', '=', 'join', '(', 'dir', ',', ')', '\\', 'n']
Detokenized (008): ['ocr_file', '=', 'join', '(', 'dir', ',', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "expected_text = { "eng" : file ( join ( dir , ) ) . read ( ) . decode ( ) } \n"
Original    (023): ['expected_text', '=', '{', '"eng"', ':', 'file', '(', 'join', '(', 'dir', ',', ')', ')', '.', 'read', '(', ')', '.', 'decode', '(', ')', '}', '\\n']
Tokenized   (030): ['<s>', 'expected', '_', 'text', '=', '{', '"', 'eng', '"', ':', 'file', '(', 'join', '(', 'dir', ',', ')', ')', '.', 'read', '(', ')', '.', 'decode', '(', ')', '}', '\\', 'n', '</s>']
Filtered   (028): ['expected', '_', 'text', '=', '{', '"', 'eng', '"', ':', 'file', '(', 'join', '(', 'dir', ',', ')', ')', '.', 'read', '(', ')', '.', 'decode', '(', ')', '}', '\\', 'n']
Detokenized (023): ['expected_text', '=', '{', '"eng"', ':', 'file', '(', 'join', '(', 'dir', ',', ')', ')', '.', 'read', '(', ')', '.', 'decode', '(', ')', '}', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "tuples . append ( ( os . path . join ( root , pair [ 0 ] ) , np . int32 ( pair [ 1 ] ) ) ) \n"
Original    (031): ['tuples', '.', 'append', '(', '(', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'pair', '[', '0', ']', ')', ',', 'np', '.', 'int32', '(', 'pair', '[', '1', ']', ')', ')', ')', '\\n']
Tokenized   (036): ['<s>', 'tu', 'ples', '.', 'append', '(', '(', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'pair', '[', '0', ']', ')', ',', 'np', '.', 'int', '32', '(', 'pair', '[', '1', ']', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (034): ['tu', 'ples', '.', 'append', '(', '(', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'pair', '[', '0', ']', ')', ',', 'np', '.', 'int', '32', '(', 'pair', '[', '1', ']', ')', ')', ')', '\\', 'n']
Detokenized (031): ['tuples', '.', 'append', '(', '(', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'pair', '[', '0', ']', ')', ',', 'np', '.', 'int32', '(', 'pair', '[', '1', ']', ')', ')', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 31, 768)
# Extracted words:  31
Sentence         : "val_list = load_image_list ( args . val , args . root ) \n"
Original    (013): ['val_list', '=', 'load_image_list', '(', 'args', '.', 'val', ',', 'args', '.', 'root', ')', '\\n']
Tokenized   (022): ['<s>', 'val', '_', 'list', '=', 'load', '_', 'image', '_', 'list', '(', 'args', '.', 'val', ',', 'args', '.', 'root', ')', '\\', 'n', '</s>']
Filtered   (020): ['val', '_', 'list', '=', 'load', '_', 'image', '_', 'list', '(', 'args', '.', 'val', ',', 'args', '.', 'root', ')', '\\', 'n']
Detokenized (013): ['val_list', '=', 'load_image_list', '(', 'args', '.', 'val', ',', 'args', '.', 'root', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "mean_image = pickle . load ( open ( args . mean , ) ) \n"
Original    (015): ['mean_image', '=', 'pickle', '.', 'load', '(', 'open', '(', 'args', '.', 'mean', ',', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'mean', '_', 'image', '=', 'pick', 'le', '.', 'load', '(', 'open', '(', 'args', '.', 'mean', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['mean', '_', 'image', '=', 'pick', 'le', '.', 'load', '(', 'open', '(', 'args', '.', 'mean', ',', ')', ')', '\\', 'n']
Detokenized (015): ['mean_image', '=', 'pickle', '.', 'load', '(', 'open', '(', 'args', '.', 'mean', ',', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "cropwidth = 256 - model . insize \n"
Original    (008): ['cropwidth', '=', '256', '-', 'model', '.', 'insize', '\\n']
Tokenized   (013): ['<s>', 'crop', 'width', '=', '256', '-', 'model', '.', 'ins', 'ize', '\\', 'n', '</s>']
Filtered   (011): ['crop', 'width', '=', '256', '-', 'model', '.', 'ins', 'ize', '\\', 'n']
Detokenized (008): ['cropwidth', '=', '256', '-', 'model', '.', 'insize', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "left = random . randint ( 0 , cropwidth - 1 ) \n"
Original    (013): ['left', '=', 'random', '.', 'randint', '(', '0', ',', 'cropwidth', '-', '1', ')', '\\n']
Tokenized   (018): ['<s>', 'left', '=', 'random', '.', 'rand', 'int', '(', '0', ',', 'crop', 'width', '-', '1', ')', '\\', 'n', '</s>']
Filtered   (016): ['left', '=', 'random', '.', 'rand', 'int', '(', '0', ',', 'crop', 'width', '-', '1', ')', '\\', 'n']
Detokenized (013): ['left', '=', 'random', '.', 'randint', '(', '0', ',', 'cropwidth', '-', '1', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "image /= 255 \n"
Original    (004): ['image', '/=', '255', '\\n']
Tokenized   (008): ['<s>', 'image', '/', '=', '255', '\\', 'n', '</s>']
Filtered   (006): ['image', '/', '=', '255', '\\', 'n']
Detokenized (004): ['image', '/=', '255', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "val_batch_pool = [ None ] * args . val_batchsize \n"
Original    (010): ['val_batch_pool', '=', '[', 'None', ']', '*', 'args', '.', 'val_batchsize', '\\n']
Tokenized   (020): ['<s>', 'val', '_', 'batch', '_', 'pool', '=', '[', 'None', ']', '*', 'args', '.', 'val', '_', 'batch', 'size', '\\', 'n', '</s>']
Filtered   (018): ['val', '_', 'batch', '_', 'pool', '=', '[', 'None', ']', '*', 'args', '.', 'val', '_', 'batch', 'size', '\\', 'n']
Detokenized (010): ['val_batch_pool', '=', '[', 'None', ']', '*', 'args', '.', 'val_batchsize', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "perm = np . random . permutation ( len ( train_list ) ) \n"
Original    (014): ['perm', '=', 'np', '.', 'random', '.', 'permutation', '(', 'len', '(', 'train_list', ')', ')', '\\n']
Tokenized   (020): ['<s>', 'perm', '=', 'np', '.', 'random', '.', 'perm', 'utation', '(', 'len', '(', 'train', '_', 'list', ')', ')', '\\', 'n', '</s>']
Filtered   (018): ['perm', '=', 'np', '.', 'random', '.', 'perm', 'utation', '(', 'len', '(', 'train', '_', 'list', ')', ')', '\\', 'n']
Detokenized (014): ['perm', '=', 'np', '.', 'random', '.', 'permutation', '(', 'len', '(', 'train_list', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "batch_pool [ i ] = pool . apply_async ( read_image , ( path , False , True ) ) \n"
Original    (020): ['batch_pool', '[', 'i', ']', '=', 'pool', '.', 'apply_async', '(', 'read_image', ',', '(', 'path', ',', 'False', ',', 'True', ')', ')', '\\n']
Tokenized   (030): ['<s>', 'batch', '_', 'pool', '[', 'i', ']', '=', 'pool', '.', 'apply', '_', 'as', 'ync', '(', 'read', '_', 'image', ',', '(', 'path', ',', 'False', ',', 'True', ')', ')', '\\', 'n', '</s>']
Filtered   (028): ['batch', '_', 'pool', '[', 'i', ']', '=', 'pool', '.', 'apply', '_', 'as', 'ync', '(', 'read', '_', 'image', ',', '(', 'path', ',', 'False', ',', 'True', ')', ')', '\\', 'n']
Detokenized (020): ['batch_pool', '[', 'i', ']', '=', 'pool', '.', 'apply_async', '(', 'read_image', ',', '(', 'path', ',', 'False', ',', 'True', ')', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "read_image , ( path , True , False ) ) \n"
Original    (011): ['read_image', ',', '(', 'path', ',', 'True', ',', 'False', ')', ')', '\\n']
Tokenized   (016): ['<s>', 'read', '_', 'image', ',', '(', 'path', ',', 'True', ',', 'False', ')', ')', '\\', 'n', '</s>']
Filtered   (014): ['read', '_', 'image', ',', '(', 'path', ',', 'True', ',', 'False', ')', ')', '\\', 'n']
Detokenized (011): ['read_image', ',', '(', 'path', ',', 'True', ',', 'False', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "val_count = val_loss = val_accuracy = 0 \n"
Original    (008): ['val_count', '=', 'val_loss', '=', 'val_accuracy', '=', '0', '\\n']
Tokenized   (018): ['<s>', 'val', '_', 'count', '=', 'val', '_', 'loss', '=', 'val', '_', 'acc', 'uracy', '=', '0', '\\', 'n', '</s>']
Filtered   (016): ['val', '_', 'count', '=', 'val', '_', 'loss', '=', 'val', '_', 'acc', 'uracy', '=', '0', '\\', 'n']
Detokenized (008): ['val_count', '=', 'val_loss', '=', 'val_accuracy', '=', '0', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "duration = time . time ( ) - val_begin_at \n"
Original    (010): ['duration', '=', 'time', '.', 'time', '(', ')', '-', 'val_begin_at', '\\n']
Tokenized   (017): ['<s>', 'duration', '=', 'time', '.', 'time', '(', ')', '-', 'val', '_', 'begin', '_', 'at', '\\', 'n', '</s>']
Filtered   (015): ['duration', '=', 'time', '.', 'time', '(', ')', '-', 'val', '_', 'begin', '_', 'at', '\\', 'n']
Detokenized (010): ['duration', '=', 'time', '.', 'time', '(', ')', '-', 'val_begin_at', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "mean_error = 1 - val_accuracy * args . val_batchsize / 50000 \n"
Original    (012): ['mean_error', '=', '1', '-', 'val_accuracy', '*', 'args', '.', 'val_batchsize', '/', '50000', '\\n']
Tokenized   (024): ['<s>', 'mean', '_', 'error', '=', '1', '-', 'val', '_', 'acc', 'uracy', '*', 'args', '.', 'val', '_', 'batch', 'size', '/', '5', '0000', '\\', 'n', '</s>']
Filtered   (022): ['mean', '_', 'error', '=', '1', '-', 'val', '_', 'acc', 'uracy', '*', 'args', '.', 'val', '_', 'batch', 'size', '/', '5', '0000', '\\', 'n']
Detokenized (012): ['mean_error', '=', '1', '-', 'val_accuracy', '*', 'args', '.', 'val_batchsize', '/', '50000', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "serializers . save_hdf5 ( args . outstate , optimizer ) \n"
Original    (011): ['serializers', '.', 'save_hdf5', '(', 'args', '.', 'outstate', ',', 'optimizer', ')', '\\n']
Tokenized   (021): ['<s>', 'serial', 'izers', '.', 'save', '_', 'h', 'df', '5', '(', 'args', '.', 'out', 'state', ',', 'optim', 'izer', ')', '\\', 'n', '</s>']
Filtered   (019): ['serial', 'izers', '.', 'save', '_', 'h', 'df', '5', '(', 'args', '.', 'out', 'state', ',', 'optim', 'izer', ')', '\\', 'n']
Detokenized (011): ['serializers', '.', 'save_hdf5', '(', 'args', '.', 'outstate', ',', 'optimizer', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "boards_name = [ slugify ( b [ ] ) for b in SETTINGS . get ( , { } ) . values ( ) ] \n"
Original    (026): ['boards_name', '=', '[', 'slugify', '(', 'b', '[', ']', ')', 'for', 'b', 'in', 'SETTINGS', '.', 'get', '(', ',', '{', '}', ')', '.', 'values', '(', ')', ']', '\\n']
Tokenized   (034): ['<s>', 'boards', '_', 'name', '=', '[', 'slug', 'ify', '(', 'b', '[', ']', ')', 'for', 'b', 'in', 'SET', 'T', 'INGS', '.', 'get', '(', ',', '{', '}', ')', '.', 'values', '(', ')', ']', '\\', 'n', '</s>']
Filtered   (032): ['boards', '_', 'name', '=', '[', 'slug', 'ify', '(', 'b', '[', ']', ')', 'for', 'b', 'in', 'SET', 'T', 'INGS', '.', 'get', '(', ',', '{', '}', ')', '.', 'values', '(', ')', ']', '\\', 'n']
Detokenized (026): ['boards_name', '=', '[', 'slugify', '(', 'b', '[', ']', ')', 'for', 'b', 'in', 'SETTINGS', '.', 'get', '(', ',', '{', '}', ')', '.', 'values', '(', ')', ']', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "LOGGING . info ( . format ( board_name , result ) ) \n"
Original    (013): ['LOGGING', '.', 'info', '(', '.', 'format', '(', 'board_name', ',', 'result', ')', ')', '\\n']
Tokenized   (020): ['<s>', 'LOG', 'G', 'ING', '.', 'info', '(', '.', 'format', '(', 'board', '_', 'name', ',', 'result', ')', ')', '\\', 'n', '</s>']
Filtered   (018): ['LOG', 'G', 'ING', '.', 'info', '(', '.', 'format', '(', 'board', '_', 'name', ',', 'result', ')', ')', '\\', 'n']
Detokenized (013): ['LOGGING', '.', 'info', '(', '.', 'format', '(', 'board_name', ',', 'result', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "is_py2 = _ver [ 0 ] == 2 \n"
Original    (009): ['is_py2', '=', '_ver', '[', '0', ']', '==', '2', '\\n']
Tokenized   (016): ['<s>', 'is', '_', 'py', '2', '=', '_', 'ver', '[', '0', ']', '==', '2', '\\', 'n', '</s>']
Filtered   (014): ['is', '_', 'py', '2', '=', '_', 'ver', '[', '0', ']', '==', '2', '\\', 'n']
Detokenized (009): ['is_py2', '=', '_ver', '[', '0', ']', '==', '2', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "is_py2_7_9_or_later = _ver [ 0 ] >= 2 and _ver [ 1 ] >= 7 and _ver [ 2 ] >= 9 \n"
Original    (023): ['is_py2_7_9_or_later', '=', '_ver', '[', '0', ']', '>=', '2', 'and', '_ver', '[', '1', ']', '>=', '7', 'and', '_ver', '[', '2', ']', '>=', '9', '\\n']
Tokenized   (040): ['<s>', 'is', '_', 'py', '2', '_', '7', '_', '9', '_', 'or', '_', 'later', '=', '_', 'ver', '[', '0', ']', '>=', '2', 'and', '_', 'ver', '[', '1', ']', '>=', '7', 'and', '_', 'ver', '[', '2', ']', '>=', '9', '\\', 'n', '</s>']
Filtered   (038): ['is', '_', 'py', '2', '_', '7', '_', '9', '_', 'or', '_', 'later', '=', '_', 'ver', '[', '0', ']', '>=', '2', 'and', '_', 'ver', '[', '1', ']', '>=', '7', 'and', '_', 'ver', '[', '2', ']', '>=', '9', '\\', 'n']
Detokenized (023): ['is_py2_7_9_or_later', '=', '_ver', '[', '0', ']', '>=', '2', 'and', '_ver', '[', '1', ']', '>=', '7', 'and', '_ver', '[', '2', ']', '>=', '9', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "is_py3_3 = is_py3 and _ver [ 1 ] == 3 \n"
Original    (011): ['is_py3_3', '=', 'is_py3', 'and', '_ver', '[', '1', ']', '==', '3', '\\n']
Tokenized   (023): ['<s>', 'is', '_', 'py', '3', '_', '3', '=', 'is', '_', 'py', '3', 'and', '_', 'ver', '[', '1', ']', '==', '3', '\\', 'n', '</s>']
Filtered   (021): ['is', '_', 'py', '3', '_', '3', '=', 'is', '_', 'py', '3', 'and', '_', 'ver', '[', '1', ']', '==', '3', '\\', 'n']
Detokenized (011): ['is_py3_3', '=', 'is_py3', 'and', '_ver', '[', '1', ']', '==', '3', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "strategy = zlib . Z_DEFAULT_STRATEGY ) : \n"
Original    (008): ['strategy', '=', 'zlib', '.', 'Z_DEFAULT_STRATEGY', ')', ':', '\\n']
Tokenized   (020): ['<s>', 'str', 'ategy', '=', 'z', 'lib', '.', 'Z', '_', 'DE', 'FAULT', '_', 'STR', 'ATE', 'GY', ')', ':', '\\', 'n', '</s>']
Filtered   (018): ['str', 'ategy', '=', 'z', 'lib', '.', 'Z', '_', 'DE', 'FAULT', '_', 'STR', 'ATE', 'GY', ')', ':', '\\', 'n']
Detokenized (008): ['strategy', '=', 'zlib', '.', 'Z_DEFAULT_STRATEGY', ')', ':', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "secure = self . secure \n"
Original    (006): ['secure', '=', 'self', '.', 'secure', '\\n']
Tokenized   (009): ['<s>', 'secure', '=', 'self', '.', 'secure', '\\', 'n', '</s>']
Filtered   (007): ['secure', '=', 'self', '.', 'secure', '\\', 'n']
Detokenized (006): ['secure', '=', 'self', '.', 'secure', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "e . huffman_coder = HuffmanEncoder ( REQUEST_CODES , REQUEST_CODES_LENGTH ) \n"
Original    (011): ['e', '.', 'huffman_coder', '=', 'HuffmanEncoder', '(', 'REQUEST_CODES', ',', 'REQUEST_CODES_LENGTH', ')', '\\n']
Tokenized   (035): ['<s>', 'e', '.', 'h', 'uff', 'man', '_', 'c', 'oder', '=', 'Huff', 'man', 'Enc', 'oder', '(', 'RE', 'QUEST', '_', 'C', 'OD', 'ES', ',', 'RE', 'QUEST', '_', 'C', 'OD', 'ES', '_', 'L', 'ENGTH', ')', '\\', 'n', '</s>']
Filtered   (033): ['e', '.', 'h', 'uff', 'man', '_', 'c', 'oder', '=', 'Huff', 'man', 'Enc', 'oder', '(', 'RE', 'QUEST', '_', 'C', 'OD', 'ES', ',', 'RE', 'QUEST', '_', 'C', 'OD', 'ES', '_', 'L', 'ENGTH', ')', '\\', 'n']
Detokenized (011): ['e', '.', 'huffman_coder', '=', 'HuffmanEncoder', '(', 'REQUEST_CODES', ',', 'REQUEST_CODES_LENGTH', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "train_seq = corpus . read_sequence_list_conll ( input_data , max_sent_len = 15 , max_nr_sent = 1000 ) \n"
Original    (017): ['train_seq', '=', 'corpus', '.', 'read_sequence_list_conll', '(', 'input_data', ',', 'max_sent_len', '=', '15', ',', 'max_nr_sent', '=', '1000', ')', '\\n']
Tokenized   (039): ['<s>', 'train', '_', 'seq', '=', 'corpus', '.', 'read', '_', 'sequence', '_', 'list', '_', 'con', 'll', '(', 'input', '_', 'data', ',', 'max', '_', 'sent', '_', 'len', '=', '15', ',', 'max', '_', 'nr', '_', 'sent', '=', '1000', ')', '\\', 'n', '</s>']
Filtered   (037): ['train', '_', 'seq', '=', 'corpus', '.', 'read', '_', 'sequence', '_', 'list', '_', 'con', 'll', '(', 'input', '_', 'data', ',', 'max', '_', 'sent', '_', 'len', '=', '15', ',', 'max', '_', 'nr', '_', 'sent', '=', '1000', ')', '\\', 'n']
Detokenized (017): ['train_seq', '=', 'corpus', '.', 'read_sequence_list_conll', '(', 'input_data', ',', 'max_sent_len', '=', '15', ',', 'max_nr_sent', '=', '1000', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "pickle . dump ( ( corpus . word_dict , corpus . tag_dict ) , open ( , ) ) \n"
Original    (020): ['pickle', '.', 'dump', '(', '(', 'corpus', '.', 'word_dict', ',', 'corpus', '.', 'tag_dict', ')', ',', 'open', '(', ',', ')', ')', '\\n']
Tokenized   (028): ['<s>', 'pick', 'le', '.', 'dump', '(', '(', 'corpus', '.', 'word', '_', 'dict', ',', 'corpus', '.', 'tag', '_', 'dict', ')', ',', 'open', '(', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (026): ['pick', 'le', '.', 'dump', '(', '(', 'corpus', '.', 'word', '_', 'dict', ',', 'corpus', '.', 'tag', '_', 'dict', ')', ',', 'open', '(', ',', ')', ')', '\\', 'n']
Detokenized (020): ['pickle', '.', 'dump', '(', '(', 'corpus', '.', 'word_dict', ',', 'corpus', '.', 'tag_dict', ')', ',', 'open', '(', ',', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "total = np . zeros ( self . features . n_feats ) \n"
Original    (013): ['total', '=', 'np', '.', 'zeros', '(', 'self', '.', 'features', '.', 'n_feats', ')', '\\n']
Tokenized   (020): ['<s>', 'total', '=', 'np', '.', 'z', 'eros', '(', 'self', '.', 'features', '.', 'n', '_', 'fe', 'ats', ')', '\\', 'n', '</s>']
Filtered   (018): ['total', '=', 'np', '.', 'z', 'eros', '(', 'self', '.', 'features', '.', 'n', '_', 'fe', 'ats', ')', '\\', 'n']
Detokenized (013): ['total', '=', 'np', '.', 'zeros', '(', 'self', '.', 'features', '.', 'n_feats', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "scores = self . features . compute_scores ( feats , self . weights ) \n"
Original    (015): ['scores', '=', 'self', '.', 'features', '.', 'compute_scores', '(', 'feats', ',', 'self', '.', 'weights', ')', '\\n']
Tokenized   (022): ['<s>', 'sc', 'ores', '=', 'self', '.', 'features', '.', 'compute', '_', 'sc', 'ores', '(', 'feats', ',', 'self', '.', 'weights', ')', '\\', 'n', '</s>']
Filtered   (020): ['sc', 'ores', '=', 'self', '.', 'features', '.', 'compute', '_', 'sc', 'ores', '(', 'feats', ',', 'self', '.', 'weights', ')', '\\', 'n']
Detokenized (015): ['scores', '=', 'self', '.', 'features', '.', 'compute_scores', '(', 'feats', ',', 'self', '.', 'weights', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "t0 = 1.0 / ( sigma * eta0 ) \n"
Original    (010): ['t0', '=', '1.0', '/', '(', 'sigma', '*', 'eta0', ')', '\\n']
Tokenized   (019): ['<s>', 't', '0', '=', '1', '.', '0', '/', '(', 's', 'igma', '*', 'et', 'a', '0', ')', '\\', 'n', '</s>']
Filtered   (017): ['t', '0', '=', '1', '.', '0', '/', '(', 's', 'igma', '*', 'et', 'a', '0', ')', '\\', 'n']
Detokenized (010): ['t0', '=', '1.0', '/', '(', 'sigma', '*', 'eta0', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "marginals , logZ = self . decoder . parse_marginals_nonproj ( scores ) \n"
Original    (013): ['marginals', ',', 'logZ', '=', 'self', '.', 'decoder', '.', 'parse_marginals_nonproj', '(', 'scores', ')', '\\n']
Tokenized   (026): ['<s>', 'marg', 'inals', ',', 'log', 'Z', '=', 'self', '.', 'dec', 'oder', '.', 'parse', '_', 'marg', 'inals', '_', 'non', 'pro', 'j', '(', 'scores', ')', '\\', 'n', '</s>']
Filtered   (024): ['marg', 'inals', ',', 'log', 'Z', '=', 'self', '.', 'dec', 'oder', '.', 'parse', '_', 'marg', 'inals', '_', 'non', 'pro', 'j', '(', 'scores', ')', '\\', 'n']
Detokenized (013): ['marginals', ',', 'logZ', '=', 'self', '.', 'decoder', '.', 'parse_marginals_nonproj', '(', 'scores', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "score_corr += scores [ h , m ] \n"
Original    (009): ['score_corr', '+=', 'scores', '[', 'h', ',', 'm', ']', '\\n']
Tokenized   (015): ['<s>', 'score', '_', 'cor', 'r', '+=', 'scores', '[', 'h', ',', 'm', ']', '\\', 'n', '</s>']
Filtered   (013): ['score', '_', 'cor', 'r', '+=', 'scores', '[', 'h', ',', 'm', ']', '\\', 'n']
Detokenized (009): ['score_corr', '+=', 'scores', '[', 'h', ',', 'm', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "features = self . add_final_features ( sequence , sequence . y [ - 1 ] , features ) \n"
Original    (019): ['features', '=', 'self', '.', 'add_final_features', '(', 'sequence', ',', 'sequence', '.', 'y', '[', '-', '1', ']', ',', 'features', ')', '\\n']
Tokenized   (026): ['<s>', 'features', '=', 'self', '.', 'add', '_', 'final', '_', 'features', '(', 'sequence', ',', 'sequence', '.', 'y', '[', '-', '1', ']', ',', 'features', ')', '\\', 'n', '</s>']
Filtered   (024): ['features', '=', 'self', '.', 'add', '_', 'final', '_', 'features', '(', 'sequence', ',', 'sequence', '.', 'y', '[', '-', '1', ']', ',', 'features', ')', '\\', 'n']
Detokenized (019): ['features', '=', 'self', '.', 'add_final_features', '(', 'sequence', ',', 'sequence', '.', 'y', '[', '-', '1', ']', ',', 'features', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "node_idx = self . add_emission_features ( sequence , pos , y , node_idx ) \n"
Original    (015): ['node_idx', '=', 'self', '.', 'add_emission_features', '(', 'sequence', ',', 'pos', ',', 'y', ',', 'node_idx', ')', '\\n']
Tokenized   (029): ['<s>', 'node', '_', 'id', 'x', '=', 'self', '.', 'add', '_', 'em', 'ission', '_', 'features', '(', 'sequence', ',', 'pos', ',', 'y', ',', 'node', '_', 'id', 'x', ')', '\\', 'n', '</s>']
Filtered   (027): ['node', '_', 'id', 'x', '=', 'self', '.', 'add', '_', 'em', 'ission', '_', 'features', '(', 'sequence', ',', 'pos', ',', 'y', ',', 'node', '_', 'id', 'x', ')', '\\', 'n']
Detokenized (015): ['node_idx', '=', 'self', '.', 'add_emission_features', '(', 'sequence', ',', 'pos', ',', 'y', ',', 'node_idx', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "edge_idx = self . add_final_features ( sequence , y_prev , edge_idx ) \n"
Original    (013): ['edge_idx', '=', 'self', '.', 'add_final_features', '(', 'sequence', ',', 'y_prev', ',', 'edge_idx', ')', '\\n']
Tokenized   (028): ['<s>', 'edge', '_', 'id', 'x', '=', 'self', '.', 'add', '_', 'final', '_', 'features', '(', 'sequence', ',', 'y', '_', 'prev', ',', 'edge', '_', 'id', 'x', ')', '\\', 'n', '</s>']
Filtered   (026): ['edge', '_', 'id', 'x', '=', 'self', '.', 'add', '_', 'final', '_', 'features', '(', 'sequence', ',', 'y', '_', 'prev', ',', 'edge', '_', 'id', 'x', ')', '\\', 'n']
Detokenized (013): ['edge_idx', '=', 'self', '.', 'add_final_features', '(', 'sequence', ',', 'y_prev', ',', 'edge_idx', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "y_name = self . dataset . y_dict . get_label_name ( y ) \n"
Original    (013): ['y_name', '=', 'self', '.', 'dataset', '.', 'y_dict', '.', 'get_label_name', '(', 'y', ')', '\\n']
Tokenized   (024): ['<s>', 'y', '_', 'name', '=', 'self', '.', 'dataset', '.', 'y', '_', 'dict', '.', 'get', '_', 'label', '_', 'name', '(', 'y', ')', '\\', 'n', '</s>']
Filtered   (022): ['y', '_', 'name', '=', 'self', '.', 'dataset', '.', 'y', '_', 'dict', '.', 'get', '_', 'label', '_', 'name', '(', 'y', ')', '\\', 'n']
Detokenized (013): ['y_name', '=', 'self', '.', 'dataset', '.', 'y_dict', '.', 'get_label_name', '(', 'y', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "feat_name = "prev_tag:%s::%s" % ( y_prev_name , y_name ) \n"
Original    (010): ['feat_name', '=', '"prev_tag:%s::%s"', '%', '(', 'y_prev_name', ',', 'y_name', ')', '\\n']
Tokenized   (031): ['<s>', 'feat', '_', 'name', '=', '"', 'prev', '_', 'tag', ':', '%', 's', '::', '%', 's', '"', '%', '(', 'y', '_', 'prev', '_', 'name', ',', 'y', '_', 'name', ')', '\\', 'n', '</s>']
Filtered   (029): ['feat', '_', 'name', '=', '"', 'prev', '_', 'tag', ':', '%', 's', '::', '%', 's', '"', '%', '(', 'y', '_', 'prev', '_', 'name', ',', 'y', '_', 'name', ')', '\\', 'n']
Detokenized (010): ['feat_name', '=', '"prev_tag:%s::%s"', '%', '(', 'y_prev_name', ',', 'y_name', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "point_geom . AddPoint ( point [ 0 ] , point [ 1 ] ) \n"
Original    (015): ['point_geom', '.', 'AddPoint', '(', 'point', '[', '0', ']', ',', 'point', '[', '1', ']', ')', '\\n']
Tokenized   (022): ['<s>', 'point', '_', 'ge', 'om', '.', 'Add', 'Point', '(', 'point', '[', '0', ']', ',', 'point', '[', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (020): ['point', '_', 'ge', 'om', '.', 'Add', 'Point', '(', 'point', '[', '0', ']', ',', 'point', '[', '1', ']', ')', '\\', 'n']
Detokenized (015): ['point_geom', '.', 'AddPoint', '(', 'point', '[', '0', ']', ',', 'point', '[', '1', ']', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "longitudes = [ 100 , 110 , 120 , 130 , 140 ] \n"
Original    (014): ['longitudes', '=', '[', '100', ',', '110', ',', '120', ',', '130', ',', '140', ']', '\\n']
Tokenized   (018): ['<s>', 'long', 'itudes', '=', '[', '100', ',', '110', ',', '120', ',', '130', ',', '140', ']', '\\', 'n', '</s>']
Filtered   (016): ['long', 'itudes', '=', '[', '100', ',', '110', ',', '120', ',', '130', ',', '140', ']', '\\', 'n']
Detokenized (014): ['longitudes', '=', '[', '100', ',', '110', ',', '120', ',', '130', ',', '140', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "point_1 . AddPoint ( longitudes [ 0 ] , latitudes [ 0 ] , elevation ) \n"
Original    (017): ['point_1', '.', 'AddPoint', '(', 'longitudes', '[', '0', ']', ',', 'latitudes', '[', '0', ']', ',', 'elevation', ')', '\\n']
Tokenized   (025): ['<s>', 'point', '_', '1', '.', 'Add', 'Point', '(', 'long', 'itudes', '[', '0', ']', ',', 'lat', 'itudes', '[', '0', ']', ',', 'elevation', ')', '\\', 'n', '</s>']
Filtered   (023): ['point', '_', '1', '.', 'Add', 'Point', '(', 'long', 'itudes', '[', '0', ']', ',', 'lat', 'itudes', '[', '0', ']', ',', 'elevation', ')', '\\', 'n']
Detokenized (017): ['point_1', '.', 'AddPoint', '(', 'longitudes', '[', '0', ']', ',', 'latitudes', '[', '0', ']', ',', 'elevation', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "di = np . array ( [ x for x in range ( projected_X . shape [ 1 ] ) ] ) \n"
Original    (023): ['di', '=', 'np', '.', 'array', '(', '[', 'x', 'for', 'x', 'in', 'range', '(', 'projected_X', '.', 'shape', '[', '1', ']', ')', ']', ')', '\\n']
Tokenized   (028): ['<s>', 'di', '=', 'np', '.', 'array', '(', '[', 'x', 'for', 'x', 'in', 'range', '(', 'projected', '_', 'X', '.', 'shape', '[', '1', ']', ')', ']', ')', '\\', 'n', '</s>']
Filtered   (026): ['di', '=', 'np', '.', 'array', '(', '[', 'x', 'for', 'x', 'in', 'range', '(', 'projected', '_', 'X', '.', 'shape', '[', '1', ']', ')', ']', ')', '\\', 'n']
Detokenized (023): ['di', '=', 'np', '.', 'array', '(', '[', 'x', 'for', 'x', 'in', 'range', '(', 'projected_X', '.', 'shape', '[', '1', ']', ')', ']', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "projected_X = np . c_ [ ids , projected_X ] \n"
Original    (011): ['projected_X', '=', 'np', '.', 'c_', '[', 'ids', ',', 'projected_X', ']', '\\n']
Tokenized   (021): ['<s>', 'project', 'ed', '_', 'X', '=', 'np', '.', 'c', '_', '[', '', 'ids', ',', 'projected', '_', 'X', ']', '\\', 'n', '</s>']
Filtered   (019): ['project', 'ed', '_', 'X', '=', 'np', '.', 'c', '_', '[', '', 'ids', ',', 'projected', '_', 'X', ']', '\\', 'n']
Detokenized (011): ['projected_X', '=', 'np', '.', 'c_', '[', 'ids', ',', 'projected_X', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "clusterer . fit ( inverse_x [ : , 1 : ] ) \n"
Original    (013): ['clusterer', '.', 'fit', '(', 'inverse_x', '[', ':', ',', '1', ':', ']', ')', '\\n']
Tokenized   (020): ['<s>', 'cl', 'ust', 'erer', '.', 'fit', '(', 'inverse', '_', 'x', '[', ':', ',', '1', ':', ']', ')', '\\', 'n', '</s>']
Filtered   (018): ['cl', 'ust', 'erer', '.', 'fit', '(', 'inverse', '_', 'x', '[', ':', ',', '1', ':', ']', ')', '\\', 'n']
Detokenized (013): ['clusterer', '.', 'fit', '(', 'inverse_x', '[', ':', ',', '1', ':', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "complex [ "meta" ] = self . projection \n"
Original    (009): ['complex', '[', '"meta"', ']', '=', 'self', '.', 'projection', '\\n']
Tokenized   (014): ['<s>', 'complex', '[', '"', 'meta', '"', ']', '=', 'self', '.', 'projection', '\\', 'n', '</s>']
Filtered   (012): ['complex', '[', '"', 'meta', '"', ']', '=', 'self', '.', 'projection', '\\', 'n']
Detokenized (009): ['complex', '[', '"meta"', ']', '=', 'self', '.', 'projection', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( complex ~~ k2e [ k ] = e \n"
Original    (040): ['json_s', '[', '"nodes"', ']', '.', 'append', '(', '{', '"name"', ':', 'str', '(', 'k', ')', ',', '"tooltip"', ':', 'tooltip_s', ',', '"group"', ':', '2', '*', 'int', '(', 'np', '.', 'log', '(', 'len', '(', 'complex', '~~', 'k2e', '[', 'k', ']', '=', 'e', '\\n']
Tokenized   (060): ['<s>', 'json', '_', 's', '[', '"', 'n', 'odes', '"', ']', '.', 'append', '(', '{', '"', 'name', '"', ':', 'str', '(', 'k', ')', ',', '"', 'tool', 'tip', '"', ':', 'tooltip', '_', 's', ',', '"', 'group', '"', ':', '2', '*', 'int', '(', 'np', '.', 'log', '(', 'len', '(', 'complex', '', '~~', 'k', '2', 'e', '[', 'k', ']', '=', 'e', '\\', 'n', '</s>']
Filtered   (058): ['json', '_', 's', '[', '"', 'n', 'odes', '"', ']', '.', 'append', '(', '{', '"', 'name', '"', ':', 'str', '(', 'k', ')', ',', '"', 'tool', 'tip', '"', ':', 'tooltip', '_', 's', ',', '"', 'group', '"', ':', '2', '*', 'int', '(', 'np', '.', 'log', '(', 'len', '(', 'complex', '', '~~', 'k', '2', 'e', '[', 'k', ']', '=', 'e', '\\', 'n']
Detokenized (040): ['json_s', '[', '"nodes"', ']', '.', 'append', '(', '{', '"name"', ':', 'str', '(', 'k', ')', ',', '"tooltip"', ':', 'tooltip_s', ',', '"group"', ':', '2', '*', 'int', '(', 'np', '.', 'log', '(', 'len', '(', 'complex', '~~', 'k2e', '[', 'k', ']', '=', 'e', '\\n']
Counter: 58
===================================================================
Hidden states:  (13, 40, 768)
# Extracted words:  40
Sentence         : "width_js = "%s" % width_html \n"
Original    (006): ['width_js', '=', '"%s"', '%', 'width_html', '\\n']
Tokenized   (015): ['<s>', 'width', '_', 'js', '=', '"%', 's', '"', '%', 'width', '_', 'html', '\\', 'n', '</s>']
Filtered   (013): ['width', '_', 'js', '=', '"%', 's', '"', '%', 'width', '_', 'html', '\\', 'n']
Detokenized (006): ['width_js', '=', '"%s"', '%', 'width_html', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "new_settings [ interface ] [ ] [ % protocol ] = server \n"
Original    (013): ['new_settings', '[', 'interface', ']', '[', ']', '[', '%', 'protocol', ']', '=', 'server', '\\n']
Tokenized   (018): ['<s>', 'new', '_', 'settings', '[', 'interface', ']', '[', ']', '[', '%', 'protocol', ']', '=', 'server', '\\', 'n', '</s>']
Filtered   (016): ['new', '_', 'settings', '[', 'interface', ']', '[', ']', '[', '%', 'protocol', ']', '=', 'server', '\\', 'n']
Detokenized (013): ['new_settings', '[', 'interface', ']', '[', ']', '[', '%', 'protocol', ']', '=', 'server', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "setups = ( less_setup , sass_setup , stylus_setup , sass_erb_setup ) \n"
Original    (012): ['setups', '=', '(', 'less_setup', ',', 'sass_setup', ',', 'stylus_setup', ',', 'sass_erb_setup', ')', '\\n']
Tokenized   (029): ['<s>', 'set', 'ups', '=', '(', 'less', '_', 'setup', ',', 's', 'ass', '_', 'setup', ',', 'styl', 'us', '_', 'setup', ',', 's', 'ass', '_', 'erb', '_', 'setup', ')', '\\', 'n', '</s>']
Filtered   (027): ['set', 'ups', '=', '(', 'less', '_', 'setup', ',', 's', 'ass', '_', 'setup', ',', 'styl', 'us', '_', 'setup', ',', 's', 'ass', '_', 'erb', '_', 'setup', ')', '\\', 'n']
Detokenized (012): ['setups', '=', '(', 'less_setup', ',', 'sass_setup', ',', 'stylus_setup', ',', 'sass_erb_setup', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "fn = self . view . file_name ( ) . encode ( "utf_8" ) \n"
Original    (015): ['fn', '=', 'self', '.', 'view', '.', 'file_name', '(', ')', '.', 'encode', '(', '"utf_8"', ')', '\\n']
Tokenized   (024): ['<s>', 'fn', '=', 'self', '.', 'view', '.', 'file', '_', 'name', '(', ')', '.', 'encode', '(', '"', 'utf', '_', '8', '"', ')', '\\', 'n', '</s>']
Filtered   (022): ['fn', '=', 'self', '.', 'view', '.', 'file', '_', 'name', '(', ')', '.', 'encode', '(', '"', 'utf', '_', '8', '"', ')', '\\', 'n']
Detokenized (015): ['fn', '=', 'self', '.', 'view', '.', 'file_name', '(', ')', '.', 'encode', '(', '"utf_8"', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "compiled_regex = re . compile ( chosen_setup . regex , re . MULTILINE ) \n"
Original    (015): ['compiled_regex', '=', 're', '.', 'compile', '(', 'chosen_setup', '.', 'regex', ',', 're', '.', 'MULTILINE', ')', '\\n']
Tokenized   (027): ['<s>', 'comp', 'iled', '_', 're', 'gex', '=', 're', '.', 'compile', '(', 'chosen', '_', 'setup', '.', 'regex', ',', 're', '.', 'M', 'ULT', 'IL', 'INE', ')', '\\', 'n', '</s>']
Filtered   (025): ['comp', 'iled', '_', 're', 'gex', '=', 're', '.', 'compile', '(', 'chosen', '_', 'setup', '.', 'regex', ',', 're', '.', 'M', 'ULT', 'IL', 'INE', ')', '\\', 'n']
Detokenized (015): ['compiled_regex', '=', 're', '.', 'compile', '(', 'chosen_setup', '.', 'regex', ',', 're', '.', 'MULTILINE', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "file_dir = os . path . dirname ( fn ) . decode ( "utf-8" ) \n"
Original    (016): ['file_dir', '=', 'os', '.', 'path', '.', 'dirname', '(', 'fn', ')', '.', 'decode', '(', '"utf-8"', ')', '\\n']
Tokenized   (026): ['<s>', 'file', '_', 'dir', '=', 'os', '.', 'path', '.', 'dir', 'name', '(', 'fn', ')', '.', 'decode', '(', '"', 'utf', '-', '8', '"', ')', '\\', 'n', '</s>']
Filtered   (024): ['file', '_', 'dir', '=', 'os', '.', 'path', '.', 'dir', 'name', '(', 'fn', ')', '.', 'decode', '(', '"', 'utf', '-', '8', '"', ')', '\\', 'n']
Detokenized (016): ['file_dir', '=', 'os', '.', 'path', '.', 'dirname', '(', 'fn', ')', '.', 'decode', '(', '"utf-8"', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "partial_filename = fn_split [ 0 ] + "/_" + fn_split [ 1 ] \n"
Original    (014): ['partial_filename', '=', 'fn_split', '[', '0', ']', '+', '"/_"', '+', 'fn_split', '[', '1', ']', '\\n']
Tokenized   (025): ['<s>', 'partial', '_', 'filename', '=', 'fn', '_', 'split', '[', '0', ']', '+', '"/', '_', '"', '+', 'fn', '_', 'split', '[', '1', ']', '\\', 'n', '</s>']
Filtered   (023): ['partial', '_', 'filename', '=', 'fn', '_', 'split', '[', '0', ']', '+', '"/', '_', '"', '+', 'fn', '_', 'split', '[', '1', ']', '\\', 'n']
Detokenized (014): ['partial_filename', '=', 'fn_split', '[', '0', ']', '+', '"/_"', '+', 'fn_split', '[', '1', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "imported_vars = imported_vars + m \n"
Original    (006): ['imported_vars', '=', 'imported_vars', '+', 'm', '\\n']
Tokenized   (016): ['<s>', 'im', 'ported', '_', 'v', 'ars', '=', 'imported', '_', 'v', 'ars', '+', 'm', '\\', 'n', '</s>']
Filtered   (014): ['im', 'ported', '_', 'v', 'ars', '=', 'imported', '_', 'v', 'ars', '+', 'm', '\\', 'n']
Detokenized (006): ['imported_vars', '=', 'imported_vars', '+', 'm', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : ""params" : [ { \n"
Original    (005): ['"params"', ':', '[', '{', '\\n']
Tokenized   (010): ['<s>', '"', 'params', '"', ':', '[', '{', '\\', 'n', '</s>']
Filtered   (008): ['"', 'params', '"', ':', '[', '{', '\\', 'n']
Detokenized (005): ['"params"', ':', '[', '{', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "LAT_MAX = + 90.0 \n"
Original    (005): ['LAT_MAX', '=', '+', '90.0', '\\n']
Tokenized   (013): ['<s>', 'L', 'AT', '_', 'MAX', '=', '+', '90', '.', '0', '\\', 'n', '</s>']
Filtered   (011): ['L', 'AT', '_', 'MAX', '=', '+', '90', '.', '0', '\\', 'n']
Detokenized (005): ['LAT_MAX', '=', '+', '90.0', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""purple" , "teal" , "lightgray" ] \n"
Original    (007): ['"purple"', ',', '"teal"', ',', '"lightgray"', ']', '\\n']
Tokenized   (019): ['<s>', '"', 'pur', 'ple', '"', ',', '"', 'te', 'al', '"', ',', '"', 'light', 'gray', '"', ']', '\\', 'n', '</s>']
Filtered   (017): ['"', 'pur', 'ple', '"', ',', '"', 'te', 'al', '"', ',', '"', 'light', 'gray', '"', ']', '\\', 'n']
Detokenized (007): ['"purple"', ',', '"teal"', ',', '"lightgray"', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "numrange = None , default = None , max_width = 72 ) : \n"
Original    (014): ['numrange', '=', 'None', ',', 'default', '=', 'None', ',', 'max_width', '=', '72', ')', ':', '\\n']
Tokenized   (020): ['<s>', 'num', 'range', '=', 'None', ',', 'default', '=', 'None', ',', 'max', '_', 'width', '=', '72', ')', ':', '\\', 'n', '</s>']
Filtered   (018): ['num', 'range', '=', 'None', ',', 'default', '=', 'None', ',', 'max', '_', 'width', '=', '72', ')', ':', '\\', 'n']
Detokenized (014): ['numrange', '=', 'None', ',', 'default', '=', 'None', ',', 'max_width', '=', '72', ')', ':', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "found_letter . lower ( ) == default . lower ( ) ) ) : \n"
Original    (015): ['found_letter', '.', 'lower', '(', ')', '==', 'default', '.', 'lower', '(', ')', ')', ')', ':', '\\n']
Tokenized   (020): ['<s>', 'found', '_', 'letter', '.', 'lower', '(', ')', '==', 'default', '.', 'lower', '(', ')', ')', ')', ':', '\\', 'n', '</s>']
Filtered   (018): ['found', '_', 'letter', '.', 'lower', '(', ')', '==', 'default', '.', 'lower', '(', ')', ')', ')', ':', '\\', 'n']
Detokenized (015): ['found_letter', '.', 'lower', '(', ')', '==', 'default', '.', 'lower', '(', ')', ')', ')', ':', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "option [ : index ] + show_letter + option [ index + 1 : ] \n"
Original    (016): ['option', '[', ':', 'index', ']', '+', 'show_letter', '+', 'option', '[', 'index', '+', '1', ':', ']', '\\n']
Tokenized   (021): ['<s>', 'option', '[', ':', 'index', ']', '+', 'show', '_', 'letter', '+', 'option', '[', 'index', '+', '1', ':', ']', '\\', 'n', '</s>']
Filtered   (019): ['option', '[', ':', 'index', ']', '+', 'show', '_', 'letter', '+', 'option', '[', 'index', '+', '1', ':', ']', '\\', 'n']
Detokenized (016): ['option', '[', ':', 'index', ']', '+', 'show_letter', '+', 'option', '[', 'index', '+', '1', ':', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "display_letters . append ( found_letter . upper ( ) ) \n"
Original    (011): ['display_letters', '.', 'append', '(', 'found_letter', '.', 'upper', '(', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'display', '_', 'letters', '.', 'append', '(', 'found', '_', 'letter', '.', 'upper', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['display', '_', 'letters', '.', 'append', '(', 'found', '_', 'letter', '.', 'upper', '(', ')', ')', '\\', 'n']
Detokenized (011): ['display_letters', '.', 'append', '(', 'found_letter', '.', 'upper', '(', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "default_name = self . colorize ( , default_name ) \n"
Original    (010): ['default_name', '=', 'self', '.', 'colorize', '(', ',', 'default_name', ')', '\\n']
Tokenized   (018): ['<s>', 'default', '_', 'name', '=', 'self', '.', 'color', 'ize', '(', ',', 'default', '_', 'name', ')', '\\', 'n', '</s>']
Filtered   (016): ['default', '_', 'name', '=', 'self', '.', 'color', 'ize', '(', ',', 'default', '_', 'name', ')', '\\', 'n']
Detokenized (010): ['default_name', '=', 'self', '.', 'colorize', '(', ',', 'default_name', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "prompt_parts . append ( tmpl % default_name ) \n"
Original    (009): ['prompt_parts', '.', 'append', '(', 'tmpl', '%', 'default_name', ')', '\\n']
Tokenized   (019): ['<s>', 'prom', 'pt', '_', 'parts', '.', 'append', '(', 't', 'm', 'pl', '%', 'default', '_', 'name', ')', '\\', 'n', '</s>']
Filtered   (017): ['prom', 'pt', '_', 'parts', '.', 'append', '(', 't', 'm', 'pl', '%', 'default', '_', 'name', ')', '\\', 'n']
Detokenized (009): ['prompt_parts', '.', 'append', '(', 'tmpl', '%', 'default_name', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "matcher = SequenceMatcher ( lambda x : False , a , b ) \n"
Original    (014): ['matcher', '=', 'SequenceMatcher', '(', 'lambda', 'x', ':', 'False', ',', 'a', ',', 'b', ')', '\\n']
Tokenized   (020): ['<s>', 'mat', 'cher', '=', 'Sequence', 'Mat', 'cher', '(', 'lambda', 'x', ':', 'False', ',', 'a', ',', 'b', ')', '\\', 'n', '</s>']
Filtered   (018): ['mat', 'cher', '=', 'Sequence', 'Mat', 'cher', '(', 'lambda', 'x', ':', 'False', ',', 'a', ',', 'b', ')', '\\', 'n']
Detokenized (014): ['matcher', '=', 'SequenceMatcher', '(', 'lambda', 'x', ':', 'False', ',', 'a', ',', 'b', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "b_out . append ( self . colorize ( color , b [ b_start : b_end ] ) ) \n"
Original    (019): ['b_out', '.', 'append', '(', 'self', '.', 'colorize', '(', 'color', ',', 'b', '[', 'b_start', ':', 'b_end', ']', ')', ')', '\\n']
Tokenized   (029): ['<s>', 'b', '_', 'out', '.', 'append', '(', 'self', '.', 'color', 'ize', '(', 'color', ',', 'b', '[', 'b', '_', 'start', ':', 'b', '_', 'end', ']', ')', ')', '\\', 'n', '</s>']
Filtered   (027): ['b', '_', 'out', '.', 'append', '(', 'self', '.', 'color', 'ize', '(', 'color', ',', 'b', '[', 'b', '_', 'start', ':', 'b', '_', 'end', ']', ')', ')', '\\', 'n']
Detokenized (019): ['b_out', '.', 'append', '(', 'self', '.', 'colorize', '(', 'color', ',', 'b', '[', 'b_start', ':', 'b_end', ']', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "variable = % varname \n"
Original    (005): ['variable', '=', '%', 'varname', '\\n']
Tokenized   (009): ['<s>', 'variable', '=', '%', 'var', 'name', '\\', 'n', '</s>']
Filtered   (007): ['variable', '=', '%', 'var', 'name', '\\', 'n']
Detokenized (005): ['variable', '=', '%', 'varname', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "62 : , \n"
Original    (004): ['62', ':', ',', '\\n']
Tokenized   (007): ['<s>', '62', ':', ',', '\\', 'n', '</s>']
Filtered   (005): ['62', ':', ',', '\\', 'n']
Detokenized (004): ['62', ':', ',', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "_push . update ( { \n"
Original    (006): ['_push', '.', 'update', '(', '{', '\\n']
Tokenized   (010): ['<s>', '_', 'push', '.', 'update', '(', '{', '\\', 'n', '</s>']
Filtered   (008): ['_', 'push', '.', 'update', '(', '{', '\\', 'n']
Detokenized (006): ['_push', '.', 'update', '(', '{', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "_readonly = Entity . _readonly | { , } \n"
Original    (010): ['_readonly', '=', 'Entity', '.', '_readonly', '|', '{', ',', '}', '\\n']
Tokenized   (017): ['<s>', '_', 'read', 'only', '=', 'Entity', '.', '_', 'read', 'only', '|', '{', ',', '}', '\\', 'n', '</s>']
Filtered   (015): ['_', 'read', 'only', '=', 'Entity', '.', '_', 'read', 'only', '|', '{', ',', '}', '\\', 'n']
Detokenized (010): ['_readonly', '=', 'Entity', '.', '_readonly', '|', '{', ',', '}', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "remove_ids = [ 6 , 7 ] \n"
Original    (008): ['remove_ids', '=', '[', '6', ',', '7', ']', '\\n']
Tokenized   (013): ['<s>', 'remove', '_', 'ids', '=', '[', '6', ',', '7', ']', '\\', 'n', '</s>']
Filtered   (011): ['remove', '_', 'ids', '=', '[', '6', ',', '7', ']', '\\', 'n']
Detokenized (008): ['remove_ids', '=', '[', '6', ',', '7', ']', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "remove_advertiser_ids = [ 8 , 9 , 10 ] \n"
Original    (010): ['remove_advertiser_ids', '=', '[', '8', ',', '9', ',', '10', ']', '\\n']
Tokenized   (019): ['<s>', 'remove', '_', 'ad', 'vertis', 'er', '_', 'ids', '=', '[', '8', ',', '9', ',', '10', ']', '\\', 'n', '</s>']
Filtered   (017): ['remove', '_', 'ad', 'vertis', 'er', '_', 'ids', '=', '[', '8', ',', '9', ',', '10', ']', '\\', 'n']
Detokenized (010): ['remove_advertiser_ids', '=', '[', '8', ',', '9', ',', '10', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "num_users , num_items = dataset . shape \n"
Original    (008): ['num_users', ',', 'num_items', '=', 'dataset', '.', 'shape', '\\n']
Tokenized   (015): ['<s>', 'num', '_', 'users', ',', 'num', '_', 'items', '=', 'dataset', '.', 'shape', '\\', 'n', '</s>']
Filtered   (013): ['num', '_', 'users', ',', 'num', '_', 'items', '=', 'dataset', '.', 'shape', '\\', 'n']
Detokenized (008): ['num_users', ',', 'num_items', '=', 'dataset', '.', 'shape', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "async_job = view . map_async ( process , tasks , retries = 2 ) \n"
Original    (015): ['async_job', '=', 'view', '.', 'map_async', '(', 'process', ',', 'tasks', ',', 'retries', '=', '2', ')', '\\n']
Tokenized   (025): ['<s>', 'as', 'ync', '_', 'job', '=', 'view', '.', 'map', '_', 'as', 'ync', '(', 'process', ',', 'tasks', ',', 'ret', 'ries', '=', '2', ')', '\\', 'n', '</s>']
Filtered   (023): ['as', 'ync', '_', 'job', '=', 'view', '.', 'map', '_', 'as', 'ync', '(', 'process', ',', 'tasks', ',', 'ret', 'ries', '=', '2', ')', '\\', 'n']
Detokenized (015): ['async_job', '=', 'view', '.', 'map_async', '(', 'process', ',', 'tasks', ',', 'retries', '=', '2', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "remaining = len ( tasks ) - len ( done ) \n"
Original    (012): ['remaining', '=', 'len', '(', 'tasks', ')', '-', 'len', '(', 'done', ')', '\\n']
Tokenized   (016): ['<s>', 'rem', 'aining', '=', 'len', '(', 'tasks', ')', '-', 'len', '(', 'done', ')', '\\', 'n', '</s>']
Filtered   (014): ['rem', 'aining', '=', 'len', '(', 'tasks', ')', '-', 'len', '(', 'done', ')', '\\', 'n']
Detokenized (012): ['remaining', '=', 'len', '(', 'tasks', ')', '-', 'len', '(', 'done', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "num_items , type ( model ) . __name__ , simsfile ) \n"
Original    (012): ['num_items', ',', 'type', '(', 'model', ')', '.', '__name__', ',', 'simsfile', ')', '\\n']
Tokenized   (021): ['<s>', 'num', '_', 'items', ',', 'type', '(', 'model', ')', '.', '__', 'name', '__', ',', 'sim', 's', 'file', ')', '\\', 'n', '</s>']
Filtered   (019): ['num', '_', 'items', ',', 'type', '(', 'model', ')', '.', '__', 'name', '__', ',', 'sim', 's', 'file', ')', '\\', 'n']
Detokenized (012): ['num_items', ',', 'type', '(', 'model', ')', '.', '__name__', ',', 'simsfile', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "before = text [ : len ( text ) - len ( like ) ] \n"
Original    (016): ['before', '=', 'text', '[', ':', 'len', '(', 'text', ')', '-', 'len', '(', 'like', ')', ']', '\\n']
Tokenized   (019): ['<s>', 'before', '=', 'text', '[', ':', 'len', '(', 'text', ')', '-', 'len', '(', 'like', ')', ']', '\\', 'n', '</s>']
Filtered   (017): ['before', '=', 'text', '[', ':', 'len', '(', 'text', ')', '-', 'len', '(', 'like', ')', ']', '\\', 'n']
Detokenized (016): ['before', '=', 'text', '[', ':', 'len', '(', 'text', ')', '-', 'len', '(', 'like', ')', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "Version = namedtuple ( , ) \n"
Original    (007): ['Version', '=', 'namedtuple', '(', ',', ')', '\\n']
Tokenized   (012): ['<s>', 'Version', '=', 'named', 't', 'uple', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (010): ['Version', '=', 'named', 't', 'uple', '(', ',', ')', '\\', 'n']
Detokenized (007): ['Version', '=', 'namedtuple', '(', ',', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "_defaults = collections . OrderedDict ( [ \n"
Original    (008): ['_defaults', '=', 'collections', '.', 'OrderedDict', '(', '[', '\\n']
Tokenized   (016): ['<s>', '_', 'default', 's', '=', 'collections', '.', 'Ord', 'ered', 'D', 'ict', '(', '[', '\\', 'n', '</s>']
Filtered   (014): ['_', 'default', 's', '=', 'collections', '.', 'Ord', 'ered', 'D', 'ict', '(', '[', '\\', 'n']
Detokenized (008): ['_defaults', '=', 'collections', '.', 'OrderedDict', '(', '[', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "rootDirectory = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , , ) \n"
Original    (027): ['rootDirectory', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'realpath', '(', '__file__', ')', ')', ',', ',', ')', '\\n']
Tokenized   (035): ['<s>', 'root', 'Directory', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'real', 'path', '(', '__', 'file', '__', ')', ')', ',', ',', ')', '\\', 'n', '</s>']
Filtered   (033): ['root', 'Directory', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'real', 'path', '(', '__', 'file', '__', ')', ')', ',', ',', ')', '\\', 'n']
Detokenized (027): ['rootDirectory', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'realpath', '(', '__file__', ')', ')', ',', ',', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "httpd . handle_request ( ) from . import TestEnable # \n"
Original    (011): ['httpd', '.', 'handle_request', '(', ')', 'from', '.', 'import', 'TestEnable', '#', '\\n']
Tokenized   (018): ['<s>', 'http', 'd', '.', 'handle', '_', 'request', '(', ')', 'from', '.', 'import', 'Test', 'Enable', '#', '\\', 'n', '</s>']
Filtered   (016): ['http', 'd', '.', 'handle', '_', 'request', '(', ')', 'from', '.', 'import', 'Test', 'Enable', '#', '\\', 'n']
Detokenized (011): ['httpd', '.', 'handle_request', '(', ')', 'from', '.', 'import', 'TestEnable', '#', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "field_names = tuple ( field_names ) , \n"
Original    (008): ['field_names', '=', 'tuple', '(', 'field_names', ')', ',', '\\n']
Tokenized   (015): ['<s>', 'field', '_', 'names', '=', 'tuple', '(', 'field', '_', 'names', ')', ',', '\\', 'n', '</s>']
Filtered   (013): ['field', '_', 'names', '=', 'tuple', '(', 'field', '_', 'names', ')', ',', '\\', 'n']
Detokenized (008): ['field_names', '=', 'tuple', '(', 'field_names', ')', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "arg_list = repr ( tuple ( field_names ) ) . replace ( "\'" , "" ) [ 1 : - 1 ] , \n"
Original    (024): ['arg_list', '=', 'repr', '(', 'tuple', '(', 'field_names', ')', ')', '.', 'replace', '(', '"\\\'"', ',', '""', ')', '[', '1', ':', '-', '1', ']', ',', '\\n']
Tokenized   (032): ['<s>', 'arg', '_', 'list', '=', 'repr', '(', 'tuple', '(', 'field', '_', 'names', ')', ')', '.', 'replace', '(', '"\\', '\'"', ',', '""', ')', '[', '1', ':', '-', '1', ']', ',', '\\', 'n', '</s>']
Filtered   (030): ['arg', '_', 'list', '=', 'repr', '(', 'tuple', '(', 'field', '_', 'names', ')', ')', '.', 'replace', '(', '"\\', '\'"', ',', '""', ')', '[', '1', ':', '-', '1', ']', ',', '\\', 'n']
Detokenized (024): ['arg_list', '=', 'repr', '(', 'tuple', '(', 'field_names', ')', ')', '.', 'replace', '(', '"\\\'"', ',', '""', ')', '[', '1', ':', '-', '1', ']', ',', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "repr_fmt = . join ( _repr_template . format ( name = name ) \n"
Original    (014): ['repr_fmt', '=', '.', 'join', '(', '_repr_template', '.', 'format', '(', 'name', '=', 'name', ')', '\\n']
Tokenized   (025): ['<s>', 're', 'pr', '_', 'f', 'mt', '=', '.', 'join', '(', '_', 're', 'pr', '_', 'template', '.', 'format', '(', 'name', '=', 'name', ')', '\\', 'n', '</s>']
Filtered   (023): ['re', 'pr', '_', 'f', 'mt', '=', '.', 'join', '(', '_', 're', 'pr', '_', 'template', '.', 'format', '(', 'name', '=', 'name', ')', '\\', 'n']
Detokenized (014): ['repr_fmt', '=', '.', 'join', '(', '_repr_template', '.', 'format', '(', 'name', '=', 'name', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "field_defs = . join ( _field_template . format ( index = index , name = name ) \n"
Original    (018): ['field_defs', '=', '.', 'join', '(', '_field_template', '.', 'format', '(', 'index', '=', 'index', ',', 'name', '=', 'name', ')', '\\n']
Tokenized   (027): ['<s>', 'field', '_', 'def', 's', '=', '.', 'join', '(', '_', 'field', '_', 'template', '.', 'format', '(', 'index', '=', 'index', ',', 'name', '=', 'name', ')', '\\', 'n', '</s>']
Filtered   (025): ['field', '_', 'def', 's', '=', '.', 'join', '(', '_', 'field', '_', 'template', '.', 'format', '(', 'index', '=', 'index', ',', 'name', '=', 'name', ')', '\\', 'n']
Detokenized (018): ['field_defs', '=', '.', 'join', '(', '_field_template', '.', 'format', '(', 'index', '=', 'index', ',', 'name', '=', 'name', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "OrderedDict = OrderedDict , _property = property , _tuple = tuple ) \n"
Original    (013): ['OrderedDict', '=', 'OrderedDict', ',', '_property', '=', 'property', ',', '_tuple', '=', 'tuple', ')', '\\n']
Tokenized   (025): ['<s>', 'Ord', 'ered', 'D', 'ict', '=', 'Ord', 'ered', 'D', 'ict', ',', '_', 'property', '=', 'property', ',', '_', 't', 'uple', '=', 'tuple', ')', '\\', 'n', '</s>']
Filtered   (023): ['Ord', 'ered', 'D', 'ict', '=', 'Ord', 'ered', 'D', 'ict', ',', '_', 'property', '=', 'property', ',', '_', 't', 'uple', '=', 'tuple', ')', '\\', 'n']
Detokenized (013): ['OrderedDict', '=', 'OrderedDict', ',', '_property', '=', 'property', ',', '_tuple', '=', 'tuple', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "xx = Xdf [ ] . values \n"
Original    (008): ['xx', '=', 'Xdf', '[', ']', '.', 'values', '\\n']
Tokenized   (012): ['<s>', 'xx', '=', 'X', 'df', '[', ']', '.', 'values', '\\', 'n', '</s>']
Filtered   (010): ['xx', '=', 'X', 'df', '[', ']', '.', 'values', '\\', 'n']
Detokenized (008): ['xx', '=', 'Xdf', '[', ']', '.', 'values', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "X_CD13 , Y_CD13 = util . get_data ( cd13 , y_names = [ , ] ) \n"
Original    (017): ['X_CD13', ',', 'Y_CD13', '=', 'util', '.', 'get_data', '(', 'cd13', ',', 'y_names', '=', '[', ',', ']', ')', '\\n']
Tokenized   (031): ['<s>', 'X', '_', 'CD', '13', ',', 'Y', '_', 'CD', '13', '=', 'util', '.', 'get', '_', 'data', '(', 'cd', '13', ',', 'y', '_', 'names', '=', '[', ',', ']', ')', '\\', 'n', '</s>']
Filtered   (029): ['X', '_', 'CD', '13', ',', 'Y', '_', 'CD', '13', '=', 'util', '.', 'get', '_', 'data', '(', 'cd', '13', ',', 'y', '_', 'names', '=', '[', ',', ']', ')', '\\', 'n']
Detokenized (017): ['X_CD13', ',', 'Y_CD13', '=', 'util', '.', 'get_data', '(', 'cd13', ',', 'y_names', '=', '[', ',', ']', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "cd33 = human_data . xs ( , level = , drop_level = False ) \n"
Original    (015): ['cd33', '=', 'human_data', '.', 'xs', '(', ',', 'level', '=', ',', 'drop_level', '=', 'False', ')', '\\n']
Tokenized   (024): ['<s>', 'cd', '33', '=', 'human', '_', 'data', '.', 'x', 's', '(', ',', 'level', '=', ',', 'drop', '_', 'level', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (022): ['cd', '33', '=', 'human', '_', 'data', '.', 'x', 's', '(', ',', 'level', '=', ',', 'drop', '_', 'level', '=', 'False', ')', '\\', 'n']
Detokenized (015): ['cd33', '=', 'human_data', '.', 'xs', '(', ',', 'level', '=', ',', 'drop_level', '=', 'False', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "X_CD33 , Y_CD33 = util . get_data ( cd33 , y_names = [ , , ] ) \n"
Original    (018): ['X_CD33', ',', 'Y_CD33', '=', 'util', '.', 'get_data', '(', 'cd33', ',', 'y_names', '=', '[', ',', ',', ']', ')', '\\n']
Tokenized   (032): ['<s>', 'X', '_', 'CD', '33', ',', 'Y', '_', 'CD', '33', '=', 'util', '.', 'get', '_', 'data', '(', 'cd', '33', ',', 'y', '_', 'names', '=', '[', ',', ',', ']', ')', '\\', 'n', '</s>']
Filtered   (030): ['X', '_', 'CD', '33', ',', 'Y', '_', 'CD', '33', '=', 'util', '.', 'get', '_', 'data', '(', 'cd', '33', ',', 'y', '_', 'names', '=', '[', ',', ',', ']', ')', '\\', 'n']
Detokenized (018): ['X_CD33', ',', 'Y_CD33', '=', 'util', '.', 'get_data', '(', 'cd33', ',', 'y_names', '=', '[', ',', ',', ']', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "X_CD15 , Y_CD15 = util . get_data ( cd15 , y_names = [ ] ) \n"
Original    (016): ['X_CD15', ',', 'Y_CD15', '=', 'util', '.', 'get_data', '(', 'cd15', ',', 'y_names', '=', '[', ']', ')', '\\n']
Tokenized   (030): ['<s>', 'X', '_', 'CD', '15', ',', 'Y', '_', 'CD', '15', '=', 'util', '.', 'get', '_', 'data', '(', 'cd', '15', ',', 'y', '_', 'names', '=', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (028): ['X', '_', 'CD', '15', ',', 'Y', '_', 'CD', '15', '=', 'util', '.', 'get', '_', 'data', '(', 'cd', '15', ',', 'y', '_', 'names', '=', '[', ']', ')', '\\', 'n']
Detokenized (016): ['X_CD15', ',', 'Y_CD15', '=', 'util', '.', 'get_data', '(', 'cd15', ',', 'y_names', '=', '[', ']', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "mouse_Y = pandas . concat ( [ mouse_Y , Y ] , axis = 0 ) \n"
Original    (017): ['mouse_Y', '=', 'pandas', '.', 'concat', '(', '[', 'mouse_Y', ',', 'Y', ']', ',', 'axis', '=', '0', ')', '\\n']
Tokenized   (026): ['<s>', 'mouse', '_', 'Y', '=', 'pand', 'as', '.', 'conc', 'at', '(', '[', 'mouse', '_', 'Y', ',', 'Y', ']', ',', 'axis', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (024): ['mouse', '_', 'Y', '=', 'pand', 'as', '.', 'conc', 'at', '(', '[', 'mouse', '_', 'Y', ',', 'Y', ']', ',', 'axis', '=', '0', ')', '\\', 'n']
Detokenized (017): ['mouse_Y', '=', 'pandas', '.', 'concat', '(', '[', 'mouse_Y', ',', 'Y', ']', ',', 'axis', '=', '0', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "mouse_data = pandas . read_excel ( data_file , sheetname = 1 , index_col = [ 0 , 1 ] ) \n"
Original    (021): ['mouse_data', '=', 'pandas', '.', 'read_excel', '(', 'data_file', ',', 'sheetname', '=', '1', ',', 'index_col', '=', '[', '0', ',', '1', ']', ')', '\\n']
Tokenized   (035): ['<s>', 'mouse', '_', 'data', '=', 'pand', 'as', '.', 'read', '_', 'ex', 'cel', '(', 'data', '_', 'file', ',', 'sheet', 'name', '=', '1', ',', 'index', '_', 'col', '=', '[', '0', ',', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (033): ['mouse', '_', 'data', '=', 'pand', 'as', '.', 'read', '_', 'ex', 'cel', '(', 'data', '_', 'file', ',', 'sheet', 'name', '=', '1', ',', 'index', '_', 'col', '=', '[', '0', ',', '1', ']', ')', '\\', 'n']
Detokenized (021): ['mouse_data', '=', 'pandas', '.', 'read_excel', '(', 'data_file', ',', 'sheetname', '=', '1', ',', 'index_col', '=', '[', '0', ',', '1', ']', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "data_efficient [ ] = 1. \n"
Original    (006): ['data_efficient', '[', ']', '=', '1.', '\\n']
Tokenized   (012): ['<s>', 'data', '_', 'efficient', '[', ']', '=', '1', '.', '\\', 'n', '</s>']
Filtered   (010): ['data', '_', 'efficient', '[', ']', '=', '1', '.', '\\', 'n']
Detokenized (006): ['data_efficient', '[', ']', '=', '1.', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "exp_data [ ] = exp_data . groupby ( ) [ ] . transform ( exp_data [ ] = exp_data . groupby ( ) [ ] . transform ( \n"
Original    (029): ['exp_data', '[', ']', '=', 'exp_data', '.', 'groupby', '(', ')', '[', ']', '.', 'transform', '(', 'exp_data', '[', ']', '=', 'exp_data', '.', 'groupby', '(', ')', '[', ']', '.', 'transform', '(', '\\n']
Tokenized   (042): ['<s>', 'exp', '_', 'data', '[', ']', '=', 'exp', '_', 'data', '.', 'group', 'by', '(', ')', '[', ']', '.', 'transform', '(', 'exp', '_', 'data', '[', ']', '=', 'exp', '_', 'data', '.', 'group', 'by', '(', ')', '[', ']', '.', 'transform', '(', '\\', 'n', '</s>']
Filtered   (040): ['exp', '_', 'data', '[', ']', '=', 'exp', '_', 'data', '.', 'group', 'by', '(', ')', '[', ']', '.', 'transform', '(', 'exp', '_', 'data', '[', ']', '=', 'exp', '_', 'data', '.', 'group', 'by', '(', ')', '[', ']', '.', 'transform', '(', '\\', 'n']
Detokenized (029): ['exp_data', '[', ']', '=', 'exp_data', '.', 'groupby', '(', ')', '[', ']', '.', 'transform', '(', 'exp_data', '[', ']', '=', 'exp_data', '.', 'groupby', '(', ')', '[', ']', '.', 'transform', '(', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "aggregated [ ] = aggregated [ [ , ] ] . mean ( axis = 1 ) \n"
Original    (018): ['aggregated', '[', ']', '=', 'aggregated', '[', '[', ',', ']', ']', '.', 'mean', '(', 'axis', '=', '1', ')', '\\n']
Tokenized   (024): ['<s>', 'agg', 'reg', 'ated', '[', ']', '=', 'aggreg', 'ated', '[', '[', ',', ']', ']', '.', 'mean', '(', 'axis', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (022): ['agg', 'reg', 'ated', '[', ']', '=', 'aggreg', 'ated', '[', '[', ',', ']', ']', '.', 'mean', '(', 'axis', '=', '1', ')', '\\', 'n']
Detokenized (018): ['aggregated', '[', ']', '=', 'aggregated', '[', '[', ',', ']', ']', '.', 'mean', '(', 'axis', '=', '1', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "known_pairs = { : [ , , , ] , \n"
Original    (011): ['known_pairs', '=', '{', ':', '[', ',', ',', ',', ']', ',', '\\n']
Tokenized   (017): ['<s>', 'known', '_', 'p', 'airs', '=', '{', ':', '[', ',', ',', ',', ']', ',', '\\', 'n', '</s>']
Filtered   (015): ['known', '_', 'p', 'airs', '=', '{', ':', '[', ',', ',', ',', ']', ',', '\\', 'n']
Detokenized (011): ['known_pairs', '=', '{', ':', '[', ',', ',', ',', ']', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "drugs_to_genes [ ] . extend ( [ , , , , , \n"
Original    (013): ['drugs_to_genes', '[', ']', '.', 'extend', '(', '[', ',', ',', ',', ',', ',', '\\n']
Tokenized   (022): ['<s>', 'drug', 's', '_', 'to', '_', 'gen', 'es', '[', ']', '.', 'extend', '(', '[', ',', ',', ',', ',', ',', '\\', 'n', '</s>']
Filtered   (020): ['drug', 's', '_', 'to', '_', 'gen', 'es', '[', ']', '.', 'extend', '(', '[', ',', ',', ',', ',', ',', '\\', 'n']
Detokenized (013): ['drugs_to_genes', '[', ']', '.', 'extend', '(', '[', ',', ',', ',', ',', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "Xtmp [ ] = drug \n"
Original    (006): ['Xtmp', '[', ']', '=', 'drug', '\\n']
Tokenized   (010): ['<s>', 'X', 'tmp', '[', ']', '=', 'drug', '\\', 'n', '</s>']
Filtered   (008): ['X', 'tmp', '[', ']', '=', 'drug', '\\', 'n']
Detokenized (006): ['Xtmp', '[', ']', '=', 'drug', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "y_rank = pandas . concat ( ( y_rank , y_ranktmp ) , axis = 0 ) \n"
Original    (017): ['y_rank', '=', 'pandas', '.', 'concat', '(', '(', 'y_rank', ',', 'y_ranktmp', ')', ',', 'axis', '=', '0', ')', '\\n']
Tokenized   (029): ['<s>', 'y', '_', 'rank', '=', 'pand', 'as', '.', 'conc', 'at', '(', '(', 'y', '_', 'rank', ',', 'y', '_', 'rank', 'tmp', ')', ',', 'axis', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (027): ['y', '_', 'rank', '=', 'pand', 'as', '.', 'conc', 'at', '(', '(', 'y', '_', 'rank', ',', 'y', '_', 'rank', 'tmp', ')', ',', 'axis', '=', '0', ')', '\\', 'n']
Detokenized (017): ['y_rank', '=', 'pandas', '.', 'concat', '(', '(', 'y_rank', ',', 'y_ranktmp', ')', ',', 'axis', '=', '0', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "experiments [ ] = [ , , , ] \n"
Original    (010): ['experiments', '[', ']', '=', '[', ',', ',', ',', ']', '\\n']
Tokenized   (014): ['<s>', 'exper', 'iments', '[', ']', '=', '[', ',', ',', ',', ']', '\\', 'n', '</s>']
Filtered   (012): ['exper', 'iments', '[', ']', '=', '[', ',', ',', ',', ']', '\\', 'n']
Detokenized (010): ['experiments', '[', ']', '=', '[', ',', ',', ',', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "data_tmp [ "variance" ] = np . var ( data_tmp . values , axis = 1 ) \n"
Original    (018): ['data_tmp', '[', '"variance"', ']', '=', 'np', '.', 'var', '(', 'data_tmp', '.', 'values', ',', 'axis', '=', '1', ')', '\\n']
Tokenized   (028): ['<s>', 'data', '_', 'tmp', '[', '"', 'vari', 'ance', '"', ']', '=', 'np', '.', 'var', '(', 'data', '_', 'tmp', '.', 'values', ',', 'axis', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (026): ['data', '_', 'tmp', '[', '"', 'vari', 'ance', '"', ']', '=', 'np', '.', 'var', '(', 'data', '_', 'tmp', '.', 'values', ',', 'axis', '=', '1', ')', '\\', 'n']
Detokenized (018): ['data_tmp', '[', '"variance"', ']', '=', 'np', '.', 'var', '(', 'data_tmp', '.', 'values', ',', 'axis', '=', '1', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "gene_position_xu , target_genes_xu , Xdf_xu , Y_xu = read_xu_et_al ( data_file3 , learn_options ) \n"
Original    (015): ['gene_position_xu', ',', 'target_genes_xu', ',', 'Xdf_xu', ',', 'Y_xu', '=', 'read_xu_et_al', '(', 'data_file3', ',', 'learn_options', ')', '\\n']
Tokenized   (049): ['<s>', 'g', 'ene', '_', 'position', '_', 'x', 'u', ',', 'target', '_', 'gen', 'es', '_', 'x', 'u', ',', 'X', 'df', '_', 'x', 'u', ',', 'Y', '_', 'x', 'u', '=', 'read', '_', 'x', 'u', '_', 'et', '_', 'al', '(', 'data', '_', 'file', '3', ',', 'learn', '_', 'options', ')', '\\', 'n', '</s>']
Filtered   (047): ['g', 'ene', '_', 'position', '_', 'x', 'u', ',', 'target', '_', 'gen', 'es', '_', 'x', 'u', ',', 'X', 'df', '_', 'x', 'u', ',', 'Y', '_', 'x', 'u', '=', 'read', '_', 'x', 'u', '_', 'et', '_', 'al', '(', 'data', '_', 'file', '3', ',', 'learn', '_', 'options', ')', '\\', 'n']
Detokenized (015): ['gene_position_xu', ',', 'target_genes_xu', ',', 'Xdf_xu', ',', 'Y_xu', '=', 'read_xu_et_al', '(', 'data_file3', ',', 'learn_options', ')', '\\n']
Counter: 47
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "annotations , gene_position1 , target_genes1 , Xdf1 , Y1 = read_V1_data ( data_file , learn_options ) \n"
Original    (017): ['annotations', ',', 'gene_position1', ',', 'target_genes1', ',', 'Xdf1', ',', 'Y1', '=', 'read_V1_data', '(', 'data_file', ',', 'learn_options', ')', '\\n']
Tokenized   (040): ['<s>', 'annot', 'ations', ',', 'gene', '_', 'position', '1', ',', 'target', '_', 'gen', 'es', '1', ',', 'X', 'df', '1', ',', 'Y', '1', '=', 'read', '_', 'V', '1', '_', 'data', '(', 'data', '_', 'file', ',', 'learn', '_', 'options', ')', '\\', 'n', '</s>']
Filtered   (038): ['annot', 'ations', ',', 'gene', '_', 'position', '1', ',', 'target', '_', 'gen', 'es', '1', ',', 'X', 'df', '1', ',', 'Y', '1', '=', 'read', '_', 'V', '1', '_', 'data', '(', 'data', '_', 'file', ',', 'learn', '_', 'options', ')', '\\', 'n']
Detokenized (017): ['annotations', ',', 'gene_position1', ',', 'target_genes1', ',', 'Xdf1', ',', 'Y1', '=', 'read_V1_data', '(', 'data_file', ',', 'learn_options', ')', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "Y_cols_to_keep = np . unique ( [ , , , \n"
Original    (011): ['Y_cols_to_keep', '=', 'np', '.', 'unique', '(', '[', ',', ',', ',', '\\n']
Tokenized   (021): ['<s>', 'Y', '_', 'col', 's', '_', 'to', '_', 'keep', '=', 'np', '.', 'unique', '(', '[', ',', ',', ',', '\\', 'n', '</s>']
Filtered   (019): ['Y', '_', 'col', 's', '_', 'to', '_', 'keep', '=', 'np', '.', 'unique', '(', '[', ',', ',', ',', '\\', 'n']
Detokenized (011): ['Y_cols_to_keep', '=', 'np', '.', 'unique', '(', '[', ',', ',', ',', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "gene_position = pandas . concat ( ( gene_position1 , gene_position2 ) ) \n"
Original    (013): ['gene_position', '=', 'pandas', '.', 'concat', '(', '(', 'gene_position1', ',', 'gene_position2', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'g', 'ene', '_', 'position', '=', 'pand', 'as', '.', 'conc', 'at', '(', '(', 'gene', '_', 'position', '1', ',', 'gene', '_', 'position', '2', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['g', 'ene', '_', 'position', '=', 'pand', 'as', '.', 'conc', 'at', '(', '(', 'gene', '_', 'position', '1', ',', 'gene', '_', 'position', '2', ')', ')', '\\', 'n']
Detokenized (013): ['gene_position', '=', 'pandas', '.', 'concat', '(', '(', 'gene_position1', ',', 'gene_position2', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "onedupind = np . where ( Y . index . duplicated ( ) ) [ 0 ] [ 0 ] \n"
Original    (021): ['onedupind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'duplicated', '(', ')', ')', '[', '0', ']', '[', '0', ']', '\\n']
Tokenized   (027): ['<s>', 'oned', 'up', 'ind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'dupl', 'icated', '(', ')', ')', '[', '0', ']', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (025): ['oned', 'up', 'ind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'dupl', 'icated', '(', ')', ')', '[', '0', ']', '[', '0', ']', '\\', 'n']
Detokenized (021): ['onedupind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'duplicated', '(', ')', ')', '[', '0', ']', '[', '0', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "alldupind = np . where ( Y . index . get_level_values ( 0 ) . values == Y . index [ onedupind ] [ 0 ] ) [ 0 ] \n"
Original    (031): ['alldupind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'get_level_values', '(', '0', ')', '.', 'values', '==', 'Y', '.', 'index', '[', 'onedupind', ']', '[', '0', ']', ')', '[', '0', ']', '\\n']
Tokenized   (044): ['<s>', 'al', 'ld', 'up', 'ind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'get', '_', 'level', '_', 'values', '(', '0', ')', '.', 'values', '==', 'Y', '.', 'index', '[', 'on', 'ed', 'up', 'ind', ']', '[', '0', ']', ')', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (042): ['al', 'ld', 'up', 'ind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'get', '_', 'level', '_', 'values', '(', '0', ')', '.', 'values', '==', 'Y', '.', 'index', '[', 'on', 'ed', 'up', 'ind', ']', '[', '0', ']', ')', '[', '0', ']', '\\', 'n']
Detokenized (031): ['alldupind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'get_level_values', '(', '0', ')', '.', 'values', '==', 'Y', '.', 'index', '[', 'onedupind', ']', '[', '0', ']', ')', '[', '0', ']', '\\n']
Counter: 42
===================================================================
Hidden states:  (13, 31, 768)
# Extracted words:  31
Sentence         : "newindex [ onedupind ] = ( newindex [ onedupind ] [ 0 ] , newindex [ onedupind ] [ 1 ] , "nodrug2" ) \n"
Original    (025): ['newindex', '[', 'onedupind', ']', '=', '(', 'newindex', '[', 'onedupind', ']', '[', '0', ']', ',', 'newindex', '[', 'onedupind', ']', '[', '1', ']', ',', '"nodrug2"', ')', '\\n']
Tokenized   (045): ['<s>', 'new', 'index', '[', 'on', 'ed', 'up', 'ind', ']', '=', '(', 'new', 'index', '[', 'on', 'ed', 'up', 'ind', ']', '[', '0', ']', ',', 'new', 'index', '[', 'on', 'ed', 'up', 'ind', ']', '[', '1', ']', ',', '"', 'n', 'od', 'rug', '2', '"', ')', '\\', 'n', '</s>']
Filtered   (043): ['new', 'index', '[', 'on', 'ed', 'up', 'ind', ']', '=', '(', 'new', 'index', '[', 'on', 'ed', 'up', 'ind', ']', '[', '0', ']', ',', 'new', 'index', '[', 'on', 'ed', 'up', 'ind', ']', '[', '1', ']', ',', '"', 'n', 'od', 'rug', '2', '"', ')', '\\', 'n']
Detokenized (025): ['newindex', '[', 'onedupind', ']', '=', '(', 'newindex', '[', 'onedupind', ']', '[', '0', ']', ',', 'newindex', '[', 'onedupind', ']', '[', '1', ']', ',', '"nodrug2"', ')', '\\n']
Counter: 43
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "Xdf . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n"
Original    (021): ['Xdf', '.', 'index', '=', 'pandas', '.', 'MultiIndex', '.', 'from_tuples', '(', 'newindex', ',', 'names', '=', 'Y', '.', 'index', '.', 'names', ')', '\\n']
Tokenized   (031): ['<s>', 'X', 'df', '.', 'index', '=', 'pand', 'as', '.', 'Multi', 'Index', '.', 'from', '_', 'tu', 'ples', '(', 'new', 'index', ',', 'names', '=', 'Y', '.', 'index', '.', 'names', ')', '\\', 'n', '</s>']
Filtered   (029): ['X', 'df', '.', 'index', '=', 'pand', 'as', '.', 'Multi', 'Index', '.', 'from', '_', 'tu', 'ples', '(', 'new', 'index', ',', 'names', '=', 'Y', '.', 'index', '.', 'names', ')', '\\', 'n']
Detokenized (021): ['Xdf', '.', 'index', '=', 'pandas', '.', 'MultiIndex', '.', 'from_tuples', '(', 'newindex', ',', 'names', '=', 'Y', '.', 'index', '.', 'names', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "mouse_genes = Xdf [ Xdf [ ] == ] [ ] . unique ( ) \n"
Original    (016): ['mouse_genes', '=', 'Xdf', '[', 'Xdf', '[', ']', '==', ']', '[', ']', '.', 'unique', '(', ')', '\\n']
Tokenized   (024): ['<s>', 'mouse', '_', 'gen', 'es', '=', 'X', 'df', '[', 'X', 'df', '[', ']', '==', ']', '[', ']', '.', 'unique', '(', ')', '\\', 'n', '</s>']
Filtered   (022): ['mouse', '_', 'gen', 'es', '=', 'X', 'df', '[', 'X', 'df', '[', ']', '==', ']', '[', ']', '.', 'unique', '(', ')', '\\', 'n']
Detokenized (016): ['mouse_genes', '=', 'Xdf', '[', 'Xdf', '[', ']', '==', ']', '[', ']', '.', 'unique', '(', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "all_genes = get_V3_genes ( None , None ) return np . setdiff1d ( all_genes , mouse_genes ) \n"
Original    (018): ['all_genes', '=', 'get_V3_genes', '(', 'None', ',', 'None', ')', 'return', 'np', '.', 'setdiff1d', '(', 'all_genes', ',', 'mouse_genes', ')', '\\n']
Tokenized   (039): ['<s>', 'all', '_', 'gen', 'es', '=', 'get', '_', 'V', '3', '_', 'gen', 'es', '(', 'None', ',', 'None', ')', 'return', 'np', '.', 'set', 'diff', '1', 'd', '(', 'all', '_', 'gen', 'es', ',', 'mouse', '_', 'gen', 'es', ')', '\\', 'n', '</s>']
Filtered   (037): ['all', '_', 'gen', 'es', '=', 'get', '_', 'V', '3', '_', 'gen', 'es', '(', 'None', ',', 'None', ')', 'return', 'np', '.', 'set', 'diff', '1', 'd', '(', 'all', '_', 'gen', 'es', ',', 'mouse', '_', 'gen', 'es', ')', '\\', 'n']
Detokenized (018): ['all_genes', '=', 'get_V3_genes', '(', 'None', ',', 'None', ')', 'return', 'np', '.', 'setdiff1d', '(', 'all_genes', ',', 'mouse_genes', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "option_value = self . cfg . migrate [ self . option_name ] \n"
Original    (013): ['option_value', '=', 'self', '.', 'cfg', '.', 'migrate', '[', 'self', '.', 'option_name', ']', '\\n']
Tokenized   (021): ['<s>', 'option', '_', 'value', '=', 'self', '.', 'cf', 'g', '.', 'migrate', '[', 'self', '.', 'option', '_', 'name', ']', '\\', 'n', '</s>']
Filtered   (019): ['option', '_', 'value', '=', 'self', '.', 'cf', 'g', '.', 'migrate', '[', 'self', '.', 'option', '_', 'name', ']', '\\', 'n']
Detokenized (013): ['option_value', '=', 'self', '.', 'cfg', '.', 'migrate', '[', 'self', '.', 'option_name', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "search_opts_tenant = kwargs . get ( , { } ) \n"
Original    (011): ['search_opts_tenant', '=', 'kwargs', '.', 'get', '(', ',', '{', '}', ')', '\\n']
Tokenized   (022): ['<s>', 'search', '_', 'op', 'ts', '_', 'ten', 'ant', '=', 'k', 'w', 'args', '.', 'get', '(', ',', '{', '}', ')', '\\', 'n', '</s>']
Filtered   (020): ['search', '_', 'op', 'ts', '_', 'ten', 'ant', '=', 'k', 'w', 'args', '.', 'get', '(', ',', '{', '}', ')', '\\', 'n']
Detokenized (011): ['search_opts_tenant', '=', 'kwargs', '.', 'get', '(', ',', '{', '}', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "tenants_without_quotas = self . get_tenants_without_quotas ( tenants_src , \n"
Original    (009): ['tenants_without_quotas', '=', 'self', '.', 'get_tenants_without_quotas', '(', 'tenants_src', ',', '\\n']
Tokenized   (030): ['<s>', 'ten', 'ants', '_', 'without', '_', 'qu', 'ot', 'as', '=', 'self', '.', 'get', '_', 'ten', 'ants', '_', 'without', '_', 'qu', 'ot', 'as', '(', 'tenants', '_', 'src', ',', '\\', 'n', '</s>']
Filtered   (028): ['ten', 'ants', '_', 'without', '_', 'qu', 'ot', 'as', '=', 'self', '.', 'get', '_', 'ten', 'ants', '_', 'without', '_', 'qu', 'ot', 'as', '(', 'tenants', '_', 'src', ',', '\\', 'n']
Detokenized (009): ['tenants_without_quotas', '=', 'self', '.', 'get_tenants_without_quotas', '(', 'tenants_src', ',', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "quot = network_src . show_quota ( tenants_without_quotas [ 0 ] ) \n"
Original    (012): ['quot', '=', 'network_src', '.', 'show_quota', '(', 'tenants_without_quotas', '[', '0', ']', ')', '\\n']
Tokenized   (027): ['<s>', 'qu', 'ot', '=', 'network', '_', 'src', '.', 'show', '_', 'qu', 'ota', '(', 'tenants', '_', 'without', '_', 'qu', 'ot', 'as', '[', '0', ']', ')', '\\', 'n', '</s>']
Filtered   (025): ['qu', 'ot', '=', 'network', '_', 'src', '.', 'show', '_', 'qu', 'ota', '(', 'tenants', '_', 'without', '_', 'qu', 'ot', 'as', '[', '0', ']', ')', '\\', 'n']
Detokenized (012): ['quot', '=', 'network_src', '.', 'show_quota', '(', 'tenants_without_quotas', '[', '0', ']', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "quot_default_dst [ item_quot ] ) \n"
Original    (006): ['quot_default_dst', '[', 'item_quot', ']', ')', '\\n']
Tokenized   (018): ['<s>', 'qu', 'ot', '_', 'default', '_', 'd', 'st', '[', 'item', '_', 'qu', 'ot', ']', ')', '\\', 'n', '</s>']
Filtered   (016): ['qu', 'ot', '_', 'default', '_', 'd', 'st', '[', 'item', '_', 'qu', 'ot', ']', ')', '\\', 'n']
Detokenized (006): ['quot_default_dst', '[', 'item_quot', ']', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "tenants = [ identity_src . keystone_client . tenants . find ( id = tnt_id ) for \n"
Original    (017): ['tenants', '=', '[', 'identity_src', '.', 'keystone_client', '.', 'tenants', '.', 'find', '(', 'id', '=', 'tnt_id', ')', 'for', '\\n']
Tokenized   (029): ['<s>', 'ten', 'ants', '=', '[', 'identity', '_', 'src', '.', 'key', 'stone', '_', 'client', '.', 'tenants', '.', 'find', '(', 'id', '=', 't', 'nt', '_', 'id', ')', 'for', '\\', 'n', '</s>']
Filtered   (027): ['ten', 'ants', '=', '[', 'identity', '_', 'src', '.', 'key', 'stone', '_', 'client', '.', 'tenants', '.', 'find', '(', 'id', '=', 't', 'nt', '_', 'id', ')', 'for', '\\', 'n']
Detokenized (017): ['tenants', '=', '[', 'identity_src', '.', 'keystone_client', '.', 'tenants', '.', 'find', '(', 'id', '=', 'tnt_id', ')', 'for', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "tnt_id in filter_tenants_ids_list ] \n"
Original    (005): ['tnt_id', 'in', 'filter_tenants_ids_list', ']', '\\n']
Tokenized   (018): ['<s>', 't', 'nt', '_', 'id', 'in', 'filter', '_', 'ten', 'ants', '_', 'ids', '_', 'list', ']', '\\', 'n', '</s>']
Filtered   (016): ['t', 'nt', '_', 'id', 'in', 'filter', '_', 'ten', 'ants', '_', 'ids', '_', 'list', ']', '\\', 'n']
Detokenized (005): ['tnt_id', 'in', 'filter_tenants_ids_list', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "instance [ ] [ ] , instance [ ] [ ] ) \n"
Original    (013): ['instance', '[', ']', '[', ']', ',', 'instance', '[', ']', '[', ']', ')', '\\n']
Tokenized   (016): ['<s>', 'instance', '[', ']', '[', ']', ',', 'instance', '[', ']', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (014): ['instance', '[', ']', '[', ']', ',', 'instance', '[', ']', '[', ']', ')', '\\', 'n']
Detokenized (013): ['instance', '[', ']', '[', ']', ',', 'instance', '[', ']', '[', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "vol [ ] , storage_resource . get_status , , \n"
Original    (010): ['vol', '[', ']', ',', 'storage_resource', '.', 'get_status', ',', ',', '\\n']
Tokenized   (017): ['<s>', 'vol', '[', ']', ',', 'storage', '_', 'resource', '.', 'get', '_', 'status', ',', ',', '\\', 'n', '</s>']
Filtered   (015): ['vol', '[', ']', ',', 'storage', '_', 'resource', '.', 'get', '_', 'status', ',', ',', '\\', 'n']
Detokenized (010): ['vol', '[', ']', ',', 'storage_resource', '.', 'get_status', ',', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "inst_name = libvirt_instance_name ) ) \n"
Original    (006): ['inst_name', '=', 'libvirt_instance_name', ')', ')', '\\n']
Tokenized   (016): ['<s>', 'inst', '_', 'name', '=', 'lib', 'virt', '_', 'instance', '_', 'name', ')', ')', '\\', 'n', '</s>']
Filtered   (014): ['inst', '_', 'name', '=', 'lib', 'virt', '_', 'instance', '_', 'name', ')', ')', '\\', 'n']
Detokenized (006): ['inst_name', '=', 'libvirt_instance_name', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "dst = instance_image_path ( instance_id ) ) \n"
Original    (008): ['dst', '=', 'instance_image_path', '(', 'instance_id', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'd', 'st', '=', 'instance', '_', 'image', '_', 'path', '(', 'instance', '_', 'id', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['d', 'st', '=', 'instance', '_', 'image', '_', 'path', '(', 'instance', '_', 'id', ')', ')', '\\', 'n']
Detokenized (008): ['dst', '=', 'instance_image_path', '(', 'instance_id', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "interface . find ( ) ) \n"
Original    (007): ['interface', '.', 'find', '(', ')', ')', '\\n']
Tokenized   (010): ['<s>', 'interface', '.', 'find', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (008): ['interface', '.', 'find', '(', ')', ')', '\\', 'n']
Detokenized (007): ['interface', '.', 'find', '(', ')', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "mac = self . mac , src = self . source_iface , dst = self . target_iface ) \n"
Original    (019): ['mac', '=', 'self', '.', 'mac', ',', 'src', '=', 'self', '.', 'source_iface', ',', 'dst', '=', 'self', '.', 'target_iface', ')', '\\n']
Tokenized   (028): ['<s>', 'mac', '=', 'self', '.', 'mac', ',', 'src', '=', 'self', '.', 'source', '_', 'if', 'ace', ',', 'dst', '=', 'self', '.', 'target', '_', 'if', 'ace', ')', '\\', 'n', '</s>']
Filtered   (026): ['mac', '=', 'self', '.', 'mac', ',', 'src', '=', 'self', '.', 'source', '_', 'if', 'ace', ',', 'dst', '=', 'self', '.', 'target', '_', 'if', 'ace', ')', '\\', 'n']
Detokenized (019): ['mac', '=', 'self', '.', 'mac', ',', 'src', '=', 'self', '.', 'source_iface', ',', 'dst', '=', 'self', '.', 'target_iface', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "element . attrib = { attr : value } \n"
Original    (010): ['element', '.', 'attrib', '=', '{', 'attr', ':', 'value', '}', '\\n']
Tokenized   (015): ['<s>', 'element', '.', 'att', 'rib', '=', '{', 'att', 'r', ':', 'value', '}', '\\', 'n', '</s>']
Filtered   (013): ['element', '.', 'att', 'rib', '=', '{', 'att', 'r', ':', 'value', '}', '\\', 'n']
Detokenized (010): ['element', '.', 'attrib', '=', '{', 'attr', ':', 'value', '}', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "rr . run ( copy . format ( src_file = source_object . path , \n"
Original    (015): ['rr', '.', 'run', '(', 'copy', '.', 'format', '(', 'src_file', '=', 'source_object', '.', 'path', ',', '\\n']
Tokenized   (022): ['<s>', 'rr', '.', 'run', '(', 'copy', '.', 'format', '(', 'src', '_', 'file', '=', 'source', '_', 'object', '.', 'path', ',', '\\', 'n', '</s>']
Filtered   (020): ['rr', '.', 'run', '(', 'copy', '.', 'format', '(', 'src', '_', 'file', '=', 'source', '_', 'object', '.', 'path', ',', '\\', 'n']
Detokenized (015): ['rr', '.', 'run', '(', 'copy', '.', 'format', '(', 'src_file', '=', 'source_object', '.', 'path', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "max_bytes = sizeof_format . parse_size ( kwargs . pop ( , 0 ) ) \n"
Original    (015): ['max_bytes', '=', 'sizeof_format', '.', 'parse_size', '(', 'kwargs', '.', 'pop', '(', ',', '0', ')', ')', '\\n']
Tokenized   (026): ['<s>', 'max', '_', 'bytes', '=', 'sizeof', '_', 'format', '.', 'parse', '_', 'size', '(', 'k', 'w', 'args', '.', 'pop', '(', ',', '0', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['max', '_', 'bytes', '=', 'sizeof', '_', 'format', '.', 'parse', '_', 'size', '(', 'k', 'w', 'args', '.', 'pop', '(', ',', '0', ')', ')', '\\', 'n']
Detokenized (015): ['max_bytes', '=', 'sizeof_format', '.', 'parse_size', '(', 'kwargs', '.', 'pop', '(', ',', '0', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "scenario = os . path . splitext ( scenario_filename ) [ 0 ] \n"
Original    (014): ['scenario', '=', 'os', '.', 'path', '.', 'splitext', '(', 'scenario_filename', ')', '[', '0', ']', '\\n']
Tokenized   (022): ['<s>', 'sc', 'enario', '=', 'os', '.', 'path', '.', 'spl', 'ite', 'xt', '(', 'scenario', '_', 'filename', ')', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (020): ['sc', 'enario', '=', 'os', '.', 'path', '.', 'spl', 'ite', 'xt', '(', 'scenario', '_', 'filename', ')', '[', '0', ']', '\\', 'n']
Detokenized (014): ['scenario', '=', 'os', '.', 'path', '.', 'splitext', '(', 'scenario_filename', ')', '[', '0', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "file_name = config . rollback_params [ ] [ ] \n"
Original    (010): ['file_name', '=', 'config', '.', 'rollback_params', '[', ']', '[', ']', '\\n']
Tokenized   (018): ['<s>', 'file', '_', 'name', '=', 'config', '.', 'roll', 'back', '_', 'params', '[', ']', '[', ']', '\\', 'n', '</s>']
Filtered   (016): ['file', '_', 'name', '=', 'config', '.', 'roll', 'back', '_', 'params', '[', ']', '[', ']', '\\', 'n']
Detokenized (010): ['file_name', '=', 'config', '.', 'rollback_params', '[', ']', '[', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "pre_file_path = os . path . join ( self . cloudferry_dir , file_name ) \n"
Original    (015): ['pre_file_path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'cloudferry_dir', ',', 'file_name', ')', '\\n']
Tokenized   (028): ['<s>', 'pre', '_', 'file', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'cloud', 'fer', 'ry', '_', 'dir', ',', 'file', '_', 'name', ')', '\\', 'n', '</s>']
Filtered   (026): ['pre', '_', 'file', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'cloud', 'fer', 'ry', '_', 'dir', ',', 'file', '_', 'name', ')', '\\', 'n']
Detokenized (015): ['pre_file_path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'cloudferry_dir', ',', 'file_name', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "o2 = C ( 2 ) \n"
Original    (007): ['o2', '=', 'C', '(', '2', ')', '\\n']
Tokenized   (011): ['<s>', 'o', '2', '=', 'C', '(', '2', ')', '\\', 'n', '</s>']
Filtered   (009): ['o', '2', '=', 'C', '(', '2', ')', '\\', 'n']
Detokenized (007): ['o2', '=', 'C', '(', '2', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "org_tag = request . user . get_profile ( ) . org_tag \n"
Original    (012): ['org_tag', '=', 'request', '.', 'user', '.', 'get_profile', '(', ')', '.', 'org_tag', '\\n']
Tokenized   (021): ['<s>', 'org', '_', 'tag', '=', 'request', '.', 'user', '.', 'get', '_', 'profile', '(', ')', '.', 'org', '_', 'tag', '\\', 'n', '</s>']
Filtered   (019): ['org', '_', 'tag', '=', 'request', '.', 'user', '.', 'get', '_', 'profile', '(', ')', '.', 'org', '_', 'tag', '\\', 'n']
Detokenized (012): ['org_tag', '=', 'request', '.', 'user', '.', 'get_profile', '(', ')', '.', 'org_tag', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "featureset = Recording . objects . filter ( lat__lt = ne_lat , lat__gt = sw_lat , lon__lt = ne_lon , lon__gt = sw_lon \n"
Original    (024): ['featureset', '=', 'Recording', '.', 'objects', '.', 'filter', '(', 'lat__lt', '=', 'ne_lat', ',', 'lat__gt', '=', 'sw_lat', ',', 'lon__lt', '=', 'ne_lon', ',', 'lon__gt', '=', 'sw_lon', '\\n']
Tokenized   (046): ['<s>', 'features', 'et', '=', 'Recording', '.', 'objects', '.', 'filter', '(', 'lat', '__', 'lt', '=', 'ne', '_', 'lat', ',', 'lat', '__', 'gt', '=', 'sw', '_', 'lat', ',', 'l', 'on', '__', 'lt', '=', 'ne', '_', 'lon', ',', 'l', 'on', '__', 'gt', '=', 'sw', '_', 'lon', '\\', 'n', '</s>']
Filtered   (044): ['features', 'et', '=', 'Recording', '.', 'objects', '.', 'filter', '(', 'lat', '__', 'lt', '=', 'ne', '_', 'lat', ',', 'lat', '__', 'gt', '=', 'sw', '_', 'lat', ',', 'l', 'on', '__', 'lt', '=', 'ne', '_', 'lon', ',', 'l', 'on', '__', 'gt', '=', 'sw', '_', 'lon', '\\', 'n']
Detokenized (024): ['featureset', '=', 'Recording', '.', 'objects', '.', 'filter', '(', 'lat__lt', '=', 'ne_lat', ',', 'lat__gt', '=', 'sw_lat', ',', 'lon__lt', '=', 'ne_lon', ',', 'lon__gt', '=', 'sw_lon', '\\n']
Counter: 44
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "httpresponse_kwargs = { : kwargs . pop ( , None ) } \n"
Original    (013): ['httpresponse_kwargs', '=', '{', ':', 'kwargs', '.', 'pop', '(', ',', 'None', ')', '}', '\\n']
Tokenized   (022): ['<s>', 'http', 'response', '_', 'kw', 'args', '=', '{', ':', 'k', 'w', 'args', '.', 'pop', '(', ',', 'None', ')', '}', '\\', 'n', '</s>']
Filtered   (020): ['http', 'response', '_', 'kw', 'args', '=', '{', ':', 'k', 'w', 'args', '.', 'pop', '(', ',', 'None', ')', '}', '\\', 'n']
Detokenized (013): ['httpresponse_kwargs', '=', '{', ':', 'kwargs', '.', 'pop', '(', ',', 'None', ')', '}', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "is_testing = in sys . argv \n"
Original    (007): ['is_testing', '=', 'in', 'sys', '.', 'argv', '\\n']
Tokenized   (013): ['<s>', 'is', '_', 'testing', '=', 'in', 'sys', '.', 'arg', 'v', '\\', 'n', '</s>']
Filtered   (011): ['is', '_', 'testing', '=', 'in', 'sys', '.', 'arg', 'v', '\\', 'n']
Detokenized (007): ['is_testing', '=', 'in', 'sys', '.', 'argv', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "up_time = end_time - self . start_time \n"
Original    (008): ['up_time', '=', 'end_time', '-', 'self', '.', 'start_time', '\\n']
Tokenized   (017): ['<s>', 'up', '_', 'time', '=', 'end', '_', 'time', '-', 'self', '.', 'start', '_', 'time', '\\', 'n', '</s>']
Filtered   (015): ['up', '_', 'time', '=', 'end', '_', 'time', '-', 'self', '.', 'start', '_', 'time', '\\', 'n']
Detokenized (008): ['up_time', '=', 'end_time', '-', 'self', '.', 'start_time', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "remaining_time = self . count_down_total - datetime . timedelta ( seconds = ( int ( up_time ) ) ) \n"
Original    (020): ['remaining_time', '=', 'self', '.', 'count_down_total', '-', 'datetime', '.', 'timedelta', '(', 'seconds', '=', '(', 'int', '(', 'up_time', ')', ')', ')', '\\n']
Tokenized   (034): ['<s>', 'rem', 'aining', '_', 'time', '=', 'self', '.', 'count', '_', 'down', '_', 'total', '-', 'dat', 'etime', '.', 'timed', 'elta', '(', 'seconds', '=', '(', 'int', '(', 'up', '_', 'time', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (032): ['rem', 'aining', '_', 'time', '=', 'self', '.', 'count', '_', 'down', '_', 'total', '-', 'dat', 'etime', '.', 'timed', 'elta', '(', 'seconds', '=', '(', 'int', '(', 'up', '_', 'time', ')', ')', ')', '\\', 'n']
Detokenized (020): ['remaining_time', '=', 'self', '.', 'count_down_total', '-', 'datetime', '.', 'timedelta', '(', 'seconds', '=', '(', 'int', '(', 'up_time', ')', ')', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "abort_time = time . time ( ) + timeout \n"
Original    (010): ['abort_time', '=', 'time', '.', 'time', '(', ')', '+', 'timeout', '\\n']
Tokenized   (016): ['<s>', 'ab', 'ort', '_', 'time', '=', 'time', '.', 'time', '(', ')', '+', 'timeout', '\\', 'n', '</s>']
Filtered   (014): ['ab', 'ort', '_', 'time', '=', 'time', '.', 'time', '(', ')', '+', 'timeout', '\\', 'n']
Detokenized (010): ['abort_time', '=', 'time', '.', 'time', '(', ')', '+', 'timeout', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "elif not stanza . getID ( ) : \n"
Original    (009): ['elif', 'not', 'stanza', '.', 'getID', '(', ')', ':', '\\n']
Tokenized   (015): ['<s>', 'el', 'if', 'not', 'st', 'anza', '.', 'get', 'ID', '(', ')', ':', '\\', 'n', '</s>']
Filtered   (013): ['el', 'if', 'not', 'st', 'anza', '.', 'get', 'ID', '(', ')', ':', '\\', 'n']
Detokenized (009): ['elif', 'not', 'stanza', '.', 'getID', '(', ')', ':', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "_ID = ` ID ` \n"
Original    (006): ['_ID', '=', '`', 'ID', '`', '\\n']
Tokenized   (010): ['<s>', '_', 'ID', '=', '`', 'ID', '`', '\\', 'n', '</s>']
Filtered   (008): ['_', 'ID', '=', '`', 'ID', '`', '\\', 'n']
Detokenized (006): ['_ID', '=', '`', 'ID', '`', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "__description__ = , \n"
Original    (004): ['__description__', '=', ',', '\\n']
Tokenized   (009): ['<s>', '__', 'description', '__', '=', ',', '\\', 'n', '</s>']
Filtered   (007): ['__', 'description', '__', '=', ',', '\\', 'n']
Detokenized (004): ['__description__', '=', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "REQUIRES = [ i . strip ( ) for i in open ( "requirements.txt" ) . readlines ( ) ] \n"
Original    (021): ['REQUIRES', '=', '[', 'i', '.', 'strip', '(', ')', 'for', 'i', 'in', 'open', '(', '"requirements.txt"', ')', '.', 'readlines', '(', ')', ']', '\\n']
Tokenized   (033): ['<s>', 'RE', 'QU', 'IR', 'ES', '=', '[', 'i', '.', 'strip', '(', ')', 'for', 'i', 'in', 'open', '(', '"', 'requ', 'irements', '.', 'txt', '"', ')', '.', 'read', 'lines', '(', ')', ']', '\\', 'n', '</s>']
Filtered   (031): ['RE', 'QU', 'IR', 'ES', '=', '[', 'i', '.', 'strip', '(', ')', 'for', 'i', 'in', 'open', '(', '"', 'requ', 'irements', '.', 'txt', '"', ')', '.', 'read', 'lines', '(', ')', ']', '\\', 'n']
Detokenized (021): ['REQUIRES', '=', '[', 'i', '.', 'strip', '(', ')', 'for', 'i', 'in', 'open', '(', '"requirements.txt"', ')', '.', 'readlines', '(', ')', ']', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "BaseField . __init__ ( self , ** kwargs ) \n"
Original    (010): ['BaseField', '.', '__init__', '(', 'self', ',', '**', 'kwargs', ')', '\\n']
Tokenized   (018): ['<s>', 'Base', 'Field', '.', '__', 'init', '__', '(', 'self', ',', '**', 'k', 'w', 'args', ')', '\\', 'n', '</s>']
Filtered   (016): ['Base', 'Field', '.', '__', 'init', '__', '(', 'self', ',', '**', 'k', 'w', 'args', ')', '\\', 'n']
Detokenized (010): ['BaseField', '.', '__init__', '(', 'self', ',', '**', 'kwargs', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "is_list = not hasattr ( items , ) \n"
Original    (009): ['is_list', '=', 'not', 'hasattr', '(', 'items', ',', ')', '\\n']
Tokenized   (015): ['<s>', 'is', '_', 'list', '=', 'not', 'has', 'attr', '(', 'items', ',', ')', '\\', 'n', '</s>']
Filtered   (013): ['is', '_', 'list', '=', 'not', 'has', 'attr', '(', 'items', ',', ')', '\\', 'n']
Detokenized (009): ['is_list', '=', 'not', 'hasattr', '(', 'items', ',', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "object_map [ ( collection , doc . id ) ] = doc \n"
Original    (013): ['object_map', '[', '(', 'collection', ',', 'doc', '.', 'id', ')', ']', '=', 'doc', '\\n']
Tokenized   (018): ['<s>', 'object', '_', 'map', '[', '(', 'collection', ',', 'doc', '.', 'id', ')', ']', '=', 'doc', '\\', 'n', '</s>']
Filtered   (016): ['object', '_', 'map', '[', '(', 'collection', ',', 'doc', '.', 'id', ')', ']', '=', 'doc', '\\', 'n']
Detokenized (013): ['object_map', '[', '(', 'collection', ',', 'doc', '.', 'id', ')', ']', '=', 'doc', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "_cls = doc . _data . pop ( , None ) \n"
Original    (012): ['_cls', '=', 'doc', '.', '_data', '.', 'pop', '(', ',', 'None', ')', '\\n']
Tokenized   (018): ['<s>', '_', 'cl', 's', '=', 'doc', '.', '_', 'data', '.', 'pop', '(', ',', 'None', ')', '\\', 'n', '</s>']
Filtered   (016): ['_', 'cl', 's', '=', 'doc', '.', '_', 'data', '.', 'pop', '(', ',', 'None', ')', '\\', 'n']
Detokenized (012): ['_cls', '=', 'doc', '.', '_data', '.', 'pop', '(', ',', 'None', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "81.4471435546875 , \n"
Original    (003): ['81.4471435546875', ',', '\\n']
Tokenized   (013): ['<s>', '81', '.', '447', '14', '35', '54', '68', '75', ',', '\\', 'n', '</s>']
Filtered   (011): ['81', '.', '447', '14', '35', '54', '68', '75', ',', '\\', 'n']
Detokenized (003): ['81.4471435546875', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "23.61432859499169 \n"
Original    (002): ['23.61432859499169', '\\n']
Tokenized   (012): ['<s>', '23', '.', '6', '143', '28', '59', '499', '169', '\\', 'n', '</s>']
Filtered   (010): ['23', '.', '6', '143', '28', '59', '499', '169', '\\', 'n']
Detokenized (002): ['23.61432859499169', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 2, 768)
# Extracted words:  2
Sentence         : "invalid_coords = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] ] ] ] \n"
Original    (020): ['invalid_coords', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ']', ']', ']', '\\n']
Tokenized   (027): ['<s>', 'in', 'valid', '_', 'co', 'ords', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ']', ']', ']', '\\', 'n', '</s>']
Filtered   (025): ['in', 'valid', '_', 'co', 'ords', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ']', ']', ']', '\\', 'n']
Detokenized (020): ['invalid_coords', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ']', ']', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "Location ( loc = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ] ) . validate ( ) \n"
Original    (039): ['Location', '(', 'loc', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ',', '[', '5', ',', '6', ']', ',', '[', '1', ',', '2', ']', ']', ']', ']', ')', '.', 'validate', '(', ')', '\\n']
Tokenized   (042): ['<s>', 'Location', '(', 'loc', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ',', '[', '5', ',', '6', ']', ',', '[', '1', ',', '2', ']', ']', ']', ']', ')', '.', 'validate', '(', ')', '\\', 'n', '</s>']
Filtered   (040): ['Location', '(', 'loc', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ',', '[', '5', ',', '6', ']', ',', '[', '1', ',', '2', ']', ']', ']', ']', ')', '.', 'validate', '(', ')', '\\', 'n']
Detokenized (039): ['Location', '(', 'loc', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ',', '[', '5', ',', '6', ']', ',', '[', '1', ',', '2', ']', ']', ']', ']', ')', '.', 'validate', '(', ')', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 39, 768)
# Extracted words:  39
Sentence         : "Parent ( name = ) . save ( ) \n"
Original    (010): ['Parent', '(', 'name', '=', ')', '.', 'save', '(', ')', '\\n']
Tokenized   (013): ['<s>', 'Parent', '(', 'name', '=', ')', '.', 'save', '(', ')', '\\', 'n', '</s>']
Filtered   (011): ['Parent', '(', 'name', '=', ')', '.', 'save', '(', ')', '\\', 'n']
Detokenized (010): ['Parent', '(', 'name', '=', ')', '.', 'save', '(', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "echo_payload = Struct ( "echo_payload" , \n"
Original    (007): ['echo_payload', '=', 'Struct', '(', '"echo_payload"', ',', '\\n']
Tokenized   (018): ['<s>', 'echo', '_', 'pay', 'load', '=', 'Struct', '(', '"', 'echo', '_', 'pay', 'load', '"', ',', '\\', 'n', '</s>']
Filtered   (016): ['echo', '_', 'pay', 'load', '=', 'Struct', '(', '"', 'echo', '_', 'pay', 'load', '"', ',', '\\', 'n']
Detokenized (007): ['echo_payload', '=', 'Struct', '(', '"echo_payload"', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "Padding ( 2 ) , \n"
Original    (006): ['Padding', '(', '2', ')', ',', '\\n']
Tokenized   (010): ['<s>', 'P', 'adding', '(', '2', ')', ',', '\\', 'n', '</s>']
Filtered   (008): ['P', 'adding', '(', '2', ')', ',', '\\', 'n']
Detokenized (006): ['Padding', '(', '2', ')', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "IpAddress ( "host" ) , \n"
Original    (006): ['IpAddress', '(', '"host"', ')', ',', '\\n']
Tokenized   (013): ['<s>', 'I', 'p', 'Address', '(', '"', 'host', '"', ')', ',', '\\', 'n', '</s>']
Filtered   (011): ['I', 'p', 'Address', '(', '"', 'host', '"', ')', ',', '\\', 'n']
Detokenized (006): ['IpAddress', '(', '"host"', ')', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "Bytes ( "echo" , 8 ) , \n"
Original    (008): ['Bytes', '(', '"echo"', ',', '8', ')', ',', '\\n']
Tokenized   (013): ['<s>', 'Bytes', '(', '"', 'echo', '"', ',', '8', ')', ',', '\\', 'n', '</s>']
Filtered   (011): ['Bytes', '(', '"', 'echo', '"', ',', '8', ')', ',', '\\', 'n']
Detokenized (008): ['Bytes', '(', '"echo"', ',', '8', ')', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "dest_unreachable_code = Enum ( Byte ( "code" ) , \n"
Original    (010): ['dest_unreachable_code', '=', 'Enum', '(', 'Byte', '(', '"code"', ')', ',', '\\n']
Tokenized   (022): ['<s>', 'dest', '_', 'un', 'reach', 'able', '_', 'code', '=', 'En', 'um', '(', 'Byte', '(', '"', 'code', '"', ')', ',', '\\', 'n', '</s>']
Filtered   (020): ['dest', '_', 'un', 'reach', 'able', '_', 'code', '=', 'En', 'um', '(', 'Byte', '(', '"', 'code', '"', ')', ',', '\\', 'n']
Detokenized (010): ['dest_unreachable_code', '=', 'Enum', '(', 'Byte', '(', '"code"', ')', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "Enum ( Byte ( "type" ) , \n"
Original    (008): ['Enum', '(', 'Byte', '(', '"type"', ')', ',', '\\n']
Tokenized   (014): ['<s>', 'En', 'um', '(', 'Byte', '(', '"', 'type', '"', ')', ',', '\\', 'n', '</s>']
Filtered   (012): ['En', 'um', '(', 'Byte', '(', '"', 'type', '"', ')', ',', '\\', 'n']
Detokenized (008): ['Enum', '(', 'Byte', '(', '"type"', ')', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "Switch ( "payload" , lambda ctx : ctx . type , \n"
Original    (012): ['Switch', '(', '"payload"', ',', 'lambda', 'ctx', ':', 'ctx', '.', 'type', ',', '\\n']
Tokenized   (020): ['<s>', 'Switch', '(', '"', 'pay', 'load', '"', ',', 'lambda', 'c', 'tx', ':', 'c', 'tx', '.', 'type', ',', '\\', 'n', '</s>']
Filtered   (018): ['Switch', '(', '"', 'pay', 'load', '"', ',', 'lambda', 'c', 'tx', ':', 'c', 'tx', '.', 'type', ',', '\\', 'n']
Detokenized (012): ['Switch', '(', '"payload"', ',', 'lambda', 'ctx', ':', 'ctx', '.', 'type', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : ""63646566676869" ) . decode ( "hex" ) \n"
Original    (008): ['"63646566676869"', ')', '.', 'decode', '(', '"hex"', ')', '\\n']
Tokenized   (021): ['<s>', '"', '63', '64', '65', '66', '67', '68', '69', '"', ')', '.', 'decode', '(', '"', 'hex', '"', ')', '\\', 'n', '</s>']
Filtered   (019): ['"', '63', '64', '65', '66', '67', '68', '69', '"', ')', '.', 'decode', '(', '"', 'hex', '"', ')', '\\', 'n']
Detokenized (008): ['"63646566676869"', ')', '.', 'decode', '(', '"hex"', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "cap2 = ( "0000385c02001b006162636465666768696a6b6c6d6e6f70717273747576776162" \n"
Original    (005): ['cap2', '=', '(', '"0000385c02001b006162636465666768696a6b6c6d6e6f70717273747576776162"', '\\n']
Tokenized   (044): ['<s>', 'cap', '2', '=', '(', '"', '0000', '385', 'c', '0', '2001', 'b', '006', '16', '26', '364', '65', '66', '67', '68', '696', 'a', '6', 'b', '6', 'c', '6', 'd', '6', 'e', '6', 'f', '707', '17', '27', '374', '75', '767', '76', '162', '"', '\\', 'n', '</s>']
Filtered   (042): ['cap', '2', '=', '(', '"', '0000', '385', 'c', '0', '2001', 'b', '006', '16', '26', '364', '65', '66', '67', '68', '696', 'a', '6', 'b', '6', 'c', '6', 'd', '6', 'e', '6', 'f', '707', '17', '27', '374', '75', '767', '76', '162', '"', '\\', 'n']
Detokenized (005): ['cap2', '=', '(', '"0000385c02001b006162636465666768696a6b6c6d6e6f70717273747576776162"', '\\n']
Counter: 42
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "cap3 = ( "0301000000001122aabbccdd0102030405060708" ) . decode ( "hex" ) \n"
Original    (011): ['cap3', '=', '(', '"0301000000001122aabbccdd0102030405060708"', ')', '.', 'decode', '(', '"hex"', ')', '\\n']
Tokenized   (034): ['<s>', 'cap', '3', '=', '(', '"', '03', '01', '00000000', '112', '2', 'a', 'abb', 'cc', 'dd', '010', '20', '30', '40', '50', '60', '708', '"', ')', '.', 'decode', '(', '"', 'hex', '"', ')', '\\', 'n', '</s>']
Filtered   (032): ['cap', '3', '=', '(', '"', '03', '01', '00000000', '112', '2', 'a', 'abb', 'cc', 'dd', '010', '20', '30', '40', '50', '60', '708', '"', ')', '.', 'decode', '(', '"', 'hex', '"', ')', '\\', 'n']
Detokenized (011): ['cap3', '=', '(', '"0301000000001122aabbccdd0102030405060708"', ')', '.', 'decode', '(', '"hex"', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "intps [ ] = dest_target . vlan \n"
Original    (008): ['intps', '[', ']', '=', 'dest_target', '.', 'vlan', '\\n']
Tokenized   (015): ['<s>', 'int', 'ps', '[', ']', '=', 'dest', '_', 'target', '.', 'v', 'lan', '\\', 'n', '</s>']
Filtered   (013): ['int', 'ps', '[', ']', '=', 'dest', '_', 'target', '.', 'v', 'lan', '\\', 'n']
Detokenized (008): ['intps', '[', ']', '=', 'dest_target', '.', 'vlan', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "router , interface = ri . split ( ) \n"
Original    (010): ['router', ',', 'interface', '=', 'ri', '.', 'split', '(', ')', '\\n']
Tokenized   (015): ['<s>', 'rou', 'ter', ',', 'interface', '=', 'r', 'i', '.', 'split', '(', ')', '\\', 'n', '</s>']
Filtered   (013): ['rou', 'ter', ',', 'interface', '=', 'r', 'i', '.', 'split', '(', ')', '\\', 'n']
Detokenized (010): ['router', ',', 'interface', '=', 'ri', '.', 'split', '(', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "cm = NCSVPNConnectionManager ( ncs_services_url , user , password , port_map , name ) \n"
Original    (015): ['cm', '=', 'NCSVPNConnectionManager', '(', 'ncs_services_url', ',', 'user', ',', 'password', ',', 'port_map', ',', 'name', ')', '\\n']
Tokenized   (029): ['<s>', 'cm', '=', 'N', 'CS', 'VPN', 'Connection', 'Manager', '(', 'n', 'cs', '_', 'services', '_', 'url', ',', 'user', ',', 'password', ',', 'port', '_', 'map', ',', 'name', ')', '\\', 'n', '</s>']
Filtered   (027): ['cm', '=', 'N', 'CS', 'VPN', 'Connection', 'Manager', '(', 'n', 'cs', '_', 'services', '_', 'url', ',', 'user', ',', 'password', ',', 'port', '_', 'map', ',', 'name', ')', '\\', 'n']
Detokenized (015): ['cm', '=', 'NCSVPNConnectionManager', '(', 'ncs_services_url', ',', 'user', ',', 'password', ',', 'port_map', ',', 'name', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "soap_resource . registerDecoder ( actions . QUERY_RECURSIVE , self . queryRecursive ) \n"
Original    (013): ['soap_resource', '.', 'registerDecoder', '(', 'actions', '.', 'QUERY_RECURSIVE', ',', 'self', '.', 'queryRecursive', ')', '\\n']
Tokenized   (029): ['<s>', 'so', 'ap', '_', 'resource', '.', 'register', 'Dec', 'oder', '(', 'actions', '.', 'QU', 'ERY', '_', 'REC', 'UR', 'S', 'IVE', ',', 'self', '.', 'query', 'Rec', 'ursive', ')', '\\', 'n', '</s>']
Filtered   (027): ['so', 'ap', '_', 'resource', '.', 'register', 'Dec', 'oder', '(', 'actions', '.', 'QU', 'ERY', '_', 'REC', 'UR', 'S', 'IVE', ',', 'self', '.', 'query', 'Rec', 'ursive', ')', '\\', 'n']
Detokenized (013): ['soap_resource', '.', 'registerDecoder', '(', 'actions', '.', 'QUERY_RECURSIVE', ',', 'self', '.', 'queryRecursive', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "soap_fault = soapresource . SOAPFault ( err . getErrorMessage ( ) , ex_element ) \n"
Original    (015): ['soap_fault', '=', 'soapresource', '.', 'SOAPFault', '(', 'err', '.', 'getErrorMessage', '(', ')', ',', 'ex_element', ')', '\\n']
Tokenized   (030): ['<s>', 'so', 'ap', '_', 'f', 'ault', '=', 'soap', 'resource', '.', 'SO', 'AP', 'F', 'ault', '(', 'err', '.', 'get', 'Error', 'Message', '(', ')', ',', 'ex', '_', 'element', ')', '\\', 'n', '</s>']
Filtered   (028): ['so', 'ap', '_', 'f', 'ault', '=', 'soap', 'resource', '.', 'SO', 'AP', 'F', 'ault', '(', 'err', '.', 'get', 'Error', 'Message', '(', ')', ',', 'ex', '_', 'element', ')', '\\', 'n']
Detokenized (015): ['soap_fault', '=', 'soapresource', '.', 'SOAPFault', '(', 'err', '.', 'getErrorMessage', '(', ')', ',', 'ex_element', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "symmetric = p2ps . symmetricPath or False sd = nsa . Point2PointService ( src_stp , dst_stp , p2ps . capacity , p2ps . directionality , symmetric , \n"
Original    (028): ['symmetric', '=', 'p2ps', '.', 'symmetricPath', 'or', 'False', 'sd', '=', 'nsa', '.', 'Point2PointService', '(', 'src_stp', ',', 'dst_stp', ',', 'p2ps', '.', 'capacity', ',', 'p2ps', '.', 'directionality', ',', 'symmetric', ',', '\\n']
Tokenized   (053): ['<s>', 'sy', 'mm', 'etric', '=', 'p', '2', 'ps', '.', 'symm', 'etric', 'Path', 'or', 'False', 'sd', '=', 'n', 'sa', '.', 'Point', '2', 'Point', 'Service', '(', 'src', '_', 'st', 'p', ',', 'dst', '_', 'st', 'p', ',', 'p', '2', 'ps', '.', 'capacity', ',', 'p', '2', 'ps', '.', 'direction', 'ality', ',', 'symm', 'etric', ',', '\\', 'n', '</s>']
Filtered   (051): ['sy', 'mm', 'etric', '=', 'p', '2', 'ps', '.', 'symm', 'etric', 'Path', 'or', 'False', 'sd', '=', 'n', 'sa', '.', 'Point', '2', 'Point', 'Service', '(', 'src', '_', 'st', 'p', ',', 'dst', '_', 'st', 'p', ',', 'p', '2', 'ps', '.', 'capacity', ',', 'p', '2', 'ps', '.', 'direction', 'ality', ',', 'symm', 'etric', ',', '\\', 'n']
Detokenized (028): ['symmetric', '=', 'p2ps', '.', 'symmetricPath', 'or', 'False', 'sd', '=', 'nsa', '.', 'Point2PointService', '(', 'src_stp', ',', 'dst_stp', ',', 'p2ps', '.', 'capacity', ',', 'p2ps', '.', 'directionality', ',', 'symmetric', ',', '\\n']
Counter: 51
===================================================================
Hidden states:  (13, 28, 768)
# Extracted words:  28
Sentence         : "crt = nsa . Criteria ( criteria . version , schedule , sd ) \n"
Original    (015): ['crt', '=', 'nsa', '.', 'Criteria', '(', 'criteria', '.', 'version', ',', 'schedule', ',', 'sd', ')', '\\n']
Tokenized   (021): ['<s>', 'cr', 't', '=', 'n', 'sa', '.', 'Crit', 'eria', '(', 'criteria', '.', 'version', ',', 'schedule', ',', 'sd', ')', '\\', 'n', '</s>']
Filtered   (019): ['cr', 't', '=', 'n', 'sa', '.', 'Crit', 'eria', '(', 'criteria', '.', 'version', ',', 'schedule', ',', 'sd', ')', '\\', 'n']
Detokenized (015): ['crt', '=', 'nsa', '.', 'Criteria', '(', 'criteria', '.', 'version', ',', 'schedule', ',', 'sd', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "tc = json . load ( open ( tcf ) ) \n"
Original    (012): ['tc', '=', 'json', '.', 'load', '(', 'open', '(', 'tcf', ')', ')', '\\n']
Tokenized   (016): ['<s>', 'tc', '=', 'json', '.', 'load', '(', 'open', '(', 't', 'cf', ')', ')', '\\', 'n', '</s>']
Filtered   (014): ['tc', '=', 'json', '.', 'load', '(', 'open', '(', 't', 'cf', ')', ')', '\\', 'n']
Detokenized (012): ['tc', '=', 'json', '.', 'load', '(', 'open', '(', 'tcf', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "source_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , dest_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , start_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 2 ) \n"
Original    (057): ['source_stp', '=', 'nsa', '.', 'STP', '(', ',', ',', 'labels', '=', '[', 'nsa', '.', 'Label', '(', 'nml', '.', 'ETHERNET_VLAN', ',', 'dest_stp', '=', 'nsa', '.', 'STP', '(', ',', ',', 'labels', '=', '[', 'nsa', '.', 'Label', '(', 'nml', '.', 'ETHERNET_VLAN', ',', 'start_time', '=', 'datetime', '.', 'datetime', '.', 'utcnow', '(', ')', '+', 'datetime', '.', 'timedelta', '(', 'seconds', '=', '2', ')', '\\n']
Tokenized   (092): ['<s>', 'source', '_', 'st', 'p', '=', 'n', 'sa', '.', 'ST', 'P', '(', ',', ',', 'labels', '=', '[', 'n', 'sa', '.', 'Label', '(', 'n', 'ml', '.', 'E', 'THER', 'NET', '_', 'V', 'LAN', ',', 'dest', '_', 'st', 'p', '=', 'n', 'sa', '.', 'ST', 'P', '(', ',', ',', 'labels', '=', '[', 'n', 'sa', '.', 'Label', '(', 'n', 'ml', '.', 'E', 'THER', 'NET', '_', 'V', 'LAN', ',', 'start', '_', 'time', '=', 'dat', 'etime', '.', 'dat', 'etime', '.', 'ut', 'c', 'now', '(', ')', '+', 'dat', 'etime', '.', 'timed', 'elta', '(', 'seconds', '=', '2', ')', '\\', 'n', '</s>']
Filtered   (090): ['source', '_', 'st', 'p', '=', 'n', 'sa', '.', 'ST', 'P', '(', ',', ',', 'labels', '=', '[', 'n', 'sa', '.', 'Label', '(', 'n', 'ml', '.', 'E', 'THER', 'NET', '_', 'V', 'LAN', ',', 'dest', '_', 'st', 'p', '=', 'n', 'sa', '.', 'ST', 'P', '(', ',', ',', 'labels', '=', '[', 'n', 'sa', '.', 'Label', '(', 'n', 'ml', '.', 'E', 'THER', 'NET', '_', 'V', 'LAN', ',', 'start', '_', 'time', '=', 'dat', 'etime', '.', 'dat', 'etime', '.', 'ut', 'c', 'now', '(', ')', '+', 'dat', 'etime', '.', 'timed', 'elta', '(', 'seconds', '=', '2', ')', '\\', 'n']
Detokenized (057): ['source_stp', '=', 'nsa', '.', 'STP', '(', ',', ',', 'labels', '=', '[', 'nsa', '.', 'Label', '(', 'nml', '.', 'ETHERNET_VLAN', ',', 'dest_stp', '=', 'nsa', '.', 'STP', '(', ',', ',', 'labels', '=', '[', 'nsa', '.', 'Label', '(', 'nml', '.', 'ETHERNET_VLAN', ',', 'start_time', '=', 'datetime', '.', 'datetime', '.', 'utcnow', '(', ')', '+', 'datetime', '.', 'timedelta', '(', 'seconds', '=', '2', ')', '\\n']
Counter: 90
===================================================================
Hidden states:  (13, 57, 768)
# Extracted words:  57
Sentence         : "end_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 30 ) \n"
Original    (019): ['end_time', '=', 'datetime', '.', 'datetime', '.', 'utcnow', '(', ')', '+', 'datetime', '.', 'timedelta', '(', 'seconds', '=', '30', ')', '\\n']
Tokenized   (030): ['<s>', 'end', '_', 'time', '=', 'dat', 'etime', '.', 'dat', 'etime', '.', 'ut', 'c', 'now', '(', ')', '+', 'dat', 'etime', '.', 'timed', 'elta', '(', 'seconds', '=', '30', ')', '\\', 'n', '</s>']
Filtered   (028): ['end', '_', 'time', '=', 'dat', 'etime', '.', 'dat', 'etime', '.', 'ut', 'c', 'now', '(', ')', '+', 'dat', 'etime', '.', 'timed', 'elta', '(', 'seconds', '=', '30', ')', '\\', 'n']
Detokenized (019): ['end_time', '=', 'datetime', '.', 'datetime', '.', 'utcnow', '(', ')', '+', 'datetime', '.', 'timedelta', '(', 'seconds', '=', '30', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "connection_id , active , version_consistent , version , timestamp = yield d_down \n"
Original    (013): ['connection_id', ',', 'active', ',', 'version_consistent', ',', 'version', ',', 'timestamp', '=', 'yield', 'd_down', '\\n']
Tokenized   (023): ['<s>', 'connection', '_', 'id', ',', 'active', ',', 'version', '_', 'cons', 'istent', ',', 'version', ',', 'timestamp', '=', 'yield', 'd', '_', 'down', '\\', 'n', '</s>']
Filtered   (021): ['connection', '_', 'id', ',', 'active', ',', 'version', '_', 'cons', 'istent', ',', 'version', ',', 'timestamp', '=', 'yield', 'd', '_', 'down', '\\', 'n']
Detokenized (013): ['connection_id', ',', 'active', ',', 'version_consistent', ',', 'version', ',', 'timestamp', '=', 'yield', 'd_down', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "scheduler = digits . scheduler . Scheduler ( config_value ( ) , True ) \n"
Original    (015): ['scheduler', '=', 'digits', '.', 'scheduler', '.', 'Scheduler', '(', 'config_value', '(', ')', ',', 'True', ')', '\\n']
Tokenized   (024): ['<s>', 'sc', 'hed', 'uler', '=', 'digits', '.', 'sched', 'uler', '.', 'Sched', 'uler', '(', 'config', '_', 'value', '(', ')', ',', 'True', ')', '\\', 'n', '</s>']
Filtered   (022): ['sc', 'hed', 'uler', '=', 'digits', '.', 'sched', 'uler', '.', 'Sched', 'uler', '(', 'config', '_', 'value', '(', ')', ',', 'True', ')', '\\', 'n']
Detokenized (015): ['scheduler', '=', 'digits', '.', 'scheduler', '.', 'Scheduler', '(', 'config_value', '(', ')', ',', 'True', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "row_index = int ( params [ ] [ 0 ] ) \n"
Original    (012): ['row_index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '\\n']
Tokenized   (017): ['<s>', 'row', '_', 'index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '\\', 'n', '</s>']
Filtered   (015): ['row', '_', 'index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '\\', 'n']
Detokenized (012): ['row_index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "char_index = int ( params [ ] [ 0 ] ) - 1 \n"
Original    (014): ['char_index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\n']
Tokenized   (019): ['<s>', 'char', '_', 'index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\', 'n', '</s>']
Filtered   (017): ['char', '_', 'index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\', 'n']
Detokenized (014): ['char_index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "comparator = comparators . index ( params [ ] [ 0 ] ) - 1 \n"
Original    (016): ['comparator', '=', 'comparators', '.', 'index', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\n']
Tokenized   (022): ['<s>', 'com', 'par', 'ator', '=', 'compar', 'ators', '.', 'index', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\', 'n', '</s>']
Filtered   (020): ['com', 'par', 'ator', '=', 'compar', 'ators', '.', 'index', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\', 'n']
Detokenized (016): ['comparator', '=', 'comparators', '.', 'index', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "truth = ( cmp ( ord ( current_character ) , test_char ) == comparator ) \n"
Original    (016): ['truth', '=', '(', 'cmp', '(', 'ord', '(', 'current_character', ')', ',', 'test_char', ')', '==', 'comparator', ')', '\\n']
Tokenized   (025): ['<s>', 'truth', '=', '(', 'c', 'mp', '(', 'ord', '(', 'current', '_', 'character', ')', ',', 'test', '_', 'char', ')', '==', 'compar', 'ator', ')', '\\', 'n', '</s>']
Filtered   (023): ['truth', '=', '(', 'c', 'mp', '(', 'ord', '(', 'current', '_', 'character', ')', ',', 'test', '_', 'char', ')', '==', 'compar', 'ator', ')', '\\', 'n']
Detokenized (016): ['truth', '=', '(', 'cmp', '(', 'ord', '(', 'current_character', ')', ',', 'test_char', ')', '==', 'comparator', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "start_response ( , [ ( , ) ] ) \n"
Original    (010): ['start_response', '(', ',', '[', '(', ',', ')', ']', ')', '\\n']
Tokenized   (015): ['<s>', 'start', '_', 'response', '(', ',', '[', '(', ',', ')', ']', ')', '\\', 'n', '</s>']
Filtered   (013): ['start', '_', 'response', '(', ',', '[', '(', ',', ')', ']', ')', '\\', 'n']
Detokenized (010): ['start_response', '(', ',', '[', '(', ',', ')', ']', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "CHARSET = [ chr ( x ) for x in xrange ( 32 , 127 ) ] \n"
Original    (018): ['CHARSET', '=', '[', 'chr', '(', 'x', ')', 'for', 'x', 'in', 'xrange', '(', '32', ',', '127', ')', ']', '\\n']
Tokenized   (025): ['<s>', 'CH', 'ARS', 'ET', '=', '[', 'ch', 'r', '(', 'x', ')', 'for', 'x', 'in', 'x', 'range', '(', '32', ',', '127', ')', ']', '\\', 'n', '</s>']
Filtered   (023): ['CH', 'ARS', 'ET', '=', '[', 'ch', 'r', '(', 'x', ')', 'for', 'x', 'in', 'x', 'range', '(', '32', ',', '127', ')', ']', '\\', 'n']
Detokenized (018): ['CHARSET', '=', '[', 'chr', '(', 'x', ')', 'for', 'x', 'in', 'xrange', '(', '32', ',', '127', ')', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "obj_struct [ ] = [ int ( bbox . find ( ) . text ) , \n"
Original    (017): ['obj_struct', '[', ']', '=', '[', 'int', '(', 'bbox', '.', 'find', '(', ')', '.', 'text', ')', ',', '\\n']
Tokenized   (023): ['<s>', 'obj', '_', 'struct', '[', ']', '=', '[', 'int', '(', 'b', 'box', '.', 'find', '(', ')', '.', 'text', ')', ',', '\\', 'n', '</s>']
Filtered   (021): ['obj', '_', 'struct', '[', ']', '=', '[', 'int', '(', 'b', 'box', '.', 'find', '(', ')', '.', 'text', ')', ',', '\\', 'n']
Detokenized (017): ['obj_struct', '[', ']', '=', '[', 'int', '(', 'bbox', '.', 'find', '(', ')', '.', 'text', ')', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "int ( bbox . find ( ) . text ) ] \n"
Original    (012): ['int', '(', 'bbox', '.', 'find', '(', ')', '.', 'text', ')', ']', '\\n']
Tokenized   (016): ['<s>', 'int', '(', 'b', 'box', '.', 'find', '(', ')', '.', 'text', ')', ']', '\\', 'n', '</s>']
Filtered   (014): ['int', '(', 'b', 'box', '.', 'find', '(', ')', '.', 'text', ')', ']', '\\', 'n']
Detokenized (012): ['int', '(', 'bbox', '.', 'find', '(', ')', '.', 'text', ')', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "mpre = np . concatenate ( ( [ 0. ] , prec , [ 0. ] ) ) \n"
Original    (019): ['mpre', '=', 'np', '.', 'concatenate', '(', '(', '[', '0.', ']', ',', 'prec', ',', '[', '0.', ']', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'mp', 're', '=', 'np', '.', 'conc', 'aten', 'ate', '(', '(', '[', '0', '.', ']', ',', 'prec', ',', '[', '0', '.', ']', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['mp', 're', '=', 'np', '.', 'conc', 'aten', 'ate', '(', '(', '[', '0', '.', ']', ',', 'prec', ',', '[', '0', '.', ']', ')', ')', '\\', 'n']
Detokenized (019): ['mpre', '=', 'np', '.', 'concatenate', '(', '(', '[', '0.', ']', ',', 'prec', ',', '[', '0.', ']', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "difficult = np . array ( [ x [ ] for x in R ] ) . astype ( np . bool ) \n"
Original    (024): ['difficult', '=', 'np', '.', 'array', '(', '[', 'x', '[', ']', 'for', 'x', 'in', 'R', ']', ')', '.', 'astype', '(', 'np', '.', 'bool', ')', '\\n']
Tokenized   (029): ['<s>', 'diff', 'icult', '=', 'np', '.', 'array', '(', '[', 'x', '[', ']', 'for', 'x', 'in', 'R', ']', ')', '.', 'ast', 'ype', '(', 'np', '.', 'bool', ')', '\\', 'n', '</s>']
Filtered   (027): ['diff', 'icult', '=', 'np', '.', 'array', '(', '[', 'x', '[', ']', 'for', 'x', 'in', 'R', ']', ')', '.', 'ast', 'ype', '(', 'np', '.', 'bool', ')', '\\', 'n']
Detokenized (024): ['difficult', '=', 'np', '.', 'array', '(', '[', 'x', '[', ']', 'for', 'x', 'in', 'R', ']', ')', '.', 'astype', '(', 'np', '.', 'bool', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "det = [ False ] * len ( R ) \n"
Original    (011): ['det', '=', '[', 'False', ']', '*', 'len', '(', 'R', ')', '\\n']
Tokenized   (014): ['<s>', 'det', '=', '[', 'False', ']', '*', 'len', '(', 'R', ')', '\\', 'n', '</s>']
Filtered   (012): ['det', '=', '[', 'False', ']', '*', 'len', '(', 'R', ')', '\\', 'n']
Detokenized (011): ['det', '=', '[', 'False', ']', '*', 'len', '(', 'R', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "npos = npos + sum ( ~ difficult ) \n"
Original    (010): ['npos', '=', 'npos', '+', 'sum', '(', '~', 'difficult', ')', '\\n']
Tokenized   (015): ['<s>', 'n', 'pos', '=', 'n', 'pos', '+', 'sum', '(', '~', 'difficult', ')', '\\', 'n', '</s>']
Filtered   (013): ['n', 'pos', '=', 'n', 'pos', '+', 'sum', '(', '~', 'difficult', ')', '\\', 'n']
Detokenized (010): ['npos', '=', 'npos', '+', 'sum', '(', '~', 'difficult', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "class_recs [ imagename ] = { : bbox , \n"
Original    (010): ['class_recs', '[', 'imagename', ']', '=', '{', ':', 'bbox', ',', '\\n']
Tokenized   (018): ['<s>', 'class', '_', 're', 'cs', '[', 'imag', 'ename', ']', '=', '{', ':', 'b', 'box', ',', '\\', 'n', '</s>']
Filtered   (016): ['class', '_', 're', 'cs', '[', 'imag', 'ename', ']', '=', '{', ':', 'b', 'box', ',', '\\', 'n']
Detokenized (010): ['class_recs', '[', 'imagename', ']', '=', '{', ':', 'bbox', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "confidence = np . array ( [ float ( x [ 1 ] ) for x in splitlines ] ) \n"
Original    (021): ['confidence', '=', 'np', '.', 'array', '(', '[', 'float', '(', 'x', '[', '1', ']', ')', 'for', 'x', 'in', 'splitlines', ']', ')', '\\n']
Tokenized   (025): ['<s>', 'confidence', '=', 'np', '.', 'array', '(', '[', 'float', '(', 'x', '[', '1', ']', ')', 'for', 'x', 'in', 'split', 'lines', ']', ')', '\\', 'n', '</s>']
Filtered   (023): ['confidence', '=', 'np', '.', 'array', '(', '[', 'float', '(', 'x', '[', '1', ']', ')', 'for', 'x', 'in', 'split', 'lines', ']', ')', '\\', 'n']
Detokenized (021): ['confidence', '=', 'np', '.', 'array', '(', '[', 'float', '(', 'x', '[', '1', ']', ')', 'for', 'x', 'in', 'splitlines', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "sorted_ind = np . argsort ( - confidence ) \n"
Original    (010): ['sorted_ind', '=', 'np', '.', 'argsort', '(', '-', 'confidence', ')', '\\n']
Tokenized   (017): ['<s>', 's', 'orted', '_', 'ind', '=', 'np', '.', 'args', 'ort', '(', '-', 'confidence', ')', '\\', 'n', '</s>']
Filtered   (015): ['s', 'orted', '_', 'ind', '=', 'np', '.', 'args', 'ort', '(', '-', 'confidence', ')', '\\', 'n']
Detokenized (010): ['sorted_ind', '=', 'np', '.', 'argsort', '(', '-', 'confidence', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "BB = BB [ sorted_ind , : ] \n"
Original    (009): ['BB', '=', 'BB', '[', 'sorted_ind', ',', ':', ']', '\\n']
Tokenized   (014): ['<s>', 'BB', '=', 'BB', '[', 'sorted', '_', 'ind', ',', ':', ']', '\\', 'n', '</s>']
Filtered   (012): ['BB', '=', 'BB', '[', 'sorted', '_', 'ind', ',', ':', ']', '\\', 'n']
Detokenized (009): ['BB', '=', 'BB', '[', 'sorted_ind', ',', ':', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "image_ids = [ image_ids [ x ] for x in sorted_ind ] \n"
Original    (013): ['image_ids', '=', '[', 'image_ids', '[', 'x', ']', 'for', 'x', 'in', 'sorted_ind', ']', '\\n']
Tokenized   (022): ['<s>', 'image', '_', 'ids', '=', '[', 'image', '_', 'ids', '[', 'x', ']', 'for', 'x', 'in', 'sorted', '_', 'ind', ']', '\\', 'n', '</s>']
Filtered   (020): ['image', '_', 'ids', '=', '[', 'image', '_', 'ids', '[', 'x', ']', 'for', 'x', 'in', 'sorted', '_', 'ind', ']', '\\', 'n']
Detokenized (013): ['image_ids', '=', '[', 'image_ids', '[', 'x', ']', 'for', 'x', 'in', 'sorted_ind', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "bb = BB [ d , : ] . astype ( float ) \n"
Original    (014): ['bb', '=', 'BB', '[', 'd', ',', ':', ']', '.', 'astype', '(', 'float', ')', '\\n']
Tokenized   (018): ['<s>', 'bb', '=', 'BB', '[', 'd', ',', ':', ']', '.', 'ast', 'ype', '(', 'float', ')', '\\', 'n', '</s>']
Filtered   (016): ['bb', '=', 'BB', '[', 'd', ',', ':', ']', '.', 'ast', 'ype', '(', 'float', ')', '\\', 'n']
Detokenized (014): ['bb', '=', 'BB', '[', 'd', ',', ':', ']', '.', 'astype', '(', 'float', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "BBGT = R [ ] . astype ( float ) \n"
Original    (011): ['BBGT', '=', 'R', '[', ']', '.', 'astype', '(', 'float', ')', '\\n']
Tokenized   (016): ['<s>', 'BB', 'GT', '=', 'R', '[', ']', '.', 'ast', 'ype', '(', 'float', ')', '\\', 'n', '</s>']
Filtered   (014): ['BB', 'GT', '=', 'R', '[', ']', '.', 'ast', 'ype', '(', 'float', ')', '\\', 'n']
Detokenized (011): ['BBGT', '=', 'R', '[', ']', '.', 'astype', '(', 'float', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "iymin = np . maximum ( BBGT [ : , 1 ] , bb [ 1 ] ) \n"
Original    (019): ['iymin', '=', 'np', '.', 'maximum', '(', 'BBGT', '[', ':', ',', '1', ']', ',', 'bb', '[', '1', ']', ')', '\\n']
Tokenized   (025): ['<s>', 'iy', 'min', '=', 'np', '.', 'maximum', '(', 'BB', 'GT', '[', ':', ',', '1', ']', ',', 'b', 'b', '[', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (023): ['iy', 'min', '=', 'np', '.', 'maximum', '(', 'BB', 'GT', '[', ':', ',', '1', ']', ',', 'b', 'b', '[', '1', ']', ')', '\\', 'n']
Detokenized (019): ['iymin', '=', 'np', '.', 'maximum', '(', 'BBGT', '[', ':', ',', '1', ']', ',', 'bb', '[', '1', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "iw = np . maximum ( ixmax - ixmin + 1. , 0. ) \n"
Original    (015): ['iw', '=', 'np', '.', 'maximum', '(', 'ixmax', '-', 'ixmin', '+', '1.', ',', '0.', ')', '\\n']
Tokenized   (024): ['<s>', 'iw', '=', 'np', '.', 'maximum', '(', '', 'ix', 'max', '-', '', 'ix', 'min', '+', '1', '.', ',', '0', '.', ')', '\\', 'n', '</s>']
Filtered   (022): ['iw', '=', 'np', '.', 'maximum', '(', '', 'ix', 'max', '-', '', 'ix', 'min', '+', '1', '.', ',', '0', '.', ')', '\\', 'n']
Detokenized (015): ['iw', '=', 'np', '.', 'maximum', '(', 'ixmax', '-', 'ixmin', '+', '1.', ',', '0.', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "uni = ( ( bb [ 2 ] - bb [ 0 ] + 1. ) * ( bb [ 3 ] - bb [ 1 ] + 1. ) + \n"
Original    (032): ['uni', '=', '(', '(', 'bb', '[', '2', ']', '-', 'bb', '[', '0', ']', '+', '1.', ')', '*', '(', 'bb', '[', '3', ']', '-', 'bb', '[', '1', ']', '+', '1.', ')', '+', '\\n']
Tokenized   (041): ['<s>', 'uni', '=', '(', '(', 'b', 'b', '[', '2', ']', '-', 'b', 'b', '[', '0', ']', '+', '1', '.', ')', '*', '(', 'b', 'b', '[', '3', ']', '-', 'b', 'b', '[', '1', ']', '+', '1', '.', ')', '+', '\\', 'n', '</s>']
Filtered   (039): ['uni', '=', '(', '(', 'b', 'b', '[', '2', ']', '-', 'b', 'b', '[', '0', ']', '+', '1', '.', ')', '*', '(', 'b', 'b', '[', '3', ']', '-', 'b', 'b', '[', '1', ']', '+', '1', '.', ')', '+', '\\', 'n']
Detokenized (032): ['uni', '=', '(', '(', 'bb', '[', '2', ']', '-', 'bb', '[', '0', ']', '+', '1.', ')', '*', '(', 'bb', '[', '3', ']', '-', 'bb', '[', '1', ']', '+', '1.', ')', '+', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "rec = tp / float ( npos ) \n"
Original    (009): ['rec', '=', 'tp', '/', 'float', '(', 'npos', ')', '\\n']
Tokenized   (014): ['<s>', 'rec', '=', 't', 'p', '/', 'float', '(', 'n', 'pos', ')', '\\', 'n', '</s>']
Filtered   (012): ['rec', '=', 't', 'p', '/', 'float', '(', 'n', 'pos', ')', '\\', 'n']
Detokenized (009): ['rec', '=', 'tp', '/', 'float', '(', 'npos', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "prec = tp / ( tp + fp + 1e-10 ) \n"
Original    (012): ['prec', '=', 'tp', '/', '(', 'tp', '+', 'fp', '+', '1e-10', ')', '\\n']
Tokenized   (022): ['<s>', 'pre', 'c', '=', 't', 'p', '/', '(', 't', 'p', '+', 'f', 'p', '+', '1', 'e', '-', '10', ')', '\\', 'n', '</s>']
Filtered   (020): ['pre', 'c', '=', 't', 'p', '/', '(', 't', 'p', '+', 'f', 'p', '+', '1', 'e', '-', '10', ')', '\\', 'n']
Detokenized (012): ['prec', '=', 'tp', '/', '(', 'tp', '+', 'fp', '+', '1e-10', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "scale = strip_mantissa ( maxval ) / float ( 1 << ( bits - sign - 1 ) ) \n"
Original    (020): ['scale', '=', 'strip_mantissa', '(', 'maxval', ')', '/', 'float', '(', '1', '<<', '(', 'bits', '-', 'sign', '-', '1', ')', ')', '\\n']
Tokenized   (028): ['<s>', 'scale', '=', 'strip', '_', 'm', 'ant', 'issa', '(', 'max', 'val', ')', '/', 'float', '(', '1', '<<', '(', 'bits', '-', 'sign', '-', '1', ')', ')', '\\', 'n', '</s>']
Filtered   (026): ['scale', '=', 'strip', '_', 'm', 'ant', 'issa', '(', 'max', 'val', ')', '/', 'float', '(', '1', '<<', '(', 'bits', '-', 'sign', '-', '1', ')', ')', '\\', 'n']
Detokenized (020): ['scale', '=', 'strip_mantissa', '(', 'maxval', ')', '/', 'float', '(', '1', '<<', '(', 'bits', '-', 'sign', '-', '1', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "ary = np . around ( ary * ( 1.0 / scale ) ) . astype ( np . int64 ) \n"
Original    (022): ['ary', '=', 'np', '.', 'around', '(', 'ary', '*', '(', '1.0', '/', 'scale', ')', ')', '.', 'astype', '(', 'np', '.', 'int64', ')', '\\n']
Tokenized   (030): ['<s>', 'ary', '=', 'np', '.', 'around', '(', 'a', 'ry', '*', '(', '1', '.', '0', '/', 'scale', ')', ')', '.', 'ast', 'ype', '(', 'np', '.', 'int', '64', ')', '\\', 'n', '</s>']
Filtered   (028): ['ary', '=', 'np', '.', 'around', '(', 'a', 'ry', '*', '(', '1', '.', '0', '/', 'scale', ')', ')', '.', 'ast', 'ype', '(', 'np', '.', 'int', '64', ')', '\\', 'n']
Detokenized (022): ['ary', '=', 'np', '.', 'around', '(', 'ary', '*', '(', '1.0', '/', 'scale', ')', ')', '.', 'astype', '(', 'np', '.', 'int64', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "f2 -= dif \n"
Original    (004): ['f2', '-=', 'dif', '\\n']
Tokenized   (009): ['<s>', 'f', '2', '-=', 'd', 'if', '\\', 'n', '</s>']
Filtered   (007): ['f', '2', '-=', 'd', 'if', '\\', 'n']
Detokenized (004): ['f2', '-=', 'dif', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "slicedF = F [ : , sliceR , sliceS , : ] . reshape ( ( - 1 , K ) ) \n"
Original    (023): ['slicedF', '=', 'F', '[', ':', ',', 'sliceR', ',', 'sliceS', ',', ':', ']', '.', 'reshape', '(', '(', '-', '1', ',', 'K', ')', ')', '\\n']
Tokenized   (032): ['<s>', 's', 'lic', 'ed', 'F', '=', 'F', '[', ':', ',', 'slice', 'R', ',', 'slice', 'S', ',', ':', ']', '.', 'resh', 'ape', '(', '(', '-', '1', ',', 'K', ')', ')', '\\', 'n', '</s>']
Filtered   (030): ['s', 'lic', 'ed', 'F', '=', 'F', '[', ':', ',', 'slice', 'R', ',', 'slice', 'S', ',', ':', ']', '.', 'resh', 'ape', '(', '(', '-', '1', ',', 'K', ')', ')', '\\', 'n']
Detokenized (023): ['slicedF', '=', 'F', '[', ':', ',', 'sliceR', ',', 'sliceS', ',', ':', ']', '.', 'reshape', '(', '(', '-', '1', ',', 'K', ')', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "K , P , Q , N = E . shape \n"
Original    (012): ['K', ',', 'P', ',', 'Q', ',', 'N', '=', 'E', '.', 'shape', '\\n']
Tokenized   (015): ['<s>', 'K', ',', 'P', ',', 'Q', ',', 'N', '=', 'E', '.', 'shape', '\\', 'n', '</s>']
Filtered   (013): ['K', ',', 'P', ',', 'Q', ',', 'N', '=', 'E', '.', 'shape', '\\', 'n']
Detokenized (012): ['K', ',', 'P', ',', 'Q', ',', 'N', '=', 'E', '.', 'shape', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "qSlice = [ fconv_slice ( q , S , X , padding [ 0 ] , strides [ 0 ] ) for q in range ( Q ) ] \n"
Original    (030): ['qSlice', '=', '[', 'fconv_slice', '(', 'q', ',', 'S', ',', 'X', ',', 'padding', '[', '0', ']', ',', 'strides', '[', '0', ']', ')', 'for', 'q', 'in', 'range', '(', 'Q', ')', ']', '\\n']
Tokenized   (038): ['<s>', 'q', 'Sl', 'ice', '=', '[', 'f', 'conv', '_', 'slice', '(', 'q', ',', 'S', ',', 'X', ',', 'padding', '[', '0', ']', ',', 'strides', '[', '0', ']', ')', 'for', 'q', 'in', 'range', '(', 'Q', ')', ']', '\\', 'n', '</s>']
Filtered   (036): ['q', 'Sl', 'ice', '=', '[', 'f', 'conv', '_', 'slice', '(', 'q', ',', 'S', ',', 'X', ',', 'padding', '[', '0', ']', ',', 'strides', '[', '0', ']', ')', 'for', 'q', 'in', 'range', '(', 'Q', ')', ']', '\\', 'n']
Detokenized (030): ['qSlice', '=', '[', 'fconv_slice', '(', 'q', ',', 'S', ',', 'X', ',', 'padding', '[', '0', ']', ',', 'strides', '[', '0', ']', ')', 'for', 'q', 'in', 'range', '(', 'Q', ')', ']', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 30, 768)
# Extracted words:  30
Sentence         : "slicedE = E [ : , p , q , : ] \n"
Original    (013): ['slicedE', '=', 'E', '[', ':', ',', 'p', ',', 'q', ',', ':', ']', '\\n']
Tokenized   (019): ['<s>', 's', 'lic', 'ed', 'E', '=', 'E', '[', ':', ',', 'p', ',', 'q', ',', ':', ']', '\\', 'n', '</s>']
Filtered   (017): ['s', 'lic', 'ed', 'E', '=', 'E', '[', ':', ',', 'p', ',', 'q', ',', ':', ']', '\\', 'n']
Detokenized (013): ['slicedE', '=', 'E', '[', ':', ',', 'p', ',', 'q', ',', ':', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "rcp3 = 1.0 / 3.0 \n"
Original    (006): ['rcp3', '=', '1.0', '/', '3.0', '\\n']
Tokenized   (015): ['<s>', 'rc', 'p', '3', '=', '1', '.', '0', '/', '3', '.', '0', '\\', 'n', '</s>']
Filtered   (013): ['rc', 'p', '3', '=', '1', '.', '0', '/', '3', '.', '0', '\\', 'n']
Detokenized (006): ['rcp3', '=', '1.0', '/', '3.0', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "t3 = I [ 1 , : ] + I [ 3 , : ] * 4.0 \n"
Original    (018): ['t3', '=', 'I', '[', '1', ',', ':', ']', '+', 'I', '[', '3', ',', ':', ']', '*', '4.0', '\\n']
Tokenized   (024): ['<s>', 't', '3', '=', 'I', '[', '1', ',', ':', ']', '+', 'I', '[', '3', ',', ':', ']', '*', '4', '.', '0', '\\', 'n', '</s>']
Filtered   (022): ['t', '3', '=', 'I', '[', '1', ',', ':', ']', '+', 'I', '[', '3', ',', ':', ']', '*', '4', '.', '0', '\\', 'n']
Detokenized (018): ['t3', '=', 'I', '[', '1', ',', ':', ']', '+', 'I', '[', '3', ',', ':', ']', '*', '4.0', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "T1 = np . empty ( ( 3 , 3 ) ) \n"
Original    (013): ['T1', '=', 'np', '.', 'empty', '(', '(', '3', ',', '3', ')', ')', '\\n']
Tokenized   (017): ['<s>', 'T', '1', '=', 'np', '.', 'empty', '(', '(', '3', ',', '3', ')', ')', '\\', 'n', '</s>']
Filtered   (015): ['T', '1', '=', 'np', '.', 'empty', '(', '(', '3', ',', '3', ')', ')', '\\', 'n']
Detokenized (013): ['T1', '=', 'np', '.', 'empty', '(', '(', '3', ',', '3', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "Fw = np . empty ( ( D , D , C , K ) ) \n"
Original    (017): ['Fw', '=', 'np', '.', 'empty', '(', '(', 'D', ',', 'D', ',', 'C', ',', 'K', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'F', 'w', '=', 'np', '.', 'empty', '(', '(', 'D', ',', 'D', ',', 'C', ',', 'K', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['F', 'w', '=', 'np', '.', 'empty', '(', '(', 'D', ',', 'D', ',', 'C', ',', 'K', ')', ')', '\\', 'n']
Detokenized (017): ['Fw', '=', 'np', '.', 'empty', '(', '(', 'D', ',', 'D', ',', 'C', ',', 'K', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "sliceI = I [ : , start_y : stop_y , start_x : stop_x , : ] \n"
Original    (017): ['sliceI', '=', 'I', '[', ':', ',', 'start_y', ':', 'stop_y', ',', 'start_x', ':', 'stop_x', ',', ':', ']', '\\n']
Tokenized   (029): ['<s>', 'slice', 'I', '=', 'I', '[', ':', ',', 'start', '_', 'y', ':', 'stop', '_', 'y', ',', 'start', '_', 'x', ':', 'stop', '_', 'x', ',', ':', ']', '\\', 'n', '</s>']
Filtered   (027): ['slice', 'I', '=', 'I', '[', ':', ',', 'start', '_', 'y', ':', 'stop', '_', 'y', ',', 'start', '_', 'x', ':', 'stop', '_', 'x', ',', ':', ']', '\\', 'n']
Detokenized (017): ['sliceI', '=', 'I', '[', ':', ',', 'start_y', ':', 'stop_y', ',', 'start_x', ':', 'stop_x', ',', ':', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "O [ k , p0 : p1 , q0 : q1 , n ] = Out [ 0 : plen , 0 : qlen ] \n"
Original    (026): ['O', '[', 'k', ',', 'p0', ':', 'p1', ',', 'q0', ':', 'q1', ',', 'n', ']', '=', 'Out', '[', '0', ':', 'plen', ',', '0', ':', 'qlen', ']', '\\n']
Tokenized   (035): ['<s>', 'O', '[', 'k', ',', 'p', '0', ':', 'p', '1', ',', 'q', '0', ':', 'q', '1', ',', 'n', ']', '=', 'Out', '[', '0', ':', 'pl', 'en', ',', '0', ':', 'q', 'len', ']', '\\', 'n', '</s>']
Filtered   (033): ['O', '[', 'k', ',', 'p', '0', ':', 'p', '1', ',', 'q', '0', ':', 'q', '1', ',', 'n', ']', '=', 'Out', '[', '0', ':', 'pl', 'en', ',', '0', ':', 'q', 'len', ']', '\\', 'n']
Detokenized (026): ['O', '[', 'k', ',', 'p0', ':', 'p1', ',', 'q0', ':', 'q1', ',', 'n', ']', '=', 'Out', '[', '0', ':', 'plen', ',', '0', ':', 'qlen', ']', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "start_p , stop_p , pad_p = image_slice ( y , P , B , B ) \n"
Original    (017): ['start_p', ',', 'stop_p', ',', 'pad_p', '=', 'image_slice', '(', 'y', ',', 'P', ',', 'B', ',', 'B', ')', '\\n']
Tokenized   (028): ['<s>', 'start', '_', 'p', ',', 'stop', '_', 'p', ',', 'pad', '_', 'p', '=', 'image', '_', 'slice', '(', 'y', ',', 'P', ',', 'B', ',', 'B', ')', '\\', 'n', '</s>']
Filtered   (026): ['start', '_', 'p', ',', 'stop', '_', 'p', ',', 'pad', '_', 'p', '=', 'image', '_', 'slice', '(', 'y', ',', 'P', ',', 'B', ',', 'B', ')', '\\', 'n']
Detokenized (017): ['start_p', ',', 'stop_p', ',', 'pad_p', '=', 'image_slice', '(', 'y', ',', 'P', ',', 'B', ',', 'B', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "trans = ( 2 , 2 ) \n"
Original    (008): ['trans', '=', '(', '2', ',', '2', ')', '\\n']
Tokenized   (011): ['<s>', 'trans', '=', '(', '2', ',', '2', ')', '\\', 'n', '</s>']
Filtered   (009): ['trans', '=', '(', '2', ',', '2', ')', '\\', 'n']
Detokenized (008): ['trans', '=', '(', '2', ',', '2', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "E = np . random . uniform ( - 1.0 , 1.0 , dimO ) \n"
Original    (016): ['E', '=', 'np', '.', 'random', '.', 'uniform', '(', '-', '1.0', ',', '1.0', ',', 'dimO', ')', '\\n']
Tokenized   (024): ['<s>', 'E', '=', 'np', '.', 'random', '.', 'uniform', '(', '-', '1', '.', '0', ',', '1', '.', '0', ',', 'dim', 'O', ')', '\\', 'n', '</s>']
Filtered   (022): ['E', '=', 'np', '.', 'random', '.', 'uniform', '(', '-', '1', '.', '0', ',', '1', '.', '0', ',', 'dim', 'O', ')', '\\', 'n']
Detokenized (016): ['E', '=', 'np', '.', 'random', '.', 'uniform', '(', '-', '1.0', ',', '1.0', ',', 'dimO', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "xprop_direct ( E , F , Bd , padding , strides , backward = True ) \n"
Original    (017): ['xprop_direct', '(', 'E', ',', 'F', ',', 'Bd', ',', 'padding', ',', 'strides', ',', 'backward', '=', 'True', ')', '\\n']
Tokenized   (024): ['<s>', 'x', 'prop', '_', 'direct', '(', 'E', ',', 'F', ',', 'B', 'd', ',', 'padding', ',', 'strides', ',', 'backward', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (022): ['x', 'prop', '_', 'direct', '(', 'E', ',', 'F', ',', 'B', 'd', ',', 'padding', ',', 'strides', ',', 'backward', '=', 'True', ')', '\\', 'n']
Detokenized (017): ['xprop_direct', '(', 'E', ',', 'F', ',', 'Bd', ',', 'padding', ',', 'strides', ',', 'backward', '=', 'True', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "xprop_winograd ( E , F , Bw , padding , minimal = minimal , trans = trans , backward = True ) \n"
Original    (023): ['xprop_winograd', '(', 'E', ',', 'F', ',', 'Bw', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ',', 'backward', '=', 'True', ')', '\\n']
Tokenized   (032): ['<s>', 'x', 'prop', '_', 'win', 'og', 'rad', '(', 'E', ',', 'F', ',', 'B', 'w', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ',', 'backward', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (030): ['x', 'prop', '_', 'win', 'og', 'rad', '(', 'E', ',', 'F', ',', 'B', 'w', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ',', 'backward', '=', 'True', ')', '\\', 'n']
Detokenized (023): ['xprop_winograd', '(', 'E', ',', 'F', ',', 'Bw', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ',', 'backward', '=', 'True', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "updat_direct ( I , E , Ud , padding , strides ) \n"
Original    (013): ['updat_direct', '(', 'I', ',', 'E', ',', 'Ud', ',', 'padding', ',', 'strides', ')', '\\n']
Tokenized   (019): ['<s>', 'up', 'dat', '_', 'direct', '(', 'I', ',', 'E', ',', 'Ud', ',', 'padding', ',', 'strides', ')', '\\', 'n', '</s>']
Filtered   (017): ['up', 'dat', '_', 'direct', '(', 'I', ',', 'E', ',', 'Ud', ',', 'padding', ',', 'strides', ')', '\\', 'n']
Detokenized (013): ['updat_direct', '(', 'I', ',', 'E', ',', 'Ud', ',', 'padding', ',', 'strides', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "updat_winograd ( I , E , Uw , padding , minimal = minimal , trans = trans ) \n"
Original    (019): ['updat_winograd', '(', 'I', ',', 'E', ',', 'Uw', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ')', '\\n']
Tokenized   (028): ['<s>', 'up', 'dat', '_', 'win', 'og', 'rad', '(', 'I', ',', 'E', ',', 'U', 'w', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ')', '\\', 'n', '</s>']
Filtered   (026): ['up', 'dat', '_', 'win', 'og', 'rad', '(', 'I', ',', 'E', ',', 'U', 'w', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ')', '\\', 'n']
Detokenized (019): ['updat_winograd', '(', 'I', ',', 'E', ',', 'Uw', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "BranchNode , SkipNode , LRN , ColorNoise ) \n"
Original    (009): ['BranchNode', ',', 'SkipNode', ',', 'LRN', ',', 'ColorNoise', ')', '\\n']
Tokenized   (018): ['<s>', 'B', 'ranch', 'Node', ',', 'Skip', 'Node', ',', 'LR', 'N', ',', 'Color', 'No', 'ise', ')', '\\', 'n', '</s>']
Filtered   (016): ['B', 'ranch', 'Node', ',', 'Skip', 'Node', ',', 'LR', 'N', ',', 'Color', 'No', 'ise', ')', '\\', 'n']
Detokenized (009): ['BranchNode', ',', 'SkipNode', ',', 'LRN', ',', 'ColorNoise', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "img_set_options = dict ( repo_dir = args . data_dir , \n"
Original    (011): ['img_set_options', '=', 'dict', '(', 'repo_dir', '=', 'args', '.', 'data_dir', ',', '\\n']
Tokenized   (022): ['<s>', 'img', '_', 'set', '_', 'options', '=', 'dict', '(', 'repo', '_', 'dir', '=', 'args', '.', 'data', '_', 'dir', ',', '\\', 'n', '</s>']
Filtered   (020): ['img', '_', 'set', '_', 'options', '=', 'dict', '(', 'repo', '_', 'dir', '=', 'args', '.', 'data', '_', 'dir', ',', '\\', 'n']
Detokenized (011): ['img_set_options', '=', 'dict', '(', 'repo_dir', '=', 'args', '.', 'data_dir', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "subset_pct = 0.09990891117239205 ) \n"
Original    (005): ['subset_pct', '=', '0.09990891117239205', ')', '\\n']
Tokenized   (020): ['<s>', 'sub', 'set', '_', 'p', 'ct', '=', '0', '.', '0', '999', '08', '911', '17', '239', '205', ')', '\\', 'n', '</s>']
Filtered   (018): ['sub', 'set', '_', 'p', 'ct', '=', '0', '.', '0', '999', '08', '911', '17', '239', '205', ')', '\\', 'n']
Detokenized (005): ['subset_pct', '=', '0.09990891117239205', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "do_transforms = False , ** img_set_options ) \n"
Original    (008): ['do_transforms', '=', 'False', ',', '**', 'img_set_options', ')', '\\n']
Tokenized   (018): ['<s>', 'do', '_', 'trans', 'forms', '=', 'False', ',', '**', 'img', '_', 'set', '_', 'options', ')', '\\', 'n', '</s>']
Filtered   (016): ['do', '_', 'trans', 'forms', '=', 'False', ',', '**', 'img', '_', 'set', '_', 'options', ')', '\\', 'n']
Detokenized (008): ['do_transforms', '=', 'False', ',', '**', 'img_set_options', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "test = ImageLoader ( set_name = , scale_range = ( 256 , 384 ) , shuffle = False , \n"
Original    (020): ['test', '=', 'ImageLoader', '(', 'set_name', '=', ',', 'scale_range', '=', '(', '256', ',', '384', ')', ',', 'shuffle', '=', 'False', ',', '\\n']
Tokenized   (028): ['<s>', 'test', '=', 'Image', 'Loader', '(', 'set', '_', 'name', '=', ',', 'scale', '_', 'range', '=', '(', '256', ',', '384', ')', ',', 'shuffle', '=', 'False', ',', '\\', 'n', '</s>']
Filtered   (026): ['test', '=', 'Image', 'Loader', '(', 'set', '_', 'name', '=', ',', 'scale', '_', 'range', '=', '(', '256', ',', '384', ')', ',', 'shuffle', '=', 'False', ',', '\\', 'n']
Detokenized (020): ['test', '=', 'ImageLoader', '(', 'set_name', '=', ',', 'scale_range', '=', '(', '256', ',', '384', ')', ',', 'shuffle', '=', 'False', ',', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "Pooling ( 3 , strides = 2 ) , \n"
Original    (010): ['Pooling', '(', '3', ',', 'strides', '=', '2', ')', ',', '\\n']
Tokenized   (014): ['<s>', 'Pool', 'ing', '(', '3', ',', 'strides', '=', '2', ')', ',', '\\', 'n', '</s>']
Filtered   (012): ['Pool', 'ing', '(', '3', ',', 'strides', '=', '2', ')', ',', '\\', 'n']
Detokenized (010): ['Pooling', '(', '3', ',', 'strides', '=', '2', ')', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "activation = Rectlin ( ) , padding = 1 ) , \n"
Original    (012): ['activation', '=', 'Rectlin', '(', ')', ',', 'padding', '=', '1', ')', ',', '\\n']
Tokenized   (016): ['<s>', 'activation', '=', 'Rect', 'lin', '(', ')', ',', 'padding', '=', '1', ')', ',', '\\', 'n', '</s>']
Filtered   (014): ['activation', '=', 'Rect', 'lin', '(', ')', ',', 'padding', '=', '1', ')', ',', '\\', 'n']
Detokenized (012): ['activation', '=', 'Rectlin', '(', ')', ',', 'padding', '=', '1', ')', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "Conv ( ( 3 , 3 , 256 ) , init = Gaussian ( scale = 0.03 ) , bias = Constant ( 1 ) , \n"
Original    (027): ['Conv', '(', '(', '3', ',', '3', ',', '256', ')', ',', 'init', '=', 'Gaussian', '(', 'scale', '=', '0.03', ')', ',', 'bias', '=', 'Constant', '(', '1', ')', ',', '\\n']
Tokenized   (034): ['<s>', 'Con', 'v', '(', '(', '3', ',', '3', ',', '256', ')', ',', 'init', '=', 'Ga', 'ussian', '(', 'scale', '=', '0', '.', '03', ')', ',', 'bias', '=', 'Constant', '(', '1', ')', ',', '\\', 'n', '</s>']
Filtered   (032): ['Con', 'v', '(', '(', '3', ',', '3', ',', '256', ')', ',', 'init', '=', 'Ga', 'ussian', '(', 'scale', '=', '0', '.', '03', ')', ',', 'bias', '=', 'Constant', '(', '1', ')', ',', '\\', 'n']
Detokenized (027): ['Conv', '(', '(', '3', ',', '3', ',', '256', ')', ',', 'init', '=', 'Gaussian', '(', 'scale', '=', '0.03', ')', ',', 'bias', '=', 'Constant', '(', '1', ')', ',', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "Dropout ( keep = 1.0 ) , \n"
Original    (008): ['Dropout', '(', 'keep', '=', '1.0', ')', ',', '\\n']
Tokenized   (014): ['<s>', 'Drop', 'out', '(', 'keep', '=', '1', '.', '0', ')', ',', '\\', 'n', '</s>']
Filtered   (012): ['Drop', 'out', '(', 'keep', '=', '1', '.', '0', ')', ',', '\\', 'n']
Detokenized (008): ['Dropout', '(', 'keep', '=', '1.0', ')', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "Affine ( nout = 1000 , init = Gaussian ( scale = 0.01 ) , bias = Constant ( - 7 ) , activation = Softmax ( ) ) ] \n"
Original    (031): ['Affine', '(', 'nout', '=', '1000', ',', 'init', '=', 'Gaussian', '(', 'scale', '=', '0.01', ')', ',', 'bias', '=', 'Constant', '(', '-', '7', ')', ',', 'activation', '=', 'Softmax', '(', ')', ')', ']', '\\n']
Tokenized   (040): ['<s>', 'Aff', 'ine', '(', 'n', 'out', '=', '1000', ',', 'init', '=', 'Ga', 'ussian', '(', 'scale', '=', '0', '.', '01', ')', ',', 'bias', '=', 'Constant', '(', '-', '7', ')', ',', 'activation', '=', 'Soft', 'max', '(', ')', ')', ']', '\\', 'n', '</s>']
Filtered   (038): ['Aff', 'ine', '(', 'n', 'out', '=', '1000', ',', 'init', '=', 'Ga', 'ussian', '(', 'scale', '=', '0', '.', '01', ')', ',', 'bias', '=', 'Constant', '(', '-', '7', ')', ',', 'activation', '=', 'Soft', 'max', '(', ')', ')', ']', '\\', 'n']
Detokenized (031): ['Affine', '(', 'nout', '=', '1000', ',', 'init', '=', 'Gaussian', '(', 'scale', '=', '0.01', ')', ',', 'bias', '=', 'Constant', '(', '-', '7', ')', ',', 'activation', '=', 'Softmax', '(', ')', ')', ']', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 31, 768)
# Extracted words:  31
Sentence         : "weight_sched = Schedule ( [ 22 , 44 , 65 ] , ( 1 / 250. ) ** ( 1 / 3. ) ) \n"
Original    (025): ['weight_sched', '=', 'Schedule', '(', '[', '22', ',', '44', ',', '65', ']', ',', '(', '1', '/', '250.', ')', '**', '(', '1', '/', '3.', ')', ')', '\\n']
Tokenized   (033): ['<s>', 'weight', '_', 'sc', 'hed', '=', 'Schedule', '(', '[', '22', ',', '44', ',', '65', ']', ',', '(', '1', '/', '250', '.', ')', '**', '(', '1', '/', '3', '.', ')', ')', '\\', 'n', '</s>']
Filtered   (031): ['weight', '_', 'sc', 'hed', '=', 'Schedule', '(', '[', '22', ',', '44', ',', '65', ']', ',', '(', '1', '/', '250', '.', ')', '**', '(', '1', '/', '3', '.', ')', ')', '\\', 'n']
Detokenized (025): ['weight_sched', '=', 'Schedule', '(', '[', '22', ',', '44', ',', '65', ']', ',', '(', '1', '/', '250.', ')', '**', '(', '1', '/', '3.', ')', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "opt_gdm = GradientDescentMomentum ( 0.01 / 10 , 0.9 , wdecay = 0.0005 , schedule = weight_sched , \n"
Original    (019): ['opt_gdm', '=', 'GradientDescentMomentum', '(', '0.01', '/', '10', ',', '0.9', ',', 'wdecay', '=', '0.0005', ',', 'schedule', '=', 'weight_sched', ',', '\\n']
Tokenized   (043): ['<s>', 'opt', '_', 'gd', 'm', '=', 'Grad', 'ient', 'Des', 'cent', 'Mom', 'ent', 'um', '(', '0', '.', '01', '/', '10', ',', '0', '.', '9', ',', 'w', 'dec', 'ay', '=', '0', '.', '000', '5', ',', 'schedule', '=', 'weight', '_', 'sc', 'hed', ',', '\\', 'n', '</s>']
Filtered   (041): ['opt', '_', 'gd', 'm', '=', 'Grad', 'ient', 'Des', 'cent', 'Mom', 'ent', 'um', '(', '0', '.', '01', '/', '10', ',', '0', '.', '9', ',', 'w', 'dec', 'ay', '=', '0', '.', '000', '5', ',', 'schedule', '=', 'weight', '_', 'sc', 'hed', ',', '\\', 'n']
Detokenized (019): ['opt_gdm', '=', 'GradientDescentMomentum', '(', '0.01', '/', '10', ',', '0.9', ',', 'wdecay', '=', '0.0005', ',', 'schedule', '=', 'weight_sched', ',', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "opt_biases = GradientDescentMomentum ( 0.02 / 10 , 0.9 , schedule = Schedule ( [ 44 ] , 0.1 ) , \n"
Original    (022): ['opt_biases', '=', 'GradientDescentMomentum', '(', '0.02', '/', '10', ',', '0.9', ',', 'schedule', '=', 'Schedule', '(', '[', '44', ']', ',', '0.1', ')', ',', '\\n']
Tokenized   (040): ['<s>', 'opt', '_', 'bi', 'ases', '=', 'Grad', 'ient', 'Des', 'cent', 'Mom', 'ent', 'um', '(', '0', '.', '02', '/', '10', ',', '0', '.', '9', ',', 'schedule', '=', 'Schedule', '(', '[', '44', ']', ',', '0', '.', '1', ')', ',', '\\', 'n', '</s>']
Filtered   (038): ['opt', '_', 'bi', 'ases', '=', 'Grad', 'ient', 'Des', 'cent', 'Mom', 'ent', 'um', '(', '0', '.', '02', '/', '10', ',', '0', '.', '9', ',', 'schedule', '=', 'Schedule', '(', '[', '44', ']', ',', '0', '.', '1', ')', ',', '\\', 'n']
Detokenized (022): ['opt_biases', '=', 'GradientDescentMomentum', '(', '0.02', '/', '10', ',', '0.9', ',', 'schedule', '=', 'Schedule', '(', '[', '44', ']', ',', '0.1', ')', ',', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "valmetric = TopKMisclassification ( k = 5 ) \n"
Original    (009): ['valmetric', '=', 'TopKMisclassification', '(', 'k', '=', '5', ')', '\\n']
Tokenized   (018): ['<s>', 'val', 'met', 'ric', '=', 'Top', 'K', 'Mis', 'class', 'ification', '(', 'k', '=', '5', ')', '\\', 'n', '</s>']
Filtered   (016): ['val', 'met', 'ric', '=', 'Top', 'K', 'Mis', 'class', 'ification', '(', 'k', '=', '5', ')', '\\', 'n']
Detokenized (009): ['valmetric', '=', 'TopKMisclassification', '(', 'k', '=', '5', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "nifm_rng = [ 8 ] \n"
Original    (006): ['nifm_rng', '=', '[', '8', ']', '\\n']
Tokenized   (014): ['<s>', 'n', 'if', 'm', '_', 'r', 'ng', '=', '[', '8', ']', '\\', 'n', '</s>']
Filtered   (012): ['n', 'if', 'm', '_', 'r', 'ng', '=', '[', '8', ']', '\\', 'n']
Detokenized (006): ['nifm_rng', '=', '[', '8', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "fargs_ . append ( itt . product ( fs_rng , nifm_rng , pad_rng , stride_rng , in_sz_rng , bsz_rng ) ) \n"
Original    (022): ['fargs_', '.', 'append', '(', 'itt', '.', 'product', '(', 'fs_rng', ',', 'nifm_rng', ',', 'pad_rng', ',', 'stride_rng', ',', 'in_sz_rng', ',', 'bsz_rng', ')', ')', '\\n']
Tokenized   (053): ['<s>', 'f', 'args', '_', '.', 'append', '(', 'it', 't', '.', 'product', '(', 'fs', '_', 'r', 'ng', ',', 'n', 'if', 'm', '_', 'r', 'ng', ',', 'pad', '_', 'r', 'ng', ',', 'stride', '_', 'r', 'ng', ',', 'in', '_', 's', 'z', '_', 'r', 'ng', ',', 'b', 's', 'z', '_', 'r', 'ng', ')', ')', '\\', 'n', '</s>']
Filtered   (051): ['f', 'args', '_', '.', 'append', '(', 'it', 't', '.', 'product', '(', 'fs', '_', 'r', 'ng', ',', 'n', 'if', 'm', '_', 'r', 'ng', ',', 'pad', '_', 'r', 'ng', ',', 'stride', '_', 'r', 'ng', ',', 'in', '_', 's', 'z', '_', 'r', 'ng', ',', 'b', 's', 'z', '_', 'r', 'ng', ')', ')', '\\', 'n']
Detokenized (022): ['fargs_', '.', 'append', '(', 'itt', '.', 'product', '(', 'fs_rng', ',', 'nifm_rng', ',', 'pad_rng', ',', 'stride_rng', ',', 'in_sz_rng', ',', 'bsz_rng', ')', ')', '\\n']
Counter: 51
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "bsz = inp . shape [ - 1 ] \n"
Original    (010): ['bsz', '=', 'inp', '.', 'shape', '[', '-', '1', ']', '\\n']
Tokenized   (015): ['<s>', 'bs', 'z', '=', 'in', 'p', '.', 'shape', '[', '-', '1', ']', '\\', 'n', '</s>']
Filtered   (013): ['bs', 'z', '=', 'in', 'p', '.', 'shape', '[', '-', '1', ']', '\\', 'n']
Detokenized (010): ['bsz', '=', 'inp', '.', 'shape', '[', '-', '1', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "check_inds = check_inds [ 0 : ncheck ] \n"
Original    (009): ['check_inds', '=', 'check_inds', '[', '0', ':', 'ncheck', ']', '\\n']
Tokenized   (019): ['<s>', 'check', '_', 'ind', 's', '=', 'check', '_', 'ind', 's', '[', '0', ':', 'n', 'check', ']', '\\', 'n', '</s>']
Filtered   (017): ['check', '_', 'ind', 's', '=', 'check', '_', 'ind', 's', '[', '0', ':', 'n', 'check', ']', '\\', 'n']
Detokenized (009): ['check_inds', '=', 'check_inds', '[', '0', ':', 'ncheck', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "inpa = inp . get ( ) . reshape ( inp_lshape ) \n"
Original    (013): ['inpa', '=', 'inp', '.', 'get', '(', ')', '.', 'reshape', '(', 'inp_lshape', ')', '\\n']
Tokenized   (023): ['<s>', 'in', 'pa', '=', 'in', 'p', '.', 'get', '(', ')', '.', 'resh', 'ape', '(', 'in', 'p', '_', 'l', 'shape', ')', '\\', 'n', '</s>']
Filtered   (021): ['in', 'pa', '=', 'in', 'p', '.', 'get', '(', ')', '.', 'resh', 'ape', '(', 'in', 'p', '_', 'l', 'shape', ')', '\\', 'n']
Detokenized (013): ['inpa', '=', 'inp', '.', 'get', '(', ')', '.', 'reshape', '(', 'inp_lshape', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "outshape = ( inp_lshape [ 0 ] , \n"
Original    (009): ['outshape', '=', '(', 'inp_lshape', '[', '0', ']', ',', '\\n']
Tokenized   (017): ['<s>', 'out', 'shape', '=', '(', 'in', 'p', '_', 'l', 'shape', '[', '0', ']', ',', '\\', 'n', '</s>']
Filtered   (015): ['out', 'shape', '=', '(', 'in', 'p', '_', 'l', 'shape', '[', '0', ']', ',', '\\', 'n']
Detokenized (009): ['outshape', '=', '(', 'inp_lshape', '[', '0', ']', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "be . output_dim ( inp_lshape [ 2 ] , fshape [ 1 ] , padding , strides [ 1 ] , pooling = True ) , \n"
Original    (027): ['be', '.', 'output_dim', '(', 'inp_lshape', '[', '2', ']', ',', 'fshape', '[', '1', ']', ',', 'padding', ',', 'strides', '[', '1', ']', ',', 'pooling', '=', 'True', ')', ',', '\\n']
Tokenized   (038): ['<s>', 'be', '.', 'output', '_', 'dim', '(', 'in', 'p', '_', 'l', 'shape', '[', '2', ']', ',', 'f', 'shape', '[', '1', ']', ',', 'padding', ',', 'strides', '[', '1', ']', ',', 'pool', 'ing', '=', 'True', ')', ',', '\\', 'n', '</s>']
Filtered   (036): ['be', '.', 'output', '_', 'dim', '(', 'in', 'p', '_', 'l', 'shape', '[', '2', ']', ',', 'f', 'shape', '[', '1', ']', ',', 'padding', ',', 'strides', '[', '1', ']', ',', 'pool', 'ing', '=', 'True', ')', ',', '\\', 'n']
Detokenized (027): ['be', '.', 'output_dim', '(', 'inp_lshape', '[', '2', ']', ',', 'fshape', '[', '1', ']', ',', 'padding', ',', 'strides', '[', '1', ']', ',', 'pooling', '=', 'True', ')', ',', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "inp_lshape [ - 1 ] ) \n"
Original    (007): ['inp_lshape', '[', '-', '1', ']', ')', '\\n']
Tokenized   (014): ['<s>', 'in', 'p', '_', 'l', 'shape', '[', '-', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (012): ['in', 'p', '_', 'l', 'shape', '[', '-', '1', ']', ')', '\\', 'n']
Detokenized (007): ['inp_lshape', '[', '-', '1', ']', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "inp_pad [ : , padding : - padding , padding : - padding , : ] = inpa [ : , 0 : , 0 : , : ] \n"
Original    (030): ['inp_pad', '[', ':', ',', 'padding', ':', '-', 'padding', ',', 'padding', ':', '-', 'padding', ',', ':', ']', '=', 'inpa', '[', ':', ',', '0', ':', ',', '0', ':', ',', ':', ']', '\\n']
Tokenized   (037): ['<s>', 'in', 'p', '_', 'pad', '[', ':', ',', 'padding', ':', '-', 'padding', ',', 'padding', ':', '-', 'padding', ',', ':', ']', '=', 'in', 'pa', '[', ':', ',', '0', ':', ',', '0', ':', ',', ':', ']', '\\', 'n', '</s>']
Filtered   (035): ['in', 'p', '_', 'pad', '[', ':', ',', 'padding', ':', '-', 'padding', ',', 'padding', ':', '-', 'padding', ',', ':', ']', '=', 'in', 'pa', '[', ':', ',', '0', ':', ',', '0', ':', ',', ':', ']', '\\', 'n']
Detokenized (030): ['inp_pad', '[', ':', ',', 'padding', ':', '-', 'padding', ',', 'padding', ':', '-', 'padding', ',', ':', ']', '=', 'inpa', '[', ':', ',', '0', ':', ',', '0', ':', ',', ':', ']', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 30, 768)
# Extracted words:  30
Sentence         : "out_exp [ indC , indh , indw , cnt ] = np . max ( inp_check ) \n"
Original    (018): ['out_exp', '[', 'indC', ',', 'indh', ',', 'indw', ',', 'cnt', ']', '=', 'np', '.', 'max', '(', 'inp_check', ')', '\\n']
Tokenized   (030): ['<s>', 'out', '_', 'exp', '[', 'ind', 'C', ',', 'ind', 'h', ',', 'ind', 'w', ',', 'c', 'nt', ']', '=', 'np', '.', 'max', '(', 'in', 'p', '_', 'check', ')', '\\', 'n', '</s>']
Filtered   (028): ['out', '_', 'exp', '[', 'ind', 'C', ',', 'ind', 'h', ',', 'ind', 'w', ',', 'c', 'nt', ']', '=', 'np', '.', 'max', '(', 'in', 'p', '_', 'check', ')', '\\', 'n']
Detokenized (018): ['out_exp', '[', 'indC', ',', 'indh', ',', 'indw', ',', 'cnt', ']', '=', 'np', '.', 'max', '(', 'inp_check', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "NervanaObject . be . bsz = batch_size \n"
Original    (008): ['NervanaObject', '.', 'be', '.', 'bsz', '=', 'batch_size', '\\n']
Tokenized   (018): ['<s>', 'N', 'erv', 'ana', 'Object', '.', 'be', '.', 'b', 's', 'z', '=', 'batch', '_', 'size', '\\', 'n', '</s>']
Filtered   (016): ['N', 'erv', 'ana', 'Object', '.', 'be', '.', 'b', 's', 'z', '=', 'batch', '_', 'size', '\\', 'n']
Detokenized (008): ['NervanaObject', '.', 'be', '.', 'bsz', '=', 'batch_size', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "inshape = ( nifm , in_sz , in_sz ) \n"
Original    (010): ['inshape', '=', '(', 'nifm', ',', 'in_sz', ',', 'in_sz', ')', '\\n']
Tokenized   (023): ['<s>', 'ins', 'h', 'ape', '=', '(', 'n', 'if', 'm', ',', 'in', '_', 's', 'z', ',', 'in', '_', 's', 'z', ')', '\\', 'n', '</s>']
Filtered   (021): ['ins', 'h', 'ape', '=', '(', 'n', 'if', 'm', ',', 'in', '_', 's', 'z', ',', 'in', '_', 's', 'z', ')', '\\', 'n']
Detokenized (010): ['inshape', '=', '(', 'nifm', ',', 'in_sz', ',', 'in_sz', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "src = "img/file-icon.jpg" , ** kw ) ] \n"
Original    (009): ['src', '=', '"img/file-icon.jpg"', ',', '**', 'kw', ')', ']', '\\n']
Tokenized   (021): ['<s>', 'src', '=', '"', 'img', '/', 'file', '-', 'icon', '.', 'jpg', '"', ',', '**', 'k', 'w', ')', ']', '\\', 'n', '</s>']
Filtered   (019): ['src', '=', '"', 'img', '/', 'file', '-', 'icon', '.', 'jpg', '"', ',', '**', 'k', 'w', ')', ']', '\\', 'n']
Detokenized (009): ['src', '=', '"img/file-icon.jpg"', ',', '**', 'kw', ')', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "render = lambda r : r . div ( comp . render ( r , model = None ) , r . script ( component_to_update = + self . comp_id , \n"
Original    (032): ['render', '=', 'lambda', 'r', ':', 'r', '.', 'div', '(', 'comp', '.', 'render', '(', 'r', ',', 'model', '=', 'None', ')', ',', 'r', '.', 'script', '(', 'component_to_update', '=', '+', 'self', '.', 'comp_id', ',', '\\n']
Tokenized   (041): ['<s>', 'render', '=', 'lambda', 'r', ':', 'r', '.', 'div', '(', 'comp', '.', 'render', '(', 'r', ',', 'model', '=', 'None', ')', ',', 'r', '.', 'script', '(', 'component', '_', 'to', '_', 'update', '=', '+', 'self', '.', 'comp', '_', 'id', ',', '\\', 'n', '</s>']
Filtered   (039): ['render', '=', 'lambda', 'r', ':', 'r', '.', 'div', '(', 'comp', '.', 'render', '(', 'r', ',', 'model', '=', 'None', ')', ',', 'r', '.', 'script', '(', 'component', '_', 'to', '_', 'update', '=', '+', 'self', '.', 'comp', '_', 'id', ',', '\\', 'n']
Detokenized (032): ['render', '=', 'lambda', 'r', ':', 'r', '.', 'div', '(', 'comp', '.', 'render', '(', 'r', ',', 'model', '=', 'None', ')', ',', 'r', '.', 'script', '(', 'component_to_update', '=', '+', 'self', '.', 'comp_id', ',', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "ajax . py2js ( self . crop_height ( ) ) \n"
Original    (011): ['ajax', '.', 'py2js', '(', 'self', '.', 'crop_height', '(', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'aj', 'ax', '.', 'py', '2', 'js', '(', 'self', '.', 'crop', '_', 'height', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['aj', 'ax', '.', 'py', '2', 'js', '(', 'self', '.', 'crop', '_', 'height', '(', ')', ')', '\\', 'n']
Detokenized (011): ['ajax', '.', 'py2js', '(', 'self', '.', 'crop_height', '(', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "local_handler = getattr ( self , , None ) \n"
Original    (010): ['local_handler', '=', 'getattr', '(', 'self', ',', ',', 'None', ')', '\\n']
Tokenized   (016): ['<s>', 'local', '_', 'handler', '=', 'get', 'attr', '(', 'self', ',', ',', 'None', ')', '\\', 'n', '</s>']
Filtered   (014): ['local', '_', 'handler', '=', 'get', 'attr', '(', 'self', ',', ',', 'None', ')', '\\', 'n']
Detokenized (010): ['local_handler', '=', 'getattr', '(', 'self', ',', ',', 'None', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "genie2 . client . wrapper . RetryPolicy ( \n"
Original    (009): ['genie2', '.', 'client', '.', 'wrapper', '.', 'RetryPolicy', '(', '\\n']
Tokenized   (016): ['<s>', 'gen', 'ie', '2', '.', 'client', '.', 'wrapper', '.', 'Ret', 'ry', 'Policy', '(', '\\', 'n', '</s>']
Filtered   (014): ['gen', 'ie', '2', '.', 'client', '.', 'wrapper', '.', 'Ret', 'ry', 'Policy', '(', '\\', 'n']
Detokenized (009): ['genie2', '.', 'client', '.', 'wrapper', '.', 'RetryPolicy', '(', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "tries = 8 , none_on_404 = True , no_retry_http_codes = range ( 400 , 500 ) ) \n"
Original    (018): ['tries', '=', '8', ',', 'none_on_404', '=', 'True', ',', 'no_retry_http_codes', '=', 'range', '(', '400', ',', '500', ')', ')', '\\n']
Tokenized   (033): ['<s>', 't', 'ries', '=', '8', ',', 'none', '_', 'on', '_', '404', '=', 'True', ',', 'no', '_', 'ret', 'ry', '_', 'http', '_', 'codes', '=', 'range', '(', '400', ',', '500', ')', ')', '\\', 'n', '</s>']
Filtered   (031): ['t', 'ries', '=', '8', ',', 'none', '_', 'on', '_', '404', '=', 'True', ',', 'no', '_', 'ret', 'ry', '_', 'http', '_', 'codes', '=', 'range', '(', '400', ',', '500', ')', ')', '\\', 'n']
Detokenized (018): ['tries', '=', '8', ',', 'none_on_404', '=', 'True', ',', 'no_retry_http_codes', '=', 'range', '(', '400', ',', '500', ')', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "tagging . add_argument ( , , dest = , action = conf_action ( context . ami ) , help = \n"
Original    (021): ['tagging', '.', 'add_argument', '(', ',', ',', 'dest', '=', ',', 'action', '=', 'conf_action', '(', 'context', '.', 'ami', ')', ',', 'help', '=', '\\n']
Tokenized   (030): ['<s>', 'tag', 'ging', '.', 'add', '_', 'argument', '(', ',', ',', 'dest', '=', ',', 'action', '=', 'conf', '_', 'action', '(', 'context', '.', 'am', 'i', ')', ',', 'help', '=', '\\', 'n', '</s>']
Filtered   (028): ['tag', 'ging', '.', 'add', '_', 'argument', '(', ',', ',', 'dest', '=', ',', 'action', '=', 'conf', '_', 'action', '(', 'context', '.', 'am', 'i', ')', ',', 'help', '=', '\\', 'n']
Detokenized (021): ['tagging', '.', 'add_argument', '(', ',', ',', 'dest', '=', ',', 'action', '=', 'conf_action', '(', 'context', '.', 'ami', ')', ',', 'help', '=', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "BASE_URL = . format ( FQDN ) \n"
Original    (008): ['BASE_URL', '=', '.', 'format', '(', 'FQDN', ')', '\\n']
Tokenized   (016): ['<s>', 'B', 'ASE', '_', 'URL', '=', '.', 'format', '(', 'F', 'Q', 'DN', ')', '\\', 'n', '</s>']
Filtered   (014): ['B', 'ASE', '_', 'URL', '=', '.', 'format', '(', 'F', 'Q', 'DN', ')', '\\', 'n']
Detokenized (008): ['BASE_URL', '=', '.', 'format', '(', 'FQDN', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test" , config ElasticSearchServiceItem ( region = "us-west-2" , account = "TEST_ACCOUNT" , name = "es_test_2" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_3" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_4" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_5" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_6" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_7" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_8" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_9" , config ] \n"
Original    (137): ['ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-west-2"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_2"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_3"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_4"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_5"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_6"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_7"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_8"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_9"', ',', 'config', ']', '\\n']
Tokenized   (328): ['<s>', 'El', 'astic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'west', '-', '2', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '2', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '3', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '4', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '5', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '6', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '7', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '8', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '9', '"', ',', 'config', ']', '\\', 'n', '</s>']
Filtered   (326): ['El', 'astic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'west', '-', '2', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '2', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '3', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '4', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '5', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '6', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '7', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '8', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '9', '"', ',', 'config', ']', '\\', 'n']
Detokenized (137): ['ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-west-2"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_2"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_3"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_4"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_5"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_6"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_7"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_8"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_9"', ',', 'config', ']', '\\n']
Counter: 326
===================================================================
Hidden states:  (13, 137, 768)
# Extracted words:  137
Sentence         : "test_account . role_name = "TEST_ACCOUNT" \n"
Original    (006): ['test_account', '.', 'role_name', '=', '"TEST_ACCOUNT"', '\\n']
Tokenized   (019): ['<s>', 'test', '_', 'account', '.', 'role', '_', 'name', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', '\\', 'n', '</s>']
Filtered   (017): ['test', '_', 'account', '.', 'role', '_', 'name', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', '\\', 'n']
Detokenized (006): ['test_account', '.', 'role_name', '=', '"TEST_ACCOUNT"', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "all_clusters . extend ( response [ ] [ if response [ ] [ ] [ ] ~~~ marker = response [ ] [ ] [ ~~ else : \n"
Original    (029): ['all_clusters', '.', 'extend', '(', 'response', '[', ']', '[', 'if', 'response', '[', ']', '[', ']', '[', ']', '~~~', 'marker', '=', 'response', '[', ']', '[', ']', '[', '~~', 'else', ':', '\\n']
Tokenized   (038): ['<s>', 'all', '_', 'cl', 'usters', '.', 'extend', '(', 'response', '[', ']', '[', 'if', 'response', '[', ']', '[', ']', '[', ']', '', '~~', '~', 'marker', '=', 'response', '[', ']', '[', ']', '[', '', '~~', 'else', ':', '\\', 'n', '</s>']
Filtered   (036): ['all', '_', 'cl', 'usters', '.', 'extend', '(', 'response', '[', ']', '[', 'if', 'response', '[', ']', '[', ']', '[', ']', '', '~~', '~', 'marker', '=', 'response', '[', ']', '[', ']', '[', '', '~~', 'else', ':', '\\', 'n']
Detokenized (029): ['all_clusters', '.', 'extend', '(', 'response', '[', ']', '[', 'if', 'response', '[', ']', '[', ']', '[', ']', '~~~', 'marker', '=', 'response', '[', ']', '[', ']', '[', '~~', 'else', ':', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "_container_child_objects = ( , ) \n"
Original    (006): ['_container_child_objects', '=', '(', ',', ')', '\\n']
Tokenized   (014): ['<s>', '_', 'container', '_', 'child', '_', 'objects', '=', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (012): ['_', 'container', '_', 'child', '_', 'objects', '=', '(', ',', ')', '\\', 'n']
Detokenized (006): ['_container_child_objects', '=', '(', ',', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "_recommended_attrs = ( ( ( , np . ndarray , 1 , np . dtype ( ) ) , \n"
Original    (020): ['_recommended_attrs', '=', '(', '(', '(', ',', 'np', '.', 'ndarray', ',', '1', ',', 'np', '.', 'dtype', '(', ')', ')', ',', '\\n']
Tokenized   (031): ['<s>', '_', 'recomm', 'ended', '_', 'att', 'rs', '=', '(', '(', '(', ',', 'np', '.', 'n', 'd', 'array', ',', '1', ',', 'np', '.', 'd', 'type', '(', ')', ')', ',', '\\', 'n', '</s>']
Filtered   (029): ['_', 'recomm', 'ended', '_', 'att', 'rs', '=', '(', '(', '(', ',', 'np', '.', 'n', 'd', 'array', ',', '1', ',', 'np', '.', 'd', 'type', '(', ')', ')', ',', '\\', 'n']
Detokenized (020): ['_recommended_attrs', '=', '(', '(', '(', ',', 'np', '.', 'ndarray', ',', '1', ',', 'np', '.', 'dtype', '(', ')', ')', ',', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "errstr = . format ( errno , pszMsgBuffer . value ) \n"
Original    (012): ['errstr', '=', '.', 'format', '(', 'errno', ',', 'pszMsgBuffer', '.', 'value', ')', '\\n']
Tokenized   (020): ['<s>', 'err', 'str', '=', '.', 'format', '(', 'err', 'no', ',', 'ps', 'z', 'Msg', 'Buffer', '.', 'value', ')', '\\', 'n', '</s>']
Filtered   (018): ['err', 'str', '=', '.', 'format', '(', 'err', 'no', ',', 'ps', 'z', 'Msg', 'Buffer', '.', 'value', ')', '\\', 'n']
Detokenized (012): ['errstr', '=', '.', 'format', '(', 'errno', ',', 'pszMsgBuffer', '.', 'value', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "supported_objects = [ Segment , AnalogSignal , EventArray , SpikeTrain ] \n"
Original    (012): ['supported_objects', '=', '[', 'Segment', ',', 'AnalogSignal', ',', 'EventArray', ',', 'SpikeTrain', ']', '\\n']
Tokenized   (022): ['<s>', 'supported', '_', 'objects', '=', '[', 'Se', 'gment', ',', 'Analog', 'Sign', 'al', ',', 'Event', 'Array', ',', 'Spike', 'Train', ']', '\\', 'n', '</s>']
Filtered   (020): ['supported', '_', 'objects', '=', '[', 'Se', 'gment', ',', 'Analog', 'Sign', 'al', ',', 'Event', 'Array', ',', 'Spike', 'Train', ']', '\\', 'n']
Detokenized (012): ['supported_objects', '=', '[', 'Segment', ',', 'AnalogSignal', ',', 'EventArray', ',', 'SpikeTrain', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "readable_objects = [ Segment ] \n"
Original    (006): ['readable_objects', '=', '[', 'Segment', ']', '\\n']
Tokenized   (012): ['<s>', 'readable', '_', 'objects', '=', '[', 'Se', 'gment', ']', '\\', 'n', '</s>']
Filtered   (010): ['readable', '_', 'objects', '=', '[', 'Se', 'gment', ']', '\\', 'n']
Detokenized (006): ['readable_objects', '=', '[', 'Segment', ']', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "read_params = { Segment : [ ] } \n"
Original    (009): ['read_params', '=', '{', 'Segment', ':', '[', ']', '}', '\\n']
Tokenized   (015): ['<s>', 'read', '_', 'params', '=', '{', 'Se', 'gment', ':', '[', ']', '}', '\\', 'n', '</s>']
Filtered   (013): ['read', '_', 'params', '=', '{', 'Se', 'gment', ':', '[', ']', '}', '\\', 'n']
Detokenized (009): ['read_params', '=', '{', 'Segment', ':', '[', ']', '}', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "labels . append ( str ( pData . value ) ) \n"
Original    (012): ['labels', '.', 'append', '(', 'str', '(', 'pData', '.', 'value', ')', ')', '\\n']
Tokenized   (017): ['<s>', 'lab', 'els', '.', 'append', '(', 'str', '(', 'p', 'Data', '.', 'value', ')', ')', '\\', 'n', '</s>']
Filtered   (015): ['lab', 'els', '.', 'append', '(', 'str', '(', 'p', 'Data', '.', 'value', ')', ')', '\\', 'n']
Detokenized (012): ['labels', '.', 'append', '(', 'str', '(', 'pData', '.', 'value', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "ea . labels = np . array ( labels , dtype = ) \n"
Original    (014): ['ea', '.', 'labels', '=', 'np', '.', 'array', '(', 'labels', ',', 'dtype', '=', ')', '\\n']
Tokenized   (018): ['<s>', 'ea', '.', 'labels', '=', 'np', '.', 'array', '(', 'labels', ',', 'd', 'type', '=', ')', '\\', 'n', '</s>']
Filtered   (016): ['ea', '.', 'labels', '=', 'np', '.', 'array', '(', 'labels', ',', 'd', 'type', '=', ')', '\\', 'n']
Detokenized (014): ['ea', '.', 'labels', '=', 'np', '.', 'array', '(', 'labels', ',', 'dtype', '=', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "dwStopIndex , ctypes . byref ( pdwContCount ) , pData [ total_read : ] . ctypes total_read += pdwContCount . value \n"
Original    (022): ['dwStopIndex', ',', 'ctypes', '.', 'byref', '(', 'pdwContCount', ')', ',', 'pData', '[', 'total_read', ':', ']', '.', 'ctypes', 'total_read', '+=', 'pdwContCount', '.', 'value', '\\n']
Tokenized   (044): ['<s>', 'd', 'w', 'Stop', 'Index', ',', 'c', 'types', '.', 'by', 'ref', '(', 'p', 'd', 'w', 'Cont', 'Count', ')', ',', 'p', 'Data', '[', 'total', '_', 'read', ':', ']', '.', 'c', 'types', 'total', '_', 'read', '+=', 'p', 'd', 'w', 'Cont', 'Count', '.', 'value', '\\', 'n', '</s>']
Filtered   (042): ['d', 'w', 'Stop', 'Index', ',', 'c', 'types', '.', 'by', 'ref', '(', 'p', 'd', 'w', 'Cont', 'Count', ')', ',', 'p', 'Data', '[', 'total', '_', 'read', ':', ']', '.', 'c', 'types', 'total', '_', 'read', '+=', 'p', 'd', 'w', 'Cont', 'Count', '.', 'value', '\\', 'n']
Detokenized (022): ['dwStopIndex', ',', 'ctypes', '.', 'byref', '(', 'pdwContCount', ')', ',', 'pData', '[', 'total_read', ':', ']', '.', 'ctypes', 'total_read', '+=', 'pdwContCount', '.', 'value', '\\n']
Counter: 42
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "anaSig . annotate ( probe_info = str ( pAnalogInfo . szProbeInfo ) ) \n"
Original    (014): ['anaSig', '.', 'annotate', '(', 'probe_info', '=', 'str', '(', 'pAnalogInfo', '.', 'szProbeInfo', ')', ')', '\\n']
Tokenized   (029): ['<s>', 'ana', 'S', 'ig', '.', 'annot', 'ate', '(', 'probe', '_', 'info', '=', 'str', '(', 'p', 'An', 'alog', 'Info', '.', 's', 'z', 'Pro', 'be', 'Info', ')', ')', '\\', 'n', '</s>']
Filtered   (027): ['ana', 'S', 'ig', '.', 'annot', 'ate', '(', 'probe', '_', 'info', '=', 'str', '(', 'p', 'An', 'alog', 'Info', '.', 's', 'z', 'Pro', 'be', 'Info', ')', ')', '\\', 'n']
Detokenized (014): ['anaSig', '.', 'annotate', '(', 'probe_info', '=', 'str', '(', 'pAnalogInfo', '.', 'szProbeInfo', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "pData = np . zeros ( ( dwDataBufferSize ) , dtype = ) \n"
Original    (014): ['pData', '=', 'np', '.', 'zeros', '(', '(', 'dwDataBufferSize', ')', ',', 'dtype', '=', ')', '\\n']
Tokenized   (023): ['<s>', 'p', 'Data', '=', 'np', '.', 'z', 'eros', '(', '(', 'dw', 'Data', 'Buffer', 'Size', ')', ',', 'd', 'type', '=', ')', '\\', 'n', '</s>']
Filtered   (021): ['p', 'Data', '=', 'np', '.', 'z', 'eros', '(', '(', 'dw', 'Data', 'Buffer', 'Size', ')', ',', 'd', 'type', '=', ')', '\\', 'n']
Detokenized (014): ['pData', '=', 'np', '.', 'zeros', '(', '(', 'dwDataBufferSize', ')', ',', 'dtype', '=', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "waveforms = pq . Quantity ( waveforms , units = str ( pdwSegmentInfo left_sweep = nsample / 2. / float ( pdwSegmentInfo . dSampleRate ) * pq sampling_rate = float ( pdwSegmentInfo . dSampleRate ) * pq . Hz , \n"
Original    (041): ['waveforms', '=', 'pq', '.', 'Quantity', '(', 'waveforms', ',', 'units', '=', 'str', '(', 'pdwSegmentInfo', 'left_sweep', '=', 'nsample', '/', '2.', '/', 'float', '(', 'pdwSegmentInfo', '.', 'dSampleRate', ')', '*', 'pq', 'sampling_rate', '=', 'float', '(', 'pdwSegmentInfo', '.', 'dSampleRate', ')', '*', 'pq', '.', 'Hz', ',', '\\n']
Tokenized   (075): ['<s>', 'wave', 'forms', '=', 'p', 'q', '.', 'Quantity', '(', 'wave', 'forms', ',', 'units', '=', 'str', '(', 'p', 'd', 'w', 'Seg', 'ment', 'Info', 'left', '_', 'swe', 'ep', '=', 'ns', 'ample', '/', '2', '.', '/', 'float', '(', 'p', 'd', 'w', 'Seg', 'ment', 'Info', '.', 'd', 'Sample', 'Rate', ')', '*', 'p', 'q', 'sampling', '_', 'rate', '=', 'float', '(', 'p', 'd', 'w', 'Seg', 'ment', 'Info', '.', 'd', 'Sample', 'Rate', ')', '*', 'p', 'q', '.', 'Hz', ',', '\\', 'n', '</s>']
Filtered   (073): ['wave', 'forms', '=', 'p', 'q', '.', 'Quantity', '(', 'wave', 'forms', ',', 'units', '=', 'str', '(', 'p', 'd', 'w', 'Seg', 'ment', 'Info', 'left', '_', 'swe', 'ep', '=', 'ns', 'ample', '/', '2', '.', '/', 'float', '(', 'p', 'd', 'w', 'Seg', 'ment', 'Info', '.', 'd', 'Sample', 'Rate', ')', '*', 'p', 'q', 'sampling', '_', 'rate', '=', 'float', '(', 'p', 'd', 'w', 'Seg', 'ment', 'Info', '.', 'd', 'Sample', 'Rate', ')', '*', 'p', 'q', '.', 'Hz', ',', '\\', 'n']
Detokenized (041): ['waveforms', '=', 'pq', '.', 'Quantity', '(', 'waveforms', ',', 'units', '=', 'str', '(', 'pdwSegmentInfo', 'left_sweep', '=', 'nsample', '/', '2.', '/', 'float', '(', 'pdwSegmentInfo', '.', 'dSampleRate', ')', '*', 'pq', 'sampling_rate', '=', 'float', '(', 'pdwSegmentInfo', '.', 'dSampleRate', ')', '*', 'pq', '.', 'Hz', ',', '\\n']
Counter: 73
===================================================================
Hidden states:  (13, 41, 768)
# Extracted words:  41
Sentence         : "ctypes . byref ( pNeuralInfo ) , ctypes . sizeof ( pNeuralInfo ) ) \n"
Original    (015): ['ctypes', '.', 'byref', '(', 'pNeuralInfo', ')', ',', 'ctypes', '.', 'sizeof', '(', 'pNeuralInfo', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'ct', 'ypes', '.', 'by', 'ref', '(', 'p', 'Ne', 'ural', 'Info', ')', ',', 'c', 'types', '.', 'sizeof', '(', 'p', 'Ne', 'ural', 'Info', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['ct', 'ypes', '.', 'by', 'ref', '(', 'p', 'Ne', 'ural', 'Info', ')', ',', 'c', 'types', '.', 'sizeof', '(', 'p', 'Ne', 'ural', 'Info', ')', ')', '\\', 'n']
Detokenized (015): ['ctypes', '.', 'byref', '(', 'pNeuralInfo', ')', ',', 'ctypes', '.', 'sizeof', '(', 'pNeuralInfo', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "neuroshare . ns_GetNeuralData ( hFile , dwEntityID , dwStartIndex , \n"
Original    (011): ['neuroshare', '.', 'ns_GetNeuralData', '(', 'hFile', ',', 'dwEntityID', ',', 'dwStartIndex', ',', '\\n']
Tokenized   (026): ['<s>', 'ne', 'uro', 'share', '.', 'ns', '_', 'Get', 'Ne', 'ural', 'Data', '(', 'h', 'File', ',', 'dw', 'Entity', 'ID', ',', 'dw', 'Start', 'Index', ',', '\\', 'n', '</s>']
Filtered   (024): ['ne', 'uro', 'share', '.', 'ns', '_', 'Get', 'Ne', 'ural', 'Data', '(', 'h', 'File', ',', 'dw', 'Entity', 'ID', ',', 'dw', 'Start', 'Index', ',', '\\', 'n']
Detokenized (011): ['neuroshare', '.', 'ns_GetNeuralData', '(', 'hFile', ',', 'dwEntityID', ',', 'dwStartIndex', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "dwIndexCount , pData . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) ) \n"
Original    (019): ['dwIndexCount', ',', 'pData', '.', 'ctypes', '.', 'data_as', '(', 'ctypes', '.', 'POINTER', '(', 'ctypes', '.', 'c_double', ')', ')', ')', '\\n']
Tokenized   (034): ['<s>', 'd', 'w', 'Index', 'Count', ',', 'p', 'Data', '.', 'c', 'types', '.', 'data', '_', 'as', '(', 'c', 'types', '.', 'PO', 'INTER', '(', 'c', 'types', '.', 'c', '_', 'double', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (032): ['d', 'w', 'Index', 'Count', ',', 'p', 'Data', '.', 'c', 'types', '.', 'data', '_', 'as', '(', 'c', 'types', '.', 'PO', 'INTER', '(', 'c', 'types', '.', 'c', '_', 'double', ')', ')', ')', '\\', 'n']
Detokenized (019): ['dwIndexCount', ',', 'pData', '.', 'ctypes', '.', 'data_as', '(', 'ctypes', '.', 'POINTER', '(', 'ctypes', '.', 'c_double', ')', ')', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "times = pData * pq . s \n"
Original    (008): ['times', '=', 'pData', '*', 'pq', '.', 's', '\\n']
Tokenized   (013): ['<s>', 'times', '=', 'p', 'Data', '*', 'p', 'q', '.', 's', '\\', 'n', '</s>']
Filtered   (011): ['times', '=', 'p', 'Data', '*', 'p', 'q', '.', 's', '\\', 'n']
Detokenized (008): ['times', '=', 'pData', '*', 'pq', '.', 's', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "clone_object , TEST_ANNOTATIONS ) \n"
Original    (005): ['clone_object', ',', 'TEST_ANNOTATIONS', ')', '\\n']
Tokenized   (014): ['<s>', 'clone', '_', 'object', ',', 'TEST', '_', 'AN', 'NOT', 'ATIONS', ')', '\\', 'n', '</s>']
Filtered   (012): ['clone', '_', 'object', ',', 'TEST', '_', 'AN', 'NOT', 'ATIONS', ')', '\\', 'n']
Detokenized (005): ['clone_object', ',', 'TEST_ANNOTATIONS', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "range ( len ( TEST_ANNOTATIONS ) ) ] ) \n"
Original    (010): ['range', '(', 'len', '(', 'TEST_ANNOTATIONS', ')', ')', ']', ')', '\\n']
Tokenized   (017): ['<s>', 'range', '(', 'len', '(', 'TEST', '_', 'AN', 'NOT', 'ATIONS', ')', ')', ']', ')', '\\', 'n', '</s>']
Filtered   (015): ['range', '(', 'len', '(', 'TEST', '_', 'AN', 'NOT', 'ATIONS', ')', ')', ']', ')', '\\', 'n']
Detokenized (010): ['range', '(', 'len', '(', 'TEST_ANNOTATIONS', ')', ')', ']', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "file_datetime = get_fake_value ( , datetime , seed = 0 ) \n"
Original    (012): ['file_datetime', '=', 'get_fake_value', '(', ',', 'datetime', ',', 'seed', '=', '0', ')', '\\n']
Tokenized   (023): ['<s>', 'file', '_', 'dat', 'etime', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'dat', 'etime', ',', 'seed', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (021): ['file', '_', 'dat', 'etime', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'dat', 'etime', ',', 'seed', '=', '0', ')', '\\', 'n']
Detokenized (012): ['file_datetime', '=', 'get_fake_value', '(', ',', 'datetime', ',', 'seed', '=', '0', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "file_origin = get_fake_value ( , str ) \n"
Original    (008): ['file_origin', '=', 'get_fake_value', '(', ',', 'str', ')', '\\n']
Tokenized   (017): ['<s>', 'file', '_', 'origin', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'str', ')', '\\', 'n', '</s>']
Filtered   (015): ['file', '_', 'origin', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'str', ')', '\\', 'n']
Detokenized (008): ['file_origin', '=', 'get_fake_value', '(', ',', 'str', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "attrs1 = { : file_datetime , \n"
Original    (007): ['attrs1', '=', '{', ':', 'file_datetime', ',', '\\n']
Tokenized   (015): ['<s>', 'att', 'rs', '1', '=', '{', ':', 'file', '_', 'dat', 'etime', ',', '\\', 'n', '</s>']
Filtered   (013): ['att', 'rs', '1', '=', '{', ':', 'file', '_', 'dat', 'etime', ',', '\\', 'n']
Detokenized (007): ['attrs1', '=', '{', ':', 'file_datetime', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "res21 = get_fake_values ( Segment , annotate = True , seed = 0 ) \n"
Original    (015): ['res21', '=', 'get_fake_values', '(', 'Segment', ',', 'annotate', '=', 'True', ',', 'seed', '=', '0', ')', '\\n']
Tokenized   (025): ['<s>', 'res', '21', '=', 'get', '_', 'fake', '_', 'values', '(', 'Se', 'gment', ',', 'annot', 'ate', '=', 'True', ',', 'seed', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (023): ['res', '21', '=', 'get', '_', 'fake', '_', 'values', '(', 'Se', 'gment', ',', 'annot', 'ate', '=', 'True', ',', 'seed', '=', '0', ')', '\\', 'n']
Detokenized (015): ['res21', '=', 'get_fake_values', '(', 'Segment', ',', 'annotate', '=', 'True', ',', 'seed', '=', '0', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "res22 = get_fake_values ( , annotate = True , seed = 0 ) \n"
Original    (014): ['res22', '=', 'get_fake_values', '(', ',', 'annotate', '=', 'True', ',', 'seed', '=', '0', ')', '\\n']
Tokenized   (023): ['<s>', 'res', '22', '=', 'get', '_', 'fake', '_', 'values', '(', ',', 'annot', 'ate', '=', 'True', ',', 'seed', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (021): ['res', '22', '=', 'get', '_', 'fake', '_', 'values', '(', ',', 'annot', 'ate', '=', 'True', ',', 'seed', '=', '0', ')', '\\', 'n']
Detokenized (014): ['res22', '=', 'get_fake_values', '(', ',', 'annotate', '=', 'True', ',', 'seed', '=', '0', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "targ0 = get_fake_value ( , datetime , seed = seed + 0 ) \n"
Original    (014): ['targ0', '=', 'get_fake_value', '(', ',', 'datetime', ',', 'seed', '=', 'seed', '+', '0', ')', '\\n']
Tokenized   (024): ['<s>', 't', 'arg', '0', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'dat', 'etime', ',', 'seed', '=', 'seed', '+', '0', ')', '\\', 'n', '</s>']
Filtered   (022): ['t', 'arg', '0', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'dat', 'etime', ',', 'seed', '=', 'seed', '+', '0', ')', '\\', 'n']
Detokenized (014): ['targ0', '=', 'get_fake_value', '(', ',', 'datetime', ',', 'seed', '=', 'seed', '+', '0', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "targ4 = get_fake_value ( , str , \n"
Original    (008): ['targ4', '=', 'get_fake_value', '(', ',', 'str', ',', '\\n']
Tokenized   (017): ['<s>', 't', 'arg', '4', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'str', ',', '\\', 'n', '</s>']
Filtered   (015): ['t', 'arg', '4', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'str', ',', '\\', 'n']
Detokenized (008): ['targ4', '=', 'get_fake_value', '(', ',', 'str', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "seed = seed + 4 , obj = Segment ) \n"
Original    (011): ['seed', '=', 'seed', '+', '4', ',', 'obj', '=', 'Segment', ')', '\\n']
Tokenized   (015): ['<s>', 'seed', '=', 'seed', '+', '4', ',', 'obj', '=', 'Se', 'gment', ')', '\\', 'n', '</s>']
Filtered   (013): ['seed', '=', 'seed', '+', '4', ',', 'obj', '=', 'Se', 'gment', ')', '\\', 'n']
Detokenized (011): ['seed', '=', 'seed', '+', '4', ',', 'obj', '=', 'Segment', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "childobjs = ( , , \n"
Original    (006): ['childobjs', '=', '(', ',', ',', '\\n']
Tokenized   (011): ['<s>', 'child', 'ob', 'js', '=', '(', ',', ',', '\\', 'n', '</s>']
Filtered   (009): ['child', 'ob', 'js', '=', '(', ',', ',', '\\', 'n']
Detokenized (006): ['childobjs', '=', '(', ',', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "children = ( self . sigs1a + self . sigarrs1a + \n"
Original    (012): ['children', '=', '(', 'self', '.', 'sigs1a', '+', 'self', '.', 'sigarrs1a', '+', '\\n']
Tokenized   (022): ['<s>', 'children', '=', '(', 'self', '.', 's', 'igs', '1', 'a', '+', 'self', '.', 'sig', 'arr', 's', '1', 'a', '+', '\\', 'n', '</s>']
Filtered   (020): ['children', '=', '(', 'self', '.', 's', 'igs', '1', 'a', '+', 'self', '.', 'sig', 'arr', 's', '1', 'a', '+', '\\', 'n']
Detokenized (012): ['children', '=', '(', 'self', '.', 'sigs1a', '+', 'self', '.', 'sigarrs1a', '+', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : ""analogsignals" : self . nchildren ** 2 , \n"
Original    (009): ['"analogsignals"', ':', 'self', '.', 'nchildren', '**', '2', ',', '\\n']
Tokenized   (018): ['<s>', '"', 'an', 'alog', 'sign', 'als', '"', ':', 'self', '.', 'n', 'children', '**', '2', ',', '\\', 'n', '</s>']
Filtered   (016): ['"', 'an', 'alog', 'sign', 'als', '"', ':', 'self', '.', 'n', 'children', '**', '2', ',', '\\', 'n']
Detokenized (009): ['"analogsignals"', ':', 'self', '.', 'nchildren', '**', '2', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ""epocharrays" : self . nchildren , "eventarrays" : self . nchildren , \n"
Original    (013): ['"epocharrays"', ':', 'self', '.', 'nchildren', ',', '"eventarrays"', ':', 'self', '.', 'nchildren', ',', '\\n']
Tokenized   (027): ['<s>', '"', 'ep', 'och', 'arr', 'ays', '"', ':', 'self', '.', 'n', 'children', ',', '"', 'event', 'arr', 'ays', '"', ':', 'self', '.', 'n', 'children', ',', '\\', 'n', '</s>']
Filtered   (025): ['"', 'ep', 'och', 'arr', 'ays', '"', ':', 'self', '.', 'n', 'children', ',', '"', 'event', 'arr', 'ays', '"', ':', 'self', '.', 'n', 'children', ',', '\\', 'n']
Detokenized (013): ['"epocharrays"', ':', 'self', '.', 'nchildren', ',', '"eventarrays"', ':', 'self', '.', 'nchildren', ',', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : ""analogsignalarrays" : self . nchildren } \n"
Original    (007): ['"analogsignalarrays"', ':', 'self', '.', 'nchildren', '}', '\\n']
Tokenized   (018): ['<s>', '"', 'an', 'alog', 'sign', 'al', 'arr', 'ays', '"', ':', 'self', '.', 'n', 'children', '}', '\\', 'n', '</s>']
Filtered   (016): ['"', 'an', 'alog', 'sign', 'al', 'arr', 'ays', '"', ':', 'self', '.', 'n', 'children', '}', '\\', 'n']
Detokenized (007): ['"analogsignalarrays"', ':', 'self', '.', 'nchildren', '}', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "targdict = { : 5 } ) \n"
Original    (008): ['targdict', '=', '{', ':', '5', '}', ')', '\\n']
Tokenized   (013): ['<s>', 't', 'arg', 'dict', '=', '{', ':', '5', '}', ')', '\\', 'n', '</s>']
Filtered   (011): ['t', 'arg', 'dict', '=', '{', ':', '5', '}', ')', '\\', 'n']
Detokenized (008): ['targdict', '=', '{', ':', '5', '}', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "res6 = filterdata ( data , name = self . epcs2 [ 0 ] . name , j = 5 ) \n"
Original    (022): ['res6', '=', 'filterdata', '(', 'data', ',', 'name', '=', 'self', '.', 'epcs2', '[', '0', ']', '.', 'name', ',', 'j', '=', '5', ')', '\\n']
Tokenized   (029): ['<s>', 'res', '6', '=', 'filter', 'data', '(', 'data', ',', 'name', '=', 'self', '.', 'ep', 'cs', '2', '[', '0', ']', '.', 'name', ',', 'j', '=', '5', ')', '\\', 'n', '</s>']
Filtered   (027): ['res', '6', '=', 'filter', 'data', '(', 'data', ',', 'name', '=', 'self', '.', 'ep', 'cs', '2', '[', '0', ']', '.', 'name', ',', 'j', '=', '5', ')', '\\', 'n']
Detokenized (022): ['res6', '=', 'filterdata', '(', 'data', ',', 'name', '=', 'self', '.', 'epcs2', '[', '0', ']', '.', 'name', ',', 'j', '=', '5', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "res7 = filterdata ( data , { : self . epcs2 [ 1 ] . name , : 5 } ) \n"
Original    (022): ['res7', '=', 'filterdata', '(', 'data', ',', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\n']
Tokenized   (029): ['<s>', 'res', '7', '=', 'filter', 'data', '(', 'data', ',', '{', ':', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\', 'n', '</s>']
Filtered   (027): ['res', '7', '=', 'filter', 'data', '(', 'data', ',', '{', ':', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\', 'n']
Detokenized (022): ['res7', '=', 'filterdata', '(', 'data', ',', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "res8 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name , : 5 } ) \n"
Original    (024): ['res8', '=', 'filterdata', '(', 'data', ',', 'targdict', '=', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\n']
Tokenized   (032): ['<s>', 'res', '8', '=', 'filter', 'data', '(', 'data', ',', 'targ', 'dict', '=', '{', ':', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\', 'n', '</s>']
Filtered   (030): ['res', '8', '=', 'filter', 'data', '(', 'data', ',', 'targ', 'dict', '=', '{', ':', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\', 'n']
Detokenized (024): ['res8', '=', 'filterdata', '(', 'data', ',', 'targdict', '=', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "res9 = filterdata ( data , { : self . epcs2 [ 1 ] . name } , j = 5 ) \n"
Original    (023): ['res9', '=', 'filterdata', '(', 'data', ',', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\n']
Tokenized   (030): ['<s>', 'res', '9', '=', 'filter', 'data', '(', 'data', ',', '{', ':', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\', 'n', '</s>']
Filtered   (028): ['res', '9', '=', 'filter', 'data', '(', 'data', ',', '{', ':', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\', 'n']
Detokenized (023): ['res9', '=', 'filterdata', '(', 'data', ',', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "res10 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name } , j = 5 ) \n"
Original    (025): ['res10', '=', 'filterdata', '(', 'data', ',', 'targdict', '=', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\n']
Tokenized   (033): ['<s>', 'res', '10', '=', 'filter', 'data', '(', 'data', ',', 'targ', 'dict', '=', '{', ':', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\', 'n', '</s>']
Filtered   (031): ['res', '10', '=', 'filter', 'data', '(', 'data', ',', 'targ', 'dict', '=', '{', ':', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\', 'n']
Detokenized (025): ['res10', '=', 'filterdata', '(', 'data', ',', 'targdict', '=', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "res11 = filterdata ( data , name = self . epcs2 [ 1 ] . name , targdict = { : 5 } ) \n"
Original    (025): ['res11', '=', 'filterdata', '(', 'data', ',', 'name', '=', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', ',', 'targdict', '=', '{', ':', '5', '}', ')', '\\n']
Tokenized   (033): ['<s>', 'res', '11', '=', 'filter', 'data', '(', 'data', ',', 'name', '=', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', ',', 'targ', 'dict', '=', '{', ':', '5', '}', ')', '\\', 'n', '</s>']
Filtered   (031): ['res', '11', '=', 'filter', 'data', '(', 'data', ',', 'name', '=', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', ',', 'targ', 'dict', '=', '{', ':', '5', '}', ')', '\\', 'n']
Detokenized (025): ['res11', '=', 'filterdata', '(', 'data', ',', 'name', '=', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', ',', 'targdict', '=', '{', ':', '5', '}', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "targ = [ self . epcs1a [ 1 ] ] \n"
Original    (011): ['targ', '=', '[', 'self', '.', 'epcs1a', '[', '1', ']', ']', '\\n']
Tokenized   (018): ['<s>', 't', 'arg', '=', '[', 'self', '.', 'ep', 'cs', '1', 'a', '[', '1', ']', ']', '\\', 'n', '</s>']
Filtered   (016): ['t', 'arg', '=', '[', 'self', '.', 'ep', 'cs', '1', 'a', '[', '1', ']', ']', '\\', 'n']
Detokenized (011): ['targ', '=', '[', 'self', '.', 'epcs1a', '[', '1', ']', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "res3 = filterdata ( data , [ { : 1 } , { : 2 } ] ) \n"
Original    (019): ['res3', '=', 'filterdata', '(', 'data', ',', '[', '{', ':', '1', '}', ',', '{', ':', '2', '}', ']', ')', '\\n']
Tokenized   (024): ['<s>', 'res', '3', '=', 'filter', 'data', '(', 'data', ',', '[', '{', ':', '1', '}', ',', '{', ':', '2', '}', ']', ')', '\\', 'n', '</s>']
Filtered   (022): ['res', '3', '=', 'filter', 'data', '(', 'data', ',', '[', '{', ':', '1', '}', ',', '{', ':', '2', '}', ']', ')', '\\', 'n']
Detokenized (019): ['res3', '=', 'filterdata', '(', 'data', ',', '[', '{', ':', '1', '}', ',', '{', ':', '2', '}', ']', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "res4 = filterdata ( data , { : 1 } , i = 2 ) \n"
Original    (016): ['res4', '=', 'filterdata', '(', 'data', ',', '{', ':', '1', '}', ',', 'i', '=', '2', ')', '\\n']
Tokenized   (021): ['<s>', 'res', '4', '=', 'filter', 'data', '(', 'data', ',', '{', ':', '1', '}', ',', 'i', '=', '2', ')', '\\', 'n', '</s>']
Filtered   (019): ['res', '4', '=', 'filter', 'data', '(', 'data', ',', '{', ':', '1', '}', ',', 'i', '=', '2', ')', '\\', 'n']
Detokenized (016): ['res4', '=', 'filterdata', '(', 'data', ',', '{', ':', '1', '}', ',', 'i', '=', '2', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "res5 = filterdata ( data , [ { : 1 } ] , i = 2 ) \n"
Original    (018): ['res5', '=', 'filterdata', '(', 'data', ',', '[', '{', ':', '1', '}', ']', ',', 'i', '=', '2', ')', '\\n']
Tokenized   (023): ['<s>', 'res', '5', '=', 'filter', 'data', '(', 'data', ',', '[', '{', ':', '1', '}', ']', ',', 'i', '=', '2', ')', '\\', 'n', '</s>']
Filtered   (021): ['res', '5', '=', 'filter', 'data', '(', 'data', ',', '[', '{', ':', '1', '}', ']', ',', 'i', '=', '2', ')', '\\', 'n']
Detokenized (018): ['res5', '=', 'filterdata', '(', 'data', ',', '[', '{', ':', '1', '}', ']', ',', 'i', '=', '2', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "ann = pretty ( ann ) . replace ( , ) \n"
Original    (012): ['ann', '=', 'pretty', '(', 'ann', ')', '.', 'replace', '(', ',', ')', '\\n']
Tokenized   (015): ['<s>', 'ann', '=', 'pretty', '(', 'ann', ')', '.', 'replace', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (013): ['ann', '=', 'pretty', '(', 'ann', ')', '.', 'replace', '(', ',', ')', '\\', 'n']
Detokenized (012): ['ann', '=', 'pretty', '(', 'ann', ')', '.', 'replace', '(', ',', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "unit_with_sig = np . array ( [ 0 , 2 , 5 ] ) \n"
Original    (015): ['unit_with_sig', '=', 'np', '.', 'array', '(', '[', '0', ',', '2', ',', '5', ']', ')', '\\n']
Tokenized   (023): ['<s>', 'unit', '_', 'with', '_', 's', 'ig', '=', 'np', '.', 'array', '(', '[', '0', ',', '2', ',', '5', ']', ')', '\\', 'n', '</s>']
Filtered   (021): ['unit', '_', 'with', '_', 's', 'ig', '=', 'np', '.', 'array', '(', '[', '0', ',', '2', ',', '5', ']', ')', '\\', 'n']
Detokenized (015): ['unit_with_sig', '=', 'np', '.', 'array', '(', '[', '0', ',', '2', ',', '5', ']', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "rcgs = [ RecordingChannelGroup ( name = , \n"
Original    (009): ['rcgs', '=', '[', 'RecordingChannelGroup', '(', 'name', '=', ',', '\\n']
Tokenized   (015): ['<s>', 'rc', 'gs', '=', '[', 'Recording', 'Channel', 'Group', '(', 'name', '=', ',', '\\', 'n', '</s>']
Filtered   (013): ['rc', 'gs', '=', '[', 'Recording', 'Channel', 'Group', '(', 'name', '=', ',', '\\', 'n']
Detokenized (009): ['rcgs', '=', '[', 'RecordingChannelGroup', '(', 'name', '=', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "RecordingChannelGroup ( name = , \n"
Original    (006): ['RecordingChannelGroup', '(', 'name', '=', ',', '\\n']
Tokenized   (012): ['<s>', 'Rec', 'ording', 'Channel', 'Group', '(', 'name', '=', ',', '\\', 'n', '</s>']
Filtered   (010): ['Rec', 'ording', 'Channel', 'Group', '(', 'name', '=', ',', '\\', 'n']
Detokenized (006): ['RecordingChannelGroup', '(', 'name', '=', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "t_start = 0. , t_stop = 10 ) \n"
Original    (009): ['t_start', '=', '0.', ',', 't_stop', '=', '10', ')', '\\n']
Tokenized   (017): ['<s>', 't', '_', 'start', '=', '0', '.', ',', 't', '_', 'stop', '=', '10', ')', '\\', 'n', '</s>']
Filtered   (015): ['t', '_', 'start', '=', '0', '.', ',', 't', '_', 'stop', '=', '10', ')', '\\', 'n']
Detokenized (009): ['t_start', '=', '0.', ',', 't_stop', '=', '10', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "st . unit = all_unit [ j ] \n"
Original    (009): ['st', '.', 'unit', '=', 'all_unit', '[', 'j', ']', '\\n']
Tokenized   (014): ['<s>', 'st', '.', 'unit', '=', 'all', '_', 'unit', '[', 'j', ']', '\\', 'n', '</s>']
Filtered   (012): ['st', '.', 'unit', '=', 'all', '_', 'unit', '[', 'j', ']', '\\', 'n']
Detokenized (009): ['st', '.', 'unit', '=', 'all_unit', '[', 'j', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "sampling_rate = 1000. * pq . Hz , \n"
Original    (009): ['sampling_rate', '=', '1000.', '*', 'pq', '.', 'Hz', ',', '\\n']
Tokenized   (017): ['<s>', 'sam', 'pling', '_', 'rate', '=', '1000', '.', '*', 'p', 'q', '.', 'Hz', ',', '\\', 'n', '</s>']
Filtered   (015): ['sam', 'pling', '_', 'rate', '=', '1000', '.', '*', 'p', 'q', '.', 'Hz', ',', '\\', 'n']
Detokenized (009): ['sampling_rate', '=', '1000.', '*', 'pq', '.', 'Hz', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "newseg = seg . construct_subsegment_by_unit ( all_unit [ : 4 ] ) \n"
Original    (013): ['newseg', '=', 'seg', '.', 'construct_subsegment_by_unit', '(', 'all_unit', '[', ':', '4', ']', ')', '\\n']
Tokenized   (029): ['<s>', 'new', 'se', 'g', '=', 'se', 'g', '.', 'construct', '_', 'sub', 'se', 'gment', '_', 'by', '_', 'unit', '(', 'all', '_', 'unit', '[', ':', '4', ']', ')', '\\', 'n', '</s>']
Filtered   (027): ['new', 'se', 'g', '=', 'se', 'g', '.', 'construct', '_', 'sub', 'se', 'gment', '_', 'by', '_', 'unit', '(', 'all', '_', 'unit', '[', ':', '4', ']', ')', '\\', 'n']
Detokenized (013): ['newseg', '=', 'seg', '.', 'construct_subsegment_by_unit', '(', 'all_unit', '[', ':', '4', ']', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "ind2 = self . unit2 . channel_indexes [ 0 ] \n"
Original    (011): ['ind2', '=', 'self', '.', 'unit2', '.', 'channel_indexes', '[', '0', ']', '\\n']
Tokenized   (019): ['<s>', 'ind', '2', '=', 'self', '.', 'unit', '2', '.', 'channel', '_', 'index', 'es', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (017): ['ind', '2', '=', 'self', '.', 'unit', '2', '.', 'channel', '_', 'index', 'es', '[', '0', ']', '\\', 'n']
Detokenized (011): ['ind2', '=', 'self', '.', 'unit2', '.', 'channel_indexes', '[', '0', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "result22 = self . seg1 . take_analogsignal_by_channelindex ( [ ind2 ] ) \n"
Original    (013): ['result22', '=', 'self', '.', 'seg1', '.', 'take_analogsignal_by_channelindex', '(', '[', 'ind2', ']', ')', '\\n']
Tokenized   (030): ['<s>', 'result', '22', '=', 'self', '.', 'se', 'g', '1', '.', 'take', '_', 'an', 'alog', 'sign', 'al', '_', 'by', '_', 'channel', 'index', '(', '[', 'ind', '2', ']', ')', '\\', 'n', '</s>']
Filtered   (028): ['result', '22', '=', 'self', '.', 'se', 'g', '1', '.', 'take', '_', 'an', 'alog', 'sign', 'al', '_', 'by', '_', 'channel', 'index', '(', '[', 'ind', '2', ']', ')', '\\', 'n']
Detokenized (013): ['result22', '=', 'self', '.', 'seg1', '.', 'take_analogsignal_by_channelindex', '(', '[', 'ind2', ']', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "targ1 = [ self . sigarrs1a [ 0 ] [ : , np . array ( [ True ] ) ] , \n"
Original    (023): ['targ1', '=', '[', 'self', '.', 'sigarrs1a', '[', '0', ']', '[', ':', ',', 'np', '.', 'array', '(', '[', 'True', ']', ')', ']', ',', '\\n']
Tokenized   (032): ['<s>', 't', 'arg', '1', '=', '[', 'self', '.', 'sig', 'arr', 's', '1', 'a', '[', '0', ']', '[', ':', ',', 'np', '.', 'array', '(', '[', 'True', ']', ')', ']', ',', '\\', 'n', '</s>']
Filtered   (030): ['t', 'arg', '1', '=', '[', 'self', '.', 'sig', 'arr', 's', '1', 'a', '[', '0', ']', '[', ':', ',', 'np', '.', 'array', '(', '[', 'True', ']', ')', ']', ',', '\\', 'n']
Detokenized (023): ['targ1', '=', '[', 'self', '.', 'sigarrs1a', '[', '0', ']', '[', ':', ',', 'np', '.', 'array', '(', '[', 'True', ']', ')', ']', ',', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "result21 = seg . take_slice_of_analogsignalarray_by_channelindex ( [ ind1 ] ) \n"
Original    (011): ['result21', '=', 'seg', '.', 'take_slice_of_analogsignalarray_by_channelindex', '(', '[', 'ind1', ']', ')', '\\n']
Tokenized   (032): ['<s>', 'result', '21', '=', 'se', 'g', '.', 'take', '_', 'slice', '_', 'of', '_', 'an', 'alog', 'sign', 'al', 'array', '_', 'by', '_', 'channel', 'index', '(', '[', 'ind', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (030): ['result', '21', '=', 'se', 'g', '.', 'take', '_', 'slice', '_', 'of', '_', 'an', 'alog', 'sign', 'al', 'array', '_', 'by', '_', 'channel', 'index', '(', '[', 'ind', '1', ']', ')', '\\', 'n']
Detokenized (011): ['result21', '=', 'seg', '.', 'take_slice_of_analogsignalarray_by_channelindex', '(', '[', 'ind1', ']', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "json_content = json_content . decode ( "utf-8" ) . replace ( , ) \n"
Original    (014): ['json_content', '=', 'json_content', '.', 'decode', '(', '"utf-8"', ')', '.', 'replace', '(', ',', ')', '\\n']
Tokenized   (025): ['<s>', 'json', '_', 'content', '=', 'json', '_', 'content', '.', 'decode', '(', '"', 'utf', '-', '8', '"', ')', '.', 'replace', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (023): ['json', '_', 'content', '=', 'json', '_', 'content', '.', 'decode', '(', '"', 'utf', '-', '8', '"', ')', '.', 'replace', '(', ',', ')', '\\', 'n']
Detokenized (014): ['json_content', '=', 'json_content', '.', 'decode', '(', '"utf-8"', ')', '.', 'replace', '(', ',', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "Image1 = StatisticMap ( name = , collection = self . Collection1 , file = , Image1 . file = SimpleUploadedFile ( , file ( os . path . join ( self . test_path , Image1 . save ( ) \n"
Original    (041): ['Image1', '=', 'StatisticMap', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection1', ',', 'file', '=', ',', 'Image1', '.', 'file', '=', 'SimpleUploadedFile', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', 'Image1', '.', 'save', '(', ')', '\\n']
Tokenized   (055): ['<s>', 'Image', '1', '=', 'Stat', 'istic', 'Map', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection', '1', ',', 'file', '=', ',', 'Image', '1', '.', 'file', '=', 'Simple', 'Upload', 'ed', 'File', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', 'Image', '1', '.', 'save', '(', ')', '\\', 'n', '</s>']
Filtered   (053): ['Image', '1', '=', 'Stat', 'istic', 'Map', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection', '1', ',', 'file', '=', ',', 'Image', '1', '.', 'file', '=', 'Simple', 'Upload', 'ed', 'File', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', 'Image', '1', '.', 'save', '(', ')', '\\', 'n']
Detokenized (041): ['Image1', '=', 'StatisticMap', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection1', ',', 'file', '=', ',', 'Image1', '.', 'file', '=', 'SimpleUploadedFile', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', 'Image1', '.', 'save', '(', ')', '\\n']
Counter: 53
===================================================================
Hidden states:  (13, 41, 768)
# Extracted words:  41
Sentence         : "fname = os . path . basename ( os . path . join ( self . test_path , ) ) file_dict = { : SimpleUploadedFile ( fname , zip_file . read ( ) ) } \n"
Original    (036): ['fname', '=', 'os', '.', 'path', '.', 'basename', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', ')', ')', 'file_dict', '=', '{', ':', 'SimpleUploadedFile', '(', 'fname', ',', 'zip_file', '.', 'read', '(', ')', ')', '}', '\\n']
Tokenized   (051): ['<s>', 'f', 'name', '=', 'os', '.', 'path', '.', 'bas', 'ename', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', ')', ')', 'file', '_', 'dict', '=', '{', ':', 'Simple', 'Upload', 'ed', 'File', '(', 'f', 'name', ',', 'zip', '_', 'file', '.', 'read', '(', ')', ')', '}', '\\', 'n', '</s>']
Filtered   (049): ['f', 'name', '=', 'os', '.', 'path', '.', 'bas', 'ename', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', ')', ')', 'file', '_', 'dict', '=', '{', ':', 'Simple', 'Upload', 'ed', 'File', '(', 'f', 'name', ',', 'zip', '_', 'file', '.', 'read', '(', ')', ')', '}', '\\', 'n']
Detokenized (036): ['fname', '=', 'os', '.', 'path', '.', 'basename', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', ')', ')', 'file_dict', '=', '{', ':', 'SimpleUploadedFile', '(', 'fname', ',', 'zip_file', '.', 'read', '(', ')', ')', '}', '\\n']
Counter: 49
===================================================================
Hidden states:  (13, 36, 768)
# Extracted words:  36
Sentence         : "Image2ss = StatisticMap ( name = , collection = self . Collection3 , file = Image2ss . file = SimpleUploadedFile ( , file ( os . path . join ( self . test_path , Image2ss . save ( ) \n"
Original    (040): ['Image2ss', '=', 'StatisticMap', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection3', ',', 'file', '=', 'Image2ss', '.', 'file', '=', 'SimpleUploadedFile', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', 'Image2ss', '.', 'save', '(', ')', '\\n']
Tokenized   (057): ['<s>', 'Image', '2', 'ss', '=', 'Stat', 'istic', 'Map', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection', '3', ',', 'file', '=', 'Image', '2', 'ss', '.', 'file', '=', 'Simple', 'Upload', 'ed', 'File', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', 'Image', '2', 'ss', '.', 'save', '(', ')', '\\', 'n', '</s>']
Filtered   (055): ['Image', '2', 'ss', '=', 'Stat', 'istic', 'Map', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection', '3', ',', 'file', '=', 'Image', '2', 'ss', '.', 'file', '=', 'Simple', 'Upload', 'ed', 'File', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', 'Image', '2', 'ss', '.', 'save', '(', ')', '\\', 'n']
Detokenized (040): ['Image2ss', '=', 'StatisticMap', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection3', ',', 'file', '=', 'Image2ss', '.', 'file', '=', 'SimpleUploadedFile', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', 'Image2ss', '.', 'save', '(', ')', '\\n']
Counter: 55
===================================================================
Hidden states:  (13, 40, 768)
# Extracted words:  40
Sentence         : "acc_new = rho * acc + ( 1 - rho ) * g ** 2 \n"
Original    (016): ['acc_new', '=', 'rho', '*', 'acc', '+', '(', '1', '-', 'rho', ')', '*', 'g', '**', '2', '\\n']
Tokenized   (023): ['<s>', 'acc', '_', 'new', '=', 'r', 'ho', '*', 'acc', '+', '(', '1', '-', 'r', 'ho', ')', '*', 'g', '**', '2', '\\', 'n', '</s>']
Filtered   (021): ['acc', '_', 'new', '=', 'r', 'ho', '*', 'acc', '+', '(', '1', '-', 'r', 'ho', ')', '*', 'g', '**', '2', '\\', 'n']
Detokenized (016): ['acc_new', '=', 'rho', '*', 'acc', '+', '(', '1', '-', 'rho', ')', '*', 'g', '**', '2', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "gradient_scaling = T . sqrt ( acc_new + epsilon ) \n"
Original    (011): ['gradient_scaling', '=', 'T', '.', 'sqrt', '(', 'acc_new', '+', 'epsilon', ')', '\\n']
Tokenized   (022): ['<s>', 'gradient', '_', 'sc', 'aling', '=', 'T', '.', 'sq', 'rt', '(', 'acc', '_', 'new', '+', 'e', 'ps', 'ilon', ')', '\\', 'n', '</s>']
Filtered   (020): ['gradient', '_', 'sc', 'aling', '=', 'T', '.', 'sq', 'rt', '(', 'acc', '_', 'new', '+', 'e', 'ps', 'ilon', ')', '\\', 'n']
Detokenized (011): ['gradient_scaling', '=', 'T', '.', 'sqrt', '(', 'acc_new', '+', 'epsilon', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "py_x = softmax ( T . dot ( h2 , w_o ) ) \n"
Original    (014): ['py_x', '=', 'softmax', '(', 'T', '.', 'dot', '(', 'h2', ',', 'w_o', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'py', '_', 'x', '=', 'soft', 'max', '(', 'T', '.', 'dot', '(', 'h', '2', ',', 'w', '_', 'o', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['py', '_', 'x', '=', 'soft', 'max', '(', 'T', '.', 'dot', '(', 'h', '2', ',', 'w', '_', 'o', ')', ')', '\\', 'n']
Detokenized (014): ['py_x', '=', 'softmax', '(', 'T', '.', 'dot', '(', 'h2', ',', 'w_o', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "w_h = init_weights ( ( 784 , 625 ) ) \n"
Original    (011): ['w_h', '=', 'init_weights', '(', '(', '784', ',', '625', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'w', '_', 'h', '=', 'init', '_', 'weights', '(', '(', '7', '84', ',', '625', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['w', '_', 'h', '=', 'init', '_', 'weights', '(', '(', '7', '84', ',', '625', ')', ')', '\\', 'n']
Detokenized (011): ['w_h', '=', 'init_weights', '(', '(', '784', ',', '625', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "noise_h , noise_h2 , noise_py_x = model ( X , w_h , w_h2 , w_o , 0.2 , 0.5 ) \n"
Original    (021): ['noise_h', ',', 'noise_h2', ',', 'noise_py_x', '=', 'model', '(', 'X', ',', 'w_h', ',', 'w_h2', ',', 'w_o', ',', '0.2', ',', '0.5', ')', '\\n']
Tokenized   (045): ['<s>', 'no', 'ise', '_', 'h', ',', 'noise', '_', 'h', '2', ',', 'noise', '_', 'py', '_', 'x', '=', 'model', '(', 'X', ',', 'w', '_', 'h', ',', 'w', '_', 'h', '2', ',', 'w', '_', 'o', ',', '0', '.', '2', ',', '0', '.', '5', ')', '\\', 'n', '</s>']
Filtered   (043): ['no', 'ise', '_', 'h', ',', 'noise', '_', 'h', '2', ',', 'noise', '_', 'py', '_', 'x', '=', 'model', '(', 'X', ',', 'w', '_', 'h', ',', 'w', '_', 'h', '2', ',', 'w', '_', 'o', ',', '0', '.', '2', ',', '0', '.', '5', ')', '\\', 'n']
Detokenized (021): ['noise_h', ',', 'noise_h2', ',', 'noise_py_x', '=', 'model', '(', 'X', ',', 'w_h', ',', 'w_h2', ',', 'w_o', ',', '0.2', ',', '0.5', ')', '\\n']
Counter: 43
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "y_x = T . argmax ( py_x , axis = 1 ) \n"
Original    (013): ['y_x', '=', 'T', '.', 'argmax', '(', 'py_x', ',', 'axis', '=', '1', ')', '\\n']
Tokenized   (021): ['<s>', 'y', '_', 'x', '=', 'T', '.', 'arg', 'max', '(', 'py', '_', 'x', ',', 'axis', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (019): ['y', '_', 'x', '=', 'T', '.', 'arg', 'max', '(', 'py', '_', 'x', ',', 'axis', '=', '1', ')', '\\', 'n']
Detokenized (013): ['y_x', '=', 'T', '.', 'argmax', '(', 'py_x', ',', 'axis', '=', '1', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "cost = T . mean ( T . nnet . categorical_crossentropy ( noise_py_x , Y ) ) \n"
Original    (018): ['cost', '=', 'T', '.', 'mean', '(', 'T', '.', 'nnet', '.', 'categorical_crossentropy', '(', 'noise_py_x', ',', 'Y', ')', ')', '\\n']
Tokenized   (031): ['<s>', 'cost', '=', 'T', '.', 'mean', '(', 'T', '.', 'n', 'net', '.', 'categ', 'orical', '_', 'cross', 'ent', 'ropy', '(', 'noise', '_', 'py', '_', 'x', ',', 'Y', ')', ')', '\\', 'n', '</s>']
Filtered   (029): ['cost', '=', 'T', '.', 'mean', '(', 'T', '.', 'n', 'net', '.', 'categ', 'orical', '_', 'cross', 'ent', 'ropy', '(', 'noise', '_', 'py', '_', 'x', ',', 'Y', ')', ')', '\\', 'n']
Detokenized (018): ['cost', '=', 'T', '.', 'mean', '(', 'T', '.', 'nnet', '.', 'categorical_crossentropy', '(', 'noise_py_x', ',', 'Y', ')', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "updates = RMSprop ( cost , params , lr = 0.001 ) \n"
Original    (013): ['updates', '=', 'RMSprop', '(', 'cost', ',', 'params', ',', 'lr', '=', '0.001', ')', '\\n']
Tokenized   (022): ['<s>', 'up', 'dates', '=', 'R', 'MS', 'prop', '(', 'cost', ',', 'params', ',', 'l', 'r', '=', '0', '.', '001', ')', '\\', 'n', '</s>']
Filtered   (020): ['up', 'dates', '=', 'R', 'MS', 'prop', '(', 'cost', ',', 'params', ',', 'l', 'r', '=', '0', '.', '001', ')', '\\', 'n']
Detokenized (013): ['updates', '=', 'RMSprop', '(', 'cost', ',', 'params', ',', 'lr', '=', '0.001', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "train = theano . function ( inputs = [ X , Y ] , outputs = cost , updates = updates , allow_input_downcast = True ) \n"
Original    (027): ['train', '=', 'theano', '.', 'function', '(', 'inputs', '=', '[', 'X', ',', 'Y', ']', ',', 'outputs', '=', 'cost', ',', 'updates', '=', 'updates', ',', 'allow_input_downcast', '=', 'True', ')', '\\n']
Tokenized   (036): ['<s>', 'train', '=', 'the', 'ano', '.', 'function', '(', 'inputs', '=', '[', 'X', ',', 'Y', ']', ',', 'outputs', '=', 'cost', ',', 'updates', '=', 'updates', ',', 'allow', '_', 'input', '_', 'down', 'cast', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (034): ['train', '=', 'the', 'ano', '.', 'function', '(', 'inputs', '=', '[', 'X', ',', 'Y', ']', ',', 'outputs', '=', 'cost', ',', 'updates', '=', 'updates', ',', 'allow', '_', 'input', '_', 'down', 'cast', '=', 'True', ')', '\\', 'n']
Detokenized (027): ['train', '=', 'theano', '.', 'function', '(', 'inputs', '=', '[', 'X', ',', 'Y', ']', ',', 'outputs', '=', 'cost', ',', 'updates', '=', 'updates', ',', 'allow_input_downcast', '=', 'True', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "predict = theano . function ( inputs = [ X ] , outputs = y_x , allow_input_downcast = True ) \n"
Original    (021): ['predict', '=', 'theano', '.', 'function', '(', 'inputs', '=', '[', 'X', ']', ',', 'outputs', '=', 'y_x', ',', 'allow_input_downcast', '=', 'True', ')', '\\n']
Tokenized   (033): ['<s>', 'p', 'redict', '=', 'the', 'ano', '.', 'function', '(', 'inputs', '=', '[', 'X', ']', ',', 'outputs', '=', 'y', '_', 'x', ',', 'allow', '_', 'input', '_', 'down', 'cast', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (031): ['p', 'redict', '=', 'the', 'ano', '.', 'function', '(', 'inputs', '=', '[', 'X', ']', ',', 'outputs', '=', 'y', '_', 'x', ',', 'allow', '_', 'input', '_', 'down', 'cast', '=', 'True', ')', '\\', 'n']
Detokenized (021): ['predict', '=', 'theano', '.', 'function', '(', 'inputs', '=', '[', 'X', ']', ',', 'outputs', '=', 'y_x', ',', 'allow_input_downcast', '=', 'True', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "settings . DATABASE_CONFIG_DICT [ ] ) \n"
Original    (007): ['settings', '.', 'DATABASE_CONFIG_DICT', '[', ']', ')', '\\n']
Tokenized   (019): ['<s>', 'settings', '.', 'D', 'AT', 'AB', 'ASE', '_', 'CON', 'FIG', '_', 'D', 'ICT', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (017): ['settings', '.', 'D', 'AT', 'AB', 'ASE', '_', 'CON', 'FIG', '_', 'D', 'ICT', '[', ']', ')', '\\', 'n']
Detokenized (007): ['settings', '.', 'DATABASE_CONFIG_DICT', '[', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "TEMPLATES [ 0 ] [ ] [ ] = DEBUG \n"
Original    (011): ['TEMPLATES', '[', '0', ']', '[', ']', '[', ']', '=', 'DEBUG', '\\n']
Tokenized   (017): ['<s>', 'T', 'EM', 'PL', 'ATES', '[', '0', ']', '[', ']', '[', ']', '=', 'DEBUG', '\\', 'n', '</s>']
Filtered   (015): ['T', 'EM', 'PL', 'ATES', '[', '0', ']', '[', ']', '[', ']', '=', 'DEBUG', '\\', 'n']
Detokenized (011): ['TEMPLATES', '[', '0', ']', '[', ']', '[', ']', '=', 'DEBUG', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "SECRET_KEY = env ( "DJANGO_SECRET_KEY" , default = ) \n"
Original    (010): ['SECRET_KEY', '=', 'env', '(', '"DJANGO_SECRET_KEY"', ',', 'default', '=', ')', '\\n']
Tokenized   (025): ['<s>', 'SEC', 'RET', '_', 'KEY', '=', 'env', '(', '"', 'DJ', 'AN', 'GO', '_', 'SEC', 'RET', '_', 'KEY', '"', ',', 'default', '=', ')', '\\', 'n', '</s>']
Filtered   (023): ['SEC', 'RET', '_', 'KEY', '=', 'env', '(', '"', 'DJ', 'AN', 'GO', '_', 'SEC', 'RET', '_', 'KEY', '"', ',', 'default', '=', ')', '\\', 'n']
Detokenized (010): ['SECRET_KEY', '=', 'env', '(', '"DJANGO_SECRET_KEY"', ',', 'default', '=', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "MIDDLEWARE_CLASSES += ( , ) \n"
Original    (006): ['MIDDLEWARE_CLASSES', '+=', '(', ',', ')', '\\n']
Tokenized   (016): ['<s>', 'M', 'ID', 'D', 'LE', 'WARE', '_', 'CLASS', 'ES', '+=', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (014): ['M', 'ID', 'D', 'LE', 'WARE', '_', 'CLASS', 'ES', '+=', '(', ',', ')', '\\', 'n']
Detokenized (006): ['MIDDLEWARE_CLASSES', '+=', '(', ',', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "INTERNAL_IPS = ( , , ) \n"
Original    (007): ['INTERNAL_IPS', '=', '(', ',', ',', ')', '\\n']
Tokenized   (014): ['<s>', 'IN', 'TERN', 'AL', '_', 'IPS', '=', '(', ',', ',', ')', '\\', 'n', '</s>']
Filtered   (012): ['IN', 'TERN', 'AL', '_', 'IPS', '=', '(', ',', ',', ')', '\\', 'n']
Detokenized (007): ['INTERNAL_IPS', '=', '(', ',', ',', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "redirect_url = request . POST . get ( ) or \n"
Original    (011): ['redirect_url', '=', 'request', '.', 'POST', '.', 'get', '(', ')', 'or', '\\n']
Tokenized   (017): ['<s>', 'red', 'irect', '_', 'url', '=', 'request', '.', 'POST', '.', 'get', '(', ')', 'or', '\\', 'n', '</s>']
Filtered   (015): ['red', 'irect', '_', 'url', '=', 'request', '.', 'POST', '.', 'get', '(', ')', 'or', '\\', 'n']
Detokenized (011): ['redirect_url', '=', 'request', '.', 'POST', '.', 'get', '(', ')', 'or', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "icon = self . get_plugin_icon ( ) , \n"
Original    (009): ['icon', '=', 'self', '.', 'get_plugin_icon', '(', ')', ',', '\\n']
Tokenized   (016): ['<s>', 'icon', '=', 'self', '.', 'get', '_', 'plugin', '_', 'icon', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (014): ['icon', '=', 'self', '.', 'get', '_', 'plugin', '_', 'icon', '(', ')', ',', '\\', 'n']
Detokenized (009): ['icon', '=', 'self', '.', 'get_plugin_icon', '(', ')', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "memoryprofiler_act . setEnabled ( is_memoryprofiler_installed ( ) ) \n"
Original    (009): ['memoryprofiler_act', '.', 'setEnabled', '(', 'is_memoryprofiler_installed', '(', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'memory', 'prof', 'iler', '_', 'act', '.', 'set', 'Enabled', '(', 'is', '_', 'memory', 'prof', 'iler', '_', 'installed', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['memory', 'prof', 'iler', '_', 'act', '.', 'set', 'Enabled', '(', 'is', '_', 'memory', 'prof', 'iler', '_', 'installed', '(', ')', ')', '\\', 'n']
Detokenized (009): ['memoryprofiler_act', '.', 'setEnabled', '(', 'is_memoryprofiler_installed', '(', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "wdir , args = None , None \n"
Original    (008): ['wdir', ',', 'args', '=', 'None', ',', 'None', '\\n']
Tokenized   (012): ['<s>', 'w', 'dir', ',', 'args', '=', 'None', ',', 'None', '\\', 'n', '</s>']
Filtered   (010): ['w', 'dir', ',', 'args', '=', 'None', ',', 'None', '\\', 'n']
Detokenized (008): ['wdir', ',', 'args', '=', 'None', ',', 'None', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "use_colors = self . get_option ( , True ) ) \n"
Original    (011): ['use_colors', '=', 'self', '.', 'get_option', '(', ',', 'True', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'use', '_', 'col', 'ors', '=', 'self', '.', 'get', '_', 'option', '(', ',', 'True', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['use', '_', 'col', 'ors', '=', 'self', '.', 'get', '_', 'option', '(', ',', 'True', ')', ')', '\\', 'n']
Detokenized (011): ['use_colors', '=', 'self', '.', 'get_option', '(', ',', 'True', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "message_type = None , enum_type = None , containing_type = None , \n"
Original    (013): ['message_type', '=', 'None', ',', 'enum_type', '=', 'None', ',', 'containing_type', '=', 'None', ',', '\\n']
Tokenized   (022): ['<s>', 'message', '_', 'type', '=', 'None', ',', 'enum', '_', 'type', '=', 'None', ',', 'containing', '_', 'type', '=', 'None', ',', '\\', 'n', '</s>']
Filtered   (020): ['message', '_', 'type', '=', 'None', ',', 'enum', '_', 'type', '=', 'None', ',', 'containing', '_', 'type', '=', 'None', ',', '\\', 'n']
Detokenized (013): ['message_type', '=', 'None', ',', 'enum_type', '=', 'None', ',', 'containing_type', '=', 'None', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "is_extension = False , extension_scope = None , \n"
Original    (009): ['is_extension', '=', 'False', ',', 'extension_scope', '=', 'None', ',', '\\n']
Tokenized   (017): ['<s>', 'is', '_', 'ext', 'ension', '=', 'False', ',', 'extension', '_', 'scope', '=', 'None', ',', '\\', 'n', '</s>']
Filtered   (015): ['is', '_', 'ext', 'ension', '=', 'False', ',', 'extension', '_', 'scope', '=', 'None', ',', '\\', 'n']
Detokenized (009): ['is_extension', '=', 'False', ',', 'extension_scope', '=', 'None', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "_BATCHNOTIFICATIONREQUEST . fields_by_name [ ] . message_type = _PUSHNOTIFICATION \n"
Original    (010): ['_BATCHNOTIFICATIONREQUEST', '.', 'fields_by_name', '[', ']', '.', 'message_type', '=', '_PUSHNOTIFICATION', '\\n']
Tokenized   (031): ['<s>', '_', 'B', 'ATCH', 'NOT', 'IFIC', 'ATION', 'RE', 'QUEST', '.', 'fields', '_', 'by', '_', 'name', '[', ']', '.', 'message', '_', 'type', '=', '_', 'P', 'USH', 'NOT', 'IFIC', 'ATION', '\\', 'n', '</s>']
Filtered   (029): ['_', 'B', 'ATCH', 'NOT', 'IFIC', 'ATION', 'RE', 'QUEST', '.', 'fields', '_', 'by', '_', 'name', '[', ']', '.', 'message', '_', 'type', '=', '_', 'P', 'USH', 'NOT', 'IFIC', 'ATION', '\\', 'n']
Detokenized (010): ['_BATCHNOTIFICATIONREQUEST', '.', 'fields_by_name', '[', ']', '.', 'message_type', '=', '_PUSHNOTIFICATION', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "expected_zip_content = [ "manifest.json" , "sd_bl.bin" , "sd_bl.dat" ] \n"
Original    (010): ['expected_zip_content', '=', '[', '"manifest.json"', ',', '"sd_bl.bin"', ',', '"sd_bl.dat"', ']', '\\n']
Tokenized   (034): ['<s>', 'expected', '_', 'zip', '_', 'content', '=', '[', '"', 'man', 'ifest', '.', 'json', '"', ',', '"', 'sd', '_', 'bl', '.', 'bin', '"', ',', '"', 'sd', '_', 'bl', '.', 'dat', '"', ']', '\\', 'n', '</s>']
Filtered   (032): ['expected', '_', 'zip', '_', 'content', '=', '[', '"', 'man', 'ifest', '.', 'json', '"', ',', '"', 'sd', '_', 'bl', '.', 'bin', '"', ',', '"', 'sd', '_', 'bl', '.', 'dat', '"', ']', '\\', 'n']
Detokenized (010): ['expected_zip_content', '=', '[', '"manifest.json"', ',', '"sd_bl.bin"', ',', '"sd_bl.dat"', ']', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sd_req = [ 0x1000 , 0xffff ] , \n"
Original    (009): ['sd_req', '=', '[', '0x1000', ',', '0xffff', ']', ',', '\\n']
Tokenized   (018): ['<s>', 'sd', '_', 'req', '=', '[', '0', 'x', '1000', ',', '0', 'x', 'ffff', ']', ',', '\\', 'n', '</s>']
Filtered   (016): ['sd', '_', 'req', '=', '[', '0', 'x', '1000', ',', '0', 'x', 'ffff', ']', ',', '\\', 'n']
Detokenized (009): ['sd_req', '=', '[', '0x1000', ',', '0xffff', ']', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "pkg_name = os . path . join ( self . work_directory , "mypackage.zip" ) \n"
Original    (015): ['pkg_name', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work_directory', ',', '"mypackage.zip"', ')', '\\n']
Tokenized   (027): ['<s>', 'pkg', '_', 'name', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work', '_', 'directory', ',', '"', 'my', 'package', '.', 'zip', '"', ')', '\\', 'n', '</s>']
Filtered   (025): ['pkg', '_', 'name', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work', '_', 'directory', ',', '"', 'my', 'package', '.', 'zip', '"', ')', '\\', 'n']
Detokenized (015): ['pkg_name', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work_directory', ',', '"mypackage.zip"', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "manifest = self . p . unpack_package ( os . path . join ( self . work_directory , pkg_name ) , unpacked_dir ) \n"
Original    (024): ['manifest', '=', 'self', '.', 'p', '.', 'unpack_package', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work_directory', ',', 'pkg_name', ')', ',', 'unpacked_dir', ')', '\\n']
Tokenized   (039): ['<s>', 'man', 'ifest', '=', 'self', '.', 'p', '.', 'un', 'pack', '_', 'package', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work', '_', 'directory', ',', 'p', 'kg', '_', 'name', ')', ',', 'unp', 'acked', '_', 'dir', ')', '\\', 'n', '</s>']
Filtered   (037): ['man', 'ifest', '=', 'self', '.', 'p', '.', 'un', 'pack', '_', 'package', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work', '_', 'directory', ',', 'p', 'kg', '_', 'name', ')', ',', 'unp', 'acked', '_', 'dir', ')', '\\', 'n']
Detokenized (024): ['manifest', '=', 'self', '.', 'p', '.', 'unpack_package', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work_directory', ',', 'pkg_name', ')', ',', 'unpacked_dir', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "style = wx . richtext . RE_MULTILINE , value = ) \n"
Original    (012): ['style', '=', 'wx', '.', 'richtext', '.', 'RE_MULTILINE', ',', 'value', '=', ')', '\\n']
Tokenized   (022): ['<s>', 'style', '=', 'w', 'x', '.', 'rich', 'text', '.', 'RE', '_', 'M', 'ULT', 'IL', 'INE', ',', 'value', '=', ')', '\\', 'n', '</s>']
Filtered   (020): ['style', '=', 'w', 'x', '.', 'rich', 'text', '.', 'RE', '_', 'M', 'ULT', 'IL', 'INE', ',', 'value', '=', ')', '\\', 'n']
Detokenized (012): ['style', '=', 'wx', '.', 'richtext', '.', 'RE_MULTILINE', ',', 'value', '=', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "fgSizer1 . Add ( self . lblChangelt , 0 , wx . ALL , 5 ) \n"
Original    (017): ['fgSizer1', '.', 'Add', '(', 'self', '.', 'lblChangelt', ',', '0', ',', 'wx', '.', 'ALL', ',', '5', ')', '\\n']
Tokenized   (028): ['<s>', 'fg', 'S', 'izer', '1', '.', 'Add', '(', 'self', '.', 'l', 'bl', 'Ch', 'ang', 'elt', ',', '0', ',', 'w', 'x', '.', 'ALL', ',', '5', ')', '\\', 'n', '</s>']
Filtered   (026): ['fg', 'S', 'izer', '1', '.', 'Add', '(', 'self', '.', 'l', 'bl', 'Ch', 'ang', 'elt', ',', '0', ',', 'w', 'x', '.', 'ALL', ',', '5', ')', '\\', 'n']
Detokenized (017): ['fgSizer1', '.', 'Add', '(', 'self', '.', 'lblChangelt', ',', '0', ',', 'wx', '.', 'ALL', ',', '5', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "sbThreshold . Add ( fgSizer1 , 0 , wx . EXPAND , 5 ) \n"
Original    (015): ['sbThreshold', '.', 'Add', '(', 'fgSizer1', ',', '0', ',', 'wx', '.', 'EXPAND', ',', '5', ')', '\\n']
Tokenized   (026): ['<s>', 'sb', 'Th', 'reshold', '.', 'Add', '(', 'f', 'g', 'S', 'izer', '1', ',', '0', ',', 'w', 'x', '.', 'EXP', 'AND', ',', '5', ')', '\\', 'n', '</s>']
Filtered   (024): ['sb', 'Th', 'reshold', '.', 'Add', '(', 'f', 'g', 'S', 'izer', '1', ',', '0', ',', 'w', 'x', '.', 'EXP', 'AND', ',', '5', ')', '\\', 'n']
Detokenized (015): ['sbThreshold', '.', 'Add', '(', 'fgSizer1', ',', '0', ',', 'wx', '.', 'EXPAND', ',', '5', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "bsValueThresh . Add ( sbThreshold , 1 , 0 , 5 ) \n"
Original    (013): ['bsValueThresh', '.', 'Add', '(', 'sbThreshold', ',', '1', ',', '0', ',', '5', ')', '\\n']
Tokenized   (022): ['<s>', 'bs', 'Value', 'Th', 'resh', '.', 'Add', '(', 's', 'b', 'Th', 'reshold', ',', '1', ',', '0', ',', '5', ')', '\\', 'n', '</s>']
Filtered   (020): ['bs', 'Value', 'Th', 'resh', '.', 'Add', '(', 's', 'b', 'Th', 'reshold', ',', '1', ',', '0', ',', '5', ')', '\\', 'n']
Detokenized (013): ['bsValueThresh', '.', 'Add', '(', 'sbThreshold', ',', '1', ',', '0', ',', '5', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "cbGapTimeChoices = [ u"second" , u"minute" , u"hour" , u"day" ] \n"
Original    (012): ['cbGapTimeChoices', '=', '[', 'u"second"', ',', 'u"minute"', ',', 'u"hour"', ',', 'u"day"', ']', '\\n']
Tokenized   (032): ['<s>', 'cb', 'G', 'ap', 'Time', 'Cho', 'ices', '=', '[', 'u', '"', 'second', '"', ',', 'u', '"', 'minute', '"', ',', 'u', '"', 'hour', '"', ',', 'u', '"', 'day', '"', ']', '\\', 'n', '</s>']
Filtered   (030): ['cb', 'G', 'ap', 'Time', 'Cho', 'ices', '=', '[', 'u', '"', 'second', '"', ',', 'u', '"', 'minute', '"', ',', 'u', '"', 'hour', '"', ',', 'u', '"', 'day', '"', ']', '\\', 'n']
Detokenized (012): ['cbGapTimeChoices', '=', '[', 'u"second"', ',', 'u"minute"', ',', 'u"hour"', ',', 'u"day"', ']', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "fmt24hr = True , spinButton = self . sbBefore , oob_color = ) \n"
Original    (014): ['fmt24hr', '=', 'True', ',', 'spinButton', '=', 'self', '.', 'sbBefore', ',', 'oob_color', '=', ')', '\\n']
Tokenized   (026): ['<s>', 'f', 'mt', '24', 'hr', '=', 'True', ',', 'spin', 'Button', '=', 'self', '.', 's', 'b', 'Before', ',', 'o', 'ob', '_', 'color', '=', ')', '\\', 'n', '</s>']
Filtered   (024): ['f', 'mt', '24', 'hr', '=', 'True', ',', 'spin', 'Button', '=', 'self', '.', 's', 'b', 'Before', ',', 'o', 'ob', '_', 'color', '=', ')', '\\', 'n']
Detokenized (014): ['fmt24hr', '=', 'True', ',', 'spinButton', '=', 'self', '.', 'sbBefore', ',', 'oob_color', '=', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "bsButtons . Add ( self . btnOK , 1 , wx . ALL | wx . EXPAND , 5 ) \n"
Original    (021): ['bsButtons', '.', 'Add', '(', 'self', '.', 'btnOK', ',', '1', ',', 'wx', '.', 'ALL', '|', 'wx', '.', 'EXPAND', ',', '5', ')', '\\n']
Tokenized   (031): ['<s>', 'bs', 'But', 'tons', '.', 'Add', '(', 'self', '.', 'b', 'tn', 'OK', ',', '1', ',', 'w', 'x', '.', 'ALL', '|', 'w', 'x', '.', 'EXP', 'AND', ',', '5', ')', '\\', 'n', '</s>']
Filtered   (029): ['bs', 'But', 'tons', '.', 'Add', '(', 'self', '.', 'b', 'tn', 'OK', ',', '1', ',', 'w', 'x', '.', 'ALL', '|', 'w', 'x', '.', 'EXP', 'AND', ',', '5', ')', '\\', 'n']
Detokenized (021): ['bsButtons', '.', 'Add', '(', 'self', '.', 'btnOK', ',', '1', ',', 'wx', '.', 'ALL', '|', 'wx', '.', 'EXPAND', ',', '5', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "mat ( op2 . INC , ( elem_node [ op2 . i [ 0 ] ] , \n"
Original    (018): ['mat', '(', 'op2', '.', 'INC', ',', '(', 'elem_node', '[', 'op2', '.', 'i', '[', '0', ']', ']', ',', '\\n']
Tokenized   (026): ['<s>', 'mat', '(', 'op', '2', '.', 'INC', ',', '(', 'ele', 'm', '_', 'node', '[', 'op', '2', '.', 'i', '[', '0', ']', ']', ',', '\\', 'n', '</s>']
Filtered   (024): ['mat', '(', 'op', '2', '.', 'INC', ',', '(', 'ele', 'm', '_', 'node', '[', 'op', '2', '.', 'i', '[', '0', ']', ']', ',', '\\', 'n']
Detokenized (018): ['mat', '(', 'op2', '.', 'INC', ',', '(', 'elem_node', '[', 'op2', '.', 'i', '[', '0', ']', ']', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "elem_node [ op2 . i [ 1 ] ] ) ) , \n"
Original    (013): ['elem_node', '[', 'op2', '.', 'i', '[', '1', ']', ']', ')', ')', ',', '\\n']
Tokenized   (020): ['<s>', 'e', 'lem', '_', 'node', '[', 'op', '2', '.', 'i', '[', '1', ']', ']', ')', ')', ',', '\\', 'n', '</s>']
Filtered   (018): ['e', 'lem', '_', 'node', '[', 'op', '2', '.', 'i', '[', '1', ']', ']', ')', ')', ',', '\\', 'n']
Detokenized (013): ['elem_node', '[', 'op2', '.', 'i', '[', '1', ']', ']', ')', ')', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "partition_size = NUM_ELE / 2 , \n"
Original    (007): ['partition_size', '=', 'NUM_ELE', '/', '2', ',', '\\n']
Tokenized   (016): ['<s>', 'part', 'ition', '_', 'size', '=', 'NUM', '_', 'E', 'LE', '/', '2', ',', '\\', 'n', '</s>']
Filtered   (014): ['part', 'ition', '_', 'size', '=', 'NUM', '_', 'E', 'LE', '/', '2', ',', '\\', 'n']
Detokenized (007): ['partition_size', '=', 'NUM_ELE', '/', '2', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "statusBarAx . barh ( [ 0 ] , [ 100.0 * expNumber / len ( data . getPaths ( ) ) ] , \n"
Original    (024): ['statusBarAx', '.', 'barh', '(', '[', '0', ']', ',', '[', '100.0', '*', 'expNumber', '/', 'len', '(', 'data', '.', 'getPaths', '(', ')', ')', ']', ',', '\\n']
Tokenized   (035): ['<s>', 'status', 'Bar', 'Ax', '.', 'bar', 'h', '(', '[', '0', ']', ',', '[', '100', '.', '0', '*', 'exp', 'Number', '/', 'len', '(', 'data', '.', 'get', 'Path', 's', '(', ')', ')', ']', ',', '\\', 'n', '</s>']
Filtered   (033): ['status', 'Bar', 'Ax', '.', 'bar', 'h', '(', '[', '0', ']', ',', '[', '100', '.', '0', '*', 'exp', 'Number', '/', 'len', '(', 'data', '.', 'get', 'Path', 's', '(', ')', ')', ']', ',', '\\', 'n']
Detokenized (024): ['statusBarAx', '.', 'barh', '(', '[', '0', ']', ',', '[', '100.0', '*', 'expNumber', '/', 'len', '(', 'data', '.', 'getPaths', '(', ')', ')', ']', ',', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "fluxes , errors , photFlags = photometry . multirad ( image , x , y , \n"
Original    (017): ['fluxes', ',', 'errors', ',', 'photFlags', '=', 'photometry', '.', 'multirad', '(', 'image', ',', 'x', ',', 'y', ',', '\\n']
Tokenized   (026): ['<s>', 'f', 'lux', 'es', ',', 'errors', ',', 'phot', 'Flags', '=', 'phot', 'ometry', '.', 'mult', 'ir', 'ad', '(', 'image', ',', 'x', ',', 'y', ',', '\\', 'n', '</s>']
Filtered   (024): ['f', 'lux', 'es', ',', 'errors', ',', 'phot', 'Flags', '=', 'phot', 'ometry', '.', 'mult', 'ir', 'ad', '(', 'image', ',', 'x', ',', 'y', ',', '\\', 'n']
Detokenized (017): ['fluxes', ',', 'errors', ',', 'photFlags', '=', 'photometry', '.', 'multirad', '(', 'image', ',', 'x', ',', 'y', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "meanComparisonStars , meanComparisonStarErrors = data . calcMeanComparison_multirad ( ccdGain = data . ccdGain ) \n"
Original    (015): ['meanComparisonStars', ',', 'meanComparisonStarErrors', '=', 'data', '.', 'calcMeanComparison_multirad', '(', 'ccdGain', '=', 'data', '.', 'ccdGain', ')', '\\n']
Tokenized   (040): ['<s>', 'mean', 'Compar', 'ison', 'Stars', ',', 'mean', 'Compar', 'ison', 'Star', 'Er', 'rors', '=', 'data', '.', 'calc', 'Me', 'an', 'Compar', 'ison', '_', 'mult', 'ir', 'ad', '(', 'c', 'cd', 'G', 'ain', '=', 'data', '.', 'c', 'cd', 'G', 'ain', ')', '\\', 'n', '</s>']
Filtered   (038): ['mean', 'Compar', 'ison', 'Stars', ',', 'mean', 'Compar', 'ison', 'Star', 'Er', 'rors', '=', 'data', '.', 'calc', 'Me', 'an', 'Compar', 'ison', '_', 'mult', 'ir', 'ad', '(', 'c', 'cd', 'G', 'ain', '=', 'data', '.', 'c', 'cd', 'G', 'ain', ')', '\\', 'n']
Detokenized (015): ['meanComparisonStars', ',', 'meanComparisonStarErrors', '=', 'data', '.', 'calcMeanComparison_multirad', '(', 'ccdGain', '=', 'data', '.', 'ccdGain', ')', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "lightCurves , lightCurveErrors = data . computeLightCurve_multirad ( meanComparisonStars , \n"
Original    (011): ['lightCurves', ',', 'lightCurveErrors', '=', 'data', '.', 'computeLightCurve_multirad', '(', 'meanComparisonStars', ',', '\\n']
Tokenized   (030): ['<s>', 'light', 'Cur', 'ves', ',', 'light', 'Cur', 've', 'Er', 'rors', '=', 'data', '.', 'compute', 'Light', 'Cur', 've', '_', 'mult', 'ir', 'ad', '(', 'mean', 'Compar', 'ison', 'Stars', ',', '\\', 'n', '</s>']
Filtered   (028): ['light', 'Cur', 'ves', ',', 'light', 'Cur', 've', 'Er', 'rors', '=', 'data', '.', 'compute', 'Light', 'Cur', 've', '_', 'mult', 'ir', 'ad', '(', 'mean', 'Compar', 'ison', 'Stars', ',', '\\', 'n']
Detokenized (011): ['lightCurves', ',', 'lightCurveErrors', '=', 'data', '.', 'computeLightCurve_multirad', '(', 'meanComparisonStars', ',', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "json_data = self . _send_request ( , url , params = params ) \n"
Original    (014): ['json_data', '=', 'self', '.', '_send_request', '(', ',', 'url', ',', 'params', '=', 'params', ')', '\\n']
Tokenized   (022): ['<s>', 'json', '_', 'data', '=', 'self', '.', '_', 'send', '_', 'request', '(', ',', 'url', ',', 'params', '=', 'params', ')', '\\', 'n', '</s>']
Filtered   (020): ['json', '_', 'data', '=', 'self', '.', '_', 'send', '_', 'request', '(', ',', 'url', ',', 'params', '=', 'params', ')', '\\', 'n']
Detokenized (014): ['json_data', '=', 'self', '.', '_send_request', '(', ',', 'url', ',', 'params', '=', 'params', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "mkdir ( env . hosts_data . log_path ( ) ) \n"
Original    (011): ['mkdir', '(', 'env', '.', 'hosts_data', '.', 'log_path', '(', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'mk', 'dir', '(', 'env', '.', 'hosts', '_', 'data', '.', 'log', '_', 'path', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['mk', 'dir', '(', 'env', '.', 'hosts', '_', 'data', '.', 'log', '_', 'path', '(', ')', ')', '\\', 'n']
Detokenized (011): ['mkdir', '(', 'env', '.', 'hosts_data', '.', 'log_path', '(', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "StringIO ( env . hosts_data . celery_supervisor_config ( ) ) , \n"
Original    (012): ['StringIO', '(', 'env', '.', 'hosts_data', '.', 'celery_supervisor_config', '(', ')', ')', ',', '\\n']
Tokenized   (024): ['<s>', 'String', 'IO', '(', 'env', '.', 'hosts', '_', 'data', '.', 'cel', 'ery', '_', 'super', 'visor', '_', 'config', '(', ')', ')', ',', '\\', 'n', '</s>']
Filtered   (022): ['String', 'IO', '(', 'env', '.', 'hosts', '_', 'data', '.', 'cel', 'ery', '_', 'super', 'visor', '_', 'config', '(', ')', ')', ',', '\\', 'n']
Detokenized (012): ['StringIO', '(', 'env', '.', 'hosts_data', '.', 'celery_supervisor_config', '(', ')', ')', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "env . hosts_data . celery_supervisor_config_path ( ) , \n"
Original    (009): ['env', '.', 'hosts_data', '.', 'celery_supervisor_config_path', '(', ')', ',', '\\n']
Tokenized   (022): ['<s>', 'env', '.', 'hosts', '_', 'data', '.', 'cel', 'ery', '_', 'super', 'visor', '_', 'config', '_', 'path', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (020): ['env', '.', 'hosts', '_', 'data', '.', 'cel', 'ery', '_', 'super', 'visor', '_', 'config', '_', 'path', '(', ')', ',', '\\', 'n']
Detokenized (009): ['env', '.', 'hosts_data', '.', 'celery_supervisor_config_path', '(', ')', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "rmdir ( env . hosts_data . celery_supervisor_config_path ( ) , sudo_access = True ) \n"
Original    (015): ['rmdir', '(', 'env', '.', 'hosts_data', '.', 'celery_supervisor_config_path', '(', ')', ',', 'sudo_access', '=', 'True', ')', '\\n']
Tokenized   (032): ['<s>', 'r', 'md', 'ir', '(', 'env', '.', 'hosts', '_', 'data', '.', 'cel', 'ery', '_', 'super', 'visor', '_', 'config', '_', 'path', '(', ')', ',', 'sudo', '_', 'access', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (030): ['r', 'md', 'ir', '(', 'env', '.', 'hosts', '_', 'data', '.', 'cel', 'ery', '_', 'super', 'visor', '_', 'config', '_', 'path', '(', ')', ',', 'sudo', '_', 'access', '=', 'True', ')', '\\', 'n']
Detokenized (015): ['rmdir', '(', 'env', '.', 'hosts_data', '.', 'celery_supervisor_config_path', '(', ')', ',', 'sudo_access', '=', 'True', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "sudo ( . format ( env . hosts_data . application_name ( ) ) ) \n"
Original    (015): ['sudo', '(', '.', 'format', '(', 'env', '.', 'hosts_data', '.', 'application_name', '(', ')', ')', ')', '\\n']
Tokenized   (022): ['<s>', 'sudo', '(', '.', 'format', '(', 'env', '.', 'hosts', '_', 'data', '.', 'application', '_', 'name', '(', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (020): ['sudo', '(', '.', 'format', '(', 'env', '.', 'hosts', '_', 'data', '.', 'application', '_', 'name', '(', ')', ')', ')', '\\', 'n']
Detokenized (015): ['sudo', '(', '.', 'format', '(', 'env', '.', 'hosts_data', '.', 'application_name', '(', ')', ')', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "collection_response = SharedCollectionResponse ( json . loads ( self . send ( ) . content ) ) \n"
Original    (018): ['collection_response', '=', 'SharedCollectionResponse', '(', 'json', '.', 'loads', '(', 'self', '.', 'send', '(', ')', '.', 'content', ')', ')', '\\n']
Tokenized   (025): ['<s>', 'collection', '_', 'response', '=', 'Shared', 'Collection', 'Response', '(', 'json', '.', 'loads', '(', 'self', '.', 'send', '(', ')', '.', 'content', ')', ')', '\\', 'n', '</s>']
Filtered   (023): ['collection', '_', 'response', '=', 'Shared', 'Collection', 'Response', '(', 'json', '.', 'loads', '(', 'self', '.', 'send', '(', ')', '.', 'content', ')', ')', '\\', 'n']
Detokenized (018): ['collection_response', '=', 'SharedCollectionResponse', '(', 'json', '.', 'loads', '(', 'self', '.', 'send', '(', ')', '.', 'content', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "libraries = [ "sodium" ] , \n"
Original    (007): ['libraries', '=', '[', '"sodium"', ']', ',', '\\n']
Tokenized   (014): ['<s>', 'l', 'ibraries', '=', '[', '"', 's', 'odium', '"', ']', ',', '\\', 'n', '</s>']
Filtered   (012): ['l', 'ibraries', '=', '[', '"', 's', 'odium', '"', ']', ',', '\\', 'n']
Detokenized (007): ['libraries', '=', '[', '"sodium"', ']', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "number = 2 , type = 12 , cpp_type = 9 , label = 2 , \n"
Original    (017): ['number', '=', '2', ',', 'type', '=', '12', ',', 'cpp_type', '=', '9', ',', 'label', '=', '2', ',', '\\n']
Tokenized   (023): ['<s>', 'number', '=', '2', ',', 'type', '=', '12', ',', 'c', 'pp', '_', 'type', '=', '9', ',', 'label', '=', '2', ',', '\\', 'n', '</s>']
Filtered   (021): ['number', '=', '2', ',', 'type', '=', '12', ',', 'c', 'pp', '_', 'type', '=', '9', ',', 'label', '=', '2', ',', '\\', 'n']
Detokenized (017): ['number', '=', '2', ',', 'type', '=', '12', ',', 'cpp_type', '=', '9', ',', 'label', '=', '2', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "has_default_value = False , default_value = _b ( "" ) , \n"
Original    (012): ['has_default_value', '=', 'False', ',', 'default_value', '=', '_b', '(', '""', ')', ',', '\\n']
Tokenized   (022): ['<s>', 'has', '_', 'default', '_', 'value', '=', 'False', ',', 'default', '_', 'value', '=', '_', 'b', '(', '""', ')', ',', '\\', 'n', '</s>']
Filtered   (020): ['has', '_', 'default', '_', 'value', '=', 'False', ',', 'default', '_', 'value', '=', '_', 'b', '(', '""', ')', ',', '\\', 'n']
Detokenized (012): ['has_default_value', '=', 'False', ',', 'default_value', '=', '_b', '(', '""', ')', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "PeerSeeds = _reflection . GeneratedProtocolMessageType ( , ( _message . Message , ) , dict ( \n"
Original    (017): ['PeerSeeds', '=', '_reflection', '.', 'GeneratedProtocolMessageType', '(', ',', '(', '_message', '.', 'Message', ',', ')', ',', 'dict', '(', '\\n']
Tokenized   (031): ['<s>', 'Pe', 'er', 'S', 'eeds', '=', '_', 'ref', 'lection', '.', 'Gener', 'ated', 'Prot', 'ocol', 'Message', 'Type', '(', ',', '(', '_', 'message', '.', 'Message', ',', ')', ',', 'dict', '(', '\\', 'n', '</s>']
Filtered   (029): ['Pe', 'er', 'S', 'eeds', '=', '_', 'ref', 'lection', '.', 'Gener', 'ated', 'Prot', 'ocol', 'Message', 'Type', '(', ',', '(', '_', 'message', '.', 'Message', ',', ')', ',', 'dict', '(', '\\', 'n']
Detokenized (017): ['PeerSeeds', '=', '_reflection', '.', 'GeneratedProtocolMessageType', '(', ',', '(', '_message', '.', 'Message', ',', ')', ',', 'dict', '(', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "__module__ = \n"
Original    (003): ['__module__', '=', '\\n']
Tokenized   (008): ['<s>', '__', 'module', '__', '=', '\\', 'n', '</s>']
Filtered   (006): ['__', 'module', '__', '=', '\\', 'n']
Detokenized (003): ['__module__', '=', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "tstream = BytearrayStream ( istream . read ( self . length ) ) \n"
Original    (014): ['tstream', '=', 'BytearrayStream', '(', 'istream', '.', 'read', '(', 'self', '.', 'length', ')', ')', '\\n']
Tokenized   (024): ['<s>', 't', 'stream', '=', 'By', 't', 'ear', 'ray', 'Stream', '(', 'is', 't', 'ream', '.', 'read', '(', 'self', '.', 'length', ')', ')', '\\', 'n', '</s>']
Filtered   (022): ['t', 'stream', '=', 'By', 't', 'ear', 'ray', 'Stream', '(', 'is', 't', 'ream', '.', 'read', '(', 'self', '.', 'length', ')', ')', '\\', 'n']
Detokenized (014): ['tstream', '=', 'BytearrayStream', '(', 'istream', '.', 'read', '(', 'self', '.', 'length', ')', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "opts , args = parser . parse_args ( sys . argv [ 1 : ] ) \n"
Original    (017): ['opts', ',', 'args', '=', 'parser', '.', 'parse_args', '(', 'sys', '.', 'argv', '[', '1', ':', ']', ')', '\\n']
Tokenized   (024): ['<s>', 'op', 'ts', ',', 'args', '=', 'parser', '.', 'parse', '_', 'args', '(', 'sys', '.', 'arg', 'v', '[', '1', ':', ']', ')', '\\', 'n', '</s>']
Filtered   (022): ['op', 'ts', ',', 'args', '=', 'parser', '.', 'parse', '_', 'args', '(', 'sys', '.', 'arg', 'v', '[', '1', ':', ']', ')', '\\', 'n']
Detokenized (017): ['opts', ',', 'args', '=', 'parser', '.', 'parse_args', '(', 'sys', '.', 'argv', '[', '1', ':', ']', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : ""{0}" . format ( uid ) ) \n"
Original    (008): ['"{0}"', '.', 'format', '(', 'uid', ')', ')', '\\n']
Tokenized   (015): ['<s>', '"', '{', '0', '}"', '.', 'format', '(', 'u', 'id', ')', ')', '\\', 'n', '</s>']
Filtered   (013): ['"', '{', '0', '}"', '.', 'format', '(', 'u', 'id', ')', ')', '\\', 'n']
Detokenized (008): ['"{0}"', '.', 'format', '(', 'uid', ')', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : ""located." . format ( path ) \n"
Original    (007): ['"located."', '.', 'format', '(', 'path', ')', '\\n']
Tokenized   (013): ['<s>', '"', 'l', 'ocated', '."', '.', 'format', '(', 'path', ')', '\\', 'n', '</s>']
Filtered   (011): ['"', 'l', 'ocated', '."', '.', 'format', '(', 'path', ')', '\\', 'n']
Detokenized (007): ['"located."', '.', 'format', '(', 'path', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "discover_versions . DiscoverVersionsResponsePayload , ** kwargs ) \n"
Original    (008): ['discover_versions', '.', 'DiscoverVersionsResponsePayload', ',', '**', 'kwargs', ')', '\\n']
Tokenized   (020): ['<s>', 'd', 'iscover', '_', 'versions', '.', 'Discover', 'Versions', 'Response', 'Pay', 'load', ',', '**', 'k', 'w', 'args', ')', '\\', 'n', '</s>']
Filtered   (018): ['d', 'iscover', '_', 'versions', '.', 'Discover', 'Versions', 'Response', 'Pay', 'load', ',', '**', 'k', 'w', 'args', ')', '\\', 'n']
Detokenized (008): ['discover_versions', '.', 'DiscoverVersionsResponsePayload', ',', '**', 'kwargs', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "sqltypes . Base . metadata . create_all ( self . engine ) \n"
Original    (013): ['sqltypes', '.', 'Base', '.', 'metadata', '.', 'create_all', '(', 'self', '.', 'engine', ')', '\\n']
Tokenized   (019): ['<s>', 'sql', 'types', '.', 'Base', '.', 'metadata', '.', 'create', '_', 'all', '(', 'self', '.', 'engine', ')', '\\', 'n', '</s>']
Filtered   (017): ['sql', 'types', '.', 'Base', '.', 'metadata', '.', 'create', '_', 'all', '(', 'self', '.', 'engine', ')', '\\', 'n']
Detokenized (013): ['sqltypes', '.', 'Base', '.', 'metadata', '.', 'create_all', '(', 'self', '.', 'engine', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "enums . OpaqueDataType . NONE , \n"
Original    (007): ['enums', '.', 'OpaqueDataType', '.', 'NONE', ',', '\\n']
Tokenized   (015): ['<s>', 'en', 'ums', '.', 'Op', 'aque', 'Data', 'Type', '.', 'N', 'ONE', ',', '\\', 'n', '</s>']
Filtered   (013): ['en', 'ums', '.', 'Op', 'aque', 'Data', 'Type', '.', 'N', 'ONE', ',', '\\', 'n']
Detokenized (007): ['enums', '.', 'OpaqueDataType', '.', 'NONE', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "binascii . hexlify ( self . bytes_a ) , enums . OpaqueDataType . NONE ) \n"
Original    (016): ['binascii', '.', 'hexlify', '(', 'self', '.', 'bytes_a', ')', ',', 'enums', '.', 'OpaqueDataType', '.', 'NONE', ')', '\\n']
Tokenized   (031): ['<s>', 'bin', 'as', 'ci', 'i', '.', 'hex', 'l', 'ify', '(', 'self', '.', 'bytes', '_', 'a', ')', ',', 'en', 'ums', '.', 'Op', 'aque', 'Data', 'Type', '.', 'N', 'ONE', ')', '\\', 'n', '</s>']
Filtered   (029): ['bin', 'as', 'ci', 'i', '.', 'hex', 'l', 'ify', '(', 'self', '.', 'bytes', '_', 'a', ')', ',', 'en', 'ums', '.', 'Op', 'aque', 'Data', 'Type', '.', 'N', 'ONE', ')', '\\', 'n']
Detokenized (016): ['binascii', '.', 'hexlify', '(', 'self', '.', 'bytes_a', ')', ',', 'enums', '.', 'OpaqueDataType', '.', 'NONE', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "Session = sessionmaker ( bind = self . engine ) \n"
Original    (011): ['Session', '=', 'sessionmaker', '(', 'bind', '=', 'self', '.', 'engine', ')', '\\n']
Tokenized   (015): ['<s>', 'Session', '=', 'session', 'maker', '(', 'bind', '=', 'self', '.', 'engine', ')', '\\', 'n', '</s>']
Filtered   (013): ['Session', '=', 'session', 'maker', '(', 'bind', '=', 'self', '.', 'engine', ')', '\\', 'n']
Detokenized (011): ['Session', '=', 'sessionmaker', '(', 'bind', '=', 'self', '.', 'engine', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "get_obj = session . query ( OpaqueObject ) . filter ( \n"
Original    (012): ['get_obj', '=', 'session', '.', 'query', '(', 'OpaqueObject', ')', '.', 'filter', '(', '\\n']
Tokenized   (019): ['<s>', 'get', '_', 'obj', '=', 'session', '.', 'query', '(', 'Op', 'aque', 'Object', ')', '.', 'filter', '(', '\\', 'n', '</s>']
Filtered   (017): ['get', '_', 'obj', '=', 'session', '.', 'query', '(', 'Op', 'aque', 'Object', ')', '.', 'filter', '(', '\\', 'n']
Detokenized (012): ['get_obj', '=', 'session', '.', 'query', '(', 'OpaqueObject', ')', '.', 'filter', '(', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "ManagedObject . unique_identifier == obj . unique_identifier \n"
Original    (008): ['ManagedObject', '.', 'unique_identifier', '==', 'obj', '.', 'unique_identifier', '\\n']
Tokenized   (019): ['<s>', 'Man', 'aged', 'Object', '.', 'unique', '_', 'ident', 'ifier', '==', 'obj', '.', 'unique', '_', 'ident', 'ifier', '\\', 'n', '</s>']
Filtered   (017): ['Man', 'aged', 'Object', '.', 'unique', '_', 'ident', 'ifier', '==', 'obj', '.', 'unique', '_', 'ident', 'ifier', '\\', 'n']
Detokenized (008): ['ManagedObject', '.', 'unique_identifier', '==', 'obj', '.', 'unique_identifier', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "0 ) ) \n"
Original    (004): ['0', ')', ')', '\\n']
Tokenized   (007): ['<s>', '0', ')', ')', '\\', 'n', '</s>']
Filtered   (005): ['0', ')', ')', '\\', 'n']
Detokenized (004): ['0', ')', ')', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "expected_mo_names . append ( sqltypes . ManagedObjectName ( expected_names [ 1 ] , \n"
Original    (014): ['expected_mo_names', '.', 'append', '(', 'sqltypes', '.', 'ManagedObjectName', '(', 'expected_names', '[', '1', ']', ',', '\\n']
Tokenized   (027): ['<s>', 'expected', '_', 'mo', '_', 'names', '.', 'append', '(', 'sql', 'types', '.', 'Man', 'aged', 'Object', 'Name', '(', 'expected', '_', 'names', '[', '1', ']', ',', '\\', 'n', '</s>']
Filtered   (025): ['expected', '_', 'mo', '_', 'names', '.', 'append', '(', 'sql', 'types', '.', 'Man', 'aged', 'Object', 'Name', '(', 'expected', '_', 'names', '[', '1', ']', ',', '\\', 'n']
Detokenized (014): ['expected_mo_names', '.', 'append', '(', 'sqltypes', '.', 'ManagedObjectName', '(', 'expected_names', '[', '1', ']', ',', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "expected_names = [ first_name , added_name ] \n"
Original    (008): ['expected_names', '=', '[', 'first_name', ',', 'added_name', ']', '\\n']
Tokenized   (017): ['<s>', 'expected', '_', 'names', '=', '[', 'first', '_', 'name', ',', 'added', '_', 'name', ']', '\\', 'n', '</s>']
Filtered   (015): ['expected', '_', 'names', '=', '[', 'first', '_', 'name', ',', 'added', '_', 'name', ']', '\\', 'n']
Detokenized (008): ['expected_names', '=', '[', 'first_name', ',', 'added_name', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "] } , \n"
Original    (004): [']', '}', ',', '\\n']
Tokenized   (007): ['<s>', ']', '}', ',', '\\', 'n', '</s>']
Filtered   (005): [']', '}', ',', '\\', 'n']
Detokenized (004): [']', '}', ',', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "types . MethodType ( _lib_dir_option , None , MSVCCompiler ) ) \n"
Original    (012): ['types', '.', 'MethodType', '(', '_lib_dir_option', ',', 'None', ',', 'MSVCCompiler', ')', ')', '\\n']
Tokenized   (025): ['<s>', 'types', '.', 'Method', 'Type', '(', '_', 'lib', '_', 'dir', '_', 'option', ',', 'None', ',', 'MS', 'V', 'CC', 'omp', 'iler', ')', ')', '\\', 'n', '</s>']
Filtered   (023): ['types', '.', 'Method', 'Type', '(', '_', 'lib', '_', 'dir', '_', 'option', ',', 'None', ',', 'MS', 'V', 'CC', 'omp', 'iler', ')', ')', '\\', 'n']
Detokenized (012): ['types', '.', 'MethodType', '(', '_lib_dir_option', ',', 'None', ',', 'MSVCCompiler', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "setup ( ** kwds ) \n"
Original    (006): ['setup', '(', '**', 'kwds', ')', '\\n']
Tokenized   (011): ['<s>', 'setup', '(', '**', 'k', 'w', 'ds', ')', '\\', 'n', '</s>']
Filtered   (009): ['setup', '(', '**', 'k', 'w', 'ds', ')', '\\', 'n']
Detokenized (006): ['setup', '(', '**', 'kwds', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "intersphinx_mapping = { : None } \n"
Original    (007): ['intersphinx_mapping', '=', '{', ':', 'None', '}', '\\n']
Tokenized   (015): ['<s>', 'inters', 'ph', 'inx', '_', 'm', 'apping', '=', '{', ':', 'None', '}', '\\', 'n', '</s>']
Filtered   (013): ['inters', 'ph', 'inx', '_', 'm', 'apping', '=', '{', ':', 'None', '}', '\\', 'n']
Detokenized (007): ['intersphinx_mapping', '=', '{', ':', 'None', '}', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "new_w = int ( width * wrat ) \n"
Original    (009): ['new_w', '=', 'int', '(', 'width', '*', 'wrat', ')', '\\n']
Tokenized   (015): ['<s>', 'new', '_', 'w', '=', 'int', '(', 'width', '*', 'wr', 'at', ')', '\\', 'n', '</s>']
Filtered   (013): ['new', '_', 'w', '=', 'int', '(', 'width', '*', 'wr', 'at', ')', '\\', 'n']
Detokenized (009): ['new_w', '=', 'int', '(', 'width', '*', 'wrat', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "im . getbbox ( ) , Image . BICUBIC ) \n"
Original    (011): ['im', '.', 'getbbox', '(', ')', ',', 'Image', '.', 'BICUBIC', ')', '\\n']
Tokenized   (019): ['<s>', 'im', '.', 'get', 'b', 'box', '(', ')', ',', 'Image', '.', 'B', 'IC', 'UB', 'IC', ')', '\\', 'n', '</s>']
Filtered   (017): ['im', '.', 'get', 'b', 'box', '(', ')', ',', 'Image', '.', 'B', 'IC', 'UB', 'IC', ')', '\\', 'n']
Detokenized (011): ['im', '.', 'getbbox', '(', ')', ',', 'Image', '.', 'BICUBIC', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "resize_image ( os . path . abspath ( os . path . join ( , dest + ) ) ) \n"
Original    (021): ['resize_image', '(', 'os', '.', 'path', '.', 'abspath', '(', 'os', '.', 'path', '.', 'join', '(', ',', 'dest', '+', ')', ')', ')', '\\n']
Tokenized   (028): ['<s>', 'res', 'ize', '_', 'image', '(', 'os', '.', 'path', '.', 'abs', 'path', '(', 'os', '.', 'path', '.', 'join', '(', ',', 'dest', '+', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (026): ['res', 'ize', '_', 'image', '(', 'os', '.', 'path', '.', 'abs', 'path', '(', 'os', '.', 'path', '.', 'join', '(', ',', 'dest', '+', ')', ')', ')', '\\', 'n']
Detokenized (021): ['resize_image', '(', 'os', '.', 'path', '.', 'abspath', '(', 'os', '.', 'path', '.', 'join', '(', ',', 'dest', '+', ')', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "f_x = Float ( 0.0 , iotype = "out" ) \n"
Original    (011): ['f_x', '=', 'Float', '(', '0.0', ',', 'iotype', '=', '"out"', ')', '\\n']
Tokenized   (021): ['<s>', 'f', '_', 'x', '=', 'Float', '(', '0', '.', '0', ',', 'i', 'otype', '=', '"', 'out', '"', ')', '\\', 'n', '</s>']
Filtered   (019): ['f', '_', 'x', '=', 'Float', '(', '0', '.', '0', ',', 'i', 'otype', '=', '"', 'out', '"', ')', '\\', 'n']
Detokenized (011): ['f_x', '=', 'Float', '(', '0.0', ',', 'iotype', '=', '"out"', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "doe_c = [ 0.1 , 0.2 , 0.3 , 0.5 , 0.7 , 0.8 , 0.9 ] + doe_e \n"
Original    (020): ['doe_c', '=', '[', '0.1', ',', '0.2', ',', '0.3', ',', '0.5', ',', '0.7', ',', '0.8', ',', '0.9', ']', '+', 'doe_e', '\\n']
Tokenized   (043): ['<s>', 'd', 'oe', '_', 'c', '=', '[', '0', '.', '1', ',', '0', '.', '2', ',', '0', '.', '3', ',', '0', '.', '5', ',', '0', '.', '7', ',', '0', '.', '8', ',', '0', '.', '9', ']', '+', 'do', 'e', '_', 'e', '\\', 'n', '</s>']
Filtered   (041): ['d', 'oe', '_', 'c', '=', '[', '0', '.', '1', ',', '0', '.', '2', ',', '0', '.', '3', ',', '0', '.', '5', ',', '0', '.', '7', ',', '0', '.', '8', ',', '0', '.', '9', ']', '+', 'do', 'e', '_', 'e', '\\', 'n']
Detokenized (020): ['doe_c', '=', '[', '0.1', ',', '0.2', ',', '0.3', ',', '0.5', ',', '0.7', ',', '0.8', ',', '0.9', ']', '+', 'doe_e', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "responses = ( , ) , nfi = self . nfi ) ) \n"
Original    (014): ['responses', '=', '(', ',', ')', ',', 'nfi', '=', 'self', '.', 'nfi', ')', ')', '\\n']
Tokenized   (020): ['<s>', 'respons', 'es', '=', '(', ',', ')', ',', 'n', 'fi', '=', 'self', '.', 'n', 'fi', ')', ')', '\\', 'n', '</s>']
Filtered   (018): ['respons', 'es', '=', '(', ',', ')', ',', 'n', 'fi', '=', 'self', '.', 'n', 'fi', ')', ')', '\\', 'n']
Detokenized (014): ['responses', '=', '(', ',', ')', ',', 'nfi', '=', 'self', '.', 'nfi', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "sigma_cok = np . array ( [ d . sigma for d in sim_cok . mm_checker . case_outputs . meta_model . f_x ] ) \n"
Original    (025): ['sigma_cok', '=', 'np', '.', 'array', '(', '[', 'd', '.', 'sigma', 'for', 'd', 'in', 'sim_cok', '.', 'mm_checker', '.', 'case_outputs', '.', 'meta_model', '.', 'f_x', ']', ')', '\\n']
Tokenized   (046): ['<s>', 's', 'igma', '_', 'c', 'ok', '=', 'np', '.', 'array', '(', '[', 'd', '.', 's', 'igma', 'for', 'd', 'in', 'sim', '_', 'c', 'ok', '.', 'mm', '_', 'check', 'er', '.', 'case', '_', 'output', 's', '.', 'meta', '_', 'model', '.', 'f', '_', 'x', ']', ')', '\\', 'n', '</s>']
Filtered   (044): ['s', 'igma', '_', 'c', 'ok', '=', 'np', '.', 'array', '(', '[', 'd', '.', 's', 'igma', 'for', 'd', 'in', 'sim', '_', 'c', 'ok', '.', 'mm', '_', 'check', 'er', '.', 'case', '_', 'output', 's', '.', 'meta', '_', 'model', '.', 'f', '_', 'x', ']', ')', '\\', 'n']
Detokenized (025): ['sigma_cok', '=', 'np', '.', 'array', '(', '[', 'd', '.', 'sigma', 'for', 'd', 'in', 'sim_cok', '.', 'mm_checker', '.', 'case_outputs', '.', 'meta_model', '.', 'f_x', ']', ')', '\\n']
Counter: 44
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "actual = sim_k . mm_checker . case_outputs . model . f_x \n"
Original    (012): ['actual', '=', 'sim_k', '.', 'mm_checker', '.', 'case_outputs', '.', 'model', '.', 'f_x', '\\n']
Tokenized   (025): ['<s>', 'actual', '=', 'sim', '_', 'k', '.', 'mm', '_', 'check', 'er', '.', 'case', '_', 'output', 's', '.', 'model', '.', 'f', '_', 'x', '\\', 'n', '</s>']
Filtered   (023): ['actual', '=', 'sim', '_', 'k', '.', 'mm', '_', 'check', 'er', '.', 'case', '_', 'output', 's', '.', 'model', '.', 'f', '_', 'x', '\\', 'n']
Detokenized (012): ['actual', '=', 'sim_k', '.', 'mm_checker', '.', 'case_outputs', '.', 'model', '.', 'f_x', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "predicted_cok - 2 * sigma_cok , facecolor = , alpha = 0.2 ) \n"
Original    (014): ['predicted_cok', '-', '2', '*', 'sigma_cok', ',', 'facecolor', '=', ',', 'alpha', '=', '0.2', ')', '\\n']
Tokenized   (028): ['<s>', 'pred', 'icted', '_', 'c', 'ok', '-', '2', '*', 's', 'igma', '_', 'c', 'ok', ',', 'face', 'color', '=', ',', 'alpha', '=', '0', '.', '2', ')', '\\', 'n', '</s>']
Filtered   (026): ['pred', 'icted', '_', 'c', 'ok', '-', '2', '*', 's', 'igma', '_', 'c', 'ok', ',', 'face', 'color', '=', ',', 'alpha', '=', '0', '.', '2', ')', '\\', 'n']
Detokenized (014): ['predicted_cok', '-', '2', '*', 'sigma_cok', ',', 'facecolor', '=', ',', 'alpha', '=', '0.2', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "newsetupfile = os . path . join ( os . path . dirname ( setupfile ) , \n"
Original    (018): ['newsetupfile', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dirname', '(', 'setupfile', ')', ',', '\\n']
Tokenized   (025): ['<s>', 'new', 'setup', 'file', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', 'setup', 'file', ')', ',', '\\', 'n', '</s>']
Filtered   (023): ['new', 'setup', 'file', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', 'setup', 'file', ')', ',', '\\', 'n']
Detokenized (018): ['newsetupfile', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dirname', '(', 'setupfile', ')', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "srcdir = os . path . abspath ( os . path . expanduser ( srcdir ) ) . replace ( , ) \n"
Original    (023): ['srcdir', '=', 'os', '.', 'path', '.', 'abspath', '(', 'os', '.', 'path', '.', 'expanduser', '(', 'srcdir', ')', ')', '.', 'replace', '(', ',', ')', '\\n']
Tokenized   (030): ['<s>', 'src', 'dir', '=', 'os', '.', 'path', '.', 'abs', 'path', '(', 'os', '.', 'path', '.', 'expand', 'user', '(', 'src', 'dir', ')', ')', '.', 'replace', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (028): ['src', 'dir', '=', 'os', '.', 'path', '.', 'abs', 'path', '(', 'os', '.', 'path', '.', 'expand', 'user', '(', 'src', 'dir', ')', ')', '.', 'replace', '(', ',', ')', '\\', 'n']
Detokenized (023): ['srcdir', '=', 'os', '.', 'path', '.', 'abspath', '(', 'os', '.', 'path', '.', 'expanduser', '(', 'srcdir', ')', ')', '.', 'replace', '(', ',', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "cmd . extend ( [ , destdir ] ) \n"
Original    (010): ['cmd', '.', 'extend', '(', '[', ',', 'destdir', ']', ')', '\\n']
Tokenized   (014): ['<s>', 'cmd', '.', 'extend', '(', '[', ',', 'dest', 'dir', ']', ')', '\\', 'n', '</s>']
Filtered   (012): ['cmd', '.', 'extend', '(', '[', ',', 'dest', 'dir', ']', ')', '\\', 'n']
Detokenized (010): ['cmd', '.', 'extend', '(', '[', ',', 'destdir', ']', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "list ( newfiles ) ) \n"
Original    (006): ['list', '(', 'newfiles', ')', ')', '\\n']
Tokenized   (010): ['<s>', 'list', '(', 'new', 'files', ')', ')', '\\', 'n', '</s>']
Filtered   (008): ['list', '(', 'new', 'files', ')', ')', '\\', 'n']
Detokenized (006): ['list', '(', 'newfiles', ')', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "destdir = os . path . abspath ( os . path . expanduser ( options . destdir ) ) \n"
Original    (020): ['destdir', '=', 'os', '.', 'path', '.', 'abspath', '(', 'os', '.', 'path', '.', 'expanduser', '(', 'options', '.', 'destdir', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'dest', 'dir', '=', 'os', '.', 'path', '.', 'abs', 'path', '(', 'os', '.', 'path', '.', 'expand', 'user', '(', 'options', '.', 'dest', 'dir', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['dest', 'dir', '=', 'os', '.', 'path', '.', 'abs', 'path', '(', 'os', '.', 'path', '.', 'expand', 'user', '(', 'options', '.', 'dest', 'dir', ')', ')', '\\', 'n']
Detokenized (020): ['destdir', '=', 'os', '.', 'path', '.', 'abspath', '(', 'os', '.', 'path', '.', 'expanduser', '(', 'options', '.', 'destdir', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "all_names . extend ( [ prefix + name \n"
Original    (009): ['all_names', '.', 'extend', '(', '[', 'prefix', '+', 'name', '\\n']
Tokenized   (014): ['<s>', 'all', '_', 'names', '.', 'extend', '(', '[', 'prefix', '+', 'name', '\\', 'n', '</s>']
Filtered   (012): ['all', '_', 'names', '.', 'extend', '(', '[', 'prefix', '+', 'name', '\\', 'n']
Detokenized (009): ['all_names', '.', 'extend', '(', '[', 'prefix', '+', 'name', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "lnames = [ prefix + rec for rec in driver [ ] ] \n"
Original    (014): ['lnames', '=', '[', 'prefix', '+', 'rec', 'for', 'rec', 'in', 'driver', '[', ']', ']', '\\n']
Tokenized   (018): ['<s>', 'l', 'names', '=', '[', 'prefix', '+', 'rec', 'for', 'rec', 'in', 'driver', '[', ']', ']', '\\', 'n', '</s>']
Filtered   (016): ['l', 'names', '=', '[', 'prefix', '+', 'rec', 'for', 'rec', 'in', 'driver', '[', ']', ']', '\\', 'n']
Detokenized (014): ['lnames', '=', '[', 'prefix', '+', 'rec', 'for', 'rec', 'in', 'driver', '[', ']', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "driver_grp = self . _inp [ ] [ driver_name ] \n"
Original    (011): ['driver_grp', '=', 'self', '.', '_inp', '[', ']', '[', 'driver_name', ']', '\\n']
Tokenized   (021): ['<s>', 'driver', '_', 'gr', 'p', '=', 'self', '.', '_', 'in', 'p', '[', ']', '[', 'driver', '_', 'name', ']', '\\', 'n', '</s>']
Filtered   (019): ['driver', '_', 'gr', 'p', '=', 'self', '.', '_', 'in', 'p', '[', ']', '[', 'driver', '_', 'name', ']', '\\', 'n']
Detokenized (011): ['driver_grp', '=', 'self', '.', '_inp', '[', ']', '[', 'driver_name', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "iteration_grp = self . _inp [ ] [ driver_name ] [ iteration_case_name ] \n"
Original    (014): ['iteration_grp', '=', 'self', '.', '_inp', '[', ']', '[', 'driver_name', ']', '[', 'iteration_case_name', ']', '\\n']
Tokenized   (029): ['<s>', 'iter', 'ation', '_', 'gr', 'p', '=', 'self', '.', '_', 'in', 'p', '[', ']', '[', 'driver', '_', 'name', ']', '[', 'iteration', '_', 'case', '_', 'name', ']', '\\', 'n', '</s>']
Filtered   (027): ['iter', 'ation', '_', 'gr', 'p', '=', 'self', '.', '_', 'in', 'p', '[', ']', '[', 'driver', '_', 'name', ']', '[', 'iteration', '_', 'case', '_', 'name', ']', '\\', 'n']
Detokenized (014): ['iteration_grp', '=', 'self', '.', '_inp', '[', ']', '[', 'driver_name', ']', '[', 'iteration_case_name', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "info = self . read_iteration_case_from_hdf5 ( self . _inp , driver_name , iteration_case_name ) yield info \n"
Original    (017): ['info', '=', 'self', '.', 'read_iteration_case_from_hdf5', '(', 'self', '.', '_inp', ',', 'driver_name', ',', 'iteration_case_name', ')', 'yield', 'info', '\\n']
Tokenized   (039): ['<s>', 'info', '=', 'self', '.', 'read', '_', 'iter', 'ation', '_', 'case', '_', 'from', '_', 'h', 'df', '5', '(', 'self', '.', '_', 'in', 'p', ',', 'driver', '_', 'name', ',', 'iteration', '_', 'case', '_', 'name', ')', 'yield', 'info', '\\', 'n', '</s>']
Filtered   (037): ['info', '=', 'self', '.', 'read', '_', 'iter', 'ation', '_', 'case', '_', 'from', '_', 'h', 'df', '5', '(', 'self', '.', '_', 'in', 'p', ',', 'driver', '_', 'name', ',', 'iteration', '_', 'case', '_', 'name', ')', 'yield', 'info', '\\', 'n']
Detokenized (017): ['info', '=', 'self', '.', 'read_iteration_case_from_hdf5', '(', 'self', '.', '_inp', ',', 'driver_name', ',', 'iteration_case_name', ')', 'yield', 'info', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "sleep_time = Float ( 0.0 , iotype = , desc = ) \n"
Original    (013): ['sleep_time', '=', 'Float', '(', '0.0', ',', 'iotype', '=', ',', 'desc', '=', ')', '\\n']
Tokenized   (021): ['<s>', 'sleep', '_', 'time', '=', 'Float', '(', '0', '.', '0', ',', 'i', 'otype', '=', ',', 'desc', '=', ')', '\\', 'n', '</s>']
Filtered   (019): ['sleep', '_', 'time', '=', 'Float', '(', '0', '.', '0', ',', 'i', 'otype', '=', ',', 'desc', '=', ')', '\\', 'n']
Detokenized (013): ['sleep_time', '=', 'Float', '(', '0.0', ',', 'iotype', '=', ',', 'desc', '=', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "accuracy = Float ( 1.0e-6 , iotype = , \n"
Original    (010): ['accuracy', '=', 'Float', '(', '1.0e-6', ',', 'iotype', '=', ',', '\\n']
Tokenized   (020): ['<s>', 'acc', 'uracy', '=', 'Float', '(', '1', '.', '0', 'e', '-', '6', ',', 'i', 'otype', '=', ',', '\\', 'n', '</s>']
Filtered   (018): ['acc', 'uracy', '=', 'Float', '(', '1', '.', '0', 'e', '-', '6', ',', 'i', 'otype', '=', ',', '\\', 'n']
Detokenized (010): ['accuracy', '=', 'Float', '(', '1.0e-6', ',', 'iotype', '=', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "iprint = Enum ( 0 , [ 0 , 1 , 2 , 3 ] , iotype = , \n"
Original    (020): ['iprint', '=', 'Enum', '(', '0', ',', '[', '0', ',', '1', ',', '2', ',', '3', ']', ',', 'iotype', '=', ',', '\\n']
Tokenized   (026): ['<s>', 'ip', 'rint', '=', 'En', 'um', '(', '0', ',', '[', '0', ',', '1', ',', '2', ',', '3', ']', ',', 'i', 'otype', '=', ',', '\\', 'n', '</s>']
Filtered   (024): ['ip', 'rint', '=', 'En', 'um', '(', '0', ',', '[', '0', ',', '1', ',', '2', ',', '3', ']', ',', 'i', 'otype', '=', ',', '\\', 'n']
Detokenized (020): ['iprint', '=', 'Enum', '(', '0', ',', '[', '0', ',', '1', ',', '2', ',', '3', ']', ',', 'iotype', '=', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "output_filename = Str ( , iotype = , \n"
Original    (009): ['output_filename', '=', 'Str', '(', ',', 'iotype', '=', ',', '\\n']
Tokenized   (015): ['<s>', 'output', '_', 'filename', '=', 'Str', '(', ',', 'i', 'otype', '=', ',', '\\', 'n', '</s>']
Filtered   (013): ['output', '_', 'filename', '=', 'Str', '(', ',', 'i', 'otype', '=', ',', '\\', 'n']
Detokenized (009): ['output_filename', '=', 'Str', '(', ',', 'iotype', '=', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "la = max ( m , 1 ) \n"
Original    (009): ['la', '=', 'max', '(', 'm', ',', '1', ')', '\\n']
Tokenized   (012): ['<s>', 'la', '=', 'max', '(', 'm', ',', '1', ')', '\\', 'n', '</s>']
Filtered   (010): ['la', '=', 'max', '(', 'm', ',', '1', ')', '\\', 'n']
Detokenized (009): ['la', '=', 'max', '(', 'm', ',', '1', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "gg = zeros ( [ la ] , ) \n"
Original    (010): ['gg', '=', 'zeros', '(', '[', 'la', ']', ',', ')', '\\n']
Tokenized   (014): ['<s>', 'gg', '=', 'z', 'eros', '(', '[', 'la', ']', ',', ')', '\\', 'n', '</s>']
Filtered   (012): ['gg', '=', 'z', 'eros', '(', '[', 'la', ']', ',', ')', '\\', 'n']
Detokenized (010): ['gg', '=', 'zeros', '(', '[', 'la', ']', ',', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "dg = zeros ( [ la , n + 1 ] , ) \n"
Original    (014): ['dg', '=', 'zeros', '(', '[', 'la', ',', 'n', '+', '1', ']', ',', ')', '\\n']
Tokenized   (019): ['<s>', 'd', 'g', '=', 'z', 'eros', '(', '[', 'la', ',', 'n', '+', '1', ']', ',', ')', '\\', 'n', '</s>']
Filtered   (017): ['d', 'g', '=', 'z', 'eros', '(', '[', 'la', ',', 'n', '+', '1', ']', ',', ')', '\\', 'n']
Detokenized (014): ['dg', '=', 'zeros', '(', '[', 'la', ',', 'n', '+', '1', ']', ',', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "mineq = m - meq + 2 * ( n + 1 ) \n"
Original    (014): ['mineq', '=', 'm', '-', 'meq', '+', '2', '*', '(', 'n', '+', '1', ')', '\\n']
Tokenized   (019): ['<s>', 'mine', 'q', '=', 'm', '-', 'me', 'q', '+', '2', '*', '(', 'n', '+', '1', ')', '\\', 'n', '</s>']
Filtered   (017): ['mine', 'q', '=', 'm', '-', 'me', 'q', '+', '2', '*', '(', 'n', '+', '1', ')', '\\', 'n']
Detokenized (014): ['mineq', '=', 'm', '-', 'meq', '+', '2', '*', '(', 'n', '+', '1', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "lsq = ( n + 1 ) * ( ( n + 1 ) + 1 ) + meq * ( ( n + 1 ) + 1 ) + mineq * ( ( n + 1 ) + 1 ) \n"
Original    (042): ['lsq', '=', '(', 'n', '+', '1', ')', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'meq', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'mineq', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '\\n']
Tokenized   (048): ['<s>', 'ls', 'q', '=', '(', 'n', '+', '1', ')', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'me', 'q', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'mine', 'q', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '\\', 'n', '</s>']
Filtered   (046): ['ls', 'q', '=', '(', 'n', '+', '1', ')', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'me', 'q', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'mine', 'q', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '\\', 'n']
Detokenized (042): ['lsq', '=', '(', 'n', '+', '1', ')', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'meq', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'mineq', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '\\n']
Counter: 46
===================================================================
Hidden states:  (13, 42, 768)
# Extracted words:  42
Sentence         : "lsi = ( ( n + 1 ) - meq + 1 ) * ( mineq + 2 ) + 2 * mineq \n"
Original    (024): ['lsi', '=', '(', '(', 'n', '+', '1', ')', '-', 'meq', '+', '1', ')', '*', '(', 'mineq', '+', '2', ')', '+', '2', '*', 'mineq', '\\n']
Tokenized   (031): ['<s>', 'ls', 'i', '=', '(', '(', 'n', '+', '1', ')', '-', 'me', 'q', '+', '1', ')', '*', '(', 'mine', 'q', '+', '2', ')', '+', '2', '*', 'mine', 'q', '\\', 'n', '</s>']
Filtered   (029): ['ls', 'i', '=', '(', '(', 'n', '+', '1', ')', '-', 'me', 'q', '+', '1', ')', '*', '(', 'mine', 'q', '+', '2', ')', '+', '2', '*', 'mine', 'q', '\\', 'n']
Detokenized (024): ['lsi', '=', '(', '(', 'n', '+', '1', ')', '-', 'meq', '+', '1', ')', '*', '(', 'mineq', '+', '2', ')', '+', '2', '*', 'mineq', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "lsei = ( ( n + 1 ) + mineq ) * ( ( n + 1 ) - meq ) + 2 * meq + ( n + 1 ) \n"
Original    (032): ['lsei', '=', '(', '(', 'n', '+', '1', ')', '+', 'mineq', ')', '*', '(', '(', 'n', '+', '1', ')', '-', 'meq', ')', '+', '2', '*', 'meq', '+', '(', 'n', '+', '1', ')', '\\n']
Tokenized   (039): ['<s>', 'l', 'sei', '=', '(', '(', 'n', '+', '1', ')', '+', 'mine', 'q', ')', '*', '(', '(', 'n', '+', '1', ')', '-', 'me', 'q', ')', '+', '2', '*', 'me', 'q', '+', '(', 'n', '+', '1', ')', '\\', 'n', '</s>']
Filtered   (037): ['l', 'sei', '=', '(', '(', 'n', '+', '1', ')', '+', 'mine', 'q', ')', '*', '(', '(', 'n', '+', '1', ')', '-', 'me', 'q', ')', '+', '2', '*', 'me', 'q', '+', '(', 'n', '+', '1', ')', '\\', 'n']
Detokenized (032): ['lsei', '=', '(', '(', 'n', '+', '1', ')', '+', 'mineq', ')', '*', '(', '(', 'n', '+', '1', ')', '-', 'meq', ')', '+', '2', '*', 'meq', '+', '(', 'n', '+', '1', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "slsqpb = ( n + 1 ) * ( n / 2 ) + 2 * m + 3 * n + 3 * ( n + 1 ) + 1 \n"
Original    (032): ['slsqpb', '=', '(', 'n', '+', '1', ')', '*', '(', 'n', '/', '2', ')', '+', '2', '*', 'm', '+', '3', '*', 'n', '+', '3', '*', '(', 'n', '+', '1', ')', '+', '1', '\\n']
Tokenized   (037): ['<s>', 'sl', 'sq', 'pb', '=', '(', 'n', '+', '1', ')', '*', '(', 'n', '/', '2', ')', '+', '2', '*', 'm', '+', '3', '*', 'n', '+', '3', '*', '(', 'n', '+', '1', ')', '+', '1', '\\', 'n', '</s>']
Filtered   (035): ['sl', 'sq', 'pb', '=', '(', 'n', '+', '1', ')', '*', '(', 'n', '/', '2', ')', '+', '2', '*', 'm', '+', '3', '*', 'n', '+', '3', '*', '(', 'n', '+', '1', ')', '+', '1', '\\', 'n']
Detokenized (032): ['slsqpb', '=', '(', 'n', '+', '1', ')', '*', '(', 'n', '/', '2', ')', '+', '2', '*', 'm', '+', '3', '*', 'n', '+', '3', '*', '(', 'n', '+', '1', ')', '+', '1', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "lw = lsq + lsi + lsei + slsqpb + n + m \n"
Original    (014): ['lw', '=', 'lsq', '+', 'lsi', '+', 'lsei', '+', 'slsqpb', '+', 'n', '+', 'm', '\\n']
Tokenized   (023): ['<s>', 'l', 'w', '=', 'l', 'sq', '+', 'l', 'si', '+', 'l', 'sei', '+', 'sl', 'sq', 'pb', '+', 'n', '+', 'm', '\\', 'n', '</s>']
Filtered   (021): ['l', 'w', '=', 'l', 'sq', '+', 'l', 'si', '+', 'l', 'sei', '+', 'sl', 'sq', 'pb', '+', 'n', '+', 'm', '\\', 'n']
Detokenized (014): ['lw', '=', 'lsq', '+', 'lsi', '+', 'lsei', '+', 'slsqpb', '+', 'n', '+', 'm', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "ljw = max ( mineq , ( n + 1 ) - meq ) \n"
Original    (015): ['ljw', '=', 'max', '(', 'mineq', ',', '(', 'n', '+', '1', ')', '-', 'meq', ')', '\\n']
Tokenized   (022): ['<s>', 'l', 'j', 'w', '=', 'max', '(', 'mine', 'q', ',', '(', 'n', '+', '1', ')', '-', 'me', 'q', ')', '\\', 'n', '</s>']
Filtered   (020): ['l', 'j', 'w', '=', 'max', '(', 'mine', 'q', ',', '(', 'n', '+', '1', ')', '-', 'me', 'q', ')', '\\', 'n']
Detokenized (015): ['ljw', '=', 'max', '(', 'mineq', ',', '(', 'n', '+', '1', ')', '-', 'meq', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "_iodict = { : , : } \n"
Original    (008): ['_iodict', '=', '{', ':', ',', ':', '}', '\\n']
Tokenized   (013): ['<s>', '_', 'iod', 'ict', '=', '{', ':', ',', ':', '}', '\\', 'n', '</s>']
Filtered   (011): ['_', 'iod', 'ict', '=', '{', ':', ',', ':', '}', '\\', 'n']
Detokenized (008): ['_iodict', '=', '{', ':', ',', ':', '}', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "state [ ] = { } \n"
Original    (007): ['state', '[', ']', '=', '{', '}', '\\n']
Tokenized   (010): ['<s>', 'state', '[', ']', '=', '{', '}', '\\', 'n', '</s>']
Filtered   (008): ['state', '[', ']', '=', '{', '}', '\\', 'n']
Detokenized (007): ['state', '[', ']', '=', '{', '}', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "key = ( addr_type , addr , proxy . _authkey ) \n"
Original    (012): ['key', '=', '(', 'addr_type', ',', 'addr', ',', 'proxy', '.', '_authkey', ')', '\\n']
Tokenized   (019): ['<s>', 'key', '=', '(', 'addr', '_', 'type', ',', 'addr', ',', 'proxy', '.', '_', 'auth', 'key', ')', '\\', 'n', '</s>']
Filtered   (017): ['key', '=', '(', 'addr', '_', 'type', ',', 'addr', ',', 'proxy', '.', '_', 'auth', 'key', ')', '\\', 'n']
Detokenized (012): ['key', '=', '(', 'addr_type', ',', 'addr', ',', 'proxy', '.', '_authkey', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "address = ( ip_addr , 0 ) \n"
Original    (008): ['address', '=', '(', 'ip_addr', ',', '0', ')', '\\n']
Tokenized   (013): ['<s>', 'address', '=', '(', 'ip', '_', 'addr', ',', '0', ')', '\\', 'n', '</s>']
Filtered   (011): ['address', '=', '(', 'ip', '_', 'addr', ',', '0', ')', '\\', 'n']
Detokenized (008): ['address', '=', '(', 'ip_addr', ',', '0', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "access = addr if addr_type == else addr_type \n"
Original    (009): ['access', '=', 'addr', 'if', 'addr_type', '==', 'else', 'addr_type', '\\n']
Tokenized   (016): ['<s>', 'access', '=', 'addr', 'if', 'addr', '_', 'type', '==', 'else', 'addr', '_', 'type', '\\', 'n', '</s>']
Filtered   (014): ['access', '=', 'addr', 'if', 'addr', '_', 'type', '==', 'else', 'addr', '_', 'type', '\\', 'n']
Detokenized (009): ['access', '=', 'addr', 'if', 'addr_type', '==', 'else', 'addr_type', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "manager = ObjectManager ( self , address , authkey = proxy . _authkey , \n"
Original    (015): ['manager', '=', 'ObjectManager', '(', 'self', ',', 'address', ',', 'authkey', '=', 'proxy', '.', '_authkey', ',', '\\n']
Tokenized   (022): ['<s>', 'manager', '=', 'Object', 'Manager', '(', 'self', ',', 'address', ',', 'auth', 'key', '=', 'proxy', '.', '_', 'auth', 'key', ',', '\\', 'n', '</s>']
Filtered   (020): ['manager', '=', 'Object', 'Manager', '(', 'self', ',', 'address', ',', 'auth', 'key', '=', 'proxy', '.', '_', 'auth', 'key', ',', '\\', 'n']
Detokenized (015): ['manager', '=', 'ObjectManager', '(', 'self', ',', 'address', ',', 'authkey', '=', 'proxy', '.', '_authkey', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "match_dict = self . _alltraits ( ** metadata ) \n"
Original    (010): ['match_dict', '=', 'self', '.', '_alltraits', '(', '**', 'metadata', ')', '\\n']
Tokenized   (018): ['<s>', 'match', '_', 'dict', '=', 'self', '.', '_', 'all', 'tra', 'its', '(', '**', 'metadata', ')', '\\', 'n', '</s>']
Filtered   (016): ['match', '_', 'dict', '=', 'self', '.', '_', 'all', 'tra', 'its', '(', '**', 'metadata', ')', '\\', 'n']
Detokenized (010): ['match_dict', '=', 'self', '.', '_alltraits', '(', '**', 'metadata', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "childname , _ , restofpath = traitpath . partition ( ) \n"
Original    (012): ['childname', ',', '_', ',', 'restofpath', '=', 'traitpath', '.', 'partition', '(', ')', '\\n']
Tokenized   (019): ['<s>', 'child', 'name', ',', '_', ',', 'rest', 'of', 'path', '=', 'trait', 'path', '.', 'partition', '(', ')', '\\', 'n', '</s>']
Filtered   (017): ['child', 'name', ',', '_', ',', 'rest', 'of', 'path', '=', 'trait', 'path', '.', 'partition', '(', ')', '\\', 'n']
Detokenized (012): ['childname', ',', '_', ',', 'restofpath', '=', 'traitpath', '.', 'partition', '(', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "mdict . setdefault ( , t . __class__ . __name__ ) \n"
Original    (012): ['mdict', '.', 'setdefault', '(', ',', 't', '.', '__class__', '.', '__name__', ')', '\\n']
Tokenized   (021): ['<s>', 'md', 'ict', '.', 'set', 'default', '(', ',', 't', '.', '__', 'class', '__', '.', '__', 'name', '__', ')', '\\', 'n', '</s>']
Filtered   (019): ['md', 'ict', '.', 'set', 'default', '(', ',', 't', '.', '__', 'class', '__', '.', '__', 'name', '__', ')', '\\', 'n']
Detokenized (012): ['mdict', '.', 'setdefault', '(', ',', 't', '.', '__class__', '.', '__name__', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "expr = compile ( assign , assign , mode = ) \n"
Original    (012): ['expr', '=', 'compile', '(', 'assign', ',', 'assign', ',', 'mode', '=', ')', '\\n']
Tokenized   (015): ['<s>', 'expr', '=', 'compile', '(', 'assign', ',', 'assign', ',', 'mode', '=', ')', '\\', 'n', '</s>']
Filtered   (013): ['expr', '=', 'compile', '(', 'assign', ',', 'assign', ',', 'mode', '=', ')', '\\', 'n']
Detokenized (012): ['expr', '=', 'compile', '(', 'assign', ',', 'assign', ',', 'mode', '=', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "tstamp = % ( now . year , now . month , now . day , now . hour , now . minute ) \n"
Original    (025): ['tstamp', '=', '%', '(', 'now', '.', 'year', ',', 'now', '.', 'month', ',', 'now', '.', 'day', ',', 'now', '.', 'hour', ',', 'now', '.', 'minute', ')', '\\n']
Tokenized   (030): ['<s>', 't', 'st', 'amp', '=', '%', '(', 'now', '.', 'year', ',', 'now', '.', 'month', ',', 'now', '.', 'day', ',', 'now', '.', 'hour', ',', 'now', '.', 'minute', ')', '\\', 'n', '</s>']
Filtered   (028): ['t', 'st', 'amp', '=', '%', '(', 'now', '.', 'year', ',', 'now', '.', 'month', ',', 'now', '.', 'day', ',', 'now', '.', 'hour', ',', 'now', '.', 'minute', ')', '\\', 'n']
Detokenized (025): ['tstamp', '=', '%', '(', 'now', '.', 'year', ',', 'now', '.', 'month', ',', 'now', '.', 'day', ',', 'now', '.', 'hour', ',', 'now', '.', 'minute', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "entry_pts = [ ( self , name , _get_entry_group ( self ) ) ] \n"
Original    (015): ['entry_pts', '=', '[', '(', 'self', ',', 'name', ',', '_get_entry_group', '(', 'self', ')', ')', ']', '\\n']
Tokenized   (026): ['<s>', 'entry', '_', 'pt', 's', '=', '[', '(', 'self', ',', 'name', ',', '_', 'get', '_', 'entry', '_', 'group', '(', 'self', ')', ')', ']', '\\', 'n', '</s>']
Filtered   (024): ['entry', '_', 'pt', 's', '=', '[', '(', 'self', ',', 'name', ',', '_', 'get', '_', 'entry', '_', 'group', '(', 'self', ')', ')', ']', '\\', 'n']
Detokenized (015): ['entry_pts', '=', '[', '(', 'self', ',', 'name', ',', '_get_entry_group', '(', 'self', ')', ')', ']', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "root_start = root_start + 1 if root_start >= 0 else 0 \n"
Original    (012): ['root_start', '=', 'root_start', '+', '1', 'if', 'root_start', '>=', '0', 'else', '0', '\\n']
Tokenized   (021): ['<s>', 'root', '_', 'start', '=', 'root', '_', 'start', '+', '1', 'if', 'root', '_', 'start', '>=', '0', 'else', '0', '\\', 'n', '</s>']
Filtered   (019): ['root', '_', 'start', '=', 'root', '_', 'start', '+', '1', 'if', 'root', '_', 'start', '>=', '0', 'else', '0', '\\', 'n']
Detokenized (012): ['root_start', '=', 'root_start', '+', '1', 'if', 'root_start', '>=', '0', 'else', '0', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "root_pathname += \n"
Original    (003): ['root_pathname', '+=', '\\n']
Tokenized   (009): ['<s>', 'root', '_', 'path', 'name', '+=', '\\', 'n', '</s>']
Filtered   (007): ['root', '_', 'path', 'name', '+=', '\\', 'n']
Detokenized (003): ['root_pathname', '+=', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "Container . _bases ( type ( obj ) , names ) \n"
Original    (012): ['Container', '.', '_bases', '(', 'type', '(', 'obj', ')', ',', 'names', ')', '\\n']
Tokenized   (017): ['<s>', 'Container', '.', '_', 'b', 'ases', '(', 'type', '(', 'obj', ')', ',', 'names', ')', '\\', 'n', '</s>']
Filtered   (015): ['Container', '.', '_', 'b', 'ases', '(', 'type', '(', 'obj', ')', ',', 'names', ')', '\\', 'n']
Detokenized (012): ['Container', '.', '_bases', '(', 'type', '(', 'obj', ')', ',', 'names', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "names . append ( % ( cls . __module__ , cls . __name__ ) ) \n"
Original    (016): ['names', '.', 'append', '(', '%', '(', 'cls', '.', '__module__', ',', 'cls', '.', '__name__', ')', ')', '\\n']
Tokenized   (025): ['<s>', 'names', '.', 'append', '(', '%', '(', 'cl', 's', '.', '__', 'module', '__', ',', 'cl', 's', '.', '__', 'name', '__', ')', ')', '\\', 'n', '</s>']
Filtered   (023): ['names', '.', 'append', '(', '%', '(', 'cl', 's', '.', '__', 'module', '__', ',', 'cl', 's', '.', '__', 'name', '__', ')', ')', '\\', 'n']
Detokenized (016): ['names', '.', 'append', '(', '%', '(', 'cls', '.', '__module__', ',', 'cls', '.', '__name__', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "_get_entry_group . group_map = [ \n"
Original    (006): ['_get_entry_group', '.', 'group_map', '=', '[', '\\n']
Tokenized   (016): ['<s>', '_', 'get', '_', 'entry', '_', 'group', '.', 'group', '_', 'map', '=', '[', '\\', 'n', '</s>']
Filtered   (014): ['_', 'get', '_', 'entry', '_', 'group', '.', 'group', '_', 'map', '=', '[', '\\', 'n']
Detokenized (006): ['_get_entry_group', '.', 'group_map', '=', '[', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "pprint . pprint ( dict ( [ ( n , str ( v ) ) \n"
Original    (016): ['pprint', '.', 'pprint', '(', 'dict', '(', '[', '(', 'n', ',', 'str', '(', 'v', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'pp', 'rint', '.', 'p', 'print', '(', 'dict', '(', '[', '(', 'n', ',', 'str', '(', 'v', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['pp', 'rint', '.', 'p', 'print', '(', 'dict', '(', '[', '(', 'n', ',', 'str', '(', 'v', ')', ')', '\\', 'n']
Detokenized (016): ['pprint', '.', 'pprint', '(', 'dict', '(', '[', '(', 'n', ',', 'str', '(', 'v', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "** metadata ) ] ) , \n"
Original    (007): ['**', 'metadata', ')', ']', ')', ',', '\\n']
Tokenized   (010): ['<s>', '**', 'metadata', ')', ']', ')', ',', '\\', 'n', '</s>']
Filtered   (008): ['**', 'metadata', ')', ']', ')', ',', '\\', 'n']
Detokenized (007): ['**', 'metadata', ')', ']', ')', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "io_attr [ ] = \n"
Original    (005): ['io_attr', '[', ']', '=', '\\n']
Tokenized   (010): ['<s>', 'io', '_', 'attr', '[', ']', '=', '\\', 'n', '</s>']
Filtered   (008): ['io', '_', 'attr', '[', ']', '=', '\\', 'n']
Detokenized (005): ['io_attr', '[', ']', '=', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "_redirect_streams ( ofile . fileno ( ) ) \n"
Original    (009): ['_redirect_streams', '(', 'ofile', '.', 'fileno', '(', ')', ')', '\\n']
Tokenized   (019): ['<s>', '_', 'red', 'irect', '_', 'stream', 's', '(', 'of', 'ile', '.', 'fil', 'eno', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['_', 'red', 'irect', '_', 'stream', 's', '(', 'of', 'ile', '.', 'fil', 'eno', '(', ')', ')', '\\', 'n']
Detokenized (009): ['_redirect_streams', '(', 'ofile', '.', 'fileno', '(', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "leftover = arr_size % num_divisions \n"
Original    (006): ['leftover', '=', 'arr_size', '%', 'num_divisions', '\\n']
Tokenized   (015): ['<s>', 'left', 'over', '=', 'arr', '_', 'size', '%', 'num', '_', 'div', 'isions', '\\', 'n', '</s>']
Filtered   (013): ['left', 'over', '=', 'arr', '_', 'size', '%', 'num', '_', 'div', 'isions', '\\', 'n']
Detokenized (006): ['leftover', '=', 'arr_size', '%', 'num_divisions', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "sizes [ : leftover ] += 1 \n"
Original    (008): ['sizes', '[', ':', 'leftover', ']', '+=', '1', '\\n']
Tokenized   (012): ['<s>', 's', 'izes', '[', ':', 'leftover', ']', '+=', '1', '\\', 'n', '</s>']
Filtered   (010): ['s', 'izes', '[', ':', 'leftover', ']', '+=', '1', '\\', 'n']
Detokenized (008): ['sizes', '[', ':', 'leftover', ']', '+=', '1', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "offsets [ 1 : ] = numpy . cumsum ( sizes ) [ : - 1 ] \n"
Original    (018): ['offsets', '[', '1', ':', ']', '=', 'numpy', '.', 'cumsum', '(', 'sizes', ')', '[', ':', '-', '1', ']', '\\n']
Tokenized   (025): ['<s>', 'offs', 'ets', '[', '1', ':', ']', '=', 'n', 'umpy', '.', 'c', 'ums', 'um', '(', 'sizes', ')', '[', ':', '-', '1', ']', '\\', 'n', '</s>']
Filtered   (023): ['offs', 'ets', '[', '1', ':', ']', '=', 'n', 'umpy', '.', 'c', 'ums', 'um', '(', 'sizes', ')', '[', ':', '-', '1', ']', '\\', 'n']
Detokenized (018): ['offsets', '[', '1', ':', ']', '=', 'numpy', '.', 'cumsum', '(', 'sizes', ')', '[', ':', '-', '1', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "z1 = Float ( 0. , iotype = ) \n"
Original    (010): ['z1', '=', 'Float', '(', '0.', ',', 'iotype', '=', ')', '\\n']
Tokenized   (016): ['<s>', 'z', '1', '=', 'Float', '(', '0', '.', ',', 'i', 'otype', '=', ')', '\\', 'n', '</s>']
Filtered   (014): ['z', '1', '=', 'Float', '(', '0', '.', ',', 'i', 'otype', '=', ')', '\\', 'n']
Detokenized (010): ['z1', '=', 'Float', '(', '0.', ',', 'iotype', '=', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "z_store = Array ( [ 0. , 0. ] , iotype = ) \n"
Original    (014): ['z_store', '=', 'Array', '(', '[', '0.', ',', '0.', ']', ',', 'iotype', '=', ')', '\\n']
Tokenized   (022): ['<s>', 'z', '_', 'store', '=', 'Array', '(', '[', '0', '.', ',', '0', '.', ']', ',', 'i', 'otype', '=', ')', '\\', 'n', '</s>']
Filtered   (020): ['z', '_', 'store', '=', 'Array', '(', '[', '0', '.', ',', '0', '.', ']', ',', 'i', 'otype', '=', ')', '\\', 'n']
Detokenized (014): ['z_store', '=', 'Array', '(', '[', '0.', ',', '0.', ']', ',', 'iotype', '=', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "ssa_F = Array ( [ 0.0 ] , iotype = ) \n"
Original    (012): ['ssa_F', '=', 'Array', '(', '[', '0.0', ']', ',', 'iotype', '=', ')', '\\n']
Tokenized   (021): ['<s>', 'ss', 'a', '_', 'F', '=', 'Array', '(', '[', '0', '.', '0', ']', ',', 'i', 'otype', '=', ')', '\\', 'n', '</s>']
Filtered   (019): ['ss', 'a', '_', 'F', '=', 'Array', '(', '[', '0', '.', '0', ']', ',', 'i', 'otype', '=', ')', '\\', 'n']
Detokenized (012): ['ssa_F', '=', 'Array', '(', '[', '0.0', ']', ',', 'iotype', '=', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "ssa_dG = Array ( [ [ 0.0 , 0.0 ] , [ 0.0 , 0.0 ] ] , iotype = ) \n"
Original    (022): ['ssa_dG', '=', 'Array', '(', '[', '[', '0.0', ',', '0.0', ']', ',', '[', '0.0', ',', '0.0', ']', ']', ',', 'iotype', '=', ')', '\\n']
Tokenized   (038): ['<s>', 'ss', 'a', '_', 'd', 'G', '=', 'Array', '(', '[', '[', '0', '.', '0', ',', '0', '.', '0', ']', ',', '[', '0', '.', '0', ',', '0', '.', '0', ']', ']', ',', 'i', 'otype', '=', ')', '\\', 'n', '</s>']
Filtered   (036): ['ss', 'a', '_', 'd', 'G', '=', 'Array', '(', '[', '[', '0', '.', '0', ',', '0', '.', '0', ']', ',', '[', '0', '.', '0', ',', '0', '.', '0', ']', ']', ',', 'i', 'otype', '=', ')', '\\', 'n']
Detokenized (022): ['ssa_dG', '=', 'Array', '(', '[', '[', '0.0', ',', '0.0', ']', ',', '[', '0.0', ',', '0.0', ']', ']', ',', 'iotype', '=', ')', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "arr_out = Array ( [ 1. , 2. , 3. ] , iotype = , units = ) \n"
Original    (019): ['arr_out', '=', 'Array', '(', '[', '1.', ',', '2.', ',', '3.', ']', ',', 'iotype', '=', ',', 'units', '=', ')', '\\n']
Tokenized   (028): ['<s>', 'arr', '_', 'out', '=', 'Array', '(', '[', '1', '.', ',', '2', '.', ',', '3', '.', ']', ',', 'i', 'otype', '=', ',', 'units', '=', ')', '\\', 'n', '</s>']
Filtered   (026): ['arr', '_', 'out', '=', 'Array', '(', '[', '1', '.', ',', '2', '.', ',', '3', '.', ']', ',', 'i', 'otype', '=', ',', 'units', '=', ')', '\\', 'n']
Detokenized (019): ['arr_out', '=', 'Array', '(', '[', '1.', ',', '2.', ',', '3.', ']', ',', 'iotype', '=', ',', 'units', '=', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "arg [ ] = np . array ( [ 3.1 ] ) \n"
Original    (013): ['arg', '[', ']', '=', 'np', '.', 'array', '(', '[', '3.1', ']', ')', '\\n']
Tokenized   (018): ['<s>', 'arg', '[', ']', '=', 'np', '.', 'array', '(', '[', '3', '.', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (016): ['arg', '[', ']', '=', 'np', '.', 'array', '(', '[', '3', '.', '1', ']', ')', '\\', 'n']
Detokenized (013): ['arg', '[', ']', '=', 'np', '.', 'array', '(', '[', '3.1', ']', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "jacs [ ] = np . array ( [ [ 100.0 , 101 , 102 , 103 ] , \n"
Original    (020): ['jacs', '[', ']', '=', 'np', '.', 'array', '(', '[', '[', '100.0', ',', '101', ',', '102', ',', '103', ']', ',', '\\n']
Tokenized   (026): ['<s>', 'j', 'acs', '[', ']', '=', 'np', '.', 'array', '(', '[', '[', '100', '.', '0', ',', '101', ',', '102', ',', '103', ']', ',', '\\', 'n', '</s>']
Filtered   (024): ['j', 'acs', '[', ']', '=', 'np', '.', 'array', '(', '[', '[', '100', '.', '0', ',', '101', ',', '102', ',', '103', ']', ',', '\\', 'n']
Detokenized (020): ['jacs', '[', ']', '=', 'np', '.', 'array', '(', '[', '[', '100.0', ',', '101', ',', '102', ',', '103', ']', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "assert_rel_error ( self , J [ 3 ] [ 0 ] , 3.0 , 1e-5 ) \n"
Original    (017): ['assert_rel_error', '(', 'self', ',', 'J', '[', '3', ']', '[', '0', ']', ',', '3.0', ',', '1e-5', ')', '\\n']
Tokenized   (029): ['<s>', 'assert', '_', 'rel', '_', 'error', '(', 'self', ',', 'J', '[', '3', ']', '[', '0', ']', ',', '3', '.', '0', ',', '1', 'e', '-', '5', ')', '\\', 'n', '</s>']
Filtered   (027): ['assert', '_', 'rel', '_', 'error', '(', 'self', ',', 'J', '[', '3', ']', '[', '0', ']', ',', '3', '.', '0', ',', '1', 'e', '-', '5', ')', '\\', 'n']
Detokenized (017): ['assert_rel_error', '(', 'self', ',', 'J', '[', '3', ']', '[', '0', ']', ',', '3.0', ',', '1e-5', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "newval = _getformat ( val ) % val \n"
Original    (009): ['newval', '=', '_getformat', '(', 'val', ')', '%', 'val', '\\n']
Tokenized   (015): ['<s>', 'new', 'val', '=', '_', 'get', 'format', '(', 'val', ')', '%', 'val', '\\', 'n', '</s>']
Filtered   (013): ['new', 'val', '=', '_', 'get', 'format', '(', 'val', ')', '%', 'val', '\\', 'n']
Detokenized (009): ['newval', '=', '_getformat', '(', 'val', ')', '%', 'val', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "newline = re . sub ( self . reg , sub . replace_array , line ) \n"
Original    (017): ['newline', '=', 're', '.', 'sub', '(', 'self', '.', 'reg', ',', 'sub', '.', 'replace_array', ',', 'line', ')', '\\n']
Tokenized   (023): ['<s>', 'new', 'line', '=', 're', '.', 'sub', '(', 'self', '.', 'reg', ',', 'sub', '.', 'replace', '_', 'array', ',', 'line', ')', '\\', 'n', '</s>']
Filtered   (021): ['new', 'line', '=', 're', '.', 'sub', '(', 'self', '.', 'reg', ',', 'sub', '.', 'replace', '_', 'array', ',', 'line', ')', '\\', 'n']
Detokenized (017): ['newline', '=', 're', '.', 'sub', '(', 'self', '.', 'reg', ',', 'sub', '.', 'replace_array', ',', 'line', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "fields = self . _parse_line ( ) . parseString ( line . replace ( key , "KeyField" ) ) \n"
Original    (020): ['fields', '=', 'self', '.', '_parse_line', '(', ')', '.', 'parseString', '(', 'line', '.', 'replace', '(', 'key', ',', '"KeyField"', ')', ')', '\\n']
Tokenized   (030): ['<s>', 'fields', '=', 'self', '.', '_', 'parse', '_', 'line', '(', ')', '.', 'parse', 'String', '(', 'line', '.', 'replace', '(', 'key', ',', '"', 'Key', 'Field', '"', ')', ')', '\\', 'n', '</s>']
Filtered   (028): ['fields', '=', 'self', '.', '_', 'parse', '_', 'line', '(', ')', '.', 'parse', 'String', '(', 'line', '.', 'replace', '(', 'key', ',', '"', 'Key', 'Field', '"', ')', ')', '\\', 'n']
Detokenized (020): ['fields', '=', 'self', '.', '_parse_line', '(', ')', '.', 'parseString', '(', 'line', '.', 'replace', '(', 'key', ',', '"KeyField"', ')', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "j2 = self . current_row + rowend + 1 \n"
Original    (010): ['j2', '=', 'self', '.', 'current_row', '+', 'rowend', '+', '1', '\\n']
Tokenized   (017): ['<s>', 'j', '2', '=', 'self', '.', 'current', '_', 'row', '+', 'row', 'end', '+', '1', '\\', 'n', '</s>']
Filtered   (015): ['j', '2', '=', 'self', '.', 'current', '_', 'row', '+', 'row', 'end', '+', '1', '\\', 'n']
Detokenized (010): ['j2', '=', 'self', '.', 'current_row', '+', 'rowend', '+', '1', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "ee = CaselessLiteral ( ) | CaselessLiteral ( ) \n"
Original    (010): ['ee', '=', 'CaselessLiteral', '(', ')', '|', 'CaselessLiteral', '(', ')', '\\n']
Tokenized   (021): ['<s>', 'ee', '=', 'Cas', 'eless', 'L', 'it', 'eral', '(', ')', '|', 'Cas', 'eless', 'L', 'it', 'eral', '(', ')', '\\', 'n', '</s>']
Filtered   (019): ['ee', '=', 'Cas', 'eless', 'L', 'it', 'eral', '(', ')', '|', 'Cas', 'eless', 'L', 'it', 'eral', '(', ')', '\\', 'n']
Detokenized (010): ['ee', '=', 'CaselessLiteral', '(', ')', '|', 'CaselessLiteral', '(', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "num_int = ToInteger ( Combine ( Optional ( sign ) + digits ) ) \n"
Original    (015): ['num_int', '=', 'ToInteger', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'num', '_', 'int', '=', 'To', 'Integer', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['num', '_', 'int', '=', 'To', 'Integer', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\', 'n']
Detokenized (015): ['num_int', '=', 'ToInteger', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "num_float = ToFloat ( Combine ( Optional ( sign ) + \n"
Original    (012): ['num_float', '=', 'ToFloat', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', '\\n']
Tokenized   (018): ['<s>', 'num', '_', 'float', '=', 'To', 'Float', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', '\\', 'n', '</s>']
Filtered   (016): ['num', '_', 'float', '=', 'To', 'Float', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', '\\', 'n']
Detokenized (012): ['num_float', '=', 'ToFloat', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "Optional ( ee + Optional ( sign ) + digits ) \n"
Original    (012): ['Optional', '(', 'ee', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', '\\n']
Tokenized   (016): ['<s>', 'Optional', '(', 'e', 'e', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', '\\', 'n', '</s>']
Filtered   (014): ['Optional', '(', 'e', 'e', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', '\\', 'n']
Detokenized (012): ['Optional', '(', 'ee', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "mixed_exp = ToFloat ( Combine ( digits + ee + Optional ( sign ) + digits ) ) \n"
Original    (019): ['mixed_exp', '=', 'ToFloat', '(', 'Combine', '(', 'digits', '+', 'ee', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'm', 'ixed', '_', 'exp', '=', 'To', 'Float', '(', 'Combine', '(', 'digits', '+', 'e', 'e', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['m', 'ixed', '_', 'exp', '=', 'To', 'Float', '(', 'Combine', '(', 'digits', '+', 'e', 'e', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\', 'n']
Detokenized (019): ['mixed_exp', '=', 'ToFloat', '(', 'Combine', '(', 'digits', '+', 'ee', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "string_text ) ) ) \n"
Original    (005): ['string_text', ')', ')', ')', '\\n']
Tokenized   (010): ['<s>', 'string', '_', 'text', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (008): ['string', '_', 'text', ')', ')', ')', '\\', 'n']
Detokenized (005): ['string_text', ')', ')', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "J [ , ] = - 1.0 \n"
Original    (008): ['J', '[', ',', ']', '=', '-', '1.0', '\\n']
Tokenized   (013): ['<s>', 'J', '[', ',', ']', '=', '-', '1', '.', '0', '\\', 'n', '</s>']
Filtered   (011): ['J', '[', ',', ']', '=', '-', '1', '.', '0', '\\', 'n']
Detokenized (008): ['J', '[', ',', ']', '=', '-', '1.0', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "top [ ] = - 7.0 \n"
Original    (007): ['top', '[', ']', '=', '-', '7.0', '\\n']
Tokenized   (012): ['<s>', 'top', '[', ']', '=', '-', '7', '.', '0', '\\', 'n', '</s>']
Filtered   (010): ['top', '[', ']', '=', '-', '7', '.', '0', '\\', 'n']
Detokenized (007): ['top', '[', ']', '=', '-', '7.0', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "lhs , op , rhs = _parse_constraint ( expr ) \n"
Original    (011): ['lhs', ',', 'op', ',', 'rhs', '=', '_parse_constraint', '(', 'expr', ')', '\\n']
Tokenized   (021): ['<s>', 'l', 'hs', ',', 'op', ',', 'rh', 's', '=', '_', 'parse', '_', 'con', 'str', 'aint', '(', 'expr', ')', '\\', 'n', '</s>']
Filtered   (019): ['l', 'hs', ',', 'op', ',', 'rh', 's', '=', '_', 'parse', '_', 'con', 'str', 'aint', '(', 'expr', ')', '\\', 'n']
Detokenized (011): ['lhs', ',', 'op', ',', 'rhs', '=', '_parse_constraint', '(', 'expr', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "first , second = ( rhs , lhs ) if op . startswith ( ) else ( lhs , rhs ) \n"
Original    (022): ['first', ',', 'second', '=', '(', 'rhs', ',', 'lhs', ')', 'if', 'op', '.', 'startswith', '(', ')', 'else', '(', 'lhs', ',', 'rhs', ')', '\\n']
Tokenized   (031): ['<s>', 'first', ',', 'second', '=', '(', 'rh', 's', ',', 'l', 'hs', ')', 'if', 'op', '.', 'start', 'sw', 'ith', '(', ')', 'else', '(', 'l', 'hs', ',', 'rh', 's', ')', '\\', 'n', '</s>']
Filtered   (029): ['first', ',', 'second', '=', '(', 'rh', 's', ',', 'l', 'hs', ')', 'if', 'op', '.', 'start', 'sw', 'ith', '(', ')', 'else', '(', 'l', 'hs', ',', 'rh', 's', ')', '\\', 'n']
Detokenized (022): ['first', ',', 'second', '=', '(', 'rhs', ',', 'lhs', ')', 'if', 'op', '.', 'startswith', '(', ')', 'else', '(', 'lhs', ',', 'rhs', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "input_graph . add_edges_from ( ( ( start , p ) for p in plist [ 1 : ] ) , \n"
Original    (021): ['input_graph', '.', 'add_edges_from', '(', '(', '(', 'start', ',', 'p', ')', 'for', 'p', 'in', 'plist', '[', '1', ':', ']', ')', ',', '\\n']
Tokenized   (032): ['<s>', 'input', '_', 'graph', '.', 'add', '_', 'ed', 'ges', '_', 'from', '(', '(', '(', 'start', ',', 'p', ')', 'for', 'p', 'in', 'pl', 'ist', '[', '1', ':', ']', ')', ',', '\\', 'n', '</s>']
Filtered   (030): ['input', '_', 'graph', '.', 'add', '_', 'ed', 'ges', '_', 'from', '(', '(', '(', 'start', ',', 'p', ')', 'for', 'p', 'in', 'pl', 'ist', '[', '1', ':', ']', ')', ',', '\\', 'n']
Detokenized (021): ['input_graph', '.', 'add_edges_from', '(', '(', '(', 'start', ',', 'p', ')', 'for', 'p', 'in', 'plist', '[', '1', ':', ']', ')', ',', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "src_idxs = { src : None } \n"
Original    (008): ['src_idxs', '=', '{', 'src', ':', 'None', '}', '\\n']
Tokenized   (014): ['<s>', 'src', '_', 'id', 'xs', '=', '{', 'src', ':', 'None', '}', '\\', 'n', '</s>']
Filtered   (012): ['src', '_', 'id', 'xs', '=', '{', 'src', ':', 'None', '}', '\\', 'n']
Detokenized (008): ['src_idxs', '=', '{', 'src', ':', 'None', '}', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "units = [ params_dict [ n ] . get ( ) for n in connected_inputs ] \n"
Original    (017): ['units', '=', '[', 'params_dict', '[', 'n', ']', '.', 'get', '(', ')', 'for', 'n', 'in', 'connected_inputs', ']', '\\n']
Tokenized   (025): ['<s>', 'units', '=', '[', 'params', '_', 'dict', '[', 'n', ']', '.', 'get', '(', ')', 'for', 'n', 'in', 'connected', '_', 'input', 's', ']', '\\', 'n', '</s>']
Filtered   (023): ['units', '=', '[', 'params', '_', 'dict', '[', 'n', ']', '.', 'get', '(', ')', 'for', 'n', 'in', 'connected', '_', 'input', 's', ']', '\\', 'n']
Detokenized (017): ['units', '=', '[', 'params_dict', '[', 'n', ']', '.', 'get', '(', ')', 'for', 'n', 'in', 'connected_inputs', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "vals = [ params_dict [ n ] [ ] for n in connected_inputs ] \n"
Original    (015): ['vals', '=', '[', 'params_dict', '[', 'n', ']', '[', ']', 'for', 'n', 'in', 'connected_inputs', ']', '\\n']
Tokenized   (023): ['<s>', 'vals', '=', '[', 'params', '_', 'dict', '[', 'n', ']', '[', ']', 'for', 'n', 'in', 'connected', '_', 'input', 's', ']', '\\', 'n', '</s>']
Filtered   (021): ['vals', '=', '[', 'params', '_', 'dict', '[', 'n', ']', '[', ']', 'for', 'n', 'in', 'connected', '_', 'input', 's', ']', '\\', 'n']
Detokenized (015): ['vals', '=', '[', 'params_dict', '[', 'n', ']', '[', ']', 'for', 'n', 'in', 'connected_inputs', ']', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "tname , t = connected_inputs [ i ] , u \n"
Original    (011): ['tname', ',', 't', '=', 'connected_inputs', '[', 'i', ']', ',', 'u', '\\n']
Tokenized   (018): ['<s>', 't', 'name', ',', 't', '=', 'connected', '_', 'input', 's', '[', 'i', ']', ',', 'u', '\\', 'n', '</s>']
Filtered   (016): ['t', 'name', ',', 't', '=', 'connected', '_', 'input', 's', '[', 'i', ']', ',', 'u', '\\', 'n']
Detokenized (011): ['tname', ',', 't', '=', 'connected_inputs', '[', 'i', ']', ',', 'u', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "correct_src = params_dict [ connected_inputs [ 0 ] ] [ ] \n"
Original    (012): ['correct_src', '=', 'params_dict', '[', 'connected_inputs', '[', '0', ']', ']', '[', ']', '\\n']
Tokenized   (022): ['<s>', 'correct', '_', 'src', '=', 'params', '_', 'dict', '[', 'connected', '_', 'input', 's', '[', '0', ']', ']', '[', ']', '\\', 'n', '</s>']
Filtered   (020): ['correct', '_', 'src', '=', 'params', '_', 'dict', '[', 'connected', '_', 'input', 's', '[', '0', ']', ']', '[', ']', '\\', 'n']
Detokenized (012): ['correct_src', '=', 'params_dict', '[', 'connected_inputs', '[', '0', ']', ']', '[', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "sorted ( [ ( v , k ) for k , v in forms . items ( ) ] ) ) ) \n"
Original    (023): ['sorted', '(', '[', '(', 'v', ',', 'k', ')', 'for', 'k', ',', 'v', 'in', 'forms', '.', 'items', '(', ')', ']', ')', ')', ')', '\\n']
Tokenized   (027): ['<s>', 's', 'orted', '(', '[', '(', 'v', ',', 'k', ')', 'for', 'k', ',', 'v', 'in', 'forms', '.', 'items', '(', ')', ']', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['s', 'orted', '(', '[', '(', 'v', ',', 'k', ')', 'for', 'k', ',', 'v', 'in', 'forms', '.', 'items', '(', ')', ']', ')', ')', ')', '\\', 'n']
Detokenized (023): ['sorted', '(', '[', '(', 'v', ',', 'k', ')', 'for', 'k', ',', 'v', 'in', 'forms', '.', 'items', '(', ')', ']', ')', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "full_order = { s . pathname : i for i , s in \n"
Original    (014): ['full_order', '=', '{', 's', '.', 'pathname', ':', 'i', 'for', 'i', ',', 's', 'in', '\\n']
Tokenized   (020): ['<s>', 'full', '_', 'order', '=', '{', 's', '.', 'path', 'name', ':', 'i', 'for', 'i', ',', 's', 'in', '\\', 'n', '</s>']
Filtered   (018): ['full', '_', 'order', '=', '{', 's', '.', 'path', 'name', ':', 'i', 'for', 'i', ',', 's', 'in', '\\', 'n']
Detokenized (014): ['full_order', '=', '{', 's', '.', 'pathname', ':', 'i', 'for', 'i', ',', 's', 'in', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "enumerate ( self . root . subsystems ( recurse = True ) ) } \n"
Original    (015): ['enumerate', '(', 'self', '.', 'root', '.', 'subsystems', '(', 'recurse', '=', 'True', ')', ')', '}', '\\n']
Tokenized   (022): ['<s>', 'en', 'umer', 'ate', '(', 'self', '.', 'root', '.', 'subsystem', 's', '(', 'rec', 'urse', '=', 'True', ')', ')', '}', '\\', 'n', '</s>']
Filtered   (020): ['en', 'umer', 'ate', '(', 'self', '.', 'root', '.', 'subsystem', 's', '(', 'rec', 'urse', '=', 'True', ')', ')', '}', '\\', 'n']
Detokenized (015): ['enumerate', '(', 'self', '.', 'root', '.', 'subsystems', '(', 'recurse', '=', 'True', ')', ')', '}', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "ssys = srcs [ 0 ] . rsplit ( , 1 ) [ 0 ] \n"
Original    (016): ['ssys', '=', 'srcs', '[', '0', ']', '.', 'rsplit', '(', ',', '1', ')', '[', '0', ']', '\\n']
Tokenized   (022): ['<s>', 'ss', 'ys', '=', 'src', 's', '[', '0', ']', '.', 'r', 'split', '(', ',', '1', ')', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (020): ['ss', 'ys', '=', 'src', 's', '[', '0', ']', '.', 'r', 'split', '(', ',', '1', ')', '[', '0', ']', '\\', 'n']
Detokenized (016): ['ssys', '=', 'srcs', '[', '0', ']', '.', 'rsplit', '(', ',', '1', ')', '[', '0', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "params_dict , unknowns_dict = self . root . _setup_variables ( ) \n"
Original    (012): ['params_dict', ',', 'unknowns_dict', '=', 'self', '.', 'root', '.', '_setup_variables', '(', ')', '\\n']
Tokenized   (024): ['<s>', 'params', '_', 'dict', ',', 'unknown', 's', '_', 'dict', '=', 'self', '.', 'root', '.', '_', 'setup', '_', 'vari', 'ables', '(', ')', '\\', 'n', '</s>']
Filtered   (022): ['params', '_', 'dict', ',', 'unknown', 's', '_', 'dict', '=', 'self', '.', 'root', '.', '_', 'setup', '_', 'vari', 'ables', '(', ')', '\\', 'n']
Detokenized (012): ['params_dict', ',', 'unknowns_dict', '=', 'self', '.', 'root', '.', '_setup_variables', '(', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "is not Component . setup_distrib ) ) : \n"
Original    (009): ['is', 'not', 'Component', '.', 'setup_distrib', ')', ')', ':', '\\n']
Tokenized   (015): ['<s>', 'is', 'not', 'Component', '.', 'setup', '_', 'dist', 'rib', ')', ')', ':', '\\', 'n', '</s>']
Filtered   (013): ['is', 'not', 'Component', '.', 'setup', '_', 'dist', 'rib', ')', ')', ':', '\\', 'n']
Detokenized (009): ['is', 'not', 'Component', '.', 'setup_distrib', ')', ')', ':', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "alloc_derivs = not self . root . fd_options [ ] \n"
Original    (011): ['alloc_derivs', '=', 'not', 'self', '.', 'root', '.', 'fd_options', '[', ']', '\\n']
Tokenized   (021): ['<s>', 'alloc', '_', 'der', 'iv', 's', '=', 'not', 'self', '.', 'root', '.', 'f', 'd', '_', 'options', '[', ']', '\\', 'n', '</s>']
Filtered   (019): ['alloc', '_', 'der', 'iv', 's', '=', 'not', 'self', '.', 'root', '.', 'f', 'd', '_', 'options', '[', ']', '\\', 'n']
Detokenized (011): ['alloc_derivs', '=', 'not', 'self', '.', 'root', '.', 'fd_options', '[', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "dangling_params = sorted ( set ( [ \n"
Original    (008): ['dangling_params', '=', 'sorted', '(', 'set', '(', '[', '\\n']
Tokenized   (014): ['<s>', 'd', 'angling', '_', 'params', '=', 'sorted', '(', 'set', '(', '[', '\\', 'n', '</s>']
Filtered   (012): ['d', 'angling', '_', 'params', '=', 'sorted', '(', 'set', '(', '[', '\\', 'n']
Detokenized (008): ['dangling_params', '=', 'sorted', '(', 'set', '(', '[', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "nocomps = sorted ( [ c . pathname for c in self . root . components ( recurse = True , \n"
Original    (022): ['nocomps', '=', 'sorted', '(', '[', 'c', '.', 'pathname', 'for', 'c', 'in', 'self', '.', 'root', '.', 'components', '(', 'recurse', '=', 'True', ',', '\\n']
Tokenized   (029): ['<s>', 'n', 'ocom', 'ps', '=', 'sorted', '(', '[', 'c', '.', 'path', 'name', 'for', 'c', 'in', 'self', '.', 'root', '.', 'components', '(', 'rec', 'urse', '=', 'True', ',', '\\', 'n', '</s>']
Filtered   (027): ['n', 'ocom', 'ps', '=', 'sorted', '(', '[', 'c', '.', 'path', 'name', 'for', 'c', 'in', 'self', '.', 'root', '.', 'components', '(', 'rec', 'urse', '=', 'True', ',', '\\', 'n']
Detokenized (022): ['nocomps', '=', 'sorted', '(', '[', 'c', '.', 'pathname', 'for', 'c', 'in', 'self', '.', 'root', '.', 'components', '(', 'recurse', '=', 'True', ',', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "local = True ) \n"
Original    (005): ['local', '=', 'True', ')', '\\n']
Tokenized   (008): ['<s>', 'local', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (006): ['local', '=', 'True', ')', '\\', 'n']
Detokenized (005): ['local', '=', 'True', ')', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "recorders . extend ( grp . ln_solver . recorders ) \n"
Original    (011): ['recorders', '.', 'extend', '(', 'grp', '.', 'ln_solver', '.', 'recorders', ')', '\\n']
Tokenized   (021): ['<s>', 'rec', 'orders', '.', 'extend', '(', 'gr', 'p', '.', 'l', 'n', '_', 's', 'olver', '.', 'record', 'ers', ')', '\\', 'n', '</s>']
Filtered   (019): ['rec', 'orders', '.', 'extend', '(', 'gr', 'p', '.', 'l', 'n', '_', 's', 'olver', '.', 'record', 'ers', ')', '\\', 'n']
Detokenized (011): ['recorders', '.', 'extend', '(', 'grp', '.', 'ln_solver', '.', 'recorders', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "conn_comps . update ( [ s . rsplit ( , 1 ) [ 0 ] \n"
Original    (016): ['conn_comps', '.', 'update', '(', '[', 's', '.', 'rsplit', '(', ',', '1', ')', '[', '0', ']', '\\n']
Tokenized   (023): ['<s>', 'conn', '_', 'com', 'ps', '.', 'update', '(', '[', 's', '.', 'r', 'split', '(', ',', '1', ')', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (021): ['conn', '_', 'com', 'ps', '.', 'update', '(', '[', 's', '.', 'r', 'split', '(', ',', '1', ')', '[', '0', ']', '\\', 'n']
Detokenized (016): ['conn_comps', '.', 'update', '(', '[', 's', '.', 'rsplit', '(', ',', '1', ')', '[', '0', ']', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "noconn_comps = sorted ( [ c . pathname \n"
Original    (009): ['noconn_comps', '=', 'sorted', '(', '[', 'c', '.', 'pathname', '\\n']
Tokenized   (018): ['<s>', 'n', 'ocon', 'n', '_', 'com', 'ps', '=', 'sorted', '(', '[', 'c', '.', 'path', 'name', '\\', 'n', '</s>']
Filtered   (016): ['n', 'ocon', 'n', '_', 'com', 'ps', '=', 'sorted', '(', '[', 'c', '.', 'path', 'name', '\\', 'n']
Detokenized (009): ['noconn_comps', '=', 'sorted', '(', '[', 'c', '.', 'pathname', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "strong = [ s for s in nx . strongly_connected_components ( graph ) \n"
Original    (014): ['strong', '=', '[', 's', 'for', 's', 'in', 'nx', '.', 'strongly_connected_components', '(', 'graph', ')', '\\n']
Tokenized   (023): ['<s>', 'strong', '=', '[', 's', 'for', 's', 'in', 'n', 'x', '.', 'strongly', '_', 'connected', '_', 'comp', 'onents', '(', 'graph', ')', '\\', 'n', '</s>']
Filtered   (021): ['strong', '=', '[', 's', 'for', 's', 'in', 'n', 'x', '.', 'strongly', '_', 'connected', '_', 'comp', 'onents', '(', 'graph', ')', '\\', 'n']
Detokenized (014): ['strong', '=', '[', 's', 'for', 's', 'in', 'nx', '.', 'strongly_connected_components', '(', 'graph', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "subs = [ s for s in grp . _subsystems ] \n"
Original    (012): ['subs', '=', '[', 's', 'for', 's', 'in', 'grp', '.', '_subsystems', ']', '\\n']
Tokenized   (020): ['<s>', 'sub', 's', '=', '[', 's', 'for', 's', 'in', 'gr', 'p', '.', '_', 'sub', 'system', 's', ']', '\\', 'n', '</s>']
Filtered   (018): ['sub', 's', '=', '[', 's', 'for', 's', 'in', 'gr', 'p', '.', '_', 'sub', 'system', 's', ']', '\\', 'n']
Detokenized (012): ['subs', '=', '[', 's', 'for', 's', 'in', 'grp', '.', '_subsystems', ']', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "tups = sorted ( [ ( subs . index ( s ) , s ) for s in relstrong [ - 1 ] ] ) \n"
Original    (026): ['tups', '=', 'sorted', '(', '[', '(', 'subs', '.', 'index', '(', 's', ')', ',', 's', ')', 'for', 's', 'in', 'relstrong', '[', '-', '1', ']', ']', ')', '\\n']
Tokenized   (031): ['<s>', 't', 'ups', '=', 'sorted', '(', '[', '(', 'subs', '.', 'index', '(', 's', ')', ',', 's', ')', 'for', 's', 'in', 'rel', 'strong', '[', '-', '1', ']', ']', ')', '\\', 'n', '</s>']
Filtered   (029): ['t', 'ups', '=', 'sorted', '(', '[', '(', 'subs', '.', 'index', '(', 's', ')', ',', 's', ')', 'for', 's', 'in', 'rel', 'strong', '[', '-', '1', ']', ']', ')', '\\', 'n']
Detokenized (026): ['tups', '=', 'sorted', '(', '[', '(', 'subs', '.', 'index', '(', 's', ')', ',', 's', ')', 'for', 's', 'in', 'relstrong', '[', '-', '1', ']', ']', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "relstrong [ - 1 ] = [ t [ 1 ] for t in tups ] \n"
Original    (017): ['relstrong', '[', '-', '1', ']', '=', '[', 't', '[', '1', ']', 'for', 't', 'in', 'tups', ']', '\\n']
Tokenized   (022): ['<s>', 'rel', 'strong', '[', '-', '1', ']', '=', '[', 't', '[', '1', ']', 'for', 't', 'in', 't', 'ups', ']', '\\', 'n', '</s>']
Filtered   (020): ['rel', 'strong', '[', '-', '1', ']', '=', '[', 't', '[', '1', ']', 'for', 't', 'in', 't', 'ups', ']', '\\', 'n']
Detokenized (017): ['relstrong', '[', '-', '1', ']', '=', '[', 't', '[', '1', ']', 'for', 't', 'in', 'tups', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "nearest_child ( grp . pathname , n ) for n in out_of_order [ name ] \n"
Original    (016): ['nearest_child', '(', 'grp', '.', 'pathname', ',', 'n', ')', 'for', 'n', 'in', 'out_of_order', '[', 'name', ']', '\\n']
Tokenized   (028): ['<s>', 'ne', 'arest', '_', 'child', '(', 'gr', 'p', '.', 'path', 'name', ',', 'n', ')', 'for', 'n', 'in', 'out', '_', 'of', '_', 'order', '[', 'name', ']', '\\', 'n', '</s>']
Filtered   (026): ['ne', 'arest', '_', 'child', '(', 'gr', 'p', '.', 'path', 'name', ',', 'n', ')', 'for', 'n', 'in', 'out', '_', 'of', '_', 'order', '[', 'name', ']', '\\', 'n']
Detokenized (016): ['nearest_child', '(', 'grp', '.', 'pathname', ',', 'n', ')', 'for', 'n', 'in', 'out_of_order', '[', 'name', ']', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "pbos = [ var for var in vec if vec . metadata ( var ) . get ( ) ] \n"
Original    (021): ['pbos', '=', '[', 'var', 'for', 'var', 'in', 'vec', 'if', 'vec', '.', 'metadata', '(', 'var', ')', '.', 'get', '(', ')', ']', '\\n']
Tokenized   (025): ['<s>', 'p', 'bos', '=', '[', 'var', 'for', 'var', 'in', 'vec', 'if', 'vec', '.', 'metadata', '(', 'var', ')', '.', 'get', '(', ')', ']', '\\', 'n', '</s>']
Filtered   (023): ['p', 'bos', '=', '[', 'var', 'for', 'var', 'in', 'vec', 'if', 'vec', '.', 'metadata', '(', 'var', ')', '.', 'get', '(', ')', ']', '\\', 'n']
Detokenized (021): ['pbos', '=', '[', 'var', 'for', 'var', 'in', 'vec', 'if', 'vec', '.', 'metadata', '(', 'var', ')', '.', 'get', '(', ')', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "iteritems ( self . root . _params_dict ) ) : \n"
Original    (011): ['iteritems', '(', 'self', '.', 'root', '.', '_params_dict', ')', ')', ':', '\\n']
Tokenized   (018): ['<s>', 'iter', 'items', '(', 'self', '.', 'root', '.', '_', 'params', '_', 'dict', ')', ')', ':', '\\', 'n', '</s>']
Filtered   (016): ['iter', 'items', '(', 'self', '.', 'root', '.', '_', 'params', '_', 'dict', ')', ')', ':', '\\', 'n']
Detokenized (011): ['iteritems', '(', 'self', '.', 'root', '.', '_params_dict', ')', ')', ':', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "dv_scale = None , cn_scale = None , sparsity = None ) : \n"
Original    (014): ['dv_scale', '=', 'None', ',', 'cn_scale', '=', 'None', ',', 'sparsity', '=', 'None', ')', ':', '\\n']
Tokenized   (024): ['<s>', 'd', 'v', '_', 'scale', '=', 'None', ',', 'c', 'n', '_', 'scale', '=', 'None', ',', 'sp', 'arsity', '=', 'None', ')', ':', '\\', 'n', '</s>']
Filtered   (022): ['d', 'v', '_', 'scale', '=', 'None', ',', 'c', 'n', '_', 'scale', '=', 'None', ',', 'sp', 'arsity', '=', 'None', ')', ':', '\\', 'n']
Detokenized (014): ['dv_scale', '=', 'None', ',', 'cn_scale', '=', 'None', ',', 'sparsity', '=', 'None', ')', ':', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "fd_unknowns = [ var for var in unknown_list if var not in indep_list ] \n"
Original    (015): ['fd_unknowns', '=', '[', 'var', 'for', 'var', 'in', 'unknown_list', 'if', 'var', 'not', 'in', 'indep_list', ']', '\\n']
Tokenized   (026): ['<s>', 'fd', '_', 'unknown', 's', '=', '[', 'var', 'for', 'var', 'in', 'unknown', '_', 'list', 'if', 'var', 'not', 'in', 'ind', 'ep', '_', 'list', ']', '\\', 'n', '</s>']
Filtered   (024): ['fd', '_', 'unknown', 's', '=', '[', 'var', 'for', 'var', 'in', 'unknown', '_', 'list', 'if', 'var', 'not', 'in', 'ind', 'ep', '_', 'list', ']', '\\', 'n']
Detokenized (015): ['fd_unknowns', '=', '[', 'var', 'for', 'var', 'in', 'unknown_list', 'if', 'var', 'not', 'in', 'indep_list', ']', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "usize += len ( idx ) \n"
Original    (007): ['usize', '+=', 'len', '(', 'idx', ')', '\\n']
Tokenized   (012): ['<s>', 'us', 'ize', '+=', 'len', '(', 'id', 'x', ')', '\\', 'n', '</s>']
Filtered   (010): ['us', 'ize', '+=', 'len', '(', 'id', 'x', ')', '\\', 'n']
Detokenized (007): ['usize', '+=', 'len', '(', 'idx', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "fwd = mode == \n"
Original    (005): ['fwd', '=', 'mode', '==', '\\n']
Tokenized   (009): ['<s>', 'f', 'wd', '=', 'mode', '==', '\\', 'n', '</s>']
Filtered   (007): ['f', 'wd', '=', 'mode', '==', '\\', 'n']
Detokenized (005): ['fwd', '=', 'mode', '==', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "poi_indices , qoi_indices = self . _poi_indices , self . _qoi_indices \n"
Original    (012): ['poi_indices', ',', 'qoi_indices', '=', 'self', '.', '_poi_indices', ',', 'self', '.', '_qoi_indices', '\\n']
Tokenized   (033): ['<s>', 'po', 'i', '_', 'ind', 'ices', ',', 'q', 'oi', '_', 'ind', 'ices', '=', 'self', '.', '_', 'po', 'i', '_', 'ind', 'ices', ',', 'self', '.', '_', 'q', 'oi', '_', 'ind', 'ices', '\\', 'n', '</s>']
Filtered   (031): ['po', 'i', '_', 'ind', 'ices', ',', 'q', 'oi', '_', 'ind', 'ices', '=', 'self', '.', '_', 'po', 'i', '_', 'ind', 'ices', ',', 'self', '.', '_', 'q', 'oi', '_', 'ind', 'ices', '\\', 'n']
Detokenized (012): ['poi_indices', ',', 'qoi_indices', '=', 'self', '.', '_poi_indices', ',', 'self', '.', '_qoi_indices', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "in_scale , un_scale = cn_scale , dv_scale \n"
Original    (008): ['in_scale', ',', 'un_scale', '=', 'cn_scale', ',', 'dv_scale', '\\n']
Tokenized   (021): ['<s>', 'in', '_', 'scale', ',', 'un', '_', 'scale', '=', 'c', 'n', '_', 'scale', ',', 'd', 'v', '_', 'scale', '\\', 'n', '</s>']
Filtered   (019): ['in', '_', 'scale', ',', 'un', '_', 'scale', '=', 'c', 'n', '_', 'scale', ',', 'd', 'v', '_', 'scale', '\\', 'n']
Detokenized (008): ['in_scale', ',', 'un_scale', '=', 'cn_scale', ',', 'dv_scale', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "duvec = self . root . dumat [ vkey ] \n"
Original    (011): ['duvec', '=', 'self', '.', 'root', '.', 'dumat', '[', 'vkey', ']', '\\n']
Tokenized   (017): ['<s>', 'du', 'vec', '=', 'self', '.', 'root', '.', 'd', 'umat', '[', 'v', 'key', ']', '\\', 'n', '</s>']
Filtered   (015): ['du', 'vec', '=', 'self', '.', 'root', '.', 'd', 'umat', '[', 'v', 'key', ']', '\\', 'n']
Detokenized (011): ['duvec', '=', 'self', '.', 'root', '.', 'dumat', '[', 'vkey', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "rhs [ vkey ] [ : ] = 0.0 \n"
Original    (010): ['rhs', '[', 'vkey', ']', '[', ':', ']', '=', '0.0', '\\n']
Tokenized   (017): ['<s>', 'r', 'hs', '[', 'v', 'key', ']', '[', ':', ']', '=', '0', '.', '0', '\\', 'n', '</s>']
Filtered   (015): ['r', 'hs', '[', 'v', 'key', ']', '[', ':', ']', '=', '0', '.', '0', '\\', 'n']
Detokenized (010): ['rhs', '[', 'vkey', ']', '[', ':', ']', '=', '0.0', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "isinstance ( self . root . ln_solver , LinearGaussSeidel ) ) : \n"
Original    (013): ['isinstance', '(', 'self', '.', 'root', '.', 'ln_solver', ',', 'LinearGaussSeidel', ')', ')', ':', '\\n']
Tokenized   (025): ['<s>', 'is', 'instance', '(', 'self', '.', 'root', '.', 'l', 'n', '_', 's', 'olver', ',', 'Linear', 'Ga', 'uss', 'Se', 'idel', ')', ')', ':', '\\', 'n', '</s>']
Filtered   (023): ['is', 'instance', '(', 'self', '.', 'root', '.', 'l', 'n', '_', 's', 'olver', ',', 'Linear', 'Ga', 'uss', 'Se', 'idel', ')', ')', ':', '\\', 'n']
Detokenized (013): ['isinstance', '(', 'self', '.', 'root', '.', 'ln_solver', ',', 'LinearGaussSeidel', ')', ')', ':', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "unkn_list = [ item for item in dunknowns if not dunknowns . metadata ( item ) . get ( ) ] \n"
Original    (022): ['unkn_list', '=', '[', 'item', 'for', 'item', 'in', 'dunknowns', 'if', 'not', 'dunknowns', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\n']
Tokenized   (032): ['<s>', 'unk', 'n', '_', 'list', '=', '[', 'item', 'for', 'item', 'in', 'd', 'unknown', 's', 'if', 'not', 'd', 'unknown', 's', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\', 'n', '</s>']
Filtered   (030): ['unk', 'n', '_', 'list', '=', '[', 'item', 'for', 'item', 'in', 'd', 'unknown', 's', 'if', 'not', 'd', 'unknown', 's', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\', 'n']
Detokenized (022): ['unkn_list', '=', '[', 'item', 'for', 'item', 'in', 'dunknowns', 'if', 'not', 'dunknowns', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "p_size = np . size ( dinputs [ p_name ] ) \n"
Original    (012): ['p_size', '=', 'np', '.', 'size', '(', 'dinputs', '[', 'p_name', ']', ')', '\\n']
Tokenized   (021): ['<s>', 'p', '_', 'size', '=', 'np', '.', 'size', '(', 'd', 'input', 's', '[', 'p', '_', 'name', ']', ')', '\\', 'n', '</s>']
Filtered   (019): ['p', '_', 'size', '=', 'np', '.', 'size', '(', 'd', 'input', 's', '[', 'p', '_', 'name', ']', ')', '\\', 'n']
Detokenized (012): ['p_size', '=', 'np', '.', 'size', '(', 'dinputs', '[', 'p_name', ']', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "dresids . _dat [ u_name ] . val [ idx ] = 1.0 \n"
Original    (014): ['dresids', '.', '_dat', '[', 'u_name', ']', '.', 'val', '[', 'idx', ']', '=', '1.0', '\\n']
Tokenized   (025): ['<s>', 'd', 'res', 'ids', '.', '_', 'dat', '[', 'u', '_', 'name', ']', '.', 'val', '[', 'id', 'x', ']', '=', '1', '.', '0', '\\', 'n', '</s>']
Filtered   (023): ['d', 'res', 'ids', '.', '_', 'dat', '[', 'u', '_', 'name', ']', '.', 'val', '[', 'id', 'x', ']', '=', '1', '.', '0', '\\', 'n']
Detokenized (014): ['dresids', '.', '_dat', '[', 'u_name', ']', '.', 'val', '[', 'idx', ']', '=', '1.0', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "dunknowns , dresids , ) \n"
Original    (006): ['dunknowns', ',', 'dresids', ',', ')', '\\n']
Tokenized   (013): ['<s>', 'd', 'unknown', 's', ',', 'd', 'res', 'ids', ',', ')', '\\', 'n', '</s>']
Filtered   (011): ['d', 'unknown', 's', ',', 'd', 'res', 'ids', ',', ')', '\\', 'n']
Detokenized (006): ['dunknowns', ',', 'dresids', ',', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "jac_rev [ ( u_name , p_name ) ] [ idx , : ] = dinputs . _dat [ p_name ] . val \n"
Original    (023): ['jac_rev', '[', '(', 'u_name', ',', 'p_name', ')', ']', '[', 'idx', ',', ':', ']', '=', 'dinputs', '.', '_dat', '[', 'p_name', ']', '.', 'val', '\\n']
Tokenized   (038): ['<s>', 'jac', '_', 'rev', '[', '(', 'u', '_', 'name', ',', 'p', '_', 'name', ')', ']', '[', 'id', 'x', ',', ':', ']', '=', 'd', 'input', 's', '.', '_', 'dat', '[', 'p', '_', 'name', ']', '.', 'val', '\\', 'n', '</s>']
Filtered   (036): ['jac', '_', 'rev', '[', '(', 'u', '_', 'name', ',', 'p', '_', 'name', ')', ']', '[', 'id', 'x', ',', ':', ']', '=', 'd', 'input', 's', '.', '_', 'dat', '[', 'p', '_', 'name', ']', '.', 'val', '\\', 'n']
Detokenized (023): ['jac_rev', '[', '(', 'u_name', ',', 'p_name', ')', ']', '[', 'idx', ',', ':', ']', '=', 'dinputs', '.', '_dat', '[', 'p_name', ']', '.', 'val', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "c_name = cname , jac_fd2 = jac_fd2 , fd_desc = fd_desc , \n"
Original    (013): ['c_name', '=', 'cname', ',', 'jac_fd2', '=', 'jac_fd2', ',', 'fd_desc', '=', 'fd_desc', ',', '\\n']
Tokenized   (033): ['<s>', 'c', '_', 'name', '=', 'c', 'name', ',', 'j', 'ac', '_', 'fd', '2', '=', 'j', 'ac', '_', 'fd', '2', ',', 'f', 'd', '_', 'desc', '=', 'f', 'd', '_', 'desc', ',', '\\', 'n', '</s>']
Filtered   (031): ['c', '_', 'name', '=', 'c', 'name', ',', 'j', 'ac', '_', 'fd', '2', '=', 'j', 'ac', '_', 'fd', '2', ',', 'f', 'd', '_', 'desc', '=', 'f', 'd', '_', 'desc', ',', '\\', 'n']
Detokenized (013): ['c_name', '=', 'cname', ',', 'jac_fd2', '=', 'jac_fd2', ',', 'fd_desc', '=', 'fd_desc', ',', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "param_srcs = [ root . connections [ p ] for p in abs_indep_list if not root . _params_dict [ p ] . get ( ) ] \n"
Original    (027): ['param_srcs', '=', '[', 'root', '.', 'connections', '[', 'p', ']', 'for', 'p', 'in', 'abs_indep_list', 'if', 'not', 'root', '.', '_params_dict', '[', 'p', ']', '.', 'get', '(', ')', ']', '\\n']
Tokenized   (041): ['<s>', 'param', '_', 'src', 's', '=', '[', 'root', '.', 'connections', '[', 'p', ']', 'for', 'p', 'in', 'abs', '_', 'ind', 'ep', '_', 'list', 'if', 'not', 'root', '.', '_', 'params', '_', 'dict', '[', 'p', ']', '.', 'get', '(', ')', ']', '\\', 'n', '</s>']
Filtered   (039): ['param', '_', 'src', 's', '=', '[', 'root', '.', 'connections', '[', 'p', ']', 'for', 'p', 'in', 'abs', '_', 'ind', 'ep', '_', 'list', 'if', 'not', 'root', '.', '_', 'params', '_', 'dict', '[', 'p', ']', '.', 'get', '(', ')', ']', '\\', 'n']
Detokenized (027): ['param_srcs', '=', '[', 'root', '.', 'connections', '[', 'p', ']', 'for', 'p', 'in', 'abs_indep_list', 'if', 'not', 'root', '.', '_params_dict', '[', 'p', ']', '.', 'get', '(', ')', ']', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "to_prom_name [ p ] for p , idxs in param_srcs \n"
Original    (011): ['to_prom_name', '[', 'p', ']', 'for', 'p', ',', 'idxs', 'in', 'param_srcs', '\\n']
Tokenized   (022): ['<s>', 'to', '_', 'prom', '_', 'name', '[', 'p', ']', 'for', 'p', ',', 'id', 'xs', 'in', 'param', '_', 'src', 's', '\\', 'n', '</s>']
Filtered   (020): ['to', '_', 'prom', '_', 'name', '[', 'p', ']', 'for', 'p', ',', 'id', 'xs', 'in', 'param', '_', 'src', 's', '\\', 'n']
Detokenized (011): ['to_prom_name', '[', 'p', ']', 'for', 'p', ',', 'idxs', 'in', 'param_srcs', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "unknown_list = [ item for item in unknown_list if not root . unknowns . metadata ( item ) . get ( ) ] \n"
Original    (024): ['unknown_list', '=', '[', 'item', 'for', 'item', 'in', 'unknown_list', 'if', 'not', 'root', '.', 'unknowns', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\n']
Tokenized   (032): ['<s>', 'unknown', '_', 'list', '=', '[', 'item', 'for', 'item', 'in', 'unknown', '_', 'list', 'if', 'not', 'root', '.', 'unknown', 's', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\', 'n', '</s>']
Filtered   (030): ['unknown', '_', 'list', '=', '[', 'item', 'for', 'item', 'in', 'unknown', '_', 'list', 'if', 'not', 'root', '.', 'unknown', 's', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\', 'n']
Detokenized (024): ['unknown_list', '=', '[', 'item', 'for', 'item', 'in', 'unknown_list', 'if', 'not', 'root', '.', 'unknowns', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "_assemble_deriv_data ( indep_list , unknown_list , data , \n"
Original    (009): ['_assemble_deriv_data', '(', 'indep_list', ',', 'unknown_list', ',', 'data', ',', '\\n']
Tokenized   (024): ['<s>', '_', 'as', 'semble', '_', 'der', 'iv', '_', 'data', '(', 'ind', 'ep', '_', 'list', ',', 'unknown', '_', 'list', ',', 'data', ',', '\\', 'n', '</s>']
Filtered   (022): ['_', 'as', 'semble', '_', 'der', 'iv', '_', 'data', '(', 'ind', 'ep', '_', 'list', ',', 'unknown', '_', 'list', ',', 'data', ',', '\\', 'n']
Detokenized (009): ['_assemble_deriv_data', '(', 'indep_list', ',', 'unknown_list', ',', 'data', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "_both_names ( tmeta , to_prom_name ) ) \n"
Original    (008): ['_both_names', '(', 'tmeta', ',', 'to_prom_name', ')', ')', '\\n']
Tokenized   (019): ['<s>', '_', 'both', '_', 'names', '(', 't', 'meta', ',', 'to', '_', 'prom', '_', 'name', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['_', 'both', '_', 'names', '(', 't', 'meta', ',', 'to', '_', 'prom', '_', 'name', ')', ')', '\\', 'n']
Detokenized (008): ['_both_names', '(', 'tmeta', ',', 'to_prom_name', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "abs_unames = self . root . _sysdata . to_abs_uname \n"
Original    (010): ['abs_unames', '=', 'self', '.', 'root', '.', '_sysdata', '.', 'to_abs_uname', '\\n']
Tokenized   (023): ['<s>', 'abs', '_', 'un', 'ames', '=', 'self', '.', 'root', '.', '_', 'sys', 'data', '.', 'to', '_', 'abs', '_', 'un', 'ame', '\\', 'n', '</s>']
Filtered   (021): ['abs', '_', 'un', 'ames', '=', 'self', '.', 'root', '.', '_', 'sys', 'data', '.', 'to', '_', 'abs', '_', 'un', 'ame', '\\', 'n']
Detokenized (010): ['abs_unames', '=', 'self', '.', 'root', '.', '_sysdata', '.', 'to_abs_uname', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "out_str = tmp1 . format ( _pad_name ( ) , _pad_name ( ) , \n"
Original    (015): ['out_str', '=', 'tmp1', '.', 'format', '(', '_pad_name', '(', ')', ',', '_pad_name', '(', ')', ',', '\\n']
Tokenized   (027): ['<s>', 'out', '_', 'str', '=', 'tmp', '1', '.', 'format', '(', '_', 'pad', '_', 'name', '(', ')', ',', '_', 'pad', '_', 'name', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (025): ['out', '_', 'str', '=', 'tmp', '1', '.', 'format', '(', '_', 'pad', '_', 'name', '(', ')', ',', '_', 'pad', '_', 'name', '(', ')', ',', '\\', 'n']
Detokenized (015): ['out_str', '=', 'tmp1', '.', 'format', '(', '_pad_name', '(', ')', ',', '_pad_name', '(', ')', ',', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "magfor , magrev , magfd , abs1 , abs2 , \n"
Original    (011): ['magfor', ',', 'magrev', ',', 'magfd', ',', 'abs1', ',', 'abs2', ',', '\\n']
Tokenized   (019): ['<s>', 'mag', 'for', ',', 'mag', 'rev', ',', 'mag', 'fd', ',', 'abs', '1', ',', 'abs', '2', ',', '\\', 'n', '</s>']
Filtered   (017): ['mag', 'for', ',', 'mag', 'rev', ',', 'mag', 'fd', ',', 'abs', '1', ',', 'abs', '2', ',', '\\', 'n']
Detokenized (011): ['magfor', ',', 'magrev', ',', 'magfd', ',', 'abs1', ',', 'abs2', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "rel1 , rel2 ) ) \n"
Original    (006): ['rel1', ',', 'rel2', ')', ')', '\\n']
Tokenized   (011): ['<s>', 'rel', '1', ',', 'rel', '2', ')', ')', '\\', 'n', '</s>']
Filtered   (009): ['rel', '1', ',', 'rel', '2', ')', ')', '\\', 'n']
Detokenized (006): ['rel1', ',', 'rel2', ')', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "_pad_name ( , 12 , quotes = False ) \n"
Original    (010): ['_pad_name', '(', ',', '12', ',', 'quotes', '=', 'False', ')', '\\n']
Tokenized   (016): ['<s>', '_', 'pad', '_', 'name', '(', ',', '12', ',', 'quotes', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (014): ['_', 'pad', '_', 'name', '(', ',', '12', ',', 'quotes', '=', 'False', ')', '\\', 'n']
Detokenized (010): ['_pad_name', '(', ',', '12', ',', 'quotes', '=', 'False', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "magfd , magfd2 , abs4 , rel4 ) ) \n"
Original    (010): ['magfd', ',', 'magfd2', ',', 'abs4', ',', 'rel4', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'mag', 'fd', ',', 'mag', 'fd', '2', ',', 'abs', '4', ',', 'rel', '4', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['mag', 'fd', ',', 'mag', 'fd', '2', ',', 'abs', '4', ',', 'rel', '4', ')', ')', '\\', 'n']
Detokenized (010): ['magfd', ',', 'magfd2', ',', 'abs4', ',', 'rel4', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "out_stream . write ( str ( Jsub_fd2 ) ) \n"
Original    (010): ['out_stream', '.', 'write', '(', 'str', '(', 'Jsub_fd2', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'out', '_', 'stream', '.', 'write', '(', 'str', '(', 'J', 'sub', '_', 'fd', '2', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['out', '_', 'stream', '.', 'write', '(', 'str', '(', 'J', 'sub', '_', 'fd', '2', ')', ')', '\\', 'n']
Detokenized (010): ['out_stream', '.', 'write', '(', 'str', '(', 'Jsub_fd2', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sqlite_dict_args . setdefault ( , ) \n"
Original    (007): ['sqlite_dict_args', '.', 'setdefault', '(', ',', ')', '\\n']
Tokenized   (016): ['<s>', 'sql', 'ite', '_', 'dict', '_', 'args', '.', 'set', 'default', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (014): ['sql', 'ite', '_', 'dict', '_', 'args', '.', 'set', 'default', '(', ',', ')', '\\', 'n']
Detokenized (007): ['sqlite_dict_args', '.', 'setdefault', '(', ',', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "ll_1 = ll_0 + n_samples - k - 1 \n"
Original    (010): ['ll_1', '=', 'll_0', '+', 'n_samples', '-', 'k', '-', '1', '\\n']
Tokenized   (020): ['<s>', 'll', '_', '1', '=', 'll', '_', '0', '+', 'n', '_', 's', 'amples', '-', 'k', '-', '1', '\\', 'n', '</s>']
Filtered   (018): ['ll', '_', '1', '=', 'll', '_', '0', '+', 'n', '_', 's', 'amples', '-', 'k', '-', '1', '\\', 'n']
Detokenized (010): ['ll_1', '=', 'll_0', '+', 'n_samples', '-', 'k', '-', '1', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "D = self . D [ lvl ] \n"
Original    (009): ['D', '=', 'self', '.', 'D', '[', 'lvl', ']', '\\n']
Tokenized   (012): ['<s>', 'D', '=', 'self', '.', 'D', '[', 'lvl', ']', '\\', 'n', '</s>']
Filtered   (010): ['D', '=', 'self', '.', 'D', '[', 'lvl', ']', '\\', 'n']
Detokenized (009): ['D', '=', 'self', '.', 'D', '[', 'lvl', ']', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "initial_range = INITIAL_RANGE_DEFAULT , tol = TOLERANCE_DEFAULT ) : \n"
Original    (010): ['initial_range', '=', 'INITIAL_RANGE_DEFAULT', ',', 'tol', '=', 'TOLERANCE_DEFAULT', ')', ':', '\\n']
Tokenized   (030): ['<s>', 'initial', '_', 'range', '=', 'IN', 'IT', 'IAL', '_', 'R', 'ANGE', '_', 'DE', 'FAULT', ',', 'to', 'l', '=', 'T', 'OL', 'ER', 'ANCE', '_', 'DE', 'FAULT', ')', ':', '\\', 'n', '</s>']
Filtered   (028): ['initial', '_', 'range', '=', 'IN', 'IT', 'IAL', '_', 'R', 'ANGE', '_', 'DE', 'FAULT', ',', 'to', 'l', '=', 'T', 'OL', 'ER', 'ANCE', '_', 'DE', 'FAULT', ')', ':', '\\', 'n']
Detokenized (010): ['initial_range', '=', 'INITIAL_RANGE_DEFAULT', ',', 'tol', '=', 'TOLERANCE_DEFAULT', ')', ':', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "y_best = y [ nlevel - 1 ] \n"
Original    (009): ['y_best', '=', 'y', '[', 'nlevel', '-', '1', ']', '\\n']
Tokenized   (015): ['<s>', 'y', '_', 'best', '=', 'y', '[', 'n', 'level', '-', '1', ']', '\\', 'n', '</s>']
Filtered   (013): ['y', '_', 'best', '=', 'y', '[', 'n', 'level', '-', '1', ']', '\\', 'n']
Detokenized (009): ['y_best', '=', 'y', '[', 'nlevel', '-', '1', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "+ str ( theta ) ) \n"
Original    (007): ['+', 'str', '(', 'theta', ')', ')', '\\n']
Tokenized   (011): ['<s>', '+', 'str', '(', 'the', 'ta', ')', ')', '\\', 'n', '</s>']
Filtered   (009): ['+', 'str', '(', 'the', 'ta', ')', ')', '\\', 'n']
Detokenized (007): ['+', 'str', '(', 'theta', ')', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "Yt = solve_triangular ( C , y , lower = True ) \n"
Original    (013): ['Yt', '=', 'solve_triangular', '(', 'C', ',', 'y', ',', 'lower', '=', 'True', ')', '\\n']
Tokenized   (020): ['<s>', 'Y', 't', '=', 'solve', '_', 'tri', 'angular', '(', 'C', ',', 'y', ',', 'lower', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (018): ['Y', 't', '=', 'solve', '_', 'tri', 'angular', '(', 'C', ',', 'y', ',', 'lower', '=', 'True', ')', '\\', 'n']
Detokenized (013): ['Yt', '=', 'solve_triangular', '(', 'C', ',', 'y', ',', 'lower', '=', 'True', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "err2 = np . dot ( err . T , err ) [ 0 , 0 ] \n"
Original    (018): ['err2', '=', 'np', '.', 'dot', '(', 'err', '.', 'T', ',', 'err', ')', '[', '0', ',', '0', ']', '\\n']
Tokenized   (022): ['<s>', 'err', '2', '=', 'np', '.', 'dot', '(', 'err', '.', 'T', ',', 'err', ')', '[', '0', ',', '0', ']', '\\', 'n', '</s>']
Filtered   (020): ['err', '2', '=', 'np', '.', 'dot', '(', 'err', '.', 'T', ',', 'err', ')', '[', '0', ',', '0', ']', '\\', 'n']
Detokenized (018): ['err2', '=', 'np', '.', 'dot', '(', 'err', '.', 'T', ',', 'err', ')', '[', '0', ',', '0', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "sigma2 = err2 / ( n_samples - p - q ) \n"
Original    (012): ['sigma2', '=', 'err2', '/', '(', 'n_samples', '-', 'p', '-', 'q', ')', '\\n']
Tokenized   (021): ['<s>', 's', 'igma', '2', '=', 'err', '2', '/', '(', 'n', '_', 's', 'amples', '-', 'p', '-', 'q', ')', '\\', 'n', '</s>']
Filtered   (019): ['s', 'igma', '2', '=', 'err', '2', '/', '(', 'n', '_', 's', 'amples', '-', 'p', '-', 'q', ')', '\\', 'n']
Detokenized (012): ['sigma2', '=', 'err2', '/', '(', 'n_samples', '-', 'p', '-', 'q', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "detR = ( ( np . diag ( C ) ) ** ( 2. / n_samples ) ) . prod ( ) \n"
Original    (023): ['detR', '=', '(', '(', 'np', '.', 'diag', '(', 'C', ')', ')', '**', '(', '2.', '/', 'n_samples', ')', ')', '.', 'prod', '(', ')', '\\n']
Tokenized   (032): ['<s>', 'det', 'R', '=', '(', '(', 'np', '.', 'di', 'ag', '(', 'C', ')', ')', '**', '(', '2', '.', '/', 'n', '_', 's', 'amples', ')', ')', '.', 'prod', '(', ')', '\\', 'n', '</s>']
Filtered   (030): ['det', 'R', '=', '(', '(', 'np', '.', 'di', 'ag', '(', 'C', ')', ')', '**', '(', '2', '.', '/', 'n', '_', 's', 'amples', ')', ')', '.', 'prod', '(', ')', '\\', 'n']
Detokenized (023): ['detR', '=', '(', '(', 'np', '.', 'diag', '(', 'C', ')', ')', '**', '(', '2.', '/', 'n_samples', ')', ')', '.', 'prod', '(', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "rlf_value = ( n_samples - p - q ) * np . log10 ( sigma2 ) + n_samples * np . log10 ( detR ) \n"
Original    (026): ['rlf_value', '=', '(', 'n_samples', '-', 'p', '-', 'q', ')', '*', 'np', '.', 'log10', '(', 'sigma2', ')', '+', 'n_samples', '*', 'np', '.', 'log10', '(', 'detR', ')', '\\n']
Tokenized   (043): ['<s>', 'r', 'lf', '_', 'value', '=', '(', 'n', '_', 's', 'amples', '-', 'p', '-', 'q', ')', '*', 'np', '.', 'log', '10', '(', 's', 'igma', '2', ')', '+', 'n', '_', 's', 'amples', '*', 'np', '.', 'log', '10', '(', 'det', 'R', ')', '\\', 'n', '</s>']
Filtered   (041): ['r', 'lf', '_', 'value', '=', '(', 'n', '_', 's', 'amples', '-', 'p', '-', 'q', ')', '*', 'np', '.', 'log', '10', '(', 's', 'igma', '2', ')', '+', 'n', '_', 's', 'amples', '*', 'np', '.', 'log', '10', '(', 'det', 'R', ')', '\\', 'n']
Detokenized (026): ['rlf_value', '=', '(', 'n_samples', '-', 'p', '-', 'q', ')', '*', 'np', '.', 'log10', '(', 'sigma2', ')', '+', 'n_samples', '*', 'np', '.', 'log10', '(', 'detR', ')', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "log10t [ i ] - np . log10 ( thetaL [ 0 ] [ i ] ) } ) \n"
Original    (020): ['log10t', '[', 'i', ']', '-', 'np', '.', 'log10', '(', 'thetaL', '[', '0', ']', '[', 'i', ']', ')', '}', ')', '\\n']
Tokenized   (028): ['<s>', 'log', '10', 't', '[', 'i', ']', '-', 'np', '.', 'log', '10', '(', 'the', 'ta', 'L', '[', '0', ']', '[', 'i', ']', ')', '}', ')', '\\', 'n', '</s>']
Filtered   (026): ['log', '10', 't', '[', 'i', ']', '-', 'np', '.', 'log', '10', '(', 'the', 'ta', 'L', '[', '0', ']', '[', 'i', ']', ')', '}', ')', '\\', 'n']
Detokenized (020): ['log10t', '[', 'i', ']', '-', 'np', '.', 'log10', '(', 'thetaL', '[', '0', ']', '[', 'i', ']', ')', '}', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "np . log10 ( thetaU [ 0 ] [ i ] ) - log10t [ i ] } ) \n"
Original    (020): ['np', '.', 'log10', '(', 'thetaU', '[', '0', ']', '[', 'i', ']', ')', '-', 'log10t', '[', 'i', ']', '}', ')', '\\n']
Tokenized   (028): ['<s>', 'np', '.', 'log', '10', '(', 'the', 'ta', 'U', '[', '0', ']', '[', 'i', ']', ')', '-', 'log', '10', 't', '[', 'i', ']', '}', ')', '\\', 'n', '</s>']
Filtered   (026): ['np', '.', 'log', '10', '(', 'the', 'ta', 'U', '[', '0', ']', '[', 'i', ']', ')', '-', 'log', '10', 't', '[', 'i', ']', '}', ')', '\\', 'n']
Detokenized (020): ['np', '.', 'log10', '(', 'thetaU', '[', '0', ']', '[', 'i', ']', ')', '-', 'log10t', '[', 'i', ']', '}', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "sol = minimize ( rlf_transform , x0 , method = , \n"
Original    (012): ['sol', '=', 'minimize', '(', 'rlf_transform', ',', 'x0', ',', 'method', '=', ',', '\\n']
Tokenized   (019): ['<s>', 'sol', '=', 'minimize', '(', 'r', 'lf', '_', 'transform', ',', 'x', '0', ',', 'method', '=', ',', '\\', 'n', '</s>']
Filtered   (017): ['sol', '=', 'minimize', '(', 'r', 'lf', '_', 'transform', ',', 'x', '0', ',', 'method', '=', ',', '\\', 'n']
Detokenized (012): ['sol', '=', 'minimize', '(', 'rlf_transform', ',', 'x0', ',', 'method', '=', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "optimal_theta = 10. ** log10_optimal_x \n"
Original    (006): ['optimal_theta', '=', '10.', '**', 'log10_optimal_x', '\\n']
Tokenized   (020): ['<s>', 'opt', 'imal', '_', 'the', 'ta', '=', '10', '.', '**', 'log', '10', '_', 'opt', 'imal', '_', 'x', '\\', 'n', '</s>']
Filtered   (018): ['opt', 'imal', '_', 'the', 'ta', '=', '10', '.', '**', 'log', '10', '_', 'opt', 'imal', '_', 'x', '\\', 'n']
Detokenized (006): ['optimal_theta', '=', '10.', '**', 'log10_optimal_x', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "r_t = solve_triangular ( C , r_ . T , lower = True ) \n"
Original    (015): ['r_t', '=', 'solve_triangular', '(', 'C', ',', 'r_', '.', 'T', ',', 'lower', '=', 'True', ')', '\\n']
Tokenized   (024): ['<s>', 'r', '_', 't', '=', 'solve', '_', 'tri', 'angular', '(', 'C', ',', 'r', '_', '.', 'T', ',', 'lower', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (022): ['r', '_', 't', '=', 'solve', '_', 'tri', 'angular', '(', 'C', ',', 'r', '_', '.', 'T', ',', 'lower', '=', 'True', ')', '\\', 'n']
Detokenized (015): ['r_t', '=', 'solve_triangular', '(', 'C', ',', 'r_', '.', 'T', ',', 'lower', '=', 'True', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "dx = l1_cross_distances ( X , Y = self . X [ i ] ) \n"
Original    (016): ['dx', '=', 'l1_cross_distances', '(', 'X', ',', 'Y', '=', 'self', '.', 'X', '[', 'i', ']', ')', '\\n']
Tokenized   (025): ['<s>', 'dx', '=', 'l', '1', '_', 'cross', '_', 'dist', 'ances', '(', 'X', ',', 'Y', '=', 'self', '.', 'X', '[', 'i', ']', ')', '\\', 'n', '</s>']
Filtered   (023): ['dx', '=', 'l', '1', '_', 'cross', '_', 'dist', 'ances', '(', 'X', ',', 'Y', '=', 'self', '.', 'X', '[', 'i', ']', ')', '\\', 'n']
Detokenized (016): ['dx', '=', 'l1_cross_distances', '(', 'X', ',', 'Y', '=', 'self', '.', 'X', '[', 'i', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "r_ = self . corr ( self . theta [ i ] , dx ) . reshape ( n_eval , self . n_samples [ i ] ) \n"
Original    (028): ['r_', '=', 'self', '.', 'corr', '(', 'self', '.', 'theta', '[', 'i', ']', ',', 'dx', ')', '.', 'reshape', '(', 'n_eval', ',', 'self', '.', 'n_samples', '[', 'i', ']', ')', '\\n']
Tokenized   (040): ['<s>', 'r', '_', '=', 'self', '.', 'cor', 'r', '(', 'self', '.', 'the', 'ta', '[', 'i', ']', ',', 'dx', ')', '.', 'resh', 'ape', '(', 'n', '_', 'eval', ',', 'self', '.', 'n', '_', 's', 'amples', '[', 'i', ']', ')', '\\', 'n', '</s>']
Filtered   (038): ['r', '_', '=', 'self', '.', 'cor', 'r', '(', 'self', '.', 'the', 'ta', '[', 'i', ']', ',', 'dx', ')', '.', 'resh', 'ape', '(', 'n', '_', 'eval', ',', 'self', '.', 'n', '_', 's', 'amples', '[', 'i', ']', ')', '\\', 'n']
Detokenized (028): ['r_', '=', 'self', '.', 'corr', '(', 'self', '.', 'theta', '[', 'i', ']', ',', 'dx', ')', '.', 'reshape', '(', 'n_eval', ',', 'self', '.', 'n_samples', '[', 'i', ']', ')', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 28, 768)
# Extracted words:  28
Sentence         : "yt = solve_triangular ( C , self . y [ i ] , lower = True ) \n"
Original    (018): ['yt', '=', 'solve_triangular', '(', 'C', ',', 'self', '.', 'y', '[', 'i', ']', ',', 'lower', '=', 'True', ')', '\\n']
Tokenized   (024): ['<s>', 'yt', '=', 'solve', '_', 'tri', 'angular', '(', 'C', ',', 'self', '.', 'y', '[', 'i', ']', ',', 'lower', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (022): ['yt', '=', 'solve', '_', 'tri', 'angular', '(', 'C', ',', 'self', '.', 'y', '[', 'i', ']', ',', 'lower', '=', 'True', ')', '\\', 'n']
Detokenized (018): ['yt', '=', 'solve_triangular', '(', 'C', ',', 'self', '.', 'y', '[', 'i', ']', ',', 'lower', '=', 'True', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "mu [ : , i ] = ( np . dot ( f . T , beta ) + np . dot ( r_t . T , yt - np . dot ( Ft , beta ) ) ) . ravel ( ) \n"
Original    (044): ['mu', '[', ':', ',', 'i', ']', '=', '(', 'np', '.', 'dot', '(', 'f', '.', 'T', ',', 'beta', ')', '+', 'np', '.', 'dot', '(', 'r_t', '.', 'T', ',', 'yt', '-', 'np', '.', 'dot', '(', 'Ft', ',', 'beta', ')', ')', ')', '.', 'ravel', '(', ')', '\\n']
Tokenized   (051): ['<s>', 'mu', '[', ':', ',', 'i', ']', '=', '(', 'np', '.', 'dot', '(', 'f', '.', 'T', ',', 'beta', ')', '+', 'np', '.', 'dot', '(', 'r', '_', 't', '.', 'T', ',', 'y', 't', '-', 'np', '.', 'dot', '(', 'Ft', ',', 'beta', ')', ')', ')', '.', 'ra', 'vel', '(', ')', '\\', 'n', '</s>']
Filtered   (049): ['mu', '[', ':', ',', 'i', ']', '=', '(', 'np', '.', 'dot', '(', 'f', '.', 'T', ',', 'beta', ')', '+', 'np', '.', 'dot', '(', 'r', '_', 't', '.', 'T', ',', 'y', 't', '-', 'np', '.', 'dot', '(', 'Ft', ',', 'beta', ')', ')', ')', '.', 'ra', 'vel', '(', ')', '\\', 'n']
Detokenized (044): ['mu', '[', ':', ',', 'i', ']', '=', '(', 'np', '.', 'dot', '(', 'f', '.', 'T', ',', 'beta', ')', '+', 'np', '.', 'dot', '(', 'r_t', '.', 'T', ',', 'yt', '-', 'np', '.', 'dot', '(', 'Ft', ',', 'beta', ')', ')', ')', '.', 'ravel', '(', ')', '\\n']
Counter: 49
===================================================================
Hidden states:  (13, 44, 768)
# Extracted words:  44
Sentence         : "u_ = solve_triangular ( G . T , f - np . dot ( Ft . T , r_t ) , lower = True ) \n"
Original    (026): ['u_', '=', 'solve_triangular', '(', 'G', '.', 'T', ',', 'f', '-', 'np', '.', 'dot', '(', 'Ft', '.', 'T', ',', 'r_t', ')', ',', 'lower', '=', 'True', ')', '\\n']
Tokenized   (035): ['<s>', 'u', '_', '=', 'solve', '_', 'tri', 'angular', '(', 'G', '.', 'T', ',', 'f', '-', 'np', '.', 'dot', '(', 'Ft', '.', 'T', ',', 'r', '_', 't', ')', ',', 'lower', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (033): ['u', '_', '=', 'solve', '_', 'tri', 'angular', '(', 'G', '.', 'T', ',', 'f', '-', 'np', '.', 'dot', '(', 'Ft', '.', 'T', ',', 'r', '_', 't', ')', ',', 'lower', '=', 'True', ')', '\\', 'n']
Detokenized (026): ['u_', '=', 'solve_triangular', '(', 'G', '.', 'T', ',', 'f', '-', 'np', '.', 'dot', '(', 'Ft', '.', 'T', ',', 'r_t', ')', ',', 'lower', '=', 'True', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "sigma2_rho = ( sigma2_rho * g ) . sum ( axis = 1 ) \n"
Original    (015): ['sigma2_rho', '=', '(', 'sigma2_rho', '*', 'g', ')', '.', 'sum', '(', 'axis', '=', '1', ')', '\\n']
Tokenized   (028): ['<s>', 's', 'igma', '2', '_', 'r', 'ho', '=', '(', 's', 'igma', '2', '_', 'r', 'ho', '*', 'g', ')', '.', 'sum', '(', 'axis', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (026): ['s', 'igma', '2', '_', 'r', 'ho', '=', '(', 's', 'igma', '2', '_', 'r', 'ho', '*', 'g', ')', '.', 'sum', '(', 'axis', '=', '1', ')', '\\', 'n']
Detokenized (015): ['sigma2_rho', '=', '(', 'sigma2_rho', '*', 'g', ')', '.', 'sum', '(', 'axis', '=', '1', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "MSE [ : , i ] = sigma2_rho * MSE [ : , i - 1 ] + Q_ / ( 2 * ( self . n_samples [ i ] - self . p [ i ] - self . q [ i ] ) ) * ( 1 - ( r_t ** 2 ) . sum ( axis = 0 ) ) + self . sigma2 [ i ] * ( u_ ** 2 ) . sum ( axis = 0 ) \n"
Original    (084): ['MSE', '[', ':', ',', 'i', ']', '=', 'sigma2_rho', '*', 'MSE', '[', ':', ',', 'i', '-', '1', ']', '+', 'Q_', '/', '(', '2', '*', '(', 'self', '.', 'n_samples', '[', 'i', ']', '-', 'self', '.', 'p', '[', 'i', ']', '-', 'self', '.', 'q', '[', 'i', ']', ')', ')', '*', '(', '1', '-', '(', 'r_t', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', ')', '+', 'self', '.', 'sigma2', '[', 'i', ']', '*', '(', 'u_', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', '\\n']
Tokenized   (103): ['<s>', 'M', 'SE', '[', ':', ',', 'i', ']', '=', 's', 'igma', '2', '_', 'r', 'ho', '*', 'M', 'SE', '[', ':', ',', 'i', '-', '1', ']', '+', 'Q', '_', '/', '(', '2', '*', '(', 'self', '.', 'n', '_', 's', 'amples', '[', 'i', ']', '-', 'self', '.', 'p', '[', 'i', ']', '-', 'self', '.', 'q', '[', 'i', ']', ')', ')', '*', '(', '1', '-', '(', 'r', '_', 't', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', ')', '+', 'self', '.', 's', 'igma', '2', '[', 'i', ']', '*', '(', 'u', '_', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (101): ['M', 'SE', '[', ':', ',', 'i', ']', '=', 's', 'igma', '2', '_', 'r', 'ho', '*', 'M', 'SE', '[', ':', ',', 'i', '-', '1', ']', '+', 'Q', '_', '/', '(', '2', '*', '(', 'self', '.', 'n', '_', 's', 'amples', '[', 'i', ']', '-', 'self', '.', 'p', '[', 'i', ']', '-', 'self', '.', 'q', '[', 'i', ']', ')', ')', '*', '(', '1', '-', '(', 'r', '_', 't', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', ')', '+', 'self', '.', 's', 'igma', '2', '[', 'i', ']', '*', '(', 'u', '_', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', '\\', 'n']
Detokenized (084): ['MSE', '[', ':', ',', 'i', ']', '=', 'sigma2_rho', '*', 'MSE', '[', ':', ',', 'i', '-', '1', ']', '+', 'Q_', '/', '(', '2', '*', '(', 'self', '.', 'n_samples', '[', 'i', ']', '-', 'self', '.', 'p', '[', 'i', ']', '-', 'self', '.', 'q', '[', 'i', ']', ')', ')', '*', '(', '1', '-', '(', 'r_t', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', ')', '+', 'self', '.', 'sigma2', '[', 'i', ']', '*', '(', 'u_', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', '\\n']
Counter: 101
===================================================================
Hidden states:  (13, 84, 768)
# Extracted words:  84
Sentence         : "n_features = np . zeros ( nlevel , dtype = int ) \n"
Original    (013): ['n_features', '=', 'np', '.', 'zeros', '(', 'nlevel', ',', 'dtype', '=', 'int', ')', '\\n']
Tokenized   (021): ['<s>', 'n', '_', 'features', '=', 'np', '.', 'z', 'eros', '(', 'n', 'level', ',', 'd', 'type', '=', 'int', ')', '\\', 'n', '</s>']
Filtered   (019): ['n', '_', 'features', '=', 'np', '.', 'z', 'eros', '(', 'n', 'level', ',', 'd', 'type', '=', 'int', ')', '\\', 'n']
Detokenized (013): ['n_features', '=', 'np', '.', 'zeros', '(', 'nlevel', ',', 'dtype', '=', 'int', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "n_samples_y [ i ] = y [ i ] . shape [ 0 ] \n"
Original    (015): ['n_samples_y', '[', 'i', ']', '=', 'y', '[', 'i', ']', '.', 'shape', '[', '0', ']', '\\n']
Tokenized   (023): ['<s>', 'n', '_', 's', 'amples', '_', 'y', '[', 'i', ']', '=', 'y', '[', 'i', ']', '.', 'shape', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (021): ['n', '_', 's', 'amples', '_', 'y', '[', 'i', ']', '=', 'y', '[', 'i', ']', '.', 'shape', '[', '0', ']', '\\', 'n']
Detokenized (015): ['n_samples_y', '[', 'i', ']', '=', 'y', '[', 'i', ']', '.', 'shape', '[', '0', ']', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "Y_pred , MSE = self . model . predict ( [ new_x ] ) \n"
Original    (015): ['Y_pred', ',', 'MSE', '=', 'self', '.', 'model', '.', 'predict', '(', '[', 'new_x', ']', ')', '\\n']
Tokenized   (023): ['<s>', 'Y', '_', 'pred', ',', 'M', 'SE', '=', 'self', '.', 'model', '.', 'predict', '(', '[', 'new', '_', 'x', ']', ')', '\\', 'n', '</s>']
Filtered   (021): ['Y', '_', 'pred', ',', 'M', 'SE', '=', 'self', '.', 'model', '.', 'predict', '(', '[', 'new', '_', 'x', ']', ')', '\\', 'n']
Detokenized (015): ['Y_pred', ',', 'MSE', '=', 'self', '.', 'model', '.', 'predict', '(', '[', 'new_x', ']', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "X , Y = self . _fit_adapter ( X , Y ) \n"
Original    (013): ['X', ',', 'Y', '=', 'self', '.', '_fit_adapter', '(', 'X', ',', 'Y', ')', '\\n']
Tokenized   (020): ['<s>', 'X', ',', 'Y', '=', 'self', '.', '_', 'fit', '_', 'ad', 'apter', '(', 'X', ',', 'Y', ')', '\\', 'n', '</s>']
Filtered   (018): ['X', ',', 'Y', '=', 'self', '.', '_', 'fit', '_', 'ad', 'apter', '(', 'X', ',', 'Y', ')', '\\', 'n']
Detokenized (013): ['X', ',', 'Y', '=', 'self', '.', '_fit_adapter', '(', 'X', ',', 'Y', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "Y = [ np . array ( y ) for y in reversed ( Y ) ] \n"
Original    (018): ['Y', '=', '[', 'np', '.', 'array', '(', 'y', ')', 'for', 'y', 'in', 'reversed', '(', 'Y', ')', ']', '\\n']
Tokenized   (021): ['<s>', 'Y', '=', '[', 'np', '.', 'array', '(', 'y', ')', 'for', 'y', 'in', 'reversed', '(', 'Y', ')', ']', '\\', 'n', '</s>']
Filtered   (019): ['Y', '=', '[', 'np', '.', 'array', '(', 'y', ')', 'for', 'y', 'in', 'reversed', '(', 'Y', ')', ']', '\\', 'n']
Detokenized (018): ['Y', '=', '[', 'np', '.', 'array', '(', 'y', ')', 'for', 'y', 'in', 'reversed', '(', 'Y', ')', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "newdata = np . array ( parsed [ : ] ) \n"
Original    (012): ['newdata', '=', 'np', '.', 'array', '(', 'parsed', '[', ':', ']', ')', '\\n']
Tokenized   (016): ['<s>', 'new', 'data', '=', 'np', '.', 'array', '(', 'parsed', '[', ':', ']', ')', '\\', 'n', '</s>']
Filtered   (014): ['new', 'data', '=', 'np', '.', 'array', '(', 'parsed', '[', ':', ']', ')', '\\', 'n']
Detokenized (012): ['newdata', '=', 'np', '.', 'array', '(', 'parsed', '[', ':', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "icc . DB_USER ) , shell = True ) \n"
Original    (010): ['icc', '.', 'DB_USER', ')', ',', 'shell', '=', 'True', ')', '\\n']
Tokenized   (015): ['<s>', 'icc', '.', 'DB', '_', 'USER', ')', ',', 'shell', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (013): ['icc', '.', 'DB', '_', 'USER', ')', ',', 'shell', '=', 'True', ')', '\\', 'n']
Detokenized (010): ['icc', '.', 'DB_USER', ')', ',', 'shell', '=', 'True', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "instance_db_name , shell = True ) \n"
Original    (007): ['instance_db_name', ',', 'shell', '=', 'True', ')', '\\n']
Tokenized   (014): ['<s>', 'instance', '_', 'db', '_', 'name', ',', 'shell', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (012): ['instance', '_', 'db', '_', 'name', ',', 'shell', '=', 'True', ')', '\\', 'n']
Detokenized (007): ['instance_db_name', ',', 'shell', '=', 'True', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "customslide = CustomSlide . objects . create ( title = , text = default_projector = Projector . objects . get ( pk = 1 ) \n"
Original    (026): ['customslide', '=', 'CustomSlide', '.', 'objects', '.', 'create', '(', 'title', '=', ',', 'text', '=', 'default_projector', '=', 'Projector', '.', 'objects', '.', 'get', '(', 'pk', '=', '1', ')', '\\n']
Tokenized   (038): ['<s>', 'custom', 'sl', 'ide', '=', 'Custom', 'Sl', 'ide', '.', 'objects', '.', 'create', '(', 'title', '=', ',', 'text', '=', 'default', '_', 'project', 'or', '=', 'Project', 'or', '.', 'objects', '.', 'get', '(', 'p', 'k', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (036): ['custom', 'sl', 'ide', '=', 'Custom', 'Sl', 'ide', '.', 'objects', '.', 'create', '(', 'title', '=', ',', 'text', '=', 'default', '_', 'project', 'or', '=', 'Project', 'or', '.', 'objects', '.', 'get', '(', 'p', 'k', '=', '1', ')', '\\', 'n']
Detokenized (026): ['customslide', '=', 'CustomSlide', '.', 'objects', '.', 'create', '(', 'title', '=', ',', 'text', '=', 'default_projector', '=', 'Projector', '.', 'objects', '.', 'get', '(', 'pk', '=', '1', ')', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "default_projector = Projector . objects . get ( pk = 1 ) \n"
Original    (013): ['default_projector', '=', 'Projector', '.', 'objects', '.', 'get', '(', 'pk', '=', '1', ')', '\\n']
Tokenized   (021): ['<s>', 'default', '_', 'project', 'or', '=', 'Project', 'or', '.', 'objects', '.', 'get', '(', 'p', 'k', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (019): ['default', '_', 'project', 'or', '=', 'Project', 'or', '.', 'objects', '.', 'get', '(', 'p', 'k', '=', '1', ')', '\\', 'n']
Detokenized (013): ['default_projector', '=', 'Projector', '.', 'objects', '.', 'get', '(', 'pk', '=', '1', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "reverse ( , args = [ ] ) ) \n"
Original    (010): ['reverse', '(', ',', 'args', '=', '[', ']', ')', ')', '\\n']
Tokenized   (013): ['<s>', 'reverse', '(', ',', 'args', '=', '[', ']', ')', ')', '\\', 'n', '</s>']
Filtered   (011): ['reverse', '(', ',', 'args', '=', '[', ']', ')', ')', '\\', 'n']
Detokenized (010): ['reverse', '(', ',', 'args', '=', '[', ']', ')', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "yield ConfigVariable ( \n"
Original    (004): ['yield', 'ConfigVariable', '(', '\\n']
Tokenized   (009): ['<s>', 'y', 'ield', 'Config', 'Variable', '(', '\\', 'n', '</s>']
Filtered   (007): ['y', 'ield', 'Config', 'Variable', '(', '\\', 'n']
Detokenized (004): ['yield', 'ConfigVariable', '(', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "{ : , : } , { : , : } ) \n"
Original    (013): ['{', ':', ',', ':', '}', ',', '{', ':', ',', ':', '}', ')', '\\n']
Tokenized   (016): ['<s>', '{', ':', ',', ':', '}', ',', '{', ':', ',', ':', '}', ')', '\\', 'n', '</s>']
Filtered   (014): ['{', ':', ',', ':', '}', ',', '{', ':', ',', ':', '}', ')', '\\', 'n']
Detokenized (013): ['{', ':', ',', ':', '}', ',', '{', ':', ',', ':', '}', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "validators = ( validator_for_testing , ) ) \n"
Original    (008): ['validators', '=', '(', 'validator_for_testing', ',', ')', ')', '\\n']
Tokenized   (017): ['<s>', 'valid', 'ators', '=', '(', 'valid', 'ator', '_', 'for', '_', 'testing', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (015): ['valid', 'ators', '=', '(', 'valid', 'ator', '_', 'for', '_', 'testing', ',', ')', ')', '\\', 'n']
Detokenized (008): ['validators', '=', '(', 'validator_for_testing', ',', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "generate_username . return_value = \n"
Original    (005): ['generate_username', '.', 'return_value', '=', '\\n']
Tokenized   (013): ['<s>', 'gener', 'ate', '_', 'username', '.', 'return', '_', 'value', '=', '\\', 'n', '</s>']
Filtered   (011): ['gener', 'ate', '_', 'username', '.', 'return', '_', 'value', '=', '\\', 'n']
Detokenized (005): ['generate_username', '.', 'return_value', '=', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "serializer = UserFullSerializer ( context = { : view } ) \n"
Original    (012): ['serializer', '=', 'UserFullSerializer', '(', 'context', '=', '{', ':', 'view', '}', ')', '\\n']
Tokenized   (019): ['<s>', 'serial', 'izer', '=', 'User', 'Full', 'Serial', 'izer', '(', 'context', '=', '{', ':', 'view', '}', ')', '\\', 'n', '</s>']
Filtered   (017): ['serial', 'izer', '=', 'User', 'Full', 'Serial', 'izer', '(', 'context', '=', '{', ':', 'view', '}', ')', '\\', 'n']
Detokenized (012): ['serializer', '=', 'UserFullSerializer', '(', 'context', '=', '{', ':', 'view', '}', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "#domain... #localhost... \n"
Original    (003): ['#domain...', '#localhost...', '\\n']
Tokenized   (010): ['<s>', '#', 'domain', '...', '#', 'localhost', '...', '\\', 'n', '</s>']
Filtered   (008): ['#', 'domain', '...', '#', 'localhost', '...', '\\', 'n']
Detokenized (003): ['#domain...', '#localhost...', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "USERNAME_REGEX = re . compile ( , re . I ) \n"
Original    (012): ['USERNAME_REGEX', '=', 're', '.', 'compile', '(', ',', 're', '.', 'I', ')', '\\n']
Tokenized   (019): ['<s>', 'USER', 'NAME', '_', 'REG', 'EX', '=', 're', '.', 'compile', '(', ',', 're', '.', 'I', ')', '\\', 'n', '</s>']
Filtered   (017): ['USER', 'NAME', '_', 'REG', 'EX', '=', 're', '.', 'compile', '(', ',', 're', '.', 'I', ')', '\\', 'n']
Detokenized (012): ['USERNAME_REGEX', '=', 're', '.', 'compile', '(', ',', 're', '.', 'I', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "RouteDistinguisher . TYPE_IP_LOC , None , \n"
Original    (007): ['RouteDistinguisher', '.', 'TYPE_IP_LOC', ',', 'None', ',', '\\n']
Tokenized   (017): ['<s>', 'Route', 'Dist', 'ingu', 'isher', '.', 'TYPE', '_', 'IP', '_', 'LOC', ',', 'None', ',', '\\', 'n', '</s>']
Filtered   (015): ['Route', 'Dist', 'ingu', 'isher', '.', 'TYPE', '_', 'IP', '_', 'LOC', ',', 'None', ',', '\\', 'n']
Detokenized (007): ['RouteDistinguisher', '.', 'TYPE_IP_LOC', ',', 'None', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "10000 + label ) \n"
Original    (005): ['10000', '+', 'label', ')', '\\n']
Tokenized   (008): ['<s>', '10000', '+', 'label', ')', '\\', 'n', '</s>']
Filtered   (006): ['10000', '+', 'label', ')', '\\', 'n']
Detokenized (005): ['10000', '+', 'label', ')', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "nh = Inet ( 1 , socket . inet_pton ( socket . AF_INET , \n"
Original    (015): ['nh', '=', 'Inet', '(', '1', ',', 'socket', '.', 'inet_pton', '(', 'socket', '.', 'AF_INET', ',', '\\n']
Tokenized   (026): ['<s>', 'n', 'h', '=', 'In', 'et', '(', '1', ',', 'socket', '.', 'in', 'et', '_', 'pton', '(', 'socket', '.', 'AF', '_', 'IN', 'ET', ',', '\\', 'n', '</s>']
Filtered   (024): ['n', 'h', '=', 'In', 'et', '(', '1', ',', 'socket', '.', 'in', 'et', '_', 'pton', '(', 'socket', '.', 'AF', '_', 'IN', 'ET', ',', '\\', 'n']
Detokenized (015): ['nh', '=', 'Inet', '(', '1', ',', 'socket', '.', 'inet_pton', '(', 'socket', '.', 'AF_INET', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "route . attributes . add ( ECommunities ( self . readvertiseToRTs ) ) \n"
Original    (014): ['route', '.', 'attributes', '.', 'add', '(', 'ECommunities', '(', 'self', '.', 'readvertiseToRTs', ')', ')', '\\n']
Tokenized   (024): ['<s>', 'route', '.', 'attributes', '.', 'add', '(', 'E', 'Commun', 'ities', '(', 'self', '.', 'read', 'vert', 'ise', 'To', 'RT', 's', ')', ')', '\\', 'n', '</s>']
Filtered   (022): ['route', '.', 'attributes', '.', 'add', '(', 'E', 'Commun', 'ities', '(', 'self', '.', 'read', 'vert', 'ise', 'To', 'RT', 's', ')', ')', '\\', 'n']
Detokenized (014): ['route', '.', 'attributes', '.', 'add', '(', 'ECommunities', '(', 'self', '.', 'readvertiseToRTs', ')', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "nlri . prefix , label ) \n"
Original    (007): ['nlri', '.', 'prefix', ',', 'label', ')', '\\n']
Tokenized   (011): ['<s>', 'nl', 'ri', '.', 'prefix', ',', 'label', ')', '\\', 'n', '</s>']
Filtered   (009): ['nl', 'ri', '.', 'prefix', ',', 'label', ')', '\\', 'n']
Detokenized (007): ['nlri', '.', 'prefix', ',', 'label', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "set ( self . importRTs ) ) ) > 0 ) \n"
Original    (012): ['set', '(', 'self', '.', 'importRTs', ')', ')', ')', '>', '0', ')', '\\n']
Tokenized   (017): ['<s>', 'set', '(', 'self', '.', 'import', 'RT', 's', ')', ')', ')', '>', '0', ')', '\\', 'n', '</s>']
Filtered   (015): ['set', '(', 'self', '.', 'import', 'RT', 's', ')', ')', ')', '>', '0', ')', '\\', 'n']
Detokenized (012): ['set', '(', 'self', '.', 'importRTs', ')', ')', ')', '>', '0', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "newRoute . nlri . labelStack [ 0 ] . labelValue , newRoute . nlri , encaps ) \n"
Original    (018): ['newRoute', '.', 'nlri', '.', 'labelStack', '[', '0', ']', '.', 'labelValue', ',', 'newRoute', '.', 'nlri', ',', 'encaps', ')', '\\n']
Tokenized   (029): ['<s>', 'new', 'Route', '.', 'n', 'l', 'ri', '.', 'label', 'Stack', '[', '0', ']', '.', 'label', 'Value', ',', 'new', 'Route', '.', 'n', 'l', 'ri', ',', 'encaps', ')', '\\', 'n', '</s>']
Filtered   (027): ['new', 'Route', '.', 'n', 'l', 'ri', '.', 'label', 'Stack', '[', '0', ']', '.', 'label', 'Value', ',', 'new', 'Route', '.', 'n', 'l', 'ri', ',', 'encaps', ')', '\\', 'n']
Detokenized (018): ['newRoute', '.', 'nlri', '.', 'labelStack', '[', '0', ']', '.', 'labelValue', ',', 'newRoute', '.', 'nlri', ',', 'encaps', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "prefix , oldRoute . attributes . get ( NextHop . ID ) . next_hop , \n"
Original    (016): ['prefix', ',', 'oldRoute', '.', 'attributes', '.', 'get', '(', 'NextHop', '.', 'ID', ')', '.', 'next_hop', ',', '\\n']
Tokenized   (023): ['<s>', 'prefix', ',', 'old', 'Route', '.', 'attributes', '.', 'get', '(', 'Next', 'Hop', '.', 'ID', ')', '.', 'next', '_', 'hop', ',', '\\', 'n', '</s>']
Filtered   (021): ['prefix', ',', 'old', 'Route', '.', 'attributes', '.', 'get', '(', 'Next', 'Hop', '.', 'ID', ')', '.', 'next', '_', 'hop', ',', '\\', 'n']
Detokenized (016): ['prefix', ',', 'oldRoute', '.', 'attributes', '.', 'get', '(', 'NextHop', '.', 'ID', ')', '.', 'next_hop', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "oldRoute . nlri . labelStack [ 0 ] . labelValue , oldRoute . nlri ) \n"
Original    (016): ['oldRoute', '.', 'nlri', '.', 'labelStack', '[', '0', ']', '.', 'labelValue', ',', 'oldRoute', '.', 'nlri', ')', '\\n']
Tokenized   (027): ['<s>', 'old', 'Route', '.', 'n', 'l', 'ri', '.', 'label', 'Stack', '[', '0', ']', '.', 'label', 'Value', ',', 'old', 'Route', '.', 'n', 'l', 'ri', ')', '\\', 'n', '</s>']
Filtered   (025): ['old', 'Route', '.', 'n', 'l', 'ri', '.', 'label', 'Stack', '[', '0', ']', '.', 'label', 'Value', ',', 'old', 'Route', '.', 'n', 'l', 'ri', ')', '\\', 'n']
Detokenized (016): ['oldRoute', '.', 'nlri', '.', 'labelStack', '[', '0', ']', '.', 'labelValue', ',', 'oldRoute', '.', 'nlri', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : ""readvertised" : ( LGMap . VALUE , [ repr ( prefix ) for prefix in \n"
Original    (016): ['"readvertised"', ':', '(', 'LGMap', '.', 'VALUE', ',', '[', 'repr', '(', 'prefix', ')', 'for', 'prefix', 'in', '\\n']
Tokenized   (025): ['<s>', '"', 'read', 'vert', 'ised', '"', ':', '(', 'LG', 'Map', '.', 'VAL', 'UE', ',', '[', 'repr', '(', 'prefix', ')', 'for', 'prefix', 'in', '\\', 'n', '</s>']
Filtered   (023): ['"', 'read', 'vert', 'ised', '"', ':', '(', 'LG', 'Map', '.', 'VAL', 'UE', ',', '[', 'repr', '(', 'prefix', ')', 'for', 'prefix', 'in', '\\', 'n']
Detokenized (016): ['"readvertised"', ':', '(', 'LGMap', '.', 'VALUE', ',', '[', 'repr', '(', 'prefix', ')', 'for', 'prefix', 'in', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "REACTORNAME = DEFAULT_REACTORS . get ( platform . system ( ) , ) \n"
Original    (014): ['REACTORNAME', '=', 'DEFAULT_REACTORS', '.', 'get', '(', 'platform', '.', 'system', '(', ')', ',', ')', '\\n']
Tokenized   (025): ['<s>', 'RE', 'ACT', 'OR', 'NAME', '=', 'DE', 'FAULT', '_', 'RE', 'ACT', 'ORS', '.', 'get', '(', 'platform', '.', 'system', '(', ')', ',', ')', '\\', 'n', '</s>']
Filtered   (023): ['RE', 'ACT', 'OR', 'NAME', '=', 'DE', 'FAULT', '_', 'RE', 'ACT', 'ORS', '.', 'get', '(', 'platform', '.', 'system', '(', ')', ',', ')', '\\', 'n']
Detokenized (014): ['REACTORNAME', '=', 'DEFAULT_REACTORS', '.', 'get', '(', 'platform', '.', 'system', '(', ')', ',', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "set_reactor = lambda : reactor \n"
Original    (006): ['set_reactor', '=', 'lambda', ':', 'reactor', '\\n']
Tokenized   (012): ['<s>', 'set', '_', 're', 'actor', '=', 'lambda', ':', 'reactor', '\\', 'n', '</s>']
Filtered   (010): ['set', '_', 're', 'actor', '=', 'lambda', ':', 'reactor', '\\', 'n']
Detokenized (006): ['set_reactor', '=', 'lambda', ':', 'reactor', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "SIGNALS = dict ( ( k , v ) for v , k in signal . __dict__ . iteritems ( ) if v . startswith ( ) and not v . startswith ( ) ) \n"
Original    (036): ['SIGNALS', '=', 'dict', '(', '(', 'k', ',', 'v', ')', 'for', 'v', ',', 'k', 'in', 'signal', '.', '__dict__', '.', 'iteritems', '(', ')', 'if', 'v', '.', 'startswith', '(', ')', 'and', 'not', 'v', '.', 'startswith', '(', ')', ')', '\\n']
Tokenized   (047): ['<s>', 'SIGN', 'ALS', '=', 'dict', '(', '(', 'k', ',', 'v', ')', 'for', 'v', ',', 'k', 'in', 'signal', '.', '__', 'dict', '__', '.', 'iter', 'items', '(', ')', 'if', 'v', '.', 'start', 'sw', 'ith', '(', ')', 'and', 'not', 'v', '.', 'start', 'sw', 'ith', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (045): ['SIGN', 'ALS', '=', 'dict', '(', '(', 'k', ',', 'v', ')', 'for', 'v', ',', 'k', 'in', 'signal', '.', '__', 'dict', '__', '.', 'iter', 'items', '(', ')', 'if', 'v', '.', 'start', 'sw', 'ith', '(', ')', 'and', 'not', 'v', '.', 'start', 'sw', 'ith', '(', ')', ')', '\\', 'n']
Detokenized (036): ['SIGNALS', '=', 'dict', '(', '(', 'k', ',', 'v', ')', 'for', 'v', ',', 'k', 'in', 'signal', '.', '__dict__', '.', 'iteritems', '(', ')', 'if', 'v', '.', 'startswith', '(', ')', 'and', 'not', 'v', '.', 'startswith', '(', ')', ')', '\\n']
Counter: 45
===================================================================
Hidden states:  (13, 36, 768)
# Extracted words:  36
Sentence         : "pargs = ( self . name , self . label , self . reactor ) \n"
Original    (016): ['pargs', '=', '(', 'self', '.', 'name', ',', 'self', '.', 'label', ',', 'self', '.', 'reactor', ')', '\\n']
Tokenized   (020): ['<s>', 'p', 'args', '=', '(', 'self', '.', 'name', ',', 'self', '.', 'label', ',', 'self', '.', 'reactor', ')', '\\', 'n', '</s>']
Filtered   (018): ['p', 'args', '=', '(', 'self', '.', 'name', ',', 'self', '.', 'label', ',', 'self', '.', 'reactor', ')', '\\', 'n']
Detokenized (016): ['pargs', '=', '(', 'self', '.', 'name', ',', 'self', '.', 'label', ',', 'self', '.', 'reactor', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "* pargs , ** pkwargs \n"
Original    (006): ['*', 'pargs', ',', '**', 'pkwargs', '\\n']
Tokenized   (012): ['<s>', '*', 'p', 'args', ',', '**', 'p', 'kw', 'args', '\\', 'n', '</s>']
Filtered   (010): ['*', 'p', 'args', ',', '**', 'p', 'kw', 'args', '\\', 'n']
Detokenized (006): ['*', 'pargs', ',', '**', 'pkwargs', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "logdir = env . pop ( , os . path . join ( os . path . sep , ) ) \n"
Original    (022): ['logdir', '=', 'env', '.', 'pop', '(', ',', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'sep', ',', ')', ')', '\\n']
Tokenized   (026): ['<s>', 'log', 'dir', '=', 'env', '.', 'pop', '(', ',', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'sep', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['log', 'dir', '=', 'env', '.', 'pop', '(', ',', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'sep', ',', ')', ')', '\\', 'n']
Detokenized (022): ['logdir', '=', 'env', '.', 'pop', '(', ',', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'sep', ',', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "masksignals = bool ( env . pop ( , True ) ) \n"
Original    (013): ['masksignals', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', 'True', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'mas', 'ks', 'ign', 'als', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', 'True', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['mas', 'ks', 'ign', 'als', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', 'True', ')', ')', '\\', 'n']
Detokenized (013): ['masksignals', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', 'True', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "usetty = bool ( env . pop ( , ) ) \n"
Original    (012): ['usetty', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', ')', ')', '\\n']
Tokenized   (017): ['<s>', 'us', 'et', 'ty', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (015): ['us', 'et', 'ty', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', ')', ')', '\\', 'n']
Detokenized (012): ['usetty', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "maxfd = resource . getrlimit ( resource . RLIMIT_NOFILE ) [ 1 ] \n"
Original    (014): ['maxfd', '=', 'resource', '.', 'getrlimit', '(', 'resource', '.', 'RLIMIT_NOFILE', ')', '[', '1', ']', '\\n']
Tokenized   (025): ['<s>', 'max', 'fd', '=', 'resource', '.', 'get', 'r', 'limit', '(', 'resource', '.', 'RL', 'IM', 'IT', '_', 'NO', 'FILE', ')', '[', '1', ']', '\\', 'n', '</s>']
Filtered   (023): ['max', 'fd', '=', 'resource', '.', 'get', 'r', 'limit', '(', 'resource', '.', 'RL', 'IM', 'IT', '_', 'NO', 'FILE', ')', '[', '1', ']', '\\', 'n']
Detokenized (014): ['maxfd', '=', 'resource', '.', 'getrlimit', '(', 'resource', '.', 'RLIMIT_NOFILE', ')', '[', '1', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "hasattr ( os , "devnull" ) and os . devnull or "/dev/null" , \n"
Original    (014): ['hasattr', '(', 'os', ',', '"devnull"', ')', 'and', 'os', '.', 'devnull', 'or', '"/dev/null"', ',', '\\n']
Tokenized   (026): ['<s>', 'has', 'attr', '(', 'os', ',', '"', 'dev', 'null', '"', ')', 'and', 'os', '.', 'dev', 'null', 'or', '"/', 'dev', '/', 'null', '"', ',', '\\', 'n', '</s>']
Filtered   (024): ['has', 'attr', '(', 'os', ',', '"', 'dev', 'null', '"', ')', 'and', 'os', '.', 'dev', 'null', 'or', '"/', 'dev', '/', 'null', '"', ',', '\\', 'n']
Detokenized (014): ['hasattr', '(', 'os', ',', '"devnull"', ')', 'and', 'os', '.', 'devnull', 'or', '"/dev/null"', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "droned . logging . logToDir ( \n"
Original    (007): ['droned', '.', 'logging', '.', 'logToDir', '(', '\\n']
Tokenized   (014): ['<s>', 'd', 'ron', 'ed', '.', 'logging', '.', 'log', 'To', 'Dir', '(', '\\', 'n', '</s>']
Filtered   (012): ['d', 'ron', 'ed', '.', 'logging', '.', 'log', 'To', 'Dir', '(', '\\', 'n']
Detokenized (007): ['droned', '.', 'logging', '.', 'logToDir', '(', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "conversation . say ( contextSummary , useHTML = False ) \n"
Original    (011): ['conversation', '.', 'say', '(', 'contextSummary', ',', 'useHTML', '=', 'False', ')', '\\n']
Tokenized   (018): ['<s>', 'con', 'vers', 'ation', '.', 'say', '(', 'context', 'Summary', ',', 'use', 'HTML', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (016): ['con', 'vers', 'ation', '.', 'say', '(', 'context', 'Summary', ',', 'use', 'HTML', '=', 'False', ')', '\\', 'n']
Detokenized (011): ['conversation', '.', 'say', '(', 'contextSummary', ',', 'useHTML', '=', 'False', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "moduleProvides ( IDroneDService ) #requirement \n"
Original    (006): ['moduleProvides', '(', 'IDroneDService', ')', '#requirement', '\\n']
Tokenized   (017): ['<s>', 'module', 'Prov', 'ides', '(', 'ID', 'rone', 'DS', 'erv', 'ice', ')', '#', 'requ', 'irement', '\\', 'n', '</s>']
Filtered   (015): ['module', 'Prov', 'ides', '(', 'ID', 'rone', 'DS', 'erv', 'ice', ')', '#', 'requ', 'irement', '\\', 'n']
Detokenized (006): ['moduleProvides', '(', 'IDroneDService', ')', '#requirement', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "hour = property ( lambda foo : 3600 ) \n"
Original    (010): ['hour', '=', 'property', '(', 'lambda', 'foo', ':', '3600', ')', '\\n']
Tokenized   (014): ['<s>', 'hour', '=', 'property', '(', 'lambda', 'foo', ':', '36', '00', ')', '\\', 'n', '</s>']
Filtered   (012): ['hour', '=', 'property', '(', 'lambda', 'foo', ':', '36', '00', ')', '\\', 'n']
Detokenized (010): ['hour', '=', 'property', '(', 'lambda', 'foo', ':', '3600', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "watchDict = property ( lambda s : SERVICECONFIG . wrapped . get ( , { } ) ) \n"
Original    (019): ['watchDict', '=', 'property', '(', 'lambda', 's', ':', 'SERVICECONFIG', '.', 'wrapped', '.', 'get', '(', ',', '{', '}', ')', ')', '\\n']
Tokenized   (028): ['<s>', 'watch', 'D', 'ict', '=', 'property', '(', 'lambda', 's', ':', 'SERV', 'IC', 'EC', 'ON', 'FIG', '.', 'wrapped', '.', 'get', '(', ',', '{', '}', ')', ')', '\\', 'n', '</s>']
Filtered   (026): ['watch', 'D', 'ict', '=', 'property', '(', 'lambda', 's', ':', 'SERV', 'IC', 'EC', 'ON', 'FIG', '.', 'wrapped', '.', 'get', '(', ',', '{', '}', ')', ')', '\\', 'n']
Detokenized (019): ['watchDict', '=', 'property', '(', 'lambda', 's', ':', 'SERVICECONFIG', '.', 'wrapped', '.', 'get', '(', ',', '{', '}', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "model = gm . throat_surface_area . cylinder ) \n"
Original    (009): ['model', '=', 'gm', '.', 'throat_surface_area', '.', 'cylinder', ')', '\\n']
Tokenized   (017): ['<s>', 'model', '=', 'g', 'm', '.', 'throat', '_', 'surface', '_', 'area', '.', 'cylinder', ')', '\\', 'n', '</s>']
Filtered   (015): ['model', '=', 'g', 'm', '.', 'throat', '_', 'surface', '_', 'area', '.', 'cylinder', ')', '\\', 'n']
Detokenized (009): ['model', '=', 'gm', '.', 'throat_surface_area', '.', 'cylinder', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "pores = network . find_connected_pores ( throats , flatten = False ) \n"
Original    (013): ['pores', '=', 'network', '.', 'find_connected_pores', '(', 'throats', ',', 'flatten', '=', 'False', ')', '\\n']
Tokenized   (023): ['<s>', 'p', 'ores', '=', 'network', '.', 'find', '_', 'connected', '_', 'p', 'ores', '(', 'throats', ',', 'flatt', 'en', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (021): ['p', 'ores', '=', 'network', '.', 'find', '_', 'connected', '_', 'p', 'ores', '(', 'throats', ',', 'flatt', 'en', '=', 'False', ')', '\\', 'n']
Detokenized (013): ['pores', '=', 'network', '.', 'find_connected_pores', '(', 'throats', ',', 'flatten', '=', 'False', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "C0 = network [ ] [ pores , 0 ] \n"
Original    (011): ['C0', '=', 'network', '[', ']', '[', 'pores', ',', '0', ']', '\\n']
Tokenized   (015): ['<s>', 'C', '0', '=', 'network', '[', ']', '[', 'pores', ',', '0', ']', '\\', 'n', '</s>']
Filtered   (013): ['C', '0', '=', 'network', '[', ']', '[', 'pores', ',', '0', ']', '\\', 'n']
Detokenized (011): ['C0', '=', 'network', '[', ']', '[', 'pores', ',', '0', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "P = phase [ pore_P ] / 100000 \n"
Original    (009): ['P', '=', 'phase', '[', 'pore_P', ']', '/', '100000', '\\n']
Tokenized   (016): ['<s>', 'P', '=', 'phase', '[', 'p', 'ore', '_', 'P', ']', '/', '100', '000', '\\', 'n', '</s>']
Filtered   (014): ['P', '=', 'phase', '[', 'p', 'ore', '_', 'P', ']', '/', '100', '000', '\\', 'n']
Detokenized (009): ['P', '=', 'phase', '[', 'pore_P', ']', '/', '100000', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "a1 = - 1 / b \n"
Original    (007): ['a1', '=', '-', '1', '/', 'b', '\\n']
Tokenized   (011): ['<s>', 'a', '1', '=', '-', '1', '/', 'b', '\\', 'n', '</s>']
Filtered   (009): ['a', '1', '=', '-', '1', '/', 'b', '\\', 'n']
Detokenized (007): ['a1', '=', '-', '1', '/', 'b', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "a2 = ( R * T + b * P ) / ( a * b ) \n"
Original    (018): ['a2', '=', '(', 'R', '*', 'T', '+', 'b', '*', 'P', ')', '/', '(', 'a', '*', 'b', ')', '\\n']
Tokenized   (022): ['<s>', 'a', '2', '=', '(', 'R', '*', 'T', '+', 'b', '*', 'P', ')', '/', '(', 'a', '*', 'b', ')', '\\', 'n', '</s>']
Filtered   (020): ['a', '2', '=', '(', 'R', '*', 'T', '+', 'b', '*', 'P', ')', '/', '(', 'a', '*', 'b', ')', '\\', 'n']
Detokenized (018): ['a2', '=', '(', 'R', '*', 'T', '+', 'b', '*', 'P', ')', '/', '(', 'a', '*', 'b', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "a3 = - P / ( a * b ) \n"
Original    (011): ['a3', '=', '-', 'P', '/', '(', 'a', '*', 'b', ')', '\\n']
Tokenized   (015): ['<s>', 'a', '3', '=', '-', 'P', '/', '(', 'a', '*', 'b', ')', '\\', 'n', '</s>']
Filtered   (013): ['a', '3', '=', '-', 'P', '/', '(', 'a', '*', 'b', ')', '\\', 'n']
Detokenized (011): ['a3', '=', '-', 'P', '/', '(', 'a', '*', 'b', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "coeffs = sp . vstack ( ( a0 , a1 , a2 , a3 ) ) . T \n"
Original    (019): ['coeffs', '=', 'sp', '.', 'vstack', '(', '(', 'a0', ',', 'a1', ',', 'a2', ',', 'a3', ')', ')', '.', 'T', '\\n']
Tokenized   (029): ['<s>', 'co', 'eff', 's', '=', 'sp', '.', 'v', 'stack', '(', '(', 'a', '0', ',', 'a', '1', ',', 'a', '2', ',', 'a', '3', ')', ')', '.', 'T', '\\', 'n', '</s>']
Filtered   (027): ['co', 'eff', 's', '=', 'sp', '.', 'v', 'stack', '(', '(', 'a', '0', ',', 'a', '1', ',', 'a', '2', ',', 'a', '3', ')', ')', '.', 'T', '\\', 'n']
Detokenized (019): ['coeffs', '=', 'sp', '.', 'vstack', '(', '(', 'a0', ',', 'a1', ',', 'a2', ',', 'a3', ')', ')', '.', 'T', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "density = sp . array ( [ sp . roots ( C ) for C in coeffs ] ) \n"
Original    (020): ['density', '=', 'sp', '.', 'array', '(', '[', 'sp', '.', 'roots', '(', 'C', ')', 'for', 'C', 'in', 'coeffs', ']', ')', '\\n']
Tokenized   (025): ['<s>', 'density', '=', 'sp', '.', 'array', '(', '[', 'sp', '.', 'roots', '(', 'C', ')', 'for', 'C', 'in', 'co', 'eff', 's', ']', ')', '\\', 'n', '</s>']
Filtered   (023): ['density', '=', 'sp', '.', 'array', '(', '[', 'sp', '.', 'roots', '(', 'C', ')', 'for', 'C', 'in', 'co', 'eff', 's', ']', ')', '\\', 'n']
Detokenized (020): ['density', '=', 'sp', '.', 'array', '(', '[', 'sp', '.', 'roots', '(', 'C', ')', 'for', 'C', 'in', 'coeffs', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "comp2 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n"
Original    (015): ['comp2', '=', 'OpenPNM', '.', 'Phases', '.', 'GenericPhase', '(', 'network', '=', 'self', '.', 'net', ')', '\\n']
Tokenized   (023): ['<s>', 'comp', '2', '=', 'Open', 'PN', 'M', '.', 'Ph', 'ases', '.', 'Generic', 'Phase', '(', 'network', '=', 'self', '.', 'net', ')', '\\', 'n', '</s>']
Filtered   (021): ['comp', '2', '=', 'Open', 'PN', 'M', '.', 'Ph', 'ases', '.', 'Generic', 'Phase', '(', 'network', '=', 'self', '.', 'net', ')', '\\', 'n']
Detokenized (015): ['comp2', '=', 'OpenPNM', '.', 'Phases', '.', 'GenericPhase', '(', 'network', '=', 'self', '.', 'net', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "OpenPNM . Phases . GenericPhase ( network = self . net , \n"
Original    (013): ['OpenPNM', '.', 'Phases', '.', 'GenericPhase', '(', 'network', '=', 'self', '.', 'net', ',', '\\n']
Tokenized   (020): ['<s>', 'Open', 'PN', 'M', '.', 'Ph', 'ases', '.', 'Generic', 'Phase', '(', 'network', '=', 'self', '.', 'net', ',', '\\', 'n', '</s>']
Filtered   (018): ['Open', 'PN', 'M', '.', 'Ph', 'ases', '.', 'Generic', 'Phase', '(', 'network', '=', 'self', '.', 'net', ',', '\\', 'n']
Detokenized (013): ['OpenPNM', '.', 'Phases', '.', 'GenericPhase', '(', 'network', '=', 'self', '.', 'net', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "components = [ comp1 , comp2 ] ) \n"
Original    (009): ['components', '=', '[', 'comp1', ',', 'comp2', ']', ')', '\\n']
Tokenized   (015): ['<s>', 'comp', 'onents', '=', '[', 'comp', '1', ',', 'comp', '2', ']', ')', '\\', 'n', '</s>']
Filtered   (013): ['comp', 'onents', '=', '[', 'comp', '1', ',', 'comp', '2', ']', ')', '\\', 'n']
Detokenized (009): ['components', '=', '[', 'comp1', ',', 'comp2', ']', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "phase . set_component ( comp2 , mode = ) \n"
Original    (010): ['phase', '.', 'set_component', '(', 'comp2', ',', 'mode', '=', ')', '\\n']
Tokenized   (016): ['<s>', 'phase', '.', 'set', '_', 'component', '(', 'comp', '2', ',', 'mode', '=', ')', '\\', 'n', '</s>']
Filtered   (014): ['phase', '.', 'set', '_', 'component', '(', 'comp', '2', ',', 'mode', '=', ')', '\\', 'n']
Detokenized (010): ['phase', '.', 'set_component', '(', 'comp2', ',', 'mode', '=', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "best_seq = fd [ x ] . sequence \n"
Original    (009): ['best_seq', '=', 'fd', '[', 'x', ']', '.', 'sequence', '\\n']
Tokenized   (015): ['<s>', 'best', '_', 'seq', '=', 'f', 'd', '[', 'x', ']', '.', 'sequence', '\\', 'n', '</s>']
Filtered   (013): ['best', '_', 'seq', '=', 'f', 'd', '[', 'x', ']', '.', 'sequence', '\\', 'n']
Detokenized (009): ['best_seq', '=', 'fd', '[', 'x', ']', '.', 'sequence', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "best_id , best_seq , best_qual = rep_info [ pb_id ] \n"
Original    (011): ['best_id', ',', 'best_seq', ',', 'best_qual', '=', 'rep_info', '[', 'pb_id', ']', '\\n']
Tokenized   (025): ['<s>', 'best', '_', 'id', ',', 'best', '_', 'seq', ',', 'best', '_', 'qual', '=', 'rep', '_', 'info', '[', 'p', 'b', '_', 'id', ']', '\\', 'n', '</s>']
Filtered   (023): ['best', '_', 'id', ',', 'best', '_', 'seq', ',', 'best', '_', 'qual', '=', 'rep', '_', 'info', '[', 'p', 'b', '_', 'id', ']', '\\', 'n']
Detokenized (011): ['best_id', ',', 'best_seq', ',', 'best_qual', '=', 'rep_info', '[', 'pb_id', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "_id_ = "{0}|{1}|{2}" . format ( pb_id , coords [ best_id ] , best_id ) \n"
Original    (016): ['_id_', '=', '"{0}|{1}|{2}"', '.', 'format', '(', 'pb_id', ',', 'coords', '[', 'best_id', ']', ',', 'best_id', ')', '\\n']
Tokenized   (039): ['<s>', '_', 'id', '_', '=', '"{', '0', '}', '|', '{', '1', '}', '|', '{', '2', '}"', '.', 'format', '(', 'p', 'b', '_', 'id', ',', 'co', 'ords', '[', 'best', '_', 'id', ']', ',', 'best', '_', 'id', ')', '\\', 'n', '</s>']
Filtered   (037): ['_', 'id', '_', '=', '"{', '0', '}', '|', '{', '1', '}', '|', '{', '2', '}"', '.', 'format', '(', 'p', 'b', '_', 'id', ',', 'co', 'ords', '[', 'best', '_', 'id', ']', ',', 'best', '_', 'id', ')', '\\', 'n']
Detokenized (016): ['_id_', '=', '"{0}|{1}|{2}"', '.', 'format', '(', 'pb_id', ',', 'coords', '[', 'best_id', ']', ',', 'best_id', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "iter = BioReaders . GMAPSAMReader ( gmap_sam_filename , True , query_len_dict = transfrag_len_dict ) \n"
Original    (015): ['iter', '=', 'BioReaders', '.', 'GMAPSAMReader', '(', 'gmap_sam_filename', ',', 'True', ',', 'query_len_dict', '=', 'transfrag_len_dict', ')', '\\n']
Tokenized   (038): ['<s>', 'iter', '=', 'Bio', 'Read', 'ers', '.', 'GM', 'APS', 'AM', 'Reader', '(', 'g', 'map', '_', 'sam', '_', 'filename', ',', 'True', ',', 'query', '_', 'len', '_', 'dict', '=', 'trans', 'fr', 'ag', '_', 'len', '_', 'dict', ')', '\\', 'n', '</s>']
Filtered   (036): ['iter', '=', 'Bio', 'Read', 'ers', '.', 'GM', 'APS', 'AM', 'Reader', '(', 'g', 'map', '_', 'sam', '_', 'filename', ',', 'True', ',', 'query', '_', 'len', '_', 'dict', '=', 'trans', 'fr', 'ag', '_', 'len', '_', 'dict', ')', '\\', 'n']
Detokenized (015): ['iter', '=', 'BioReaders', '.', 'GMAPSAMReader', '(', 'gmap_sam_filename', ',', 'True', ',', 'query_len_dict', '=', 'transfrag_len_dict', ')', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "TmpRec = namedtuple ( , [ , , , , , , ] ) \n"
Original    (015): ['TmpRec', '=', 'namedtuple', '(', ',', '[', ',', ',', ',', ',', ',', ',', ']', ')', '\\n']
Tokenized   (022): ['<s>', 'T', 'mp', 'Rec', '=', 'named', 't', 'uple', '(', ',', '[', ',', ',', ',', ',', ',', ',', ']', ')', '\\', 'n', '</s>']
Filtered   (020): ['T', 'mp', 'Rec', '=', 'named', 't', 'uple', '(', ',', '[', ',', ',', ',', ',', ',', ',', ']', ')', '\\', 'n']
Detokenized (015): ['TmpRec', '=', 'namedtuple', '(', ',', '[', ',', ',', ',', ',', ',', ',', ']', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "compressed_records_pointer_dict = defaultdict ( lambda : [ ] ) \n"
Original    (010): ['compressed_records_pointer_dict', '=', 'defaultdict', '(', 'lambda', ':', '[', ']', ')', '\\n']
Tokenized   (022): ['<s>', 'comp', 'ressed', '_', 'rec', 'ords', '_', 'pointer', '_', 'dict', '=', 'default', 'dict', '(', 'lambda', ':', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (020): ['comp', 'ressed', '_', 'rec', 'ords', '_', 'pointer', '_', 'dict', '=', 'default', 'dict', '(', 'lambda', ':', '[', ']', ')', '\\', 'n']
Detokenized (010): ['compressed_records_pointer_dict', '=', 'defaultdict', '(', 'lambda', ':', '[', ']', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "check_ids_unique ( fa_or_fq_filename , is_fq = is_fq ) \n"
Original    (009): ['check_ids_unique', '(', 'fa_or_fq_filename', ',', 'is_fq', '=', 'is_fq', ')', '\\n']
Tokenized   (029): ['<s>', 'check', '_', 'ids', '_', 'unique', '(', 'fa', '_', 'or', '_', 'f', 'q', '_', 'filename', ',', 'is', '_', 'f', 'q', '=', 'is', '_', 'f', 'q', ')', '\\', 'n', '</s>']
Filtered   (027): ['check', '_', 'ids', '_', 'unique', '(', 'fa', '_', 'or', '_', 'f', 'q', '_', 'filename', ',', 'is', '_', 'f', 'q', '=', 'is', '_', 'f', 'q', ')', '\\', 'n']
Detokenized (009): ['check_ids_unique', '(', 'fa_or_fq_filename', ',', 'is_fq', '=', 'is_fq', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "fusion_candidates = find_fusion_candidates ( sam_filename , bs . transfrag_len_dict , min_locus_coverage \n"
Original    (012): ['fusion_candidates', '=', 'find_fusion_candidates', '(', 'sam_filename', ',', 'bs', '.', 'transfrag_len_dict', ',', 'min_locus_coverage', '\\n']
Tokenized   (040): ['<s>', 'f', 'usion', '_', 'cand', 'idates', '=', 'find', '_', 'f', 'usion', '_', 'cand', 'idates', '(', 'sam', '_', 'filename', ',', 'b', 's', '.', 'trans', 'fr', 'ag', '_', 'len', '_', 'dict', ',', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', '\\', 'n', '</s>']
Filtered   (038): ['f', 'usion', '_', 'cand', 'idates', '=', 'find', '_', 'f', 'usion', '_', 'cand', 'idates', '(', 'sam', '_', 'filename', ',', 'b', 's', '.', 'trans', 'fr', 'ag', '_', 'len', '_', 'dict', ',', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', '\\', 'n']
Detokenized (012): ['fusion_candidates', '=', 'find_fusion_candidates', '(', 'sam_filename', ',', 'bs', '.', 'transfrag_len_dict', ',', 'min_locus_coverage', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "pbid1 , groups1 = line . strip ( ) . split ( ) \n"
Original    (014): ['pbid1', ',', 'groups1', '=', 'line', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Tokenized   (020): ['<s>', 'p', 'bid', '1', ',', 'groups', '1', '=', 'line', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n', '</s>']
Filtered   (018): ['p', 'bid', '1', ',', 'groups', '1', '=', 'line', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n']
Detokenized (014): ['pbid1', ',', 'groups1', '=', 'line', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "pbid2 , groups2 = f . readline ( ) . strip ( ) . split ( ) \n"
Original    (018): ['pbid2', ',', 'groups2', '=', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Tokenized   (025): ['<s>', 'p', 'bid', '2', ',', 'groups', '2', '=', 'f', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n', '</s>']
Filtered   (023): ['p', 'bid', '2', ',', 'groups', '2', '=', 'f', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n']
Detokenized (018): ['pbid2', ',', 'groups2', '=', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "f_group . write ( "{0}\\t{1}\\n" . format ( pbid1 [ : pbid1 . rfind ( ) ] , "," . join ( group ) ) ) \n"
Original    (027): ['f_group', '.', 'write', '(', '"{0}\\\\t{1}\\\\n"', '.', 'format', '(', 'pbid1', '[', ':', 'pbid1', '.', 'rfind', '(', ')', ']', ',', '","', '.', 'join', '(', 'group', ')', ')', ')', '\\n']
Tokenized   (048): ['<s>', 'f', '_', 'group', '.', 'write', '(', '"{', '0', '}', '\\\\', 't', '{', '1', '}', '\\\\', 'n', '"', '.', 'format', '(', 'p', 'bid', '1', '[', ':', 'p', 'bid', '1', '.', 'r', 'find', '(', ')', ']', ',', '"', ',"', '.', 'join', '(', 'group', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (046): ['f', '_', 'group', '.', 'write', '(', '"{', '0', '}', '\\\\', 't', '{', '1', '}', '\\\\', 'n', '"', '.', 'format', '(', 'p', 'bid', '1', '[', ':', 'p', 'bid', '1', '.', 'r', 'find', '(', ')', ']', ',', '"', ',"', '.', 'join', '(', 'group', ')', ')', ')', '\\', 'n']
Detokenized (027): ['f_group', '.', 'write', '(', '"{0}\\\\t{1}\\\\n"', '.', 'format', '(', 'pbid1', '[', ':', 'pbid1', '.', 'rfind', '(', ')', ']', ',', '","', '.', 'join', '(', 'group', ')', ')', ')', '\\n']
Counter: 46
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "group_info [ pbid1 [ : pbid1 . rfind ( ) ] ] = list ( group ) \n"
Original    (018): ['group_info', '[', 'pbid1', '[', ':', 'pbid1', '.', 'rfind', '(', ')', ']', ']', '=', 'list', '(', 'group', ')', '\\n']
Tokenized   (028): ['<s>', 'group', '_', 'info', '[', 'p', 'bid', '1', '[', ':', 'p', 'bid', '1', '.', 'r', 'find', '(', ')', ']', ']', '=', 'list', '(', 'group', ')', '\\', 'n', '</s>']
Filtered   (026): ['group', '_', 'info', '[', 'p', 'bid', '1', '[', ':', 'p', 'bid', '1', '.', 'r', 'find', '(', ')', ']', ']', '=', 'list', '(', 'group', ')', '\\', 'n']
Detokenized (018): ['group_info', '[', 'pbid1', '[', ':', 'pbid1', '.', 'rfind', '(', ')', ']', ']', '=', 'list', '(', 'group', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "d1 . update ( d [ ] ) \n"
Original    (009): ['d1', '.', 'update', '(', 'd', '[', ']', ')', '\\n']
Tokenized   (013): ['<s>', 'd', '1', '.', 'update', '(', 'd', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (011): ['d', '1', '.', 'update', '(', 'd', '[', ']', ')', '\\', 'n']
Detokenized (009): ['d1', '.', 'update', '(', 'd', '[', ']', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "fusion_main ( args . input , args . sam , args . prefix , \n"
Original    (015): ['fusion_main', '(', 'args', '.', 'input', ',', 'args', '.', 'sam', ',', 'args', '.', 'prefix', ',', '\\n']
Tokenized   (021): ['<s>', 'f', 'usion', '_', 'main', '(', 'args', '.', 'input', ',', 'args', '.', 'sam', ',', 'args', '.', 'prefix', ',', '\\', 'n', '</s>']
Filtered   (019): ['f', 'usion', '_', 'main', '(', 'args', '.', 'input', ',', 'args', '.', 'sam', ',', 'args', '.', 'prefix', ',', '\\', 'n']
Detokenized (015): ['fusion_main', '(', 'args', '.', 'input', ',', 'args', '.', 'sam', ',', 'args', '.', 'prefix', ',', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "is_fq = args . fq , allow_extra_5_exons = args . allow_extra_5exon , \n"
Original    (013): ['is_fq', '=', 'args', '.', 'fq', ',', 'allow_extra_5_exons', '=', 'args', '.', 'allow_extra_5exon', ',', '\\n']
Tokenized   (033): ['<s>', 'is', '_', 'f', 'q', '=', 'args', '.', 'f', 'q', ',', 'allow', '_', 'extra', '_', '5', '_', 'ex', 'ons', '=', 'args', '.', 'allow', '_', 'extra', '_', '5', 'ex', 'on', ',', '\\', 'n', '</s>']
Filtered   (031): ['is', '_', 'f', 'q', '=', 'args', '.', 'f', 'q', ',', 'allow', '_', 'extra', '_', '5', '_', 'ex', 'ons', '=', 'args', '.', 'allow', '_', 'extra', '_', '5', 'ex', 'on', ',', '\\', 'n']
Detokenized (013): ['is_fq', '=', 'args', '.', 'fq', ',', 'allow_extra_5_exons', '=', 'args', '.', 'allow_extra_5exon', ',', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "skip_5_exon_alt = False , prefix_dict_pickle_filename = args . prefix_dict_pickle_filename , min_locus_coverage = args . min_locus_coverage , min_locus_coverage_bp = args . min_locus_coverage_bp min_total_coverage = args . min_total_coverage , \n"
Original    (028): ['skip_5_exon_alt', '=', 'False', ',', 'prefix_dict_pickle_filename', '=', 'args', '.', 'prefix_dict_pickle_filename', ',', 'min_locus_coverage', '=', 'args', '.', 'min_locus_coverage', ',', 'min_locus_coverage_bp', '=', 'args', '.', 'min_locus_coverage_bp', 'min_total_coverage', '=', 'args', '.', 'min_total_coverage', ',', '\\n']
Tokenized   (090): ['<s>', 'skip', '_', '5', '_', 'ex', 'on', '_', 'alt', '=', 'False', ',', 'prefix', '_', 'dict', '_', 'pick', 'le', '_', 'filename', '=', 'args', '.', 'prefix', '_', 'dict', '_', 'pick', 'le', '_', 'filename', ',', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', '=', 'args', '.', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', ',', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', '_', 'bp', '=', 'args', '.', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', '_', 'bp', 'min', '_', 'total', '_', 'co', 'verage', '=', 'args', '.', 'min', '_', 'total', '_', 'co', 'verage', ',', '\\', 'n', '</s>']
Filtered   (088): ['skip', '_', '5', '_', 'ex', 'on', '_', 'alt', '=', 'False', ',', 'prefix', '_', 'dict', '_', 'pick', 'le', '_', 'filename', '=', 'args', '.', 'prefix', '_', 'dict', '_', 'pick', 'le', '_', 'filename', ',', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', '=', 'args', '.', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', ',', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', '_', 'bp', '=', 'args', '.', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', '_', 'bp', 'min', '_', 'total', '_', 'co', 'verage', '=', 'args', '.', 'min', '_', 'total', '_', 'co', 'verage', ',', '\\', 'n']
Detokenized (028): ['skip_5_exon_alt', '=', 'False', ',', 'prefix_dict_pickle_filename', '=', 'args', '.', 'prefix_dict_pickle_filename', ',', 'min_locus_coverage', '=', 'args', '.', 'min_locus_coverage', ',', 'min_locus_coverage_bp', '=', 'args', '.', 'min_locus_coverage_bp', 'min_total_coverage', '=', 'args', '.', 'min_total_coverage', ',', '\\n']
Counter: 88
===================================================================
Hidden states:  (13, 28, 768)
# Extracted words:  28
Sentence         : "raw = self . f . readline ( ) . strip ( ) . split ( ) \n"
Original    (018): ['raw', '=', 'self', '.', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Tokenized   (022): ['<s>', 'raw', '=', 'self', '.', 'f', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n', '</s>']
Filtered   (020): ['raw', '=', 'self', '.', 'f', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n']
Detokenized (018): ['raw', '=', 'self', '.', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "iden = float ( raw [ 3 ] ) \n"
Original    (010): ['iden', '=', 'float', '(', 'raw', '[', '3', ']', ')', '\\n']
Tokenized   (013): ['<s>', 'iden', '=', 'float', '(', 'raw', '[', '3', ']', ')', '\\', 'n', '</s>']
Filtered   (011): ['iden', '=', 'float', '(', 'raw', '[', '3', ']', ')', '\\', 'n']
Detokenized (010): ['iden', '=', 'float', '(', 'raw', '[', '3', ']', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "_qStart , qAln = self . f . readline ( ) . strip ( ) . split ( ) \n"
Original    (020): ['_qStart', ',', 'qAln', '=', 'self', '.', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Tokenized   (028): ['<s>', '_', 'q', 'Start', ',', 'q', 'Al', 'n', '=', 'self', '.', 'f', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n', '</s>']
Filtered   (026): ['_', 'q', 'Start', ',', 'q', 'Al', 'n', '=', 'self', '.', 'f', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n']
Detokenized (020): ['_qStart', ',', 'qAln', '=', 'self', '.', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "_sStart , sAln = self . f . readline ( ) . strip ( ) . split ( ) [ : 2 ] \n"
Original    (024): ['_sStart', ',', 'sAln', '=', 'self', '.', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '[', ':', '2', ']', '\\n']
Tokenized   (032): ['<s>', '_', 's', 'Start', ',', 's', 'Al', 'n', '=', 'self', '.', 'f', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '[', ':', '2', ']', '\\', 'n', '</s>']
Filtered   (030): ['_', 's', 'Start', ',', 's', 'Al', 'n', '=', 'self', '.', 'f', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '[', ':', '2', ']', '\\', 'n']
Detokenized (024): ['_sStart', ',', 'sAln', '=', 'self', '.', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '[', ':', '2', ']', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "missed_q = missed_q * 1. / r . qLength , \n"
Original    (011): ['missed_q', '=', 'missed_q', '*', '1.', '/', 'r', '.', 'qLength', ',', '\\n']
Tokenized   (021): ['<s>', 'miss', 'ed', '_', 'q', '=', 'missed', '_', 'q', '*', '1', '.', '/', 'r', '.', 'q', 'Length', ',', '\\', 'n', '</s>']
Filtered   (019): ['miss', 'ed', '_', 'q', '=', 'missed', '_', 'q', '*', '1', '.', '/', 'r', '.', 'q', 'Length', ',', '\\', 'n']
Detokenized (011): ['missed_q', '=', 'missed_q', '*', '1.', '/', 'r', '.', 'qLength', ',', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "ece_penalty , ece_min_len ) : \n"
Original    (006): ['ece_penalty', ',', 'ece_min_len', ')', ':', '\\n']
Tokenized   (018): ['<s>', 'e', 'ce', '_', 'pen', 'alty', ',', 'e', 'ce', '_', 'min', '_', 'len', ')', ':', '\\', 'n', '</s>']
Filtered   (016): ['e', 'ce', '_', 'pen', 'alty', ',', 'e', 'ce', '_', 'min', '_', 'len', ')', ':', '\\', 'n']
Detokenized (006): ['ece_penalty', ',', 'ece_min_len', ')', ':', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "heading = % ( current_indent , , self . heading ) \n"
Original    (012): ['heading', '=', '%', '(', 'current_indent', ',', ',', 'self', '.', 'heading', ')', '\\n']
Tokenized   (018): ['<s>', 'heading', '=', '%', '(', 'current', '_', 'ind', 'ent', ',', ',', 'self', '.', 'heading', ')', '\\', 'n', '</s>']
Filtered   (016): ['heading', '=', '%', '(', 'current', '_', 'ind', 'ent', ',', ',', 'self', '.', 'heading', ')', '\\', 'n']
Detokenized (012): ['heading', '=', '%', '(', 'current_indent', ',', ',', 'self', '.', 'heading', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "section = self . _Section ( self , self . _current_section , heading ) \n"
Original    (015): ['section', '=', 'self', '.', '_Section', '(', 'self', ',', 'self', '.', '_current_section', ',', 'heading', ')', '\\n']
Tokenized   (022): ['<s>', 'section', '=', 'self', '.', '_', 'Section', '(', 'self', ',', 'self', '.', '_', 'current', '_', 'section', ',', 'heading', ')', '\\', 'n', '</s>']
Filtered   (020): ['section', '=', 'self', '.', '_', 'Section', '(', 'self', ',', 'self', '.', '_', 'current', '_', 'section', ',', 'heading', ')', '\\', 'n']
Detokenized (015): ['section', '=', 'self', '.', '_Section', '(', 'self', ',', 'self', '.', '_current_section', ',', 'heading', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "invocations = [ get_invocation ( action ) ] \n"
Original    (009): ['invocations', '=', '[', 'get_invocation', '(', 'action', ')', ']', '\\n']
Tokenized   (016): ['<s>', 'inv', 'ocations', '=', '[', 'get', '_', 'inv', 'ocation', '(', 'action', ')', ']', '\\', 'n', '</s>']
Filtered   (014): ['inv', 'ocations', '=', '[', 'get', '_', 'inv', 'ocation', '(', 'action', ')', ']', '\\', 'n']
Detokenized (009): ['invocations', '=', '[', 'get_invocation', '(', 'action', ')', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "action_usage = format ( optionals + positionals , groups ) \n"
Original    (011): ['action_usage', '=', 'format', '(', 'optionals', '+', 'positionals', ',', 'groups', ')', '\\n']
Tokenized   (018): ['<s>', 'action', '_', 'usage', '=', 'format', '(', 'option', 'als', '+', 'position', 'als', ',', 'groups', ')', '\\', 'n', '</s>']
Filtered   (016): ['action', '_', 'usage', '=', 'format', '(', 'option', 'als', '+', 'position', 'als', ',', 'groups', ')', '\\', 'n']
Detokenized (011): ['action_usage', '=', 'format', '(', 'optionals', '+', 'positionals', ',', 'groups', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "text_width = self . _width - self . _current_indent \n"
Original    (010): ['text_width', '=', 'self', '.', '_width', '-', 'self', '.', '_current_indent', '\\n']
Tokenized   (020): ['<s>', 'text', '_', 'width', '=', 'self', '.', '_', 'width', '-', 'self', '.', '_', 'current', '_', 'ind', 'ent', '\\', 'n', '</s>']
Filtered   (018): ['text', '_', 'width', '=', 'self', '.', '_', 'width', '-', 'self', '.', '_', 'current', '_', 'ind', 'ent', '\\', 'n']
Detokenized (010): ['text_width', '=', 'self', '.', '_width', '-', 'self', '.', '_current_indent', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "line_len += len ( part ) + 1 \n"
Original    (009): ['line_len', '+=', 'len', '(', 'part', ')', '+', '1', '\\n']
Tokenized   (014): ['<s>', 'line', '_', 'len', '+=', 'len', '(', 'part', ')', '+', '1', '\\', 'n', '</s>']
Filtered   (012): ['line', '_', 'len', '+=', 'len', '(', 'part', ')', '+', '1', '\\', 'n']
Detokenized (009): ['line_len', '+=', 'len', '(', 'part', ')', '+', '1', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "part = % ( option_string , args_string ) \n"
Original    (009): ['part', '=', '%', '(', 'option_string', ',', 'args_string', ')', '\\n']
Tokenized   (016): ['<s>', 'part', '=', '%', '(', 'option', '_', 'string', ',', 'args', '_', 'string', ')', '\\', 'n', '</s>']
Filtered   (014): ['part', '=', '%', '(', 'option', '_', 'string', ',', 'args', '_', 'string', ')', '\\', 'n']
Detokenized (009): ['part', '=', '%', '(', 'option_string', ',', 'args_string', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "indent = * self . _current_indent \n"
Original    (007): ['indent', '=', '*', 'self', '.', '_current_indent', '\\n']
Tokenized   (015): ['<s>', 'ind', 'ent', '=', '*', 'self', '.', '_', 'current', '_', 'ind', 'ent', '\\', 'n', '</s>']
Filtered   (013): ['ind', 'ent', '=', '*', 'self', '.', '_', 'current', '_', 'ind', 'ent', '\\', 'n']
Detokenized (007): ['indent', '=', '*', 'self', '.', '_current_indent', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "help_width = self . _width - help_position \n"
Original    (008): ['help_width', '=', 'self', '.', '_width', '-', 'help_position', '\\n']
Tokenized   (016): ['<s>', 'help', '_', 'width', '=', 'self', '.', '_', 'width', '-', 'help', '_', 'position', '\\', 'n', '</s>']
Filtered   (014): ['help', '_', 'width', '=', 'self', '.', '_', 'width', '-', 'help', '_', 'position', '\\', 'n']
Detokenized (008): ['help_width', '=', 'self', '.', '_width', '-', 'help_position', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "action_width = help_position - self . _current_indent - 2 \n"
Original    (010): ['action_width', '=', 'help_position', '-', 'self', '.', '_current_indent', '-', '2', '\\n']
Tokenized   (021): ['<s>', 'action', '_', 'width', '=', 'help', '_', 'position', '-', 'self', '.', '_', 'current', '_', 'ind', 'ent', '-', '2', '\\', 'n', '</s>']
Filtered   (019): ['action', '_', 'width', '=', 'help', '_', 'position', '-', 'self', '.', '_', 'current', '_', 'ind', 'ent', '-', '2', '\\', 'n']
Detokenized (010): ['action_width', '=', 'help_position', '-', 'self', '.', '_current_indent', '-', '2', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sup . __init__ ( option_strings = [ ] , dest = name , help = help ) \n"
Original    (018): ['sup', '.', '__init__', '(', 'option_strings', '=', '[', ']', ',', 'dest', '=', 'name', ',', 'help', '=', 'help', ')', '\\n']
Tokenized   (025): ['<s>', 'sup', '.', '__', 'init', '__', '(', 'option', '_', 'strings', '=', '[', ']', ',', 'dest', '=', 'name', ',', 'help', '=', 'help', ')', '\\', 'n', '</s>']
Filtered   (023): ['sup', '.', '__', 'init', '__', '(', 'option', '_', 'strings', '=', '[', ']', ',', 'dest', '=', 'name', ',', 'help', '=', 'help', ')', '\\', 'n']
Detokenized (018): ['sup', '.', '__init__', '(', 'option_strings', '=', '[', ']', ',', 'dest', '=', 'name', ',', 'help', '=', 'help', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "arg_strings = values [ 1 : ] \n"
Original    (008): ['arg_strings', '=', 'values', '[', '1', ':', ']', '\\n']
Tokenized   (013): ['<s>', 'arg', '_', 'strings', '=', 'values', '[', '1', ':', ']', '\\', 'n', '</s>']
Filtered   (011): ['arg', '_', 'strings', '=', 'values', '[', '1', ':', ']', '\\', 'n']
Detokenized (008): ['arg_strings', '=', 'values', '[', '1', ':', ']', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "args_str = . join ( [ repr ( arg ) for arg in args if arg is not None ] ) \n"
Original    (022): ['args_str', '=', '.', 'join', '(', '[', 'repr', '(', 'arg', ')', 'for', 'arg', 'in', 'args', 'if', 'arg', 'is', 'not', 'None', ']', ')', '\\n']
Tokenized   (027): ['<s>', 'args', '_', 'str', '=', '.', 'join', '(', '[', 'repr', '(', 'arg', ')', 'for', 'arg', 'in', 'args', 'if', 'arg', 'is', 'not', 'None', ']', ')', '\\', 'n', '</s>']
Filtered   (025): ['args', '_', 'str', '=', '.', 'join', '(', '[', 'repr', '(', 'arg', ')', 'for', 'arg', 'in', 'args', 'if', 'arg', 'is', 'not', 'None', ']', ')', '\\', 'n']
Detokenized (022): ['args_str', '=', '.', 'join', '(', '[', 'repr', '(', 'arg', ')', 'for', 'arg', 'in', 'args', 'if', 'arg', 'is', 'not', 'None', ']', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "type_func = self . _registry_get ( , action . type , action . type ) \n"
Original    (016): ['type_func', '=', 'self', '.', '_registry_get', '(', ',', 'action', '.', 'type', ',', 'action', '.', 'type', ')', '\\n']
Tokenized   (025): ['<s>', 'type', '_', 'func', '=', 'self', '.', '_', 'reg', 'istry', '_', 'get', '(', ',', 'action', '.', 'type', ',', 'action', '.', 'type', ')', '\\', 'n', '</s>']
Filtered   (023): ['type', '_', 'func', '=', 'self', '.', '_', 'reg', 'istry', '_', 'get', '(', ',', 'action', '.', 'type', ',', 'action', '.', 'type', ')', '\\', 'n']
Detokenized (016): ['type_func', '=', 'self', '.', '_registry_get', '(', ',', 'action', '.', 'type', ',', 'action', '.', 'type', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "conflict_string = . join ( [ option_string \n"
Original    (008): ['conflict_string', '=', '.', 'join', '(', '[', 'option_string', '\\n']
Tokenized   (016): ['<s>', 'conf', 'lict', '_', 'string', '=', '.', 'join', '(', '[', 'option', '_', 'string', '\\', 'n', '</s>']
Filtered   (014): ['conf', 'lict', '_', 'string', '=', '.', 'join', '(', '[', 'option', '_', 'string', '\\', 'n']
Detokenized (008): ['conflict_string', '=', '.', 'join', '(', '[', 'option_string', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "in conflicting_actions ] ) \n"
Original    (005): ['in', 'conflicting_actions', ']', ')', '\\n']
Tokenized   (010): ['<s>', 'in', 'conflicting', '_', 'actions', ']', ')', '\\', 'n', '</s>']
Filtered   (008): ['in', 'conflicting', '_', 'actions', ']', ')', '\\', 'n']
Detokenized (005): ['in', 'conflicting_actions', ']', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "super_init ( description = description , ** kwargs ) \n"
Original    (010): ['super_init', '(', 'description', '=', 'description', ',', '**', 'kwargs', ')', '\\n']
Tokenized   (017): ['<s>', 'super', '_', 'init', '(', 'description', '=', 'description', ',', '**', 'k', 'w', 'args', ')', '\\', 'n', '</s>']
Filtered   (015): ['super', '_', 'init', '(', 'description', '=', 'description', ',', '**', 'k', 'w', 'args', ')', '\\', 'n']
Detokenized (010): ['super_init', '(', 'description', '=', 'description', ',', '**', 'kwargs', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : """"instead""" , DeprecationWarning ) \n"
Original    (005): ['"""instead"""', ',', 'DeprecationWarning', ')', '\\n']
Tokenized   (013): ['<s>', '"""', 'instead', '"""', ',', 'Dep', 'rec', 'ation', 'Warning', ')', '\\', 'n', '</s>']
Filtered   (011): ['"""', 'instead', '"""', ',', 'Dep', 'rec', 'ation', 'Warning', ')', '\\', 'n']
Detokenized (005): ['"""instead"""', ',', 'DeprecationWarning', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "superinit ( description = description , \n"
Original    (007): ['superinit', '(', 'description', '=', 'description', ',', '\\n']
Tokenized   (011): ['<s>', 'super', 'init', '(', 'description', '=', 'description', ',', '\\', 'n', '</s>']
Filtered   (009): ['super', 'init', '(', 'description', '=', 'description', ',', '\\', 'n']
Detokenized (007): ['superinit', '(', 'description', '=', 'description', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "default_prefix + , default_prefix * 2 + , \n"
Original    (009): ['default_prefix', '+', ',', 'default_prefix', '*', '2', '+', ',', '\\n']
Tokenized   (016): ['<s>', 'default', '_', 'prefix', '+', ',', 'default', '_', 'prefix', '*', '2', '+', ',', '\\', 'n', '</s>']
Filtered   (014): ['default', '_', 'prefix', '+', ',', 'default', '_', 'prefix', '*', '2', '+', ',', '\\', 'n']
Detokenized (009): ['default_prefix', '+', ',', 'default_prefix', '*', '2', '+', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "conflicts . extend ( group_actions [ i + 1 : ] ) \n"
Original    (013): ['conflicts', '.', 'extend', '(', 'group_actions', '[', 'i', '+', '1', ':', ']', ')', '\\n']
Tokenized   (019): ['<s>', 'conf', 'licts', '.', 'extend', '(', 'group', '_', 'actions', '[', 'i', '+', '1', ':', ']', ')', '\\', 'n', '</s>']
Filtered   (017): ['conf', 'licts', '.', 'extend', '(', 'group', '_', 'actions', '[', 'i', '+', '1', ':', ']', ')', '\\', 'n']
Detokenized (013): ['conflicts', '.', 'extend', '(', 'group_actions', '[', 'i', '+', '1', ':', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "action , option_string , explicit_arg = option_tuple \n"
Original    (008): ['action', ',', 'option_string', ',', 'explicit_arg', '=', 'option_tuple', '\\n']
Tokenized   (018): ['<s>', 'action', ',', 'option', '_', 'string', ',', 'explicit', '_', 'arg', '=', 'option', '_', 't', 'uple', '\\', 'n', '</s>']
Filtered   (016): ['action', ',', 'option', '_', 'string', ',', 'explicit', '_', 'arg', '=', 'option', '_', 't', 'uple', '\\', 'n']
Detokenized (008): ['action', ',', 'option_string', ',', 'explicit_arg', '=', 'option_tuple', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "option_string = char + explicit_arg [ 0 ] \n"
Original    (009): ['option_string', '=', 'char', '+', 'explicit_arg', '[', '0', ']', '\\n']
Tokenized   (016): ['<s>', 'option', '_', 'string', '=', 'char', '+', 'explicit', '_', 'arg', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (014): ['option', '_', 'string', '=', 'char', '+', 'explicit', '_', 'arg', '[', '0', ']', '\\', 'n']
Detokenized (009): ['option_string', '=', 'char', '+', 'explicit_arg', '[', '0', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "new_explicit_arg = explicit_arg [ 1 : ] or None \n"
Original    (010): ['new_explicit_arg', '=', 'explicit_arg', '[', '1', ':', ']', 'or', 'None', '\\n']
Tokenized   (020): ['<s>', 'new', '_', 'expl', 'icit', '_', 'arg', '=', 'explicit', '_', 'arg', '[', '1', ':', ']', 'or', 'None', '\\', 'n', '</s>']
Filtered   (018): ['new', '_', 'expl', 'icit', '_', 'arg', '=', 'explicit', '_', 'arg', '[', '1', ':', ']', 'or', 'None', '\\', 'n']
Detokenized (010): ['new_explicit_arg', '=', 'explicit_arg', '[', '1', ':', ']', 'or', 'None', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "action_tuples . append ( ( action , args , option_string ) ) \n"
Original    (013): ['action_tuples', '.', 'append', '(', '(', 'action', ',', 'args', ',', 'option_string', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'action', '_', 'tu', 'ples', '.', 'append', '(', '(', 'action', ',', 'args', ',', 'option', '_', 'string', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['action', '_', 'tu', 'ples', '.', 'append', '(', '(', 'action', ',', 'args', ',', 'option', '_', 'string', ')', ')', '\\', 'n']
Detokenized (013): ['action_tuples', '.', 'append', '(', '(', 'action', ',', 'args', ',', 'option_string', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "selected_patterns = arg_strings_pattern [ start : ] \n"
Original    (008): ['selected_patterns', '=', 'arg_strings_pattern', '[', 'start', ':', ']', '\\n']
Tokenized   (018): ['<s>', 'selected', '_', 'pattern', 's', '=', 'arg', '_', 'strings', '_', 'pattern', '[', 'start', ':', ']', '\\', 'n', '</s>']
Filtered   (016): ['selected', '_', 'pattern', 's', '=', 'arg', '_', 'strings', '_', 'pattern', '[', 'start', ':', ']', '\\', 'n']
Detokenized (008): ['selected_patterns', '=', 'arg_strings_pattern', '[', 'start', ':', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "extras . extend ( arg_strings [ stop_index : ] ) \n"
Original    (011): ['extras', '.', 'extend', '(', 'arg_strings', '[', 'stop_index', ':', ']', ')', '\\n']
Tokenized   (019): ['<s>', 'ext', 'ras', '.', 'extend', '(', 'arg', '_', 'strings', '[', 'stop', '_', 'index', ':', ']', ')', '\\', 'n', '</s>']
Filtered   (017): ['ext', 'ras', '.', 'extend', '(', 'arg', '_', 'strings', '[', 'stop', '_', 'index', ':', ']', ')', '\\', 'n']
Detokenized (011): ['extras', '.', 'extend', '(', 'arg_strings', '[', 'stop_index', ':', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "OPTIONAL : _ ( ) , \n"
Original    (007): ['OPTIONAL', ':', '_', '(', ')', ',', '\\n']
Tokenized   (012): ['<s>', 'OP', 'TION', 'AL', ':', '_', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (010): ['OP', 'TION', 'AL', ':', '_', '(', ')', ',', '\\', 'n']
Detokenized (007): ['OPTIONAL', ':', '_', '(', ')', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "pattern = . join ( [ self . _get_nargs_pattern ( action ) \n"
Original    (013): ['pattern', '=', '.', 'join', '(', '[', 'self', '.', '_get_nargs_pattern', '(', 'action', ')', '\\n']
Tokenized   (022): ['<s>', 'pattern', '=', '.', 'join', '(', '[', 'self', '.', '_', 'get', '_', 'n', 'args', '_', 'pattern', '(', 'action', ')', '\\', 'n', '</s>']
Filtered   (020): ['pattern', '=', '.', 'join', '(', '[', 'self', '.', '_', 'get', '_', 'n', 'args', '_', 'pattern', '(', 'action', ')', '\\', 'n']
Detokenized (013): ['pattern', '=', '.', 'join', '(', '[', 'self', '.', '_get_nargs_pattern', '(', 'action', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "short_option_prefix = option_string [ : 2 ] \n"
Original    (008): ['short_option_prefix', '=', 'option_string', '[', ':', '2', ']', '\\n']
Tokenized   (017): ['<s>', 'short', '_', 'option', '_', 'prefix', '=', 'option', '_', 'string', '[', ':', '2', ']', '\\', 'n', '</s>']
Filtered   (015): ['short', '_', 'option', '_', 'prefix', '=', 'option', '_', 'string', '[', ':', '2', ']', '\\', 'n']
Detokenized (008): ['short_option_prefix', '=', 'option_string', '[', ':', '2', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "tup = action , option_string , short_explicit_arg \n"
Original    (008): ['tup', '=', 'action', ',', 'option_string', ',', 'short_explicit_arg', '\\n']
Tokenized   (019): ['<s>', 't', 'up', '=', 'action', ',', 'option', '_', 'string', ',', 'short', '_', 'expl', 'icit', '_', 'arg', '\\', 'n', '</s>']
Filtered   (017): ['t', 'up', '=', 'action', ',', 'option', '_', 'string', ',', 'short', '_', 'expl', 'icit', '_', 'arg', '\\', 'n']
Detokenized (008): ['tup', '=', 'action', ',', 'option_string', ',', 'short_explicit_arg', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "not action . option_strings ) : \n"
Original    (007): ['not', 'action', '.', 'option_strings', ')', ':', '\\n']
Tokenized   (012): ['<s>', 'not', 'action', '.', 'option', '_', 'strings', ')', ':', '\\', 'n', '</s>']
Filtered   (010): ['not', 'action', '.', 'option', '_', 'strings', ')', ':', '\\', 'n']
Detokenized (007): ['not', 'action', '.', 'option_strings', ')', ':', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "vulnerability = obj [ ] [ ] [ ] [ ] [ ] \n"
Original    (014): ['vulnerability', '=', 'obj', '[', ']', '[', ']', '[', ']', '[', ']', '[', ']', '\\n']
Tokenized   (018): ['<s>', 'v', 'ulnerability', '=', 'obj', '[', ']', '[', ']', '[', ']', '[', ']', '[', ']', '\\', 'n', '</s>']
Filtered   (016): ['v', 'ulnerability', '=', 'obj', '[', ']', '[', ']', '[', ']', '[', ']', '[', ']', '\\', 'n']
Detokenized (014): ['vulnerability', '=', 'obj', '[', ']', '[', ']', '[', ']', '[', ']', '[', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "apikey = common . apikey ( sessionKey , args [ 0 ] , debug ) \n"
Original    (016): ['apikey', '=', 'common', '.', 'apikey', '(', 'sessionKey', ',', 'args', '[', '0', ']', ',', 'debug', ')', '\\n']
Tokenized   (024): ['<s>', 'ap', 'ike', 'y', '=', 'common', '.', 'ap', 'ike', 'y', '(', 'session', 'Key', ',', 'args', '[', '0', ']', ',', 'debug', ')', '\\', 'n', '</s>']
Filtered   (022): ['ap', 'ike', 'y', '=', 'common', '.', 'ap', 'ike', 'y', '(', 'session', 'Key', ',', 'args', '[', '0', ']', ',', 'debug', ')', '\\', 'n']
Detokenized (016): ['apikey', '=', 'common', '.', 'apikey', '(', 'sessionKey', ',', 'args', '[', '0', ']', ',', 'debug', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "device = pandevice . base . PanDevice ( args [ 0 ] , api_key = apikey ) \n"
Original    (018): ['device', '=', 'pandevice', '.', 'base', '.', 'PanDevice', '(', 'args', '[', '0', ']', ',', 'api_key', '=', 'apikey', ')', '\\n']
Tokenized   (028): ['<s>', 'device', '=', 'pand', 'ev', 'ice', '.', 'base', '.', 'Pan', 'Device', '(', 'args', '[', '0', ']', ',', 'api', '_', 'key', '=', 'ap', 'ike', 'y', ')', '\\', 'n', '</s>']
Filtered   (026): ['device', '=', 'pand', 'ev', 'ice', '.', 'base', '.', 'Pan', 'Device', '(', 'args', '[', '0', ']', ',', 'api', '_', 'key', '=', 'ap', 'ike', 'y', ')', '\\', 'n']
Detokenized (018): ['device', '=', 'pandevice', '.', 'base', '.', 'PanDevice', '(', 'args', '[', '0', ']', ',', 'api_key', '=', 'apikey', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "rebalance_backoff_ms = 2 * 1000 , \n"
Original    (007): ['rebalance_backoff_ms', '=', '2', '*', '1000', ',', '\\n']
Tokenized   (016): ['<s>', 're', 'balance', '_', 'back', 'off', '_', 'ms', '=', '2', '*', '1000', ',', '\\', 'n', '</s>']
Filtered   (014): ['re', 'balance', '_', 'back', 'off', '_', 'ms', '=', '2', '*', '1000', ',', '\\', 'n']
Detokenized (007): ['rebalance_backoff_ms', '=', '2', '*', '1000', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "uuid = uuid4 ( ) \n"
Original    (006): ['uuid', '=', 'uuid4', '(', ')', '\\n']
Tokenized   (012): ['<s>', 'uu', 'id', '=', 'u', 'uid', '4', '(', ')', '\\', 'n', '</s>']
Filtered   (010): ['uu', 'id', '=', 'u', 'uid', '4', '(', ')', '\\', 'n']
Detokenized (006): ['uuid', '=', 'uuid4', '(', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : """ . join ( traceback . format_tb ( tb ) ) ) \n"
Original    (013): ['""', '.', 'join', '(', 'traceback', '.', 'format_tb', '(', 'tb', ')', ')', ')', '\\n']
Tokenized   (021): ['<s>', '""', '.', 'join', '(', 'trace', 'back', '.', 'format', '_', 't', 'b', '(', 't', 'b', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['""', '.', 'join', '(', 'trace', 'back', '.', 'format', '_', 't', 'b', '(', 't', 'b', ')', ')', ')', '\\', 'n']
Detokenized (013): ['""', '.', 'join', '(', 'traceback', '.', 'format_tb', '(', 'tb', ')', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "kazoo_kwargs = { : timeout / 1000 } \n"
Original    (009): ['kazoo_kwargs', '=', '{', ':', 'timeout', '/', '1000', '}', '\\n']
Tokenized   (017): ['<s>', 'k', 'az', 'oo', '_', 'kw', 'args', '=', '{', ':', 'timeout', '/', '1000', '}', '\\', 'n', '</s>']
Filtered   (015): ['k', 'az', 'oo', '_', 'kw', 'args', '=', '{', ':', 'timeout', '/', '1000', '}', '\\', 'n']
Detokenized (009): ['kazoo_kwargs', '=', '{', ':', 'timeout', '/', '1000', '}', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "p_to_str = lambda p : . join ( [ str ( p . topic . name ) , str ( p . leader . id ) , str ( p . id ) ] ) \n"
Original    (036): ['p_to_str', '=', 'lambda', 'p', ':', '.', 'join', '(', '[', 'str', '(', 'p', '.', 'topic', '.', 'name', ')', ',', 'str', '(', 'p', '.', 'leader', '.', 'id', ')', ',', 'str', '(', 'p', '.', 'id', ')', ']', ')', '\\n']
Tokenized   (043): ['<s>', 'p', '_', 'to', '_', 'str', '=', 'lambda', 'p', ':', '.', 'join', '(', '[', 'str', '(', 'p', '.', 'topic', '.', 'name', ')', ',', 'str', '(', 'p', '.', 'leader', '.', 'id', ')', ',', 'str', '(', 'p', '.', 'id', ')', ']', ')', '\\', 'n', '</s>']
Filtered   (041): ['p', '_', 'to', '_', 'str', '=', 'lambda', 'p', ':', '.', 'join', '(', '[', 'str', '(', 'p', '.', 'topic', '.', 'name', ')', ',', 'str', '(', 'p', '.', 'leader', '.', 'id', ')', ',', 'str', '(', 'p', '.', 'id', ')', ']', ')', '\\', 'n']
Detokenized (036): ['p_to_str', '=', 'lambda', 'p', ':', '.', 'join', '(', '[', 'str', '(', 'p', '.', 'topic', '.', 'name', ')', ',', 'str', '(', 'p', '.', 'leader', '.', 'id', ')', ',', 'str', '(', 'p', '.', 'id', ')', ']', ')', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 36, 768)
# Extracted words:  36
Sentence         : "idx = participants . index ( consumer_id or self . _consumer_id ) \n"
Original    (013): ['idx', '=', 'participants', '.', 'index', '(', 'consumer_id', 'or', 'self', '.', '_consumer_id', ')', '\\n']
Tokenized   (022): ['<s>', 'id', 'x', '=', 'participants', '.', 'index', '(', 'consumer', '_', 'id', 'or', 'self', '.', '_', 'consumer', '_', 'id', ')', '\\', 'n', '</s>']
Filtered   (020): ['id', 'x', '=', 'participants', '.', 'index', '(', 'consumer', '_', 'id', 'or', 'self', '.', '_', 'consumer', '_', 'id', ')', '\\', 'n']
Detokenized (013): ['idx', '=', 'participants', '.', 'index', '(', 'consumer_id', 'or', 'self', '.', '_consumer_id', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "parts_per_consumer = len ( all_parts ) // len ( participants ) \n"
Original    (012): ['parts_per_consumer', '=', 'len', '(', 'all_parts', ')', '//', 'len', '(', 'participants', ')', '\\n']
Tokenized   (021): ['<s>', 'parts', '_', 'per', '_', 'consumer', '=', 'len', '(', 'all', '_', 'parts', ')', '//', 'len', '(', 'participants', ')', '\\', 'n', '</s>']
Filtered   (019): ['parts', '_', 'per', '_', 'consumer', '=', 'len', '(', 'all', '_', 'parts', ')', '//', 'len', '(', 'participants', ')', '\\', 'n']
Detokenized (012): ['parts_per_consumer', '=', 'len', '(', 'all_parts', ')', '//', 'len', '(', 'participants', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "remainder_ppc = len ( all_parts ) % len ( participants ) \n"
Original    (012): ['remainder_ppc', '=', 'len', '(', 'all_parts', ')', '%', 'len', '(', 'participants', ')', '\\n']
Tokenized   (022): ['<s>', 'rem', 'ain', 'der', '_', 'pp', 'c', '=', 'len', '(', 'all', '_', 'parts', ')', '%', 'len', '(', 'participants', ')', '\\', 'n', '</s>']
Filtered   (020): ['rem', 'ain', 'der', '_', 'pp', 'c', '=', 'len', '(', 'all', '_', 'parts', ')', '%', 'len', '(', 'participants', ')', '\\', 'n']
Detokenized (012): ['remainder_ppc', '=', 'len', '(', 'all_parts', ')', '%', 'len', '(', 'participants', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "num_parts = parts_per_consumer + ( 0 if ( idx + 1 > remainder_ppc ) else 1 ) \n"
Original    (018): ['num_parts', '=', 'parts_per_consumer', '+', '(', '0', 'if', '(', 'idx', '+', '1', '>', 'remainder_ppc', ')', 'else', '1', ')', '\\n']
Tokenized   (031): ['<s>', 'num', '_', 'parts', '=', 'parts', '_', 'per', '_', 'consumer', '+', '(', '0', 'if', '(', 'id', 'x', '+', '1', '>', 'remainder', '_', 'pp', 'c', ')', 'else', '1', ')', '\\', 'n', '</s>']
Filtered   (029): ['num', '_', 'parts', '=', 'parts', '_', 'per', '_', 'consumer', '+', '(', '0', 'if', '(', 'id', 'x', '+', '1', '>', 'remainder', '_', 'pp', 'c', ')', 'else', '1', ')', '\\', 'n']
Detokenized (018): ['num_parts', '=', 'parts_per_consumer', '+', '(', '0', 'if', '(', 'idx', '+', '1', '>', 'remainder_ppc', ')', 'else', '1', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "log . debug ( , [ p_to_str ( p ) for p in new_partitions ] ) \n"
Original    (017): ['log', '.', 'debug', '(', ',', '[', 'p_to_str', '(', 'p', ')', 'for', 'p', 'in', 'new_partitions', ']', ')', '\\n']
Tokenized   (027): ['<s>', 'log', '.', 'debug', '(', ',', '[', 'p', '_', 'to', '_', 'str', '(', 'p', ')', 'for', 'p', 'in', 'new', '_', 'part', 'itions', ']', ')', '\\', 'n', '</s>']
Filtered   (025): ['log', '.', 'debug', '(', ',', '[', 'p', '_', 'to', '_', 'str', '(', 'p', ')', 'for', 'p', 'in', 'new', '_', 'part', 'itions', ']', ')', '\\', 'n']
Detokenized (017): ['log', '.', 'debug', '(', ',', '[', 'p_to_str', '(', 'p', ')', 'for', 'p', 'in', 'new_partitions', ']', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "id_ = get_string ( self . _consumer_id ) \n"
Original    (009): ['id_', '=', 'get_string', '(', 'self', '.', '_consumer_id', ')', '\\n']
Tokenized   (018): ['<s>', 'id', '_', '=', 'get', '_', 'string', '(', 'self', '.', '_', 'consumer', '_', 'id', ')', '\\', 'n', '</s>']
Filtered   (016): ['id', '_', '=', 'get', '_', 'string', '(', 'self', '.', '_', 'consumer', '_', 'id', ')', '\\', 'n']
Detokenized (009): ['id_', '=', 'get_string', '(', 'self', '.', '_consumer_id', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "path = self . _topic_path , slug = partition_slug ) ) \n"
Original    (012): ['path', '=', 'self', '.', '_topic_path', ',', 'slug', '=', 'partition_slug', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'path', '=', 'self', '.', '_', 'topic', '_', 'path', ',', 'slug', '=', 'partition', '_', 'sl', 'ug', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['path', '=', 'self', '.', '_', 'topic', '_', 'path', ',', 'slug', '=', 'partition', '_', 'sl', 'ug', ')', ')', '\\', 'n']
Detokenized (012): ['path', '=', 'self', '.', '_topic_path', ',', 'slug', '=', 'partition_slug', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "HZCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 ) \n"
Original    (016): ['HZCharLenTable', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Tokenized   (023): ['<s>', 'H', 'Z', 'Char', 'Len', 'Table', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n', '</s>']
Filtered   (021): ['H', 'Z', 'Char', 'Len', 'Table', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n']
Detokenized (016): ['HZCharLenTable', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "ISO2022CNCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ) \n"
Original    (022): ['ISO2022CNCharLenTable', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Tokenized   (031): ['<s>', 'ISO', '20', '22', 'CN', 'Char', 'Len', 'Table', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n', '</s>']
Filtered   (029): ['ISO', '20', '22', 'CN', 'Char', 'Len', 'Table', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n']
Detokenized (022): ['ISO2022CNCharLenTable', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "ISO2022JPCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ) \n"
Original    (024): ['ISO2022JPCharLenTable', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Tokenized   (033): ['<s>', 'ISO', '20', '22', 'JP', 'Char', 'Len', 'Table', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n', '</s>']
Filtered   (031): ['ISO', '20', '22', 'JP', 'Char', 'Len', 'Table', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n']
Detokenized (024): ['ISO2022JPCharLenTable', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "logging . basicConfig ( level = logging . INFO ) \n"
Original    (011): ['logging', '.', 'basicConfig', '(', 'level', '=', 'logging', '.', 'INFO', ')', '\\n']
Tokenized   (016): ['<s>', 'log', 'ging', '.', 'basic', 'Config', '(', 'level', '=', 'logging', '.', 'INFO', ')', '\\', 'n', '</s>']
Filtered   (014): ['log', 'ging', '.', 'basic', 'Config', '(', 'level', '=', 'logging', '.', 'INFO', ')', '\\', 'n']
Detokenized (011): ['logging', '.', 'basicConfig', '(', 'level', '=', 'logging', '.', 'INFO', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "tasa . __version__ , sys . version ) ) \n"
Original    (010): ['tasa', '.', '__version__', ',', 'sys', '.', 'version', ')', ')', '\\n']
Tokenized   (016): ['<s>', 't', 'asa', '.', '__', 'version', '__', ',', 'sys', '.', 'version', ')', ')', '\\', 'n', '</s>']
Filtered   (014): ['t', 'asa', '.', '__', 'version', '__', ',', 'sys', '.', 'version', ')', ')', '\\', 'n']
Detokenized (010): ['tasa', '.', '__version__', ',', 'sys', '.', 'version', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "type = lambda w : w . partition ( ) [ : : 2 ] , \n"
Original    (017): ['type', '=', 'lambda', 'w', ':', 'w', '.', 'partition', '(', ')', '[', ':', ':', '2', ']', ',', '\\n']
Tokenized   (020): ['<s>', 'type', '=', 'lambda', 'w', ':', 'w', '.', 'partition', '(', ')', '[', ':', ':', '2', ']', ',', '\\', 'n', '</s>']
Filtered   (018): ['type', '=', 'lambda', 'w', ':', 'w', '.', 'partition', '(', ')', '[', ':', ':', '2', ']', ',', '\\', 'n']
Detokenized (017): ['type', '=', 'lambda', 'w', ':', 'w', '.', 'partition', '(', ')', '[', ':', ':', '2', ']', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "worker_class_name = args . worker [ 1 ] or \n"
Original    (010): ['worker_class_name', '=', 'args', '.', 'worker', '[', '1', ']', 'or', '\\n']
Tokenized   (017): ['<s>', 'worker', '_', 'class', '_', 'name', '=', 'args', '.', 'worker', '[', '1', ']', 'or', '\\', 'n', '</s>']
Filtered   (015): ['worker', '_', 'class', '_', 'name', '=', 'args', '.', 'worker', '[', '1', ']', 'or', '\\', 'n']
Detokenized (010): ['worker_class_name', '=', 'args', '.', 'worker', '[', '1', ']', 'or', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "str ( job ) [ : 50 ] ) \n"
Original    (010): ['str', '(', 'job', ')', '[', ':', '50', ']', ')', '\\n']
Tokenized   (013): ['<s>', 'str', '(', 'job', ')', '[', ':', '50', ']', ')', '\\', 'n', '</s>']
Filtered   (011): ['str', '(', 'job', ')', '[', ':', '50', ']', ')', '\\', 'n']
Detokenized (010): ['str', '(', 'job', ')', '[', ':', '50', ']', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "processes = [ Process ( target = run , args = ( ) ) for x in range ( count ) ] \n"
Original    (023): ['processes', '=', '[', 'Process', '(', 'target', '=', 'run', ',', 'args', '=', '(', ')', ')', 'for', 'x', 'in', 'range', '(', 'count', ')', ']', '\\n']
Tokenized   (027): ['<s>', 'process', 'es', '=', '[', 'Process', '(', 'target', '=', 'run', ',', 'args', '=', '(', ')', ')', 'for', 'x', 'in', 'range', '(', 'count', ')', ']', '\\', 'n', '</s>']
Filtered   (025): ['process', 'es', '=', '[', 'Process', '(', 'target', '=', 'run', ',', 'args', '=', '(', ')', ')', 'for', 'x', 'in', 'range', '(', 'count', ')', ']', '\\', 'n']
Detokenized (023): ['processes', '=', '[', 'Process', '(', 'target', '=', 'run', ',', 'args', '=', '(', ')', ')', 'for', 'x', 'in', 'range', '(', 'count', ')', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "color = models . CharField ( max_length = 6 , validators = [ color_regex ] , help_text = \n"
Original    (019): ['color', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '6', ',', 'validators', '=', '[', 'color_regex', ']', ',', 'help_text', '=', '\\n']
Tokenized   (031): ['<s>', 'color', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '6', ',', 'valid', 'ators', '=', '[', 'color', '_', 're', 'gex', ']', ',', 'help', '_', 'text', '=', '\\', 'n', '</s>']
Filtered   (029): ['color', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '6', ',', 'valid', 'ators', '=', '[', 'color', '_', 're', 'gex', ']', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (019): ['color', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '6', ',', 'validators', '=', '[', 'color_regex', ']', ',', 'help_text', '=', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "first_name = models . CharField ( max_length = 64 ) \n"
Original    (011): ['first_name', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '64', ')', '\\n']
Tokenized   (019): ['<s>', 'first', '_', 'name', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '64', ')', '\\', 'n', '</s>']
Filtered   (017): ['first', '_', 'name', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '64', ')', '\\', 'n']
Detokenized (011): ['first_name', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '64', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "role = models . CharField ( max_length = 17 , choices = ROLE_CHOICES ) \n"
Original    (015): ['role', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '17', ',', 'choices', '=', 'ROLE_CHOICES', ')', '\\n']
Tokenized   (025): ['<s>', 'role', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '17', ',', 'choices', '=', 'RO', 'LE', '_', 'CHO', 'ICES', ')', '\\', 'n', '</s>']
Filtered   (023): ['role', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '17', ',', 'choices', '=', 'RO', 'LE', '_', 'CHO', 'ICES', ')', '\\', 'n']
Detokenized (015): ['role', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '17', ',', 'choices', '=', 'ROLE_CHOICES', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "phone_work = models . CharField ( max_length = 15 , validators = [ phone_regex ] , blank = True ) \n"
Original    (021): ['phone_work', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '15', ',', 'validators', '=', '[', 'phone_regex', ']', ',', 'blank', '=', 'True', ')', '\\n']
Tokenized   (033): ['<s>', 'phone', '_', 'work', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '15', ',', 'valid', 'ators', '=', '[', 'phone', '_', 're', 'gex', ']', ',', 'blank', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (031): ['phone', '_', 'work', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '15', ',', 'valid', 'ators', '=', '[', 'phone', '_', 're', 'gex', ']', ',', 'blank', '=', 'True', ')', '\\', 'n']
Detokenized (021): ['phone_work', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '15', ',', 'validators', '=', '[', 'phone_regex', ']', ',', 'blank', '=', 'True', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "job_title = models . CharField ( max_length = 128 , blank = True ) \n"
Original    (015): ['job_title', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '128', ',', 'blank', '=', 'True', ')', '\\n']
Tokenized   (023): ['<s>', 'job', '_', 'title', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '128', ',', 'blank', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (021): ['job', '_', 'title', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '128', ',', 'blank', '=', 'True', ')', '\\', 'n']
Detokenized (015): ['job_title', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '128', ',', 'blank', '=', 'True', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "category = models . CharField ( max_length = 21 , choices = CATEGORY_CHOICES , help_text = description = models . CharField ( max_length = 256 , blank = True , help_text = reference = models . URLField ( blank = True , help_text = ) \n"
Original    (046): ['category', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '21', ',', 'choices', '=', 'CATEGORY_CHOICES', ',', 'help_text', '=', 'description', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '256', ',', 'blank', '=', 'True', ',', 'help_text', '=', 'reference', '=', 'models', '.', 'URLField', '(', 'blank', '=', 'True', ',', 'help_text', '=', ')', '\\n']
Tokenized   (068): ['<s>', 'category', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '21', ',', 'choices', '=', 'C', 'ATE', 'G', 'ORY', '_', 'CHO', 'ICES', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '256', ',', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', 'reference', '=', 'models', '.', 'URL', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', ')', '\\', 'n', '</s>']
Filtered   (066): ['category', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '21', ',', 'choices', '=', 'C', 'ATE', 'G', 'ORY', '_', 'CHO', 'ICES', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '256', ',', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', 'reference', '=', 'models', '.', 'URL', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', ')', '\\', 'n']
Detokenized (046): ['category', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '21', ',', 'choices', '=', 'CATEGORY_CHOICES', ',', 'help_text', '=', 'description', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '256', ',', 'blank', '=', 'True', ',', 'help_text', '=', 'reference', '=', 'models', '.', 'URLField', '(', 'blank', '=', 'True', ',', 'help_text', '=', ')', '\\n']
Counter: 66
===================================================================
Hidden states:  (13, 46, 768)
# Extracted words:  46
Sentence         : "acronym = models . CharField ( max_length = 20 , unique = True , help_text = category = models . CharField ( max_length = 9 , choices = CATEGORY_CHOICES , help_text = jurisdiction = models . CharField ( max_length = 64 , help_text = description = models . TextField ( blank = True , help_text = reference = models . URLField ( blank = True , help_text = ) \n"
Original    (070): ['acronym', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '20', ',', 'unique', '=', 'True', ',', 'help_text', '=', 'category', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '9', ',', 'choices', '=', 'CATEGORY_CHOICES', ',', 'help_text', '=', 'jurisdiction', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '64', ',', 'help_text', '=', 'description', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', 'reference', '=', 'models', '.', 'URLField', '(', 'blank', '=', 'True', ',', 'help_text', '=', ')', '\\n']
Tokenized   (102): ['<s>', 'ac', 'ron', 'ym', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '20', ',', 'unique', '=', 'True', ',', 'help', '_', 'text', '=', 'category', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '9', ',', 'choices', '=', 'C', 'ATE', 'G', 'ORY', '_', 'CHO', 'ICES', ',', 'help', '_', 'text', '=', 'jurisdiction', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '64', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', 'reference', '=', 'models', '.', 'URL', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', ')', '\\', 'n', '</s>']
Filtered   (100): ['ac', 'ron', 'ym', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '20', ',', 'unique', '=', 'True', ',', 'help', '_', 'text', '=', 'category', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '9', ',', 'choices', '=', 'C', 'ATE', 'G', 'ORY', '_', 'CHO', 'ICES', ',', 'help', '_', 'text', '=', 'jurisdiction', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '64', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', 'reference', '=', 'models', '.', 'URL', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', ')', '\\', 'n']
Detokenized (070): ['acronym', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '20', ',', 'unique', '=', 'True', ',', 'help_text', '=', 'category', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '9', ',', 'choices', '=', 'CATEGORY_CHOICES', ',', 'help_text', '=', 'jurisdiction', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '64', ',', 'help_text', '=', 'description', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', 'reference', '=', 'models', '.', 'URLField', '(', 'blank', '=', 'True', ',', 'help_text', '=', ')', '\\n']
Counter: 100
===================================================================
Hidden states:  (13, 70, 768)
# Extracted words:  70
Sentence         : "description = models . CharField ( max_length = 256 , blank = True , help_text = \n"
Original    (017): ['description', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '256', ',', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (025): ['<s>', 'description', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '256', ',', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n', '</s>']
Filtered   (023): ['description', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '256', ',', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (017): ['description', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '256', ',', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "business_criticality = models . CharField ( max_length = 9 , choices = BUSINESS_CRITICALITY_CHOICES , blank platform = models . CharField ( max_length = 11 , choices = PLATFORM_CHOICES , blank = True , null = True ) \n"
Original    (038): ['business_criticality', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '9', ',', 'choices', '=', 'BUSINESS_CRITICALITY_CHOICES', ',', 'blank', 'platform', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '11', ',', 'choices', '=', 'PLATFORM_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\n']
Tokenized   (064): ['<s>', 'business', '_', 'critical', 'ity', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '9', ',', 'choices', '=', 'BUS', 'INESS', '_', 'CR', 'IT', 'ICAL', 'ITY', '_', 'CHO', 'ICES', ',', 'blank', 'platform', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '11', ',', 'choices', '=', 'PL', 'AT', 'FORM', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (062): ['business', '_', 'critical', 'ity', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '9', ',', 'choices', '=', 'BUS', 'INESS', '_', 'CR', 'IT', 'ICAL', 'ITY', '_', 'CHO', 'ICES', ',', 'blank', 'platform', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '11', ',', 'choices', '=', 'PL', 'AT', 'FORM', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\', 'n']
Detokenized (038): ['business_criticality', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '9', ',', 'choices', '=', 'BUSINESS_CRITICALITY_CHOICES', ',', 'blank', 'platform', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '11', ',', 'choices', '=', 'PLATFORM_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\n']
Counter: 62
===================================================================
Hidden states:  (13, 38, 768)
# Extracted words:  38
Sentence         : "lifecycle = models . CharField ( max_length = 8 , choices = LIFECYCLE_CHOICES , blank = True , null = True ) \n"
Original    (023): ['lifecycle', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '8', ',', 'choices', '=', 'LIFECYCLE_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\n']
Tokenized   (037): ['<s>', 'lif', 'ecycle', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '8', ',', 'choices', '=', 'L', 'IF', 'EC', 'Y', 'CLE', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (035): ['lif', 'ecycle', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '8', ',', 'choices', '=', 'L', 'IF', 'EC', 'Y', 'CLE', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\', 'n']
Detokenized (023): ['lifecycle', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '8', ',', 'choices', '=', 'LIFECYCLE_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "user_records = models . PositiveIntegerField ( blank = True , null = True , help_text = revenue = models . DecimalField ( max_digits = 15 , decimal_places = 2 , blank = True , null = True , help_text = external_audience = models . BooleanField ( default = False , help_text = internet_accessible = models . BooleanField ( default = False , help_text = requestable = models . NullBooleanField ( default = True , help_text = _ ( \n"
Original    (079): ['user_records', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'revenue', '=', 'models', '.', 'DecimalField', '(', 'max_digits', '=', '15', ',', 'decimal_places', '=', '2', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'external_audience', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ',', 'help_text', '=', 'internet_accessible', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ',', 'help_text', '=', 'requestable', '=', 'models', '.', 'NullBooleanField', '(', 'default', '=', 'True', ',', 'help_text', '=', '_', '(', '\\n']
Tokenized   (115): ['<s>', 'user', '_', 'rec', 'ords', '=', 'models', '.', 'Positive', 'Integer', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'revenue', '=', 'models', '.', 'Dec', 'imal', 'Field', '(', 'max', '_', 'dig', 'its', '=', '15', ',', 'decimal', '_', 'places', '=', '2', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'external', '_', 'aud', 'ience', '=', 'models', '.', 'Boolean', 'Field', '(', 'default', '=', 'False', ',', 'help', '_', 'text', '=', 'internet', '_', 'accessible', '=', 'models', '.', 'Boolean', 'Field', '(', 'default', '=', 'False', ',', 'help', '_', 'text', '=', 'request', 'able', '=', 'models', '.', 'Null', 'Boo', 'lean', 'Field', '(', 'default', '=', 'True', ',', 'help', '_', 'text', '=', '_', '(', '\\', 'n', '</s>']
Filtered   (113): ['user', '_', 'rec', 'ords', '=', 'models', '.', 'Positive', 'Integer', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'revenue', '=', 'models', '.', 'Dec', 'imal', 'Field', '(', 'max', '_', 'dig', 'its', '=', '15', ',', 'decimal', '_', 'places', '=', '2', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'external', '_', 'aud', 'ience', '=', 'models', '.', 'Boolean', 'Field', '(', 'default', '=', 'False', ',', 'help', '_', 'text', '=', 'internet', '_', 'accessible', '=', 'models', '.', 'Boolean', 'Field', '(', 'default', '=', 'False', ',', 'help', '_', 'text', '=', 'request', 'able', '=', 'models', '.', 'Null', 'Boo', 'lean', 'Field', '(', 'default', '=', 'True', ',', 'help', '_', 'text', '=', '_', '(', '\\', 'n']
Detokenized (079): ['user_records', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'revenue', '=', 'models', '.', 'DecimalField', '(', 'max_digits', '=', '15', ',', 'decimal_places', '=', '2', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'external_audience', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ',', 'help_text', '=', 'internet_accessible', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ',', 'help_text', '=', 'requestable', '=', 'models', '.', 'NullBooleanField', '(', 'default', '=', 'True', ',', 'help_text', '=', '_', '(', '\\n']
Counter: 113
===================================================================
Hidden states:  (13, 79, 768)
# Extracted words:  79
Sentence         : "override_dcl = models . IntegerField ( choices = DATA_CLASSIFICATION_CHOICES , blank = True , null = True , help_text override_reason = models . TextField ( blank = True , help_text = \n"
Original    (032): ['override_dcl', '=', 'models', '.', 'IntegerField', '(', 'choices', '=', 'DATA_CLASSIFICATION_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', 'override_reason', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (054): ['<s>', 'over', 'ride', '_', 'd', 'cl', '=', 'models', '.', 'Integer', 'Field', '(', 'choices', '=', 'DATA', '_', 'CLASS', 'IFIC', 'ATION', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', 'override', '_', 'reason', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n', '</s>']
Filtered   (052): ['over', 'ride', '_', 'd', 'cl', '=', 'models', '.', 'Integer', 'Field', '(', 'choices', '=', 'DATA', '_', 'CLASS', 'IFIC', 'ATION', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', 'override', '_', 'reason', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (032): ['override_dcl', '=', 'models', '.', 'IntegerField', '(', 'choices', '=', 'DATA_CLASSIFICATION_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', 'override_reason', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Counter: 52
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "threadfix = models . ForeignKey ( ThreadFix , blank = True , null = True , help_text = threadfix_team_id = models . PositiveIntegerField ( blank = True , null = True , help_text = threadfix_application_id = models . PositiveIntegerField ( blank = True , null = True , help_text = \n"
Original    (051): ['threadfix', '=', 'models', '.', 'ForeignKey', '(', 'ThreadFix', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'threadfix_team_id', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'threadfix_application_id', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (077): ['<s>', 'thread', 'fix', '=', 'models', '.', 'Foreign', 'Key', '(', 'Thread', 'Fix', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'thread', 'fix', '_', 'team', '_', 'id', '=', 'models', '.', 'Positive', 'Integer', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'thread', 'fix', '_', 'application', '_', 'id', '=', 'models', '.', 'Positive', 'Integer', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n', '</s>']
Filtered   (075): ['thread', 'fix', '=', 'models', '.', 'Foreign', 'Key', '(', 'Thread', 'Fix', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'thread', 'fix', '_', 'team', '_', 'id', '=', 'models', '.', 'Positive', 'Integer', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'thread', 'fix', '_', 'application', '_', 'id', '=', 'models', '.', 'Positive', 'Integer', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (051): ['threadfix', '=', 'models', '.', 'ForeignKey', '(', 'ThreadFix', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'threadfix_team_id', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'threadfix_application_id', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', '\\n']
Counter: 75
===================================================================
Hidden states:  (13, 51, 768)
# Extracted words:  51
Sentence         : "asvs_level = models . IntegerField ( choices = ASVS_CHOICES , blank = True , null = True , help_text = asvs_level_percent_achieved = models . PositiveIntegerField ( blank = True , null = True , help_text = asvs_doc_url = models . URLField ( blank = True , help_text = ) \n"
Original    (050): ['asvs_level', '=', 'models', '.', 'IntegerField', '(', 'choices', '=', 'ASVS_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'asvs_level_percent_achieved', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'asvs_doc_url', '=', 'models', '.', 'URLField', '(', 'blank', '=', 'True', ',', 'help_text', '=', ')', '\\n']
Tokenized   (083): ['<s>', 'as', 'vs', '_', 'level', '=', 'models', '.', 'Integer', 'Field', '(', 'choices', '=', 'AS', 'VS', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'as', 'vs', '_', 'level', '_', 'percent', '_', 'ach', 'ieved', '=', 'models', '.', 'Positive', 'Integer', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'as', 'vs', '_', 'doc', '_', 'url', '=', 'models', '.', 'URL', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', ')', '\\', 'n', '</s>']
Filtered   (081): ['as', 'vs', '_', 'level', '=', 'models', '.', 'Integer', 'Field', '(', 'choices', '=', 'AS', 'VS', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'as', 'vs', '_', 'level', '_', 'percent', '_', 'ach', 'ieved', '=', 'models', '.', 'Positive', 'Integer', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'as', 'vs', '_', 'doc', '_', 'url', '=', 'models', '.', 'URL', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', ')', '\\', 'n']
Detokenized (050): ['asvs_level', '=', 'models', '.', 'IntegerField', '(', 'choices', '=', 'ASVS_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'asvs_level_percent_achieved', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'asvs_doc_url', '=', 'models', '.', 'URLField', '(', 'blank', '=', 'True', ',', 'help_text', '=', ')', '\\n']
Counter: 81
===================================================================
Hidden states:  (13, 50, 768)
# Extracted words:  50
Sentence         : "asvs_level_target = models . IntegerField ( choices = ASVS_CHOICES , blank = True , null = True , help_text = \n"
Original    (021): ['asvs_level_target', '=', 'models', '.', 'IntegerField', '(', 'choices', '=', 'ASVS_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (036): ['<s>', 'as', 'vs', '_', 'level', '_', 'target', '=', 'models', '.', 'Integer', 'Field', '(', 'choices', '=', 'AS', 'VS', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n', '</s>']
Filtered   (034): ['as', 'vs', '_', 'level', '_', 'target', '=', 'models', '.', 'Integer', 'Field', '(', 'choices', '=', 'AS', 'VS', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (021): ['asvs_level_target', '=', 'models', '.', 'IntegerField', '(', 'choices', '=', 'ASVS_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "organization = models . ForeignKey ( Organization , help_text = people = models . ManyToManyField ( Person , through = , blank = True ) \n"
Original    (026): ['organization', '=', 'models', '.', 'ForeignKey', '(', 'Organization', ',', 'help_text', '=', 'people', '=', 'models', '.', 'ManyToManyField', '(', 'Person', ',', 'through', '=', ',', 'blank', '=', 'True', ')', '\\n']
Tokenized   (036): ['<s>', 'organ', 'ization', '=', 'models', '.', 'Foreign', 'Key', '(', 'Organization', ',', 'help', '_', 'text', '=', 'people', '=', 'models', '.', 'Many', 'To', 'Many', 'Field', '(', 'Person', ',', 'through', '=', ',', 'blank', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (034): ['organ', 'ization', '=', 'models', '.', 'Foreign', 'Key', '(', 'Organization', ',', 'help', '_', 'text', '=', 'people', '=', 'models', '.', 'Many', 'To', 'Many', 'Field', '(', 'Person', ',', 'through', '=', ',', 'blank', '=', 'True', ')', '\\', 'n']
Detokenized (026): ['organization', '=', 'models', '.', 'ForeignKey', '(', 'Organization', ',', 'help_text', '=', 'people', '=', 'models', '.', 'ManyToManyField', '(', 'Person', ',', 'through', '=', ',', 'blank', '=', 'True', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "delta = self . created_date - timezone . now ( ) \n"
Original    (012): ['delta', '=', 'self', '.', 'created_date', '-', 'timezone', '.', 'now', '(', ')', '\\n']
Tokenized   (019): ['<s>', 'd', 'elta', '=', 'self', '.', 'created', '_', 'date', '-', 'time', 'zone', '.', 'now', '(', ')', '\\', 'n', '</s>']
Filtered   (017): ['d', 'elta', '=', 'self', '.', 'created', '_', 'date', '-', 'time', 'zone', '.', 'now', '(', ')', '\\', 'n']
Detokenized (012): ['delta', '=', 'self', '.', 'created_date', '-', 'timezone', '.', 'now', '(', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "person = models . ForeignKey ( Person , help_text = ) \n"
Original    (012): ['person', '=', 'models', '.', 'ForeignKey', '(', 'Person', ',', 'help_text', '=', ')', '\\n']
Tokenized   (018): ['<s>', 'person', '=', 'models', '.', 'Foreign', 'Key', '(', 'Person', ',', 'help', '_', 'text', '=', ')', '\\', 'n', '</s>']
Filtered   (016): ['person', '=', 'models', '.', 'Foreign', 'Key', '(', 'Person', ',', 'help', '_', 'text', '=', ')', '\\', 'n']
Detokenized (012): ['person', '=', 'models', '.', 'ForeignKey', '(', 'Person', ',', 'help_text', '=', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "environment_type = models . CharField ( max_length = 4 , choices = ENVIRONMENT_CHOICES , help_text = description = models . TextField ( blank = True , help_text = testing_approved = models . BooleanField ( default = False , help_text = \n"
Original    (041): ['environment_type', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '4', ',', 'choices', '=', 'ENVIRONMENT_CHOICES', ',', 'help_text', '=', 'description', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', 'testing_approved', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ',', 'help_text', '=', '\\n']
Tokenized   (066): ['<s>', 'environment', '_', 'type', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '4', ',', 'choices', '=', 'EN', 'V', 'IR', 'ON', 'MENT', '_', 'CHO', 'ICES', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', 'testing', '_', 'approved', '=', 'models', '.', 'Boolean', 'Field', '(', 'default', '=', 'False', ',', 'help', '_', 'text', '=', '\\', 'n', '</s>']
Filtered   (064): ['environment', '_', 'type', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '4', ',', 'choices', '=', 'EN', 'V', 'IR', 'ON', 'MENT', '_', 'CHO', 'ICES', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', 'testing', '_', 'approved', '=', 'models', '.', 'Boolean', 'Field', '(', 'default', '=', 'False', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (041): ['environment_type', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '4', ',', 'choices', '=', 'ENVIRONMENT_CHOICES', ',', 'help_text', '=', 'description', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', 'testing_approved', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ',', 'help_text', '=', '\\n']
Counter: 64
===================================================================
Hidden states:  (13, 41, 768)
# Extracted words:  41
Sentence         : "location = models . URLField ( help_text = notes = models . TextField ( blank = True , help_text = \n"
Original    (021): ['location', '=', 'models', '.', 'URLField', '(', 'help_text', '=', 'notes', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (030): ['<s>', 'location', '=', 'models', '.', 'URL', 'Field', '(', 'help', '_', 'text', '=', 'notes', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n', '</s>']
Filtered   (028): ['location', '=', 'models', '.', 'URL', 'Field', '(', 'help', '_', 'text', '=', 'notes', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (021): ['location', '=', 'models', '.', 'URLField', '(', 'help_text', '=', 'notes', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "role_description = models . CharField ( max_length = 128 , blank = True , help_text = notes = models . TextField ( blank = True , help_text = \n"
Original    (029): ['role_description', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '128', ',', 'blank', '=', 'True', ',', 'help_text', '=', 'notes', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (042): ['<s>', 'role', '_', 'description', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '128', ',', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', 'notes', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n', '</s>']
Filtered   (040): ['role', '_', 'description', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '128', ',', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', 'notes', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (029): ['role_description', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '128', ',', 'blank', '=', 'True', ',', 'help_text', '=', 'notes', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "start_date = models . DateField ( help_text = ) \n"
Original    (010): ['start_date', '=', 'models', '.', 'DateField', '(', 'help_text', '=', ')', '\\n']
Tokenized   (018): ['<s>', 'start', '_', 'date', '=', 'models', '.', 'Date', 'Field', '(', 'help', '_', 'text', '=', ')', '\\', 'n', '</s>']
Filtered   (016): ['start', '_', 'date', '=', 'models', '.', 'Date', 'Field', '(', 'help', '_', 'text', '=', ')', '\\', 'n']
Detokenized (010): ['start_date', '=', 'models', '.', 'DateField', '(', 'help_text', '=', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "open_date = models . DateTimeField ( blank = True , null = True , help_text = close_date = models . DateTimeField ( blank = True , null = True , help_text = duration = models . DurationField ( blank = True , null = True ) \n"
Original    (047): ['open_date', '=', 'models', '.', 'DateTimeField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'close_date', '=', 'models', '.', 'DateTimeField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'duration', '=', 'models', '.', 'DurationField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\n']
Tokenized   (063): ['<s>', 'open', '_', 'date', '=', 'models', '.', 'Date', 'Time', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'close', '_', 'date', '=', 'models', '.', 'Date', 'Time', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'duration', '=', 'models', '.', 'Duration', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (061): ['open', '_', 'date', '=', 'models', '.', 'Date', 'Time', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'close', '_', 'date', '=', 'models', '.', 'Date', 'Time', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'duration', '=', 'models', '.', 'Duration', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\', 'n']
Detokenized (047): ['open_date', '=', 'models', '.', 'DateTimeField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'close_date', '=', 'models', '.', 'DateTimeField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'duration', '=', 'models', '.', 'DurationField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\n']
Counter: 61
===================================================================
Hidden states:  (13, 47, 768)
# Extracted words:  47
Sentence         : "metrics = managers . ActivityTypeMetrics . from_queryset ( managers . ActivityTypeQuerySet ) ( ) \n"
Original    (015): ['metrics', '=', 'managers', '.', 'ActivityTypeMetrics', '.', 'from_queryset', '(', 'managers', '.', 'ActivityTypeQuerySet', ')', '(', ')', '\\n']
Tokenized   (029): ['<s>', 'met', 'rics', '=', 'managers', '.', 'Activity', 'Type', 'Met', 'rics', '.', 'from', '_', 'quer', 'ys', 'et', '(', 'managers', '.', 'Activity', 'Type', 'Query', 'Set', ')', '(', ')', '\\', 'n', '</s>']
Filtered   (027): ['met', 'rics', '=', 'managers', '.', 'Activity', 'Type', 'Met', 'rics', '.', 'from', '_', 'quer', 'ys', 'et', '(', 'managers', '.', 'Activity', 'Type', 'Query', 'Set', ')', '(', ')', '\\', 'n']
Detokenized (015): ['metrics', '=', 'managers', '.', 'ActivityTypeMetrics', '.', 'from_queryset', '(', 'managers', '.', 'ActivityTypeQuerySet', ')', '(', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "token = models . UUIDField ( default = uuid . uuid4 , editable = False ) \n"
Original    (017): ['token', '=', 'models', '.', 'UUIDField', '(', 'default', '=', 'uuid', '.', 'uuid4', ',', 'editable', '=', 'False', ')', '\\n']
Tokenized   (026): ['<s>', 'token', '=', 'models', '.', 'U', 'UID', 'Field', '(', 'default', '=', 'u', 'uid', '.', 'u', 'uid', '4', ',', 'edit', 'able', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (024): ['token', '=', 'models', '.', 'U', 'UID', 'Field', '(', 'default', '=', 'u', 'uid', '.', 'u', 'uid', '4', ',', 'edit', 'able', '=', 'False', ')', '\\', 'n']
Detokenized (017): ['token', '=', 'models', '.', 'UUIDField', '(', 'default', '=', 'uuid', '.', 'uuid4', ',', 'editable', '=', 'False', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "activities = models . ManyToManyField ( ActivityType , limit_choices_to = { : True } ) \n"
Original    (016): ['activities', '=', 'models', '.', 'ManyToManyField', '(', 'ActivityType', ',', 'limit_choices_to', '=', '{', ':', 'True', '}', ')', '\\n']
Tokenized   (029): ['<s>', 'activ', 'ities', '=', 'models', '.', 'Many', 'To', 'Many', 'Field', '(', 'Activity', 'Type', ',', 'limit', '_', 'cho', 'ices', '_', 'to', '=', '{', ':', 'True', '}', ')', '\\', 'n', '</s>']
Filtered   (027): ['activ', 'ities', '=', 'models', '.', 'Many', 'To', 'Many', 'Field', '(', 'Activity', 'Type', ',', 'limit', '_', 'cho', 'ices', '_', 'to', '=', '{', ':', 'True', '}', ')', '\\', 'n']
Detokenized (016): ['activities', '=', 'models', '.', 'ManyToManyField', '(', 'ActivityType', ',', 'limit_choices_to', '=', '{', ':', 'True', '}', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "opener . addheaders = [ ( , ) ] \n"
Original    (010): ['opener', '.', 'addheaders', '=', '[', '(', ',', ')', ']', '\\n']
Tokenized   (015): ['<s>', 'op', 'ener', '.', 'add', 'headers', '=', '[', '(', ',', ')', ']', '\\', 'n', '</s>']
Filtered   (013): ['op', 'ener', '.', 'add', 'headers', '=', '[', '(', ',', ')', ']', '\\', 'n']
Detokenized (010): ['opener', '.', 'addheaders', '=', '[', '(', ',', ')', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "False = 0 \n"
Original    (004): ['False', '=', '0', '\\n']
Tokenized   (007): ['<s>', 'False', '=', '0', '\\', 'n', '</s>']
Filtered   (005): ['False', '=', '0', '\\', 'n']
Detokenized (004): ['False', '=', '0', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "option_pattern = chr ( 0 ) * 8 \n"
Original    (009): ['option_pattern', '=', 'chr', '(', '0', ')', '*', '8', '\\n']
Tokenized   (015): ['<s>', 'option', '_', 'pattern', '=', 'ch', 'r', '(', '0', ')', '*', '8', '\\', 'n', '</s>']
Filtered   (013): ['option', '_', 'pattern', '=', 'ch', 'r', '(', '0', ')', '*', '8', '\\', 'n']
Detokenized (009): ['option_pattern', '=', 'chr', '(', '0', ')', '*', '8', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "begin = toint ( s [ 5 : 9 ] ) \n"
Original    (012): ['begin', '=', 'toint', '(', 's', '[', '5', ':', '9', ']', ')', '\\n']
Tokenized   (016): ['<s>', 'begin', '=', 'to', 'int', '(', 's', '[', '5', ':', '9', ']', ')', '\\', 'n', '</s>']
Filtered   (014): ['begin', '=', 'to', 'int', '(', 's', '[', '5', ':', '9', ']', ')', '\\', 'n']
Detokenized (012): ['begin', '=', 'toint', '(', 's', '[', '5', ':', '9', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "length = len ( s ) - 9 \n"
Original    (009): ['length', '=', 'len', '(', 's', ')', '-', '9', '\\n']
Tokenized   (012): ['<s>', 'length', '=', 'len', '(', 's', ')', '-', '9', '\\', 'n', '</s>']
Filtered   (010): ['length', '=', 'len', '(', 's', ')', '-', '9', '\\', 'n']
Detokenized (009): ['length', '=', 'len', '(', 's', ')', '-', '9', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "DivergeCommits = namedtuple ( "DivergeCommits" , [ "common_parent" , \n"
Original    (010): ['DivergeCommits', '=', 'namedtuple', '(', '"DivergeCommits"', ',', '[', '"common_parent"', ',', '\\n']
Tokenized   (029): ['<s>', 'D', 'iver', 'ge', 'Comm', 'its', '=', 'named', 't', 'uple', '(', '"', 'D', 'iver', 'ge', 'Comm', 'its', '"', ',', '[', '"', 'common', '_', 'parent', '"', ',', '\\', 'n', '</s>']
Filtered   (027): ['D', 'iver', 'ge', 'Comm', 'its', '=', 'named', 't', 'uple', '(', '"', 'D', 'iver', 'ge', 'Comm', 'its', '"', ',', '[', '"', 'common', '_', 'parent', '"', ',', '\\', 'n']
Detokenized (010): ['DivergeCommits', '=', 'namedtuple', '(', '"DivergeCommits"', ',', '[', '"common_parent"', ',', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : ""first_commits" , "second_commits" ] ) \n"
Original    (006): ['"first_commits"', ',', '"second_commits"', ']', ')', '\\n']
Tokenized   (019): ['<s>', '"', 'first', '_', 'comm', 'its', '"', ',', '"', 'second', '_', 'comm', 'its', '"', ']', ')', '\\', 'n', '</s>']
Filtered   (017): ['"', 'first', '_', 'comm', 'its', '"', ',', '"', 'second', '_', 'comm', 'its', '"', ']', ')', '\\', 'n']
Detokenized (006): ['"first_commits"', ',', '"second_commits"', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "behind = len ( diverge_commits . second_commits ) > 0 \n"
Original    (011): ['behind', '=', 'len', '(', 'diverge_commits', '.', 'second_commits', ')', '>', '0', '\\n']
Tokenized   (021): ['<s>', 'behind', '=', 'len', '(', 'diver', 'ge', '_', 'comm', 'its', '.', 'second', '_', 'comm', 'its', ')', '>', '0', '\\', 'n', '</s>']
Filtered   (019): ['behind', '=', 'len', '(', 'diver', 'ge', '_', 'comm', 'its', '.', 'second', '_', 'comm', 'its', ')', '>', '0', '\\', 'n']
Detokenized (011): ['behind', '=', 'len', '(', 'diverge_commits', '.', 'second_commits', ')', '>', '0', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "onerror = lambda function , fpath , excinfo : log . info ( \n"
Original    (014): ['onerror', '=', 'lambda', 'function', ',', 'fpath', ',', 'excinfo', ':', 'log', '.', 'info', '(', '\\n']
Tokenized   (020): ['<s>', 'oner', 'ror', '=', 'lambda', 'function', ',', 'f', 'path', ',', 'exc', 'info', ':', 'log', '.', 'info', '(', '\\', 'n', '</s>']
Filtered   (018): ['oner', 'ror', '=', 'lambda', 'function', ',', 'f', 'path', ',', 'exc', 'info', ':', 'log', '.', 'info', '(', '\\', 'n']
Detokenized (014): ['onerror', '=', 'lambda', 'function', ',', 'fpath', ',', 'excinfo', ':', 'log', '.', 'info', '(', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "commiter = Signature ( commiter [ 0 ] , commiter [ 1 ] ) \n"
Original    (015): ['commiter', '=', 'Signature', '(', 'commiter', '[', '0', ']', ',', 'commiter', '[', '1', ']', ')', '\\n']
Tokenized   (021): ['<s>', 'comm', 'iter', '=', 'Signature', '(', 'comm', 'iter', '[', '0', ']', ',', 'comm', 'iter', '[', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (019): ['comm', 'iter', '=', 'Signature', '(', 'comm', 'iter', '[', '0', ']', ',', 'comm', 'iter', '[', '1', ']', ')', '\\', 'n']
Detokenized (015): ['commiter', '=', 'Signature', '(', 'commiter', '[', '0', ']', ',', 'commiter', '[', '1', ']', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "len ( path_components ) == 1 and \n"
Original    (008): ['len', '(', 'path_components', ')', '==', '1', 'and', '\\n']
Tokenized   (014): ['<s>', 'len', '(', 'path', '_', 'comp', 'onents', ')', '==', '1', 'and', '\\', 'n', '</s>']
Filtered   (012): ['len', '(', 'path', '_', 'comp', 'onents', ')', '==', '1', 'and', '\\', 'n']
Detokenized (008): ['len', '(', 'path_components', ')', '==', '1', 'and', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "entry_name == path_components [ 0 ] ) \n"
Original    (008): ['entry_name', '==', 'path_components', '[', '0', ']', ')', '\\n']
Tokenized   (016): ['<s>', 'entry', '_', 'name', '==', 'path', '_', 'comp', 'onents', '[', '0', ']', ')', '\\', 'n', '</s>']
Filtered   (014): ['entry', '_', 'name', '==', 'path', '_', 'comp', 'onents', '[', '0', ']', ')', '\\', 'n']
Detokenized (008): ['entry_name', '==', 'path_components', '[', '0', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "lambda entry : self . _repo [ entry . id ] ) \n"
Original    (013): ['lambda', 'entry', ':', 'self', '.', '_repo', '[', 'entry', '.', 'id', ']', ')', '\\n']
Tokenized   (018): ['<s>', 'lambda', 'entry', ':', 'self', '.', '_', 're', 'po', '[', 'entry', '.', 'id', ']', ')', '\\', 'n', '</s>']
Filtered   (016): ['lambda', 'entry', ':', 'self', '.', '_', 're', 'po', '[', 'entry', '.', 'id', ']', ')', '\\', 'n']
Detokenized (013): ['lambda', 'entry', ':', 'self', '.', '_repo', '[', 'entry', '.', 'id', ']', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "GIT_FILEMODE_LINK : { \n"
Original    (004): ['GIT_FILEMODE_LINK', ':', '{', '\\n']
Tokenized   (014): ['<s>', 'G', 'IT', '_', 'FILE', 'MODE', '_', 'L', 'INK', ':', '{', '\\', 'n', '</s>']
Filtered   (012): ['G', 'IT', '_', 'FILE', 'MODE', '_', 'L', 'INK', ':', '{', '\\', 'n']
Detokenized (004): ['GIT_FILEMODE_LINK', ':', '{', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "iterators = [ self . _repo . walk ( branch . target , sort ) \n"
Original    (016): ['iterators', '=', '[', 'self', '.', '_repo', '.', 'walk', '(', 'branch', '.', 'target', ',', 'sort', ')', '\\n']
Tokenized   (022): ['<s>', 'iter', 'ators', '=', '[', 'self', '.', '_', 're', 'po', '.', 'walk', '(', 'branch', '.', 'target', ',', 'sort', ')', '\\', 'n', '</s>']
Filtered   (020): ['iter', 'ators', '=', '[', 'self', '.', '_', 're', 'po', '.', 'walk', '(', 'branch', '.', 'target', ',', 'sort', ')', '\\', 'n']
Detokenized (016): ['iterators', '=', '[', 'self', '.', '_repo', '.', 'walk', '(', 'branch', '.', 'target', ',', 'sort', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "stop_iteration = [ False for branch in branches ] \n"
Original    (010): ['stop_iteration', '=', '[', 'False', 'for', 'branch', 'in', 'branches', ']', '\\n']
Tokenized   (016): ['<s>', 'stop', '_', 'iter', 'ation', '=', '[', 'False', 'for', 'branch', 'in', 'branches', ']', '\\', 'n', '</s>']
Filtered   (014): ['stop', '_', 'iter', 'ation', '=', '[', 'False', 'for', 'branch', 'in', 'branches', ']', '\\', 'n']
Detokenized (010): ['stop_iteration', '=', '[', 'False', 'for', 'branch', 'in', 'branches', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "second_commit in first_commits ) : \n"
Original    (006): ['second_commit', 'in', 'first_commits', ')', ':', '\\n']
Tokenized   (014): ['<s>', 'second', '_', 'commit', 'in', 'first', '_', 'comm', 'its', ')', ':', '\\', 'n', '</s>']
Filtered   (012): ['second', '_', 'commit', 'in', 'first', '_', 'comm', 'its', ')', ':', '\\', 'n']
Detokenized (006): ['second_commit', 'in', 'first_commits', ')', ':', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "new_commit = Commit ( 2 , 2 , "21111111111" ) \n"
Original    (011): ['new_commit', '=', 'Commit', '(', '2', ',', '2', ',', '"21111111111"', ')', '\\n']
Tokenized   (020): ['<s>', 'new', '_', 'commit', '=', 'Commit', '(', '2', ',', '2', ',', '"', '211', '1111', '1111', '"', ')', '\\', 'n', '</s>']
Filtered   (018): ['new', '_', 'commit', '=', 'Commit', '(', '2', ',', '2', ',', '"', '211', '1111', '1111', '"', ')', '\\', 'n']
Detokenized (011): ['new_commit', '=', 'Commit', '(', '2', ',', '2', ',', '"21111111111"', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "mocked_repo . walk . assert_called_once_with ( "head" , GIT_SORT_TIME ) \n"
Original    (011): ['mocked_repo', '.', 'walk', '.', 'assert_called_once_with', '(', '"head"', ',', 'GIT_SORT_TIME', ')', '\\n']
Tokenized   (032): ['<s>', 'm', 'ocked', '_', 're', 'po', '.', 'walk', '.', 'assert', '_', 'called', '_', 'once', '_', 'with', '(', '"', 'head', '"', ',', 'G', 'IT', '_', 'S', 'ORT', '_', 'TIME', ')', '\\', 'n', '</s>']
Filtered   (030): ['m', 'ocked', '_', 're', 'po', '.', 'walk', '.', 'assert', '_', 'called', '_', 'once', '_', 'with', '(', '"', 'head', '"', ',', 'G', 'IT', '_', 'S', 'ORT', '_', 'TIME', ')', '\\', 'n']
Detokenized (011): ['mocked_repo', '.', 'walk', '.', 'assert_called_once_with', '(', '"head"', ',', 'GIT_SORT_TIME', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "to_datetime = True ) == datetime \n"
Original    (007): ['to_datetime', '=', 'True', ')', '==', 'datetime', '\\n']
Tokenized   (014): ['<s>', 'to', '_', 'dat', 'etime', '=', 'True', ')', '==', 'dat', 'etime', '\\', 'n', '</s>']
Filtered   (012): ['to', '_', 'dat', 'etime', '=', 'True', ')', '==', 'dat', 'etime', '\\', 'n']
Detokenized (007): ['to_datetime', '=', 'True', ')', '==', 'datetime', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "date = dt . date ( 1970 , 1 , 1 ) \n"
Original    (013): ['date', '=', 'dt', '.', 'date', '(', '1970', ',', '1', ',', '1', ')', '\\n']
Tokenized   (017): ['<s>', 'date', '=', 'd', 't', '.', 'date', '(', '1970', ',', '1', ',', '1', ')', '\\', 'n', '</s>']
Filtered   (015): ['date', '=', 'd', 't', '.', 'date', '(', '1970', ',', '1', ',', '1', ')', '\\', 'n']
Detokenized (013): ['date', '=', 'dt', '.', 'date', '(', '1970', ',', '1', ',', '1', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "datetime = dt . datetime ( 1970 , 1 , 1 , 13 , 30 ) \n"
Original    (017): ['datetime', '=', 'dt', '.', 'datetime', '(', '1970', ',', '1', ',', '1', ',', '13', ',', '30', ')', '\\n']
Tokenized   (023): ['<s>', 'dat', 'etime', '=', 'd', 't', '.', 'dat', 'etime', '(', '1970', ',', '1', ',', '1', ',', '13', ',', '30', ')', '\\', 'n', '</s>']
Filtered   (021): ['dat', 'etime', '=', 'd', 't', '.', 'dat', 'etime', '(', '1970', ',', '1', ',', '1', ',', '13', ',', '30', ')', '\\', 'n']
Detokenized (017): ['datetime', '=', 'dt', '.', 'datetime', '(', '1970', ',', '1', ',', '1', ',', '13', ',', '30', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "internationalizeDocstring = lambda x : x \n"
Original    (007): ['internationalizeDocstring', '=', 'lambda', 'x', ':', 'x', '\\n']
Tokenized   (013): ['<s>', 'international', 'ize', 'Doc', 'string', '=', 'lambda', 'x', ':', 'x', '\\', 'n', '</s>']
Filtered   (011): ['international', 'ize', 'Doc', 'string', '=', 'lambda', 'x', ':', 'x', '\\', 'n']
Detokenized (007): ['internationalizeDocstring', '=', 'lambda', 'x', ':', 'x', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "conf . supybot . drivers . maxReconnectWait ( ) ) \n"
Original    (011): ['conf', '.', 'supybot', '.', 'drivers', '.', 'maxReconnectWait', '(', ')', ')', '\\n']
Tokenized   (020): ['<s>', 'conf', '.', 'sup', 'y', 'bot', '.', 'drivers', '.', 'max', 'Rec', 'on', 'nect', 'Wait', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (018): ['conf', '.', 'sup', 'y', 'bot', '.', 'drivers', '.', 'max', 'Rec', 'on', 'nect', 'Wait', '(', ')', ')', '\\', 'n']
Detokenized (011): ['conf', '.', 'supybot', '.', 'drivers', '.', 'maxReconnectWait', '(', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "inst . conn . _sock . __class__ is socket . _closedsocket ) : \n"
Original    (014): ['inst', '.', 'conn', '.', '_sock', '.', '__class__', 'is', 'socket', '.', '_closedsocket', ')', ':', '\\n']
Tokenized   (023): ['<s>', 'inst', '.', 'conn', '.', '_', 's', 'ock', '.', '__', 'class', '__', 'is', 'socket', '.', '_', 'closed', 'socket', ')', ':', '\\', 'n', '</s>']
Filtered   (021): ['inst', '.', 'conn', '.', '_', 's', 'ock', '.', '__', 'class', '__', 'is', 'socket', '.', '_', 'closed', 'socket', ')', ':', '\\', 'n']
Detokenized (014): ['inst', '.', 'conn', '.', '_sock', '.', '__class__', 'is', 'socket', '.', '_closedsocket', ')', ':', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "network_config = getattr ( conf . supybot . networks , self . irc . network ) \n"
Original    (017): ['network_config', '=', 'getattr', '(', 'conf', '.', 'supybot', '.', 'networks', ',', 'self', '.', 'irc', '.', 'network', ')', '\\n']
Tokenized   (026): ['<s>', 'network', '_', 'config', '=', 'get', 'attr', '(', 'conf', '.', 'sup', 'y', 'bot', '.', 'networks', ',', 'self', '.', '', 'irc', '.', 'network', ')', '\\', 'n', '</s>']
Filtered   (024): ['network', '_', 'config', '=', 'get', 'attr', '(', 'conf', '.', 'sup', 'y', 'bot', '.', 'networks', ',', 'self', '.', '', 'irc', '.', 'network', ')', '\\', 'n']
Detokenized (017): ['network_config', '=', 'getattr', '(', 'conf', '.', 'supybot', '.', 'networks', ',', 'self', '.', 'irc', '.', 'network', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "vhost = conf . supybot . protocols . irc . vhost ( ) , \n"
Original    (015): ['vhost', '=', 'conf', '.', 'supybot', '.', 'protocols', '.', 'irc', '.', 'vhost', '(', ')', ',', '\\n']
Tokenized   (023): ['<s>', 'v', 'host', '=', 'conf', '.', 'sup', 'y', 'bot', '.', 'protocols', '.', '', 'irc', '.', 'v', 'host', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (021): ['v', 'host', '=', 'conf', '.', 'sup', 'y', 'bot', '.', 'protocols', '.', '', 'irc', '.', 'v', 'host', '(', ')', ',', '\\', 'n']
Detokenized (015): ['vhost', '=', 'conf', '.', 'supybot', '.', 'protocols', '.', 'irc', '.', 'vhost', '(', ')', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "trusted_fingerprints = network_config . ssl . serverFingerprints ( ) , \n"
Original    (011): ['trusted_fingerprints', '=', 'network_config', '.', 'ssl', '.', 'serverFingerprints', '(', ')', ',', '\\n']
Tokenized   (024): ['<s>', 'tr', 'usted', '_', 'finger', 'prints', '=', 'network', '_', 'config', '.', 's', 'sl', '.', 'server', 'F', 'inger', 'prints', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (022): ['tr', 'usted', '_', 'finger', 'prints', '=', 'network', '_', 'config', '.', 's', 'sl', '.', 'server', 'F', 'inger', 'prints', '(', ')', ',', '\\', 'n']
Detokenized (011): ['trusted_fingerprints', '=', 'network_config', '.', 'ssl', '.', 'serverFingerprints', '(', ')', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "while tb : \n"
Original    (004): ['while', 'tb', ':', '\\n']
Tokenized   (008): ['<s>', 'while', 't', 'b', ':', '\\', 'n', '</s>']
Filtered   (006): ['while', 't', 'b', ':', '\\', 'n']
Detokenized (004): ['while', 'tb', ':', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "frame . f_lineno ) ) \n"
Original    (006): ['frame', '.', 'f_lineno', ')', ')', '\\n']
Tokenized   (012): ['<s>', 'frame', '.', 'f', '_', 'lin', 'eno', ')', ')', '\\', 'n', '</s>']
Filtered   (010): ['frame', '.', 'f', '_', 'lin', 'eno', ')', ')', '\\', 'n']
Detokenized (006): ['frame', '.', 'f_lineno', ')', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "window = timedelta ( 0 , 0 , 0 , 0 , settings . SESSION_TIMEOUT ) \n"
Original    (017): ['window', '=', 'timedelta', '(', '0', ',', '0', ',', '0', ',', '0', ',', 'settings', '.', 'SESSION_TIMEOUT', ')', '\\n']
Tokenized   (025): ['<s>', 'window', '=', 'timed', 'elta', '(', '0', ',', '0', ',', '0', ',', '0', ',', 'settings', '.', 'S', 'ESSION', '_', 'TIME', 'OUT', ')', '\\', 'n', '</s>']
Filtered   (023): ['window', '=', 'timed', 'elta', '(', '0', ',', '0', ',', '0', ',', '0', ',', 'settings', '.', 'S', 'ESSION', '_', 'TIME', 'OUT', ')', '\\', 'n']
Detokenized (017): ['window', '=', 'timedelta', '(', '0', ',', '0', ',', '0', ',', '0', ',', 'settings', '.', 'SESSION_TIMEOUT', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "shared = request . POST . get ( "shared" , False ) \n"
Original    (013): ['shared', '=', 'request', '.', 'POST', '.', 'get', '(', '"shared"', ',', 'False', ')', '\\n']
Tokenized   (018): ['<s>', 'shared', '=', 'request', '.', 'POST', '.', 'get', '(', '"', 'shared', '"', ',', 'False', ')', '\\', 'n', '</s>']
Filtered   (016): ['shared', '=', 'request', '.', 'POST', '.', 'get', '(', '"', 'shared', '"', ',', 'False', ')', '\\', 'n']
Detokenized (013): ['shared', '=', 'request', '.', 'POST', '.', 'get', '(', '"shared"', ',', 'False', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "} , p . score_functions . all ( ) . filter ( selectable_bodies = plan . legislative_body } , report_displays [ 0 ] . scorepanel_set . all ( ) . order_by ( ) ) \n"
Original    (035): ['}', ',', 'p', '.', 'score_functions', '.', 'all', '(', ')', '.', 'filter', '(', 'selectable_bodies', '=', 'plan', '.', 'legislative_body', '}', ',', 'report_displays', '[', '0', ']', '.', 'scorepanel_set', '.', 'all', '(', ')', '.', 'order_by', '(', ')', ')', '\\n']
Tokenized   (055): ['<s>', '}', ',', 'p', '.', 'score', '_', 'fun', 'ctions', '.', 'all', '(', ')', '.', 'filter', '(', 'select', 'able', '_', 'b', 'odies', '=', 'plan', '.', 'legislative', '_', 'body', '}', ',', 'report', '_', 'dis', 'plays', '[', '0', ']', '.', 'score', 'panel', '_', 'set', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (053): ['}', ',', 'p', '.', 'score', '_', 'fun', 'ctions', '.', 'all', '(', ')', '.', 'filter', '(', 'select', 'able', '_', 'b', 'odies', '=', 'plan', '.', 'legislative', '_', 'body', '}', ',', 'report', '_', 'dis', 'plays', '[', '0', ']', '.', 'score', 'panel', '_', 'set', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ')', ')', '\\', 'n']
Detokenized (035): ['}', ',', 'p', '.', 'score_functions', '.', 'all', '(', ')', '.', 'filter', '(', 'selectable_bodies', '=', 'plan', '.', 'legislative_body', '}', ',', 'report_displays', '[', '0', ']', '.', 'scorepanel_set', '.', 'all', '(', ')', '.', 'order_by', '(', ')', ')', '\\n']
Counter: 53
===================================================================
Hidden states:  (13, 35, 768)
# Extracted words:  35
Sentence         : "body_member_long_label = _ ( ) + \n"
Original    (007): ['body_member_long_label', '=', '_', '(', ')', '+', '\\n']
Tokenized   (016): ['<s>', 'body', '_', 'member', '_', 'long', '_', 'label', '=', '_', '(', ')', '+', '\\', 'n', '</s>']
Filtered   (014): ['body', '_', 'member', '_', 'long', '_', 'label', '=', '_', '(', ')', '+', '\\', 'n']
Detokenized (007): ['body_member_long_label', '=', '_', '(', ')', '+', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "body_members = _n ( , , 2 ) \n"
Original    (009): ['body_members', '=', '_n', '(', ',', ',', '2', ')', '\\n']
Tokenized   (015): ['<s>', 'body', '_', 'members', '=', '_', 'n', '(', ',', ',', '2', ')', '\\', 'n', '</s>']
Filtered   (013): ['body', '_', 'members', '=', '_', 'n', '(', ',', ',', '2', ')', '\\', 'n']
Detokenized (009): ['body_members', '=', '_n', '(', ',', ',', '2', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "has_regions = Region . objects . all ( ) . count ( ) > 1 \n"
Original    (016): ['has_regions', '=', 'Region', '.', 'objects', '.', 'all', '(', ')', '.', 'count', '(', ')', '>', '1', '\\n']
Tokenized   (022): ['<s>', 'has', '_', 'reg', 'ions', '=', 'Region', '.', 'objects', '.', 'all', '(', ')', '.', 'count', '(', ')', '>', '1', '\\', 'n', '</s>']
Filtered   (020): ['has', '_', 'reg', 'ions', '=', 'Region', '.', 'objects', '.', 'all', '(', ')', '.', 'count', '(', ')', '>', '1', '\\', 'n']
Detokenized (016): ['has_regions', '=', 'Region', '.', 'objects', '.', 'all', '(', ')', '.', 'count', '(', ')', '>', '1', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "bodies = LegislativeBody . objects . all ( ) . order_by ( , ) \n"
Original    (015): ['bodies', '=', 'LegislativeBody', '.', 'objects', '.', 'all', '(', ')', '.', 'order_by', '(', ',', ')', '\\n']
Tokenized   (022): ['<s>', 'b', 'odies', '=', 'Legislative', 'Body', '.', 'objects', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (020): ['b', 'odies', '=', 'Legislative', 'Body', '.', 'objects', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ',', ')', '\\', 'n']
Detokenized (015): ['bodies', '=', 'LegislativeBody', '.', 'objects', '.', 'all', '(', ')', '.', 'order_by', '(', ',', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "l_bodies = [ b for b in bodies if b in [ sd . legislative_body for sd in ScoreDisplay . objects . filter \n"
Original    (024): ['l_bodies', '=', '[', 'b', 'for', 'b', 'in', 'bodies', 'if', 'b', 'in', '[', 'sd', '.', 'legislative_body', 'for', 'sd', 'in', 'ScoreDisplay', '.', 'objects', '.', 'filter', '\\n']
Tokenized   (033): ['<s>', 'l', '_', 'b', 'odies', '=', '[', 'b', 'for', 'b', 'in', 'bodies', 'if', 'b', 'in', '[', 'sd', '.', 'legislative', '_', 'body', 'for', 'sd', 'in', 'Score', 'Display', '.', 'objects', '.', 'filter', '\\', 'n', '</s>']
Filtered   (031): ['l', '_', 'b', 'odies', '=', '[', 'b', 'for', 'b', 'in', 'bodies', 'if', 'b', 'in', '[', 'sd', '.', 'legislative', '_', 'body', 'for', 'sd', 'in', 'Score', 'Display', '.', 'objects', '.', 'filter', '\\', 'n']
Detokenized (024): ['l_bodies', '=', '[', 'b', 'for', 'b', 'in', 'bodies', 'if', 'b', 'in', '[', 'sd', '.', 'legislative_body', 'for', 'sd', 'in', 'ScoreDisplay', '.', 'objects', '.', 'filter', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "cfg [ ] = datetime . now ( ) \n"
Original    (010): ['cfg', '[', ']', '=', 'datetime', '.', 'now', '(', ')', '\\n']
Tokenized   (014): ['<s>', 'cfg', '[', ']', '=', 'dat', 'etime', '.', 'now', '(', ')', '\\', 'n', '</s>']
Filtered   (012): ['cfg', '[', ']', '=', 'dat', 'etime', '.', 'now', '(', ')', '\\', 'n']
Detokenized (010): ['cfg', '[', ']', '=', 'datetime', '.', 'now', '(', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "ll = ModestMaps . Geo . Location ( pt1 . y , pt1 . x ) \n"
Original    (017): ['ll', '=', 'ModestMaps', '.', 'Geo', '.', 'Location', '(', 'pt1', '.', 'y', ',', 'pt1', '.', 'x', ')', '\\n']
Tokenized   (023): ['<s>', 'll', '=', 'Modest', 'Maps', '.', 'Geo', '.', 'Location', '(', 'pt', '1', '.', 'y', ',', 'pt', '1', '.', 'x', ')', '\\', 'n', '</s>']
Filtered   (021): ['ll', '=', 'Modest', 'Maps', '.', 'Geo', '.', 'Location', '(', 'pt', '1', '.', 'y', ',', 'pt', '1', '.', 'x', ')', '\\', 'n']
Detokenized (017): ['ll', '=', 'ModestMaps', '.', 'Geo', '.', 'Location', '(', 'pt1', '.', 'y', ',', 'pt1', '.', 'x', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "provider = ModestMaps . WMS . Provider ( cfg [ ] , { \n"
Original    (014): ['provider', '=', 'ModestMaps', '.', 'WMS', '.', 'Provider', '(', 'cfg', '[', ']', ',', '{', '\\n']
Tokenized   (021): ['<s>', 'prov', 'ider', '=', 'Modest', 'Maps', '.', 'W', 'MS', '.', 'Provider', '(', 'cf', 'g', '[', ']', ',', '{', '\\', 'n', '</s>']
Filtered   (019): ['prov', 'ider', '=', 'Modest', 'Maps', '.', 'W', 'MS', '.', 'Provider', '(', 'cf', 'g', '[', ']', ',', '{', '\\', 'n']
Detokenized (014): ['provider', '=', 'ModestMaps', '.', 'WMS', '.', 'Provider', '(', 'cfg', '[', ']', ',', '{', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "overlayImg = Image . blend ( overlayImg , ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) , \n"
Original    (026): ['overlayImg', '=', 'Image', '.', 'blend', '(', 'overlayImg', ',', 'ModestMaps', '.', 'mapByExtent', '(', 'provider', ',', 'll', ',', 'ur', ',', 'dims', ')', '.', 'draw', '(', ')', ',', '\\n']
Tokenized   (039): ['<s>', 'over', 'lay', 'Im', 'g', '=', 'Image', '.', 'blend', '(', 'overlay', 'Im', 'g', ',', 'Modest', 'Maps', '.', 'map', 'By', 'Ext', 'ent', '(', 'provider', ',', 'll', ',', 'ur', ',', 'dim', 's', ')', '.', 'draw', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (037): ['over', 'lay', 'Im', 'g', '=', 'Image', '.', 'blend', '(', 'overlay', 'Im', 'g', ',', 'Modest', 'Maps', '.', 'map', 'By', 'Ext', 'ent', '(', 'provider', ',', 'll', ',', 'ur', ',', 'dim', 's', ')', '.', 'draw', '(', ')', ',', '\\', 'n']
Detokenized (026): ['overlayImg', '=', 'Image', '.', 'blend', '(', 'overlayImg', ',', 'ModestMaps', '.', 'mapByExtent', '(', 'provider', ',', 'll', ',', 'ur', ',', 'dims', ')', '.', 'draw', '(', ')', ',', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "fullImg . save ( settings . WEB_TEMP + ( % sha . hexdigest ( ) ) , , quality = 100 ) \n"
Original    (023): ['fullImg', '.', 'save', '(', 'settings', '.', 'WEB_TEMP', '+', '(', '%', 'sha', '.', 'hexdigest', '(', ')', ')', ',', ',', 'quality', '=', '100', ')', '\\n']
Tokenized   (035): ['<s>', 'full', 'Im', 'g', '.', 'save', '(', 'settings', '.', 'WE', 'B', '_', 'T', 'EMP', '+', '(', '%', 'sh', 'a', '.', 'hex', 'dig', 'est', '(', ')', ')', ',', ',', 'quality', '=', '100', ')', '\\', 'n', '</s>']
Filtered   (033): ['full', 'Im', 'g', '.', 'save', '(', 'settings', '.', 'WE', 'B', '_', 'T', 'EMP', '+', '(', '%', 'sh', 'a', '.', 'hex', 'dig', 'est', '(', ')', ')', ',', ',', 'quality', '=', '100', ')', '\\', 'n']
Detokenized (023): ['fullImg', '.', 'save', '(', 'settings', '.', 'WEB_TEMP', '+', '(', '%', 'sha', '.', 'hexdigest', '(', ')', ')', ',', ',', 'quality', '=', '100', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "CreatePDF ( page , result , show_error_as_pdf = True ) \n"
Original    (011): ['CreatePDF', '(', 'page', ',', 'result', ',', 'show_error_as_pdf', '=', 'True', ')', '\\n']
Tokenized   (021): ['<s>', 'Create', 'PDF', '(', 'page', ',', 'result', ',', 'show', '_', 'error', '_', 'as', '_', 'pdf', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (019): ['Create', 'PDF', '(', 'page', ',', 'result', ',', 'show', '_', 'error', '_', 'as', '_', 'pdf', '=', 'True', ')', '\\', 'n']
Detokenized (011): ['CreatePDF', '(', 'page', ',', 'result', ',', 'show_error_as_pdf', '=', 'True', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "body = LegislativeBody . objects . get ( id = int ( request . POST [ ] ) ) \n"
Original    (020): ['body', '=', 'LegislativeBody', '.', 'objects', '.', 'get', '(', 'id', '=', 'int', '(', 'request', '.', 'POST', '[', ']', ')', ')', '\\n']
Tokenized   (024): ['<s>', 'body', '=', 'Legislative', 'Body', '.', 'objects', '.', 'get', '(', 'id', '=', 'int', '(', 'request', '.', 'POST', '[', ']', ')', ')', '\\', 'n', '</s>']
Filtered   (022): ['body', '=', 'Legislative', 'Body', '.', 'objects', '.', 'get', '(', 'id', '=', 'int', '(', 'request', '.', 'POST', '[', ']', ')', ')', '\\', 'n']
Detokenized (020): ['body', '=', 'LegislativeBody', '.', 'objects', '.', 'get', '(', 'id', '=', 'int', '(', 'request', '.', 'POST', '[', ']', ')', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "PlanReport . createreport . delay ( planid , stamp , req , language = translation . get_language ( ) ) \n"
Original    (021): ['PlanReport', '.', 'createreport', '.', 'delay', '(', 'planid', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get_language', '(', ')', ')', '\\n']
Tokenized   (029): ['<s>', 'Plan', 'Report', '.', 'create', 'report', '.', 'delay', '(', 'plan', 'id', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get', '_', 'language', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (027): ['Plan', 'Report', '.', 'create', 'report', '.', 'delay', '(', 'plan', 'id', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get', '_', 'language', '(', ')', ')', '\\', 'n']
Detokenized (021): ['PlanReport', '.', 'createreport', '.', 'delay', '(', 'planid', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get_language', '(', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "stamp = request . POST . get ( , sha . hexdigest ( ) ) \n"
Original    (016): ['stamp', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'sha', '.', 'hexdigest', '(', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'st', 'amp', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'sh', 'a', '.', 'hex', 'dig', 'est', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['st', 'amp', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'sh', 'a', '.', 'hex', 'dig', 'est', '(', ')', ')', '\\', 'n']
Detokenized (016): ['stamp', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'sha', '.', 'hexdigest', '(', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "CalculatorReport . createcalculatorreport . delay ( planid , stamp , req , language = translation . get_language ~~ else : \n"
Original    (021): ['CalculatorReport', '.', 'createcalculatorreport', '.', 'delay', '(', 'planid', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get_language', '~~', 'else', ':', '\\n']
Tokenized   (035): ['<s>', 'Cal', 'cul', 'ator', 'Report', '.', 'create', 'cal', 'cul', 'ator', 'report', '.', 'delay', '(', 'plan', 'id', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get', '_', 'language', '', '~~', 'else', ':', '\\', 'n', '</s>']
Filtered   (033): ['Cal', 'cul', 'ator', 'Report', '.', 'create', 'cal', 'cul', 'ator', 'report', '.', 'delay', '(', 'plan', 'id', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get', '_', 'language', '', '~~', 'else', ':', '\\', 'n']
Detokenized (021): ['CalculatorReport', '.', 'createcalculatorreport', '.', 'delay', '(', 'planid', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get_language', '~~', 'else', ':', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "site_id = Site . objects . get_current ( ) . id , \n"
Original    (013): ['site_id', '=', 'Site', '.', 'objects', '.', 'get_current', '(', ')', '.', 'id', ',', '\\n']
Tokenized   (020): ['<s>', 'site', '_', 'id', '=', 'Site', '.', 'objects', '.', 'get', '_', 'current', '(', ')', '.', 'id', ',', '\\', 'n', '</s>']
Filtered   (018): ['site', '_', 'id', '=', 'Site', '.', 'objects', '.', 'get', '_', 'current', '(', ')', '.', 'id', ',', '\\', 'n']
Detokenized (013): ['site_id', '=', 'Site', '.', 'objects', '.', 'get_current', '(', ')', '.', 'id', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "from_id = int ( request . POST . get ( , - 1 ) ) \n"
Original    (016): ['from_id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '-', '1', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'from', '_', 'id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '-', '1', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['from', '_', 'id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '-', '1', ')', ')', '\\', 'n']
Detokenized (016): ['from_id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '-', '1', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "to_id = int ( request . POST . get ( , None ) ) \n"
Original    (015): ['to_id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', 'None', ')', ')', '\\n']
Tokenized   (020): ['<s>', 'to', '_', 'id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', 'None', ')', ')', '\\', 'n', '</s>']
Filtered   (018): ['to', '_', 'id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', 'None', ')', ')', '\\', 'n']
Detokenized (015): ['to_id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', 'None', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "from_districts = filter ( lambda d : True if d . district_id == from_id else False , all_districts to_district = filter ( lambda d : True if d . district_id == to_id else False , all_districts ) [ 0 ] \n"
Original    (041): ['from_districts', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district_id', '==', 'from_id', 'else', 'False', ',', 'all_districts', 'to_district', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district_id', '==', 'to_id', 'else', 'False', ',', 'all_districts', ')', '[', '0', ']', '\\n']
Tokenized   (067): ['<s>', 'from', '_', 'dist', 'rict', 's', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district', '_', 'id', '==', 'from', '_', 'id', 'else', 'False', ',', 'all', '_', 'dist', 'rict', 's', 'to', '_', 'dist', 'rict', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district', '_', 'id', '==', 'to', '_', 'id', 'else', 'False', ',', 'all', '_', 'dist', 'rict', 's', ')', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (065): ['from', '_', 'dist', 'rict', 's', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district', '_', 'id', '==', 'from', '_', 'id', 'else', 'False', ',', 'all', '_', 'dist', 'rict', 's', 'to', '_', 'dist', 'rict', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district', '_', 'id', '==', 'to', '_', 'id', 'else', 'False', ',', 'all', '_', 'dist', 'rict', 's', ')', '[', '0', ']', '\\', 'n']
Detokenized (041): ['from_districts', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district_id', '==', 'from_id', 'else', 'False', ',', 'all_districts', 'to_district', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district_id', '==', 'to_id', 'else', 'False', ',', 'all_districts', ')', '[', '0', ']', '\\n']
Counter: 65
===================================================================
Hidden states:  (13, 41, 768)
# Extracted words:  41
Sentence         : "inverse = request . REQUEST [ ] == if in request . REQUEST else False \n"
Original    (016): ['inverse', '=', 'request', '.', 'REQUEST', '[', ']', '==', 'if', 'in', 'request', '.', 'REQUEST', 'else', 'False', '\\n']
Tokenized   (022): ['<s>', 'in', 'verse', '=', 'request', '.', 'RE', 'QUEST', '[', ']', '==', 'if', 'in', 'request', '.', 'RE', 'QUEST', 'else', 'False', '\\', 'n', '</s>']
Filtered   (020): ['in', 'verse', '=', 'request', '.', 'RE', 'QUEST', '[', ']', '==', 'if', 'in', 'request', '.', 'RE', 'QUEST', 'else', 'False', '\\', 'n']
Detokenized (016): ['inverse', '=', 'request', '.', 'REQUEST', '[', ']', '==', 'if', 'in', 'request', '.', 'REQUEST', 'else', 'False', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "my_context . update ( plan . compute_splits ( layer , version = version , inverse = inverse , extended last_item = layer is layers [ - 1 ] \n"
Original    (029): ['my_context', '.', 'update', '(', 'plan', '.', 'compute_splits', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', ',', 'extended', 'last_item', '=', 'layer', 'is', 'layers', '[', '-', '1', ']', '\\n']
Tokenized   (039): ['<s>', 'my', '_', 'context', '.', 'update', '(', 'plan', '.', 'compute', '_', 'spl', 'its', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', ',', 'extended', 'last', '_', 'item', '=', 'layer', 'is', 'layers', '[', '-', '1', ']', '\\', 'n', '</s>']
Filtered   (037): ['my', '_', 'context', '.', 'update', '(', 'plan', '.', 'compute', '_', 'spl', 'its', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', ',', 'extended', 'last', '_', 'item', '=', 'layer', 'is', 'layers', '[', '-', '1', ']', '\\', 'n']
Detokenized (029): ['my_context', '.', 'update', '(', 'plan', '.', 'compute_splits', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', ',', 'extended', 'last_item', '=', 'layer', 'is', 'layers', '[', '-', '1', ']', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "community_info = plan . get_community_type_info ( layer , version = version , inverse = inverse if community_info is not None : \n"
Original    (022): ['community_info', '=', 'plan', '.', 'get_community_type_info', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', 'if', 'community_info', 'is', 'not', 'None', ':', '\\n']
Tokenized   (035): ['<s>', 'community', '_', 'info', '=', 'plan', '.', 'get', '_', 'community', '_', 'type', '_', 'info', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', 'if', 'community', '_', 'info', 'is', 'not', 'None', ':', '\\', 'n', '</s>']
Filtered   (033): ['community', '_', 'info', '=', 'plan', '.', 'get', '_', 'community', '_', 'type', '_', 'info', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', 'if', 'community', '_', 'info', 'is', 'not', 'None', ':', '\\', 'n']
Detokenized (022): ['community_info', '=', 'plan', '.', 'get_community_type_info', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', 'if', 'community_info', 'is', 'not', 'None', ':', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "html += report . render ( calc_context ) \n"
Original    (009): ['html', '+=', 'report', '.', 'render', '(', 'calc_context', ')', '\\n']
Tokenized   (014): ['<s>', 'html', '+=', 'report', '.', 'render', '(', 'calc', '_', 'context', ')', '\\', 'n', '</s>']
Filtered   (012): ['html', '+=', 'report', '.', 'render', '(', 'calc', '_', 'context', ')', '\\', 'n']
Detokenized (009): ['html', '+=', 'report', '.', 'render', '(', 'calc_context', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "geounit_ids = string . split ( request . REQUEST [ "geounits" ] , "|" ) \n"
Original    (016): ['geounit_ids', '=', 'string', '.', 'split', '(', 'request', '.', 'REQUEST', '[', '"geounits"', ']', ',', '"|"', ')', '\\n']
Tokenized   (030): ['<s>', 'ge', 'oun', 'it', '_', 'ids', '=', 'string', '.', 'split', '(', 'request', '.', 'RE', 'QUEST', '[', '"', 'ge', 'oun', 'its', '"', ']', ',', '"', '|', '"', ')', '\\', 'n', '</s>']
Filtered   (028): ['ge', 'oun', 'it', '_', 'ids', '=', 'string', '.', 'split', '(', 'request', '.', 'RE', 'QUEST', '[', '"', 'ge', 'oun', 'its', '"', ']', ',', '"', '|', '"', ')', '\\', 'n']
Detokenized (016): ['geounit_ids', '=', 'string', '.', 'split', '(', 'request', '.', 'REQUEST', '[', '"geounits"', ']', ',', '"|"', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "max_version = max ( [ d . version for d in districts ] ) \n"
Original    (015): ['max_version', '=', 'max', '(', '[', 'd', '.', 'version', 'for', 'd', 'in', 'districts', ']', ')', '\\n']
Tokenized   (020): ['<s>', 'max', '_', 'version', '=', 'max', '(', '[', 'd', '.', 'version', 'for', 'd', 'in', 'districts', ']', ')', '\\', 'n', '</s>']
Filtered   (018): ['max', '_', 'version', '=', 'max', '(', '[', 'd', '.', 'version', 'for', 'd', 'in', 'districts', ']', ')', '\\', 'n']
Detokenized (015): ['max_version', '=', 'max', '(', '[', 'd', '.', 'version', 'for', 'd', 'in', 'districts', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "can_undo = max_version > plan . min_version \n"
Original    (008): ['can_undo', '=', 'max_version', '>', 'plan', '.', 'min_version', '\\n']
Tokenized   (017): ['<s>', 'can', '_', 'undo', '=', 'max', '_', 'version', '>', 'plan', '.', 'min', '_', 'version', '\\', 'n', '</s>']
Filtered   (015): ['can', '_', 'undo', '=', 'max', '_', 'version', '>', 'plan', '.', 'min', '_', 'version', '\\', 'n']
Detokenized (008): ['can_undo', '=', 'max_version', '>', 'plan', '.', 'min_version', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "bbox = tuple ( map ( lambda x : float ( x ) , bbox . split ( ) ) ) \n"
Original    (022): ['bbox', '=', 'tuple', '(', 'map', '(', 'lambda', 'x', ':', 'float', '(', 'x', ')', ',', 'bbox', '.', 'split', '(', ')', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'b', 'box', '=', 'tuple', '(', 'map', '(', 'lambda', 'x', ':', 'float', '(', 'x', ')', ',', 'b', 'box', '.', 'split', '(', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['b', 'box', '=', 'tuple', '(', 'map', '(', 'lambda', 'x', ':', 'float', '(', 'x', ')', ',', 'b', 'box', '.', 'split', '(', ')', ')', ')', '\\', 'n']
Detokenized (022): ['bbox', '=', 'tuple', '(', 'map', '(', 'lambda', 'x', ':', 'float', '(', 'x', ')', ',', 'bbox', '.', 'split', '(', ')', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "wkt = wkt . replace ( , ) . replace ( , ) \n"
Original    (014): ['wkt', '=', 'wkt', '.', 'replace', '(', ',', ')', '.', 'replace', '(', ',', ')', '\\n']
Tokenized   (019): ['<s>', 'w', 'kt', '=', 'w', 'kt', '.', 'replace', '(', ',', ')', '.', 'replace', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (017): ['w', 'kt', '=', 'w', 'kt', '.', 'replace', '(', ',', ')', '.', 'replace', '(', ',', ')', '\\', 'n']
Detokenized (014): ['wkt', '=', 'wkt', '.', 'replace', '(', ',', ')', '.', 'replace', '(', ',', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "districts = [ d . id for d in plan . get_districts_at_version ( version , include_geom = True ) if locked = District . objects . filter ( id__in = districts ) . collect ( ) \n"
Original    (037): ['districts', '=', '[', 'd', '.', 'id', 'for', 'd', 'in', 'plan', '.', 'get_districts_at_version', '(', 'version', ',', 'include_geom', '=', 'True', ')', 'if', 'locked', '=', 'District', '.', 'objects', '.', 'filter', '(', 'id__in', '=', 'districts', ')', '.', 'collect', '(', ')', '\\n']
Tokenized   (055): ['<s>', 'dist', 'rict', 's', '=', '[', 'd', '.', 'id', 'for', 'd', 'in', 'plan', '.', 'get', '_', 'dist', 'rict', 's', '_', 'at', '_', 'version', '(', 'version', ',', 'include', '_', 'ge', 'om', '=', 'True', ')', 'if', 'locked', '=', 'District', '.', 'objects', '.', 'filter', '(', 'id', '__', 'in', '=', 'districts', ')', '.', 'collect', '(', ')', '\\', 'n', '</s>']
Filtered   (053): ['dist', 'rict', 's', '=', '[', 'd', '.', 'id', 'for', 'd', 'in', 'plan', '.', 'get', '_', 'dist', 'rict', 's', '_', 'at', '_', 'version', '(', 'version', ',', 'include', '_', 'ge', 'om', '=', 'True', ')', 'if', 'locked', '=', 'District', '.', 'objects', '.', 'filter', '(', 'id', '__', 'in', '=', 'districts', ')', '.', 'collect', '(', ')', '\\', 'n']
Detokenized (037): ['districts', '=', '[', 'd', '.', 'id', 'for', 'd', 'in', 'plan', '.', 'get_districts_at_version', '(', 'version', ',', 'include_geom', '=', 'True', ')', 'if', 'locked', '=', 'District', '.', 'objects', '.', 'filter', '(', 'id__in', '=', 'districts', ')', '.', 'collect', '(', ')', '\\n']
Counter: 53
===================================================================
Hidden states:  (13, 37, 768)
# Extracted words:  37
Sentence         : "locked_buffered = locked . simplify ( 100 , True ) . buffer ( 100 ) if locked else None \n"
Original    (020): ['locked_buffered', '=', 'locked', '.', 'simplify', '(', '100', ',', 'True', ')', '.', 'buffer', '(', '100', ')', 'if', 'locked', 'else', 'None', '\\n']
Tokenized   (026): ['<s>', 'locked', '_', 'buff', 'ered', '=', 'locked', '.', 'simplify', '(', '100', ',', 'True', ')', '.', 'buffer', '(', '100', ')', 'if', 'locked', 'else', 'None', '\\', 'n', '</s>']
Filtered   (024): ['locked', '_', 'buff', 'ered', '=', 'locked', '.', 'simplify', '(', '100', ',', 'True', ')', '.', 'buffer', '(', '100', ')', 'if', 'locked', 'else', 'None', '\\', 'n']
Detokenized (020): ['locked_buffered', '=', 'locked', '.', 'simplify', '(', '100', ',', 'True', ')', '.', 'buffer', '(', '100', ')', 'if', 'locked', 'else', 'None', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "filtered = Geolevel . objects . get ( id = geolevel ) . geounit_set . filter ( selection ) \n"
Original    (020): ['filtered', '=', 'Geolevel', '.', 'objects', '.', 'get', '(', 'id', '=', 'geolevel', ')', '.', 'geounit_set', '.', 'filter', '(', 'selection', ')', '\\n']
Tokenized   (032): ['<s>', 'fil', 'tered', '=', 'Ge', 'ole', 'vel', '.', 'objects', '.', 'get', '(', 'id', '=', 'ge', 'ole', 'vel', ')', '.', 'ge', 'oun', 'it', '_', 'set', '.', 'filter', '(', 'selection', ')', '\\', 'n', '</s>']
Filtered   (030): ['fil', 'tered', '=', 'Ge', 'ole', 'vel', '.', 'objects', '.', 'get', '(', 'id', '=', 'ge', 'ole', 'vel', ')', '.', 'ge', 'oun', 'it', '_', 'set', '.', 'filter', '(', 'selection', ')', '\\', 'n']
Detokenized (020): ['filtered', '=', 'Geolevel', '.', 'objects', '.', 'get', '(', 'id', '=', 'geolevel', ')', '.', 'geounit_set', '.', 'filter', '(', 'selection', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "pfilter = Q ( legislative_body = leg_body ) & Q ( is_valid = True ) \n"
Original    (016): ['pfilter', '=', 'Q', '(', 'legislative_body', '=', 'leg_body', ')', '&', 'Q', '(', 'is_valid', '=', 'True', ')', '\\n']
Tokenized   (026): ['<s>', 'p', 'filter', '=', 'Q', '(', 'legislative', '_', 'body', '=', 'leg', '_', 'body', ')', '&', 'Q', '(', 'is', '_', 'valid', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (024): ['p', 'filter', '=', 'Q', '(', 'legislative', '_', 'body', '=', 'leg', '_', 'body', ')', '&', 'Q', '(', 'is', '_', 'valid', '=', 'True', ')', '\\', 'n']
Detokenized (016): ['pfilter', '=', 'Q', '(', 'legislative_body', '=', 'leg_body', ')', '&', 'Q', '(', 'is_valid', '=', 'True', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "panels = display . scorepanel_set . all ( ) . order_by ( ) \n"
Original    (014): ['panels', '=', 'display', '.', 'scorepanel_set', '.', 'all', '(', ')', '.', 'order_by', '(', ')', '\\n']
Tokenized   (023): ['<s>', 'pan', 'els', '=', 'display', '.', 'score', 'panel', '_', 'set', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ')', '\\', 'n', '</s>']
Filtered   (021): ['pan', 'els', '=', 'display', '.', 'score', 'panel', '_', 'set', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ')', '\\', 'n']
Detokenized (014): ['panels', '=', 'display', '.', 'scorepanel_set', '.', 'all', '(', ')', '.', 'order_by', '(', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "writer . writerow ( [ , , ] + [ p . __unicode__ ( ) for p in panels ] ) \n"
Original    (022): ['writer', '.', 'writerow', '(', '[', ',', ',', ']', '+', '[', 'p', '.', '__unicode__', '(', ')', 'for', 'p', 'in', 'panels', ']', ')', '\\n']
Tokenized   (029): ['<s>', 'writer', '.', 'writer', 'ow', '(', '[', ',', ',', ']', '+', '[', 'p', '.', '__', 'unic', 'ode', '__', '(', ')', 'for', 'p', 'in', 'panels', ']', ')', '\\', 'n', '</s>']
Filtered   (027): ['writer', '.', 'writer', 'ow', '(', '[', ',', ',', ']', '+', '[', 'p', '.', '__', 'unic', 'ode', '__', '(', ')', 'for', 'p', 'in', 'panels', ']', ')', '\\', 'n']
Detokenized (022): ['writer', '.', 'writerow', '(', '[', ',', ',', ']', '+', '[', 'p', '.', '__unicode__', '(', ')', 'for', 'p', 'in', 'panels', ']', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "rows = int ( request . POST . get ( , 10 ) ) \n"
Original    (015): ['rows', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '10', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'rows', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '10', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['rows', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '10', ')', ')', '\\', 'n']
Detokenized (015): ['rows', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '10', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "sidx = request . POST . get ( , ) \n"
Original    (011): ['sidx', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', '\\n']
Tokenized   (015): ['<s>', 'sid', 'x', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (013): ['sid', 'x', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', '\\', 'n']
Detokenized (011): ['sidx', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "owner_filter = request . POST . get ( ) ; \n"
Original    (011): ['owner_filter', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ';', '\\n']
Tokenized   (016): ['<s>', 'owner', '_', 'filter', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ';', '\\', 'n', '</s>']
Filtered   (014): ['owner', '_', 'filter', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ';', '\\', 'n']
Detokenized (011): ['owner_filter', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ';', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "body_pk = int ( body_pk ) if body_pk else body_pk ; \n"
Original    (012): ['body_pk', '=', 'int', '(', 'body_pk', ')', 'if', 'body_pk', 'else', 'body_pk', ';', '\\n']
Tokenized   (027): ['<s>', 'body', '_', 'p', 'k', '=', 'int', '(', 'body', '_', 'p', 'k', ')', 'if', 'body', '_', 'p', 'k', 'else', 'body', '_', 'p', 'k', ';', '\\', 'n', '</s>']
Filtered   (025): ['body', '_', 'p', 'k', '=', 'int', '(', 'body', '_', 'p', 'k', ')', 'if', 'body', '_', 'p', 'k', 'else', 'body', '_', 'p', 'k', ';', '\\', 'n']
Detokenized (012): ['body_pk', '=', 'int', '(', 'body_pk', ')', 'if', 'body_pk', 'else', 'body_pk', ';', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "search = request . POST . get ( , False ) ; \n"
Original    (013): ['search', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', ';', '\\n']
Tokenized   (016): ['<s>', 'search', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', ';', '\\', 'n', '</s>']
Filtered   (014): ['search', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', ';', '\\', 'n']
Detokenized (013): ['search', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', ';', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "search_string = request . POST . get ( , ) ; \n"
Original    (012): ['search_string', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', ';', '\\n']
Tokenized   (017): ['<s>', 'search', '_', 'string', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', ';', '\\', 'n', '</s>']
Filtered   (015): ['search', '_', 'string', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', ';', '\\', 'n']
Detokenized (012): ['search_string', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', ';', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "is_community = request . POST . get ( , False ) == ; \n"
Original    (014): ['is_community', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', '==', ';', '\\n']
Tokenized   (019): ['<s>', 'is', '_', 'community', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', '==', ';', '\\', 'n', '</s>']
Filtered   (017): ['is', '_', 'community', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', '==', ';', '\\', 'n']
Detokenized (014): ['is_community', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', '==', ';', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "all_plans = Plan . objects . filter ( available , not_creating , search_filter , community_filter ) . order_by \n"
Original    (019): ['all_plans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'available', ',', 'not_creating', ',', 'search_filter', ',', 'community_filter', ')', '.', 'order_by', '\\n']
Tokenized   (034): ['<s>', 'all', '_', 'pl', 'ans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'available', ',', 'not', '_', 'creat', 'ing', ',', 'search', '_', 'filter', ',', 'community', '_', 'filter', ')', '.', 'order', '_', 'by', '\\', 'n', '</s>']
Filtered   (032): ['all', '_', 'pl', 'ans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'available', ',', 'not', '_', 'creat', 'ing', ',', 'search', '_', 'filter', ',', 'community', '_', 'filter', ')', '.', 'order', '_', 'by', '\\', 'n']
Detokenized (019): ['all_plans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'available', ',', 'not_creating', ',', 'search_filter', ',', 'community_filter', ')', '.', 'order_by', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "all_districts = ( ) \n"
Original    (005): ['all_districts', '=', '(', ')', '\\n']
Tokenized   (012): ['<s>', 'all', '_', 'dist', 'rict', 's', '=', '(', ')', '\\', 'n', '</s>']
Filtered   (010): ['all', '_', 'dist', 'rict', 's', '=', '(', ')', '\\', 'n']
Detokenized (005): ['all_districts', '=', '(', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "_ ( ) ) \n"
Original    (005): ['_', '(', ')', ')', '\\n']
Tokenized   (008): ['<s>', '_', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (006): ['_', '(', ')', ')', '\\', 'n']
Detokenized (005): ['_', '(', ')', ')', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "user_functions = ScoreFunction . objects . filter ( selectable_bodies = plan . legislative_body ) . order_by for f in user_functions : \n"
Original    (022): ['user_functions', '=', 'ScoreFunction', '.', 'objects', '.', 'filter', '(', 'selectable_bodies', '=', 'plan', '.', 'legislative_body', ')', '.', 'order_by', 'for', 'f', 'in', 'user_functions', ':', '\\n']
Tokenized   (040): ['<s>', 'user', '_', 'fun', 'ctions', '=', 'Score', 'Function', '.', 'objects', '.', 'filter', '(', 'select', 'able', '_', 'b', 'odies', '=', 'plan', '.', 'legislative', '_', 'body', ')', '.', 'order', '_', 'by', 'for', 'f', 'in', 'user', '_', 'fun', 'ctions', ':', '\\', 'n', '</s>']
Filtered   (038): ['user', '_', 'fun', 'ctions', '=', 'Score', 'Function', '.', 'objects', '.', 'filter', '(', 'select', 'able', '_', 'b', 'odies', '=', 'plan', '.', 'legislative', '_', 'body', ')', '.', 'order', '_', 'by', 'for', 'f', 'in', 'user', '_', 'fun', 'ctions', ':', '\\', 'n']
Detokenized (022): ['user_functions', '=', 'ScoreFunction', '.', 'objects', '.', 'filter', '(', 'selectable_bodies', '=', 'plan', '.', 'legislative_body', ')', '.', 'order_by', 'for', 'f', 'in', 'user_functions', ':', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : ""%s_sidebar_demo" % plan . legislative_body . name , \n"
Original    (009): ['"%s_sidebar_demo"', '%', 'plan', '.', 'legislative_body', '.', 'name', ',', '\\n']
Tokenized   (023): ['<s>', '"', '%', 's', '_', 'side', 'bar', '_', 'dem', 'o', '"', '%', 'plan', '.', 'legislative', '_', 'body', '.', 'name', ',', '\\', 'n', '</s>']
Filtered   (021): ['"', '%', 's', '_', 'side', 'bar', '_', 'dem', 'o', '"', '%', 'plan', '.', 'legislative', '_', 'body', '.', 'name', ',', '\\', 'n']
Detokenized (009): ['"%s_sidebar_demo"', '%', 'plan', '.', 'legislative_body', '.', 'name', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "plan . legislative_body . name ) \n"
Original    (007): ['plan', '.', 'legislative_body', '.', 'name', ')', '\\n']
Tokenized   (012): ['<s>', 'plan', '.', 'legislative', '_', 'body', '.', 'name', ')', '\\', 'n', '</s>']
Filtered   (010): ['plan', '.', 'legislative', '_', 'body', '.', 'name', ')', '\\', 'n']
Detokenized (007): ['plan', '.', 'legislative_body', '.', 'name', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "functions = map ( lambda x : int ( x ) , functions ) \n"
Original    (015): ['functions', '=', 'map', '(', 'lambda', 'x', ':', 'int', '(', 'x', ')', ',', 'functions', ')', '\\n']
Tokenized   (019): ['<s>', 'fun', 'ctions', '=', 'map', '(', 'lambda', 'x', ':', 'int', '(', 'x', ')', ',', 'functions', ')', '\\', 'n', '</s>']
Filtered   (017): ['fun', 'ctions', '=', 'map', '(', 'lambda', 'x', ':', 'int', '(', 'x', ')', ',', 'functions', ')', '\\', 'n']
Detokenized (015): ['functions', '=', 'map', '(', 'lambda', 'x', ':', 'int', '(', 'x', ')', ',', 'functions', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "display = display . copy_from ( display = demo , title = request . POST . get ( ) , owner = result [ ] = True \n"
Original    (028): ['display', '=', 'display', '.', 'copy_from', '(', 'display', '=', 'demo', ',', 'title', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ',', 'owner', '=', 'result', '[', ']', '=', 'True', '\\n']
Tokenized   (033): ['<s>', 'display', '=', 'display', '.', 'copy', '_', 'from', '(', 'display', '=', 'demo', ',', 'title', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ',', 'owner', '=', 'result', '[', ']', '=', 'True', '\\', 'n', '</s>']
Filtered   (031): ['display', '=', 'display', '.', 'copy', '_', 'from', '(', 'display', '=', 'demo', ',', 'title', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ',', 'owner', '=', 'result', '[', ']', '=', 'True', '\\', 'n']
Detokenized (028): ['display', '=', 'display', '.', 'copy_from', '(', 'display', '=', 'demo', ',', 'title', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ',', 'owner', '=', 'result', '[', ']', '=', 'True', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 28, 768)
# Extracted words:  28
Sentence         : "version = min ( plan . version , int ( version ) ) \n"
Original    (014): ['version', '=', 'min', '(', 'plan', '.', 'version', ',', 'int', '(', 'version', ')', ')', '\\n']
Tokenized   (017): ['<s>', 'version', '=', 'min', '(', 'plan', '.', 'version', ',', 'int', '(', 'version', ')', ')', '\\', 'n', '</s>']
Filtered   (015): ['version', '=', 'min', '(', 'plan', '.', 'version', ',', 'int', '(', 'version', ')', ')', '\\', 'n']
Detokenized (014): ['version', '=', 'min', '(', 'plan', '.', 'version', ',', 'int', '(', 'version', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "Comment . objects . filter ( object_pk = district . id , content_type = ct ) . delete ( ) \n"
Original    (021): ['Comment', '.', 'objects', '.', 'filter', '(', 'object_pk', '=', 'district', '.', 'id', ',', 'content_type', '=', 'ct', ')', '.', 'delete', '(', ')', '\\n']
Tokenized   (030): ['<s>', 'Comment', '.', 'objects', '.', 'filter', '(', 'object', '_', 'p', 'k', '=', 'district', '.', 'id', ',', 'content', '_', 'type', '=', 'c', 't', ')', '.', 'delete', '(', ')', '\\', 'n', '</s>']
Filtered   (028): ['Comment', '.', 'objects', '.', 'filter', '(', 'object', '_', 'p', 'k', '=', 'district', '.', 'id', ',', 'content', '_', 'type', '=', 'c', 't', ')', '.', 'delete', '(', ')', '\\', 'n']
Detokenized (021): ['Comment', '.', 'objects', '.', 'filter', '(', 'object_pk', '=', 'district', '.', 'id', ',', 'content_type', '=', 'ct', ')', '.', 'delete', '(', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "TaggedItem . objects . filter ( tag__in = tset , object_id = district . id ) . delete ( ) \n"
Original    (021): ['TaggedItem', '.', 'objects', '.', 'filter', '(', 'tag__in', '=', 'tset', ',', 'object_id', '=', 'district', '.', 'id', ')', '.', 'delete', '(', ')', '\\n']
Tokenized   (031): ['<s>', 'T', 'agged', 'Item', '.', 'objects', '.', 'filter', '(', 'tag', '__', 'in', '=', 't', 'set', ',', 'object', '_', 'id', '=', 'district', '.', 'id', ')', '.', 'delete', '(', ')', '\\', 'n', '</s>']
Filtered   (029): ['T', 'agged', 'Item', '.', 'objects', '.', 'filter', '(', 'tag', '__', 'in', '=', 't', 'set', ',', 'object', '_', 'id', '=', 'district', '.', 'id', ')', '.', 'delete', '(', ')', '\\', 'n']
Detokenized (021): ['TaggedItem', '.', 'objects', '.', 'filter', '(', 'tag__in', '=', 'tset', ',', 'object_id', '=', 'district', '.', 'id', ')', '.', 'delete', '(', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "geolevel = plans [ 0 ] . legislative_body . get_geolevels ( ) [ 0 ] \n"
Original    (016): ['geolevel', '=', 'plans', '[', '0', ']', '.', 'legislative_body', '.', 'get_geolevels', '(', ')', '[', '0', ']', '\\n']
Tokenized   (028): ['<s>', 'ge', 'ole', 'vel', '=', 'plans', '[', '0', ']', '.', 'legislative', '_', 'body', '.', 'get', '_', 'ge', 'ole', 'vel', 's', '(', ')', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (026): ['ge', 'ole', 'vel', '=', 'plans', '[', '0', ']', '.', 'legislative', '_', 'body', '.', 'get', '_', 'ge', 'ole', 'vel', 's', '(', ')', '[', '0', ']', '\\', 'n']
Detokenized (016): ['geolevel', '=', 'plans', '[', '0', ']', '.', 'legislative_body', '.', 'get_geolevels', '(', ')', '[', '0', ']', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "plans = Plan . objects . filter ( is_shared = True ) . order_by ( ) [ 0 : 10 ] \n"
Original    (022): ['plans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'is_shared', '=', 'True', ')', '.', 'order_by', '(', ')', '[', '0', ':', '10', ']', '\\n']
Tokenized   (030): ['<s>', 'pl', 'ans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'is', '_', 'shared', '=', 'True', ')', '.', 'order', '_', 'by', '(', ')', '[', '0', ':', '10', ']', '\\', 'n', '</s>']
Filtered   (028): ['pl', 'ans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'is', '_', 'shared', '=', 'True', ')', '.', 'order', '_', 'by', '(', ')', '[', '0', ':', '10', ']', '\\', 'n']
Detokenized (022): ['plans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'is_shared', '=', 'True', ')', '.', 'order_by', '(', ')', '[', '0', ':', '10', ']', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "write_page = 0 if write_page == 1 else write_page + 1 \n"
Original    (012): ['write_page', '=', '0', 'if', 'write_page', '==', '1', 'else', 'write_page', '+', '1', '\\n']
Tokenized   (021): ['<s>', 'write', '_', 'page', '=', '0', 'if', 'write', '_', 'page', '==', '1', 'else', 'write', '_', 'page', '+', '1', '\\', 'n', '</s>']
Filtered   (019): ['write', '_', 'page', '=', '0', 'if', 'write', '_', 'page', '==', '1', 'else', 'write', '_', 'page', '+', '1', '\\', 'n']
Detokenized (012): ['write_page', '=', '0', 'if', 'write_page', '==', '1', 'else', 'write_page', '+', '1', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "init_sum = 1 if i == 0 else 0 \n"
Original    (010): ['init_sum', '=', '1', 'if', 'i', '==', '0', 'else', '0', '\\n']
Tokenized   (015): ['<s>', 'init', '_', 'sum', '=', '1', 'if', 'i', '==', '0', 'else', '0', '\\', 'n', '</s>']
Filtered   (013): ['init', '_', 'sum', '=', '1', 'if', 'i', '==', '0', 'else', '0', '\\', 'n']
Detokenized (010): ['init_sum', '=', '1', 'if', 'i', '==', '0', 'else', '0', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "mem_d0 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n"
Original    (013): ['mem_d0', '.', 'read_nonblocking', '(', '1', ',', 'write_addr', ',', 'mesh_size', '-', '2', ')', '\\n']
Tokenized   (026): ['<s>', 'mem', '_', 'd', '0', '.', 'read', '_', 'non', 'blocking', '(', '1', ',', 'write', '_', 'addr', ',', 'mesh', '_', 'size', '-', '2', ')', '\\', 'n', '</s>']
Filtered   (024): ['mem', '_', 'd', '0', '.', 'read', '_', 'non', 'blocking', '(', '1', ',', 'write', '_', 'addr', ',', 'mesh', '_', 'size', '-', '2', ')', '\\', 'n']
Detokenized (013): ['mem_d0', '.', 'read_nonblocking', '(', '1', ',', 'write_addr', ',', 'mesh_size', '-', '2', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "write_addr += mesh_size * DSIZE \n"
Original    (006): ['write_addr', '+=', 'mesh_size', '*', 'DSIZE', '\\n']
Tokenized   (014): ['<s>', 'write', '_', 'addr', '+=', 'mesh', '_', 'size', '*', 'D', 'SIZE', '\\', 'n', '</s>']
Filtered   (012): ['write', '_', 'addr', '+=', 'mesh', '_', 'size', '*', 'D', 'SIZE', '\\', 'n']
Detokenized (006): ['write_addr', '+=', 'mesh_size', '*', 'DSIZE', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "sub_id_base = ( 10 if sub_id_m . group ( 1 ) . count ( "\'d" ) > 0 else \n"
Original    (020): ['sub_id_base', '=', '(', '10', 'if', 'sub_id_m', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\\'d"', ')', '>', '0', 'else', '\\n']
Tokenized   (034): ['<s>', 'sub', '_', 'id', '_', 'base', '=', '(', '10', 'if', 'sub', '_', 'id', '_', 'm', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\', "'", 'd', '"', ')', '>', '0', 'else', '\\', 'n', '</s>']
Filtered   (032): ['sub', '_', 'id', '_', 'base', '=', '(', '10', 'if', 'sub', '_', 'id', '_', 'm', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\', "'", 'd', '"', ')', '>', '0', 'else', '\\', 'n']
Detokenized (020): ['sub_id_base', '=', '(', '10', 'if', 'sub_id_m', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\\'d"', ')', '>', '0', 'else', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "16 if sub_id_m . group ( 1 ) . count ( "\'h" ) > 0 else \n"
Original    (017): ['16', 'if', 'sub_id_m', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\\'h"', ')', '>', '0', 'else', '\\n']
Tokenized   (027): ['<s>', '16', 'if', 'sub', '_', 'id', '_', 'm', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\', "'", 'h', '"', ')', '>', '0', 'else', '\\', 'n', '</s>']
Filtered   (025): ['16', 'if', 'sub', '_', 'id', '_', 'm', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\', "'", 'h', '"', ')', '>', '0', 'else', '\\', 'n']
Detokenized (017): ['16', 'if', 'sub_id_m', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\\'h"', ')', '>', '0', 'else', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "10 ) \n"
Original    (003): ['10', ')', '\\n']
Tokenized   (006): ['<s>', '10', ')', '\\', 'n', '</s>']
Filtered   (004): ['10', ')', '\\', 'n']
Detokenized (003): ['10', ')', '\\n']
Counter: 4
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "optparser . add_option ( "--noreorder" , action = "store_true" , dest = "noreorder" , \n"
Original    (015): ['optparser', '.', 'add_option', '(', '"--noreorder"', ',', 'action', '=', '"store_true"', ',', 'dest', '=', '"noreorder"', ',', '\\n']
Tokenized   (034): ['<s>', 'opt', 'parser', '.', 'add', '_', 'option', '(', '"', '--', 'n', 'ore', 'order', '"', ',', 'action', '=', '"', 'store', '_', 'true', '"', ',', 'dest', '=', '"', 'n', 'ore', 'order', '"', ',', '\\', 'n', '</s>']
Filtered   (032): ['opt', 'parser', '.', 'add', '_', 'option', '(', '"', '--', 'n', 'ore', 'order', '"', ',', 'action', '=', '"', 'store', '_', 'true', '"', ',', 'dest', '=', '"', 'n', 'ore', 'order', '"', ',', '\\', 'n']
Detokenized (015): ['optparser', '.', 'add_option', '(', '"--noreorder"', ',', 'action', '=', '"store_true"', ',', 'dest', '=', '"noreorder"', ',', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "truenode = replaceUndefined ( tree . truenode , termname ) \n"
Original    (011): ['truenode', '=', 'replaceUndefined', '(', 'tree', '.', 'truenode', ',', 'termname', ')', '\\n']
Tokenized   (022): ['<s>', 't', 'ru', 'en', 'ode', '=', 'replace', 'Und', 'efined', '(', 'tree', '.', 'tru', 'en', 'ode', ',', 'term', 'name', ')', '\\', 'n', '</s>']
Filtered   (020): ['t', 'ru', 'en', 'ode', '=', 'replace', 'Und', 'efined', '(', 'tree', '.', 'tru', 'en', 'ode', ',', 'term', 'name', ')', '\\', 'n']
Detokenized (011): ['truenode', '=', 'replaceUndefined', '(', 'tree', '.', 'truenode', ',', 'termname', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "codedir = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) + \n"
Original    (033): ['codedir', '=', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'abspath', '(', '__file__', ')', ')', ')', ')', '+', '\\n']
Tokenized   (043): ['<s>', 'coded', 'ir', '=', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'abs', 'path', '(', '__', 'file', '__', ')', ')', ')', ')', '+', '\\', 'n', '</s>']
Filtered   (041): ['coded', 'ir', '=', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'abs', 'path', '(', '__', 'file', '__', ')', ')', ')', ')', '+', '\\', 'n']
Detokenized (033): ['codedir', '=', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'abspath', '(', '__file__', ')', ')', ')', ')', '+', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 33, 768)
# Extracted words:  33
Sentence         : "analyzer = VerilogDataflowAnalyzer ( filelist , topmodule , \n"
Original    (009): ['analyzer', '=', 'VerilogDataflowAnalyzer', '(', 'filelist', ',', 'topmodule', ',', '\\n']
Tokenized   (021): ['<s>', 'analy', 'zer', '=', 'Ver', 'il', 'og', 'Data', 'flow', 'Analy', 'zer', '(', 'file', 'list', ',', 'top', 'module', ',', '\\', 'n', '</s>']
Filtered   (019): ['analy', 'zer', '=', 'Ver', 'il', 'og', 'Data', 'flow', 'Analy', 'zer', '(', 'file', 'list', ',', 'top', 'module', ',', '\\', 'n']
Detokenized (009): ['analyzer', '=', 'VerilogDataflowAnalyzer', '(', 'filelist', ',', 'topmodule', ',', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "constlist = optimizer . getConstlist ( ) \n"
Original    (008): ['constlist', '=', 'optimizer', '.', 'getConstlist', '(', ')', '\\n']
Tokenized   (015): ['<s>', 'const', 'list', '=', 'optim', 'izer', '.', 'get', 'Const', 'list', '(', ')', '\\', 'n', '</s>']
Filtered   (013): ['const', 'list', '=', 'optim', 'izer', '.', 'get', 'Const', 'list', '(', ')', '\\', 'n']
Detokenized (008): ['constlist', '=', 'optimizer', '.', 'getConstlist', '(', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "interval = m . Parameter ( , 16 ) \n"
Original    (010): ['interval', '=', 'm', '.', 'Parameter', '(', ',', '16', ')', '\\n']
Tokenized   (015): ['<s>', 'inter', 'val', '=', 'm', '.', 'Param', 'eter', '(', ',', '16', ')', '\\', 'n', '</s>']
Filtered   (013): ['inter', 'val', '=', 'm', '.', 'Param', 'eter', '(', ',', '16', ')', '\\', 'n']
Detokenized (010): ['interval', '=', 'm', '.', 'Parameter', '(', ',', '16', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "led ( led + 1 ) , \n"
Original    (008): ['led', '(', 'led', '+', '1', ')', ',', '\\n']
Tokenized   (011): ['<s>', 'led', '(', 'led', '+', '1', ')', ',', '\\', 'n', '</s>']
Filtered   (009): ['led', '(', 'led', '+', '1', ')', ',', '\\', 'n']
Detokenized (008): ['led', '(', 'led', '+', '1', ')', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "SingleStatement ( SystemTask ( , , led ) ) \n"
Original    (010): ['SingleStatement', '(', 'SystemTask', '(', ',', ',', 'led', ')', ')', '\\n']
Tokenized   (015): ['<s>', 'Single', 'Statement', '(', 'System', 'Task', '(', ',', ',', 'led', ')', ')', '\\', 'n', '</s>']
Filtered   (013): ['Single', 'Statement', '(', 'System', 'Task', '(', ',', ',', 'led', ')', ')', '\\', 'n']
Detokenized (010): ['SingleStatement', '(', 'SystemTask', '(', ',', ',', 'led', ')', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "y = dataflow . Variable ( , valid = , ready = , point = 4 ) \n"
Original    (018): ['y', '=', 'dataflow', '.', 'Variable', '(', ',', 'valid', '=', ',', 'ready', '=', ',', 'point', '=', '4', ')', '\\n']
Tokenized   (022): ['<s>', 'y', '=', 'data', 'flow', '.', 'Variable', '(', ',', 'valid', '=', ',', 'ready', '=', ',', 'point', '=', '4', ')', '\\', 'n', '</s>']
Filtered   (020): ['y', '=', 'data', 'flow', '.', 'Variable', '(', ',', 'valid', '=', ',', 'ready', '=', ',', 'point', '=', '4', ')', '\\', 'n']
Detokenized (018): ['y', '=', 'dataflow', '.', 'Variable', '(', ',', 'valid', '=', ',', 'ready', '=', ',', 'point', '=', '4', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "z . output ( , valid = , ready = ) \n"
Original    (012): ['z', '.', 'output', '(', ',', 'valid', '=', ',', 'ready', '=', ')', '\\n']
Tokenized   (015): ['<s>', 'z', '.', 'output', '(', ',', 'valid', '=', ',', 'ready', '=', ')', '\\', 'n', '</s>']
Filtered   (013): ['z', '.', 'output', '(', ',', 'valid', '=', ',', 'ready', '=', ')', '\\', 'n']
Detokenized (012): ['z', '.', 'output', '(', ',', 'valid', '=', ',', 'ready', '=', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "xdata_orig = m . RegLike ( ports [ ] , name = , initval = 0 ) \n"
Original    (018): ['xdata_orig', '=', 'm', '.', 'RegLike', '(', 'ports', '[', ']', ',', 'name', '=', ',', 'initval', '=', '0', ')', '\\n']
Tokenized   (026): ['<s>', 'x', 'data', '_', 'orig', '=', 'm', '.', 'Reg', 'Like', '(', 'ports', '[', ']', ',', 'name', '=', ',', 'init', 'val', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (024): ['x', 'data', '_', 'orig', '=', 'm', '.', 'Reg', 'Like', '(', 'ports', '[', ']', ',', 'name', '=', ',', 'init', 'val', '=', '0', ')', '\\', 'n']
Detokenized (018): ['xdata_orig', '=', 'm', '.', 'RegLike', '(', 'ports', '[', ']', ',', 'name', '=', ',', 'initval', '=', '0', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "zdata_orig = m . WireLike ( ports [ ] , name = ) \n"
Original    (014): ['zdata_orig', '=', 'm', '.', 'WireLike', '(', 'ports', '[', ']', ',', 'name', '=', ')', '\\n']
Tokenized   (021): ['<s>', 'z', 'data', '_', 'orig', '=', 'm', '.', 'Wire', 'Like', '(', 'ports', '[', ']', ',', 'name', '=', ')', '\\', 'n', '</s>']
Filtered   (019): ['z', 'data', '_', 'orig', '=', 'm', '.', 'Wire', 'Like', '(', 'ports', '[', ']', ',', 'name', '=', ')', '\\', 'n']
Detokenized (014): ['zdata_orig', '=', 'm', '.', 'WireLike', '(', 'ports', '[', ']', ',', 'name', '=', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "params = m . connect_params ( main ) , \n"
Original    (010): ['params', '=', 'm', '.', 'connect_params', '(', 'main', ')', ',', '\\n']
Tokenized   (015): ['<s>', 'params', '=', 'm', '.', 'connect', '_', 'params', '(', 'main', ')', ',', '\\', 'n', '</s>']
Filtered   (013): ['params', '=', 'm', '.', 'connect', '_', 'params', '(', 'main', ')', ',', '\\', 'n']
Detokenized (010): ['params', '=', 'm', '.', 'connect_params', '(', 'main', ')', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "reset_stmt . append ( ydata_orig ( 0 ) ) \n"
Original    (010): ['reset_stmt', '.', 'append', '(', 'ydata_orig', '(', '0', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'reset', '_', 'st', 'mt', '.', 'append', '(', 'y', 'data', '_', 'orig', '(', '0', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['reset', '_', 'st', 'mt', '.', 'append', '(', 'y', 'data', '_', 'orig', '(', '0', ')', ')', '\\', 'n']
Detokenized (010): ['reset_stmt', '.', 'append', '(', 'ydata_orig', '(', '0', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "nclk ( clk ) , \n"
Original    (006): ['nclk', '(', 'clk', ')', ',', '\\n']
Tokenized   (012): ['<s>', 'n', 'cl', 'k', '(', 'cl', 'k', ')', ',', '\\', 'n', '</s>']
Filtered   (010): ['n', 'cl', 'k', '(', 'cl', 'k', ')', ',', '\\', 'n']
Detokenized (006): ['nclk', '(', 'clk', ')', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "send ( , ydata_orig , yvalid , yready , step = 1 , waitnum = 20 ) \n"
Original    (018): ['send', '(', ',', 'ydata_orig', ',', 'yvalid', ',', 'yready', ',', 'step', '=', '1', ',', 'waitnum', '=', '20', ')', '\\n']
Tokenized   (027): ['<s>', 'send', '(', ',', 'y', 'data', '_', 'orig', ',', 'y', 'valid', ',', 'y', 'ready', ',', 'step', '=', '1', ',', 'wait', 'num', '=', '20', ')', '\\', 'n', '</s>']
Filtered   (025): ['send', '(', ',', 'y', 'data', '_', 'orig', ',', 'y', 'valid', ',', 'y', 'ready', ',', 'step', '=', '1', ',', 'wait', 'num', '=', '20', ')', '\\', 'n']
Detokenized (018): ['send', '(', ',', 'ydata_orig', ',', 'yvalid', ',', 'yready', ',', 'step', '=', '1', ',', 'waitnum', '=', '20', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "receive ( , zdata , zvalid , zready , waitnum = 50 ) \n"
Original    (014): ['receive', '(', ',', 'zdata', ',', 'zvalid', ',', 'zready', ',', 'waitnum', '=', '50', ')', '\\n']
Tokenized   (022): ['<s>', 're', 'ceive', '(', ',', 'z', 'data', ',', 'z', 'valid', ',', 'z', 'ready', ',', 'wait', 'num', '=', '50', ')', '\\', 'n', '</s>']
Filtered   (020): ['re', 'ceive', '(', ',', 'z', 'data', ',', 'z', 'valid', ',', 'z', 'ready', ',', 'wait', 'num', '=', '50', ')', '\\', 'n']
Detokenized (014): ['receive', '(', ',', 'zdata', ',', 'zvalid', ',', 'zready', ',', 'waitnum', '=', '50', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "If ( AndList ( zvalid , zready ) ) ( \n"
Original    (011): ['If', '(', 'AndList', '(', 'zvalid', ',', 'zready', ')', ')', '(', '\\n']
Tokenized   (017): ['<s>', 'If', '(', 'And', 'List', '(', 'z', 'valid', ',', 'z', 'ready', ')', ')', '(', '\\', 'n', '</s>']
Filtered   (015): ['If', '(', 'And', 'List', '(', 'z', 'valid', ',', 'z', 'ready', ')', ')', '(', '\\', 'n']
Detokenized (011): ['If', '(', 'AndList', '(', 'zvalid', ',', 'zready', ')', ')', '(', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "Systask ( , , zdata_orig ) \n"
Original    (007): ['Systask', '(', ',', ',', 'zdata_orig', ')', '\\n']
Tokenized   (015): ['<s>', 'Sy', 'st', 'ask', '(', ',', ',', 'z', 'data', '_', 'orig', ')', '\\', 'n', '</s>']
Filtered   (013): ['Sy', 'st', 'ask', '(', ',', ',', 'z', 'data', '_', 'orig', ')', '\\', 'n']
Detokenized (007): ['Systask', '(', ',', ',', 'zdata_orig', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "count = m . Reg ( , width = 32 , initval = 0 ) \n"
Original    (016): ['count', '=', 'm', '.', 'Reg', '(', ',', 'width', '=', '32', ',', 'initval', '=', '0', ')', '\\n']
Tokenized   (020): ['<s>', 'count', '=', 'm', '.', 'Reg', '(', ',', 'width', '=', '32', ',', 'init', 'val', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (018): ['count', '=', 'm', '.', 'Reg', '(', ',', 'width', '=', '32', ',', 'init', 'val', '=', '0', ')', '\\', 'n']
Detokenized (016): ['count', '=', 'm', '.', 'Reg', '(', ',', 'width', '=', '32', ',', 'initval', '=', '0', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "fsm . add ( valid ( down ) , cond = c , delay = 4 , eager_val = True , lazy_cond = True ) \n"
Original    (026): ['fsm', '.', 'add', '(', 'valid', '(', 'down', ')', ',', 'cond', '=', 'c', ',', 'delay', '=', '4', ',', 'eager_val', '=', 'True', ',', 'lazy_cond', '=', 'True', ')', '\\n']
Tokenized   (034): ['<s>', 'f', 'sm', '.', 'add', '(', 'valid', '(', 'down', ')', ',', 'cond', '=', 'c', ',', 'delay', '=', '4', ',', 'eager', '_', 'val', '=', 'True', ',', 'lazy', '_', 'cond', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (032): ['f', 'sm', '.', 'add', '(', 'valid', '(', 'down', ')', ',', 'cond', '=', 'c', ',', 'delay', '=', '4', ',', 'eager', '_', 'val', '=', 'True', ',', 'lazy', '_', 'cond', '=', 'True', ')', '\\', 'n']
Detokenized (026): ['fsm', '.', 'add', '(', 'valid', '(', 'down', ')', ',', 'cond', '=', 'c', ',', 'delay', '=', '4', ',', 'eager_val', '=', 'True', ',', 'lazy_cond', '=', 'True', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "uut = m . Instance ( mkLed ( ) , , \n"
Original    (012): ['uut', '=', 'm', '.', 'Instance', '(', 'mkLed', '(', ')', ',', ',', '\\n']
Tokenized   (018): ['<s>', 'u', 'ut', '=', 'm', '.', 'Inst', 'ance', '(', 'mk', 'Led', '(', ')', ',', ',', '\\', 'n', '</s>']
Filtered   (016): ['u', 'ut', '=', 'm', '.', 'Inst', 'ance', '(', 'mk', 'Led', '(', ')', ',', ',', '\\', 'n']
Detokenized (012): ['uut', '=', 'm', '.', 'Instance', '(', 'mkLed', '(', ')', ',', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "rslt = m . Wire ( , retwidth , signed = True ) \n"
Original    (014): ['rslt', '=', 'm', '.', 'Wire', '(', ',', 'retwidth', ',', 'signed', '=', 'True', ')', '\\n']
Tokenized   (019): ['<s>', 'rs', 'lt', '=', 'm', '.', 'Wire', '(', ',', 'ret', 'width', ',', 'signed', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (017): ['rs', 'lt', '=', 'm', '.', 'Wire', '(', ',', 'ret', 'width', ',', 'signed', '=', 'True', ')', '\\', 'n']
Detokenized (014): ['rslt', '=', 'm', '.', 'Wire', '(', ',', 'retwidth', ',', 'signed', '=', 'True', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "tmpval [ 0 ] ( rslt ) , \n"
Original    (009): ['tmpval', '[', '0', ']', '(', 'rslt', ')', ',', '\\n']
Tokenized   (014): ['<s>', 'tmp', 'val', '[', '0', ']', '(', 'rs', 'lt', ')', ',', '\\', 'n', '</s>']
Filtered   (012): ['tmp', 'val', '[', '0', ']', '(', 'rs', 'lt', ')', ',', '\\', 'n']
Detokenized (009): ['tmpval', '[', '0', ']', '(', 'rslt', ')', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "vtypes . If ( rst ) ( \n"
Original    (008): ['vtypes', '.', 'If', '(', 'rst', ')', '(', '\\n']
Tokenized   (013): ['<s>', 'v', 'types', '.', 'If', '(', 'r', 'st', ')', '(', '\\', 'n', '</s>']
Filtered   (011): ['v', 'types', '.', 'If', '(', 'r', 'st', ')', '(', '\\', 'n']
Detokenized (008): ['vtypes', '.', 'If', '(', 'rst', ')', '(', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "ports = [ ( , clk ) , ( , update ) , ( , a ) , ( , b ) , ( , c ) ] \n"
Original    (029): ['ports', '=', '[', '(', ',', 'clk', ')', ',', '(', ',', 'update', ')', ',', '(', ',', 'a', ')', ',', '(', ',', 'b', ')', ',', '(', ',', 'c', ')', ']', '\\n']
Tokenized   (033): ['<s>', 'ports', '=', '[', '(', ',', 'cl', 'k', ')', ',', '(', ',', 'update', ')', ',', '(', ',', 'a', ')', ',', '(', ',', 'b', ')', ',', '(', ',', 'c', ')', ']', '\\', 'n', '</s>']
Filtered   (031): ['ports', '=', '[', '(', ',', 'cl', 'k', ')', ',', '(', ',', 'update', ')', ',', '(', ',', 'a', ')', ',', '(', ',', 'b', ')', ',', '(', ',', 'c', ')', ']', '\\', 'n']
Detokenized (029): ['ports', '=', '[', '(', ',', 'clk', ')', ',', '(', ',', 'update', ')', ',', '(', ',', 'a', ')', ',', '(', ',', 'b', ')', ',', '(', ',', 'c', ')', ']', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "m . Instance ( mult , , ports = ports ) \n"
Original    (012): ['m', '.', 'Instance', '(', 'mult', ',', ',', 'ports', '=', 'ports', ')', '\\n']
Tokenized   (016): ['<s>', 'm', '.', 'Inst', 'ance', '(', 'mult', ',', ',', 'ports', '=', 'ports', ')', '\\', 'n', '</s>']
Filtered   (014): ['m', '.', 'Inst', 'ance', '(', 'mult', ',', ',', 'ports', '=', 'ports', ')', '\\', 'n']
Detokenized (012): ['m', '.', 'Instance', '(', 'mult', ',', ',', 'ports', '=', 'ports', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "stdout = subprocess . PIPE ) . stdout \n"
Original    (009): ['stdout', '=', 'subprocess', '.', 'PIPE', ')', '.', 'stdout', '\\n']
Tokenized   (017): ['<s>', 'std', 'out', '=', 'sub', 'process', '.', 'P', 'IP', 'E', ')', '.', 'std', 'out', '\\', 'n', '</s>']
Filtered   (015): ['std', 'out', '=', 'sub', 'process', '.', 'P', 'IP', 'E', ')', '.', 'std', 'out', '\\', 'n']
Detokenized (009): ['stdout', '=', 'subprocess', '.', 'PIPE', ')', '.', 'stdout', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "line = [ l for l in sout ] [ 0 ] \n"
Original    (013): ['line', '=', '[', 'l', 'for', 'l', 'in', 'sout', ']', '[', '0', ']', '\\n']
Tokenized   (017): ['<s>', 'line', '=', '[', 'l', 'for', 'l', 'in', 's', 'out', ']', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (015): ['line', '=', '[', 'l', 'for', 'l', 'in', 's', 'out', ']', '[', '0', ']', '\\', 'n']
Detokenized (013): ['line', '=', '[', 'l', 'for', 'l', 'in', 'sout', ']', '[', '0', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "round ( wmean , prec ) , "+-" , round ( wstd , prec ) ) \n"
Original    (017): ['round', '(', 'wmean', ',', 'prec', ')', ',', '"+-"', ',', 'round', '(', 'wstd', ',', 'prec', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'round', '(', 'w', 'mean', ',', 'prec', ')', ',', '"+', '-"', ',', 'round', '(', 'w', 'std', ',', 'prec', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['round', '(', 'w', 'mean', ',', 'prec', ')', ',', '"+', '-"', ',', 'round', '(', 'w', 'std', ',', 'prec', ')', ')', '\\', 'n']
Detokenized (017): ['round', '(', 'wmean', ',', 'prec', ')', ',', '"+-"', ',', 'round', '(', 'wstd', ',', 'prec', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "size = stop - start ) \n"
Original    (007): ['size', '=', 'stop', '-', 'start', ')', '\\n']
Tokenized   (010): ['<s>', 'size', '=', 'stop', '-', 'start', ')', '\\', 'n', '</s>']
Filtered   (008): ['size', '=', 'stop', '-', 'start', ')', '\\', 'n']
Detokenized (007): ['size', '=', 'stop', '-', 'start', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "rndbase = numpy . random . randint ( self . nrows , size = niter ) \n"
Original    (017): ['rndbase', '=', 'numpy', '.', 'random', '.', 'randint', '(', 'self', '.', 'nrows', ',', 'size', '=', 'niter', ')', '\\n']
Tokenized   (026): ['<s>', 'r', 'nd', 'base', '=', 'n', 'umpy', '.', 'random', '.', 'rand', 'int', '(', 'self', '.', 'n', 'rows', ',', 'size', '=', 'n', 'iter', ')', '\\', 'n', '</s>']
Filtered   (024): ['r', 'nd', 'base', '=', 'n', 'umpy', '.', 'random', '.', 'rand', 'int', '(', 'self', '.', 'n', 'rows', ',', 'size', '=', 'n', 'iter', ')', '\\', 'n']
Detokenized (017): ['rndbase', '=', 'numpy', '.', 'random', '.', 'randint', '(', 'self', '.', 'nrows', ',', 'size', '=', 'niter', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "rng = [ - 1000 , - 1000 ] \n"
Original    (010): ['rng', '=', '[', '-', '1000', ',', '-', '1000', ']', '\\n']
Tokenized   (014): ['<s>', 'r', 'ng', '=', '[', '-', '1000', ',', '-', '1000', ']', '\\', 'n', '</s>']
Filtered   (012): ['r', 'ng', '=', '[', '-', '1000', ',', '-', '1000', ']', '\\', 'n']
Detokenized (010): ['rng', '=', '[', '-', '1000', ',', '-', '1000', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "benchtime , stones = prof . run ( \n"
Original    (009): ['benchtime', ',', 'stones', '=', 'prof', '.', 'run', '(', '\\n']
Tokenized   (013): ['<s>', 'bench', 'time', ',', 'stones', '=', 'prof', '.', 'run', '(', '\\', 'n', '</s>']
Filtered   (011): ['bench', 'time', ',', 'stones', '=', 'prof', '.', 'run', '(', '\\', 'n']
Detokenized (009): ['benchtime', ',', 'stones', '=', 'prof', '.', 'run', '(', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "db . rng = [ - rng / 2 , rng / 2 ] \n"
Original    (015): ['db', '.', 'rng', '=', '[', '-', 'rng', '/', '2', ',', 'rng', '/', '2', ']', '\\n']
Tokenized   (021): ['<s>', 'db', '.', 'r', 'ng', '=', '[', '-', 'r', 'ng', '/', '2', ',', 'r', 'ng', '/', '2', ']', '\\', 'n', '</s>']
Filtered   (019): ['db', '.', 'r', 'ng', '=', '[', '-', 'r', 'ng', '/', '2', ',', 'r', 'ng', '/', '2', ']', '\\', 'n']
Detokenized (015): ['db', '.', 'rng', '=', '[', '-', 'rng', '/', '2', ',', 'rng', '/', '2', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "MB_ = 1024 * KB_ \n"
Original    (006): ['MB_', '=', '1024', '*', 'KB_', '\\n']
Tokenized   (011): ['<s>', 'MB', '_', '=', '1024', '*', 'KB', '_', '\\', 'n', '</s>']
Filtered   (009): ['MB', '_', '=', '1024', '*', 'KB', '_', '\\', 'n']
Detokenized (006): ['MB_', '=', '1024', '*', 'KB_', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "markers = [ , , , , , , , , , ] \n"
Original    (014): ['markers', '=', '[', ',', ',', ',', ',', ',', ',', ',', ',', ',', ']', '\\n']
Tokenized   (018): ['<s>', 'mark', 'ers', '=', '[', ',', ',', ',', ',', ',', ',', ',', ',', ',', ']', '\\', 'n', '</s>']
Filtered   (016): ['mark', 'ers', '=', '[', ',', ',', ',', ',', ',', ',', ',', ',', ',', ']', '\\', 'n']
Detokenized (014): ['markers', '=', '[', ',', ',', ',', ',', ',', ',', ',', ',', ',', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "memcpyw = float ( tmp . split ( ) [ 1 ] ) \n"
Original    (014): ['memcpyw', '=', 'float', '(', 'tmp', '.', 'split', '(', ')', '[', '1', ']', ')', '\\n']
Tokenized   (020): ['<s>', 'mem', 'c', 'py', 'w', '=', 'float', '(', 'tmp', '.', 'split', '(', ')', '[', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (018): ['mem', 'c', 'py', 'w', '=', 'float', '(', 'tmp', '.', 'split', '(', ')', '[', '1', ']', ')', '\\', 'n']
Detokenized (014): ['memcpyw', '=', 'float', '(', 'tmp', '.', 'split', '(', ')', '[', '1', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "values [ "memcpyr" ] . append ( memcpyr ) \n"
Original    (010): ['values', '[', '"memcpyr"', ']', '.', 'append', '(', 'memcpyr', ')', '\\n']
Tokenized   (019): ['<s>', 'values', '[', '"', 'mem', 'cp', 'yr', '"', ']', '.', 'append', '(', 'mem', 'cp', 'yr', ')', '\\', 'n', '</s>']
Filtered   (017): ['values', '[', '"', 'mem', 'cp', 'yr', '"', ']', '.', 'append', '(', 'mem', 'cp', 'yr', ')', '\\', 'n']
Detokenized (010): ['values', '[', '"memcpyr"', ']', '.', 'append', '(', 'memcpyr', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "ratio = float ( line . split ( ) [ - 1 ] ) \n"
Original    (015): ['ratio', '=', 'float', '(', 'line', '.', 'split', '(', ')', '[', '-', '1', ']', ')', '\\n']
Tokenized   (019): ['<s>', 'rat', 'io', '=', 'float', '(', 'line', '.', 'split', '(', ')', '[', '-', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (017): ['rat', 'io', '=', 'float', '(', 'line', '.', 'split', '(', ')', '[', '-', '1', ']', ')', '\\', 'n']
Detokenized (015): ['ratio', '=', 'float', '(', 'line', '.', 'split', '(', ')', '[', '-', '1', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "xlim ( 0 , xmax ) \n"
Original    (007): ['xlim', '(', '0', ',', 'xmax', ')', '\\n']
Tokenized   (012): ['<s>', 'x', 'lim', '(', '0', ',', 'x', 'max', ')', '\\', 'n', '</s>']
Filtered   (010): ['x', 'lim', '(', '0', ',', 'x', 'max', ')', '\\', 'n']
Detokenized (007): ['xlim', '(', '0', ',', 'xmax', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "ylim ( 0 , None ) \n"
Original    (007): ['ylim', '(', '0', ',', 'None', ')', '\\n']
Tokenized   (011): ['<s>', 'y', 'lim', '(', '0', ',', 'None', ')', '\\', 'n', '</s>']
Filtered   (009): ['y', 'lim', '(', '0', ',', 'None', ')', '\\', 'n']
Detokenized (007): ['ylim', '(', '0', ',', 'None', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "grid ( True ) \n"
Original    (005): ['grid', '(', 'True', ')', '\\n']
Tokenized   (008): ['<s>', 'grid', '(', 'True', ')', '\\', 'n', '</s>']
Filtered   (006): ['grid', '(', 'True', ')', '\\', 'n']
Detokenized (005): ['grid', '(', 'True', ')', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "legend ( [ p [ 0 ] for p in plots \n"
Original    (012): ['legend', '(', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'plots', '\\n']
Tokenized   (016): ['<s>', 'leg', 'end', '(', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'plots', '\\', 'n', '</s>']
Filtered   (014): ['leg', 'end', '(', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'plots', '\\', 'n']
Detokenized (012): ['legend', '(', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'plots', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "savefig ( outfile , dpi = 64 ) \n"
Original    (009): ['savefig', '(', 'outfile', ',', 'dpi', '=', '64', ')', '\\n']
Tokenized   (015): ['<s>', 'save', 'fig', '(', 'out', 'file', ',', 'd', 'pi', '=', '64', ')', '\\', 'n', '</s>']
Filtered   (013): ['save', 'fig', '(', 'out', 'file', ',', 'd', 'pi', '=', '64', ')', '\\', 'n']
Detokenized (009): ['savefig', '(', 'outfile', ',', 'dpi', '=', '64', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "help = , ) \n"
Original    (005): ['help', '=', ',', ')', '\\n']
Tokenized   (008): ['<s>', 'help', '=', ',', ')', '\\', 'n', '</s>']
Filtered   (006): ['help', '=', ',', ')', '\\', 'n']
Detokenized (005): ['help', '=', ',', ')', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "parser . add_option ( , , action = , \n"
Original    (010): ['parser', '.', 'add_option', '(', ',', ',', 'action', '=', ',', '\\n']
Tokenized   (015): ['<s>', 'parser', '.', 'add', '_', 'option', '(', ',', ',', 'action', '=', ',', '\\', 'n', '</s>']
Filtered   (013): ['parser', '.', 'add', '_', 'option', '(', ',', ',', 'action', '=', ',', '\\', 'n']
Detokenized (010): ['parser', '.', 'add_option', '(', ',', ',', 'action', '=', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "show_plot ( plots , yaxis , legends , gtitle , xmax = int ( options . xmax ) if \n"
Original    (020): ['show_plot', '(', 'plots', ',', 'yaxis', ',', 'legends', ',', 'gtitle', ',', 'xmax', '=', 'int', '(', 'options', '.', 'xmax', ')', 'if', '\\n']
Tokenized   (029): ['<s>', 'show', '_', 'plot', '(', 'plots', ',', 'y', 'axis', ',', 'legends', ',', 'g', 'title', ',', 'x', 'max', '=', 'int', '(', 'options', '.', 'x', 'max', ')', 'if', '\\', 'n', '</s>']
Filtered   (027): ['show', '_', 'plot', '(', 'plots', ',', 'y', 'axis', ',', 'legends', ',', 'g', 'title', ',', 'x', 'max', '=', 'int', '(', 'options', '.', 'x', 'max', ')', 'if', '\\', 'n']
Detokenized (020): ['show_plot', '(', 'plots', ',', 'yaxis', ',', 'legends', ',', 'gtitle', ',', 'xmax', '=', 'int', '(', 'options', '.', 'xmax', ')', 'if', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "options . xmax else None ) \n"
Original    (007): ['options', '.', 'xmax', 'else', 'None', ')', '\\n']
Tokenized   (011): ['<s>', 'options', '.', 'x', 'max', 'else', 'None', ')', '\\', 'n', '</s>']
Filtered   (009): ['options', '.', 'x', 'max', 'else', 'None', ')', '\\', 'n']
Detokenized (007): ['options', '.', 'xmax', 'else', 'None', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "hdfarray . attrs . object = { "a" : 32.1 , "b" : 1 , "c" : [ 1 , 2 ] } \n"
Original    (024): ['hdfarray', '.', 'attrs', '.', 'object', '=', '{', '"a"', ':', '32.1', ',', '"b"', ':', '1', ',', '"c"', ':', '[', '1', ',', '2', ']', '}', '\\n']
Tokenized   (038): ['<s>', 'h', 'df', 'array', '.', 'att', 'rs', '.', 'object', '=', '{', '"', 'a', '"', ':', '32', '.', '1', ',', '"', 'b', '"', ':', '1', ',', '"', 'c', '"', ':', '[', '1', ',', '2', ']', '}', '\\', 'n', '</s>']
Filtered   (036): ['h', 'df', 'array', '.', 'att', 'rs', '.', 'object', '=', '{', '"', 'a', '"', ':', '32', '.', '1', ',', '"', 'b', '"', ':', '1', ',', '"', 'c', '"', ':', '[', '1', ',', '2', ']', '}', '\\', 'n']
Detokenized (024): ['hdfarray', '.', 'attrs', '.', 'object', '=', '{', '"a"', ':', '32.1', ',', '"b"', ':', '1', ',', '"c"', ':', '[', '1', ',', '2', ']', '}', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "addr = hex ( id ( self ) ) \n"
Original    (010): ['addr', '=', 'hex', '(', 'id', '(', 'self', ')', ')', '\\n']
Tokenized   (013): ['<s>', 'addr', '=', 'hex', '(', 'id', '(', 'self', ')', ')', '\\', 'n', '</s>']
Filtered   (011): ['addr', '=', 'hex', '(', 'id', '(', 'self', ')', ')', '\\', 'n']
Detokenized (010): ['addr', '=', 'hex', '(', 'id', '(', 'self', ')', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "node_manager . registry . pop ( pathname , None ) \n"
Original    (011): ['node_manager', '.', 'registry', '.', 'pop', '(', 'pathname', ',', 'None', ')', '\\n']
Tokenized   (017): ['<s>', 'node', '_', 'manager', '.', 'registry', '.', 'pop', '(', 'path', 'name', ',', 'None', ')', '\\', 'n', '</s>']
Filtered   (015): ['node', '_', 'manager', '.', 'registry', '.', 'pop', '(', 'path', 'name', ',', 'None', ')', '\\', 'n']
Detokenized (011): ['node_manager', '.', 'registry', '.', 'pop', '(', 'pathname', ',', 'None', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "oldpathname , self . _v_pathname ) \n"
Original    (007): ['oldpathname', ',', 'self', '.', '_v_pathname', ')', '\\n']
Tokenized   (016): ['<s>', 'old', 'path', 'name', ',', 'self', '.', '_', 'v', '_', 'path', 'name', ')', '\\', 'n', '</s>']
Filtered   (014): ['old', 'path', 'name', ',', 'self', '.', '_', 'v', '_', 'path', 'name', ')', '\\', 'n']
Detokenized (007): ['oldpathname', ',', 'self', '.', '_v_pathname', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "recursive = False , _log = False , ** kwargs ) \n"
Original    (012): ['recursive', '=', 'False', ',', '_log', '=', 'False', ',', '**', 'kwargs', ')', '\\n']
Tokenized   (019): ['<s>', 'rec', 'ursive', '=', 'False', ',', '_', 'log', '=', 'False', ',', '**', 'k', 'w', 'args', ')', '\\', 'n', '</s>']
Filtered   (017): ['rec', 'ursive', '=', 'False', ',', '_', 'log', '=', 'False', ',', '**', 'k', 'w', 'args', ')', '\\', 'n']
Detokenized (012): ['recursive', '=', 'False', ',', '_log', '=', 'False', ',', '**', 'kwargs', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "% node . _v_pathname ) \n"
Original    (006): ['%', 'node', '.', '_v_pathname', ')', '\\n']
Tokenized   (013): ['<s>', '%', 'node', '.', '_', 'v', '_', 'path', 'name', ')', '\\', 'n', '</s>']
Filtered   (011): ['%', 'node', '.', '_', 'v', '_', 'path', 'name', ')', '\\', 'n']
Detokenized (006): ['%', 'node', '.', '_v_pathname', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "or pathname . startswith ( mypathname + ) ) : \n"
Original    (011): ['or', 'pathname', '.', 'startswith', '(', 'mypathname', '+', ')', ')', ':', '\\n']
Tokenized   (019): ['<s>', 'or', 'path', 'name', '.', 'start', 'sw', 'ith', '(', 'my', 'path', 'name', '+', ')', ')', ':', '\\', 'n', '</s>']
Filtered   (017): ['or', 'path', 'name', '.', 'start', 'sw', 'ith', '(', 'my', 'path', 'name', '+', ')', ')', ':', '\\', 'n']
Detokenized (011): ['or', 'pathname', '.', 'startswith', '(', 'mypathname', '+', ')', ')', ':', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "newarr = self . h5file . create_array ( , , [ 1 ] ) \n"
Original    (015): ['newarr', '=', 'self', '.', 'h5file', '.', 'create_array', '(', ',', ',', '[', '1', ']', ')', '\\n']
Tokenized   (023): ['<s>', 'new', 'arr', '=', 'self', '.', 'h', '5', 'file', '.', 'create', '_', 'array', '(', ',', ',', '[', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (021): ['new', 'arr', '=', 'self', '.', 'h', '5', 'file', '.', 'create', '_', 'array', '(', ',', ',', '[', '1', ']', ')', '\\', 'n']
Detokenized (015): ['newarr', '=', 'self', '.', 'h5file', '.', 'create_array', '(', ',', ',', '[', '1', ']', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "with self . assertRaises ( tables . UndoRedoError ) : \n"
Original    (011): ['with', 'self', '.', 'assertRaises', '(', 'tables', '.', 'UndoRedoError', ')', ':', '\\n']
Tokenized   (020): ['<s>', 'with', 'self', '.', 'assert', 'Ra', 'ises', '(', 'tables', '.', 'Und', 'o', 'Red', 'o', 'Error', ')', ':', '\\', 'n', '</s>']
Filtered   (018): ['with', 'self', '.', 'assert', 'Ra', 'ises', '(', 'tables', '.', 'Und', 'o', 'Red', 'o', 'Error', ')', ':', '\\', 'n']
Detokenized (011): ['with', 'self', '.', 'assertRaises', '(', 'tables', '.', 'UndoRedoError', ')', ':', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""/othergroup1/othergroup2/othergroup3" not in self . h5file ) \n"
Original    (008): ['"/othergroup1/othergroup2/othergroup3"', 'not', 'in', 'self', '.', 'h5file', ')', '\\n']
Tokenized   (026): ['<s>', '"', '/', 'other', 'group', '1', '/', 'other', 'group', '2', '/', 'other', 'group', '3', '"', 'not', 'in', 'self', '.', 'h', '5', 'file', ')', '\\', 'n', '</s>']
Filtered   (024): ['"', '/', 'other', 'group', '1', '/', 'other', 'group', '2', '/', 'other', 'group', '3', '"', 'not', 'in', 'self', '.', 'h', '5', 'file', ')', '\\', 'n']
Detokenized (008): ['"/othergroup1/othergroup2/othergroup3"', 'not', 'in', 'self', '.', 'h5file', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "var2 = BoolCol ( dflt = 0 , pos = 2 ) \n"
Original    (013): ['var2', '=', 'BoolCol', '(', 'dflt', '=', '0', ',', 'pos', '=', '2', ')', '\\n']
Tokenized   (020): ['<s>', 'var', '2', '=', 'B', 'ool', 'Col', '(', 'df', 'lt', '=', '0', ',', 'pos', '=', '2', ')', '\\', 'n', '</s>']
Filtered   (018): ['var', '2', '=', 'B', 'ool', 'Col', '(', 'df', 'lt', '=', '0', ',', 'pos', '=', '2', ')', '\\', 'n']
Detokenized (013): ['var2', '=', 'BoolCol', '(', 'dflt', '=', '0', ',', 'pos', '=', '2', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "None , nrows ) \n"
Original    (005): ['None', ',', 'nrows', ')', '\\n']
Tokenized   (009): ['<s>', 'None', ',', 'n', 'rows', ')', '\\', 'n', '</s>']
Filtered   (007): ['None', ',', 'n', 'rows', ')', '\\', 'n']
Detokenized (005): ['None', ',', 'nrows', ')', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "populateTable ( self . h5file . root , ) \n"
Original    (010): ['populateTable', '(', 'self', '.', 'h5file', '.', 'root', ',', ')', '\\n']
Tokenized   (017): ['<s>', 'pop', 'ulate', 'Table', '(', 'self', '.', 'h', '5', 'file', '.', 'root', ',', ')', '\\', 'n', '</s>']
Filtered   (015): ['pop', 'ulate', 'Table', '(', 'self', '.', 'h', '5', 'file', '.', 'root', ',', ')', '\\', 'n']
Detokenized (010): ['populateTable', '(', 'self', '.', 'h5file', '.', 'root', ',', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "new_node = self . h5file . copy_children ( \n"
Original    (009): ['new_node', '=', 'self', '.', 'h5file', '.', 'copy_children', '(', '\\n']
Tokenized   (018): ['<s>', 'new', '_', 'node', '=', 'self', '.', 'h', '5', 'file', '.', 'copy', '_', 'children', '(', '\\', 'n', '</s>']
Filtered   (016): ['new', '_', 'node', '=', 'self', '.', 'h', '5', 'file', '.', 'copy', '_', 'children', '(', '\\', 'n']
Detokenized (009): ['new_node', '=', 'self', '.', 'h5file', '.', 'copy_children', '(', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ", , recursive = 1 ) \n"
Original    (007): [',', ',', 'recursive', '=', '1', ')', '\\n']
Tokenized   (010): ['<s>', ',', ',', 'recursive', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (008): [',', ',', 'recursive', '=', '1', ')', '\\', 'n']
Detokenized (007): [',', ',', 'recursive', '=', '1', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "setattr ( attrs , , 11 ) \n"
Original    (008): ['setattr', '(', 'attrs', ',', ',', '11', ')', '\\n']
Tokenized   (013): ['<s>', 'set', 'attr', '(', 'att', 'rs', ',', ',', '11', ')', '\\', 'n', '</s>']
Filtered   (011): ['set', 'attr', '(', 'att', 'rs', ',', ',', '11', ')', '\\', 'n']
Detokenized (008): ['setattr', '(', 'attrs', ',', ',', '11', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "delattr ( attrs , ) \n"
Original    (006): ['delattr', '(', 'attrs', ',', ')', '\\n']
Tokenized   (011): ['<s>', 'del', 'attr', '(', 'att', 'rs', ',', ')', '\\', 'n', '</s>']
Filtered   (009): ['del', 'attr', '(', 'att', 'rs', ',', ')', '\\', 'n']
Detokenized (006): ['delattr', '(', 'attrs', ',', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "arr . _v_attrs . foo = \n"
Original    (007): ['arr', '.', '_v_attrs', '.', 'foo', '=', '\\n']
Tokenized   (014): ['<s>', 'arr', '.', '_', 'v', '_', 'att', 'rs', '.', 'foo', '=', '\\', 'n', '</s>']
Filtered   (012): ['arr', '.', '_', 'v', '_', 'att', 'rs', '.', 'foo', '=', '\\', 'n']
Detokenized (007): ['arr', '.', '_v_attrs', '.', 'foo', '=', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "assert response . header ( ) == \n"
Original    (008): ['assert', 'response', '.', 'header', '(', ')', '==', '\\n']
Tokenized   (011): ['<s>', 'assert', 'response', '.', 'header', '(', ')', '==', '\\', 'n', '</s>']
Filtered   (009): ['assert', 'response', '.', 'header', '(', ')', '==', '\\', 'n']
Detokenized (008): ['assert', 'response', '.', 'header', '(', ')', '==', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "CHANGES = open ( os . path . join ( here , ) , encoding = "utf-8" ) . read ( ) \n"
Original    (023): ['CHANGES', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ',', 'encoding', '=', '"utf-8"', ')', '.', 'read', '(', ')', '\\n']
Tokenized   (032): ['<s>', 'CH', 'ANG', 'ES', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ',', 'encoding', '=', '"', 'utf', '-', '8', '"', ')', '.', 'read', '(', ')', '\\', 'n', '</s>']
Filtered   (030): ['CH', 'ANG', 'ES', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ',', 'encoding', '=', '"', 'utf', '-', '8', '"', ')', '.', 'read', '(', ')', '\\', 'n']
Detokenized (023): ['CHANGES', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ',', 'encoding', '=', '"utf-8"', ')', '.', 'read', '(', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "tests_require = requires + [ ] , \n"
Original    (008): ['tests_require', '=', 'requires', '+', '[', ']', ',', '\\n']
Tokenized   (013): ['<s>', 'tests', '_', 'require', '=', 'requires', '+', '[', ']', ',', '\\', 'n', '</s>']
Filtered   (011): ['tests', '_', 'require', '=', 'requires', '+', '[', ']', ',', '\\', 'n']
Detokenized (008): ['tests_require', '=', 'requires', '+', '[', ']', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "user_id = Column ( Integer , primary_key = True ) \n"
Original    (011): ['user_id', '=', 'Column', '(', 'Integer', ',', 'primary_key', '=', 'True', ')', '\\n']
Tokenized   (018): ['<s>', 'user', '_', 'id', '=', 'Column', '(', 'Integer', ',', 'primary', '_', 'key', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (016): ['user', '_', 'id', '=', 'Column', '(', 'Integer', ',', 'primary', '_', 'key', '=', 'True', ')', '\\', 'n']
Detokenized (011): ['user_id', '=', 'Column', '(', 'Integer', ',', 'primary_key', '=', 'True', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "username = Column ( Unicode ( 20 ) , unique = True ) \n"
Original    (014): ['username', '=', 'Column', '(', 'Unicode', '(', '20', ')', ',', 'unique', '=', 'True', ')', '\\n']
Tokenized   (017): ['<s>', 'username', '=', 'Column', '(', 'Unicode', '(', '20', ')', ',', 'unique', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (015): ['username', '=', 'Column', '(', 'Unicode', '(', '20', ')', ',', 'unique', '=', 'True', ')', '\\', 'n']
Detokenized (014): ['username', '=', 'Column', '(', 'Unicode', '(', '20', ')', ',', 'unique', '=', 'True', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "hits = Column ( Integer , default = 0 ) \n"
Original    (011): ['hits', '=', 'Column', '(', 'Integer', ',', 'default', '=', '0', ')', '\\n']
Tokenized   (015): ['<s>', 'h', 'its', '=', 'Column', '(', 'Integer', ',', 'default', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (013): ['h', 'its', '=', 'Column', '(', 'Integer', ',', 'default', '=', '0', ')', '\\', 'n']
Detokenized (011): ['hits', '=', 'Column', '(', 'Integer', ',', 'default', '=', '0', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "_password = Column ( , Unicode ( 60 ) ) \n"
Original    (011): ['_password', '=', 'Column', '(', ',', 'Unicode', '(', '60', ')', ')', '\\n']
Tokenized   (015): ['<s>', '_', 'password', '=', 'Column', '(', ',', 'Unicode', '(', '60', ')', ')', '\\', 'n', '</s>']
Filtered   (013): ['_', 'password', '=', 'Column', '(', ',', 'Unicode', '(', '60', ')', ')', '\\', 'n']
Detokenized (011): ['_password', '=', 'Column', '(', ',', 'Unicode', '(', '60', ')', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "Column ( , Integer , ForeignKey ( ) ) \n"
Original    (010): ['Column', '(', ',', 'Integer', ',', 'ForeignKey', '(', ')', ')', '\\n']
Tokenized   (014): ['<s>', 'Column', '(', ',', 'Integer', ',', 'Foreign', 'Key', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (012): ['Column', '(', ',', 'Integer', ',', 'Foreign', 'Key', '(', ')', ')', '\\', 'n']
Detokenized (010): ['Column', '(', ',', 'Integer', ',', 'ForeignKey', '(', ')', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "target_id = Column ( Integer , ForeignKey ( ) ) \n"
Original    (011): ['target_id', '=', 'Column', '(', 'Integer', ',', 'ForeignKey', '(', ')', ')', '\\n']
Tokenized   (017): ['<s>', 'target', '_', 'id', '=', 'Column', '(', 'Integer', ',', 'Foreign', 'Key', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (015): ['target', '_', 'id', '=', 'Column', '(', 'Integer', ',', 'Foreign', 'Key', '(', ')', ')', '\\', 'n']
Detokenized (011): ['target_id', '=', 'Column', '(', 'Integer', ',', 'ForeignKey', '(', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "comments = relation ( , cascade = "delete" , \n"
Original    (010): ['comments', '=', 'relation', '(', ',', 'cascade', '=', '"delete"', ',', '\\n']
Tokenized   (015): ['<s>', 'comments', '=', 'relation', '(', ',', 'cascade', '=', '"', 'delete', '"', ',', '\\', 'n', '</s>']
Filtered   (013): ['comments', '=', 'relation', '(', ',', 'cascade', '=', '"', 'delete', '"', ',', '\\', 'n']
Detokenized (010): ['comments', '=', 'relation', '(', ',', 'cascade', '=', '"delete"', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "author = relation ( User , cascade = "delete" , backref = ) \n"
Original    (014): ['author', '=', 'relation', '(', 'User', ',', 'cascade', '=', '"delete"', ',', 'backref', '=', ')', '\\n']
Tokenized   (020): ['<s>', 'author', '=', 'relation', '(', 'User', ',', 'cascade', '=', '"', 'delete', '"', ',', 'back', 'ref', '=', ')', '\\', 'n', '</s>']
Filtered   (018): ['author', '=', 'relation', '(', 'User', ',', 'cascade', '=', '"', 'delete', '"', ',', 'back', 'ref', '=', ')', '\\', 'n']
Detokenized (014): ['author', '=', 'relation', '(', 'User', ',', 'cascade', '=', '"delete"', ',', 'backref', '=', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "tags = relation ( Tag , secondary = ideas_tags , backref = ) \n"
Original    (014): ['tags', '=', 'relation', '(', 'Tag', ',', 'secondary', '=', 'ideas_tags', ',', 'backref', '=', ')', '\\n']
Tokenized   (020): ['<s>', 'tags', '=', 'relation', '(', 'Tag', ',', 'secondary', '=', 'ideas', '_', 'tags', ',', 'back', 'ref', '=', ')', '\\', 'n', '</s>']
Filtered   (018): ['tags', '=', 'relation', '(', 'Tag', ',', 'secondary', '=', 'ideas', '_', 'tags', ',', 'back', 'ref', '=', ')', '\\', 'n']
Detokenized (014): ['tags', '=', 'relation', '(', 'Tag', ',', 'secondary', '=', 'ideas_tags', ',', 'backref', '=', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "voted_users = relation ( User , secondary = voted_users , lazy = , \n"
Original    (014): ['voted_users', '=', 'relation', '(', 'User', ',', 'secondary', '=', 'voted_users', ',', 'lazy', '=', ',', '\\n']
Tokenized   (022): ['<s>', 'v', 'oted', '_', 'users', '=', 'relation', '(', 'User', ',', 'secondary', '=', 'voted', '_', 'users', ',', 'lazy', '=', ',', '\\', 'n', '</s>']
Filtered   (020): ['v', 'oted', '_', 'users', '=', 'relation', '(', 'User', ',', 'secondary', '=', 'voted', '_', 'users', ',', 'lazy', '=', ',', '\\', 'n']
Detokenized (014): ['voted_users', '=', 'relation', '(', 'User', ',', 'secondary', '=', 'voted_users', ',', 'lazy', '=', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "total_votes = column_property ( ( hits + misses ) . label ( ) ) \n"
Original    (015): ['total_votes', '=', 'column_property', '(', '(', 'hits', '+', 'misses', ')', '.', 'label', '(', ')', ')', '\\n']
Tokenized   (022): ['<s>', 'total', '_', 'votes', '=', 'column', '_', 'property', '(', '(', 'hits', '+', 'misses', ')', '.', 'label', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (020): ['total', '_', 'votes', '=', 'column', '_', 'property', '(', '(', 'hits', '+', 'misses', ')', '.', 'label', '(', ')', ')', '\\', 'n']
Detokenized (015): ['total_votes', '=', 'column_property', '(', '(', 'hits', '+', 'misses', ')', '.', 'label', '(', ')', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "query = query . filter ( cls . target == None ) . order_by ( order_by ) \n"
Original    (018): ['query', '=', 'query', '.', 'filter', '(', 'cls', '.', 'target', '==', 'None', ')', '.', 'order_by', '(', 'order_by', ')', '\\n']
Tokenized   (026): ['<s>', 'query', '=', 'query', '.', 'filter', '(', 'cl', 's', '.', 'target', '==', 'None', ')', '.', 'order', '_', 'by', '(', 'order', '_', 'by', ')', '\\', 'n', '</s>']
Filtered   (024): ['query', '=', 'query', '.', 'filter', '(', 'cl', 's', '.', 'target', '==', 'None', ')', '.', 'order', '_', 'by', '(', 'order', '_', 'by', ')', '\\', 'n']
Detokenized (018): ['query', '=', 'query', '.', 'filter', '(', 'cls', '.', 'target', '==', 'None', ')', '.', 'order_by', '(', 'order_by', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "mock_get_auditlog . side_effect = lambda c : auditlog \n"
Original    (009): ['mock_get_auditlog', '.', 'side_effect', '=', 'lambda', 'c', ':', 'auditlog', '\\n']
Tokenized   (022): ['<s>', 'm', 'ock', '_', 'get', '_', 'aud', 'it', 'log', '.', 'side', '_', 'effect', '=', 'lambda', 'c', ':', 'audit', 'log', '\\', 'n', '</s>']
Filtered   (020): ['m', 'ock', '_', 'get', '_', 'aud', 'it', 'log', '.', 'side', '_', 'effect', '=', 'lambda', 'c', ':', 'audit', 'log', '\\', 'n']
Detokenized (009): ['mock_get_auditlog', '.', 'side_effect', '=', 'lambda', 'c', ':', 'auditlog', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "json . loads ( entry [ 2 ] . payload ) , \n"
Original    (013): ['json', '.', 'loads', '(', 'entry', '[', '2', ']', '.', 'payload', ')', ',', '\\n']
Tokenized   (016): ['<s>', 'json', '.', 'loads', '(', 'entry', '[', '2', ']', '.', 'payload', ')', ',', '\\', 'n', '</s>']
Filtered   (014): ['json', '.', 'loads', '(', 'entry', '[', '2', ']', '.', 'payload', ')', ',', '\\', 'n']
Detokenized (013): ['json', '.', 'loads', '(', 'entry', '[', '2', ']', '.', 'payload', ')', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : ": 5 \n"
Original    (003): [':', '5', '\\n']
Tokenized   (006): ['<s>', ':', '5', '\\', 'n', '</s>']
Filtered   (004): [':', '5', '\\', 'n']
Detokenized (003): [':', '5', '\\n']
Counter: 4
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "registry . content = DummyContentRegistry ( ) \n"
Original    (008): ['registry', '.', 'content', '=', 'DummyContentRegistry', '(', ')', '\\n']
Tokenized   (016): ['<s>', 'reg', 'istry', '.', 'content', '=', 'D', 'ummy', 'Content', 'Reg', 'istry', '(', ')', '\\', 'n', '</s>']
Filtered   (014): ['reg', 'istry', '.', 'content', '=', 'D', 'ummy', 'Content', 'Reg', 'istry', '(', ')', '\\', 'n']
Detokenized (008): ['registry', '.', 'content', '=', 'DummyContentRegistry', '(', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "ep = DummyFunction ( True ) \n"
Original    (007): ['ep', '=', 'DummyFunction', '(', 'True', ')', '\\n']
Tokenized   (012): ['<s>', 'ep', '=', 'D', 'ummy', 'Function', '(', 'True', ')', '\\', 'n', '</s>']
Filtered   (010): ['ep', '=', 'D', 'ummy', 'Function', '(', 'True', ')', '\\', 'n']
Detokenized (007): ['ep', '=', 'DummyFunction', '(', 'True', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "name_node . validator ( node [ ] , filename ) \n"
Original    (011): ['name_node', '.', 'validator', '(', 'node', '[', ']', ',', 'filename', ')', '\\n']
Tokenized   (017): ['<s>', 'name', '_', 'node', '.', 'valid', 'ator', '(', 'node', '[', ']', ',', 'filename', ')', '\\', 'n', '</s>']
Filtered   (015): ['name', '_', 'node', '.', 'valid', 'ator', '(', 'node', '[', ']', ',', 'filename', ')', '\\', 'n']
Detokenized (011): ['name_node', '.', 'validator', '(', 'node', '[', ']', ',', 'filename', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "schema [ ] . missing = colander . null \n"
Original    (010): ['schema', '[', ']', '.', 'missing', '=', 'colander', '.', 'null', '\\n']
Tokenized   (015): ['<s>', 'sche', 'ma', '[', ']', '.', 'missing', '=', 'col', 'ander', '.', 'null', '\\', 'n', '</s>']
Filtered   (013): ['sche', 'ma', '[', ']', '.', 'missing', '=', 'col', 'ander', '.', 'null', '\\', 'n']
Detokenized (010): ['schema', '[', ']', '.', 'missing', '=', 'colander', '.', 'null', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "title = appstruct [ ] or None \n"
Original    (008): ['title', '=', 'appstruct', '[', ']', 'or', 'None', '\\n']
Tokenized   (012): ['<s>', 'title', '=', 'app', 'struct', '[', ']', 'or', 'None', '\\', 'n', '</s>']
Filtered   (010): ['title', '=', 'app', 'struct', '[', ']', 'or', 'None', '\\', 'n']
Detokenized (008): ['title', '=', 'appstruct', '[', ']', 'or', 'None', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "mimetype = appstruct [ ] or USE_MAGIC \n"
Original    (008): ['mimetype', '=', 'appstruct', '[', ']', 'or', 'USE_MAGIC', '\\n']
Tokenized   (018): ['<s>', 'm', 'im', 'ety', 'pe', '=', 'app', 'struct', '[', ']', 'or', 'USE', '_', 'MAG', 'IC', '\\', 'n', '</s>']
Filtered   (016): ['m', 'im', 'ety', 'pe', '=', 'app', 'struct', '[', ']', 'or', 'USE', '_', 'MAG', 'IC', '\\', 'n']
Detokenized (008): ['mimetype', '=', 'appstruct', '[', ']', 'or', 'USE_MAGIC', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "filedata = tempstore . get ( uid , { } ) \n"
Original    (012): ['filedata', '=', 'tempstore', '.', 'get', '(', 'uid', ',', '{', '}', ')', '\\n']
Tokenized   (019): ['<s>', 'f', 'iled', 'ata', '=', 'temp', 'store', '.', 'get', '(', 'u', 'id', ',', '{', '}', ')', '\\', 'n', '</s>']
Filtered   (017): ['f', 'iled', 'ata', '=', 'temp', 'store', '.', 'get', '(', 'u', 'id', ',', '{', '}', ')', '\\', 'n']
Detokenized (012): ['filedata', '=', 'tempstore', '.', 'get', '(', 'uid', ',', '{', '}', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "resource1 . __acl__ = [ ( None , , None ) , ( None , 1 , None ) ] \n"
Original    (021): ['resource1', '.', '__acl__', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', '\\n']
Tokenized   (027): ['<s>', 'resource', '1', '.', '__', 'acl', '__', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', '\\', 'n', '</s>']
Filtered   (025): ['resource', '1', '.', '__', 'acl', '__', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', '\\', 'n']
Detokenized (021): ['resource1', '.', '__acl__', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "new_acl = [ ( None , , None ) , ( None , 1 , None ) ] , \n"
Original    (020): ['new_acl', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', ',', '\\n']
Tokenized   (025): ['<s>', 'new', '_', 'acl', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', ',', '\\', 'n', '</s>']
Filtered   (023): ['new', '_', 'acl', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', ',', '\\', 'n']
Detokenized (020): ['new_acl', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "request . registry . notify ( LoggedIn ( login , user , context , request ) ) \n"
Original    (018): ['request', '.', 'registry', '.', 'notify', '(', 'LoggedIn', '(', 'login', ',', 'user', ',', 'context', ',', 'request', ')', ')', '\\n']
Tokenized   (022): ['<s>', 'request', '.', 'registry', '.', 'notify', '(', 'Logged', 'In', '(', 'login', ',', 'user', ',', 'context', ',', 'request', ')', ')', '\\', 'n', '</s>']
Filtered   (020): ['request', '.', 'registry', '.', 'notify', '(', 'Logged', 'In', '(', 'login', ',', 'user', ',', 'context', ',', 'request', ')', ')', '\\', 'n']
Detokenized (018): ['request', '.', 'registry', '.', 'notify', '(', 'LoggedIn', '(', 'login', ',', 'user', ',', 'context', ',', 'request', ')', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "dirname , filename = os . path . split ( context . path ) \n"
Original    (015): ['dirname', ',', 'filename', '=', 'os', '.', 'path', '.', 'split', '(', 'context', '.', 'path', ')', '\\n']
Tokenized   (019): ['<s>', 'dir', 'name', ',', 'filename', '=', 'os', '.', 'path', '.', 'split', '(', 'context', '.', 'path', ')', '\\', 'n', '</s>']
Filtered   (017): ['dir', 'name', ',', 'filename', '=', 'os', '.', 'path', '.', 'split', '(', 'context', '.', 'path', ')', '\\', 'n']
Detokenized (015): ['dirname', ',', 'filename', '=', 'os', '.', 'path', '.', 'split', '(', 'context', '.', 'path', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "response . content_type = mt or \n"
Original    (007): ['response', '.', 'content_type', '=', 'mt', 'or', '\\n']
Tokenized   (012): ['<s>', 'response', '.', 'content', '_', 'type', '=', 'mt', 'or', '\\', 'n', '</s>']
Filtered   (010): ['response', '.', 'content', '_', 'type', '=', 'mt', 'or', '\\', 'n']
Detokenized (007): ['response', '.', 'content_type', '=', 'mt', 'or', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "getattr ( SkipCase ( ) , ) ) \n"
Original    (009): ['getattr', '(', 'SkipCase', '(', ')', ',', ')', ')', '\\n']
Tokenized   (014): ['<s>', 'get', 'attr', '(', 'Skip', 'Case', '(', ')', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (012): ['get', 'attr', '(', 'Skip', 'Case', '(', ')', ',', ')', ')', '\\', 'n']
Detokenized (009): ['getattr', '(', 'SkipCase', '(', ')', ',', ')', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "iterator . __class__ . __name__ ) ) \n"
Original    (008): ['iterator', '.', '__class__', '.', '__name__', ')', ')', '\\n']
Tokenized   (015): ['<s>', 'iterator', '.', '__', 'class', '__', '.', '__', 'name', '__', ')', ')', '\\', 'n', '</s>']
Filtered   (013): ['iterator', '.', '__', 'class', '__', '.', '__', 'name', '__', ')', ')', '\\', 'n']
Detokenized (008): ['iterator', '.', '__class__', '.', '__name__', ')', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "parts = super ( newbytes , self ) . splitlines ( keepends ) \n"
Original    (014): ['parts', '=', 'super', '(', 'newbytes', ',', 'self', ')', '.', 'splitlines', '(', 'keepends', ')', '\\n']
Tokenized   (020): ['<s>', 'parts', '=', 'super', '(', 'new', 'bytes', ',', 'self', ')', '.', 'split', 'lines', '(', 'keep', 'ends', ')', '\\', 'n', '</s>']
Filtered   (018): ['parts', '=', 'super', '(', 'new', 'bytes', ',', 'self', ')', '.', 'split', 'lines', '(', 'keep', 'ends', ')', '\\', 'n']
Detokenized (014): ['parts', '=', 'super', '(', 'newbytes', ',', 'self', ')', '.', 'splitlines', '(', 'keepends', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "pos = self . rfind ( sub , * args ) \n"
Original    (012): ['pos', '=', 'self', '.', 'rfind', '(', 'sub', ',', '*', 'args', ')', '\\n']
Tokenized   (016): ['<s>', 'pos', '=', 'self', '.', 'r', 'find', '(', 'sub', ',', '*', 'args', ')', '\\', 'n', '</s>']
Filtered   (014): ['pos', '=', 'self', '.', 'r', 'find', '(', 'sub', ',', '*', 'args', ')', '\\', 'n']
Detokenized (012): ['pos', '=', 'self', '.', 'rfind', '(', 'sub', ',', '*', 'args', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "replaced_builtins = . split ( ) \n"
Original    (007): ['replaced_builtins', '=', '.', 'split', '(', ')', '\\n']
Tokenized   (014): ['<s>', 're', 'placed', '_', 'built', 'ins', '=', '.', 'split', '(', ')', '\\', 'n', '</s>']
Filtered   (012): ['re', 'placed', '_', 'built', 'ins', '=', '.', 'split', '(', ')', '\\', 'n']
Detokenized (007): ['replaced_builtins', '=', '.', 'split', '(', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "expression = . join ( [ "name=\'{0}\'" . format ( name ) for name in replaced_builtins ] ) \n"
Original    (019): ['expression', '=', '.', 'join', '(', '[', '"name=\\\'{0}\\\'"', '.', 'format', '(', 'name', ')', 'for', 'name', 'in', 'replaced_builtins', ']', ')', '\\n']
Tokenized   (032): ['<s>', 'expression', '=', '.', 'join', '(', '[', '"', 'name', '=', "\\'", '{', '0', '}\\', '\'"', '.', 'format', '(', 'name', ')', 'for', 'name', 'in', 'replaced', '_', 'built', 'ins', ']', ')', '\\', 'n', '</s>']
Filtered   (030): ['expression', '=', '.', 'join', '(', '[', '"', 'name', '=', "\\'", '{', '0', '}\\', '\'"', '.', 'format', '(', 'name', ')', 'for', 'name', 'in', 'replaced', '_', 'built', 'ins', ']', ')', '\\', 'n']
Detokenized (019): ['expression', '=', '.', 'join', '(', '[', '"name=\\\'{0}\\\'"', '.', 'format', '(', 'name', ')', 'for', 'name', 'in', 'replaced_builtins', ']', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "touch_import_top ( , name . value , node ) \n"
Original    (010): ['touch_import_top', '(', ',', 'name', '.', 'value', ',', 'node', ')', '\\n']
Tokenized   (017): ['<s>', 'touch', '_', 'import', '_', 'top', '(', ',', 'name', '.', 'value', ',', 'node', ')', '\\', 'n', '</s>']
Filtered   (015): ['touch', '_', 'import', '_', 'top', '(', ',', 'name', '.', 'value', ',', 'node', ')', '\\', 'n']
Detokenized (010): ['touch_import_top', '(', ',', 'name', '.', 'value', ',', 'node', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "retcode = main ( [ self . textfilename ] ) \n"
Original    (011): ['retcode', '=', 'main', '(', '[', 'self', '.', 'textfilename', ']', ')', '\\n']
Tokenized   (016): ['<s>', 'ret', 'code', '=', 'main', '(', '[', 'self', '.', 'text', 'filename', ']', ')', '\\', 'n', '</s>']
Filtered   (014): ['ret', 'code', '=', 'main', '(', '[', 'self', '.', 'text', 'filename', ']', ')', '\\', 'n']
Detokenized (011): ['retcode', '=', 'main', '(', '[', 'self', '.', 'textfilename', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "v = self . visit ( node . values [ i ] ) \n"
Original    (014): ['v', '=', 'self', '.', 'visit', '(', 'node', '.', 'values', '[', 'i', ']', ')', '\\n']
Tokenized   (017): ['<s>', 'v', '=', 'self', '.', 'visit', '(', 'node', '.', 'values', '[', 'i', ']', ')', '\\', 'n', '</s>']
Filtered   (015): ['v', '=', 'self', '.', 'visit', '(', 'node', '.', 'values', '[', 'i', ']', ')', '\\', 'n']
Detokenized (014): ['v', '=', 'self', '.', 'visit', '(', 'node', '.', 'values', '[', 'i', ']', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "props . update ( self . _class_props [ p ] ) \n"
Original    (012): ['props', '.', 'update', '(', 'self', '.', '_class_props', '[', 'p', ']', ')', '\\n']
Tokenized   (020): ['<s>', 'pro', 'ps', '.', 'update', '(', 'self', '.', '_', 'class', '_', 'pro', 'ps', '[', 'p', ']', ')', '\\', 'n', '</s>']
Filtered   (018): ['pro', 'ps', '.', 'update', '(', 'self', '.', '_', 'class', '_', 'pro', 'ps', '[', 'p', ']', ')', '\\', 'n']
Detokenized (012): ['props', '.', 'update', '(', 'self', '.', '_class_props', '[', 'p', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "kwargs_init = [ % ( x . split ( ) [ 0 ] , x . split ( ) [ 0 ] ) for x in kwargs ] \n"
Original    (029): ['kwargs_init', '=', '[', '%', '(', 'x', '.', 'split', '(', ')', '[', '0', ']', ',', 'x', '.', 'split', '(', ')', '[', '0', ']', ')', 'for', 'x', 'in', 'kwargs', ']', '\\n']
Tokenized   (037): ['<s>', 'kw', 'args', '_', 'init', '=', '[', '%', '(', 'x', '.', 'split', '(', ')', '[', '0', ']', ',', 'x', '.', 'split', '(', ')', '[', '0', ']', ')', 'for', 'x', 'in', 'k', 'w', 'args', ']', '\\', 'n', '</s>']
Filtered   (035): ['kw', 'args', '_', 'init', '=', '[', '%', '(', 'x', '.', 'split', '(', ')', '[', '0', ']', ',', 'x', '.', 'split', '(', ')', '[', '0', ']', ')', 'for', 'x', 'in', 'k', 'w', 'args', ']', '\\', 'n']
Detokenized (029): ['kwargs_init', '=', '[', '%', '(', 'x', '.', 'split', '(', ')', '[', '0', ']', ',', 'x', '.', 'split', '(', ')', '[', '0', ']', ')', 'for', 'x', 'in', 'kwargs', ']', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "else : nargs = node . args . args \n"
Original    (010): ['else', ':', 'nargs', '=', 'node', '.', 'args', '.', 'args', '\\n']
Tokenized   (014): ['<s>', 'else', ':', 'n', 'args', '=', 'node', '.', 'args', '.', 'args', '\\', 'n', '</s>']
Filtered   (012): ['else', ':', 'n', 'args', '=', 'node', '.', 'args', '.', 'args', '\\', 'n']
Detokenized (010): ['else', ':', 'nargs', '=', 'node', '.', 'args', '.', 'args', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "kwargs . append ( % ( a , default_value ) ) \n"
Original    (012): ['kwargs', '.', 'append', '(', '%', '(', 'a', ',', 'default_value', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'kw', 'args', '.', 'append', '(', '%', '(', 'a', ',', 'default', '_', 'value', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['kw', 'args', '.', 'append', '(', '%', '(', 'a', ',', 'default', '_', 'value', ')', ')', '\\', 'n']
Detokenized (012): ['kwargs', '.', 'append', '(', '%', '(', 'a', ',', 'default_value', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "offset = len ( node . args . args ) - len ( node . args . defaults ) \n"
Original    (020): ['offset', '=', 'len', '(', 'node', '.', 'args', '.', 'args', ')', '-', 'len', '(', 'node', '.', 'args', '.', 'defaults', ')', '\\n']
Tokenized   (023): ['<s>', 'offset', '=', 'len', '(', 'node', '.', 'args', '.', 'args', ')', '-', 'len', '(', 'node', '.', 'args', '.', 'defaults', ')', '\\', 'n', '</s>']
Filtered   (021): ['offset', '=', 'len', '(', 'node', '.', 'args', '.', 'args', ')', '-', 'len', '(', 'node', '.', 'args', '.', 'defaults', ')', '\\', 'n']
Detokenized (020): ['offset', '=', 'len', '(', 'node', '.', 'args', '.', 'args', ')', '-', 'len', '(', 'node', '.', 'args', '.', 'defaults', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "varargs = [ % n for n in range ( 16 ) ] \n"
Original    (014): ['varargs', '=', '[', '%', 'n', 'for', 'n', 'in', 'range', '(', '16', ')', ']', '\\n']
Tokenized   (018): ['<s>', 'var', 'args', '=', '[', '%', 'n', 'for', 'n', 'in', 'range', '(', '16', ')', ']', '\\', 'n', '</s>']
Filtered   (016): ['var', 'args', '=', '[', '%', 'n', 'for', 'n', 'in', 'range', '(', '16', ')', ']', '\\', 'n']
Detokenized (014): ['varargs', '=', '[', '%', 'n', 'for', 'n', 'in', 'range', '(', '16', ')', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "buffer += % self . indent ( ) \n"
Original    (009): ['buffer', '+=', '%', 'self', '.', 'indent', '(', ')', '\\n']
Tokenized   (012): ['<s>', 'buffer', '+=', '%', 'self', '.', 'indent', '(', ')', '\\', 'n', '</s>']
Filtered   (010): ['buffer', '+=', '%', 'self', '.', 'indent', '(', ')', '\\', 'n']
Detokenized (009): ['buffer', '+=', '%', 'self', '.', 'indent', '(', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "arg_name = args = None \n"
Original    (006): ['arg_name', '=', 'args', '=', 'None', '\\n']
Tokenized   (011): ['<s>', 'arg', '_', 'name', '=', 'args', '=', 'None', '\\', 'n', '</s>']
Filtered   (009): ['arg', '_', 'name', '=', 'args', '=', 'None', '\\', 'n']
Detokenized (006): ['arg_name', '=', 'args', '=', 'None', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "comp . append ( self . visit ( node . comparators [ i ] ) ) \n"
Original    (017): ['comp', '.', 'append', '(', 'self', '.', 'visit', '(', 'node', '.', 'comparators', '[', 'i', ']', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'comp', '.', 'append', '(', 'self', '.', 'visit', '(', 'node', '.', 'compar', 'ators', '[', 'i', ']', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['comp', '.', 'append', '(', 'self', '.', 'visit', '(', 'node', '.', 'compar', 'ators', '[', 'i', ']', ')', ')', '\\', 'n']
Detokenized (017): ['comp', '.', 'append', '(', 'self', '.', 'visit', '(', 'node', '.', 'comparators', '[', 'i', ']', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "testtime = time ( ) - starttime \n"
Original    (008): ['testtime', '=', 'time', '(', ')', '-', 'starttime', '\\n']
Tokenized   (013): ['<s>', 'test', 'time', '=', 'time', '(', ')', '-', 'start', 'time', '\\', 'n', '</s>']
Filtered   (011): ['test', 'time', '=', 'time', '(', ')', '-', 'start', 'time', '\\', 'n']
Detokenized (008): ['testtime', '=', 'time', '(', ')', '-', 'starttime', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "primes_per_sec = len ( seq ) * ( 1.0 / testtime ) \n"
Original    (013): ['primes_per_sec', '=', 'len', '(', 'seq', ')', '*', '(', '1.0', '/', 'testtime', ')', '\\n']
Tokenized   (024): ['<s>', 'pr', 'imes', '_', 'per', '_', 'sec', '=', 'len', '(', 'seq', ')', '*', '(', '1', '.', '0', '/', 'test', 'time', ')', '\\', 'n', '</s>']
Filtered   (022): ['pr', 'imes', '_', 'per', '_', 'sec', '=', 'len', '(', 'seq', ')', '*', '(', '1', '.', '0', '/', 'test', 'time', ')', '\\', 'n']
Detokenized (013): ['primes_per_sec', '=', 'len', '(', 'seq', ')', '*', '(', '1.0', '/', 'testtime', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "b = range ( 1 , 10 ) \n"
Original    (009): ['b', '=', 'range', '(', '1', ',', '10', ')', '\\n']
Tokenized   (012): ['<s>', 'b', '=', 'range', '(', '1', ',', '10', ')', '\\', 'n', '</s>']
Filtered   (010): ['b', '=', 'range', '(', '1', ',', '10', ')', '\\', 'n']
Detokenized (009): ['b', '=', 'range', '(', '1', ',', '10', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "w1 = threading . start_webworker ( worker , ( seq , , ) ) \n"
Original    (015): ['w1', '=', 'threading', '.', 'start_webworker', '(', 'worker', ',', '(', 'seq', ',', ',', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'w', '1', '=', 'thread', 'ing', '.', 'start', '_', 'web', 'worker', '(', 'worker', ',', '(', 'seq', ',', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['w', '1', '=', 'thread', 'ing', '.', 'start', '_', 'web', 'worker', '(', 'worker', ',', '(', 'seq', ',', ',', ')', ')', '\\', 'n']
Detokenized (015): ['w1', '=', 'threading', '.', 'start_webworker', '(', 'worker', ',', '(', 'seq', ',', ',', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "TestError ( in seq ) \n"
Original    (006): ['TestError', '(', 'in', 'seq', ')', '\\n']
Tokenized   (010): ['<s>', 'Test', 'Error', '(', 'in', 'seq', ')', '\\', 'n', '</s>']
Filtered   (008): ['Test', 'Error', '(', 'in', 'seq', ')', '\\', 'n']
Detokenized (006): ['TestError', '(', 'in', 'seq', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "del self . face_groups [ : ] \n"
Original    (008): ['del', 'self', '.', 'face_groups', '[', ':', ']', '\\n']
Tokenized   (013): ['<s>', 'del', 'self', '.', 'face', '_', 'groups', '[', ':', ']', '\\', 'n', '</s>']
Filtered   (011): ['del', 'self', '.', 'face', '_', 'groups', '[', ':', ']', '\\', 'n']
Detokenized (008): ['del', 'self', '.', 'face_groups', '[', ':', ']', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "mtllib_path = os . path . join ( model_path , data [ 0 ] ) \n"
Original    (016): ['mtllib_path', '=', 'os', '.', 'path', '.', 'join', '(', 'model_path', ',', 'data', '[', '0', ']', ')', '\\n']
Tokenized   (025): ['<s>', 'mt', 'll', 'ib', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'model', '_', 'path', ',', 'data', '[', '0', ']', ')', '\\', 'n', '</s>']
Filtered   (023): ['mt', 'll', 'ib', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'model', '_', 'path', ',', 'data', '[', '0', ']', ')', '\\', 'n']
Detokenized (016): ['mtllib_path', '=', 'os', '.', 'path', '.', 'join', '(', 'model_path', ',', 'data', '[', '0', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "vertex = ( float ( x ) , float ( y ) , float ( z ) ) \n"
Original    (019): ['vertex', '=', '(', 'float', '(', 'x', ')', ',', 'float', '(', 'y', ')', ',', 'float', '(', 'z', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'ver', 'tex', '=', '(', 'float', '(', 'x', ')', ',', 'float', '(', 'y', ')', ',', 'float', '(', 'z', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['ver', 'tex', '=', '(', 'float', '(', 'x', ')', ',', 'float', '(', 'y', ')', ',', 'float', '(', 'z', ')', ')', '\\', 'n']
Detokenized (019): ['vertex', '=', '(', 'float', '(', 'x', ')', ',', 'float', '(', 'y', ')', ',', 'float', '(', 'z', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "tex_coord = ( float ( s ) , float ( t ) ) \n"
Original    (014): ['tex_coord', '=', '(', 'float', '(', 's', ')', ',', 'float', '(', 't', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'tex', '_', 'coord', '=', '(', 'float', '(', 's', ')', ',', 'float', '(', 't', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['tex', '_', 'coord', '=', '(', 'float', '(', 's', ')', ',', 'float', '(', 't', ')', ')', '\\', 'n']
Detokenized (014): ['tex_coord', '=', '(', 'float', '(', 's', ')', ',', 'float', '(', 't', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "indices = ( int ( vi ) - 1 , int ( ti ) - 1 , int ( ni ) - 1 ) \n"
Original    (025): ['indices', '=', '(', 'int', '(', 'vi', ')', '-', '1', ',', 'int', '(', 'ti', ')', '-', '1', ',', 'int', '(', 'ni', ')', '-', '1', ')', '\\n']
Tokenized   (029): ['<s>', 'ind', 'ices', '=', '(', 'int', '(', 'vi', ')', '-', '1', ',', 'int', '(', 'ti', ')', '-', '1', ',', 'int', '(', 'ni', ')', '-', '1', ')', '\\', 'n', '</s>']
Filtered   (027): ['ind', 'ices', '=', '(', 'int', '(', 'vi', ')', '-', '1', ',', 'int', '(', 'ti', ')', '-', '1', ',', 'int', '(', 'ni', ')', '-', '1', ')', '\\', 'n']
Detokenized (025): ['indices', '=', '(', 'int', '(', 'vi', ')', '-', '1', ',', 'int', '(', 'ti', ')', '-', '1', ',', 'int', '(', 'ni', ')', '-', '1', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "glBindTexture ( GL_TEXTURE_2D , material . texture_id ) \n"
Original    (009): ['glBindTexture', '(', 'GL_TEXTURE_2D', ',', 'material', '.', 'texture_id', ')', '\\n']
Tokenized   (021): ['<s>', 'gl', 'Bind', 'Texture', '(', 'GL', '_', 'TEXTURE', '_', '2', 'D', ',', 'material', '.', 'texture', '_', 'id', ')', '\\', 'n', '</s>']
Filtered   (019): ['gl', 'Bind', 'Texture', '(', 'GL', '_', 'TEXTURE', '_', '2', 'D', ',', 'material', '.', 'texture', '_', 'id', ')', '\\', 'n']
Detokenized (009): ['glBindTexture', '(', 'GL_TEXTURE_2D', ',', 'material', '.', 'texture_id', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "glPixelStorei ( GL_UNPACK_ALIGNMENT , 1 ) \n"
Original    (007): ['glPixelStorei', '(', 'GL_UNPACK_ALIGNMENT', ',', '1', ')', '\\n']
Tokenized   (021): ['<s>', 'gl', 'Pixel', 'Store', 'i', '(', 'GL', '_', 'UN', 'P', 'ACK', '_', 'AL', 'IGN', 'MENT', ',', '1', ')', '\\', 'n', '</s>']
Filtered   (019): ['gl', 'Pixel', 'Store', 'i', '(', 'GL', '_', 'UN', 'P', 'ACK', '_', 'AL', 'IGN', 'MENT', ',', '1', ')', '\\', 'n']
Detokenized (007): ['glPixelStorei', '(', 'GL_UNPACK_ALIGNMENT', ',', '1', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "glNormal3fv ( normals [ ni ] ) \n"
Original    (008): ['glNormal3fv', '(', 'normals', '[', 'ni', ']', ')', '\\n']
Tokenized   (016): ['<s>', 'gl', 'Normal', '3', 'f', 'v', '(', 'norm', 'als', '[', 'ni', ']', ')', '\\', 'n', '</s>']
Filtered   (014): ['gl', 'Normal', '3', 'f', 'v', '(', 'norm', 'als', '[', 'ni', ']', ')', '\\', 'n']
Detokenized (008): ['glNormal3fv', '(', 'normals', '[', 'ni', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "picture = pygame . image . load ( picture_file ) . convert ( ) \n"
Original    (015): ['picture', '=', 'pygame', '.', 'image', '.', 'load', '(', 'picture_file', ')', '.', 'convert', '(', ')', '\\n']
Tokenized   (021): ['<s>', 'picture', '=', 'py', 'game', '.', 'image', '.', 'load', '(', 'picture', '_', 'file', ')', '.', 'convert', '(', ')', '\\', 'n', '</s>']
Filtered   (019): ['picture', '=', 'py', 'game', '.', 'image', '.', 'load', '(', 'picture', '_', 'file', ')', '.', 'convert', '(', ')', '\\', 'n']
Detokenized (015): ['picture', '=', 'pygame', '.', 'image', '.', 'load', '(', 'picture_file', ')', '.', 'convert', '(', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "screen . blit ( picture , ( - picture_pos . x , picture_pos . y ) ) \n"
Original    (018): ['screen', '.', 'blit', '(', 'picture', ',', '(', '-', 'picture_pos', '.', 'x', ',', 'picture_pos', '.', 'y', ')', ')', '\\n']
Tokenized   (026): ['<s>', 'screen', '.', 'bl', 'it', '(', 'picture', ',', '(', '-', 'picture', '_', 'pos', '.', 'x', ',', 'picture', '_', 'pos', '.', 'y', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['screen', '.', 'bl', 'it', '(', 'picture', ',', '(', '-', 'picture', '_', 'pos', '.', 'x', ',', 'picture', '_', 'pos', '.', 'y', ')', ')', '\\', 'n']
Detokenized (018): ['screen', '.', 'blit', '(', 'picture', ',', '(', '-', 'picture_pos', '.', 'x', ',', 'picture_pos', '.', 'y', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "time_passed_seconds = time_passed / 1000.0 \n"
Original    (006): ['time_passed_seconds', '=', 'time_passed', '/', '1000.0', '\\n']
Tokenized   (019): ['<s>', 'time', '_', 'pass', 'ed', '_', 'seconds', '=', 'time', '_', 'pass', 'ed', '/', '1000', '.', '0', '\\', 'n', '</s>']
Filtered   (017): ['time', '_', 'pass', 'ed', '_', 'seconds', '=', 'time', '_', 'pass', 'ed', '/', '1000', '.', '0', '\\', 'n']
Detokenized (006): ['time_passed_seconds', '=', 'time_passed', '/', '1000.0', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "picture_pos += scroll_direction * scroll_speed * time_passed_seconds \n"
Original    (008): ['picture_pos', '+=', 'scroll_direction', '*', 'scroll_speed', '*', 'time_passed_seconds', '\\n']
Tokenized   (022): ['<s>', 'picture', '_', 'pos', '+=', 'scroll', '_', 'direction', '*', 'scroll', '_', 'speed', '*', 'time', '_', 'pass', 'ed', '_', 'seconds', '\\', 'n', '</s>']
Filtered   (020): ['picture', '_', 'pos', '+=', 'scroll', '_', 'direction', '*', 'scroll', '_', 'speed', '*', 'time', '_', 'pass', 'ed', '_', 'seconds', '\\', 'n']
Detokenized (008): ['picture_pos', '+=', 'scroll_direction', '*', 'scroll_speed', '*', 'time_passed_seconds', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "VERSION = open ( "version.txt" ) . readline ( ) . strip ( ) \n"
Original    (015): ['VERSION', '=', 'open', '(', '"version.txt"', ')', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '\\n']
Tokenized   (023): ['<s>', 'VERSION', '=', 'open', '(', '"', 'version', '.', 'txt', '"', ')', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '\\', 'n', '</s>']
Filtered   (021): ['VERSION', '=', 'open', '(', '"', 'version', '.', 'txt', '"', ')', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '\\', 'n']
Detokenized (015): ['VERSION', '=', 'open', '(', '"version.txt"', ')', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "DOWNLOAD_URL = DOWNLOAD_BASEURL + "dubbo-client-%s-py2.7.egg" % VERSION \n"
Original    (008): ['DOWNLOAD_URL', '=', 'DOWNLOAD_BASEURL', '+', '"dubbo-client-%s-py2.7.egg"', '%', 'VERSION', '\\n']
Tokenized   (036): ['<s>', 'DOWN', 'LOAD', '_', 'URL', '=', 'DOWN', 'LOAD', '_', 'B', 'ASE', 'URL', '+', '"', 'd', 'ub', 'bo', '-', 'client', '-', '%', 's', '-', 'py', '2', '.', '7', '.', 'egg', '"', '%', 'VERS', 'ION', '\\', 'n', '</s>']
Filtered   (034): ['DOWN', 'LOAD', '_', 'URL', '=', 'DOWN', 'LOAD', '_', 'B', 'ASE', 'URL', '+', '"', 'd', 'ub', 'bo', '-', 'client', '-', '%', 's', '-', 'py', '2', '.', '7', '.', 'egg', '"', '%', 'VERS', 'ION', '\\', 'n']
Detokenized (008): ['DOWNLOAD_URL', '=', 'DOWNLOAD_BASEURL', '+', '"dubbo-client-%s-py2.7.egg"', '%', 'VERSION', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "long_description = open ( "README.md" ) . read ( ) , \n"
Original    (012): ['long_description', '=', 'open', '(', '"README.md"', ')', '.', 'read', '(', ')', ',', '\\n']
Tokenized   (022): ['<s>', 'long', '_', 'description', '=', 'open', '(', '"', 'READ', 'ME', '.', 'md', '"', ')', '.', 'read', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (020): ['long', '_', 'description', '=', 'open', '(', '"', 'READ', 'ME', '.', 'md', '"', ')', '.', 'read', '(', ')', ',', '\\', 'n']
Detokenized (012): ['long_description', '=', 'open', '(', '"README.md"', ')', '.', 'read', '(', ')', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "install_requires = [ "kazoo>=2.0" , "python-jsonrpc>=0.7.3" ] , \n"
Original    (009): ['install_requires', '=', '[', '"kazoo>=2.0"', ',', '"python-jsonrpc>=0.7.3"', ']', ',', '\\n']
Tokenized   (036): ['<s>', 'install', '_', 'requires', '=', '[', '"', 'k', 'az', 'oo', '>', '=', '2', '.', '0', '"', ',', '"', 'python', '-', 'json', 'r', 'pc', '>', '=', '0', '.', '7', '.', '3', '"', ']', ',', '\\', 'n', '</s>']
Filtered   (034): ['install', '_', 'requires', '=', '[', '"', 'k', 'az', 'oo', '>', '=', '2', '.', '0', '"', ',', '"', 'python', '-', 'json', 'r', 'pc', '>', '=', '0', '.', '7', '.', '3', '"', ']', ',', '\\', 'n']
Detokenized (009): ['install_requires', '=', '[', '"kazoo>=2.0"', ',', '"python-jsonrpc>=0.7.3"', ']', ',', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "field = models . BooleanField ( default = False ) , \n"
Original    (012): ['field', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ')', ',', '\\n']
Tokenized   (016): ['<s>', 'field', '=', 'models', '.', 'Boolean', 'Field', '(', 'default', '=', 'False', ')', ',', '\\', 'n', '</s>']
Filtered   (014): ['field', '=', 'models', '.', 'Boolean', 'Field', '(', 'default', '=', 'False', ')', ',', '\\', 'n']
Detokenized (012): ['field', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ')', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "args = [ in_path , user_out_path ] , env = [ "PATH=" + os . environ . get ( "PATH" , "" ) use_sandbox = True , use_nobody = True ) \n"
Original    (032): ['args', '=', '[', 'in_path', ',', 'user_out_path', ']', ',', 'env', '=', '[', '"PATH="', '+', 'os', '.', 'environ', '.', 'get', '(', '"PATH"', ',', '""', ')', 'use_sandbox', '=', 'True', ',', 'use_nobody', '=', 'True', ')', '\\n']
Tokenized   (052): ['<s>', 'args', '=', '[', 'in', '_', 'path', ',', 'user', '_', 'out', '_', 'path', ']', ',', 'env', '=', '[', '"', 'PATH', '="', '+', 'os', '.', 'en', 'viron', '.', 'get', '(', '"', 'PATH', '"', ',', '""', ')', 'use', '_', 'sand', 'box', '=', 'True', ',', 'use', '_', 'nob', 'ody', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (050): ['args', '=', '[', 'in', '_', 'path', ',', 'user', '_', 'out', '_', 'path', ']', ',', 'env', '=', '[', '"', 'PATH', '="', '+', 'os', '.', 'en', 'viron', '.', 'get', '(', '"', 'PATH', '"', ',', '""', ')', 'use', '_', 'sand', 'box', '=', 'True', ',', 'use', '_', 'nob', 'ody', '=', 'True', ')', '\\', 'n']
Detokenized (032): ['args', '=', '[', 'in_path', ',', 'user_out_path', ']', ',', 'env', '=', '[', '"PATH="', '+', 'os', '.', 'environ', '.', 'get', '(', '"PATH"', ',', '""', ')', 'use_sandbox', '=', 'True', ',', 'use_nobody', '=', 'True', ')', '\\n']
Counter: 50
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "print_skip = 5 , * args , ** kwargs ) : \n"
Original    (012): ['print_skip', '=', '5', ',', '*', 'args', ',', '**', 'kwargs', ')', ':', '\\n']
Tokenized   (019): ['<s>', 'print', '_', 'skip', '=', '5', ',', '*', 'args', ',', '**', 'k', 'w', 'args', ')', ':', '\\', 'n', '</s>']
Filtered   (017): ['print', '_', 'skip', '=', '5', ',', '*', 'args', ',', '**', 'k', 'w', 'args', ')', ':', '\\', 'n']
Detokenized (012): ['print_skip', '=', '5', ',', '*', 'args', ',', '**', 'kwargs', ')', ':', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "error = np . max ( np . abs ( new_v - v ) ) \n"
Original    (016): ['error', '=', 'np', '.', 'max', '(', 'np', '.', 'abs', '(', 'new_v', '-', 'v', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'error', '=', 'np', '.', 'max', '(', 'np', '.', 'abs', '(', 'new', '_', 'v', '-', 'v', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['error', '=', 'np', '.', 'max', '(', 'np', '.', 'abs', '(', 'new', '_', 'v', '-', 'v', ')', ')', '\\', 'n']
Detokenized (016): ['error', '=', 'np', '.', 'max', '(', 'np', '.', 'abs', '(', 'new_v', '-', 'v', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "ac = ( a_0 - c ) / 2.0 \n"
Original    (010): ['ac', '=', '(', 'a_0', '-', 'c', ')', '/', '2.0', '\\n']
Tokenized   (017): ['<s>', 'ac', '=', '(', 'a', '_', '0', '-', 'c', ')', '/', '2', '.', '0', '\\', 'n', '</s>']
Filtered   (015): ['ac', '=', '(', 'a', '_', '0', '-', 'c', ')', '/', '2', '.', '0', '\\', 'n']
Detokenized (010): ['ac', '=', '(', 'a_0', '-', 'c', ')', '/', '2.0', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "R = - R \n"
Original    (005): ['R', '=', '-', 'R', '\\n']
Tokenized   (008): ['<s>', 'R', '=', '-', 'R', '\\', 'n', '</s>']
Filtered   (006): ['R', '=', '-', 'R', '\\', 'n']
Detokenized (005): ['R', '=', '-', 'R', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "B = np . array ( [ [ 0. ] , \n"
Original    (012): ['B', '=', 'np', '.', 'array', '(', '[', '[', '0.', ']', ',', '\\n']
Tokenized   (016): ['<s>', 'B', '=', 'np', '.', 'array', '(', '[', '[', '0', '.', ']', ',', '\\', 'n', '</s>']
Filtered   (014): ['B', '=', 'np', '.', 'array', '(', '[', '[', '0', '.', ']', ',', '\\', 'n']
Detokenized (012): ['B', '=', 'np', '.', 'array', '(', '[', '[', '0.', ']', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "Fr , Kr , Pr = self . Fr , self . Kr , self . Pr \n"
Original    (018): ['Fr', ',', 'Kr', ',', 'Pr', '=', 'self', '.', 'Fr', ',', 'self', '.', 'Kr', ',', 'self', '.', 'Pr', '\\n']
Tokenized   (021): ['<s>', 'Fr', ',', 'Kr', ',', 'Pr', '=', 'self', '.', 'Fr', ',', 'self', '.', 'Kr', ',', 'self', '.', 'Pr', '\\', 'n', '</s>']
Filtered   (019): ['Fr', ',', 'Kr', ',', 'Pr', '=', 'self', '.', 'Fr', ',', 'self', '.', 'Kr', ',', 'self', '.', 'Pr', '\\', 'n']
Detokenized (018): ['Fr', ',', 'Kr', ',', 'Pr', '=', 'self', '.', 'Fr', ',', 'self', '.', 'Kr', ',', 'self', '.', 'Pr', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "Fs , Ks , Ps = rblq . robust_rule_simple ( P_init = Pr , tol = 1e-12 ) \n"
Original    (019): ['Fs', ',', 'Ks', ',', 'Ps', '=', 'rblq', '.', 'robust_rule_simple', '(', 'P_init', '=', 'Pr', ',', 'tol', '=', '1e-12', ')', '\\n']
Tokenized   (035): ['<s>', 'Fs', ',', 'K', 's', ',', 'Ps', '=', 'r', 'bl', 'q', '.', 'robust', '_', 'rule', '_', 'simple', '(', 'P', '_', 'init', '=', 'Pr', ',', 'to', 'l', '=', '1', 'e', '-', '12', ')', '\\', 'n', '</s>']
Filtered   (033): ['Fs', ',', 'K', 's', ',', 'Ps', '=', 'r', 'bl', 'q', '.', 'robust', '_', 'rule', '_', 'simple', '(', 'P', '_', 'init', '=', 'Pr', ',', 'to', 'l', '=', '1', 'e', '-', '12', ')', '\\', 'n']
Detokenized (019): ['Fs', ',', 'Ks', ',', 'Ps', '=', 'rblq', '.', 'robust_rule_simple', '(', 'P_init', '=', 'Pr', ',', 'tol', '=', '1e-12', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "Kf , Pf , df , Of , of = rblq . evaluate_F ( Fr ) \n"
Original    (017): ['Kf', ',', 'Pf', ',', 'df', ',', 'Of', ',', 'of', '=', 'rblq', '.', 'evaluate_F', '(', 'Fr', ')', '\\n']
Tokenized   (025): ['<s>', 'K', 'f', ',', 'Pf', ',', 'df', ',', 'Of', ',', 'of', '=', 'r', 'bl', 'q', '.', 'evaluate', '_', 'F', '(', 'Fr', ')', '\\', 'n', '</s>']
Filtered   (023): ['K', 'f', ',', 'Pf', ',', 'df', ',', 'Of', ',', 'of', '=', 'r', 'bl', 'q', '.', 'evaluate', '_', 'F', '(', 'Fr', ')', '\\', 'n']
Detokenized (017): ['Kf', ',', 'Pf', ',', 'df', ',', 'Of', ',', 'of', '=', 'rblq', '.', 'evaluate_F', '(', 'Fr', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "group = h5f . createGroup ( "/" , ) \n"
Original    (010): ['group', '=', 'h5f', '.', 'createGroup', '(', '"/"', ',', ')', '\\n']
Tokenized   (017): ['<s>', 'group', '=', 'h', '5', 'f', '.', 'create', 'Group', '(', '"/', '"', ',', ')', '\\', 'n', '</s>']
Filtered   (015): ['group', '=', 'h', '5', 'f', '.', 'create', 'Group', '(', '"/', '"', ',', ')', '\\', 'n']
Detokenized (010): ['group', '=', 'h5f', '.', 'createGroup', '(', '"/"', ',', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "global ctr \n"
Original    (003): ['global', 'ctr', '\\n']
Tokenized   (007): ['<s>', 'global', 'c', 'tr', '\\', 'n', '</s>']
Filtered   (005): ['global', 'c', 'tr', '\\', 'n']
Detokenized (003): ['global', 'ctr', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "listOfInputPaths . append ( rootdir + "/Raw/Yahoo/US/NYSE/" ) \n"
Original    (009): ['listOfInputPaths', '.', 'append', '(', 'rootdir', '+', '"/Raw/Yahoo/US/NYSE/"', ')', '\\n']
Tokenized   (026): ['<s>', 'list', 'Of', 'Input', 'Path', 's', '.', 'append', '(', 'root', 'dir', '+', '"/', 'Raw', '/', 'Y', 'ahoo', '/', 'US', '/', 'NYSE', '/"', ')', '\\', 'n', '</s>']
Filtered   (024): ['list', 'Of', 'Input', 'Path', 's', '.', 'append', '(', 'root', 'dir', '+', '"/', 'Raw', '/', 'Y', 'ahoo', '/', 'US', '/', 'NYSE', '/"', ')', '\\', 'n']
Detokenized (009): ['listOfInputPaths', '.', 'append', '(', 'rootdir', '+', '"/Raw/Yahoo/US/NYSE/"', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "filtered_names = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , filtered_names ) \n"
Original    (025): ['filtered_names', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'fileExtensionToRemove', ')', ')', '[', '0', ']', ')', ',', 'filtered_names', ')', '\\n']
Tokenized   (037): ['<s>', 'fil', 'tered', '_', 'names', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'file', 'Ext', 'ension', 'To', 'Remove', ')', ')', '[', '0', ']', ')', ',', 'filtered', '_', 'names', ')', '\\', 'n', '</s>']
Filtered   (035): ['fil', 'tered', '_', 'names', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'file', 'Ext', 'ension', 'To', 'Remove', ')', ')', '[', '0', ']', ')', ',', 'filtered', '_', 'names', ')', '\\', 'n']
Detokenized (025): ['filtered_names', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'fileExtensionToRemove', ')', ')', '[', '0', ']', ')', ',', 'filtered_names', ')', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "stock_data = np . loadtxt ( path + stock + ".csv" , np . float , None , "," , None , 1 , use_cols ) \n"
Original    (027): ['stock_data', '=', 'np', '.', 'loadtxt', '(', 'path', '+', 'stock', '+', '".csv"', ',', 'np', '.', 'float', ',', 'None', ',', '","', ',', 'None', ',', '1', ',', 'use_cols', ')', '\\n']
Tokenized   (039): ['<s>', 'stock', '_', 'data', '=', 'np', '.', 'load', 'txt', '(', 'path', '+', 'stock', '+', '".', 'csv', '"', ',', 'np', '.', 'float', ',', 'None', ',', '"', ',"', ',', 'None', ',', '1', ',', 'use', '_', 'col', 's', ')', '\\', 'n', '</s>']
Filtered   (037): ['stock', '_', 'data', '=', 'np', '.', 'load', 'txt', '(', 'path', '+', 'stock', '+', '".', 'csv', '"', ',', 'np', '.', 'float', ',', 'None', ',', '"', ',"', ',', 'None', ',', '1', ',', 'use', '_', 'col', 's', ')', '\\', 'n']
Detokenized (027): ['stock_data', '=', 'np', '.', 'loadtxt', '(', 'path', '+', 'stock', '+', '".csv"', ',', 'np', '.', 'float', ',', 'None', ',', '","', ',', 'None', ',', '1', ',', 'use_cols', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "pkl . dump ( stock_data , f , - 1 ) \n"
Original    (012): ['pkl', '.', 'dump', '(', 'stock_data', ',', 'f', ',', '-', '1', ')', '\\n']
Tokenized   (018): ['<s>', 'p', 'kl', '.', 'dump', '(', 'stock', '_', 'data', ',', 'f', ',', '-', '1', ')', '\\', 'n', '</s>']
Filtered   (016): ['p', 'kl', '.', 'dump', '(', 'stock', '_', 'data', ',', 'f', ',', '-', '1', ')', '\\', 'n']
Detokenized (012): ['pkl', '.', 'dump', '(', 'stock_data', ',', 'f', ',', '-', '1', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "startday = dt . datetime ( t [ 2 ] , t [ 0 ] , t [ 1 ] ) \n"
Original    (022): ['startday', '=', 'dt', '.', 'datetime', '(', 't', '[', '2', ']', ',', 't', '[', '0', ']', ',', 't', '[', '1', ']', ')', '\\n']
Tokenized   (028): ['<s>', 'start', 'day', '=', 'd', 't', '.', 'dat', 'etime', '(', 't', '[', '2', ']', ',', 't', '[', '0', ']', ',', 't', '[', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (026): ['start', 'day', '=', 'd', 't', '.', 'dat', 'etime', '(', 't', '[', '2', ']', ',', 't', '[', '0', ']', ',', 't', '[', '1', ']', ')', '\\', 'n']
Detokenized (022): ['startday', '=', 'dt', '.', 'datetime', '(', 't', '[', '2', ']', ',', 't', '[', '0', ']', ',', 't', '[', '1', ']', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "t = map ( int , sys . argv [ 2 ] . split ( ) ) \n"
Original    (018): ['t', '=', 'map', '(', 'int', ',', 'sys', '.', 'argv', '[', '2', ']', '.', 'split', '(', ')', ')', '\\n']
Tokenized   (022): ['<s>', 't', '=', 'map', '(', 'int', ',', 'sys', '.', 'arg', 'v', '[', '2', ']', '.', 'split', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (020): ['t', '=', 'map', '(', 'int', ',', 'sys', '.', 'arg', 'v', '[', '2', ']', '.', 'split', '(', ')', ')', '\\', 'n']
Detokenized (018): ['t', '=', 'map', '(', 'int', ',', 'sys', '.', 'argv', '[', '2', ']', '.', 'split', '(', ')', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "historic = dataobj . get_data ( timestamps , symbols , "close" ) \n"
Original    (013): ['historic', '=', 'dataobj', '.', 'get_data', '(', 'timestamps', ',', 'symbols', ',', '"close"', ')', '\\n']
Tokenized   (023): ['<s>', 'historic', '=', 'data', 'obj', '.', 'get', '_', 'data', '(', 'tim', 'est', 'amps', ',', 'symbols', ',', '"', 'close', '"', ')', '\\', 'n', '</s>']
Filtered   (021): ['historic', '=', 'data', 'obj', '.', 'get', '_', 'data', '(', 'tim', 'est', 'amps', ',', 'symbols', ',', '"', 'close', '"', ')', '\\', 'n']
Detokenized (013): ['historic', '=', 'dataobj', '.', 'get_data', '(', 'timestamps', ',', 'symbols', ',', '"close"', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "alloc = alloc . append ( DataMatrix ( index = [ historic . index [ date ] ] , data = [ alloc_val ] , columns = [ symbols ~~ alloc [ ] = 1 - alloc [ symbols [ 0 ] ] \n"
Original    (044): ['alloc', '=', 'alloc', '.', 'append', '(', 'DataMatrix', '(', 'index', '=', '[', 'historic', '.', 'index', '[', 'date', ']', ']', ',', 'data', '=', '[', 'alloc_val', ']', ',', 'columns', '=', '[', 'symbols', '~~', 'alloc', '[', ']', '=', '1', '-', 'alloc', '[', 'symbols', '[', '0', ']', ']', '\\n']
Tokenized   (051): ['<s>', 'alloc', '=', 'alloc', '.', 'append', '(', 'Data', 'Matrix', '(', 'index', '=', '[', 'historic', '.', 'index', '[', 'date', ']', ']', ',', 'data', '=', '[', 'alloc', '_', 'val', ']', ',', 'columns', '=', '[', 'symbols', '', '~~', 'alloc', '[', ']', '=', '1', '-', 'alloc', '[', 'symbols', '[', '0', ']', ']', '\\', 'n', '</s>']
Filtered   (049): ['alloc', '=', 'alloc', '.', 'append', '(', 'Data', 'Matrix', '(', 'index', '=', '[', 'historic', '.', 'index', '[', 'date', ']', ']', ',', 'data', '=', '[', 'alloc', '_', 'val', ']', ',', 'columns', '=', '[', 'symbols', '', '~~', 'alloc', '[', ']', '=', '1', '-', 'alloc', '[', 'symbols', '[', '0', ']', ']', '\\', 'n']
Detokenized (044): ['alloc', '=', 'alloc', '.', 'append', '(', 'DataMatrix', '(', 'index', '=', '[', 'historic', '.', 'index', '[', 'date', ']', ']', ',', 'data', '=', '[', 'alloc_val', ']', ',', 'columns', '=', '[', 'symbols', '~~', 'alloc', '[', ']', '=', '1', '-', 'alloc', '[', 'symbols', '[', '0', ']', ']', '\\n']
Counter: 49
===================================================================
Hidden states:  (13, 44, 768)
# Extracted words:  44
Sentence         : "output = open ( sys . argv [ 3 ] , "wb" ) \n"
Original    (014): ['output', '=', 'open', '(', 'sys', '.', 'argv', '[', '3', ']', ',', '"wb"', ')', '\\n']
Tokenized   (020): ['<s>', 'output', '=', 'open', '(', 'sys', '.', 'arg', 'v', '[', '3', ']', ',', '"', 'wb', '"', ')', '\\', 'n', '</s>']
Filtered   (018): ['output', '=', 'open', '(', 'sys', '.', 'arg', 'v', '[', '3', ']', ',', '"', 'wb', '"', ')', '\\', 'n']
Detokenized (014): ['output', '=', 'open', '(', 'sys', '.', 'argv', '[', '3', ']', ',', '"wb"', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "stocksAtThisPath = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , stocksAtThisPath \n"
Original    (024): ['stocksAtThisPath', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'fileExtensionToRemove', ')', ')', '[', '0', ']', ')', ',', 'stocksAtThisPath', '\\n']
Tokenized   (037): ['<s>', 'stocks', 'At', 'This', 'Path', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'file', 'Ext', 'ension', 'To', 'Remove', ')', ')', '[', '0', ']', ')', ',', 'stocks', 'At', 'This', 'Path', '\\', 'n', '</s>']
Filtered   (035): ['stocks', 'At', 'This', 'Path', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'file', 'Ext', 'ension', 'To', 'Remove', ')', ')', '[', '0', ']', ')', ',', 'stocks', 'At', 'This', 'Path', '\\', 'n']
Detokenized (024): ['stocksAtThisPath', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'fileExtensionToRemove', ')', ')', '[', '0', ']', ')', ',', 'stocksAtThisPath', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "@ memoize_default ( None , evaluator_is_first_arg = True ) \n"
Original    (010): ['@', 'memoize_default', '(', 'None', ',', 'evaluator_is_first_arg', '=', 'True', ')', '\\n']
Tokenized   (024): ['<s>', '@', 'memo', 'ize', '_', 'default', '(', 'None', ',', 'eval', 'u', 'ator', '_', 'is', '_', 'first', '_', 'arg', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (022): ['@', 'memo', 'ize', '_', 'default', '(', 'None', ',', 'eval', 'u', 'ator', '_', 'is', '_', 'first', '_', 'arg', '=', 'True', ')', '\\', 'n']
Detokenized (010): ['@', 'memoize_default', '(', 'None', ',', 'evaluator_is_first_arg', '=', 'True', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "param_str = _search_param_in_docstr ( func . raw_doc , str ( param . get_name ( ) ) ) \n"
Original    (018): ['param_str', '=', '_search_param_in_docstr', '(', 'func', '.', 'raw_doc', ',', 'str', '(', 'param', '.', 'get_name', '(', ')', ')', ')', '\\n']
Tokenized   (035): ['<s>', 'param', '_', 'str', '=', '_', 'search', '_', 'param', '_', 'in', '_', 'doc', 'str', '(', 'func', '.', 'raw', '_', 'doc', ',', 'str', '(', 'param', '.', 'get', '_', 'name', '(', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (033): ['param', '_', 'str', '=', '_', 'search', '_', 'param', '_', 'in', '_', 'doc', 'str', '(', 'func', '.', 'raw', '_', 'doc', ',', 'str', '(', 'param', '.', 'get', '_', 'name', '(', ')', ')', ')', '\\', 'n']
Detokenized (018): ['param_str', '=', '_search_param_in_docstr', '(', 'func', '.', 'raw_doc', ',', 'str', '(', 'param', '.', 'get_name', '(', ')', ')', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "patterns = [ re . compile ( p % re . escape ( param_str ) ) \n"
Original    (017): ['patterns', '=', '[', 're', '.', 'compile', '(', 'p', '%', 're', '.', 'escape', '(', 'param_str', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'pattern', 's', '=', '[', 're', '.', 'compile', '(', 'p', '%', 're', '.', 'escape', '(', 'param', '_', 'str', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['pattern', 's', '=', '[', 're', '.', 'compile', '(', 'p', '%', 're', '.', 'escape', '(', 'param', '_', 'str', ')', ')', '\\', 'n']
Detokenized (017): ['patterns', '=', '[', 're', '.', 'compile', '(', 'p', '%', 're', '.', 'escape', '(', 'param_str', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "it = ( evaluator . execute ( d ) for d in definitions ) \n"
Original    (015): ['it', '=', '(', 'evaluator', '.', 'execute', '(', 'd', ')', 'for', 'd', 'in', 'definitions', ')', '\\n']
Tokenized   (020): ['<s>', 'it', '=', '(', 'eval', 'u', 'ator', '.', 'execute', '(', 'd', ')', 'for', 'd', 'in', 'definitions', ')', '\\', 'n', '</s>']
Filtered   (018): ['it', '=', '(', 'eval', 'u', 'ator', '.', 'execute', '(', 'd', ')', 'for', 'd', 'in', 'definitions', ')', '\\', 'n']
Detokenized (015): ['it', '=', '(', 'evaluator', '.', 'execute', '(', 'd', ')', 'for', 'd', 'in', 'definitions', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "tok = parsed . module . subscopes [ 0 ] . statements [ 0 ] . _token_list [ 2 ] \n"
Original    (021): ['tok', '=', 'parsed', '.', 'module', '.', 'subscopes', '[', '0', ']', '.', 'statements', '[', '0', ']', '.', '_token_list', '[', '2', ']', '\\n']
Tokenized   (029): ['<s>', 't', 'ok', '=', 'parsed', '.', 'module', '.', 'subsc', 'opes', '[', '0', ']', '.', 'statements', '[', '0', ']', '.', '_', 'token', '_', 'list', '[', '2', ']', '\\', 'n', '</s>']
Filtered   (027): ['t', 'ok', '=', 'parsed', '.', 'module', '.', 'subsc', 'opes', '[', '0', ']', '.', 'statements', '[', '0', ']', '.', '_', 'token', '_', 'list', '[', '2', ']', '\\', 'n']
Detokenized (021): ['tok', '=', 'parsed', '.', 'module', '.', 'subscopes', '[', '0', ']', '.', 'statements', '[', '0', ']', '.', '_token_list', '[', '2', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "__slots__ = ( "graphVariable" , \n"
Original    (006): ['__slots__', '=', '(', '"graphVariable"', ',', '\\n']
Tokenized   (015): ['<s>', '__', 'sl', 'ots', '__', '=', '(', '"', 'graph', 'Variable', '"', ',', '\\', 'n', '</s>']
Filtered   (013): ['__', 'sl', 'ots', '__', '=', '(', '"', 'graph', 'Variable', '"', ',', '\\', 'n']
Detokenized (006): ['__slots__', '=', '(', '"graphVariable"', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "( None , \n"
Original    (004): ['(', 'None', ',', '\\n']
Tokenized   (007): ['<s>', '(', 'None', ',', '\\', 'n', '</s>']
Filtered   (005): ['(', 'None', ',', '\\', 'n']
Detokenized (004): ['(', 'None', ',', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "def __init__ ( self , patterns = [ ] , prolog = None ) : \n"
Original    (016): ['def', '__init__', '(', 'self', ',', 'patterns', '=', '[', ']', ',', 'prolog', '=', 'None', ')', ':', '\\n']
Tokenized   (022): ['<s>', 'def', '__', 'init', '__', '(', 'self', ',', 'patterns', '=', '[', ']', ',', 'pro', 'log', '=', 'None', ')', ':', '\\', 'n', '</s>']
Filtered   (020): ['def', '__', 'init', '__', '(', 'self', ',', 'patterns', '=', '[', ']', ',', 'pro', 'log', '=', 'None', ')', ':', '\\', 'n']
Detokenized (016): ['def', '__init__', '(', 'self', ',', 'patterns', '=', '[', ']', ',', 'prolog', '=', 'None', ')', ':', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "return term . n3 ( ) \n"
Original    (007): ['return', 'term', '.', 'n3', '(', ')', '\\n']
Tokenized   (011): ['<s>', 'return', 'term', '.', 'n', '3', '(', ')', '\\', 'n', '</s>']
Filtered   (009): ['return', 'term', '.', 'n', '3', '(', ')', '\\', 'n']
Detokenized (007): ['return', 'term', '.', 'n3', '(', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ". join ( [ + . join ( [ \n"
Original    (010): ['.', 'join', '(', '[', '+', '.', 'join', '(', '[', '\\n']
Tokenized   (013): ['<s>', '.', 'join', '(', '[', '+', '.', 'join', '(', '[', '\\', 'n', '</s>']
Filtered   (011): ['.', 'join', '(', '[', '+', '.', 'join', '(', '[', '\\', 'n']
Detokenized (010): ['.', 'join', '(', '[', '+', '.', 'join', '(', '[', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "[ ( "a" , "?b" , 24 ) , ( "?r" , "?c" , 12345 ) , ( v1 , "?c" , 3333 ) , ( u1 , "?c" , 9999 ) ] ) \n"
Original    (035): ['[', '(', '"a"', ',', '"?b"', ',', '24', ')', ',', '(', '"?r"', ',', '"?c"', ',', '12345', ')', ',', '(', 'v1', ',', '"?c"', ',', '3333', ')', ',', '(', 'u1', ',', '"?c"', ',', '9999', ')', ']', ')', '\\n']
Tokenized   (060): ['<s>', '[', '(', '"', 'a', '"', ',', '"', '?', 'b', '"', ',', '24', ')', ',', '(', '"', '?', 'r', '"', ',', '"', '?', 'c', '"', ',', '123', '45', ')', ',', '(', 'v', '1', ',', '"', '?', 'c', '"', ',', '3', '333', ')', ',', '(', 'u', '1', ',', '"', '?', 'c', '"', ',', '9', '999', ')', ']', ')', '\\', 'n', '</s>']
Filtered   (058): ['[', '(', '"', 'a', '"', ',', '"', '?', 'b', '"', ',', '24', ')', ',', '(', '"', '?', 'r', '"', ',', '"', '?', 'c', '"', ',', '123', '45', ')', ',', '(', 'v', '1', ',', '"', '?', 'c', '"', ',', '3', '333', ')', ',', '(', 'u', '1', ',', '"', '?', 'c', '"', ',', '9', '999', ')', ']', ')', '\\', 'n']
Detokenized (035): ['[', '(', '"a"', ',', '"?b"', ',', '24', ')', ',', '(', '"?r"', ',', '"?c"', ',', '12345', ')', ',', '(', 'v1', ',', '"?c"', ',', '3333', ')', ',', '(', 'u1', ',', '"?c"', ',', '9999', ')', ']', ')', '\\n']
Counter: 58
===================================================================
Hidden states:  (13, 35, 768)
# Extracted words:  35
Sentence         : "unittest . TextTestRunner ( verbosity = 3 ) . run ( suite ) \n"
Original    (014): ['unittest', '.', 'TextTestRunner', '(', 'verbosity', '=', '3', ')', '.', 'run', '(', 'suite', ')', '\\n']
Tokenized   (022): ['<s>', 'un', 'itt', 'est', '.', 'Text', 'Test', 'Runner', '(', 'verb', 'osity', '=', '3', ')', '.', 'run', '(', 'suite', ')', '\\', 'n', '</s>']
Filtered   (020): ['un', 'itt', 'est', '.', 'Text', 'Test', 'Runner', '(', 'verb', 'osity', '=', '3', ')', '.', 'run', '(', 'suite', ')', '\\', 'n']
Detokenized (014): ['unittest', '.', 'TextTestRunner', '(', 'verbosity', '=', '3', ')', '.', 'run', '(', 'suite', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : ") . parse ( "http://www.w3.org/People/Berners-Lee/card.rdf" ) \n"
Original    (007): [')', '.', 'parse', '(', '"http://www.w3.org/People/Berners-Lee/card.rdf"', ')', '\\n']
Tokenized   (031): ['<s>', ')', '.', 'parse', '(', '"', 'http', '://', 'www', '.', 'w', '3', '.', 'org', '/', 'People', '/', 'Bern', 'ers', '-', 'Lee', '/', 'card', '.', 'rd', 'f', '"', ')', '\\', 'n', '</s>']
Filtered   (029): [')', '.', 'parse', '(', '"', 'http', '://', 'www', '.', 'w', '3', '.', 'org', '/', 'People', '/', 'Bern', 'ers', '-', 'Lee', '/', 'card', '.', 'rd', 'f', '"', ')', '\\', 'n']
Detokenized (007): [')', '.', 'parse', '(', '"http://www.w3.org/People/Berners-Lee/card.rdf"', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "graph . get_context ( URIRef ( ) \n"
Original    (008): ['graph', '.', 'get_context', '(', 'URIRef', '(', ')', '\\n']
Tokenized   (015): ['<s>', 'graph', '.', 'get', '_', 'context', '(', 'UR', 'IR', 'ef', '(', ')', '\\', 'n', '</s>']
Filtered   (013): ['graph', '.', 'get', '_', 'context', '(', 'UR', 'IR', 'ef', '(', ')', '\\', 'n']
Detokenized (008): ['graph', '.', 'get_context', '(', 'URIRef', '(', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "bob . set ( FOAF . name , Literal ( "Bob" ) ) \n"
Original    (014): ['bob', '.', 'set', '(', 'FOAF', '.', 'name', ',', 'Literal', '(', '"Bob"', ')', ')', '\\n']
Tokenized   (022): ['<s>', 'b', 'ob', '.', 'set', '(', 'FO', 'AF', '.', 'name', ',', 'Lit', 'eral', '(', '"', 'Bob', '"', ')', ')', '\\', 'n', '</s>']
Filtered   (020): ['b', 'ob', '.', 'set', '(', 'FO', 'AF', '.', 'name', ',', 'Lit', 'eral', '(', '"', 'Bob', '"', ')', ')', '\\', 'n']
Detokenized (014): ['bob', '.', 'set', '(', 'FOAF', '.', 'name', ',', 'Literal', '(', '"Bob"', ')', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "print g . serialize ( format = ) \n"
Original    (009): ['print', 'g', '.', 'serialize', '(', 'format', '=', ')', '\\n']
Tokenized   (013): ['<s>', 'print', 'g', '.', 'serial', 'ize', '(', 'format', '=', ')', '\\', 'n', '</s>']
Filtered   (011): ['print', 'g', '.', 'serial', 'ize', '(', 'format', '=', ')', '\\', 'n']
Detokenized (009): ['print', 'g', '.', 'serialize', '(', 'format', '=', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "context = self . uriref ( ) or self . nodeid ( ) or self . sink . identifier \n"
Original    (020): ['context', '=', 'self', '.', 'uriref', '(', ')', 'or', 'self', '.', 'nodeid', '(', ')', 'or', 'self', '.', 'sink', '.', 'identifier', '\\n']
Tokenized   (026): ['<s>', 'context', '=', 'self', '.', 'ur', 'ire', 'f', '(', ')', 'or', 'self', '.', 'node', 'id', '(', ')', 'or', 'self', '.', 'sink', '.', 'identifier', '\\', 'n', '</s>']
Filtered   (024): ['context', '=', 'self', '.', 'ur', 'ire', 'f', '(', ')', 'or', 'self', '.', 'node', 'id', '(', ')', 'or', 'self', '.', 'sink', '.', 'identifier', '\\', 'n']
Detokenized (020): ['context', '=', 'self', '.', 'uriref', '(', ')', 'or', 'self', '.', 'nodeid', '(', ')', 'or', 'self', '.', 'sink', '.', 'identifier', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "from rdflib . query import Result , ResultSerializer , ResultParser \n"
Original    (011): ['from', 'rdflib', '.', 'query', 'import', 'Result', ',', 'ResultSerializer', ',', 'ResultParser', '\\n']
Tokenized   (020): ['<s>', 'from', 'r', 'd', 'fl', 'ib', '.', 'query', 'import', 'Result', ',', 'Result', 'Serial', 'izer', ',', 'Result', 'Parser', '\\', 'n', '</s>']
Filtered   (018): ['from', 'r', 'd', 'fl', 'ib', '.', 'query', 'import', 'Result', ',', 'Result', 'Serial', 'izer', ',', 'Result', 'Parser', '\\', 'n']
Detokenized (011): ['from', 'rdflib', '.', 'query', 'import', 'Result', ',', 'ResultSerializer', ',', 'ResultParser', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "class CSVResultParser ( ResultParser ) : \n"
Original    (007): ['class', 'CSVResultParser', '(', 'ResultParser', ')', ':', '\\n']
Tokenized   (013): ['<s>', 'class', 'CSV', 'Result', 'Parser', '(', 'Result', 'Parser', ')', ':', '\\', 'n', '</s>']
Filtered   (011): ['class', 'CSV', 'Result', 'Parser', '(', 'Result', 'Parser', ')', ':', '\\', 'n']
Detokenized (007): ['class', 'CSVResultParser', '(', 'ResultParser', ')', ':', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "r . bindings = [ ] \n"
Original    (007): ['r', '.', 'bindings', '=', '[', ']', '\\n']
Tokenized   (010): ['<s>', 'r', '.', 'bindings', '=', '[', ']', '\\', 'n', '</s>']
Filtered   (008): ['r', '.', 'bindings', '=', '[', ']', '\\', 'n']
Detokenized (007): ['r', '.', 'bindings', '=', '[', ']', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "if result . type != "SELECT" : \n"
Original    (008): ['if', 'result', '.', 'type', '!=', '"SELECT"', ':', '\\n']
Tokenized   (013): ['<s>', 'if', 'result', '.', 'type', '!=', '"', 'SELECT', '"', ':', '\\', 'n', '</s>']
Filtered   (011): ['if', 'result', '.', 'type', '!=', '"', 'SELECT', '"', ':', '\\', 'n']
Detokenized (008): ['if', 'result', '.', 'type', '!=', '"SELECT"', ':', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "stream = codecs . getwriter ( encoding ) ( stream ) \n"
Original    (012): ['stream', '=', 'codecs', '.', 'getwriter', '(', 'encoding', ')', '(', 'stream', ')', '\\n']
Tokenized   (017): ['<s>', 'stream', '=', 'codec', 's', '.', 'get', 'writer', '(', 'encoding', ')', '(', 'stream', ')', '\\', 'n', '</s>']
Filtered   (015): ['stream', '=', 'codec', 's', '.', 'get', 'writer', '(', 'encoding', ')', '(', 'stream', ')', '\\', 'n']
Detokenized (012): ['stream', '=', 'codecs', '.', 'getwriter', '(', 'encoding', ')', '(', 'stream', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "vs = [ self . serializeTerm ( v , encoding ) for v in self . result . vars ] \n"
Original    (021): ['vs', '=', '[', 'self', '.', 'serializeTerm', '(', 'v', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'vars', ']', '\\n']
Tokenized   (027): ['<s>', 'vs', '=', '[', 'self', '.', 'serial', 'ize', 'Term', '(', 'v', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'v', 'ars', ']', '\\', 'n', '</s>']
Filtered   (025): ['vs', '=', '[', 'self', '.', 'serial', 'ize', 'Term', '(', 'v', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'v', 'ars', ']', '\\', 'n']
Detokenized (021): ['vs', '=', '[', 'self', '.', 'serializeTerm', '(', 'v', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'vars', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "for row in self . result . bindings : \n"
Original    (010): ['for', 'row', 'in', 'self', '.', 'result', '.', 'bindings', ':', '\\n']
Tokenized   (013): ['<s>', 'for', 'row', 'in', 'self', '.', 'result', '.', 'bindings', ':', '\\', 'n', '</s>']
Filtered   (011): ['for', 'row', 'in', 'self', '.', 'result', '.', 'bindings', ':', '\\', 'n']
Detokenized (010): ['for', 'row', 'in', 'self', '.', 'result', '.', 'bindings', ':', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "row . get ( v ) , encoding ) for v in self . result . vars ] ) \n"
Original    (020): ['row', '.', 'get', '(', 'v', ')', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'vars', ']', ')', '\\n']
Tokenized   (024): ['<s>', 'row', '.', 'get', '(', 'v', ')', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'v', 'ars', ']', ')', '\\', 'n', '</s>']
Filtered   (022): ['row', '.', 'get', '(', 'v', ')', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'v', 'ars', ']', ')', '\\', 'n']
Detokenized (020): ['row', '.', 'get', '(', 'v', ')', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'vars', ']', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "try : import nose \n"
Original    (005): ['try', ':', 'import', 'nose', '\\n']
Tokenized   (008): ['<s>', 'try', ':', 'import', 'nose', '\\', 'n', '</s>']
Filtered   (006): ['try', ':', 'import', 'nose', '\\', 'n']
Detokenized (005): ['try', ':', 'import', 'nose', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "~~~ argv += DEFAULT_DIRS \n"
Original    (005): ['~~~', 'argv', '+=', 'DEFAULT_DIRS', '\\n']
Tokenized   (014): ['<s>', '~~', '~', 'arg', 'v', '+=', 'DE', 'FAULT', '_', 'DIR', 'S', '\\', 'n', '</s>']
Filtered   (012): ['~~', '~', 'arg', 'v', '+=', 'DE', 'FAULT', '_', 'DIR', 'S', '\\', 'n']
Detokenized (005): ['~~~', 'argv', '+=', 'DEFAULT_DIRS', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "nose . run_exit ( argv = finalArgs ) \n"
Original    (009): ['nose', '.', 'run_exit', '(', 'argv', '=', 'finalArgs', ')', '\\n']
Tokenized   (017): ['<s>', 'n', 'ose', '.', 'run', '_', 'exit', '(', 'arg', 'v', '=', 'final', 'Args', ')', '\\', 'n', '</s>']
Filtered   (015): ['n', 'ose', '.', 'run', '_', 'exit', '(', 'arg', 'v', '=', 'final', 'Args', ')', '\\', 'n']
Detokenized (009): ['nose', '.', 'run_exit', '(', 'argv', '=', 'finalArgs', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Loading model: microsoft/graphcodebert-base
Reading input corpus
Preparing output file
Extracting representations from model
Sentence         : "\n"
Original    (001): ['\\n']
Tokenized   (004): ['<s>', '\\', 'n', '</s>']
Filtered   (002): ['\\', 'n']
Detokenized (001): ['\\n']
Counter: 2
===================================================================
Hidden states:  (13, 1, 768)
# Extracted words:  1
Sentence         : "# \n"
Original    (002): ['#', '\\n']
Tokenized   (005): ['<s>', '#', '\\', 'n', '</s>']
Filtered   (003): ['#', '\\', 'n']
Detokenized (002): ['#', '\\n']
Counter: 3
===================================================================
Hidden states:  (13, 2, 768)
# Extracted words:  2
Sentence         : "template_name = \n"
Original    (003): ['template_name', '=', '\\n']
Tokenized   (008): ['<s>', 'template', '_', 'name', '=', '\\', 'n', '</s>']
Filtered   (006): ['template', '_', 'name', '=', '\\', 'n']
Detokenized (003): ['template_name', '=', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "slug = "policy_profile" \n"
Original    (004): ['slug', '=', '"policy_profile"', '\\n']
Tokenized   (012): ['<s>', 'sl', 'ug', '=', '"', 'policy', '_', 'profile', '"', '\\', 'n', '</s>']
Filtered   (010): ['sl', 'ug', '=', '"', 'policy', '_', 'profile', '"', '\\', 'n']
Detokenized (004): ['slug', '=', '"policy_profile"', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "preload = False \n"
Original    (004): ['preload', '=', 'False', '\\n']
Tokenized   (008): ['<s>', 'pre', 'load', '=', 'False', '\\', 'n', '</s>']
Filtered   (006): ['pre', 'load', '=', 'False', '\\', 'n']
Detokenized (004): ['preload', '=', 'False', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "tabs = ( NetworkProfileTab , PolicyProfileTab ) \n"
Original    (008): ['tabs', '=', '(', 'NetworkProfileTab', ',', 'PolicyProfileTab', ')', '\\n']
Tokenized   (016): ['<s>', 'tab', 's', '=', '(', 'Network', 'Profile', 'Tab', ',', 'Policy', 'Profile', 'Tab', ')', '\\', 'n', '</s>']
Filtered   (014): ['tab', 's', '=', '(', 'Network', 'Profile', 'Tab', ',', 'Policy', 'Profile', 'Tab', ')', '\\', 'n']
Detokenized (008): ['tabs', '=', '(', 'NetworkProfileTab', ',', 'PolicyProfileTab', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "weak_store = WeakLocal ( ) \n"
Original    (006): ['weak_store', '=', 'WeakLocal', '(', ')', '\\n']
Tokenized   (012): ['<s>', 'weak', '_', 'store', '=', 'Weak', 'Local', '(', ')', '\\', 'n', '</s>']
Filtered   (010): ['weak', '_', 'store', '=', 'Weak', 'Local', '(', ')', '\\', 'n']
Detokenized (006): ['weak_store', '=', 'WeakLocal', '(', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "strong_store = corolocal . local \n"
Original    (006): ['strong_store', '=', 'corolocal', '.', 'local', '\\n']
Tokenized   (013): ['<s>', 'strong', '_', 'store', '=', 'cor', 'ol', 'ocal', '.', 'local', '\\', 'n', '</s>']
Filtered   (011): ['strong', '_', 'store', '=', 'cor', 'ol', 'ocal', '.', 'local', '\\', 'n']
Detokenized (006): ['strong_store', '=', 'corolocal', '.', 'local', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "eventlet . monkey_patch ( ) \n"
Original    (006): ['eventlet', '.', 'monkey_patch', '(', ')', '\\n']
Tokenized   (012): ['<s>', 'event', 'let', '.', 'monkey', '_', 'patch', '(', ')', '\\', 'n', '</s>']
Filtered   (010): ['event', 'let', '.', 'monkey', '_', 'patch', '(', ')', '\\', 'n']
Detokenized (006): ['eventlet', '.', 'monkey_patch', '(', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "CONF . register_opts ( impl_zmq . zmq_opts ) \n"
Original    (009): ['CONF', '.', 'register_opts', '(', 'impl_zmq', '.', 'zmq_opts', ')', '\\n']
Tokenized   (025): ['<s>', 'CON', 'F', '.', 'register', '_', 'op', 'ts', '(', 'impl', '_', 'z', 'm', 'q', '.', 'z', 'm', 'q', '_', 'op', 'ts', ')', '\\', 'n', '</s>']
Filtered   (023): ['CON', 'F', '.', 'register', '_', 'op', 'ts', '(', 'impl', '_', 'z', 'm', 'q', '.', 'z', 'm', 'q', '_', 'op', 'ts', ')', '\\', 'n']
Detokenized (009): ['CONF', '.', 'register_opts', '(', 'impl_zmq', '.', 'zmq_opts', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "vpnservices_dict = { : self . api_vpnservices . list ( ) } \n"
Original    (013): ['vpnservices_dict', '=', '{', ':', 'self', '.', 'api_vpnservices', '.', 'list', '(', ')', '}', '\\n']
Tokenized   (024): ['<s>', 'v', 'pn', 'services', '_', 'dict', '=', '{', ':', 'self', '.', 'api', '_', 'v', 'pn', 'services', '.', 'list', '(', ')', '}', '\\', 'n', '</s>']
Filtered   (022): ['v', 'pn', 'services', '_', 'dict', '=', '{', ':', 'self', '.', 'api', '_', 'v', 'pn', 'services', '.', 'list', '(', ')', '}', '\\', 'n']
Detokenized (013): ['vpnservices_dict', '=', '{', ':', 'self', '.', 'api_vpnservices', '.', 'list', '(', ')', '}', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "vpnservice [ ] [ ] ) \n"
Original    (007): ['vpnservice', '[', ']', '[', ']', ')', '\\n']
Tokenized   (012): ['<s>', 'v', 'pn', 'service', '[', ']', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (010): ['v', 'pn', 'service', '[', ']', '[', ']', ')', '\\', 'n']
Detokenized (007): ['vpnservice', '[', ']', '[', ']', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "form_data } ) . AndReturn ( ipsecsiteconnection ) \n"
Original    (009): ['form_data', '}', ')', '.', 'AndReturn', '(', 'ipsecsiteconnection', ')', '\\n']
Tokenized   (018): ['<s>', 'form', '_', 'data', '}', ')', '.', 'And', 'Return', '(', 'ip', 'sec', 'site', 'connection', ')', '\\', 'n', '</s>']
Filtered   (016): ['form', '_', 'data', '}', ')', '.', 'And', 'Return', '(', 'ip', 'sec', 'site', 'connection', ')', '\\', 'n']
Detokenized (009): ['form_data', '}', ')', '.', 'AndReturn', '(', 'ipsecsiteconnection', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "ipsecsiteconnections_dict ) \n"
Original    (003): ['ipsecsiteconnections_dict', ')', '\\n']
Tokenized   (012): ['<s>', 'ip', 'sec', 'site', 'connect', 'ions', '_', 'dict', ')', '\\', 'n', '</s>']
Filtered   (010): ['ip', 'sec', 'site', 'connect', 'ions', '_', 'dict', ')', '\\', 'n']
Detokenized (003): ['ipsecsiteconnections_dict', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "ipsecsiteconnections [ ] ) : \n"
Original    (006): ['ipsecsiteconnections', '[', ']', ')', ':', '\\n']
Tokenized   (013): ['<s>', 'ip', 'sec', 'site', 'connect', 'ions', '[', ']', ')', ':', '\\', 'n', '</s>']
Filtered   (011): ['ip', 'sec', 'site', 'connect', 'ions', '[', ']', ')', ':', '\\', 'n']
Detokenized (006): ['ipsecsiteconnections', '[', ']', ')', ':', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "neutronclient . show_ipsec_site_connection ( \n"
Original    (005): ['neutronclient', '.', 'show_ipsec_site_connection', '(', '\\n']
Tokenized   (018): ['<s>', 'ne', 'ut', 'ron', 'client', '.', 'show', '_', 'ip', 'sec', '_', 'site', '_', 'connection', '(', '\\', 'n', '</s>']
Filtered   (016): ['ne', 'ut', 'ron', 'client', '.', 'show', '_', 'ip', 'sec', '_', 'site', '_', 'connection', '(', '\\', 'n']
Detokenized (005): ['neutronclient', '.', 'show_ipsec_site_connection', '(', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "ret_val = api . vpn . ipsecsiteconnection_get ( self . request , \n"
Original    (013): ['ret_val', '=', 'api', '.', 'vpn', '.', 'ipsecsiteconnection_get', '(', 'self', '.', 'request', ',', '\\n']
Tokenized   (024): ['<s>', 'ret', '_', 'val', '=', 'api', '.', 'v', 'pn', '.', 'ip', 'sec', 'site', 'connection', '_', 'get', '(', 'self', '.', 'request', ',', '\\', 'n', '</s>']
Filtered   (022): ['ret', '_', 'val', '=', 'api', '.', 'v', 'pn', '.', 'ip', 'sec', 'site', 'connection', '_', 'get', '(', 'self', '.', 'request', ',', '\\', 'n']
Detokenized (013): ['ret_val', '=', 'api', '.', 'vpn', '.', 'ipsecsiteconnection_get', '(', 'self', '.', 'request', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "response_kwargs . setdefault ( "filename" , "usage.csv" ) \n"
Original    (009): ['response_kwargs', '.', 'setdefault', '(', '"filename"', ',', '"usage.csv"', ')', '\\n']
Tokenized   (022): ['<s>', 'response', '_', 'kw', 'args', '.', 'set', 'default', '(', '"', 'filename', '"', ',', '"', 'usage', '.', 'csv', '"', ')', '\\', 'n', '</s>']
Filtered   (020): ['response', '_', 'kw', 'args', '.', 'set', 'default', '(', '"', 'filename', '"', ',', '"', 'usage', '.', 'csv', '"', ')', '\\', 'n']
Detokenized (009): ['response_kwargs', '.', 'setdefault', '(', '"filename"', ',', '"usage.csv"', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "BlendProbes = 1 \n"
Original    (004): ['BlendProbes', '=', '1', '\\n']
Tokenized   (010): ['<s>', 'Bl', 'end', 'Pro', 'bes', '=', '1', '\\', 'n', '</s>']
Filtered   (008): ['Bl', 'end', 'Pro', 'bes', '=', '1', '\\', 'n']
Detokenized (004): ['BlendProbes', '=', '1', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "lightmap_index = field ( "m_LightmapIndex" ) \n"
Original    (007): ['lightmap_index', '=', 'field', '(', '"m_LightmapIndex"', ')', '\\n']
Tokenized   (019): ['<s>', 'light', 'map', '_', 'index', '=', 'field', '(', '"', 'm', '_', 'Light', 'map', 'Index', '"', ')', '\\', 'n', '</s>']
Filtered   (017): ['light', 'map', '_', 'index', '=', 'field', '(', '"', 'm', '_', 'Light', 'map', 'Index', '"', ')', '\\', 'n']
Detokenized (007): ['lightmap_index', '=', 'field', '(', '"m_LightmapIndex"', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "receive_shadows = field ( "m_ReceiveShadows" , bool ) \n"
Original    (009): ['receive_shadows', '=', 'field', '(', '"m_ReceiveShadows"', ',', 'bool', ')', '\\n']
Tokenized   (023): ['<s>', 're', 'ceive', '_', 'sh', 'adows', '=', 'field', '(', '"', 'm', '_', 'Re', 'ceive', 'Sh', 'adows', '"', ',', 'bool', ')', '\\', 'n', '</s>']
Filtered   (021): ['re', 'ceive', '_', 'sh', 'adows', '=', 'field', '(', '"', 'm', '_', 'Re', 'ceive', 'Sh', 'adows', '"', ',', 'bool', ')', '\\', 'n']
Detokenized (009): ['receive_shadows', '=', 'field', '(', '"m_ReceiveShadows"', ',', 'bool', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "Config . parser . readfp ( sconf ) \n"
Original    (009): ['Config', '.', 'parser', '.', 'readfp', '(', 'sconf', ')', '\\n']
Tokenized   (015): ['<s>', 'Config', '.', 'parser', '.', 'read', 'fp', '(', 'sc', 'on', 'f', ')', '\\', 'n', '</s>']
Filtered   (013): ['Config', '.', 'parser', '.', 'read', 'fp', '(', 'sc', 'on', 'f', ')', '\\', 'n']
Detokenized (009): ['Config', '.', 'parser', '.', 'readfp', '(', 'sconf', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "BBS_XMPP_CERT_FILE = BBS_ROOT + "xmpp.crt" \n"
Original    (006): ['BBS_XMPP_CERT_FILE', '=', 'BBS_ROOT', '+', '"xmpp.crt"', '\\n']
Tokenized   (030): ['<s>', 'B', 'BS', '_', 'X', 'MP', 'P', '_', 'C', 'ERT', '_', 'FILE', '=', 'B', 'BS', '_', 'RO', 'OT', '+', '"', 'x', 'm', 'pp', '.', 'cr', 't', '"', '\\', 'n', '</s>']
Filtered   (028): ['B', 'BS', '_', 'X', 'MP', 'P', '_', 'C', 'ERT', '_', 'FILE', '=', 'B', 'BS', '_', 'RO', 'OT', '+', '"', 'x', 'm', 'pp', '.', 'cr', 't', '"', '\\', 'n']
Detokenized (006): ['BBS_XMPP_CERT_FILE', '=', 'BBS_ROOT', '+', '"xmpp.crt"', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "BOARDS_FILE = BBS_ROOT + \n"
Original    (005): ['BOARDS_FILE', '=', 'BBS_ROOT', '+', '\\n']
Tokenized   (015): ['<s>', 'BO', 'ARDS', '_', 'FILE', '=', 'B', 'BS', '_', 'RO', 'OT', '+', '\\', 'n', '</s>']
Filtered   (013): ['BO', 'ARDS', '_', 'FILE', '=', 'B', 'BS', '_', 'RO', 'OT', '+', '\\', 'n']
Detokenized (005): ['BOARDS_FILE', '=', 'BBS_ROOT', '+', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "USHM_SIZE = MAXACTIVE + 10 \n"
Original    (006): ['USHM_SIZE', '=', 'MAXACTIVE', '+', '10', '\\n']
Tokenized   (014): ['<s>', 'USH', 'M', '_', 'SIZE', '=', 'MAX', 'ACT', 'IVE', '+', '10', '\\', 'n', '</s>']
Filtered   (012): ['USH', 'M', '_', 'SIZE', '=', 'MAX', 'ACT', 'IVE', '+', '10', '\\', 'n']
Detokenized (006): ['USHM_SIZE', '=', 'MAXACTIVE', '+', '10', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "UTMP_HASHSIZE = USHM_SIZE * 4 \n"
Original    (006): ['UTMP_HASHSIZE', '=', 'USHM_SIZE', '*', '4', '\\n']
Tokenized   (018): ['<s>', 'UT', 'MP', '_', 'H', 'AS', 'HS', 'IZE', '=', 'US', 'HM', '_', 'SIZE', '*', '4', '\\', 'n', '</s>']
Filtered   (016): ['UT', 'MP', '_', 'H', 'AS', 'HS', 'IZE', '=', 'US', 'HM', '_', 'SIZE', '*', '4', '\\', 'n']
Detokenized (006): ['UTMP_HASHSIZE', '=', 'USHM_SIZE', '*', '4', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "SESSION_TIMEOUT = datetime . timedelta ( 30 ) \n"
Original    (009): ['SESSION_TIMEOUT', '=', 'datetime', '.', 'timedelta', '(', '30', ')', '\\n']
Tokenized   (018): ['<s>', 'S', 'ESSION', '_', 'TIME', 'OUT', '=', 'dat', 'etime', '.', 'timed', 'elta', '(', '30', ')', '\\', 'n', '</s>']
Filtered   (016): ['S', 'ESSION', '_', 'TIME', 'OUT', '=', 'dat', 'etime', '.', 'timed', 'elta', '(', '30', ')', '\\', 'n']
Detokenized (009): ['SESSION_TIMEOUT', '=', 'datetime', '.', 'timedelta', '(', '30', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "SESSION_TIMEOUT_SECONDS = 86400 * 30 \n"
Original    (006): ['SESSION_TIMEOUT_SECONDS', '=', '86400', '*', '30', '\\n']
Tokenized   (019): ['<s>', 'S', 'ESSION', '_', 'TIME', 'OUT', '_', 'SEC', 'ON', 'DS', '=', '8', '64', '00', '*', '30', '\\', 'n', '</s>']
Filtered   (017): ['S', 'ESSION', '_', 'TIME', 'OUT', '_', 'SEC', 'ON', 'DS', '=', '8', '64', '00', '*', '30', '\\', 'n']
Detokenized (006): ['SESSION_TIMEOUT_SECONDS', '=', '86400', '*', '30', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "MAX_ATTACHSIZE = 20 * 1024 * 1024 \n"
Original    (008): ['MAX_ATTACHSIZE', '=', '20', '*', '1024', '*', '1024', '\\n']
Tokenized   (016): ['<s>', 'MAX', '_', 'ATT', 'AC', 'HS', 'IZE', '=', '20', '*', '1024', '*', '1024', '\\', 'n', '</s>']
Filtered   (014): ['MAX', '_', 'ATT', 'AC', 'HS', 'IZE', '=', '20', '*', '1024', '*', '1024', '\\', 'n']
Detokenized (008): ['MAX_ATTACHSIZE', '=', '20', '*', '1024', '*', '1024', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "MAIL_SIZE_LIMIT = - 1 \n"
Original    (005): ['MAIL_SIZE_LIMIT', '=', '-', '1', '\\n']
Tokenized   (015): ['<s>', 'MA', 'IL', '_', 'SIZE', '_', 'L', 'IM', 'IT', '=', '-', '1', '\\', 'n', '</s>']
Filtered   (013): ['MA', 'IL', '_', 'SIZE', '_', 'L', 'IM', 'IT', '=', '-', '1', '\\', 'n']
Detokenized (005): ['MAIL_SIZE_LIMIT', '=', '-', '1', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "newparts = [ ] \n"
Original    (005): ['newparts', '=', '[', ']', '\\n']
Tokenized   (009): ['<s>', 'new', 'parts', '=', '[', ']', '\\', 'n', '</s>']
Filtered   (007): ['new', 'parts', '=', '[', ']', '\\', 'n']
Detokenized (005): ['newparts', '=', '[', ']', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "firstitem = self . GetItem ( user , route + [ start ] , has_perm , need_perm ) \n"
Original    (019): ['firstitem', '=', 'self', '.', 'GetItem', '(', 'user', ',', 'route', '+', '[', 'start', ']', ',', 'has_perm', ',', 'need_perm', ')', '\\n']
Tokenized   (028): ['<s>', 'first', 'item', '=', 'self', '.', 'Get', 'Item', '(', 'user', ',', 'route', '+', '[', 'start', ']', ',', 'has', '_', 'perm', ',', 'need', '_', 'perm', ')', '\\', 'n', '</s>']
Filtered   (026): ['first', 'item', '=', 'self', '.', 'Get', 'Item', '(', 'user', ',', 'route', '+', '[', 'start', ']', ',', 'has', '_', 'perm', ',', 'need', '_', 'perm', ')', '\\', 'n']
Detokenized (019): ['firstitem', '=', 'self', '.', 'GetItem', '(', 'user', ',', 'route', '+', '[', 'start', ']', ',', 'has_perm', ',', 'need_perm', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "_id = start - 1 \n"
Original    (006): ['_id', '=', 'start', '-', '1', '\\n']
Tokenized   (010): ['<s>', '_', 'id', '=', 'start', '-', '1', '\\', 'n', '</s>']
Filtered   (008): ['_', 'id', '=', 'start', '-', '1', '\\', 'n']
Detokenized (006): ['_id', '=', 'start', '-', '1', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "linkfile = "%s/boards/xattach/%s" % ( Config . BBS_ROOT , filename ) \n"
Original    (012): ['linkfile', '=', '"%s/boards/xattach/%s"', '%', '(', 'Config', '.', 'BBS_ROOT', ',', 'filename', ')', '\\n']
Tokenized   (030): ['<s>', 'link', 'file', '=', '"%', 's', '/', 'boards', '/', 'x', 'attach', '/', '%', 's', '"', '%', '(', 'Config', '.', 'B', 'BS', '_', 'RO', 'OT', ',', 'filename', ')', '\\', 'n', '</s>']
Filtered   (028): ['link', 'file', '=', '"%', 's', '/', 'boards', '/', 'x', 'attach', '/', '%', 's', '"', '%', '(', 'Config', '.', 'B', 'BS', '_', 'RO', 'OT', ',', 'filename', ')', '\\', 'n']
Detokenized (012): ['linkfile', '=', '"%s/boards/xattach/%s"', '%', '(', 'Config', '.', 'BBS_ROOT', ',', 'filename', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "boardname = svc . get_str ( params , , ) \n"
Original    (011): ['boardname', '=', 'svc', '.', 'get_str', '(', 'params', ',', ',', ')', '\\n']
Tokenized   (018): ['<s>', 'board', 'name', '=', 's', 'vc', '.', 'get', '_', 'str', '(', 'params', ',', ',', ')', '\\', 'n', '</s>']
Filtered   (016): ['board', 'name', '=', 's', 'vc', '.', 'get', '_', 'str', '(', 'params', ',', ',', ')', '\\', 'n']
Detokenized (011): ['boardname', '=', 'svc', '.', 'get_str', '(', 'params', ',', ',', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "has_perm = user . IsDigestMgr ( ) \n"
Original    (008): ['has_perm', '=', 'user', '.', 'IsDigestMgr', '(', ')', '\\n']
Tokenized   (017): ['<s>', 'has', '_', 'perm', '=', 'user', '.', 'Is', 'Dig', 'est', 'M', 'gr', '(', ')', '\\', 'n', '</s>']
Filtered   (015): ['has', '_', 'perm', '=', 'user', '.', 'Is', 'Dig', 'est', 'M', 'gr', '(', ')', '\\', 'n']
Detokenized (008): ['has_perm', '=', 'user', '.', 'IsDigestMgr', '(', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "Digest . View ( svc , basenode , route , session , has_perm , start , count ) \n"
Original    (019): ['Digest', '.', 'View', '(', 'svc', ',', 'basenode', ',', 'route', ',', 'session', ',', 'has_perm', ',', 'start', ',', 'count', ')', '\\n']
Tokenized   (028): ['<s>', 'Dig', 'est', '.', 'View', '(', 's', 'vc', ',', 'bas', 'en', 'ode', ',', 'route', ',', 'session', ',', 'has', '_', 'perm', ',', 'start', ',', 'count', ')', '\\', 'n', '</s>']
Filtered   (026): ['Dig', 'est', '.', 'View', '(', 's', 'vc', ',', 'bas', 'en', 'ode', ',', 'route', ',', 'session', ',', 'has', '_', 'perm', ',', 'start', ',', 'count', ')', '\\', 'n']
Detokenized (019): ['Digest', '.', 'View', '(', 'svc', ',', 'basenode', ',', 'route', ',', 'session', ',', 'has_perm', ',', 'start', ',', 'count', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "svc . writedata ( json . dumps ( result ) ) \n"
Original    (012): ['svc', '.', 'writedata', '(', 'json', '.', 'dumps', '(', 'result', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'sv', 'c', '.', 'writ', 'ed', 'ata', '(', 'json', '.', 'dumps', '(', 'result', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['sv', 'c', '.', 'writ', 'ed', 'ata', '(', 'json', '.', 'dumps', '(', 'result', ')', ')', '\\', 'n']
Detokenized (012): ['svc', '.', 'writedata', '(', 'json', '.', 'dumps', '(', 'result', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "postinfo = Post . Post ( item . realpath ( ) , None ) \n"
Original    (015): ['postinfo', '=', 'Post', '.', 'Post', '(', 'item', '.', 'realpath', '(', ')', ',', 'None', ')', '\\n']
Tokenized   (020): ['<s>', 'post', 'info', '=', 'Post', '.', 'Post', '(', 'item', '.', 'real', 'path', '(', ')', ',', 'None', ')', '\\', 'n', '</s>']
Filtered   (018): ['post', 'info', '=', 'Post', '.', 'Post', '(', 'item', '.', 'real', 'path', '(', ')', ',', 'None', ')', '\\', 'n']
Detokenized (015): ['postinfo', '=', 'Post', '.', 'Post', '(', 'item', '.', 'realpath', '(', ')', ',', 'None', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "msg_count = msgbox . GetMsgCount ( all = False ) \n"
Original    (011): ['msg_count', '=', 'msgbox', '.', 'GetMsgCount', '(', 'all', '=', 'False', ')', '\\n']
Tokenized   (019): ['<s>', 'msg', '_', 'count', '=', 'msg', 'box', '.', 'Get', 'Msg', 'Count', '(', 'all', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (017): ['msg', '_', 'count', '=', 'msg', 'box', '.', 'Get', 'Msg', 'Count', '(', 'all', '=', 'False', ')', '\\', 'n']
Detokenized (011): ['msg_count', '=', 'msgbox', '.', 'GetMsgCount', '(', 'all', '=', 'False', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "xmpp_read = self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) \n"
Original    (017): ['xmpp_read', '=', 'self', '.', 'rosters', '.', 'get_xmpp_read', '(', 'self', '.', '_user', '.', 'GetUID', '(', ')', ')', '\\n']
Tokenized   (032): ['<s>', 'x', 'm', 'pp', '_', 'read', '=', 'self', '.', 'rosters', '.', 'get', '_', 'x', 'm', 'pp', '_', 'read', '(', 'self', '.', '_', 'user', '.', 'Get', 'UID', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (030): ['x', 'm', 'pp', '_', 'read', '=', 'self', '.', 'rosters', '.', 'get', '_', 'x', 'm', 'pp', '_', 'read', '(', 'self', '.', '_', 'user', '.', 'Get', 'UID', '(', ')', ')', '\\', 'n']
Detokenized (017): ['xmpp_read', '=', 'self', '.', 'rosters', '.', 'get_xmpp_read', '(', 'self', '.', '_user', '.', 'GetUID', '(', ')', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "read_count = msg_count - msg_unread \n"
Original    (006): ['read_count', '=', 'msg_count', '-', 'msg_unread', '\\n']
Tokenized   (016): ['<s>', 'read', '_', 'count', '=', 'msg', '_', 'count', '-', 'msg', '_', 'un', 'read', '\\', 'n', '</s>']
Filtered   (014): ['read', '_', 'count', '=', 'msg', '_', 'count', '-', 'msg', '_', 'un', 'read', '\\', 'n']
Detokenized (006): ['read_count', '=', 'msg_count', '-', 'msg_unread', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "term_read = self . rosters . get_term_read ( self . get_uid ( ) ) \n"
Original    (015): ['term_read', '=', 'self', '.', 'rosters', '.', 'get_term_read', '(', 'self', '.', 'get_uid', '(', ')', ')', '\\n']
Tokenized   (026): ['<s>', 'term', '_', 'read', '=', 'self', '.', 'rosters', '.', 'get', '_', 'term', '_', 'read', '(', 'self', '.', 'get', '_', 'uid', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['term', '_', 'read', '=', 'self', '.', 'rosters', '.', 'get', '_', 'term', '_', 'read', '(', 'self', '.', 'get', '_', 'uid', '(', ')', ')', '\\', 'n']
Detokenized (015): ['term_read', '=', 'self', '.', 'rosters', '.', 'get_term_read', '(', 'self', '.', 'get_uid', '(', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "new_unread [ msghead . topid ] = i \n"
Original    (009): ['new_unread', '[', 'msghead', '.', 'topid', ']', '=', 'i', '\\n']
Tokenized   (017): ['<s>', 'new', '_', 'un', 'read', '[', 'msg', 'head', '.', 'top', 'id', ']', '=', 'i', '\\', 'n', '</s>']
Filtered   (015): ['new', '_', 'un', 'read', '[', 'msg', 'head', '.', 'top', 'id', ']', '=', 'i', '\\', 'n']
Detokenized (009): ['new_unread', '[', 'msghead', '.', 'topid', ']', '=', 'i', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "to_steal = { } \n"
Original    (005): ['to_steal', '=', '{', '}', '\\n']
Tokenized   (011): ['<s>', 'to', '_', 'st', 'eal', '=', '{', '}', '\\', 'n', '</s>']
Filtered   (009): ['to', '_', 'st', 'eal', '=', '{', '}', '\\', 'n']
Detokenized (005): ['to_steal', '=', '{', '}', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "to_steal_begin = msg_count \n"
Original    (004): ['to_steal_begin', '=', 'msg_count', '\\n']
Tokenized   (014): ['<s>', 'to', '_', 'st', 'eal', '_', 'begin', '=', 'msg', '_', 'count', '\\', 'n', '</s>']
Filtered   (012): ['to', '_', 'st', 'eal', '_', 'begin', '=', 'msg', '_', 'count', '\\', 'n']
Detokenized (004): ['to_steal_begin', '=', 'msg_count', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "pass \n"
Original    (002): ['pass', '\\n']
Tokenized   (005): ['<s>', 'pass', '\\', 'n', '</s>']
Filtered   (003): ['pass', '\\', 'n']
Detokenized (002): ['pass', '\\n']
Counter: 3
===================================================================
Hidden states:  (13, 2, 768)
# Extracted words:  2
Sentence         : "final_unread [ pid ] = ( new_unread [ pid ] , 1 ) \n"
Original    (014): ['final_unread', '[', 'pid', ']', '=', '(', 'new_unread', '[', 'pid', ']', ',', '1', ')', '\\n']
Tokenized   (023): ['<s>', 'final', '_', 'un', 'read', '[', 'pid', ']', '=', '(', 'new', '_', 'un', 'read', '[', 'pid', ']', ',', '1', ')', '\\', 'n', '</s>']
Filtered   (021): ['final', '_', 'un', 'read', '[', 'pid', ']', '=', '(', 'new', '_', 'un', 'read', '[', 'pid', ']', ',', '1', ')', '\\', 'n']
Detokenized (014): ['final_unread', '[', 'pid', ']', '=', '(', 'new_unread', '[', 'pid', ']', ',', '1', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "msgtext = msgbox . LoadMsgText ( msghead ) \n"
Original    (009): ['msgtext', '=', 'msgbox', '.', 'LoadMsgText', '(', 'msghead', ')', '\\n']
Tokenized   (017): ['<s>', 'msg', 'text', '=', 'msg', 'box', '.', 'Load', 'Msg', 'Text', '(', 'msg', 'head', ')', '\\', 'n', '</s>']
Filtered   (015): ['msg', 'text', '=', 'msg', 'box', '.', 'Load', 'Msg', 'Text', '(', 'msg', 'head', ')', '\\', 'n']
Detokenized (009): ['msgtext', '=', 'msgbox', '.', 'LoadMsgText', '(', 'msghead', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "roster = self . rosters . get ( self ) \n"
Original    (011): ['roster', '=', 'self', '.', 'rosters', '.', 'get', '(', 'self', ')', '\\n']
Tokenized   (015): ['<s>', 'ro', 'ster', '=', 'self', '.', 'rosters', '.', 'get', '(', 'self', ')', '\\', 'n', '</s>']
Filtered   (013): ['ro', 'ster', '=', 'self', '.', 'rosters', '.', 'get', '(', 'self', ')', '\\', 'n']
Detokenized (011): ['roster', '=', 'self', '.', 'rosters', '.', 'get', '(', 'self', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "PYTHON_VERSION = sys . version_info [ : 3 ] \n"
Original    (010): ['PYTHON_VERSION', '=', 'sys', '.', 'version_info', '[', ':', '3', ']', '\\n']
Tokenized   (020): ['<s>', 'P', 'Y', 'TH', 'ON', '_', 'VERSION', '=', 'sys', '.', 'version', '_', 'info', '[', ':', '3', ']', '\\', 'n', '</s>']
Filtered   (018): ['P', 'Y', 'TH', 'ON', '_', 'VERSION', '=', 'sys', '.', 'version', '_', 'info', '[', ':', '3', ']', '\\', 'n']
Detokenized (010): ['PYTHON_VERSION', '=', 'sys', '.', 'version_info', '[', ':', '3', ']', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "PY2 = ( PYTHON_VERSION [ 0 ] == 2 ) \n"
Original    (011): ['PY2', '=', '(', 'PYTHON_VERSION', '[', '0', ']', '==', '2', ')', '\\n']
Tokenized   (021): ['<s>', 'P', 'Y', '2', '=', '(', 'P', 'Y', 'TH', 'ON', '_', 'VERSION', '[', '0', ']', '==', '2', ')', '\\', 'n', '</s>']
Filtered   (019): ['P', 'Y', '2', '=', '(', 'P', 'Y', 'TH', 'ON', '_', 'VERSION', '[', '0', ']', '==', '2', ')', '\\', 'n']
Detokenized (011): ['PY2', '=', '(', 'PYTHON_VERSION', '[', '0', ']', '==', '2', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "sp_desc , \n"
Original    (003): ['sp_desc', ',', '\\n']
Tokenized   (008): ['<s>', 'sp', '_', 'desc', ',', '\\', 'n', '</s>']
Filtered   (006): ['sp', '_', 'desc', ',', '\\', 'n']
Detokenized (003): ['sp_desc', ',', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "con = hpov . connection ( args . host ) \n"
Original    (011): ['con', '=', 'hpov', '.', 'connection', '(', 'args', '.', 'host', ')', '\\n']
Tokenized   (015): ['<s>', 'con', '=', 'hp', 'ov', '.', 'connection', '(', 'args', '.', 'host', ')', '\\', 'n', '</s>']
Filtered   (013): ['con', '=', 'hp', 'ov', '.', 'connection', '(', 'args', '.', 'host', ')', '\\', 'n']
Detokenized (011): ['con', '=', 'hpov', '.', 'connection', '(', 'args', '.', 'host', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "acceptEULA ( con ) \n"
Original    (005): ['acceptEULA', '(', 'con', ')', '\\n']
Tokenized   (011): ['<s>', 'accept', 'E', 'UL', 'A', '(', 'con', ')', '\\', 'n', '</s>']
Filtered   (009): ['accept', 'E', 'UL', 'A', '(', 'con', ')', '\\', 'n']
Detokenized (005): ['acceptEULA', '(', 'con', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "fw_settings = profile . make_firmware_dict ( sts , args . baseline ) \n"
Original    (013): ['fw_settings', '=', 'profile', '.', 'make_firmware_dict', '(', 'sts', ',', 'args', '.', 'baseline', ')', '\\n']
Tokenized   (024): ['<s>', 'fw', '_', 'settings', '=', 'profile', '.', 'make', '_', 'f', 'irm', 'ware', '_', 'dict', '(', 'sts', ',', 'args', '.', 'baseline', ')', '\\', 'n', '</s>']
Filtered   (022): ['fw', '_', 'settings', '=', 'profile', '.', 'make', '_', 'f', 'irm', 'ware', '_', 'dict', '(', 'sts', ',', 'args', '.', 'baseline', ')', '\\', 'n']
Detokenized (013): ['fw_settings', '=', 'profile', '.', 'make_firmware_dict', '(', 'sts', ',', 'args', '.', 'baseline', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "boot , bootmode = profile . make_boot_settings_dict ( srv , sht , args . disable_manage_boot , \n"
Original    (017): ['boot', ',', 'bootmode', '=', 'profile', '.', 'make_boot_settings_dict', '(', 'srv', ',', 'sht', ',', 'args', '.', 'disable_manage_boot', ',', '\\n']
Tokenized   (034): ['<s>', 'boot', ',', 'boot', 'mode', '=', 'profile', '.', 'make', '_', 'boot', '_', 'settings', '_', 'dict', '(', 'sr', 'v', ',', 'sh', 't', ',', 'args', '.', 'disable', '_', 'man', 'age', '_', 'boot', ',', '\\', 'n', '</s>']
Filtered   (032): ['boot', ',', 'boot', 'mode', '=', 'profile', '.', 'make', '_', 'boot', '_', 'settings', '_', 'dict', '(', 'sr', 'v', ',', 'sh', 't', ',', 'args', '.', 'disable', '_', 'man', 'age', '_', 'boot', ',', '\\', 'n']
Detokenized (017): ['boot', ',', 'bootmode', '=', 'profile', '.', 'make_boot_settings_dict', '(', 'srv', ',', 'sht', ',', 'args', '.', 'disable_manage_boot', ',', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "define_profile_template ( srv , \n"
Original    (005): ['define_profile_template', '(', 'srv', ',', '\\n']
Tokenized   (013): ['<s>', 'define', '_', 'profile', '_', 'template', '(', 'sr', 'v', ',', '\\', 'n', '</s>']
Filtered   (011): ['define', '_', 'profile', '_', 'template', '(', 'sr', 'v', ',', '\\', 'n']
Detokenized (005): ['define_profile_template', '(', 'srv', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "sht [ ] , \n"
Original    (005): ['sht', '[', ']', ',', '\\n']
Tokenized   (009): ['<s>', 'sh', 't', '[', ']', ',', '\\', 'n', '</s>']
Filtered   (007): ['sh', 't', '[', ']', ',', '\\', 'n']
Detokenized (005): ['sht', '[', ']', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "credential = { : args . domain . upper ( ) , : args . user , : args . passwd } \n"
Original    (023): ['credential', '=', '{', ':', 'args', '.', 'domain', '.', 'upper', '(', ')', ',', ':', 'args', '.', 'user', ',', ':', 'args', '.', 'passwd', '}', '\\n']
Tokenized   (029): ['<s>', 'c', 'red', 'ential', '=', '{', ':', 'args', '.', 'domain', '.', 'upper', '(', ')', ',', ':', 'args', '.', 'user', ',', ':', 'args', '.', 'pass', 'wd', '}', '\\', 'n', '</s>']
Filtered   (027): ['c', 'red', 'ential', '=', '{', ':', 'args', '.', 'domain', '.', 'upper', '(', ')', ',', ':', 'args', '.', 'user', ',', ':', 'args', '.', 'pass', 'wd', '}', '\\', 'n']
Detokenized (023): ['credential', '=', '{', ':', 'args', '.', 'domain', '.', 'upper', '(', ')', ',', ':', 'args', '.', 'user', ',', ':', 'args', '.', 'passwd', '}', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "get_address_pools ( con , srv , args . types ) \n"
Original    (011): ['get_address_pools', '(', 'con', ',', 'srv', ',', 'args', '.', 'types', ')', '\\n']
Tokenized   (020): ['<s>', 'get', '_', 'address', '_', 'pool', 's', '(', 'con', ',', 'sr', 'v', ',', 'args', '.', 'types', ')', '\\', 'n', '</s>']
Filtered   (018): ['get', '_', 'address', '_', 'pool', 's', '(', 'con', ',', 'sr', 'v', ',', 'args', '.', 'types', ')', '\\', 'n']
Detokenized (011): ['get_address_pools', '(', 'con', ',', 'srv', ',', 'args', '.', 'types', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "enclosure_group = None , server_profile = None ) : \n"
Original    (010): ['enclosure_group', '=', 'None', ',', 'server_profile', '=', 'None', ')', ':', '\\n']
Tokenized   (018): ['<s>', 'en', 'closure', '_', 'group', '=', 'None', ',', 'server', '_', 'profile', '=', 'None', ')', ':', '\\', 'n', '</s>']
Filtered   (016): ['en', 'closure', '_', 'group', '=', 'None', ',', 'server', '_', 'profile', '=', 'None', ')', ':', '\\', 'n']
Detokenized (010): ['enclosure_group', '=', 'None', ',', 'server_profile', '=', 'None', ')', ':', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "biosSettings = None , \n"
Original    (005): ['biosSettings', '=', 'None', ',', '\\n']
Tokenized   (010): ['<s>', 'b', 'ios', 'Settings', '=', 'None', ',', '\\', 'n', '</s>']
Filtered   (008): ['b', 'ios', 'Settings', '=', 'None', ',', '\\', 'n']
Detokenized (005): ['biosSettings', '=', 'None', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "macType = , \n"
Original    (004): ['macType', '=', ',', '\\n']
Tokenized   (008): ['<s>', 'mac', 'Type', '=', ',', '\\', 'n', '</s>']
Filtered   (006): ['mac', 'Type', '=', ',', '\\', 'n']
Detokenized (004): ['macType', '=', ',', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "localStorageSettingsV3 , macType , name , \n"
Original    (007): ['localStorageSettingsV3', ',', 'macType', ',', 'name', ',', '\\n']
Tokenized   (015): ['<s>', 'local', 'Storage', 'Settings', 'V', '3', ',', 'mac', 'Type', ',', 'name', ',', '\\', 'n', '</s>']
Filtered   (013): ['local', 'Storage', 'Settings', 'V', '3', ',', 'mac', 'Type', ',', 'name', ',', '\\', 'n']
Detokenized (007): ['localStorageSettingsV3', ',', 'macType', ',', 'name', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "sanStorageV3 , serialNumber , \n"
Original    (005): ['sanStorageV3', ',', 'serialNumber', ',', '\\n']
Tokenized   (012): ['<s>', 'san', 'Storage', 'V', '3', ',', 'serial', 'Number', ',', '\\', 'n', '</s>']
Filtered   (010): ['san', 'Storage', 'V', '3', ',', 'serial', 'Number', ',', '\\', 'n']
Detokenized (005): ['sanStorageV3', ',', 'serialNumber', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "serverProfileTemplateUri , uuid , wwnType ) \n"
Original    (007): ['serverProfileTemplateUri', ',', 'uuid', ',', 'wwnType', ')', '\\n']
Tokenized   (017): ['<s>', 'server', 'Profile', 'Template', 'U', 'ri', ',', 'u', 'uid', ',', 'w', 'wn', 'Type', ')', '\\', 'n', '</s>']
Filtered   (015): ['server', 'Profile', 'Template', 'U', 'ri', ',', 'u', 'uid', ',', 'w', 'wn', 'Type', ')', '\\', 'n']
Detokenized (007): ['serverProfileTemplateUri', ',', 'uuid', ',', 'wwnType', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "profile_template = self . _con . get ( entity [ ] ) \n"
Original    (013): ['profile_template', '=', 'self', '.', '_con', '.', 'get', '(', 'entity', '[', ']', ')', '\\n']
Tokenized   (019): ['<s>', 'profile', '_', 'template', '=', 'self', '.', '_', 'con', '.', 'get', '(', 'entity', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (017): ['profile', '_', 'template', '=', 'self', '.', '_', 'con', '.', 'get', '(', 'entity', '[', ']', ')', '\\', 'n']
Detokenized (013): ['profile_template', '=', 'self', '.', '_con', '.', 'get', '(', 'entity', '[', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "powerMode = ) : \n"
Original    (005): ['powerMode', '=', ')', ':', '\\n']
Tokenized   (009): ['<s>', 'power', 'Mode', '=', ')', ':', '\\', 'n', '</s>']
Filtered   (007): ['power', 'Mode', '=', ')', ':', '\\', 'n']
Detokenized (005): ['powerMode', '=', ')', ':', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "egroup = make_EnclosureGroupV200 ( associatedLIGs , name , powerMode ) \n"
Original    (011): ['egroup', '=', 'make_EnclosureGroupV200', '(', 'associatedLIGs', ',', 'name', ',', 'powerMode', ')', '\\n']
Tokenized   (025): ['<s>', 'eg', 'roup', '=', 'make', '_', 'En', 'closure', 'Group', 'V', '200', '(', 'associated', 'L', 'IG', 's', ',', 'name', ',', 'power', 'Mode', ')', '\\', 'n', '</s>']
Filtered   (023): ['eg', 'roup', '=', 'make', '_', 'En', 'closure', 'Group', 'V', '200', '(', 'associated', 'L', 'IG', 's', ',', 'name', ',', 'power', 'Mode', ')', '\\', 'n']
Detokenized (011): ['egroup', '=', 'make_EnclosureGroupV200', '(', 'associatedLIGs', ',', 'name', ',', 'powerMode', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "allocatorBody = { : count } \n"
Original    (007): ['allocatorBody', '=', '{', ':', 'count', '}', '\\n']
Tokenized   (012): ['<s>', 'alloc', 'ator', 'Body', '=', '{', ':', 'count', '}', '\\', 'n', '</s>']
Filtered   (010): ['alloc', 'ator', 'Body', '=', '{', ':', 'count', '}', '\\', 'n']
Detokenized (007): ['allocatorBody', '=', '{', ':', 'count', '}', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "prange [ ] = False \n"
Original    (006): ['prange', '[', ']', '=', 'False', '\\n']
Tokenized   (010): ['<s>', 'pr', 'ange', '[', ']', '=', 'False', '\\', 'n', '</s>']
Filtered   (008): ['pr', 'ange', '[', ']', '=', 'False', '\\', 'n']
Detokenized (006): ['prange', '[', ']', '=', 'False', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "tempstr = "hp-rest-classes-bios-" + romfamily + "-" + biosversion \n"
Original    (010): ['tempstr', '=', '"hp-rest-classes-bios-"', '+', 'romfamily', '+', '"-"', '+', 'biosversion', '\\n']
Tokenized   (026): ['<s>', 'temp', 'str', '=', '"', 'hp', '-', 'rest', '-', 'classes', '-', 'b', 'ios', '-"', '+', 'rom', 'family', '+', '"', '-"', '+', 'bios', 'version', '\\', 'n', '</s>']
Filtered   (024): ['temp', 'str', '=', '"', 'hp', '-', 'rest', '-', 'classes', '-', 'b', 'ios', '-"', '+', 'rom', 'family', '+', '"', '-"', '+', 'bios', 'version', '\\', 'n']
Detokenized (010): ['tempstr', '=', '"hp-rest-classes-bios-"', '+', 'romfamily', '+', '"-"', '+', 'biosversion', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "monolith = None ) : \n"
Original    (006): ['monolith', '=', 'None', ')', ':', '\\n']
Tokenized   (010): ['<s>', 'mon', 'olith', '=', 'None', ')', ':', '\\', 'n', '</s>']
Filtered   (008): ['mon', 'olith', '=', 'None', ')', ':', '\\', 'n']
Detokenized (006): ['monolith', '=', 'None', ')', ':', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "pathjoinstr ) ) : \n"
Original    (005): ['pathjoinstr', ')', ')', ':', '\\n']
Tokenized   (010): ['<s>', 'path', 'join', 'str', ')', ')', ':', '\\', 'n', '</s>']
Filtered   (008): ['path', 'join', 'str', ')', ')', ':', '\\', 'n']
Detokenized (005): ['pathjoinstr', ')', ')', ':', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "newclass . set_root ( root ) \n"
Original    (007): ['newclass', '.', 'set_root', '(', 'root', ')', '\\n']
Tokenized   (013): ['<s>', 'new', 'class', '.', 'set', '_', 'root', '(', 'root', ')', '\\', 'n', '</s>']
Filtered   (011): ['new', 'class', '.', 'set', '_', 'root', '(', 'root', ')', '\\', 'n']
Detokenized (007): ['newclass', '.', 'set_root', '(', 'root', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "folderentries = data [ "links" ] \n"
Original    (007): ['folderentries', '=', 'data', '[', '"links"', ']', '\\n']
Tokenized   (014): ['<s>', 'fold', 'erent', 'ries', '=', 'data', '[', '"', 'links', '"', ']', '\\', 'n', '</s>']
Filtered   (012): ['fold', 'erent', 'ries', '=', 'data', '[', '"', 'links', '"', ']', '\\', 'n']
Detokenized (007): ['folderentries', '=', 'data', '[', '"links"', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "datareturn . append ( self . load_file ( fqpath , root = root , biossection = True , registries = True , datareturn = True ) ) \n"
Original    (028): ['datareturn', '.', 'append', '(', 'self', '.', 'load_file', '(', 'fqpath', ',', 'root', '=', 'root', ',', 'biossection', '=', 'True', ',', 'registries', '=', 'True', ',', 'datareturn', '=', 'True', ')', ')', '\\n']
Tokenized   (041): ['<s>', 'dat', 'aret', 'urn', '.', 'append', '(', 'self', '.', 'load', '_', 'file', '(', 'f', 'q', 'path', ',', 'root', '=', 'root', ',', 'bios', 'section', '=', 'True', ',', 'regist', 'ries', '=', 'True', ',', 'dat', 'aret', 'urn', '=', 'True', ')', ')', '\\', 'n', '</s>']
Filtered   (039): ['dat', 'aret', 'urn', '.', 'append', '(', 'self', '.', 'load', '_', 'file', '(', 'f', 'q', 'path', ',', 'root', '=', 'root', ',', 'bios', 'section', '=', 'True', ',', 'regist', 'ries', '=', 'True', ',', 'dat', 'aret', 'urn', '=', 'True', ')', ')', '\\', 'n']
Detokenized (028): ['datareturn', '.', 'append', '(', 'self', '.', 'load_file', '(', 'fqpath', ',', 'root', '=', 'root', ',', 'biossection', '=', 'True', ',', 'registries', '=', 'True', ',', 'datareturn', '=', 'True', ')', ')', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 28, 768)
# Extracted words:  28
Sentence         : "currdict = currdict , monolith = monolith , \n"
Original    (009): ['currdict', '=', 'currdict', ',', 'monolith', '=', 'monolith', ',', '\\n']
Tokenized   (018): ['<s>', 'cur', 'rd', 'ict', '=', 'cur', 'rd', 'ict', ',', 'mon', 'olith', '=', 'mon', 'olith', ',', '\\', 'n', '</s>']
Filtered   (016): ['cur', 'rd', 'ict', '=', 'cur', 'rd', 'ict', ',', 'mon', 'olith', '=', 'mon', 'olith', ',', '\\', 'n']
Detokenized (009): ['currdict', '=', 'currdict', ',', 'monolith', '=', 'monolith', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "newarg = newarg , checkall = checkall ) \n"
Original    (009): ['newarg', '=', 'newarg', ',', 'checkall', '=', 'checkall', ')', '\\n']
Tokenized   (016): ['<s>', 'new', 'arg', '=', 'new', 'arg', ',', 'check', 'all', '=', 'check', 'all', ')', '\\', 'n', '</s>']
Filtered   (014): ['new', 'arg', '=', 'new', 'arg', ',', 'check', 'all', '=', 'check', 'all', ')', '\\', 'n']
Detokenized (009): ['newarg', '=', 'newarg', ',', 'checkall', '=', 'checkall', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "attrreg = self . find_bios_registry ( regname = regname ) \n"
Original    (011): ['attrreg', '=', 'self', '.', 'find_bios_registry', '(', 'regname', '=', 'regname', ')', '\\n']
Tokenized   (023): ['<s>', 'attr', 'reg', '=', 'self', '.', 'find', '_', 'b', 'ios', '_', 'reg', 'istry', '(', 'reg', 'name', '=', 'reg', 'name', ')', '\\', 'n', '</s>']
Filtered   (021): ['attr', 'reg', '=', 'self', '.', 'find', '_', 'b', 'ios', '_', 'reg', 'istry', '(', 'reg', 'name', '=', 'reg', 'name', ')', '\\', 'n']
Detokenized (011): ['attrreg', '=', 'self', '.', 'find_bios_registry', '(', 'regname', '=', 'regname', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "schlink = schlink [ len ( schlink ) - 2 ] \n"
Original    (012): ['schlink', '=', 'schlink', '[', 'len', '(', 'schlink', ')', '-', '2', ']', '\\n']
Tokenized   (018): ['<s>', 'sch', 'link', '=', 'sch', 'link', '[', 'len', '(', 'sch', 'link', ')', '-', '2', ']', '\\', 'n', '</s>']
Filtered   (016): ['sch', 'link', '=', 'sch', 'link', '[', 'len', '(', 'sch', 'link', ')', '-', '2', ']', '\\', 'n']
Detokenized (012): ['schlink', '=', 'schlink', '[', 'len', '(', 'schlink', ')', '-', '2', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "schname . lower ( ) ) : \n"
Original    (008): ['schname', '.', 'lower', '(', ')', ')', ':', '\\n']
Tokenized   (013): ['<s>', 's', 'chn', 'ame', '.', 'lower', '(', ')', ')', ':', '\\', 'n', '</s>']
Filtered   (011): ['s', 'chn', 'ame', '.', 'lower', '(', ')', ')', ':', '\\', 'n']
Detokenized (008): ['schname', '.', 'lower', '(', ')', ')', ':', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "xref = os . path . normpath ( currloc . Uri . extref ) . lstrip ( os . path . sep ) \n"
Original    (024): ['xref', '=', 'os', '.', 'path', '.', 'normpath', '(', 'currloc', '.', 'Uri', '.', 'extref', ')', '.', 'lstrip', '(', 'os', '.', 'path', '.', 'sep', ')', '\\n']
Tokenized   (033): ['<s>', 'x', 'ref', '=', 'os', '.', 'path', '.', 'norm', 'path', '(', 'cur', 'r', 'loc', '.', 'Uri', '.', 'ext', 'ref', ')', '.', 'l', 'strip', '(', 'os', '.', 'path', '.', 'sep', ')', '\\', 'n', '</s>']
Filtered   (031): ['x', 'ref', '=', 'os', '.', 'path', '.', 'norm', 'path', '(', 'cur', 'r', 'loc', '.', 'Uri', '.', 'ext', 'ref', ')', '.', 'l', 'strip', '(', 'os', '.', 'path', '.', 'sep', ')', '\\', 'n']
Detokenized (024): ['xref', '=', 'os', '.', 'path', '.', 'normpath', '(', 'currloc', '.', 'Uri', '.', 'extref', ')', '.', 'lstrip', '(', 'os', '.', 'path', '.', 'sep', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "fqpath = os . path . join ( root , xref ) \n"
Original    (013): ['fqpath', '=', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'xref', ')', '\\n']
Tokenized   (019): ['<s>', 'f', 'q', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'x', 'ref', ')', '\\', 'n', '</s>']
Filtered   (017): ['f', 'q', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'x', 'ref', ')', '\\', 'n']
Detokenized (013): ['fqpath', '=', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'xref', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "langcode = list ( locale . getdefaultlocale ( ) ) \n"
Original    (011): ['langcode', '=', 'list', '(', 'locale', '.', 'getdefaultlocale', '(', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'lang', 'code', '=', 'list', '(', 'locale', '.', 'get', 'default', 'loc', 'ale', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['lang', 'code', '=', 'list', '(', 'locale', '.', 'get', 'default', 'loc', 'ale', '(', ')', ')', '\\', 'n']
Detokenized (011): ['langcode', '=', 'list', '(', 'locale', '.', 'getdefaultlocale', '(', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "locationlanguage = locationlanguage . replace ( "-" , "_" ) \n"
Original    (011): ['locationlanguage', '=', 'locationlanguage', '.', 'replace', '(', '"-"', ',', '"_"', ')', '\\n']
Tokenized   (018): ['<s>', 'location', 'language', '=', 'location', 'language', '.', 'replace', '(', '"', '-"', ',', '"_', '"', ')', '\\', 'n', '</s>']
Filtered   (016): ['location', 'language', '=', 'location', 'language', '.', 'replace', '(', '"', '-"', ',', '"_', '"', ')', '\\', 'n']
Detokenized (011): ['locationlanguage', '=', 'locationlanguage', '.', 'replace', '(', '"-"', ',', '"_"', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "currtype = currtype . split ( ) [ 0 ] + \n"
Original    (012): ['currtype', '=', 'currtype', '.', 'split', '(', ')', '[', '0', ']', '+', '\\n']
Tokenized   (019): ['<s>', 'cur', 'r', 'type', '=', 'cur', 'r', 'type', '.', 'split', '(', ')', '[', '0', ']', '+', '\\', 'n', '</s>']
Filtered   (017): ['cur', 'r', 'type', '=', 'cur', 'r', 'type', '.', 'split', '(', ')', '[', '0', ']', '+', '\\', 'n']
Detokenized (012): ['currtype', '=', 'currtype', '.', 'split', '(', ')', '[', '0', ']', '+', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "insttype = instance . resp . dict [ "title" ] . split ( ) [ : 1 ] \n"
Original    (019): ['insttype', '=', 'instance', '.', 'resp', '.', 'dict', '[', '"title"', ']', '.', 'split', '(', ')', '[', ':', '1', ']', '\\n']
Tokenized   (025): ['<s>', 'inst', 'type', '=', 'instance', '.', 'resp', '.', 'dict', '[', '"', 'title', '"', ']', '.', 'split', '(', ')', '[', ':', '1', ']', '\\', 'n', '</s>']
Filtered   (023): ['inst', 'type', '=', 'instance', '.', 'resp', '.', 'dict', '[', '"', 'title', '"', ']', '.', 'split', '(', ')', '[', ':', '1', ']', '\\', 'n']
Detokenized (019): ['insttype', '=', 'instance', '.', 'resp', '.', 'dict', '[', '"title"', ']', '.', 'split', '(', ')', '[', ':', '1', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "nextarg = newarg [ newarg . index ( arg ) + 1 ] \n"
Original    (014): ['nextarg', '=', 'newarg', '[', 'newarg', '.', 'index', '(', 'arg', ')', '+', '1', ']', '\\n']
Tokenized   (020): ['<s>', 'next', 'arg', '=', 'new', 'arg', '[', 'new', 'arg', '.', 'index', '(', 'arg', ')', '+', '1', ']', '\\', 'n', '</s>']
Filtered   (018): ['next', 'arg', '=', 'new', 'arg', '[', 'new', 'arg', '.', 'index', '(', 'arg', ')', '+', '1', ']', '\\', 'n']
Detokenized (014): ['nextarg', '=', 'newarg', '[', 'newarg', '.', 'index', '(', 'arg', ')', '+', '1', ']', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "regcopy [ nextarg ] = patterninfo \n"
Original    (007): ['regcopy', '[', 'nextarg', ']', '=', 'patterninfo', '\\n']
Tokenized   (013): ['<s>', 'reg', 'copy', '[', 'next', 'arg', ']', '=', 'pattern', 'info', '\\', 'n', '</s>']
Filtered   (011): ['reg', 'copy', '[', 'next', 'arg', ']', '=', 'pattern', 'info', '\\', 'n']
Detokenized (007): ['regcopy', '[', 'nextarg', ']', '=', 'patterninfo', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "validictory . validate ( tdict , jsonsch ) \n"
Original    (009): ['validictory', '.', 'validate', '(', 'tdict', ',', 'jsonsch', ')', '\\n']
Tokenized   (017): ['<s>', 'valid', 'ict', 'ory', '.', 'validate', '(', 't', 'dict', ',', 'js', 'ons', 'ch', ')', '\\', 'n', '</s>']
Filtered   (015): ['valid', 'ict', 'ory', '.', 'validate', '(', 't', 'dict', ',', 'js', 'ons', 'ch', ')', '\\', 'n']
Detokenized (009): ['validictory', '.', 'validate', '(', 'tdict', ',', 'jsonsch', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "wrapper . subsequent_indent = * 4 \n"
Original    (007): ['wrapper', '.', 'subsequent_indent', '=', '*', '4', '\\n']
Tokenized   (013): ['<s>', 'wrapper', '.', 'subsequent', '_', 'ind', 'ent', '=', '*', '4', '\\', 'n', '</s>']
Filtered   (011): ['wrapper', '.', 'subsequent', '_', 'ind', 'ent', '=', '*', '4', '\\', 'n']
Detokenized (007): ['wrapper', '.', 'subsequent_indent', '=', '*', '4', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "RegistryValidationError ( \n"
Original    (003): ['RegistryValidationError', '(', '\\n']
Tokenized   (010): ['<s>', 'Reg', 'istry', 'Val', 'idation', 'Error', '(', '\\', 'n', '</s>']
Filtered   (008): ['Reg', 'istry', 'Val', 'idation', 'Error', '(', '\\', 'n']
Detokenized (003): ['RegistryValidationError', '(', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "regentry = self \n"
Original    (004): ['regentry', '=', 'self', '\\n']
Tokenized   (008): ['<s>', 'reg', 'entry', '=', 'self', '\\', 'n', '</s>']
Filtered   (006): ['reg', 'entry', '=', 'self', '\\', 'n']
Detokenized (004): ['regentry', '=', 'self', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : ""\'%(ValueExpression)s\'" % ( self ) , regentry = self ) ) \n"
Original    (012): ['"\\\'%(ValueExpression)s\\\'"', '%', '(', 'self', ')', ',', 'regentry', '=', 'self', ')', ')', '\\n']
Tokenized   (026): ['<s>', '"', "\\'", '%', '(', 'Value', 'Exp', 'ression', ')', 's', '\\', '\'"', '%', '(', 'self', ')', ',', 'reg', 'entry', '=', 'self', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['"', "\\'", '%', '(', 'Value', 'Exp', 'ression', ')', 's', '\\', '\'"', '%', '(', 'self', ')', ',', 'reg', 'entry', '=', 'self', ')', ')', '\\', 'n']
Detokenized (012): ['"\\\'%(ValueExpression)s\\\'"', '%', '(', 'self', ')', ',', 'regentry', '=', 'self', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "intval = int ( newval ) \n"
Original    (007): ['intval', '=', 'int', '(', 'newval', ')', '\\n']
Tokenized   (012): ['<s>', 'int', 'val', '=', 'int', '(', 'new', 'val', ')', '\\', 'n', '</s>']
Filtered   (010): ['int', 'val', '=', 'int', '(', 'new', 'val', ')', '\\', 'n']
Detokenized (007): ['intval', '=', 'int', '(', 'newval', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "MICROS_TRANSLATIONS = ( \n"
Original    (004): ['MICROS_TRANSLATIONS', '=', '(', '\\n']
Tokenized   (014): ['<s>', 'MIC', 'R', 'OS', '_', 'TR', 'AN', 'SL', 'ATIONS', '=', '(', '\\', 'n', '</s>']
Filtered   (012): ['MIC', 'R', 'OS', '_', 'TR', 'AN', 'SL', 'ATIONS', '=', '(', '\\', 'n']
Detokenized (004): ['MICROS_TRANSLATIONS', '=', '(', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "MICROS_TRANSLATION_HASH = dict ( ( alt , v ) for k , v in MICROS_TRANSLATIONS for alt in k ) \n"
Original    (021): ['MICROS_TRANSLATION_HASH', '=', 'dict', '(', '(', 'alt', ',', 'v', ')', 'for', 'k', ',', 'v', 'in', 'MICROS_TRANSLATIONS', 'for', 'alt', 'in', 'k', ')', '\\n']
Tokenized   (041): ['<s>', 'MIC', 'R', 'OS', '_', 'TR', 'AN', 'SL', 'ATION', '_', 'H', 'ASH', '=', 'dict', '(', '(', 'alt', ',', 'v', ')', 'for', 'k', ',', 'v', 'in', 'MIC', 'R', 'OS', '_', 'TR', 'AN', 'SL', 'ATIONS', 'for', 'alt', 'in', 'k', ')', '\\', 'n', '</s>']
Filtered   (039): ['MIC', 'R', 'OS', '_', 'TR', 'AN', 'SL', 'ATION', '_', 'H', 'ASH', '=', 'dict', '(', '(', 'alt', ',', 'v', ')', 'for', 'k', ',', 'v', 'in', 'MIC', 'R', 'OS', '_', 'TR', 'AN', 'SL', 'ATIONS', 'for', 'alt', 'in', 'k', ')', '\\', 'n']
Detokenized (021): ['MICROS_TRANSLATION_HASH', '=', 'dict', '(', '(', 'alt', ',', 'v', ')', 'for', 'k', ',', 'v', 'in', 'MICROS_TRANSLATIONS', 'for', 'alt', 'in', 'k', ')', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "epoch_milliseconds = epoch_millis = milliseconds = millis = ms \n"
Original    (010): ['epoch_milliseconds', '=', 'epoch_millis', '=', 'milliseconds', '=', 'millis', '=', 'ms', '\\n']
Tokenized   (022): ['<s>', 'ep', 'och', '_', 'mill', 'isec', 'onds', '=', 'epoch', '_', 'mill', 'is', '=', 'milliseconds', '=', 'mill', 'is', '=', 'ms', '\\', 'n', '</s>']
Filtered   (020): ['ep', 'och', '_', 'mill', 'isec', 'onds', '=', 'epoch', '_', 'mill', 'is', '=', 'milliseconds', '=', 'mill', 'is', '=', 'ms', '\\', 'n']
Detokenized (010): ['epoch_milliseconds', '=', 'epoch_millis', '=', 'milliseconds', '=', 'millis', '=', 'ms', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "epoch_microseconds = epoch_micros = microseconds = micros \n"
Original    (008): ['epoch_microseconds', '=', 'epoch_micros', '=', 'microseconds', '=', 'micros', '\\n']
Tokenized   (020): ['<s>', 'ep', 'och', '_', 'micro', 'seconds', '=', 'epoch', '_', 'micro', 's', '=', 'micro', 'seconds', '=', 'micro', 's', '\\', 'n', '</s>']
Filtered   (018): ['ep', 'och', '_', 'micro', 'seconds', '=', 'epoch', '_', 'micro', 's', '=', 'micro', 'seconds', '=', 'micro', 's', '\\', 'n']
Detokenized (008): ['epoch_microseconds', '=', 'epoch_micros', '=', 'microseconds', '=', 'micros', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "micros = u".%06d" % dt . microsecond if dt . microsecond else \n"
Original    (013): ['micros', '=', 'u".%06d"', '%', 'dt', '.', 'microsecond', 'if', 'dt', '.', 'microsecond', 'else', '\\n']
Tokenized   (026): ['<s>', 'micro', 's', '=', 'u', '".', '%', '06', 'd', '"', '%', 'd', 't', '.', 'micro', 'second', 'if', 'd', 't', '.', 'micro', 'second', 'else', '\\', 'n', '</s>']
Filtered   (024): ['micro', 's', '=', 'u', '".', '%', '06', 'd', '"', '%', 'd', 't', '.', 'micro', 'second', 'if', 'd', 't', '.', 'micro', 'second', 'else', '\\', 'n']
Detokenized (013): ['micros', '=', 'u".%06d"', '%', 'dt', '.', 'microsecond', 'if', 'dt', '.', 'microsecond', 'else', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "datastore_owner_uuid = request . REQUEST [ "datastore_owner__uuid" ] \n"
Original    (009): ['datastore_owner_uuid', '=', 'request', '.', 'REQUEST', '[', '"datastore_owner__uuid"', ']', '\\n']
Tokenized   (029): ['<s>', 'dat', 'ast', 'ore', '_', 'owner', '_', 'uu', 'id', '=', 'request', '.', 'RE', 'QUEST', '[', '"', 'dat', 'ast', 'ore', '_', 'owner', '__', 'uu', 'id', '"', ']', '\\', 'n', '</s>']
Filtered   (027): ['dat', 'ast', 'ore', '_', 'owner', '_', 'uu', 'id', '=', 'request', '.', 'RE', 'QUEST', '[', '"', 'dat', 'ast', 'ore', '_', 'owner', '__', 'uu', 'id', '"', ']', '\\', 'n']
Detokenized (009): ['datastore_owner_uuid', '=', 'request', '.', 'REQUEST', '[', '"datastore_owner__uuid"', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "datastore_owner , ds_owner_created = Profile . objects . get_or_create ( uuid = datastore_owner_uuid ) \n"
Original    (015): ['datastore_owner', ',', 'ds_owner_created', '=', 'Profile', '.', 'objects', '.', 'get_or_create', '(', 'uuid', '=', 'datastore_owner_uuid', ')', '\\n']
Tokenized   (039): ['<s>', 'dat', 'ast', 'ore', '_', 'owner', ',', 'd', 's', '_', 'owner', '_', 'created', '=', 'Profile', '.', 'objects', '.', 'get', '_', 'or', '_', 'create', '(', 'u', 'uid', '=', 'dat', 'ast', 'ore', '_', 'owner', '_', 'uu', 'id', ')', '\\', 'n', '</s>']
Filtered   (037): ['dat', 'ast', 'ore', '_', 'owner', ',', 'd', 's', '_', 'owner', '_', 'created', '=', 'Profile', '.', 'objects', '.', 'get', '_', 'or', '_', 'create', '(', 'u', 'uid', '=', 'dat', 'ast', 'ore', '_', 'owner', '_', 'uu', 'id', ')', '\\', 'n']
Detokenized (015): ['datastore_owner', ',', 'ds_owner_created', '=', 'Profile', '.', 'objects', '.', 'get_or_create', '(', 'uuid', '=', 'datastore_owner_uuid', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "port = db . Column ( db . Integer , nullable = False ) \n"
Original    (015): ['port', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'nullable', '=', 'False', ')', '\\n']
Tokenized   (019): ['<s>', 'port', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'null', 'able', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (017): ['port', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'null', 'able', '=', 'False', ')', '\\', 'n']
Detokenized (015): ['port', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'nullable', '=', 'False', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "eru_container_id = db . Column ( db . String ( 64 ) , index = True ) \n"
Original    (018): ['eru_container_id', '=', 'db', '.', 'Column', '(', 'db', '.', 'String', '(', '64', ')', ',', 'index', '=', 'True', ')', '\\n']
Tokenized   (026): ['<s>', 'er', 'u', '_', 'container', '_', 'id', '=', 'db', '.', 'Column', '(', 'db', '.', 'String', '(', '64', ')', ',', 'index', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (024): ['er', 'u', '_', 'container', '_', 'id', '=', 'db', '.', 'Column', '(', 'db', '.', 'String', '(', '64', ')', ',', 'index', '=', 'True', ')', '\\', 'n']
Detokenized (018): ['eru_container_id', '=', 'db', '.', 'Column', '(', 'db', '.', 'String', '(', '64', ')', ',', 'index', '=', 'True', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "suppress_alert = db . Column ( db . Integer , nullable = False , default = 1 ) \n"
Original    (019): ['suppress_alert', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'nullable', '=', 'False', ',', 'default', '=', '1', ')', '\\n']
Tokenized   (026): ['<s>', 'supp', 'ress', '_', 'alert', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'null', 'able', '=', 'False', ',', 'default', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (024): ['supp', 'ress', '_', 'alert', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'null', 'able', '=', 'False', ',', 'default', '=', '1', ')', '\\', 'n']
Detokenized (019): ['suppress_alert', '=', 'db', '.', 'Column', '(', 'db', '.', 'Integer', ',', 'nullable', '=', 'False', ',', 'default', '=', '1', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "__table_args__ = ( db . Index ( , , , unique = True ) , ) \n"
Original    (017): ['__table_args__', '=', '(', 'db', '.', 'Index', '(', ',', ',', ',', 'unique', '=', 'True', ')', ',', ')', '\\n']
Tokenized   (024): ['<s>', '__', 'table', '_', 'args', '__', '=', '(', 'db', '.', 'Index', '(', ',', ',', ',', 'unique', '=', 'True', ')', ',', ')', '\\', 'n', '</s>']
Filtered   (022): ['__', 'table', '_', 'args', '__', '=', '(', 'db', '.', 'Index', '(', ',', ',', ',', 'unique', '=', 'True', ')', ',', ')', '\\', 'n']
Detokenized (017): ['__table_args__', '=', '(', 'db', '.', 'Index', '(', ',', ',', ',', 'unique', '=', 'True', ')', ',', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "cluster_id = cluster_id ) \n"
Original    (005): ['cluster_id', '=', 'cluster_id', ')', '\\n']
Tokenized   (013): ['<s>', 'cl', 'uster', '_', 'id', '=', 'cluster', '_', 'id', ')', '\\', 'n', '</s>']
Filtered   (011): ['cl', 'uster', '_', 'id', '=', 'cluster', '_', 'id', ')', '\\', 'n']
Detokenized (005): ['cluster_id', '=', 'cluster_id', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "Proxy . id . desc ( ) ) . offset ( offset ) . limit ( limit ) . all ( ) \n"
Original    (023): ['Proxy', '.', 'id', '.', 'desc', '(', ')', ')', '.', 'offset', '(', 'offset', ')', '.', 'limit', '(', 'limit', ')', '.', 'all', '(', ')', '\\n']
Tokenized   (026): ['<s>', 'Proxy', '.', 'id', '.', 'desc', '(', ')', ')', '.', 'offset', '(', 'offset', ')', '.', 'limit', '(', 'limit', ')', '.', 'all', '(', ')', '\\', 'n', '</s>']
Filtered   (024): ['Proxy', '.', 'id', '.', 'desc', '(', ')', ')', '.', 'offset', '(', 'offset', ')', '.', 'limit', '(', 'limit', ')', '.', 'all', '(', ')', '\\', 'n']
Detokenized (023): ['Proxy', '.', 'id', '.', 'desc', '(', ')', ')', '.', 'offset', '(', 'offset', ')', '.', 'limit', '(', 'limit', ')', '.', 'all', '(', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "iou_as_issuer . issue_funds ( amount_issued , ) \n"
Original    (008): ['iou_as_issuer', '.', 'issue_funds', '(', 'amount_issued', ',', ')', '\\n']
Tokenized   (022): ['<s>', 'i', 'ou', '_', 'as', '_', 'iss', 'uer', '.', 'issue', '_', 'fund', 's', '(', 'amount', '_', 'issued', ',', ')', '\\', 'n', '</s>']
Filtered   (020): ['i', 'ou', '_', 'as', '_', 'iss', 'uer', '.', 'issue', '_', 'fund', 's', '(', 'amount', '_', 'issued', ',', ')', '\\', 'n']
Detokenized (008): ['iou_as_issuer', '.', 'issue_funds', '(', 'amount_issued', ',', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "github_info_json = urllib2 . urlopen ( latest ) . read ( ) \n"
Original    (013): ['github_info_json', '=', 'urllib2', '.', 'urlopen', '(', 'latest', ')', '.', 'read', '(', ')', '\\n']
Tokenized   (024): ['<s>', 'github', '_', 'info', '_', 'json', '=', 'ur', 'll', 'ib', '2', '.', 'url', 'open', '(', 'latest', ')', '.', 'read', '(', ')', '\\', 'n', '</s>']
Filtered   (022): ['github', '_', 'info', '_', 'json', '=', 'ur', 'll', 'ib', '2', '.', 'url', 'open', '(', 'latest', ')', '.', 'read', '(', ')', '\\', 'n']
Detokenized (013): ['github_info_json', '=', 'urllib2', '.', 'urlopen', '(', 'latest', ')', '.', 'read', '(', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "drawing_tool . set_window_title ( update_notifier , watching_player = twitch_username , updates_queued = len ( new_states_queue ) , read_delay = opt . read_delay ) \n"
Original    (024): ['drawing_tool', '.', 'set_window_title', '(', 'update_notifier', ',', 'watching_player', '=', 'twitch_username', ',', 'updates_queued', '=', 'len', '(', 'new_states_queue', ')', ',', 'read_delay', '=', 'opt', '.', 'read_delay', ')', '\\n']
Tokenized   (052): ['<s>', 'draw', 'ing', '_', 'tool', '.', 'set', '_', 'window', '_', 'title', '(', 'update', '_', 'not', 'ifier', ',', 'watching', '_', 'player', '=', 'twitch', '_', 'username', ',', 'updates', '_', 'que', 'ued', '=', 'len', '(', 'new', '_', 'states', '_', 'queue', ')', ',', 'read', '_', 'delay', '=', 'opt', '.', 'read', '_', 'delay', ')', '\\', 'n', '</s>']
Filtered   (050): ['draw', 'ing', '_', 'tool', '.', 'set', '_', 'window', '_', 'title', '(', 'update', '_', 'not', 'ifier', ',', 'watching', '_', 'player', '=', 'twitch', '_', 'username', ',', 'updates', '_', 'que', 'ued', '=', 'len', '(', 'new', '_', 'states', '_', 'queue', ')', ',', 'read', '_', 'delay', '=', 'opt', '.', 'read', '_', 'delay', ')', '\\', 'n']
Detokenized (024): ['drawing_tool', '.', 'set_window_title', '(', 'update_notifier', ',', 'watching_player', '=', 'twitch_username', ',', 'updates_queued', '=', 'len', '(', 'new_states_queue', ')', ',', 'read_delay', '=', 'opt', '.', 'read_delay', ')', '\\n']
Counter: 50
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "put_url = opt . trackerserver_url + "/tracker/api/update/" + opt . trackerserver_authkey \n"
Original    (012): ['put_url', '=', 'opt', '.', 'trackerserver_url', '+', '"/tracker/api/update/"', '+', 'opt', '.', 'trackerserver_authkey', '\\n']
Tokenized   (033): ['<s>', 'put', '_', 'url', '=', 'opt', '.', 'track', 'ers', 'erver', '_', 'url', '+', '"/', 'tr', 'acker', '/', 'api', '/', 'update', '/"', '+', 'opt', '.', 'track', 'ers', 'erver', '_', 'auth', 'key', '\\', 'n', '</s>']
Filtered   (031): ['put', '_', 'url', '=', 'opt', '.', 'track', 'ers', 'erver', '_', 'url', '+', '"/', 'tr', 'acker', '/', 'api', '/', 'update', '/"', '+', 'opt', '.', 'track', 'ers', 'erver', '_', 'auth', 'key', '\\', 'n']
Detokenized (012): ['put_url', '=', 'opt', '.', 'trackerserver_url', '+', '"/tracker/api/update/"', '+', 'opt', '.', 'trackerserver_authkey', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "json_string = json . dumps ( state , cls = TrackerStateEncoder , sort_keys = True ) \n"
Original    (017): ['json_string', '=', 'json', '.', 'dumps', '(', 'state', ',', 'cls', '=', 'TrackerStateEncoder', ',', 'sort_keys', '=', 'True', ')', '\\n']
Tokenized   (028): ['<s>', 'json', '_', 'string', '=', 'json', '.', 'dumps', '(', 'state', ',', 'cl', 's', '=', 'Tracker', 'State', 'Enc', 'oder', ',', 'sort', '_', 'keys', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (026): ['json', '_', 'string', '=', 'json', '.', 'dumps', '(', 'state', ',', 'cl', 's', '=', 'Tracker', 'State', 'Enc', 'oder', ',', 'sort', '_', 'keys', '=', 'True', ')', '\\', 'n']
Detokenized (017): ['json_string', '=', 'json', '.', 'dumps', '(', 'state', ',', 'cls', '=', 'TrackerStateEncoder', ',', 'sort_keys', '=', 'True', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "new_states_queue . pop ( 0 ) \n"
Original    (007): ['new_states_queue', '.', 'pop', '(', '0', ')', '\\n']
Tokenized   (014): ['<s>', 'new', '_', 'states', '_', 'queue', '.', 'pop', '(', '0', ')', '\\', 'n', '</s>']
Filtered   (012): ['new', '_', 'states', '_', 'queue', '.', 'pop', '(', '0', ')', '\\', 'n']
Detokenized (007): ['new_states_queue', '.', 'pop', '(', '0', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "framecount += 1 \n"
Original    (004): ['framecount', '+=', '1', '\\n']
Tokenized   (008): ['<s>', 'frame', 'count', '+=', '1', '\\', 'n', '</s>']
Filtered   (006): ['frame', 'count', '+=', '1', '\\', 'n']
Detokenized (004): ['framecount', '+=', '1', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "decay = decay , \n"
Original    (005): ['decay', '=', 'decay', ',', '\\n']
Tokenized   (009): ['<s>', 'dec', 'ay', '=', 'decay', ',', '\\', 'n', '</s>']
Filtered   (007): ['dec', 'ay', '=', 'decay', ',', '\\', 'n']
Detokenized (005): ['decay', '=', 'decay', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "expected_kwargs = { , } \n"
Original    (006): ['expected_kwargs', '=', '{', ',', '}', '\\n']
Tokenized   (012): ['<s>', 'expected', '_', 'kw', 'args', '=', '{', ',', '}', '\\', 'n', '</s>']
Filtered   (010): ['expected', '_', 'kw', 'args', '=', '{', ',', '}', '\\', 'n']
Detokenized (006): ['expected_kwargs', '=', '{', ',', '}', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "outputs [ ] = in_shapes [ ] \n"
Original    (008): ['outputs', '[', ']', '=', 'in_shapes', '[', ']', '\\n']
Tokenized   (015): ['<s>', 'output', 's', '[', ']', '=', 'in', '_', 'sh', 'apes', '[', ']', '\\', 'n', '</s>']
Filtered   (013): ['output', 's', '[', ']', '=', 'in', '_', 'sh', 'apes', '[', ']', '\\', 'n']
Detokenized (008): ['outputs', '[', ']', '=', 'in_shapes', '[', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "buf = BufferStructure ( self . in_shapes [ ] . feature_shape [ - 1 ] ) \n"
Original    (017): ['buf', '=', 'BufferStructure', '(', 'self', '.', 'in_shapes', '[', ']', '.', 'feature_shape', '[', '-', '1', ']', ')', '\\n']
Tokenized   (027): ['<s>', 'buf', '=', 'Buffer', 'St', 'ructure', '(', 'self', '.', 'in', '_', 'sh', 'apes', '[', ']', '.', 'feature', '_', 'shape', '[', '-', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (025): ['buf', '=', 'Buffer', 'St', 'ructure', '(', 'self', '.', 'in', '_', 'sh', 'apes', '[', ']', '.', 'feature', '_', 'shape', '[', '-', '1', ']', ')', '\\', 'n']
Detokenized (017): ['buf', '=', 'BufferStructure', '(', 'self', '.', 'in_shapes', '[', ']', '.', 'feature_shape', '[', '-', '1', ']', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "internals [ ] = self . in_shapes [ ] \n"
Original    (010): ['internals', '[', ']', '=', 'self', '.', 'in_shapes', '[', ']', '\\n']
Tokenized   (017): ['<s>', 'intern', 'als', '[', ']', '=', 'self', '.', 'in', '_', 'sh', 'apes', '[', ']', '\\', 'n', '</s>']
Filtered   (015): ['intern', 'als', '[', ']', '=', 'self', '.', 'in', '_', 'sh', 'apes', '[', ']', '\\', 'n']
Detokenized (010): ['internals', '[', ']', '=', 'self', '.', 'in_shapes', '[', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sigma_b , centered , x_hat = buffers . internals \n"
Original    (010): ['sigma_b', ',', 'centered', ',', 'x_hat', '=', 'buffers', '.', 'internals', '\\n']
Tokenized   (019): ['<s>', 's', 'igma', '_', 'b', ',', 'centered', ',', 'x', '_', 'hat', '=', 'buffers', '.', 'intern', 'als', '\\', 'n', '</s>']
Filtered   (017): ['s', 'igma', '_', 'b', ',', 'centered', ',', 'x', '_', 'hat', '=', 'buffers', '.', 'intern', 'als', '\\', 'n']
Detokenized (010): ['sigma_b', ',', 'centered', ',', 'x_hat', '=', 'buffers', '.', 'internals', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "dgamma = buffers . gradients . gamma \n"
Original    (008): ['dgamma', '=', 'buffers', '.', 'gradients', '.', 'gamma', '\\n']
Tokenized   (014): ['<s>', 'd', 'gam', 'ma', '=', 'buffers', '.', 'grad', 'ients', '.', 'gamma', '\\', 'n', '</s>']
Filtered   (012): ['d', 'gam', 'ma', '=', 'buffers', '.', 'grad', 'ients', '.', 'gamma', '\\', 'n']
Detokenized (008): ['dgamma', '=', 'buffers', '.', 'gradients', '.', 'gamma', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "outdeltas = flatten_all_but_last ( buffers . output_deltas . default ) \n"
Original    (011): ['outdeltas', '=', 'flatten_all_but_last', '(', 'buffers', '.', 'output_deltas', '.', 'default', ')', '\\n']
Tokenized   (028): ['<s>', 'out', 'd', 'elt', 'as', '=', 'flatt', 'en', '_', 'all', '_', 'but', '_', 'last', '(', 'buffers', '.', 'output', '_', 'd', 'elt', 'as', '.', 'default', ')', '\\', 'n', '</s>']
Filtered   (026): ['out', 'd', 'elt', 'as', '=', 'flatt', 'en', '_', 'all', '_', 'but', '_', 'last', '(', 'buffers', '.', 'output', '_', 'd', 'elt', 'as', '.', 'default', ')', '\\', 'n']
Detokenized (011): ['outdeltas', '=', 'flatten_all_but_last', '(', 'buffers', '.', 'output_deltas', '.', 'default', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "_h . add_tt ( term4 , indeltas , indeltas ) \n"
Original    (011): ['_h', '.', 'add_tt', '(', 'term4', ',', 'indeltas', ',', 'indeltas', ')', '\\n']
Tokenized   (022): ['<s>', '_', 'h', '.', 'add', '_', 'tt', '(', 'term', '4', ',', 'ind', 'elt', 'as', ',', 'ind', 'elt', 'as', ')', '\\', 'n', '</s>']
Filtered   (020): ['_', 'h', '.', 'add', '_', 'tt', '(', 'term', '4', ',', 'ind', 'elt', 'as', ',', 'ind', 'elt', 'as', ')', '\\', 'n']
Detokenized (011): ['_h', '.', 'add_tt', '(', 'term4', ',', 'indeltas', ',', 'indeltas', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "targets_name = , mask_name = ) : \n"
Original    (008): ['targets_name', '=', ',', 'mask_name', '=', ')', ':', '\\n']
Tokenized   (017): ['<s>', 't', 'arg', 'ets', '_', 'name', '=', ',', 'mask', '_', 'name', '=', ')', ':', '\\', 'n', '</s>']
Filtered   (015): ['t', 'arg', 'ets', '_', 'name', '=', ',', 'mask', '_', 'name', '=', ')', ':', '\\', 'n']
Detokenized (008): ['targets_name', '=', ',', 'mask_name', '=', ')', ':', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "mask_name = , name = None ) : \n"
Original    (009): ['mask_name', '=', ',', 'name', '=', 'None', ')', ':', '\\n']
Tokenized   (014): ['<s>', 'mask', '_', 'name', '=', ',', 'name', '=', 'None', ')', ':', '\\', 'n', '</s>']
Filtered   (012): ['mask', '_', 'name', '=', ',', 'name', '=', 'None', ')', ':', '\\', 'n']
Detokenized (009): ['mask_name', '=', ',', 'name', '=', 'None', ')', ':', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "true_labels ) . astype ( np . float ) \n"
Original    (010): ['true_labels', ')', '.', 'astype', '(', 'np', '.', 'float', ')', '\\n']
Tokenized   (017): ['<s>', 'true', '_', 'lab', 'els', ')', '.', 'ast', 'ype', '(', 'np', '.', 'float', ')', '\\', 'n', '</s>']
Filtered   (015): ['true', '_', 'lab', 'els', ')', '.', 'ast', 'ype', '(', 'np', '.', 'float', ')', '\\', 'n']
Detokenized (010): ['true_labels', ')', '.', 'astype', '(', 'np', '.', 'float', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "epochs = [ 0 ] * 4 + [ 1 ] * 4 + [ 2 ] * 4 \n"
Original    (020): ['epochs', '=', '[', '0', ']', '*', '4', '+', '[', '1', ']', '*', '4', '+', '[', '2', ']', '*', '4', '\\n']
Tokenized   (025): ['<s>', 'ep', 'och', 's', '=', '[', '0', ']', '*', '4', '+', '[', '1', ']', '*', '4', '+', '[', '2', ']', '*', '4', '\\', 'n', '</s>']
Filtered   (023): ['ep', 'och', 's', '=', '[', '0', ']', '*', '4', '+', '[', '1', ']', '*', '4', '+', '[', '2', ']', '*', '4', '\\', 'n']
Detokenized (020): ['epochs', '=', '[', '0', ']', '*', '4', '+', '[', '1', ']', '*', '4', '+', '[', '2', ']', '*', '4', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "on_rtd = os . environ . get ( , None ) == \n"
Original    (013): ['on_rtd', '=', 'os', '.', 'environ', '.', 'get', '(', ',', 'None', ')', '==', '\\n']
Tokenized   (020): ['<s>', 'on', '_', 'r', 'td', '=', 'os', '.', 'en', 'viron', '.', 'get', '(', ',', 'None', ')', '==', '\\', 'n', '</s>']
Filtered   (018): ['on', '_', 'r', 'td', '=', 'os', '.', 'en', 'viron', '.', 'get', '(', ',', 'None', ')', '==', '\\', 'n']
Detokenized (013): ['on_rtd', '=', 'os', '.', 'environ', '.', 'get', '(', ',', 'None', ')', '==', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "html_theme_path = [ sphinx_rtd_theme . get_html_theme_path ( ) ] \n"
Original    (010): ['html_theme_path', '=', '[', 'sphinx_rtd_theme', '.', 'get_html_theme_path', '(', ')', ']', '\\n']
Tokenized   (030): ['<s>', 'html', '_', 'theme', '_', 'path', '=', '[', 'sp', 'hin', 'x', '_', 'r', 'td', '_', 'theme', '.', 'get', '_', 'html', '_', 'theme', '_', 'path', '(', ')', ']', '\\', 'n', '</s>']
Filtered   (028): ['html', '_', 'theme', '_', 'path', '=', '[', 'sp', 'hin', 'x', '_', 'r', 'td', '_', 'theme', '.', 'get', '_', 'html', '_', 'theme', '_', 'path', '(', ')', ']', '\\', 'n']
Detokenized (010): ['html_theme_path', '=', '[', 'sphinx_rtd_theme', '.', 'get_html_theme_path', '(', ')', ']', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "latex_elements = { \n"
Original    (004): ['latex_elements', '=', '{', '\\n']
Tokenized   (011): ['<s>', 'late', 'x', '_', 'e', 'lements', '=', '{', '\\', 'n', '</s>']
Filtered   (009): ['late', 'x', '_', 'e', 'lements', '=', '{', '\\', 'n']
Detokenized (004): ['latex_elements', '=', '{', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "latex_documents = [ \n"
Original    (004): ['latex_documents', '=', '[', '\\n']
Tokenized   (011): ['<s>', 'late', 'x', '_', 'doc', 'uments', '=', '[', '\\', 'n', '</s>']
Filtered   (009): ['late', 'x', '_', 'doc', 'uments', '=', '[', '\\', 'n']
Detokenized (004): ['latex_documents', '=', '[', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "ignored_fallbacks = ( ) ) : \n"
Original    (007): ['ignored_fallbacks', '=', '(', ')', ')', ':', '\\n']
Tokenized   (014): ['<s>', 'ign', 'ored', '_', 'fall', 'backs', '=', '(', ')', ')', ':', '\\', 'n', '</s>']
Filtered   (012): ['ign', 'ored', '_', 'fall', 'backs', '=', '(', ')', ')', ':', '\\', 'n']
Detokenized (007): ['ignored_fallbacks', '=', '(', ')', ')', ':', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""b" : 2.0 , \n"
Original    (005): ['"b"', ':', '2.0', ',', '\\n']
Tokenized   (012): ['<s>', '"', 'b', '"', ':', '2', '.', '0', ',', '\\', 'n', '</s>']
Filtered   (010): ['"', 'b', '"', ':', '2', '.', '0', ',', '\\', 'n']
Detokenized (005): ['"b"', ':', '2.0', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""c" : True , \n"
Original    (005): ['"c"', ':', 'True', ',', '\\n']
Tokenized   (010): ['<s>', '"', 'c', '"', ':', 'True', ',', '\\', 'n', '</s>']
Filtered   (008): ['"', 'c', '"', ':', 'True', ',', '\\', 'n']
Detokenized (005): ['"c"', ':', 'True', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""d" : , \n"
Original    (004): ['"d"', ':', ',', '\\n']
Tokenized   (009): ['<s>', '"', 'd', '"', ':', ',', '\\', 'n', '</s>']
Filtered   (007): ['"', 'd', '"', ':', ',', '\\', 'n']
Detokenized (004): ['"d"', ':', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : ""e" : [ 1 , 2 , 3 ] , \n"
Original    (011): ['"e"', ':', '[', '1', ',', '2', ',', '3', ']', ',', '\\n']
Tokenized   (016): ['<s>', '"', 'e', '"', ':', '[', '1', ',', '2', ',', '3', ']', ',', '\\', 'n', '</s>']
Filtered   (014): ['"', 'e', '"', ':', '[', '1', ',', '2', ',', '3', ']', ',', '\\', 'n']
Detokenized (011): ['"e"', ':', '[', '1', ',', '2', ',', '3', ']', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""f" : { : , : } , \n"
Original    (009): ['"f"', ':', '{', ':', ',', ':', '}', ',', '\\n']
Tokenized   (014): ['<s>', '"', 'f', '"', ':', '{', ':', ',', ':', '}', ',', '\\', 'n', '</s>']
Filtered   (012): ['"', 'f', '"', ':', '{', ':', ',', ':', '}', ',', '\\', 'n']
Detokenized (009): ['"f"', ':', '{', ':', ',', ':', '}', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ""answer" : 42 \n"
Original    (004): ['"answer"', ':', '42', '\\n']
Tokenized   (009): ['<s>', '"', 'answer', '"', ':', '42', '\\', 'n', '</s>']
Filtered   (007): ['"', 'answer', '"', ':', '42', '\\', 'n']
Detokenized (004): ['"answer"', ':', '42', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "p_error = self . kp * current_error \n"
Original    (008): ['p_error', '=', 'self', '.', 'kp', '*', 'current_error', '\\n']
Tokenized   (016): ['<s>', 'p', '_', 'error', '=', 'self', '.', 'k', 'p', '*', 'current', '_', 'error', '\\', 'n', '</s>']
Filtered   (014): ['p', '_', 'error', '=', 'self', '.', 'k', 'p', '*', 'current', '_', 'error', '\\', 'n']
Detokenized (008): ['p_error', '=', 'self', '.', 'kp', '*', 'current_error', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "d_error = self . kd * ( current_error - self . previous_error ) / timestep \n"
Original    (016): ['d_error', '=', 'self', '.', 'kd', '*', '(', 'current_error', '-', 'self', '.', 'previous_error', ')', '/', 'timestep', '\\n']
Tokenized   (028): ['<s>', 'd', '_', 'error', '=', 'self', '.', 'k', 'd', '*', '(', 'current', '_', 'error', '-', 'self', '.', 'previous', '_', 'error', ')', '/', 'tim', 'est', 'ep', '\\', 'n', '</s>']
Filtered   (026): ['d', '_', 'error', '=', 'self', '.', 'k', 'd', '*', '(', 'current', '_', 'error', '-', 'self', '.', 'previous', '_', 'error', ')', '/', 'tim', 'est', 'ep', '\\', 'n']
Detokenized (016): ['d_error', '=', 'self', '.', 'kd', '*', '(', 'current_error', '-', 'self', '.', 'previous_error', ')', '/', 'timestep', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "current_error + self . previous_error ) / 2 + self . integral_error \n"
Original    (013): ['current_error', '+', 'self', '.', 'previous_error', ')', '/', '2', '+', 'self', '.', 'integral_error', '\\n']
Tokenized   (022): ['<s>', 'current', '_', 'error', '+', 'self', '.', 'previous', '_', 'error', ')', '/', '2', '+', 'self', '.', 'integral', '_', 'error', '\\', 'n', '</s>']
Filtered   (020): ['current', '_', 'error', '+', 'self', '.', 'previous', '_', 'error', ')', '/', '2', '+', 'self', '.', 'integral', '_', 'error', '\\', 'n']
Detokenized (013): ['current_error', '+', 'self', '.', 'previous_error', ')', '/', '2', '+', 'self', '.', 'integral_error', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "i_error = self . ki * self . integral_error \n"
Original    (010): ['i_error', '=', 'self', '.', 'ki', '*', 'self', '.', 'integral_error', '\\n']
Tokenized   (017): ['<s>', 'i', '_', 'error', '=', 'self', '.', 'ki', '*', 'self', '.', 'integral', '_', 'error', '\\', 'n', '</s>']
Filtered   (015): ['i', '_', 'error', '=', 'self', '.', 'ki', '*', 'self', '.', 'integral', '_', 'error', '\\', 'n']
Detokenized (010): ['i_error', '=', 'self', '.', 'ki', '*', 'self', '.', 'integral_error', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "total_error = p_error + d_error + i_error \n"
Original    (008): ['total_error', '=', 'p_error', '+', 'd_error', '+', 'i_error', '\\n']
Tokenized   (019): ['<s>', 'total', '_', 'error', '=', 'p', '_', 'error', '+', 'd', '_', 'error', '+', 'i', '_', 'error', '\\', 'n', '</s>']
Filtered   (017): ['total', '_', 'error', '=', 'p', '_', 'error', '+', 'd', '_', 'error', '+', 'i', '_', 'error', '\\', 'n']
Detokenized (008): ['total_error', '=', 'p_error', '+', 'd_error', '+', 'i_error', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "cmd_match_names = cmd . Cmd . completenames ( self , text , * ignored ) \n"
Original    (016): ['cmd_match_names', '=', 'cmd', '.', 'Cmd', '.', 'completenames', '(', 'self', ',', 'text', ',', '*', 'ignored', ')', '\\n']
Tokenized   (026): ['<s>', 'cmd', '_', 'match', '_', 'names', '=', 'cmd', '.', 'C', 'md', '.', 'comple', 'ten', 'ames', '(', 'self', ',', 'text', ',', '*', 'ignored', ')', '\\', 'n', '</s>']
Filtered   (024): ['cmd', '_', 'match', '_', 'names', '=', 'cmd', '.', 'C', 'md', '.', 'comple', 'ten', 'ames', '(', 'self', ',', 'text', ',', '*', 'ignored', ')', '\\', 'n']
Detokenized (016): ['cmd_match_names', '=', 'cmd', '.', 'Cmd', '.', 'completenames', '(', 'self', ',', 'text', ',', '*', 'ignored', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "obj_names = self . ctrl_client . objects . keys ( ) \n"
Original    (012): ['obj_names', '=', 'self', '.', 'ctrl_client', '.', 'objects', '.', 'keys', '(', ')', '\\n']
Tokenized   (020): ['<s>', 'obj', '_', 'names', '=', 'self', '.', 'c', 'trl', '_', 'client', '.', 'objects', '.', 'keys', '(', ')', '\\', 'n', '</s>']
Filtered   (018): ['obj', '_', 'names', '=', 'self', '.', 'c', 'trl', '_', 'client', '.', 'objects', '.', 'keys', '(', ')', '\\', 'n']
Detokenized (012): ['obj_names', '=', 'self', '.', 'ctrl_client', '.', 'objects', '.', 'keys', '(', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "api_match_names = [ x for x in obj_names if x . startswith ( text ) ] \n"
Original    (017): ['api_match_names', '=', '[', 'x', 'for', 'x', 'in', 'obj_names', 'if', 'x', '.', 'startswith', '(', 'text', ')', ']', '\\n']
Tokenized   (028): ['<s>', 'api', '_', 'match', '_', 'names', '=', '[', 'x', 'for', 'x', 'in', 'obj', '_', 'names', 'if', 'x', '.', 'start', 'sw', 'ith', '(', 'text', ')', ']', '\\', 'n', '</s>']
Filtered   (026): ['api', '_', 'match', '_', 'names', '=', '[', 'x', 'for', 'x', 'in', 'obj', '_', 'names', 'if', 'x', '.', 'start', 'sw', 'ith', '(', 'text', ')', ']', '\\', 'n']
Detokenized (017): ['api_match_names', '=', '[', 'x', 'for', 'x', 'in', 'obj_names', 'if', 'x', '.', 'startswith', '(', 'text', ')', ']', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "reply_time = self . ctrl_client . ping ( ) \n"
Original    (010): ['reply_time', '=', 'self', '.', 'ctrl_client', '.', 'ping', '(', ')', '\\n']
Tokenized   (018): ['<s>', 'reply', '_', 'time', '=', 'self', '.', 'c', 'trl', '_', 'client', '.', 'ping', '(', ')', '\\', 'n', '</s>']
Filtered   (016): ['reply', '_', 'time', '=', 'self', '.', 'c', 'trl', '_', 'client', '.', 'ping', '(', ')', '\\', 'n']
Detokenized (010): ['reply_time', '=', 'self', '.', 'ctrl_client', '.', 'ping', '(', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sub_addr = sys . argv [ 2 ] \n"
Original    (009): ['sub_addr', '=', 'sys', '.', 'argv', '[', '2', ']', '\\n']
Tokenized   (015): ['<s>', 'sub', '_', 'addr', '=', 'sys', '.', 'arg', 'v', '[', '2', ']', '\\', 'n', '</s>']
Filtered   (013): ['sub', '_', 'addr', '=', 'sys', '.', 'arg', 'v', '[', '2', ']', '\\', 'n']
Detokenized (009): ['sub_addr', '=', 'sys', '.', 'argv', '[', '2', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "CLI ( ctrl_addr , sub_addr ) . cmdloop ( ) \n"
Original    (011): ['CLI', '(', 'ctrl_addr', ',', 'sub_addr', ')', '.', 'cmdloop', '(', ')', '\\n']
Tokenized   (021): ['<s>', 'CL', 'I', '(', 'c', 'trl', '_', 'addr', ',', 'sub', '_', 'addr', ')', '.', 'cmd', 'loop', '(', ')', '\\', 'n', '</s>']
Filtered   (019): ['CL', 'I', '(', 'c', 'trl', '_', 'addr', ',', 'sub', '_', 'addr', ')', '.', 'cmd', 'loop', '(', ')', '\\', 'n']
Detokenized (011): ['CLI', '(', 'ctrl_addr', ',', 'sub_addr', ')', '.', 'cmdloop', '(', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "duty = int ( cur_pwm [ "duty_ns" ] ) \n"
Original    (010): ['duty', '=', 'int', '(', 'cur_pwm', '[', '"duty_ns"', ']', ')', '\\n']
Tokenized   (020): ['<s>', 'duty', '=', 'int', '(', 'cur', '_', 'p', 'wm', '[', '"', 'duty', '_', 'ns', '"', ']', ')', '\\', 'n', '</s>']
Filtered   (018): ['duty', '=', 'int', '(', 'cur', '_', 'p', 'wm', '[', '"', 'duty', '_', 'ns', '"', ']', ')', '\\', 'n']
Detokenized (010): ['duty', '=', 'int', '(', 'cur_pwm', '[', '"duty_ns"', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "read_pos = int ( round ( ( ( duty - 580000 ) / 2320000. ) * 180 ) ) \n"
Original    (020): ['read_pos', '=', 'int', '(', 'round', '(', '(', '(', 'duty', '-', '580000', ')', '/', '2320000.', ')', '*', '180', ')', ')', '\\n']
Tokenized   (029): ['<s>', 'read', '_', 'pos', '=', 'int', '(', 'round', '(', '(', '(', 'duty', '-', '58', '0000', ')', '/', '23', '200', '00', '.', ')', '*', '180', ')', ')', '\\', 'n', '</s>']
Filtered   (027): ['read', '_', 'pos', '=', 'int', '(', 'round', '(', '(', '(', 'duty', '-', '58', '0000', ')', '/', '23', '200', '00', '.', ')', '*', '180', ')', ')', '\\', 'n']
Detokenized (020): ['read_pos', '=', 'int', '(', 'round', '(', '(', '(', 'duty', '-', '580000', ')', '/', '2320000.', ')', '*', '180', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "create_login_url , create_logout_url \n"
Original    (004): ['create_login_url', ',', 'create_logout_url', '\\n']
Tokenized   (016): ['<s>', 'create', '_', 'login', '_', 'url', ',', 'create', '_', 'log', 'out', '_', 'url', '\\', 'n', '</s>']
Filtered   (014): ['create', '_', 'login', '_', 'url', ',', 'create', '_', 'log', 'out', '_', 'url', '\\', 'n']
Detokenized (004): ['create_login_url', ',', 'create_logout_url', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "create_logout_url ( request . url ) \n"
Original    (007): ['create_logout_url', '(', 'request', '.', 'url', ')', '\\n']
Tokenized   (015): ['<s>', 'create', '_', 'log', 'out', '_', 'url', '(', 'request', '.', 'url', ')', '\\', 'n', '</s>']
Filtered   (013): ['create', '_', 'log', 'out', '_', 'url', '(', 'request', '.', 'url', ')', '\\', 'n']
Detokenized (007): ['create_logout_url', '(', 'request', '.', 'url', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "_value = _options_header_vkw ( _value , kw ) \n"
Original    (009): ['_value', '=', '_options_header_vkw', '(', '_value', ',', 'kw', ')', '\\n']
Tokenized   (021): ['<s>', '_', 'value', '=', '_', 'options', '_', 'header', '_', 'v', 'kw', '(', '_', 'value', ',', 'k', 'w', ')', '\\', 'n', '</s>']
Filtered   (019): ['_', 'value', '=', '_', 'options', '_', 'header', '_', 'v', 'kw', '(', '_', 'value', ',', 'k', 'w', ')', '\\', 'n']
Detokenized (009): ['_value', '=', '_options_header_vkw', '(', '_value', ',', 'kw', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "value_type == value_subtype == ) or \n"
Original    (007): ['value_type', '==', 'value_subtype', '==', ')', 'or', '\\n']
Tokenized   (015): ['<s>', 'value', '_', 'type', '==', 'value', '_', 'sub', 'type', '==', ')', 'or', '\\', 'n', '</s>']
Filtered   (013): ['value', '_', 'type', '==', 'value', '_', 'sub', 'type', '==', ')', 'or', '\\', 'n']
Detokenized (007): ['value_type', '==', 'value_subtype', '==', ')', 'or', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "value_subtype == or \n"
Original    (004): ['value_subtype', '==', 'or', '\\n']
Tokenized   (010): ['<s>', 'value', '_', 'sub', 'type', '==', 'or', '\\', 'n', '</s>']
Filtered   (008): ['value', '_', 'sub', 'type', '==', 'or', '\\', 'n']
Detokenized (004): ['value_subtype', '==', 'or', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "item_subtype == value_subtype ) ) \n"
Original    (006): ['item_subtype', '==', 'value_subtype', ')', ')', '\\n']
Tokenized   (015): ['<s>', 'item', '_', 'sub', 'type', '==', 'value', '_', 'sub', 'type', ')', ')', '\\', 'n', '</s>']
Filtered   (013): ['item', '_', 'sub', 'type', '==', 'value', '_', 'sub', 'type', ')', ')', '\\', 'n']
Detokenized (006): ['item_subtype', '==', 'value_subtype', ')', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "no_cache = cache_property ( , , None ) \n"
Original    (009): ['no_cache', '=', 'cache_property', '(', ',', ',', 'None', ')', '\\n']
Tokenized   (016): ['<s>', 'no', '_', 'cache', '=', 'cache', '_', 'property', '(', ',', ',', 'None', ')', '\\', 'n', '</s>']
Filtered   (014): ['no', '_', 'cache', '=', 'cache', '_', 'property', '(', ',', ',', 'None', ')', '\\', 'n']
Detokenized (009): ['no_cache', '=', 'cache_property', '(', ',', ',', 'None', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "no_store = cache_property ( , None , bool ) \n"
Original    (010): ['no_store', '=', 'cache_property', '(', ',', 'None', ',', 'bool', ')', '\\n']
Tokenized   (017): ['<s>', 'no', '_', 'store', '=', 'cache', '_', 'property', '(', ',', 'None', ',', 'bool', ')', '\\', 'n', '</s>']
Filtered   (015): ['no', '_', 'store', '=', 'cache', '_', 'property', '(', ',', 'None', ',', 'bool', ')', '\\', 'n']
Detokenized (010): ['no_store', '=', 'cache_property', '(', ',', 'None', ',', 'bool', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "max_age = cache_property ( , - 1 , int ) \n"
Original    (011): ['max_age', '=', 'cache_property', '(', ',', '-', '1', ',', 'int', ')', '\\n']
Tokenized   (018): ['<s>', 'max', '_', 'age', '=', 'cache', '_', 'property', '(', ',', '-', '1', ',', 'int', ')', '\\', 'n', '</s>']
Filtered   (016): ['max', '_', 'age', '=', 'cache', '_', 'property', '(', ',', '-', '1', ',', 'int', ')', '\\', 'n']
Detokenized (011): ['max_age', '=', 'cache_property', '(', ',', '-', '1', ',', 'int', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "no_transform = cache_property ( , None , None ) \n"
Original    (010): ['no_transform', '=', 'cache_property', '(', ',', 'None', ',', 'None', ')', '\\n']
Tokenized   (017): ['<s>', 'no', '_', 'transform', '=', 'cache', '_', 'property', '(', ',', 'None', ',', 'None', ')', '\\', 'n', '</s>']
Filtered   (015): ['no', '_', 'transform', '=', 'cache', '_', 'property', '(', ',', 'None', ',', 'None', ')', '\\', 'n']
Detokenized (010): ['no_transform', '=', 'cache_property', '(', ',', 'None', ',', 'None', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "max_stale = cache_property ( , , int ) \n"
Original    (009): ['max_stale', '=', 'cache_property', '(', ',', ',', 'int', ')', '\\n']
Tokenized   (017): ['<s>', 'max', '_', 'st', 'ale', '=', 'cache', '_', 'property', '(', ',', ',', 'int', ')', '\\', 'n', '</s>']
Filtered   (015): ['max', '_', 'st', 'ale', '=', 'cache', '_', 'property', '(', ',', ',', 'int', ')', '\\', 'n']
Detokenized (009): ['max_stale', '=', 'cache_property', '(', ',', ',', 'int', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "etag , weak = unquote_etag ( etag ) \n"
Original    (009): ['etag', ',', 'weak', '=', 'unquote_etag', '(', 'etag', ')', '\\n']
Tokenized   (018): ['<s>', 'et', 'ag', ',', 'weak', '=', 'un', 'quote', '_', 'et', 'ag', '(', 'et', 'ag', ')', '\\', 'n', '</s>']
Filtered   (016): ['et', 'ag', ',', 'weak', '=', 'un', 'quote', '_', 'et', 'ag', '(', 'et', 'ag', ')', '\\', 'n']
Detokenized (009): ['etag', ',', 'weak', '=', 'unquote_etag', '(', 'etag', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "uri = property ( lambda x : x . get ( ) , doc = ) \n"
Original    (017): ['uri', '=', 'property', '(', 'lambda', 'x', ':', 'x', '.', 'get', '(', ')', ',', 'doc', '=', ')', '\\n']
Tokenized   (020): ['<s>', 'uri', '=', 'property', '(', 'lambda', 'x', ':', 'x', '.', 'get', '(', ')', ',', 'doc', '=', ')', '\\', 'n', '</s>']
Filtered   (018): ['uri', '=', 'property', '(', 'lambda', 'x', ':', 'x', '.', 'get', '(', ')', ',', 'doc', '=', ')', '\\', 'n']
Detokenized (017): ['uri', '=', 'property', '(', 'lambda', 'x', ':', 'x', '.', 'get', '(', ')', ',', 'doc', '=', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "_require_quoting = frozenset ( [ , , , ] ) \n"
Original    (011): ['_require_quoting', '=', 'frozenset', '(', '[', ',', ',', ',', ']', ')', '\\n']
Tokenized   (020): ['<s>', '_', 'require', '_', 'qu', 'oting', '=', 'fro', 'zens', 'et', '(', '[', ',', ',', ',', ']', ')', '\\', 'n', '</s>']
Filtered   (018): ['_', 'require', '_', 'qu', 'oting', '=', 'fro', 'zens', 'et', '(', '[', ',', ',', ',', ']', ')', '\\', 'n']
Detokenized (011): ['_require_quoting', '=', 'frozenset', '(', '[', ',', ',', ',', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "auth_type = d . pop ( , None ) or \n"
Original    (011): ['auth_type', '=', 'd', '.', 'pop', '(', ',', 'None', ')', 'or', '\\n']
Tokenized   (016): ['<s>', 'auth', '_', 'type', '=', 'd', '.', 'pop', '(', ',', 'None', ')', 'or', '\\', 'n', '</s>']
Filtered   (014): ['auth', '_', 'type', '=', 'd', '.', 'pop', '(', ',', 'None', ')', 'or', '\\', 'n']
Detokenized (011): ['auth_type', '=', 'd', '.', 'pop', '(', ',', 'None', ')', 'or', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "allow_token = key not in self . _require_quoting ) ) \n"
Original    (011): ['allow_token', '=', 'key', 'not', 'in', 'self', '.', '_require_quoting', ')', ')', '\\n']
Tokenized   (020): ['<s>', 'allow', '_', 'token', '=', 'key', 'not', 'in', 'self', '.', '_', 'require', '_', 'qu', 'oting', ')', ')', '\\', 'n', '</s>']
Filtered   (018): ['allow', '_', 'token', '=', 'key', 'not', 'in', 'self', '.', '_', 'require', '_', 'qu', 'oting', ')', ')', '\\', 'n']
Detokenized (011): ['allow_token', '=', 'key', 'not', 'in', 'self', '.', '_require_quoting', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "realm = auth_property ( , doc = ) \n"
Original    (009): ['realm', '=', 'auth_property', '(', ',', 'doc', '=', ')', '\\n']
Tokenized   (015): ['<s>', 'real', 'm', '=', 'auth', '_', 'property', '(', ',', 'doc', '=', ')', '\\', 'n', '</s>']
Filtered   (013): ['real', 'm', '=', 'auth', '_', 'property', '(', ',', 'doc', '=', ')', '\\', 'n']
Detokenized (009): ['realm', '=', 'auth_property', '(', ',', 'doc', '=', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "rshell , shell , clear_datastore , create_user , \n"
Original    (009): ['rshell', ',', 'shell', ',', 'clear_datastore', ',', 'create_user', ',', '\\n']
Tokenized   (019): ['<s>', 'rs', 'hell', ',', 'shell', ',', 'clear', '_', 'dat', 'ast', 'ore', ',', 'create', '_', 'user', ',', '\\', 'n', '</s>']
Filtered   (017): ['rs', 'hell', ',', 'shell', ',', 'clear', '_', 'dat', 'ast', 'ore', ',', 'create', '_', 'user', ',', '\\', 'n']
Detokenized (009): ['rshell', ',', 'shell', ',', 'clear_datastore', ',', 'create_user', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "Rule ( , endpoint = , \n"
Original    (007): ['Rule', '(', ',', 'endpoint', '=', ',', '\\n']
Tokenized   (010): ['<s>', 'Rule', '(', ',', 'endpoint', '=', ',', '\\', 'n', '</s>']
Filtered   (008): ['Rule', '(', ',', 'endpoint', '=', ',', '\\', 'n']
Detokenized (007): ['Rule', '(', ',', 'endpoint', '=', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "data_field = db . StringProperty ( required = True , \n"
Original    (011): ['data_field', '=', 'db', '.', 'StringProperty', '(', 'required', '=', 'True', ',', '\\n']
Tokenized   (017): ['<s>', 'data', '_', 'field', '=', 'db', '.', 'String', 'Property', '(', 'required', '=', 'True', ',', '\\', 'n', '</s>']
Filtered   (015): ['data', '_', 'field', '=', 'db', '.', 'String', 'Property', '(', 'required', '=', 'True', ',', '\\', 'n']
Detokenized (011): ['data_field', '=', 'db', '.', 'StringProperty', '(', 'required', '=', 'True', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "dst_name = path . join ( dst_path , filename ) \n"
Original    (011): ['dst_name', '=', 'path', '.', 'join', '(', 'dst_path', ',', 'filename', ')', '\\n']
Tokenized   (019): ['<s>', 'd', 'st', '_', 'name', '=', 'path', '.', 'join', '(', 'dst', '_', 'path', ',', 'filename', ')', '\\', 'n', '</s>']
Filtered   (017): ['d', 'st', '_', 'name', '=', 'path', '.', 'join', '(', 'dst', '_', 'path', ',', 'filename', ')', '\\', 'n']
Detokenized (011): ['dst_name', '=', 'path', '.', 'join', '(', 'dst_path', ',', 'filename', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "modifiable_problem_fields = [ "description" ] \n"
Original    (006): ['modifiable_problem_fields', '=', '[', '"description"', ']', '\\n']
Tokenized   (016): ['<s>', 'mod', 'ifiable', '_', 'problem', '_', 'fields', '=', '[', '"', 'description', '"', ']', '\\', 'n', '</s>']
Filtered   (014): ['mod', 'ifiable', '_', 'problem', '_', 'fields', '=', '[', '"', 'description', '"', ']', '\\', 'n']
Detokenized (006): ['modifiable_problem_fields', '=', '[', '"description"', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "problem = api . problem . get_problem ( pid = pid ) \n"
Original    (013): ['problem', '=', 'api', '.', 'problem', '.', 'get_problem', '(', 'pid', '=', 'pid', ')', '\\n']
Tokenized   (018): ['<s>', 'problem', '=', 'api', '.', 'problem', '.', 'get', '_', 'problem', '(', 'pid', '=', 'pid', ')', '\\', 'n', '</s>']
Filtered   (016): ['problem', '=', 'api', '.', 'problem', '.', 'get', '_', 'problem', '(', 'pid', '=', 'pid', ')', '\\', 'n']
Detokenized (013): ['problem', '=', 'api', '.', 'problem', '.', 'get_problem', '(', 'pid', '=', 'pid', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "build = get_generator ( pid ) . generate ( random , pid , api . autogen_tools , n ) \n"
Original    (020): ['build', '=', 'get_generator', '(', 'pid', ')', '.', 'generate', '(', 'random', ',', 'pid', ',', 'api', '.', 'autogen_tools', ',', 'n', ')', '\\n']
Tokenized   (029): ['<s>', 'build', '=', 'get', '_', 'gener', 'ator', '(', 'pid', ')', '.', 'generate', '(', 'random', ',', 'pid', ',', 'api', '.', 'aut', 'ogen', '_', 'tools', ',', 'n', ')', '\\', 'n', '</s>']
Filtered   (027): ['build', '=', 'get', '_', 'gener', 'ator', '(', 'pid', ')', '.', 'generate', '(', 'random', ',', 'pid', ',', 'api', '.', 'aut', 'ogen', '_', 'tools', ',', 'n', ')', '\\', 'n']
Detokenized (020): ['build', '=', 'get_generator', '(', 'pid', ')', '.', 'generate', '(', 'random', ',', 'pid', ',', 'api', '.', 'autogen_tools', ',', 'n', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "autogen_instance_path = get_instance_path ( pid , n = n ) \n"
Original    (011): ['autogen_instance_path', '=', 'get_instance_path', '(', 'pid', ',', 'n', '=', 'n', ')', '\\n']
Tokenized   (023): ['<s>', 'aut', 'ogen', '_', 'instance', '_', 'path', '=', 'get', '_', 'instance', '_', 'path', '(', 'pid', ',', 'n', '=', 'n', ')', '\\', 'n', '</s>']
Filtered   (021): ['aut', 'ogen', '_', 'instance', '_', 'path', '=', 'get', '_', 'instance', '_', 'path', '(', 'pid', ',', 'n', '=', 'n', ')', '\\', 'n']
Detokenized (011): ['autogen_instance_path', '=', 'get_instance_path', '(', 'pid', ',', 'n', '=', 'n', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""resource_files" : { \n"
Original    (004): ['"resource_files"', ':', '{', '\\n']
Tokenized   (011): ['<s>', '"', 'resource', '_', 'files', '"', ':', '{', '\\', 'n', '</s>']
Filtered   (009): ['"', 'resource', '_', 'files', '"', ':', '{', '\\', 'n']
Detokenized (004): ['"resource_files"', ':', '{', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "instance_path = path . join ( path . dirname ( generator_path ) , "instances" , name , str ( n ) ) \n"
Original    (023): ['instance_path', '=', 'path', '.', 'join', '(', 'path', '.', 'dirname', '(', 'generator_path', ')', ',', '"instances"', ',', 'name', ',', 'str', '(', 'n', ')', ')', '\\n']
Tokenized   (034): ['<s>', 'instance', '_', 'path', '=', 'path', '.', 'join', '(', 'path', '.', 'dir', 'name', '(', 'generator', '_', 'path', ')', ',', '"', 'inst', 'ances', '"', ',', 'name', ',', 'str', '(', 'n', ')', ')', '\\', 'n', '</s>']
Filtered   (032): ['instance', '_', 'path', '=', 'path', '.', 'join', '(', 'path', '.', 'dir', 'name', '(', 'generator', '_', 'path', ')', ',', '"', 'inst', 'ances', '"', ',', 'name', ',', 'str', '(', 'n', ')', ')', '\\', 'n']
Detokenized (023): ['instance_path', '=', 'path', '.', 'join', '(', 'path', '.', 'dirname', '(', 'generator_path', ')', ',', '"instances"', ',', 'name', ',', 'str', '(', 'n', ')', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : ""correct" : correct , \n"
Original    (005): ['"correct"', ':', 'correct', ',', '\\n']
Tokenized   (010): ['<s>', '"', 'correct', '"', ':', 'correct', ',', '\\', 'n', '</s>']
Filtered   (008): ['"', 'correct', '"', ':', 'correct', ',', '\\', 'n']
Detokenized (005): ['"correct"', ':', 'correct', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""points" : problem [ "score" ] , \n"
Original    (008): ['"points"', ':', 'problem', '[', '"score"', ']', ',', '\\n']
Tokenized   (015): ['<s>', '"', 'points', '"', ':', 'problem', '[', '"', 'score', '"', ']', ',', '\\', 'n', '</s>']
Filtered   (013): ['"', 'points', '"', ':', 'problem', '[', '"', 'score', '"', ']', ',', '\\', 'n']
Detokenized (008): ['"points"', ':', 'problem', '[', '"score"', ']', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : ""message" : message \n"
Original    (004): ['"message"', ':', 'message', '\\n']
Tokenized   (009): ['<s>', '"', 'message', '"', ':', 'message', '\\', 'n', '</s>']
Filtered   (007): ['"', 'message', '"', ':', 'message', '\\', 'n']
Detokenized (004): ['"message"', ':', 'message', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "k = str ( random . randint ( 0 , 1000 ) ) \n"
Original    (014): ['k', '=', 'str', '(', 'random', '.', 'randint', '(', '0', ',', '1000', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'k', '=', 'str', '(', 'random', '.', 'rand', 'int', '(', '0', ',', '1000', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['k', '=', 'str', '(', 'random', '.', 'rand', 'int', '(', '0', ',', '1000', ')', ')', '\\', 'n']
Detokenized (014): ['k', '=', 'str', '(', 'random', '.', 'randint', '(', '0', ',', '1000', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : ""public" : [ ( "/tmp/key" , "public_static" ) ] , \n"
Original    (011): ['"public"', ':', '[', '(', '"/tmp/key"', ',', '"public_static"', ')', ']', ',', '\\n']
Tokenized   (024): ['<s>', '"', 'public', '"', ':', '[', '(', '"/', 'tmp', '/', 'key', '"', ',', '"', 'public', '_', 'static', '"', ')', ']', ',', '\\', 'n', '</s>']
Filtered   (022): ['"', 'public', '"', ':', '[', '(', '"/', 'tmp', '/', 'key', '"', ',', '"', 'public', '_', 'static', '"', ')', ']', ',', '\\', 'n']
Detokenized (011): ['"public"', ':', '[', '(', '"/tmp/key"', ',', '"public_static"', ')', ']', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""private" : [ ( "/tmp/key" , "private_static" ) ] \n"
Original    (010): ['"private"', ':', '[', '(', '"/tmp/key"', ',', '"private_static"', ')', ']', '\\n']
Tokenized   (023): ['<s>', '"', 'private', '"', ':', '[', '(', '"/', 'tmp', '/', 'key', '"', ',', '"', 'private', '_', 'static', '"', ')', ']', '\\', 'n', '</s>']
Filtered   (021): ['"', 'private', '"', ':', '[', '(', '"/', 'tmp', '/', 'key', '"', ',', '"', 'private', '_', 'static', '"', ')', ']', '\\', 'n']
Detokenized (010): ['"private"', ':', '[', '(', '"/tmp/key"', ',', '"private_static"', ')', ']', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "layout = eval ( scriptWindow . setLayout ( layout ) \n"
Original    (011): ['layout', '=', 'eval', '(', 'scriptWindow', '.', 'setLayout', '(', 'layout', ')', '\\n']
Tokenized   (016): ['<s>', 'layout', '=', 'eval', '(', 'script', 'Window', '.', 'set', 'Layout', '(', 'layout', ')', '\\', 'n', '</s>']
Filtered   (014): ['layout', '=', 'eval', '(', 'script', 'Window', '.', 'set', 'Layout', '(', 'layout', ')', '\\', 'n']
Detokenized (011): ['layout', '=', 'eval', '(', 'scriptWindow', '.', 'setLayout', '(', 'layout', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "scriptWindow . _Widget__qtWidget . resize ( 995 , 500 ) \n"
Original    (011): ['scriptWindow', '.', '_Widget__qtWidget', '.', 'resize', '(', '995', ',', '500', ')', '\\n']
Tokenized   (020): ['<s>', 'script', 'Window', '.', '_', 'Widget', '__', 'qt', 'Widget', '.', 'resize', '(', '9', '95', ',', '500', ')', '\\', 'n', '</s>']
Filtered   (018): ['script', 'Window', '.', '_', 'Widget', '__', 'qt', 'Widget', '.', 'resize', '(', '9', '95', ',', '500', ')', '\\', 'n']
Detokenized (011): ['scriptWindow', '.', '_Widget__qtWidget', '.', 'resize', '(', '995', ',', '500', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""inputSequence" , \n"
Original    (003): ['"inputSequence"', ',', '\\n']
Tokenized   (010): ['<s>', '"', 'input', 'Sequ', 'ence', '"', ',', '\\', 'n', '</s>']
Filtered   (008): ['"', 'input', 'Sequ', 'ence', '"', ',', '\\', 'n']
Detokenized (003): ['"inputSequence"', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "defaultValue = "" , \n"
Original    (005): ['defaultValue', '=', '""', ',', '\\n']
Tokenized   (009): ['<s>', 'default', 'Value', '=', '""', ',', '\\', 'n', '</s>']
Filtered   (007): ['default', 'Value', '=', '""', ',', '\\', 'n']
Detokenized (005): ['defaultValue', '=', '""', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "dc . write ( "-" + objectName , "bound" , b ) \n"
Original    (013): ['dc', '.', 'write', '(', '"-"', '+', 'objectName', ',', '"bound"', ',', 'b', ')', '\\n']
Tokenized   (020): ['<s>', 'dc', '.', 'write', '(', '"', '-"', '+', 'object', 'Name', ',', '"', 'bound', '"', ',', 'b', ')', '\\', 'n', '</s>']
Filtered   (018): ['dc', '.', 'write', '(', '"', '-"', '+', 'object', 'Name', ',', '"', 'bound', '"', ',', 'b', ')', '\\', 'n']
Detokenized (013): ['dc', '.', 'write', '(', '"-"', '+', 'objectName', ',', '"bound"', ',', 'b', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "additionalTerminalPlugTypes = ( GafferScene . ScenePlug , ) \n"
Original    (009): ['additionalTerminalPlugTypes', '=', '(', 'GafferScene', '.', 'ScenePlug', ',', ')', '\\n']
Tokenized   (020): ['<s>', 'add', 'itional', 'Termin', 'al', 'Plug', 'Types', '=', '(', 'G', 'affer', 'Scene', '.', 'Scene', 'Plug', ',', ')', '\\', 'n', '</s>']
Filtered   (018): ['add', 'itional', 'Termin', 'al', 'Plug', 'Types', '=', '(', 'G', 'affer', 'Scene', '.', 'Scene', 'Plug', ',', ')', '\\', 'n']
Detokenized (009): ['additionalTerminalPlugTypes', '=', '(', 'GafferScene', '.', 'ScenePlug', ',', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "replace = context . get ( "textWriter:replace" , IECore . StringVectorData ( ) ) \n"
Original    (015): ['replace', '=', 'context', '.', 'get', '(', '"textWriter:replace"', ',', 'IECore', '.', 'StringVectorData', '(', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'replace', '=', 'context', '.', 'get', '(', '"', 'text', 'Writer', ':', 'replace', '"', ',', 'I', 'EC', 'ore', '.', 'String', 'Vector', 'Data', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['replace', '=', 'context', '.', 'get', '(', '"', 'text', 'Writer', ':', 'replace', '"', ',', 'I', 'EC', 'ore', '.', 'String', 'Vector', 'Data', '(', ')', ')', '\\', 'n']
Detokenized (015): ['replace', '=', 'context', '.', 'get', '(', '"textWriter:replace"', ',', 'IECore', '.', 'StringVectorData', '(', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "inMetadata = r [ "out" ] [ "metadata" ] . getValue ( ) \n"
Original    (014): ['inMetadata', '=', 'r', '[', '"out"', ']', '[', '"metadata"', ']', '.', 'getValue', '(', ')', '\\n']
Tokenized   (024): ['<s>', 'in', 'Met', 'adata', '=', 'r', '[', '"', 'out', '"', ']', '[', '"', 'metadata', '"', ']', '.', 'get', 'Value', '(', ')', '\\', 'n', '</s>']
Filtered   (022): ['in', 'Met', 'adata', '=', 'r', '[', '"', 'out', '"', ']', '[', '"', 'metadata', '"', ']', '.', 'get', 'Value', '(', ')', '\\', 'n']
Detokenized (014): ['inMetadata', '=', 'r', '[', '"out"', ']', '[', '"metadata"', ']', '.', 'getValue', '(', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "negFileName = os . path . expandvars ( "$GAFFER_ROOT/python/GafferImageTest/images/checkerWithNegativeDataWindow.200x150.exr" \n"
Original    (010): ['negFileName', '=', 'os', '.', 'path', '.', 'expandvars', '(', '"$GAFFER_ROOT/python/GafferImageTest/images/checkerWithNegativeDataWindow.200x150.exr"', '\\n']
Tokenized   (047): ['<s>', 'neg', 'File', 'Name', '=', 'os', '.', 'path', '.', 'expand', 'v', 'ars', '(', '"$', 'GA', 'FFER', '_', 'RO', 'OT', '/', 'python', '/', 'G', 'affer', 'Image', 'Test', '/', 'images', '/', 'check', 'er', 'With', 'Neg', 'ative', 'Data', 'Window', '.', '200', 'x', '150', '.', 'ex', 'r', '"', '\\', 'n', '</s>']
Filtered   (045): ['neg', 'File', 'Name', '=', 'os', '.', 'path', '.', 'expand', 'v', 'ars', '(', '"$', 'GA', 'FFER', '_', 'RO', 'OT', '/', 'python', '/', 'G', 'affer', 'Image', 'Test', '/', 'images', '/', 'check', 'er', 'With', 'Neg', 'ative', 'Data', 'Window', '.', '200', 'x', '150', '.', 'ex', 'r', '"', '\\', 'n']
Detokenized (010): ['negFileName', '=', 'os', '.', 'path', '.', 'expandvars', '(', '"$GAFFER_ROOT/python/GafferImageTest/images/checkerWithNegativeDataWindow.200x150.exr"', '\\n']
Counter: 45
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "GafferImage . Display , \n"
Original    (005): ['GafferImage', '.', 'Display', ',', '\\n']
Tokenized   (010): ['<s>', 'G', 'affer', 'Image', '.', 'Display', ',', '\\', 'n', '</s>']
Filtered   (008): ['G', 'affer', 'Image', '.', 'Display', ',', '\\', 'n']
Detokenized (005): ['GafferImage', '.', 'Display', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""port" : [ \n"
Original    (004): ['"port"', ':', '[', '\\n']
Tokenized   (009): ['<s>', '"', 'port', '"', ':', '[', '\\', 'n', '</s>']
Filtered   (007): ['"', 'port', '"', ':', '[', '\\', 'n']
Detokenized (004): ['"port"', ':', '[', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "updateCountPlug . setValue ( updateCountPlug . getValue ( ) + 1 ) \n"
Original    (013): ['updateCountPlug', '.', 'setValue', '(', 'updateCountPlug', '.', 'getValue', '(', ')', '+', '1', ')', '\\n']
Tokenized   (022): ['<s>', 'update', 'Count', 'Plug', '.', 'set', 'Value', '(', 'update', 'Count', 'Plug', '.', 'get', 'Value', '(', ')', '+', '1', ')', '\\', 'n', '</s>']
Filtered   (020): ['update', 'Count', 'Plug', '.', 'set', 'Value', '(', 'update', 'Count', 'Plug', '.', 'get', 'Value', '(', ')', '+', '1', ')', '\\', 'n']
Detokenized (013): ['updateCountPlug', '.', 'setValue', '(', 'updateCountPlug', '.', 'getValue', '(', ')', '+', '1', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "__import__ ( "IECore" ) . loadConfig ( "GAFFER_STARTUP_PATHS" , { } , subdirectory = "GafferImageUI" ) \n"
Original    (017): ['__import__', '(', '"IECore"', ')', '.', 'loadConfig', '(', '"GAFFER_STARTUP_PATHS"', ',', '{', '}', ',', 'subdirectory', '=', '"GafferImageUI"', ')', '\\n']
Tokenized   (044): ['<s>', '__', 'import', '__', '(', '"', 'I', 'EC', 'ore', '"', ')', '.', 'load', 'Config', '(', '"', 'GA', 'FFER', '_', 'ST', 'ART', 'UP', '_', 'P', 'AT', 'HS', '"', ',', '{', '}', ',', 'sub', 'directory', '=', '"', 'G', 'affer', 'Image', 'UI', '"', ')', '\\', 'n', '</s>']
Filtered   (042): ['__', 'import', '__', '(', '"', 'I', 'EC', 'ore', '"', ')', '.', 'load', 'Config', '(', '"', 'GA', 'FFER', '_', 'ST', 'ART', 'UP', '_', 'P', 'AT', 'HS', '"', ',', '{', '}', ',', 'sub', 'directory', '=', '"', 'G', 'affer', 'Image', 'UI', '"', ')', '\\', 'n']
Detokenized (017): ['__import__', '(', '"IECore"', ')', '.', 'loadConfig', '(', '"GAFFER_STARTUP_PATHS"', ',', '{', '}', ',', 'subdirectory', '=', '"GafferImageUI"', ')', '\\n']
Counter: 42
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "GafferRenderMan . RenderManShader . shaderLoader ( ) . clear ( ) \n"
Original    (012): ['GafferRenderMan', '.', 'RenderManShader', '.', 'shaderLoader', '(', ')', '.', 'clear', '(', ')', '\\n']
Tokenized   (022): ['<s>', 'G', 'affer', 'Render', 'Man', '.', 'Render', 'Man', 'Sh', 'ader', '.', 'shader', 'Loader', '(', ')', '.', 'clear', '(', ')', '\\', 'n', '</s>']
Filtered   (020): ['G', 'affer', 'Render', 'Man', '.', 'Render', 'Man', 'Sh', 'ader', '.', 'shader', 'Loader', '(', ')', '.', 'clear', '(', ')', '\\', 'n']
Detokenized (012): ['GafferRenderMan', '.', 'RenderManShader', '.', 'shaderLoader', '(', ')', '.', 'clear', '(', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n"
Original    (018): ['coshader', '=', 'self', '.', 'compileShader', '(', 'os', '.', 'path', '.', 'dirname', '(', '__file__', ')', '+', '"/shaders/coshader.sl"', ')', '\\n']
Tokenized   (037): ['<s>', 'c', 'osh', 'ader', '=', 'self', '.', 'compile', 'Sh', 'ader', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', '__', 'file', '__', ')', '+', '"/', 'sh', 'aders', '/', 'c', 'osh', 'ader', '.', 'sl', '"', ')', '\\', 'n', '</s>']
Filtered   (035): ['c', 'osh', 'ader', '=', 'self', '.', 'compile', 'Sh', 'ader', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', '__', 'file', '__', ')', '+', '"/', 'sh', 'aders', '/', 'c', 'osh', 'ader', '.', 'sl', '"', ')', '\\', 'n']
Detokenized (018): ['coshader', '=', 'self', '.', 'compileShader', '(', 'os', '.', 'path', '.', 'dirname', '(', '__file__', ')', '+', '"/shaders/coshader.sl"', ')', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "nn [ "outString" ] = Gaffer . StringPlug ( direction = Gaffer . Plug . Direction . Out ) \n"
Original    (020): ['nn', '[', '"outString"', ']', '=', 'Gaffer', '.', 'StringPlug', '(', 'direction', '=', 'Gaffer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\n']
Tokenized   (029): ['<s>', 'nn', '[', '"', 'out', 'String', '"', ']', '=', 'G', 'affer', '.', 'String', 'Plug', '(', 'direction', '=', 'G', 'affer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\', 'n', '</s>']
Filtered   (027): ['nn', '[', '"', 'out', 'String', '"', ']', '=', 'G', 'affer', '.', 'String', 'Plug', '(', 'direction', '=', 'G', 'affer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\', 'n']
Detokenized (020): ['nn', '[', '"outString"', ']', '=', 'Gaffer', '.', 'StringPlug', '(', 'direction', '=', 'Gaffer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "shader2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version2.sl" , shaderName = "unversioned" \n"
Original    (021): ['shader2', '=', 'self', '.', 'compileShader', '(', 'os', '.', 'path', '.', 'dirname', '(', '__file__', ')', '+', '"/shaders/version2.sl"', ',', 'shaderName', '=', '"unversioned"', '\\n']
Tokenized   (044): ['<s>', 'sh', 'ader', '2', '=', 'self', '.', 'compile', 'Sh', 'ader', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', '__', 'file', '__', ')', '+', '"/', 'sh', 'aders', '/', 'version', '2', '.', 'sl', '"', ',', 'shader', 'Name', '=', '"', 'un', 'version', 'ed', '"', '\\', 'n', '</s>']
Filtered   (042): ['sh', 'ader', '2', '=', 'self', '.', 'compile', 'Sh', 'ader', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', '__', 'file', '__', ')', '+', '"/', 'sh', 'aders', '/', 'version', '2', '.', 'sl', '"', ',', 'shader', 'Name', '=', '"', 'un', 'version', 'ed', '"', '\\', 'n']
Detokenized (021): ['shader2', '=', 'self', '.', 'compileShader', '(', 'os', '.', 'path', '.', 'dirname', '(', '__file__', ')', '+', '"/shaders/version2.sl"', ',', 'shaderName', '=', '"unversioned"', '\\n']
Counter: 42
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "assignment [ "shader" ] . setInput ( shaderNode [ "out" ] ) \n"
Original    (013): ['assignment', '[', '"shader"', ']', '.', 'setInput', '(', 'shaderNode', '[', '"out"', ']', ')', '\\n']
Tokenized   (024): ['<s>', 'ass', 'ignment', '[', '"', 'sh', 'ader', '"', ']', '.', 'set', 'Input', '(', 'shader', 'Node', '[', '"', 'out', '"', ']', ')', '\\', 'n', '</s>']
Filtered   (022): ['ass', 'ignment', '[', '"', 'sh', 'ader', '"', ']', '.', 'set', 'Input', '(', 'shader', 'Node', '[', '"', 'out', '"', ']', ')', '\\', 'n']
Detokenized (013): ['assignment', '[', '"shader"', ']', '.', 'setInput', '(', 'shaderNode', '[', '"out"', ']', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "dirtiedNames = [ x [ 0 ] . fullName ( ) for x in cs ] \n"
Original    (017): ['dirtiedNames', '=', '[', 'x', '[', '0', ']', '.', 'fullName', '(', ')', 'for', 'x', 'in', 'cs', ']', '\\n']
Tokenized   (024): ['<s>', 'd', 'irt', 'ied', 'Names', '=', '[', 'x', '[', '0', ']', '.', 'full', 'Name', '(', ')', 'for', 'x', 'in', 'cs', ']', '\\', 'n', '</s>']
Filtered   (022): ['d', 'irt', 'ied', 'Names', '=', '[', 'x', '[', '0', ']', '.', 'full', 'Name', '(', ')', 'for', 'x', 'in', 'cs', ']', '\\', 'n']
Detokenized (017): ['dirtiedNames', '=', '[', 'x', '[', '0', ']', '.', 'fullName', '(', ')', 'for', 'x', 'in', 'cs', ']', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : ""dynamicFloatArray" : IECore . FloatVectorData ( [ ] ) , \n"
Original    (011): ['"dynamicFloatArray"', ':', 'IECore', '.', 'FloatVectorData', '(', '[', ']', ')', ',', '\\n']
Tokenized   (023): ['<s>', '"', 'd', 'ynamic', 'Float', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'Float', 'Vector', 'Data', '(', '[', ']', ')', ',', '\\', 'n', '</s>']
Filtered   (021): ['"', 'd', 'ynamic', 'Float', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'Float', 'Vector', 'Data', '(', '[', ']', ')', ',', '\\', 'n']
Detokenized (011): ['"dynamicFloatArray"', ':', 'IECore', '.', 'FloatVectorData', '(', '[', ']', ')', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""fixedFloatArray" : IECore . FloatVectorData ( [ 1 , 2 , 3 , 4 ] ) , \n"
Original    (018): ['"fixedFloatArray"', ':', 'IECore', '.', 'FloatVectorData', '(', '[', '1', ',', '2', ',', '3', ',', '4', ']', ')', ',', '\\n']
Tokenized   (029): ['<s>', '"', 'fixed', 'Float', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'Float', 'Vector', 'Data', '(', '[', '1', ',', '2', ',', '3', ',', '4', ']', ')', ',', '\\', 'n', '</s>']
Filtered   (027): ['"', 'fixed', 'Float', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'Float', 'Vector', 'Data', '(', '[', '1', ',', '2', ',', '3', ',', '4', ']', ')', ',', '\\', 'n']
Detokenized (018): ['"fixedFloatArray"', ':', 'IECore', '.', 'FloatVectorData', '(', '[', '1', ',', '2', ',', '3', ',', '4', ']', ')', ',', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : ""dynamicStringArray" : IECore . StringVectorData ( [ "dynamic" , "arrays" , "can" , "still" , "have" , "defaults" "fixedStringArray" : IECore . StringVectorData ( [ "hello" , "goodbye" ] ) , \n"
Original    (032): ['"dynamicStringArray"', ':', 'IECore', '.', 'StringVectorData', '(', '[', '"dynamic"', ',', '"arrays"', ',', '"can"', ',', '"still"', ',', '"have"', ',', '"defaults"', '"fixedStringArray"', ':', 'IECore', '.', 'StringVectorData', '(', '[', '"hello"', ',', '"goodbye"', ']', ')', ',', '\\n']
Tokenized   (072): ['<s>', '"', 'd', 'ynamic', 'String', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'String', 'Vector', 'Data', '(', '[', '"', 'd', 'ynamic', '"', ',', '"', 'arr', 'ays', '"', ',', '"', 'can', '"', ',', '"', 'still', '"', ',', '"', 'have', '"', ',', '"', 'default', 's', '"', '"', 'fixed', 'String', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'String', 'Vector', 'Data', '(', '[', '"', 'hello', '"', ',', '"', 'good', 'bye', '"', ']', ')', ',', '\\', 'n', '</s>']
Filtered   (070): ['"', 'd', 'ynamic', 'String', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'String', 'Vector', 'Data', '(', '[', '"', 'd', 'ynamic', '"', ',', '"', 'arr', 'ays', '"', ',', '"', 'can', '"', ',', '"', 'still', '"', ',', '"', 'have', '"', ',', '"', 'default', 's', '"', '"', 'fixed', 'String', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'String', 'Vector', 'Data', '(', '[', '"', 'hello', '"', ',', '"', 'good', 'bye', '"', ']', ')', ',', '\\', 'n']
Detokenized (032): ['"dynamicStringArray"', ':', 'IECore', '.', 'StringVectorData', '(', '[', '"dynamic"', ',', '"arrays"', ',', '"can"', ',', '"still"', ',', '"have"', ',', '"defaults"', '"fixedStringArray"', ':', 'IECore', '.', 'StringVectorData', '(', '[', '"hello"', ',', '"goodbye"', ']', ')', ',', '\\n']
Counter: 70
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : ""dynamicColorArray" : IECore . Color3fVectorData ( [ IECore . Color3f ( 1 ) , IECore . Color3f ( 2 ) ] ) , \n"
Original    (024): ['"dynamicColorArray"', ':', 'IECore', '.', 'Color3fVectorData', '(', '[', 'IECore', '.', 'Color3f', '(', '1', ')', ',', 'IECore', '.', 'Color3f', '(', '2', ')', ']', ')', ',', '\\n']
Tokenized   (046): ['<s>', '"', 'd', 'ynamic', 'Color', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'Color', '3', 'f', 'Vector', 'Data', '(', '[', 'I', 'EC', 'ore', '.', 'Color', '3', 'f', '(', '1', ')', ',', 'I', 'EC', 'ore', '.', 'Color', '3', 'f', '(', '2', ')', ']', ')', ',', '\\', 'n', '</s>']
Filtered   (044): ['"', 'd', 'ynamic', 'Color', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'Color', '3', 'f', 'Vector', 'Data', '(', '[', 'I', 'EC', 'ore', '.', 'Color', '3', 'f', '(', '1', ')', ',', 'I', 'EC', 'ore', '.', 'Color', '3', 'f', '(', '2', ')', ']', ')', ',', '\\', 'n']
Detokenized (024): ['"dynamicColorArray"', ':', 'IECore', '.', 'Color3fVectorData', '(', '[', 'IECore', '.', 'Color3f', '(', '1', ')', ',', 'IECore', '.', 'Color3f', '(', '2', ')', ']', ')', ',', '\\n']
Counter: 44
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : ""dynamicVectorArray" : IECore . V3fVectorData ( [ ] , IECore . GeometricData . Interpretation . Vector ) , \n"
Original    (019): ['"dynamicVectorArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', ']', ',', 'IECore', '.', 'GeometricData', '.', 'Interpretation', '.', 'Vector', ')', ',', '\\n']
Tokenized   (038): ['<s>', '"', 'd', 'ynamic', 'Vector', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'V', '3', 'f', 'Vector', 'Data', '(', '[', ']', ',', 'I', 'EC', 'ore', '.', 'Ge', 'ometric', 'Data', '.', 'Interpret', 'ation', '.', 'Vector', ')', ',', '\\', 'n', '</s>']
Filtered   (036): ['"', 'd', 'ynamic', 'Vector', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'V', '3', 'f', 'Vector', 'Data', '(', '[', ']', ',', 'I', 'EC', 'ore', '.', 'Ge', 'ometric', 'Data', '.', 'Interpret', 'ation', '.', 'Vector', ')', ',', '\\', 'n']
Detokenized (019): ['"dynamicVectorArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', ']', ',', 'IECore', '.', 'GeometricData', '.', 'Interpretation', '.', 'Vector', ')', ',', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : ""fixedVectorArray" : IECore . V3fVectorData ( [ IECore . V3f ( x ) for x in range ( 1 , 6 ) ] , IECore . GeometricData "dynamicPointArray" : IECore . V3fVectorData ( [ ] , IECore . GeometricData . Interpretation . Point ) , \n"
Original    (046): ['"fixedVectorArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', 'IECore', '.', 'V3f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'IECore', '.', 'GeometricData', '"dynamicPointArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', ']', ',', 'IECore', '.', 'GeometricData', '.', 'Interpretation', '.', 'Point', ')', ',', '\\n']
Tokenized   (083): ['<s>', '"', 'fixed', 'Vector', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'V', '3', 'f', 'Vector', 'Data', '(', '[', 'I', 'EC', 'ore', '.', 'V', '3', 'f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'I', 'EC', 'ore', '.', 'Ge', 'ometric', 'Data', '"', 'd', 'ynamic', 'Point', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'V', '3', 'f', 'Vector', 'Data', '(', '[', ']', ',', 'I', 'EC', 'ore', '.', 'Ge', 'ometric', 'Data', '.', 'Interpret', 'ation', '.', 'Point', ')', ',', '\\', 'n', '</s>']
Filtered   (081): ['"', 'fixed', 'Vector', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'V', '3', 'f', 'Vector', 'Data', '(', '[', 'I', 'EC', 'ore', '.', 'V', '3', 'f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'I', 'EC', 'ore', '.', 'Ge', 'ometric', 'Data', '"', 'd', 'ynamic', 'Point', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'V', '3', 'f', 'Vector', 'Data', '(', '[', ']', ',', 'I', 'EC', 'ore', '.', 'Ge', 'ometric', 'Data', '.', 'Interpret', 'ation', '.', 'Point', ')', ',', '\\', 'n']
Detokenized (046): ['"fixedVectorArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', 'IECore', '.', 'V3f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'IECore', '.', 'GeometricData', '"dynamicPointArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', ']', ',', 'IECore', '.', 'GeometricData', '.', 'Interpretation', '.', 'Point', ')', ',', '\\n']
Counter: 81
===================================================================
Hidden states:  (13, 46, 768)
# Extracted words:  46
Sentence         : ""fixedNormalArray" : IECore . V3fVectorData ( [ IECore . V3f ( x ) for x in range ( 1 , 6 ) ] , IECore . GeometricData } \n"
Original    (029): ['"fixedNormalArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', 'IECore', '.', 'V3f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'IECore', '.', 'GeometricData', '}', '\\n']
Tokenized   (050): ['<s>', '"', 'fixed', 'Normal', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'V', '3', 'f', 'Vector', 'Data', '(', '[', 'I', 'EC', 'ore', '.', 'V', '3', 'f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'I', 'EC', 'ore', '.', 'Ge', 'ometric', 'Data', '}', '\\', 'n', '</s>']
Filtered   (048): ['"', 'fixed', 'Normal', 'Array', '"', ':', 'I', 'EC', 'ore', '.', 'V', '3', 'f', 'Vector', 'Data', '(', '[', 'I', 'EC', 'ore', '.', 'V', '3', 'f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'I', 'EC', 'ore', '.', 'Ge', 'ometric', 'Data', '}', '\\', 'n']
Detokenized (029): ['"fixedNormalArray"', ':', 'IECore', '.', 'V3fVectorData', '(', '[', 'IECore', '.', 'V3f', '(', 'x', ')', 'for', 'x', 'in', 'range', '(', '1', ',', '6', ')', ']', ',', 'IECore', '.', 'GeometricData', '}', '\\n']
Counter: 48
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "arrayShader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" n4 = GafferRenderMan . RenderManShader ( ) \n"
Original    (024): ['arrayShader', '=', 'self', '.', 'compileShader', '(', 'os', '.', 'path', '.', 'dirname', '(', '__file__', ')', '+', '"/shaders/coshaderArrayParameters.sl"', 'n4', '=', 'GafferRenderMan', '.', 'RenderManShader', '(', ')', '\\n']
Tokenized   (052): ['<s>', 'array', 'Sh', 'ader', '=', 'self', '.', 'compile', 'Sh', 'ader', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', '__', 'file', '__', ')', '+', '"/', 'sh', 'aders', '/', 'c', 'osh', 'ader', 'Array', 'Parameters', '.', 'sl', '"', 'n', '4', '=', 'G', 'affer', 'Render', 'Man', '.', 'Render', 'Man', 'Sh', 'ader', '(', ')', '\\', 'n', '</s>']
Filtered   (050): ['array', 'Sh', 'ader', '=', 'self', '.', 'compile', 'Sh', 'ader', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', '__', 'file', '__', ')', '+', '"/', 'sh', 'aders', '/', 'c', 'osh', 'ader', 'Array', 'Parameters', '.', 'sl', '"', 'n', '4', '=', 'G', 'affer', 'Render', 'Man', '.', 'Render', 'Man', 'Sh', 'ader', '(', ')', '\\', 'n']
Detokenized (024): ['arrayShader', '=', 'self', '.', 'compileShader', '(', 'os', '.', 'path', '.', 'dirname', '(', '__file__', ')', '+', '"/shaders/coshaderArrayParameters.sl"', 'n4', '=', 'GafferRenderMan', '.', 'RenderManShader', '(', ')', '\\n']
Counter: 50
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "coshaderNode [ "enabled" ] . setValue ( False ) \n"
Original    (010): ['coshaderNode', '[', '"enabled"', ']', '.', 'setValue', '(', 'False', ')', '\\n']
Tokenized   (019): ['<s>', 'c', 'osh', 'ader', 'Node', '[', '"', 'enabled', '"', ']', '.', 'set', 'Value', '(', 'False', ')', '\\', 'n', '</s>']
Filtered   (017): ['c', 'osh', 'ader', 'Node', '[', '"', 'enabled', '"', ']', '.', 'set', 'Value', '(', 'False', ')', '\\', 'n']
Detokenized (010): ['coshaderNode', '[', '"enabled"', ']', '.', 'setValue', '(', 'False', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "floatValue = IECore . Splineff ( \n"
Original    (007): ['floatValue', '=', 'IECore', '.', 'Splineff', '(', '\\n']
Tokenized   (015): ['<s>', 'float', 'Value', '=', 'I', 'EC', 'ore', '.', 'Spl', 'ine', 'ff', '(', '\\', 'n', '</s>']
Filtered   (013): ['float', 'Value', '=', 'I', 'EC', 'ore', '.', 'Spl', 'ine', 'ff', '(', '\\', 'n']
Detokenized (007): ['floatValue', '=', 'IECore', '.', 'Splineff', '(', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "S [ "parameters" ] [ "coshaderParameter" ] . setInput ( D2 [ "out" ] ) \n"
Original    (016): ['S', '[', '"parameters"', ']', '[', '"coshaderParameter"', ']', '.', 'setInput', '(', 'D2', '[', '"out"', ']', ')', '\\n']
Tokenized   (031): ['<s>', 'S', '[', '"', 'param', 'eters', '"', ']', '[', '"', 'c', 'osh', 'ader', 'Parameter', '"', ']', '.', 'set', 'Input', '(', 'D', '2', '[', '"', 'out', '"', ']', ')', '\\', 'n', '</s>']
Filtered   (029): ['S', '[', '"', 'param', 'eters', '"', ']', '[', '"', 'c', 'osh', 'ader', 'Parameter', '"', ']', '.', 'set', 'Input', '(', 'D', '2', '[', '"', 'out', '"', ']', ')', '\\', 'n']
Detokenized (016): ['S', '[', '"parameters"', ']', '[', '"coshaderParameter"', ']', '.', 'setInput', '(', 'D2', '[', '"out"', ']', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "coshaderNode0 [ "parameters" ] [ "floatParameter" ] . setValue ( 0 ) \n"
Original    (013): ['coshaderNode0', '[', '"parameters"', ']', '[', '"floatParameter"', ']', '.', 'setValue', '(', '0', ')', '\\n']
Tokenized   (027): ['<s>', 'c', 'osh', 'ader', 'Node', '0', '[', '"', 'param', 'eters', '"', ']', '[', '"', 'float', 'Parameter', '"', ']', '.', 'set', 'Value', '(', '0', ')', '\\', 'n', '</s>']
Filtered   (025): ['c', 'osh', 'ader', 'Node', '0', '[', '"', 'param', 'eters', '"', ']', '[', '"', 'float', 'Parameter', '"', ']', '.', 'set', 'Value', '(', '0', ')', '\\', 'n']
Detokenized (013): ['coshaderNode0', '[', '"parameters"', ']', '[', '"floatParameter"', ']', '.', 'setValue', '(', '0', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "sn1 = GafferRenderMan . RenderManShader ( "Shader1" ) \n"
Original    (009): ['sn1', '=', 'GafferRenderMan', '.', 'RenderManShader', '(', '"Shader1"', ')', '\\n']
Tokenized   (023): ['<s>', 'sn', '1', '=', 'G', 'affer', 'Render', 'Man', '.', 'Render', 'Man', 'Sh', 'ader', '(', '"', 'Sh', 'ader', '1', '"', ')', '\\', 'n', '</s>']
Filtered   (021): ['sn', '1', '=', 'G', 'affer', 'Render', 'Man', '.', 'Render', 'Man', 'Sh', 'ader', '(', '"', 'Sh', 'ader', '1', '"', ')', '\\', 'n']
Detokenized (009): ['sn1', '=', 'GafferRenderMan', '.', 'RenderManShader', '(', '"Shader1"', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "script [ "assignment" ] [ "shader" ] . setInput ( script [ "shader" ] [ "out" ] ) \n"
Original    (019): ['script', '[', '"assignment"', ']', '[', '"shader"', ']', '.', 'setInput', '(', 'script', '[', '"shader"', ']', '[', '"out"', ']', ')', '\\n']
Tokenized   (034): ['<s>', 'script', '[', '"', 'ass', 'ignment', '"', ']', '[', '"', 'sh', 'ader', '"', ']', '.', 'set', 'Input', '(', 'script', '[', '"', 'sh', 'ader', '"', ']', '[', '"', 'out', '"', ']', ')', '\\', 'n', '</s>']
Filtered   (032): ['script', '[', '"', 'ass', 'ignment', '"', ']', '[', '"', 'sh', 'ader', '"', ']', '.', 'set', 'Input', '(', 'script', '[', '"', 'sh', 'ader', '"', ']', '[', '"', 'out', '"', ']', ')', '\\', 'n']
Detokenized (019): ['script', '[', '"assignment"', ']', '[', '"shader"', ']', '.', 'setInput', '(', 'script', '[', '"shader"', ']', '[', '"out"', ']', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "traverseConnection = Gaffer . ScopedConnection ( GafferSceneTest . connectTraverseSceneToPlugDirtiedSignal script [ "shader" ] . loadShader ( "matte" ) \n"
Original    (019): ['traverseConnection', '=', 'Gaffer', '.', 'ScopedConnection', '(', 'GafferSceneTest', '.', 'connectTraverseSceneToPlugDirtiedSignal', 'script', '[', '"shader"', ']', '.', 'loadShader', '(', '"matte"', ')', '\\n']
Tokenized   (048): ['<s>', 'tra', 'verse', 'Connection', '=', 'G', 'affer', '.', 'Sc', 'oped', 'Connection', '(', 'G', 'affer', 'Scene', 'Test', '.', 'connect', 'Tra', 'verse', 'Scene', 'To', 'Plug', 'D', 'irt', 'ied', 'Sign', 'al', 'script', '[', '"', 'sh', 'ader', '"', ']', '.', 'load', 'Sh', 'ader', '(', '"', 'mat', 'te', '"', ')', '\\', 'n', '</s>']
Filtered   (046): ['tra', 'verse', 'Connection', '=', 'G', 'affer', '.', 'Sc', 'oped', 'Connection', '(', 'G', 'affer', 'Scene', 'Test', '.', 'connect', 'Tra', 'verse', 'Scene', 'To', 'Plug', 'D', 'irt', 'ied', 'Sign', 'al', 'script', '[', '"', 'sh', 'ader', '"', ']', '.', 'load', 'Sh', 'ader', '(', '"', 'mat', 'te', '"', ')', '\\', 'n']
Detokenized (019): ['traverseConnection', '=', 'Gaffer', '.', 'ScopedConnection', '(', 'GafferSceneTest', '.', 'connectTraverseSceneToPlugDirtiedSignal', 'script', '[', '"shader"', ']', '.', 'loadShader', '(', '"matte"', ')', '\\n']
Counter: 46
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "current = s [ "render" ] . hash ( c ) \n"
Original    (012): ['current', '=', 's', '[', '"render"', ']', '.', 'hash', '(', 'c', ')', '\\n']
Tokenized   (017): ['<s>', 'current', '=', 's', '[', '"', 'render', '"', ']', '.', 'hash', '(', 'c', ')', '\\', 'n', '</s>']
Filtered   (015): ['current', '=', 's', '[', '"', 'render', '"', ']', '.', 'hash', '(', 'c', ')', '\\', 'n']
Detokenized (012): ['current', '=', 's', '[', '"render"', ']', '.', 'hash', '(', 'c', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : ""layout:section" , "Transform" , \n"
Original    (005): ['"layout:section"', ',', '"Transform"', ',', '\\n']
Tokenized   (014): ['<s>', '"', 'layout', ':', 'section', '"', ',', '"', 'Transform', '"', ',', '\\', 'n', '</s>']
Filtered   (012): ['"', 'layout', ':', 'section', '"', ',', '"', 'Transform', '"', ',', '\\', 'n']
Detokenized (005): ['"layout:section"', ',', '"Transform"', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""toolbarLayout:index" , 2 , \n"
Original    (005): ['"toolbarLayout:index"', ',', '2', ',', '\\n']
Tokenized   (014): ['<s>', '"', 'tool', 'bar', 'Layout', ':', 'index', '"', ',', '2', ',', '\\', 'n', '</s>']
Filtered   (012): ['"', 'tool', 'bar', 'Layout', ':', 'index', '"', ',', '2', ',', '\\', 'n']
Detokenized (005): ['"toolbarLayout:index"', ',', '2', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""toolbarLayout:divider" , True , \n"
Original    (005): ['"toolbarLayout:divider"', ',', 'True', ',', '\\n']
Tokenized   (015): ['<s>', '"', 'tool', 'bar', 'Layout', ':', 'div', 'ider', '"', ',', 'True', ',', '\\', 'n', '</s>']
Filtered   (013): ['"', 'tool', 'bar', 'Layout', ':', 'div', 'ider', '"', ',', 'True', ',', '\\', 'n']
Detokenized (005): ['"toolbarLayout:divider"', ',', 'True', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "currentName = self . getPlug ( ) . getValue ( ) \n"
Original    (012): ['currentName', '=', 'self', '.', 'getPlug', '(', ')', '.', 'getValue', '(', ')', '\\n']
Tokenized   (018): ['<s>', 'current', 'Name', '=', 'self', '.', 'get', 'Plug', '(', ')', '.', 'get', 'Value', '(', ')', '\\', 'n', '</s>']
Filtered   (016): ['current', 'Name', '=', 'self', '.', 'get', 'Plug', '(', ')', '.', 'get', 'Value', '(', ')', '\\', 'n']
Detokenized (012): ['currentName', '=', 'self', '.', 'getPlug', '(', ')', '.', 'getValue', '(', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "menuButton = GafferUI . MenuButton ( menu = menu , image = "grid.png" , hasFrame = False ) \n"
Original    (019): ['menuButton', '=', 'GafferUI', '.', 'MenuButton', '(', 'menu', '=', 'menu', ',', 'image', '=', '"grid.png"', ',', 'hasFrame', '=', 'False', ')', '\\n']
Tokenized   (031): ['<s>', 'menu', 'Button', '=', 'G', 'affer', 'UI', '.', 'Menu', 'Button', '(', 'menu', '=', 'menu', ',', 'image', '=', '"', 'grid', '.', 'png', '"', ',', 'has', 'Frame', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (029): ['menu', 'Button', '=', 'G', 'affer', 'UI', '.', 'Menu', 'Button', '(', 'menu', '=', 'menu', ',', 'image', '=', '"', 'grid', '.', 'png', '"', ',', 'has', 'Frame', '=', 'False', ')', '\\', 'n']
Detokenized (019): ['menuButton', '=', 'GafferUI', '.', 'MenuButton', '(', 'menu', '=', 'menu', ',', 'image', '=', '"grid.png"', ',', 'hasFrame', '=', 'False', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "p3 = Gaffer . IntPlug ( "sum" , Gaffer . Plug . Direction . Out ) \n"
Original    (017): ['p3', '=', 'Gaffer', '.', 'IntPlug', '(', '"sum"', ',', 'Gaffer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\n']
Tokenized   (026): ['<s>', 'p', '3', '=', 'G', 'affer', '.', 'Int', 'Plug', '(', '"', 'sum', '"', ',', 'G', 'affer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\', 'n', '</s>']
Filtered   (024): ['p', '3', '=', 'G', 'affer', '.', 'Int', 'Plug', '(', '"', 'sum', '"', ',', 'G', 'affer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\', 'n']
Detokenized (017): ['p3', '=', 'Gaffer', '.', 'IntPlug', '(', '"sum"', ',', 'Gaffer', '.', 'Plug', '.', 'Direction', '.', 'Out', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "childrenStrings = [ str ( c ) for c in children ] \n"
Original    (013): ['childrenStrings', '=', '[', 'str', '(', 'c', ')', 'for', 'c', 'in', 'children', ']', '\\n']
Tokenized   (018): ['<s>', 'children', 'Str', 'ings', '=', '[', 'str', '(', 'c', ')', 'for', 'c', 'in', 'children', ']', '\\', 'n', '</s>']
Filtered   (016): ['children', 'Str', 'ings', '=', '[', 'str', '(', 'c', ')', 'for', 'c', 'in', 'children', ']', '\\', 'n']
Detokenized (013): ['childrenStrings', '=', '[', 'str', '(', 'c', ')', 'for', 'c', 'in', 'children', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "c2 = [ str ( p ) for p in path2 . children ( ) ] \n"
Original    (017): ['c2', '=', '[', 'str', '(', 'p', ')', 'for', 'p', 'in', 'path2', '.', 'children', '(', ')', ']', '\\n']
Tokenized   (022): ['<s>', 'c', '2', '=', '[', 'str', '(', 'p', ')', 'for', 'p', 'in', 'path', '2', '.', 'children', '(', ')', ']', '\\', 'n', '</s>']
Filtered   (020): ['c', '2', '=', '[', 'str', '(', 'p', ')', 'for', 'p', 'in', 'path', '2', '.', 'children', '(', ')', ']', '\\', 'n']
Detokenized (017): ['c2', '=', '[', 'str', '(', 'p', ')', 'for', 'p', 'in', 'path2', '.', 'children', '(', ')', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "horizontalAlignment = GafferUI . Label . HorizontalAlignment . Right , \n"
Original    (011): ['horizontalAlignment', '=', 'GafferUI', '.', 'Label', '.', 'HorizontalAlignment', '.', 'Right', ',', '\\n']
Tokenized   (022): ['<s>', 'hor', 'izontal', 'Al', 'ignment', '=', 'G', 'affer', 'UI', '.', 'Label', '.', 'Hor', 'izontal', 'Al', 'ignment', '.', 'Right', ',', '\\', 'n', '</s>']
Filtered   (020): ['hor', 'izontal', 'Al', 'ignment', '=', 'G', 'affer', 'UI', '.', 'Label', '.', 'Hor', 'izontal', 'Al', 'ignment', '.', 'Right', ',', '\\', 'n']
Detokenized (011): ['horizontalAlignment', '=', 'GafferUI', '.', 'Label', '.', 'HorizontalAlignment', '.', 'Right', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "nameWidget . textWidget ( ) . _qtWidget ( ) . setFixedWidth ( GafferUI . PlugWidget . labelWidth ( ) ) \n"
Original    (021): ['nameWidget', '.', 'textWidget', '(', ')', '.', '_qtWidget', '(', ')', '.', 'setFixedWidth', '(', 'GafferUI', '.', 'PlugWidget', '.', 'labelWidth', '(', ')', ')', '\\n']
Tokenized   (034): ['<s>', 'name', 'Widget', '.', 'text', 'Widget', '(', ')', '.', '_', 'qt', 'Widget', '(', ')', '.', 'set', 'Fixed', 'Width', '(', 'G', 'affer', 'UI', '.', 'Plug', 'Widget', '.', 'label', 'Width', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (032): ['name', 'Widget', '.', 'text', 'Widget', '(', ')', '.', '_', 'qt', 'Widget', '(', ')', '.', 'set', 'Fixed', 'Width', '(', 'G', 'affer', 'UI', '.', 'Plug', 'Widget', '.', 'label', 'Width', '(', ')', ')', '\\', 'n']
Detokenized (021): ['nameWidget', '.', 'textWidget', '(', ')', '.', '_qtWidget', '(', ')', '.', 'setFixedWidth', '(', 'GafferUI', '.', 'PlugWidget', '.', 'labelWidth', '(', ')', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "childPlug [ "enabled" ] , \n"
Original    (006): ['childPlug', '[', '"enabled"', ']', ',', '\\n']
Tokenized   (012): ['<s>', 'child', 'Plug', '[', '"', 'enabled', '"', ']', ',', '\\', 'n', '</s>']
Filtered   (010): ['child', 'Plug', '[', '"', 'enabled', '"', ']', ',', '\\', 'n']
Detokenized (006): ['childPlug', '[', '"enabled"', ']', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "memberPlug = memberPlug if memberPlug is not None else plug . ancestor ( Gaffer . CompoundDataPlug . MemberPlug if memberPlug is None : \n"
Original    (024): ['memberPlug', '=', 'memberPlug', 'if', 'memberPlug', 'is', 'not', 'None', 'else', 'plug', '.', 'ancestor', '(', 'Gaffer', '.', 'CompoundDataPlug', '.', 'MemberPlug', 'if', 'memberPlug', 'is', 'None', ':', '\\n']
Tokenized   (036): ['<s>', 'member', 'Plug', '=', 'member', 'Plug', 'if', 'member', 'Plug', 'is', 'not', 'None', 'else', 'plug', '.', 'ancestor', '(', 'G', 'affer', '.', 'Comp', 'ound', 'Data', 'Plug', '.', 'Member', 'Plug', 'if', 'member', 'Plug', 'is', 'None', ':', '\\', 'n', '</s>']
Filtered   (034): ['member', 'Plug', '=', 'member', 'Plug', 'if', 'member', 'Plug', 'is', 'not', 'None', 'else', 'plug', '.', 'ancestor', '(', 'G', 'affer', '.', 'Comp', 'ound', 'Data', 'Plug', '.', 'Member', 'Plug', 'if', 'member', 'Plug', 'is', 'None', ':', '\\', 'n']
Detokenized (024): ['memberPlug', '=', 'memberPlug', 'if', 'memberPlug', 'is', 'not', 'None', 'else', 'plug', '.', 'ancestor', '(', 'Gaffer', '.', 'CompoundDataPlug', '.', 'MemberPlug', 'if', 'memberPlug', 'is', 'None', ':', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "menuDefinition . append ( "/Delete" , { "command" : IECore . curry ( __deletePlug , memberPlug ) , "active" \n"
Original    (020): ['menuDefinition', '.', 'append', '(', '"/Delete"', ',', '{', '"command"', ':', 'IECore', '.', 'curry', '(', '__deletePlug', ',', 'memberPlug', ')', ',', '"active"', '\\n']
Tokenized   (035): ['<s>', 'menu', 'Definition', '.', 'append', '(', '"/', 'Delete', '"', ',', '{', '"', 'command', '"', ':', 'I', 'EC', 'ore', '.', 'curry', '(', '__', 'delete', 'Plug', ',', 'member', 'Plug', ')', ',', '"', 'active', '"', '\\', 'n', '</s>']
Filtered   (033): ['menu', 'Definition', '.', 'append', '(', '"/', 'Delete', '"', ',', '{', '"', 'command', '"', ':', 'I', 'EC', 'ore', '.', 'curry', '(', '__', 'delete', 'Plug', ',', 'member', 'Plug', ')', ',', '"', 'active', '"', '\\', 'n']
Detokenized (020): ['menuDefinition', '.', 'append', '(', '"/Delete"', ',', '{', '"command"', ':', 'IECore', '.', 'curry', '(', '__deletePlug', ',', 'memberPlug', ')', ',', '"active"', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "includeSequences = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "fileSystemPathPlugValueWidget:includeSequences" \n"
Original    (016): ['includeSequences', '=', 'Gaffer', '.', 'Metadata', '.', 'plugValue', '(', 'self', '.', 'getPlug', '(', ')', ',', '"fileSystemPathPlugValueWidget:includeSequences"', '\\n']
Tokenized   (036): ['<s>', 'include', 'Sequ', 'ences', '=', 'G', 'affer', '.', 'Met', 'adata', '.', 'plug', 'Value', '(', 'self', '.', 'get', 'Plug', '(', ')', ',', '"', 'file', 'System', 'Path', 'Plug', 'Value', 'Widget', ':', 'include', 'Sequ', 'ences', '"', '\\', 'n', '</s>']
Filtered   (034): ['include', 'Sequ', 'ences', '=', 'G', 'affer', '.', 'Met', 'adata', '.', 'plug', 'Value', '(', 'self', '.', 'get', 'Plug', '(', ')', ',', '"', 'file', 'System', 'Path', 'Plug', 'Value', 'Widget', ':', 'include', 'Sequ', 'ences', '"', '\\', 'n']
Detokenized (016): ['includeSequences', '=', 'Gaffer', '.', 'Metadata', '.', 'plugValue', '(', 'self', '.', 'getPlug', '(', ')', ',', '"fileSystemPathPlugValueWidget:includeSequences"', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "reuse = reuseUntil is not None \n"
Original    (007): ['reuse', '=', 'reuseUntil', 'is', 'not', 'None', '\\n']
Tokenized   (012): ['<s>', 're', 'use', '=', 'reuse', 'Until', 'is', 'not', 'None', '\\', 'n', '</s>']
Filtered   (010): ['re', 'use', '=', 'reuse', 'Until', 'is', 'not', 'None', '\\', 'n']
Detokenized (007): ['reuse', '=', 'reuseUntil', 'is', 'not', 'None', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "_MultiLineStringMetadataWidget ( key = "description" ) \n"
Original    (007): ['_MultiLineStringMetadataWidget', '(', 'key', '=', '"description"', ')', '\\n']
Tokenized   (018): ['<s>', '_', 'Multi', 'Line', 'String', 'Met', 'adata', 'Widget', '(', 'key', '=', '"', 'description', '"', ')', '\\', 'n', '</s>']
Filtered   (016): ['_', 'Multi', 'Line', 'String', 'Met', 'adata', 'Widget', '(', 'key', '=', '"', 'description', '"', ')', '\\', 'n']
Detokenized (007): ['_MultiLineStringMetadataWidget', '(', 'key', '=', '"description"', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""active" : isinstance ( node , Gaffer . Box ) or nodeEditor . nodeUI ( ) . plugValueWidget ( node [ "user" ] ) } \n"
Original    (026): ['"active"', ':', 'isinstance', '(', 'node', ',', 'Gaffer', '.', 'Box', ')', 'or', 'nodeEditor', '.', 'nodeUI', '(', ')', '.', 'plugValueWidget', '(', 'node', '[', '"user"', ']', ')', '}', '\\n']
Tokenized   (039): ['<s>', '"', 'active', '"', ':', 'is', 'instance', '(', 'node', ',', 'G', 'affer', '.', 'Box', ')', 'or', 'node', 'Editor', '.', 'node', 'UI', '(', ')', '.', 'plug', 'Value', 'Widget', '(', 'node', '[', '"', 'user', '"', ']', ')', '}', '\\', 'n', '</s>']
Filtered   (037): ['"', 'active', '"', ':', 'is', 'instance', '(', 'node', ',', 'G', 'affer', '.', 'Box', ')', 'or', 'node', 'Editor', '.', 'node', 'UI', '(', ')', '.', 'plug', 'Value', 'Widget', '(', 'node', '[', '"', 'user', '"', ']', ')', '}', '\\', 'n']
Detokenized (026): ['"active"', ':', 'isinstance', '(', 'node', ',', 'Gaffer', '.', 'Box', ')', 'or', 'nodeEditor', '.', 'nodeUI', '(', ')', '.', 'plugValueWidget', '(', 'node', '[', '"user"', ']', ')', '}', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "dialogue = GafferUI . ColorChooserDialogue ( color = color , useDisplayTransform = False ) \n"
Original    (015): ['dialogue', '=', 'GafferUI', '.', 'ColorChooserDialogue', '(', 'color', '=', 'color', ',', 'useDisplayTransform', '=', 'False', ')', '\\n']
Tokenized   (026): ['<s>', 'dial', 'ogue', '=', 'G', 'affer', 'UI', '.', 'Color', 'Cho', 'oser', 'Dialogue', '(', 'color', '=', 'color', ',', 'use', 'Display', 'Transform', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (024): ['dial', 'ogue', '=', 'G', 'affer', 'UI', '.', 'Color', 'Cho', 'oser', 'Dialogue', '(', 'color', '=', 'color', ',', 'use', 'Display', 'Transform', '=', 'False', ')', '\\', 'n']
Detokenized (015): ['dialogue', '=', 'GafferUI', '.', 'ColorChooserDialogue', '(', 'color', '=', 'color', ',', 'useDisplayTransform', '=', 'False', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "editor . plugEditor ( ) . reveal ( ) \n"
Original    (010): ['editor', '.', 'plugEditor', '(', ')', '.', 'reveal', '(', ')', '\\n']
Tokenized   (014): ['<s>', 'editor', '.', 'plug', 'Editor', '(', ')', '.', 'reveal', '(', ')', '\\', 'n', '</s>']
Filtered   (012): ['editor', '.', 'plug', 'Editor', '(', ')', '.', 'reveal', '(', ')', '\\', 'n']
Detokenized (010): ['editor', '.', 'plugEditor', '(', ')', '.', 'reveal', '(', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "_MetadataWidget . __init__ ( self , self . __menuButton , key , target , parenting = parenting ) \n"
Original    (019): ['_MetadataWidget', '.', '__init__', '(', 'self', ',', 'self', '.', '__menuButton', ',', 'key', ',', 'target', ',', 'parenting', '=', 'parenting', ')', '\\n']
Tokenized   (029): ['<s>', '_', 'Met', 'adata', 'Widget', '.', '__', 'init', '__', '(', 'self', ',', 'self', '.', '__', 'menu', 'Button', ',', 'key', ',', 'target', ',', 'parenting', '=', 'parenting', ')', '\\', 'n', '</s>']
Filtered   (027): ['_', 'Met', 'adata', 'Widget', '.', '__', 'init', '__', '(', 'self', ',', 'self', '.', '__', 'menu', 'Button', ',', 'key', ',', 'target', ',', 'parenting', '=', 'parenting', ')', '\\', 'n']
Detokenized (019): ['_MetadataWidget', '.', '__init__', '(', 'self', ',', 'self', '.', '__menuButton', ',', 'key', ',', 'target', ',', 'parenting', '=', 'parenting', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : ""checkBox" : value == self . __currentValue \n"
Original    (008): ['"checkBox"', ':', 'value', '==', 'self', '.', '__currentValue', '\\n']
Tokenized   (016): ['<s>', '"', 'check', 'Box', '"', ':', 'value', '==', 'self', '.', '__', 'current', 'Value', '\\', 'n', '</s>']
Filtered   (014): ['"', 'check', 'Box', '"', ':', 'value', '==', 'self', '.', '__', 'current', 'Value', '\\', 'n']
Detokenized (008): ['"checkBox"', ':', 'value', '==', 'self', '.', '__currentValue', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "child . __parent = None \n"
Original    (006): ['child', '.', '__parent', '=', 'None', '\\n']
Tokenized   (010): ['<s>', 'child', '.', '__', 'parent', '=', 'None', '\\', 'n', '</s>']
Filtered   (008): ['child', '.', '__', 'parent', '=', 'None', '\\', 'n']
Detokenized (006): ['child', '.', '__parent', '=', 'None', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "columns = ( GafferUI . PathListingWidget . defaultNameColumn , ) , \n"
Original    (012): ['columns', '=', '(', 'GafferUI', '.', 'PathListingWidget', '.', 'defaultNameColumn', ',', ')', ',', '\\n']
Tokenized   (023): ['<s>', 'column', 's', '=', '(', 'G', 'affer', 'UI', '.', 'Path', 'List', 'ing', 'Widget', '.', 'default', 'Name', 'Column', ',', ')', ',', '\\', 'n', '</s>']
Filtered   (021): ['column', 's', '=', '(', 'G', 'affer', 'UI', '.', 'Path', 'List', 'ing', 'Widget', '.', 'default', 'Name', 'Column', ',', ')', ',', '\\', 'n']
Detokenized (012): ['columns', '=', '(', 'GafferUI', '.', 'PathListingWidget', '.', 'defaultNameColumn', ',', ')', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "definition = Gaffer . WeakMethod ( self . __addMenuDefinition ) \n"
Original    (011): ['definition', '=', 'Gaffer', '.', 'WeakMethod', '(', 'self', '.', '__addMenuDefinition', ')', '\\n']
Tokenized   (019): ['<s>', 'definition', '=', 'G', 'affer', '.', 'Weak', 'Method', '(', 'self', '.', '__', 'add', 'Menu', 'Definition', ')', '\\', 'n', '</s>']
Filtered   (017): ['definition', '=', 'G', 'affer', '.', 'Weak', 'Method', '(', 'self', '.', '__', 'add', 'Menu', 'Definition', ')', '\\', 'n']
Detokenized (011): ['definition', '=', 'Gaffer', '.', 'WeakMethod', '(', 'self', '.', '__addMenuDefinition', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "newIndex = 0 if event . line . p0 . y < 1 else len ( newParent ) \n"
Original    (019): ['newIndex', '=', '0', 'if', 'event', '.', 'line', '.', 'p0', '.', 'y', '<', '1', 'else', 'len', '(', 'newParent', ')', '\\n']
Tokenized   (025): ['<s>', 'new', 'Index', '=', '0', 'if', 'event', '.', 'line', '.', 'p', '0', '.', 'y', '<', '1', 'else', 'len', '(', 'new', 'Parent', ')', '\\', 'n', '</s>']
Filtered   (023): ['new', 'Index', '=', '0', 'if', 'event', '.', 'line', '.', 'p', '0', '.', 'y', '<', '1', 'else', 'len', '(', 'new', 'Parent', ')', '\\', 'n']
Detokenized (019): ['newIndex', '=', '0', 'if', 'event', '.', 'line', '.', 'p0', '.', 'y', '<', '1', 'else', 'len', '(', 'newParent', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "newParent . insert ( newIndex , self . __dragItem ) \n"
Original    (011): ['newParent', '.', 'insert', '(', 'newIndex', ',', 'self', '.', '__dragItem', ')', '\\n']
Tokenized   (019): ['<s>', 'new', 'Parent', '.', 'insert', '(', 'new', 'Index', ',', 'self', '.', '__', 'dr', 'ag', 'Item', ')', '\\', 'n', '</s>']
Filtered   (017): ['new', 'Parent', '.', 'insert', '(', 'new', 'Index', ',', 'self', '.', '__', 'dr', 'ag', 'Item', ')', '\\', 'n']
Detokenized (011): ['newParent', '.', 'insert', '(', 'newIndex', ',', 'self', '.', '__dragItem', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "selection [ : ] = self . __dragItem . fullName ( ) . split ( "." ) \n"
Original    (018): ['selection', '[', ':', ']', '=', 'self', '.', '__dragItem', '.', 'fullName', '(', ')', '.', 'split', '(', '"."', ')', '\\n']
Tokenized   (026): ['<s>', 'selection', '[', ':', ']', '=', 'self', '.', '__', 'dr', 'ag', 'Item', '.', 'full', 'Name', '(', ')', '.', 'split', '(', '"', '."', ')', '\\', 'n', '</s>']
Filtered   (024): ['selection', '[', ':', ']', '=', 'self', '.', '__', 'dr', 'ag', 'Item', '.', 'full', 'Name', '(', ')', '.', 'split', '(', '"', '."', ')', '\\', 'n']
Detokenized (018): ['selection', '[', ':', ']', '=', 'self', '.', '__dragItem', '.', 'fullName', '(', ')', '.', 'split', '(', '"."', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "_registerMetadata ( plug , "nodule:type" , "" ) \n"
Original    (009): ['_registerMetadata', '(', 'plug', ',', '"nodule:type"', ',', '""', ')', '\\n']
Tokenized   (021): ['<s>', '_', 'register', 'Met', 'adata', '(', 'plug', ',', '"', 'n', 'od', 'ule', ':', 'type', '"', ',', '""', ')', '\\', 'n', '</s>']
Filtered   (019): ['_', 'register', 'Met', 'adata', '(', 'plug', ',', '"', 'n', 'od', 'ule', ':', 'type', '"', ',', '""', ')', '\\', 'n']
Detokenized (009): ['_registerMetadata', '(', 'plug', ',', '"nodule:type"', ',', '""', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "parentItem \n"
Original    (002): ['parentItem', '\\n']
Tokenized   (006): ['<s>', 'parent', 'Item', '\\', 'n', '</s>']
Filtered   (004): ['parent', 'Item', '\\', 'n']
Detokenized (002): ['parentItem', '\\n']
Counter: 4
===================================================================
Hidden states:  (13, 2, 768)
# Extracted words:  2
Sentence         : "existingSectionNames = set ( c . name ( ) for c in rootItem if isinstance ( c , _SectionLayoutItem ) ) \n"
Original    (022): ['existingSectionNames', '=', 'set', '(', 'c', '.', 'name', '(', ')', 'for', 'c', 'in', 'rootItem', 'if', 'isinstance', '(', 'c', ',', '_SectionLayoutItem', ')', ')', '\\n']
Tokenized   (032): ['<s>', 'existing', 'Section', 'Names', '=', 'set', '(', 'c', '.', 'name', '(', ')', 'for', 'c', 'in', 'root', 'Item', 'if', 'is', 'instance', '(', 'c', ',', '_', 'Section', 'Layout', 'Item', ')', ')', '\\', 'n', '</s>']
Filtered   (030): ['existing', 'Section', 'Names', '=', 'set', '(', 'c', '.', 'name', '(', ')', 'for', 'c', 'in', 'root', 'Item', 'if', 'is', 'instance', '(', 'c', ',', '_', 'Section', 'Layout', 'Item', ')', ')', '\\', 'n']
Detokenized (022): ['existingSectionNames', '=', 'set', '(', 'c', '.', 'name', '(', ')', 'for', 'c', 'in', 'rootItem', 'if', 'isinstance', '(', 'c', ',', '_SectionLayoutItem', ')', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "Gaffer . Metadata . plugValue ( self . getPlug ( ) , "preset:" + selectedPaths [ 0 ] [ 0 ] ) \n"
Original    (023): ['Gaffer', '.', 'Metadata', '.', 'plugValue', '(', 'self', '.', 'getPlug', '(', ')', ',', '"preset:"', '+', 'selectedPaths', '[', '0', ']', '[', '0', ']', ')', '\\n']
Tokenized   (035): ['<s>', 'G', 'affer', '.', 'Met', 'adata', '.', 'plug', 'Value', '(', 'self', '.', 'get', 'Plug', '(', ')', ',', '"', 'pres', 'et', ':"', '+', 'selected', 'Path', 's', '[', '0', ']', '[', '0', ']', ')', '\\', 'n', '</s>']
Filtered   (033): ['G', 'affer', '.', 'Met', 'adata', '.', 'plug', 'Value', '(', 'self', '.', 'get', 'Plug', '(', ')', ',', '"', 'pres', 'et', ':"', '+', 'selected', 'Path', 's', '[', '0', ']', '[', '0', ']', ')', '\\', 'n']
Detokenized (023): ['Gaffer', '.', 'Metadata', '.', 'plugValue', '(', 'self', '.', 'getPlug', '(', ')', ',', '"preset:"', '+', 'selectedPaths', '[', '0', ']', '[', '0', ']', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "srcPath = self . __pathListing . getPath ( ) . copy ( ) . setFromString ( event . data [ 0 ] ) \n"
Original    (024): ['srcPath', '=', 'self', '.', '__pathListing', '.', 'getPath', '(', ')', '.', 'copy', '(', ')', '.', 'setFromString', '(', 'event', '.', 'data', '[', '0', ']', ')', '\\n']
Tokenized   (034): ['<s>', 'src', 'Path', '=', 'self', '.', '__', 'path', 'List', 'ing', '.', 'get', 'Path', '(', ')', '.', 'copy', '(', ')', '.', 'set', 'From', 'String', '(', 'event', '.', 'data', '[', '0', ']', ')', '\\', 'n', '</s>']
Filtered   (032): ['src', 'Path', '=', 'self', '.', '__', 'path', 'List', 'ing', '.', 'get', 'Path', '(', ')', '.', 'copy', '(', ')', '.', 'set', 'From', 'String', '(', 'event', '.', 'data', '[', '0', ']', ')', '\\', 'n']
Detokenized (024): ['srcPath', '=', 'self', '.', '__pathListing', '.', 'getPath', '(', ')', '.', 'copy', '(', ')', '.', 'setFromString', '(', 'event', '.', 'data', '[', '0', ']', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "srcIndex = d . keys ( ) . index ( srcPath [ 0 ] ) \n"
Original    (016): ['srcIndex', '=', 'd', '.', 'keys', '(', ')', '.', 'index', '(', 'srcPath', '[', '0', ']', ')', '\\n']
Tokenized   (021): ['<s>', 'src', 'Index', '=', 'd', '.', 'keys', '(', ')', '.', 'index', '(', 'src', 'Path', '[', '0', ']', ')', '\\', 'n', '</s>']
Filtered   (019): ['src', 'Index', '=', 'd', '.', 'keys', '(', ')', '.', 'index', '(', 'src', 'Path', '[', '0', ']', ')', '\\', 'n']
Detokenized (016): ['srcIndex', '=', 'd', '.', 'keys', '(', ')', '.', 'index', '(', 'srcPath', '[', '0', ']', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "targetPath = self . __pathListing . pathAt ( event . line . p0 ) \n"
Original    (015): ['targetPath', '=', 'self', '.', '__pathListing', '.', 'pathAt', '(', 'event', '.', 'line', '.', 'p0', ')', '\\n']
Tokenized   (024): ['<s>', 'target', 'Path', '=', 'self', '.', '__', 'path', 'List', 'ing', '.', 'path', 'At', '(', 'event', '.', 'line', '.', 'p', '0', ')', '\\', 'n', '</s>']
Filtered   (022): ['target', 'Path', '=', 'self', '.', '__', 'path', 'List', 'ing', '.', 'path', 'At', '(', 'event', '.', 'line', '.', 'p', '0', ')', '\\', 'n']
Detokenized (015): ['targetPath', '=', 'self', '.', '__pathListing', '.', 'pathAt', '(', 'event', '.', 'line', '.', 'p0', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "item = items [ srcIndex ] \n"
Original    (007): ['item', '=', 'items', '[', 'srcIndex', ']', '\\n']
Tokenized   (011): ['<s>', 'item', '=', 'items', '[', 'src', 'Index', ']', '\\', 'n', '</s>']
Filtered   (009): ['item', '=', 'items', '[', 'src', 'Index', ']', '\\', 'n']
Detokenized (007): ['item', '=', 'items', '[', 'srcIndex', ']', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "selectedPreset = self . __pathListing . getSelectedPaths ( ) [ 0 ] [ 0 ] \n"
Original    (016): ['selectedPreset', '=', 'self', '.', '__pathListing', '.', 'getSelectedPaths', '(', ')', '[', '0', ']', '[', '0', ']', '\\n']
Tokenized   (028): ['<s>', 'selected', 'Pres', 'et', '=', 'self', '.', '__', 'path', 'List', 'ing', '.', 'get', 'Se', 'lected', 'Path', 's', '(', ')', '[', '0', ']', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (026): ['selected', 'Pres', 'et', '=', 'self', '.', '__', 'path', 'List', 'ing', '.', 'get', 'Se', 'lected', 'Path', 's', '(', ')', '[', '0', ']', '[', '0', ']', '\\', 'n']
Detokenized (016): ['selectedPreset', '=', 'self', '.', '__pathListing', '.', 'getSelectedPaths', '(', ')', '[', '0', ']', '[', '0', ']', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "selectedIndex = [ p [ 0 ] for p in paths ] . index ( selectedPreset ) \n"
Original    (018): ['selectedIndex', '=', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'paths', ']', '.', 'index', '(', 'selectedPreset', ')', '\\n']
Tokenized   (024): ['<s>', 'selected', 'Index', '=', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'paths', ']', '.', 'index', '(', 'selected', 'Pres', 'et', ')', '\\', 'n', '</s>']
Filtered   (022): ['selected', 'Index', '=', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'paths', ']', '.', 'index', '(', 'selected', 'Pres', 'et', ')', '\\', 'n']
Detokenized (018): ['selectedIndex', '=', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'paths', ']', '.', 'index', '(', 'selectedPreset', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "preset = selectedPaths [ 0 ] [ 0 ] \n"
Original    (010): ['preset', '=', 'selectedPaths', '[', '0', ']', '[', '0', ']', '\\n']
Tokenized   (016): ['<s>', 'pres', 'et', '=', 'selected', 'Path', 's', '[', '0', ']', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (014): ['pres', 'et', '=', 'selected', 'Path', 's', '[', '0', ']', '[', '0', ']', '\\', 'n']
Detokenized (010): ['preset', '=', 'selectedPaths', '[', '0', ']', '[', '0', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "scrolledContainer . setChild ( GafferUI . ListContainer ( spacing = 4 ) ) \n"
Original    (014): ['scrolledContainer', '.', 'setChild', '(', 'GafferUI', '.', 'ListContainer', '(', 'spacing', '=', '4', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'sc', 'rolled', 'Container', '.', 'set', 'Child', '(', 'G', 'affer', 'UI', '.', 'List', 'Container', '(', 'spacing', '=', '4', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['sc', 'rolled', 'Container', '.', 'set', 'Child', '(', 'G', 'affer', 'UI', '.', 'List', 'Container', '(', 'spacing', '=', '4', ')', ')', '\\', 'n']
Detokenized (014): ['scrolledContainer', '.', 'setChild', '(', 'GafferUI', '.', 'ListContainer', '(', 'spacing', '=', '4', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __gadgetMenuDefinition ) ) \n"
Original    (016): ['menu', '=', 'GafferUI', '.', 'Menu', '(', 'Gaffer', '.', 'WeakMethod', '(', 'self', '.', '__gadgetMenuDefinition', ')', ')', '\\n']
Tokenized   (028): ['<s>', 'menu', '=', 'G', 'affer', 'UI', '.', 'Menu', '(', 'G', 'affer', '.', 'Weak', 'Method', '(', 'self', '.', '__', 'g', 'ad', 'get', 'Menu', 'Definition', ')', ')', '\\', 'n', '</s>']
Filtered   (026): ['menu', '=', 'G', 'affer', 'UI', '.', 'Menu', '(', 'G', 'affer', '.', 'Weak', 'Method', '(', 'self', '.', '__', 'g', 'ad', 'get', 'Menu', 'Definition', ')', ')', '\\', 'n']
Detokenized (016): ['menu', '=', 'GafferUI', '.', 'Menu', '(', 'Gaffer', '.', 'WeakMethod', '(', 'self', '.', '__gadgetMenuDefinition', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : ""/" + g . label , \n"
Original    (007): ['"/"', '+', 'g', '.', 'label', ',', '\\n']
Tokenized   (011): ['<s>', '"', '/"', '+', 'g', '.', 'label', ',', '\\', 'n', '</s>']
Filtered   (009): ['"', '/"', '+', 'g', '.', 'label', ',', '\\', 'n']
Detokenized (007): ['"/"', '+', 'g', '.', 'label', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""command" : functools . partial ( Gaffer . WeakMethod ( self . __registerOrDeregisterMetadata ) , key = "checkBox" : metadata == g . metadata , \n"
Original    (026): ['"command"', ':', 'functools', '.', 'partial', '(', 'Gaffer', '.', 'WeakMethod', '(', 'self', '.', '__registerOrDeregisterMetadata', ')', ',', 'key', '=', '"checkBox"', ':', 'metadata', '==', 'g', '.', 'metadata', ',', '\\n']
Tokenized   (046): ['<s>', '"', 'command', '"', ':', 'fun', 'ct', 'ools', '.', 'partial', '(', 'G', 'affer', '.', 'Weak', 'Method', '(', 'self', '.', '__', 'register', 'Or', 'D', 'ere', 'g', 'ister', 'Met', 'adata', ')', ',', 'key', '=', '"', 'check', 'Box', '"', ':', 'metadata', '==', 'g', '.', 'metadata', ',', '\\', 'n', '</s>']
Filtered   (044): ['"', 'command', '"', ':', 'fun', 'ct', 'ools', '.', 'partial', '(', 'G', 'affer', '.', 'Weak', 'Method', '(', 'self', '.', '__', 'register', 'Or', 'D', 'ere', 'g', 'ister', 'Met', 'adata', ')', ',', 'key', '=', '"', 'check', 'Box', '"', ':', 'metadata', '==', 'g', '.', 'metadata', ',', '\\', 'n']
Detokenized (026): ['"command"', ':', 'functools', '.', 'partial', '(', 'Gaffer', '.', 'WeakMethod', '(', 'self', '.', '__registerOrDeregisterMetadata', ')', ',', 'key', '=', '"checkBox"', ':', 'metadata', '==', 'g', '.', 'metadata', ',', '\\n']
Counter: 44
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "__WidgetDefinition ( "None" , Gaffer . Plug , "" ) , \n"
Original    (012): ['__WidgetDefinition', '(', '"None"', ',', 'Gaffer', '.', 'Plug', ',', '""', ')', ',', '\\n']
Tokenized   (020): ['<s>', '__', 'Widget', 'Definition', '(', '"', 'None', '"', ',', 'G', 'affer', '.', 'Plug', ',', '""', ')', ',', '\\', 'n', '</s>']
Filtered   (018): ['__', 'Widget', 'Definition', '(', '"', 'None', '"', ',', 'G', 'affer', '.', 'Plug', ',', '""', ')', ',', '\\', 'n']
Detokenized (012): ['__WidgetDefinition', '(', '"None"', ',', 'Gaffer', '.', 'Plug', ',', '""', ')', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "__MetadataDefinition = collections . namedtuple ( "MetadataDefinition" , ( "key" , "label" , "metadataWidgetType" __metadataDefinitions = ( \n"
Original    (018): ['__MetadataDefinition', '=', 'collections', '.', 'namedtuple', '(', '"MetadataDefinition"', ',', '(', '"key"', ',', '"label"', ',', '"metadataWidgetType"', '__metadataDefinitions', '=', '(', '\\n']
Tokenized   (041): ['<s>', '__', 'Met', 'adata', 'Definition', '=', 'collections', '.', 'named', 't', 'uple', '(', '"', 'Met', 'adata', 'Definition', '"', ',', '(', '"', 'key', '"', ',', '"', 'label', '"', ',', '"', 'metadata', 'Widget', 'Type', '"', '__', 'metadata', 'Def', 'initions', '=', '(', '\\', 'n', '</s>']
Filtered   (039): ['__', 'Met', 'adata', 'Definition', '=', 'collections', '.', 'named', 't', 'uple', '(', '"', 'Met', 'adata', 'Definition', '"', ',', '(', '"', 'key', '"', ',', '"', 'label', '"', ',', '"', 'metadata', 'Widget', 'Type', '"', '__', 'metadata', 'Def', 'initions', '=', '(', '\\', 'n']
Detokenized (018): ['__MetadataDefinition', '=', 'collections', '.', 'namedtuple', '(', '"MetadataDefinition"', ',', '(', '"key"', ',', '"label"', ',', '"metadataWidgetType"', '__metadataDefinitions', '=', '(', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "newSectionPath [ - 1 ] = nameWidget . getText ( ) . replace ( "." , "" ) \n"
Original    (019): ['newSectionPath', '[', '-', '1', ']', '=', 'nameWidget', '.', 'getText', '(', ')', '.', 'replace', '(', '"."', ',', '""', ')', '\\n']
Tokenized   (027): ['<s>', 'new', 'Section', 'Path', '[', '-', '1', ']', '=', 'name', 'Widget', '.', 'get', 'Text', '(', ')', '.', 'replace', '(', '"', '."', ',', '""', ')', '\\', 'n', '</s>']
Filtered   (025): ['new', 'Section', 'Path', '[', '-', '1', ']', '=', 'name', 'Widget', '.', 'get', 'Text', '(', ')', '.', 'replace', '(', '"', '."', ',', '""', ')', '\\', 'n']
Detokenized (019): ['newSectionPath', '[', '-', '1', ']', '=', 'nameWidget', '.', 'getText', '(', ')', '.', 'replace', '(', '"."', ',', '""', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "_metadata ( self . getPlugParent ( ) , name ) \n"
Original    (011): ['_metadata', '(', 'self', '.', 'getPlugParent', '(', ')', ',', 'name', ')', '\\n']
Tokenized   (017): ['<s>', '_', 'metadata', '(', 'self', '.', 'get', 'Plug', 'Parent', '(', ')', ',', 'name', ')', '\\', 'n', '</s>']
Filtered   (015): ['_', 'metadata', '(', 'self', '.', 'get', 'Plug', 'Parent', '(', ')', ',', 'name', ')', '\\', 'n']
Detokenized (011): ['_metadata', '(', 'self', '.', 'getPlugParent', '(', ')', ',', 'name', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "_deregisterMetadata ( self . getPlugParent ( ) , name ) \n"
Original    (011): ['_deregisterMetadata', '(', 'self', '.', 'getPlugParent', '(', ')', ',', 'name', ')', '\\n']
Tokenized   (022): ['<s>', '_', 'd', 'ere', 'g', 'ister', 'Met', 'adata', '(', 'self', '.', 'get', 'Plug', 'Parent', '(', ')', ',', 'name', ')', '\\', 'n', '</s>']
Filtered   (020): ['_', 'd', 'ere', 'g', 'ister', 'Met', 'adata', '(', 'self', '.', 'get', 'Plug', 'Parent', '(', ')', ',', 'name', ')', '\\', 'n']
Detokenized (011): ['_deregisterMetadata', '(', 'self', '.', 'getPlugParent', '(', ')', ',', 'name', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "wr2 = weakref . ref ( w . _qtWidget ( ) ) \n"
Original    (013): ['wr2', '=', 'weakref', '.', 'ref', '(', 'w', '.', '_qtWidget', '(', ')', ')', '\\n']
Tokenized   (020): ['<s>', 'wr', '2', '=', 'weak', 'ref', '.', 'ref', '(', 'w', '.', '_', 'qt', 'Widget', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (018): ['wr', '2', '=', 'weak', 'ref', '.', 'ref', '(', 'w', '.', '_', 'qt', 'Widget', '(', ')', ')', '\\', 'n']
Detokenized (013): ['wr2', '=', 'weakref', '.', 'ref', '(', 'w', '.', '_qtWidget', '(', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "WidgetTest . signalsEmitted = 0 \n"
Original    (006): ['WidgetTest', '.', 'signalsEmitted', '=', '0', '\\n']
Tokenized   (012): ['<s>', 'Widget', 'Test', '.', 'signals', 'E', 'mitted', '=', '0', '\\', 'n', '</s>']
Filtered   (010): ['Widget', 'Test', '.', 'signals', 'E', 'mitted', '=', '0', '\\', 'n']
Detokenized (006): ['WidgetTest', '.', 'signalsEmitted', '=', '0', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "QtGui . QApplication . instance ( ) . sendEvent ( w . _qtWidget ( ) , event ) \n"
Original    (019): ['QtGui', '.', 'QApplication', '.', 'instance', '(', ')', '.', 'sendEvent', '(', 'w', '.', '_qtWidget', '(', ')', ',', 'event', ')', '\\n']
Tokenized   (029): ['<s>', 'Q', 't', 'Gu', 'i', '.', 'Q', 'Application', '.', 'instance', '(', ')', '.', 'send', 'Event', '(', 'w', '.', '_', 'qt', 'Widget', '(', ')', ',', 'event', ')', '\\', 'n', '</s>']
Filtered   (027): ['Q', 't', 'Gu', 'i', '.', 'Q', 'Application', '.', 'instance', '(', ')', '.', 'send', 'Event', '(', 'w', '.', '_', 'qt', 'Widget', '(', ')', ',', 'event', ')', '\\', 'n']
Detokenized (019): ['QtGui', '.', 'QApplication', '.', 'instance', '(', ')', '.', 'sendEvent', '(', 'w', '.', '_qtWidget', '(', ')', ',', 'event', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "GafferUI . BoxUI . appendNodeEditorToolMenuDefinitions ( nodeEditor , node , menuDefinition ) \n"
Original    (013): ['GafferUI', '.', 'BoxUI', '.', 'appendNodeEditorToolMenuDefinitions', '(', 'nodeEditor', ',', 'node', ',', 'menuDefinition', ')', '\\n']
Tokenized   (027): ['<s>', 'G', 'affer', 'UI', '.', 'Box', 'UI', '.', 'append', 'Node', 'Editor', 'Tool', 'Menu', 'Def', 'initions', '(', 'node', 'Editor', ',', 'node', ',', 'menu', 'Definition', ')', '\\', 'n', '</s>']
Filtered   (025): ['G', 'affer', 'UI', '.', 'Box', 'UI', '.', 'append', 'Node', 'Editor', 'Tool', 'Menu', 'Def', 'initions', '(', 'node', 'Editor', ',', 'node', ',', 'menu', 'Definition', ')', '\\', 'n']
Detokenized (013): ['GafferUI', '.', 'BoxUI', '.', 'appendNodeEditorToolMenuDefinitions', '(', 'nodeEditor', ',', 'node', ',', 'menuDefinition', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "GafferSceneUI . FilteredSceneProcessorUI . appendNodeEditorToolMenuDefinitions ( nodeEditor , node , menuDefinition \n"
Original    (012): ['GafferSceneUI', '.', 'FilteredSceneProcessorUI', '.', 'appendNodeEditorToolMenuDefinitions', '(', 'nodeEditor', ',', 'node', ',', 'menuDefinition', '\\n']
Tokenized   (031): ['<s>', 'G', 'affer', 'Scene', 'UI', '.', 'Fil', 'tered', 'Scene', 'Process', 'or', 'UI', '.', 'append', 'Node', 'Editor', 'Tool', 'Menu', 'Def', 'initions', '(', 'node', 'Editor', ',', 'node', ',', 'menu', 'Definition', '\\', 'n', '</s>']
Filtered   (029): ['G', 'affer', 'Scene', 'UI', '.', 'Fil', 'tered', 'Scene', 'Process', 'or', 'UI', '.', 'append', 'Node', 'Editor', 'Tool', 'Menu', 'Def', 'initions', '(', 'node', 'Editor', ',', 'node', ',', 'menu', 'Definition', '\\', 'n']
Detokenized (012): ['GafferSceneUI', '.', 'FilteredSceneProcessorUI', '.', 'appendNodeEditorToolMenuDefinitions', '(', 'nodeEditor', ',', 'node', ',', 'menuDefinition', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "yappi . print_stats ( sort_type = yappi . SORTTYPE_TTOT , limit = 30 , thread_stats_on = False ) \n"
Original    (019): ['yappi', '.', 'print_stats', '(', 'sort_type', '=', 'yappi', '.', 'SORTTYPE_TTOT', ',', 'limit', '=', '30', ',', 'thread_stats_on', '=', 'False', ')', '\\n']
Tokenized   (039): ['<s>', 'y', 'app', 'i', '.', 'print', '_', 'stats', '(', 'sort', '_', 'type', '=', 'y', 'app', 'i', '.', 'S', 'ORT', 'TYPE', '_', 'TT', 'OT', ',', 'limit', '=', '30', ',', 'thread', '_', 'stats', '_', 'on', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (037): ['y', 'app', 'i', '.', 'print', '_', 'stats', '(', 'sort', '_', 'type', '=', 'y', 'app', 'i', '.', 'S', 'ORT', 'TYPE', '_', 'TT', 'OT', ',', 'limit', '=', '30', ',', 'thread', '_', 'stats', '_', 'on', '=', 'False', ')', '\\', 'n']
Detokenized (019): ['yappi', '.', 'print_stats', '(', 'sort_type', '=', 'yappi', '.', 'SORTTYPE_TTOT', ',', 'limit', '=', '30', ',', 'thread_stats_on', '=', 'False', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "SAMPLE_EXTRACT_METRICS_PAGE = os . path . join ( datadir , "monthly_download" ) \n"
Original    (013): ['SAMPLE_EXTRACT_METRICS_PAGE', '=', 'os', '.', 'path', '.', 'join', '(', 'datadir', ',', '"monthly_download"', ')', '\\n']
Tokenized   (035): ['<s>', 'SAM', 'PLE', '_', 'EX', 'TR', 'ACT', '_', 'MET', 'R', 'ICS', '_', 'PA', 'GE', '=', 'os', '.', 'path', '.', 'join', '(', 'dat', 'ad', 'ir', ',', '"', 'month', 'ly', '_', 'download', '"', ')', '\\', 'n', '</s>']
Filtered   (033): ['SAM', 'PLE', '_', 'EX', 'TR', 'ACT', '_', 'MET', 'R', 'ICS', '_', 'PA', 'GE', '=', 'os', '.', 'path', '.', 'join', '(', 'dat', 'ad', 'ir', ',', '"', 'month', 'ly', '_', 'download', '"', ')', '\\', 'n']
Detokenized (013): ['SAMPLE_EXTRACT_METRICS_PAGE', '=', 'os', '.', 'path', '.', 'join', '(', 'datadir', ',', '"monthly_download"', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH = os . path . join ( datadir , "monthly_download_different_month" \n"
Original    (012): ['SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH', '=', 'os', '.', 'path', '.', 'join', '(', 'datadir', ',', '"monthly_download_different_month"', '\\n']
Tokenized   (046): ['<s>', 'SAM', 'PLE', '_', 'EX', 'TR', 'ACT', '_', 'MET', 'R', 'ICS', '_', 'PA', 'GE', '_', 'D', 'IF', 'FER', 'ENT', '_', 'MON', 'TH', '=', 'os', '.', 'path', '.', 'join', '(', 'dat', 'ad', 'ir', ',', '"', 'month', 'ly', '_', 'download', '_', 'different', '_', 'month', '"', '\\', 'n', '</s>']
Filtered   (044): ['SAM', 'PLE', '_', 'EX', 'TR', 'ACT', '_', 'MET', 'R', 'ICS', '_', 'PA', 'GE', '_', 'D', 'IF', 'FER', 'ENT', '_', 'MON', 'TH', '=', 'os', '.', 'path', '.', 'join', '(', 'dat', 'ad', 'ir', ',', '"', 'month', 'ly', '_', 'download', '_', 'different', '_', 'month', '"', '\\', 'n']
Detokenized (012): ['SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH', '=', 'os', '.', 'path', '.', 'join', '(', 'datadir', ',', '"monthly_download_different_month"', '\\n']
Counter: 44
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "testitem_aliases = ( "pmid" , TEST_PMID ) \n"
Original    (008): ['testitem_aliases', '=', '(', '"pmid"', ',', 'TEST_PMID', ')', '\\n']
Tokenized   (021): ['<s>', 'test', 'item', '_', 'ali', 'ases', '=', '(', '"', 'pm', 'id', '"', ',', 'TEST', '_', 'PM', 'ID', ')', '\\', 'n', '</s>']
Filtered   (019): ['test', 'item', '_', 'ali', 'ases', '=', '(', '"', 'pm', 'id', '"', ',', 'TEST', '_', 'PM', 'ID', ')', '\\', 'n']
Detokenized (008): ['testitem_aliases', '=', '(', '"pmid"', ',', 'TEST_PMID', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "sample_data_dump = open ( SAMPLE_EXTRACT_METRICS_PAGE , "r" ) . read ( ) \n"
Original    (013): ['sample_data_dump', '=', 'open', '(', 'SAMPLE_EXTRACT_METRICS_PAGE', ',', '"r"', ')', '.', 'read', '(', ')', '\\n']
Tokenized   (034): ['<s>', 'sample', '_', 'data', '_', 'dump', '=', 'open', '(', 'SAM', 'PLE', '_', 'EX', 'TR', 'ACT', '_', 'MET', 'R', 'ICS', '_', 'PA', 'GE', ',', '"', 'r', '"', ')', '.', 'read', '(', ')', '\\', 'n', '</s>']
Filtered   (032): ['sample', '_', 'data', '_', 'dump', '=', 'open', '(', 'SAM', 'PLE', '_', 'EX', 'TR', 'ACT', '_', 'MET', 'R', 'ICS', '_', 'PA', 'GE', ',', '"', 'r', '"', ')', '.', 'read', '(', ')', '\\', 'n']
Detokenized (013): ['sample_data_dump', '=', 'open', '(', 'SAMPLE_EXTRACT_METRICS_PAGE', ',', '"r"', ')', '.', 'read', '(', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "sample_data_dump_different_month = open ( SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH , "r" ) . read \n"
Original    (011): ['sample_data_dump_different_month', '=', 'open', '(', 'SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH', ',', '"r"', ')', '.', 'read', '\\n']
Tokenized   (044): ['<s>', 'sample', '_', 'data', '_', 'dump', '_', 'different', '_', 'month', '=', 'open', '(', 'SAM', 'PLE', '_', 'EX', 'TR', 'ACT', '_', 'MET', 'R', 'ICS', '_', 'PA', 'GE', '_', 'D', 'IF', 'FER', 'ENT', '_', 'MON', 'TH', ',', '"', 'r', '"', ')', '.', 'read', '\\', 'n', '</s>']
Filtered   (042): ['sample', '_', 'data', '_', 'dump', '_', 'different', '_', 'month', '=', 'open', '(', 'SAM', 'PLE', '_', 'EX', 'TR', 'ACT', '_', 'MET', 'R', 'ICS', '_', 'PA', 'GE', '_', 'D', 'IF', 'FER', 'ENT', '_', 'MON', 'TH', ',', '"', 'r', '"', ')', '.', 'read', '\\', 'n']
Detokenized (011): ['sample_data_dump_different_month', '=', 'open', '(', 'SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH', ',', '"r"', ')', '.', 'read', '\\n']
Counter: 42
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""max_event_date" : "2012-01-31T07:34:01.126892" \n"
Original    (004): ['"max_event_date"', ':', '"2012-01-31T07:34:01.126892"', '\\n']
Tokenized   (029): ['<s>', '"', 'max', '_', 'event', '_', 'date', '"', ':', '"', '2012', '-', '01', '-', '31', 'T', '07', ':', '34', ':', '01', '.', '12', '68', '92', '"', '\\', 'n', '</s>']
Filtered   (027): ['"', 'max', '_', 'event', '_', 'date', '"', ':', '"', '2012', '-', '01', '-', '31', 'T', '07', ':', '34', ':', '01', '.', '12', '68', '92', '"', '\\', 'n']
Detokenized (004): ['"max_event_date"', ':', '"2012-01-31T07:34:01.126892"', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : ""_id" : "abc123" , \n"
Original    (005): ['"_id"', ':', '"abc123"', ',', '\\n']
Tokenized   (014): ['<s>', '"', '_', 'id', '"', ':', '"', 'abc', '123', '"', ',', '\\', 'n', '</s>']
Filtered   (012): ['"', '_', 'id', '"', ':', '"', 'abc', '123', '"', ',', '\\', 'n']
Detokenized (005): ['"_id"', ':', '"abc123"', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""raw" : "max_event_date" : "2012-10-31T07:34:01.126892" , \n"
Original    (007): ['"raw"', ':', '"max_event_date"', ':', '"2012-10-31T07:34:01.126892"', ',', '\\n']
Tokenized   (034): ['<s>', '"', 'raw', '"', ':', '"', 'max', '_', 'event', '_', 'date', '"', ':', '"', '2012', '-', '10', '-', '31', 'T', '07', ':', '34', ':', '01', '.', '12', '68', '92', '"', ',', '\\', 'n', '</s>']
Filtered   (032): ['"', 'raw', '"', ':', '"', 'max', '_', 'event', '_', 'date', '"', ':', '"', '2012', '-', '10', '-', '31', 'T', '07', ':', '34', ':', '01', '.', '12', '68', '92', '"', ',', '\\', 'n']
Detokenized (007): ['"raw"', ':', '"max_event_date"', ':', '"2012-10-31T07:34:01.126892"', ',', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""23110252" \n"
Original    (002): ['"23110252"', '\\n']
Tokenized   (009): ['<s>', '"', '23', '110', '252', '"', '\\', 'n', '</s>']
Filtered   (007): ['"', '23', '110', '252', '"', '\\', 'n']
Detokenized (002): ['"23110252"', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 2, 768)
# Extracted words:  2
Sentence         : "cache_client = redis . from_url ( os . getenv ( "REDIS_URL" ) , REDIS_CACHE_DATABASE_NUMBER ) \n"
Original    (016): ['cache_client', '=', 'redis', '.', 'from_url', '(', 'os', '.', 'getenv', '(', '"REDIS_URL"', ')', ',', 'REDIS_CACHE_DATABASE_NUMBER', ')', '\\n']
Tokenized   (043): ['<s>', 'cache', '_', 'client', '=', 'red', 'is', '.', 'from', '_', 'url', '(', 'os', '.', 'get', 'env', '(', '"', 'RED', 'IS', '_', 'URL', '"', ')', ',', 'RED', 'IS', '_', 'C', 'AC', 'HE', '_', 'D', 'AT', 'AB', 'ASE', '_', 'NUM', 'BER', ')', '\\', 'n', '</s>']
Filtered   (041): ['cache', '_', 'client', '=', 'red', 'is', '.', 'from', '_', 'url', '(', 'os', '.', 'get', 'env', '(', '"', 'RED', 'IS', '_', 'URL', '"', ')', ',', 'RED', 'IS', '_', 'C', 'AC', 'HE', '_', 'D', 'AT', 'AB', 'ASE', '_', 'NUM', 'BER', ')', '\\', 'n']
Detokenized (016): ['cache_client', '=', 'redis', '.', 'from_url', '(', 'os', '.', 'getenv', '(', '"REDIS_URL"', ')', ',', 'REDIS_CACHE_DATABASE_NUMBER', ')', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "MAX_CACHE_SIZE_BYTES = 100 * 1000 * 1000 #100mb \n"
Original    (009): ['MAX_CACHE_SIZE_BYTES', '=', '100', '*', '1000', '*', '1000', '#100mb', '\\n']
Tokenized   (024): ['<s>', 'MAX', '_', 'C', 'AC', 'HE', '_', 'SIZE', '_', 'BY', 'T', 'ES', '=', '100', '*', '1000', '*', '1000', '#', '100', 'mb', '\\', 'n', '</s>']
Filtered   (022): ['MAX', '_', 'C', 'AC', 'HE', '_', 'SIZE', '_', 'BY', 'T', 'ES', '=', '100', '*', '1000', '*', '1000', '#', '100', 'mb', '\\', 'n']
Detokenized (009): ['MAX_CACHE_SIZE_BYTES', '=', '100', '*', '1000', '*', '1000', '#100mb', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "set_response = mc . set ( hash_key , json . dumps ( data ) ) \n"
Original    (016): ['set_response', '=', 'mc', '.', 'set', '(', 'hash_key', ',', 'json', '.', 'dumps', '(', 'data', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'set', '_', 'response', '=', 'mc', '.', 'set', '(', 'hash', '_', 'key', ',', 'json', '.', 'dumps', '(', 'data', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['set', '_', 'response', '=', 'mc', '.', 'set', '(', 'hash', '_', 'key', ',', 'json', '.', 'dumps', '(', 'data', ')', ')', '\\', 'n']
Detokenized (016): ['set_response', '=', 'mc', '.', 'set', '(', 'hash_key', ',', 'json', '.', 'dumps', '(', 'data', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "metrics_url_template = "http://alm.plos.org/api/v3/articles?ids=%s&source=citations,counter&api_key=" provenance_url_template = "http://dx.doi.org/%s" \n"
Original    (007): ['metrics_url_template', '=', '"http://alm.plos.org/api/v3/articles?ids=%s&source=citations,counter&api_key="', 'provenance_url_template', '=', '"http://dx.doi.org/%s"', '\\n']
Tokenized   (063): ['<s>', 'met', 'rics', '_', 'url', '_', 'template', '=', '"', 'http', '://', 'alm', '.', 'pl', 'os', '.', 'org', '/', 'api', '/', 'v', '3', '/', 'articles', '?', 'ids', '=', '%', 's', '&', 'source', '=', 'c', 'itations', ',', 'counter', '&', 'api', '_', 'key', '="', 'proven', 'ance', '_', 'url', '_', 'template', '=', '"', 'http', '://', 'dx', '.', 'doi', '.', 'org', '/', '%', 's', '"', '\\', 'n', '</s>']
Filtered   (061): ['met', 'rics', '_', 'url', '_', 'template', '=', '"', 'http', '://', 'alm', '.', 'pl', 'os', '.', 'org', '/', 'api', '/', 'v', '3', '/', 'articles', '?', 'ids', '=', '%', 's', '&', 'source', '=', 'c', 'itations', ',', 'counter', '&', 'api', '_', 'key', '="', 'proven', 'ance', '_', 'url', '_', 'template', '=', '"', 'http', '://', 'dx', '.', 'doi', '.', 'org', '/', '%', 's', '"', '\\', 'n']
Detokenized (007): ['metrics_url_template', '=', '"http://alm.plos.org/api/v3/articles?ids=%s&source=citations,counter&api_key="', 'provenance_url_template', '=', '"http://dx.doi.org/%s"', '\\n']
Counter: 61
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "relevant = ( ( "doi" == namespace ) and ( "10.1371/" in nid ) ) \n"
Original    (016): ['relevant', '=', '(', '(', '"doi"', '==', 'namespace', ')', 'and', '(', '"10.1371/"', 'in', 'nid', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'relevant', '=', '(', '(', '"', 'doi', '"', '==', 'namespace', ')', 'and', '(', '"', '10', '.', '13', '71', '/"', 'in', 'n', 'id', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['relevant', '=', '(', '(', '"', 'doi', '"', '==', 'namespace', ')', 'and', '(', '"', '10', '.', '13', '71', '/"', 'in', 'n', 'id', ')', ')', '\\', 'n']
Detokenized (016): ['relevant', '=', '(', '(', '"doi"', '==', 'namespace', ')', 'and', '(', '"10.1371/"', 'in', 'nid', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "this_article = json_response [ 0 ] [ "sources" ] [ 0 ] [ "metrics" ] \n"
Original    (016): ['this_article', '=', 'json_response', '[', '0', ']', '[', '"sources"', ']', '[', '0', ']', '[', '"metrics"', ']', '\\n']
Tokenized   (029): ['<s>', 'this', '_', 'article', '=', 'json', '_', 'response', '[', '0', ']', '[', '"', 's', 'ources', '"', ']', '[', '0', ']', '[', '"', 'met', 'rics', '"', ']', '\\', 'n', '</s>']
Filtered   (027): ['this', '_', 'article', '=', 'json', '_', 'response', '[', '0', ']', '[', '"', 's', 'ources', '"', ']', '[', '0', ']', '[', '"', 'met', 'rics', '"', ']', '\\', 'n']
Detokenized (016): ['this_article', '=', 'json_response', '[', '0', ']', '[', '"sources"', ']', '[', '0', ']', '[', '"metrics"', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "redis_url = os . environ . get ( , "redis://127.0.0.1:6379/" ) \n"
Original    (012): ['redis_url', '=', 'os', '.', 'environ', '.', 'get', '(', ',', '"redis://127.0.0.1:6379/"', ')', '\\n']
Tokenized   (033): ['<s>', 'red', 'is', '_', 'url', '=', 'os', '.', 'en', 'viron', '.', 'get', '(', ',', '"', 'red', 'is', '://', '127', '.', '0', '.', '0', '.', '1', ':', '6', '379', '/"', ')', '\\', 'n', '</s>']
Filtered   (031): ['red', 'is', '_', 'url', '=', 'os', '.', 'en', 'viron', '.', 'get', '(', ',', '"', 'red', 'is', '://', '127', '.', '0', '.', '0', '.', '1', ':', '6', '379', '/"', ')', '\\', 'n']
Detokenized (012): ['redis_url', '=', 'os', '.', 'environ', '.', 'get', '(', ',', '"redis://127.0.0.1:6379/"', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "Queue ( , routing_key = ) \n"
Original    (007): ['Queue', '(', ',', 'routing_key', '=', ')', '\\n']
Tokenized   (012): ['<s>', 'Queue', '(', ',', 'routing', '_', 'key', '=', ')', '\\', 'n', '</s>']
Filtered   (010): ['Queue', '(', ',', 'routing', '_', 'key', '=', ')', '\\', 'n']
Detokenized (007): ['Queue', '(', ',', 'routing_key', '=', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "CELERY_ACCEPT_CONTENT = [ , ] \n"
Original    (006): ['CELERY_ACCEPT_CONTENT', '=', '[', ',', ']', '\\n']
Tokenized   (017): ['<s>', 'C', 'EL', 'ERY', '_', 'AC', 'CEPT', '_', 'CONT', 'ENT', '=', '[', ',', ']', '\\', 'n', '</s>']
Filtered   (015): ['C', 'EL', 'ERY', '_', 'AC', 'CEPT', '_', 'CONT', 'ENT', '=', '[', ',', ']', '\\', 'n']
Detokenized (006): ['CELERY_ACCEPT_CONTENT', '=', '[', ',', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "CELERY_IMPORTS = ( "core_tasks" , ) \n"
Original    (007): ['CELERY_IMPORTS', '=', '(', '"core_tasks"', ',', ')', '\\n']
Tokenized   (021): ['<s>', 'C', 'EL', 'ERY', '_', 'IM', 'P', 'ORTS', '=', '(', '"', 'core', '_', 't', 'asks', '"', ',', ')', '\\', 'n', '</s>']
Filtered   (019): ['C', 'EL', 'ERY', '_', 'IM', 'P', 'ORTS', '=', '(', '"', 'core', '_', 't', 'asks', '"', ',', ')', '\\', 'n']
Detokenized (007): ['CELERY_IMPORTS', '=', '(', '"core_tasks"', ',', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "sampledir = os . path . join ( os . path . split ( __file__ ) [ 0 ] , "../../../extras/sample_provider_pages/" ) \n"
Original    (023): ['sampledir', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'split', '(', '__file__', ')', '[', '0', ']', ',', '"../../../extras/sample_provider_pages/"', ')', '\\n']
Tokenized   (043): ['<s>', 'sam', 'pled', 'ir', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'split', '(', '__', 'file', '__', ')', '[', '0', ']', ',', '"', '../', '../', '../', 'ext', 'ras', '/', 'sample', '_', 'prov', 'ider', '_', 'pages', '/"', ')', '\\', 'n', '</s>']
Filtered   (041): ['sam', 'pled', 'ir', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'split', '(', '__', 'file', '__', ')', '[', '0', ']', ',', '"', '../', '../', '../', 'ext', 'ras', '/', 'sample', '_', 'prov', 'ider', '_', 'pages', '/"', ')', '\\', 'n']
Detokenized (023): ['sampledir', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'split', '(', '__file__', ')', '[', '0', ']', ',', '"../../../extras/sample_provider_pages/"', ')', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "TEST_XML = open ( os . path . join ( sampledir , "facebook" , "metrics" ) ) . read ( ) \n"
Original    (022): ['TEST_XML', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'sampledir', ',', '"facebook"', ',', '"metrics"', ')', ')', '.', 'read', '(', ')', '\\n']
Tokenized   (035): ['<s>', 'T', 'EST', '_', 'X', 'ML', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'sampled', 'ir', ',', '"', 'facebook', '"', ',', '"', 'met', 'rics', '"', ')', ')', '.', 'read', '(', ')', '\\', 'n', '</s>']
Filtered   (033): ['T', 'EST', '_', 'X', 'ML', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'sampled', 'ir', ',', '"', 'facebook', '"', ',', '"', 'met', 'rics', '"', ')', ')', '.', 'read', '(', ')', '\\', 'n']
Detokenized (022): ['TEST_XML', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'sampledir', ',', '"facebook"', ',', '"metrics"', ')', ')', '.', 'read', '(', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "provider_names = [ provider . __class__ . __name__ for provider in providers ] \n"
Original    (014): ['provider_names', '=', '[', 'provider', '.', '__class__', '.', '__name__', 'for', 'provider', 'in', 'providers', ']', '\\n']
Tokenized   (024): ['<s>', 'prov', 'ider', '_', 'names', '=', '[', 'provider', '.', '__', 'class', '__', '.', '__', 'name', '__', 'for', 'provider', 'in', 'providers', ']', '\\', 'n', '</s>']
Filtered   (022): ['prov', 'ider', '_', 'names', '=', '[', 'provider', '.', '__', 'class', '__', '.', '__', 'name', '__', 'for', 'provider', 'in', 'providers', ']', '\\', 'n']
Detokenized (014): ['provider_names', '=', '[', 'provider', '.', '__class__', '.', '__name__', 'for', 'provider', 'in', 'providers', ']', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "assert_equals ( md [ "pubmed" ] [ ] , ) \n"
Original    (011): ['assert_equals', '(', 'md', '[', '"pubmed"', ']', '[', ']', ',', ')', '\\n']
Tokenized   (020): ['<s>', 'assert', '_', 'equ', 'als', '(', 'md', '[', '"', 'pub', 'med', '"', ']', '[', ']', ',', ')', '\\', 'n', '</s>']
Filtered   (018): ['assert', '_', 'equ', 'als', '(', 'md', '[', '"', 'pub', 'med', '"', ']', '[', ']', ',', ')', '\\', 'n']
Detokenized (011): ['assert_equals', '(', 'md', '[', '"pubmed"', ']', '[', ']', ',', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "tiid = db . Column ( db . Text , db . ForeignKey ( ) , primary_key = True ) \n"
Original    (021): ['tiid', '=', 'db', '.', 'Column', '(', 'db', '.', 'Text', ',', 'db', '.', 'ForeignKey', '(', ')', ',', 'primary_key', '=', 'True', ')', '\\n']
Tokenized   (028): ['<s>', 'ti', 'id', '=', 'db', '.', 'Column', '(', 'db', '.', 'Text', ',', 'db', '.', 'Foreign', 'Key', '(', ')', ',', 'primary', '_', 'key', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (026): ['ti', 'id', '=', 'db', '.', 'Column', '(', 'db', '.', 'Text', ',', 'db', '.', 'Foreign', 'Key', '(', ')', ',', 'primary', '_', 'key', '=', 'True', ')', '\\', 'n']
Detokenized (021): ['tiid', '=', 'db', '.', 'Column', '(', 'db', '.', 'Text', ',', 'db', '.', 'ForeignKey', '(', ')', ',', 'primary_key', '=', 'True', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "tweet_ids_with_response = [ tweet [ "id_str" ] for tweet in data ] \n"
Original    (013): ['tweet_ids_with_response', '=', '[', 'tweet', '[', '"id_str"', ']', 'for', 'tweet', 'in', 'data', ']', '\\n']
Tokenized   (027): ['<s>', 't', 'weet', '_', 'ids', '_', 'with', '_', 'response', '=', '[', 'tweet', '[', '"', 'id', '_', 'str', '"', ']', 'for', 'tweet', 'in', 'data', ']', '\\', 'n', '</s>']
Filtered   (025): ['t', 'weet', '_', 'ids', '_', 'with', '_', 'response', '=', '[', 'tweet', '[', '"', 'id', '_', 'str', '"', ']', 'for', 'tweet', 'in', 'data', ']', '\\', 'n']
Detokenized (013): ['tweet_ids_with_response', '=', '[', 'tweet', '[', '"id_str"', ']', 'for', 'tweet', 'in', 'data', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "tweet_ids_without_response = [ tweet for tweet in tweet_ids if tweet not in tweet_ids_with_response flag_deleted_tweets ( tweet_ids_without_response ) \n"
Original    (018): ['tweet_ids_without_response', '=', '[', 'tweet', 'for', 'tweet', 'in', 'tweet_ids', 'if', 'tweet', 'not', 'in', 'tweet_ids_with_response', 'flag_deleted_tweets', '(', 'tweet_ids_without_response', ')', '\\n']
Tokenized   (049): ['<s>', 't', 'weet', '_', 'ids', '_', 'without', '_', 'response', '=', '[', 'tweet', 'for', 'tweet', 'in', 'tweet', '_', 'ids', 'if', 'tweet', 'not', 'in', 'tweet', '_', 'ids', '_', 'with', '_', 'response', 'flag', '_', 'de', 'leted', '_', 't', 'we', 'ets', '(', 'tweet', '_', 'ids', '_', 'without', '_', 'response', ')', '\\', 'n', '</s>']
Filtered   (047): ['t', 'weet', '_', 'ids', '_', 'without', '_', 'response', '=', '[', 'tweet', 'for', 'tweet', 'in', 'tweet', '_', 'ids', 'if', 'tweet', 'not', 'in', 'tweet', '_', 'ids', '_', 'with', '_', 'response', 'flag', '_', 'de', 'leted', '_', 't', 'we', 'ets', '(', 'tweet', '_', 'ids', '_', 'without', '_', 'response', ')', '\\', 'n']
Detokenized (018): ['tweet_ids_without_response', '=', '[', 'tweet', 'for', 'tweet', 'in', 'tweet_ids', 'if', 'tweet', 'not', 'in', 'tweet_ids_with_response', 'flag_deleted_tweets', '(', 'tweet_ids_without_response', ')', '\\n']
Counter: 47
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "access_token = os . getenv ( "TWITTER_ACCESS_TOKEN" ) \n"
Original    (009): ['access_token', '=', 'os', '.', 'getenv', '(', '"TWITTER_ACCESS_TOKEN"', ')', '\\n']
Tokenized   (025): ['<s>', 'access', '_', 'token', '=', 'os', '.', 'get', 'env', '(', '"', 'TW', 'IT', 'TER', '_', 'ACC', 'ESS', '_', 'TO', 'KEN', '"', ')', '\\', 'n', '</s>']
Filtered   (023): ['access', '_', 'token', '=', 'os', '.', 'get', 'env', '(', '"', 'TW', 'IT', 'TER', '_', 'ACC', 'ESS', '_', 'TO', 'KEN', '"', ')', '\\', 'n']
Detokenized (009): ['access_token', '=', 'os', '.', 'getenv', '(', '"TWITTER_ACCESS_TOKEN"', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "num = len ( tweets ) ) ) \n"
Original    (009): ['num', '=', 'len', '(', 'tweets', ')', ')', ')', '\\n']
Tokenized   (012): ['<s>', 'num', '=', 'len', '(', 'tweets', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (010): ['num', '=', 'len', '(', 'tweets', ')', ')', ')', '\\', 'n']
Detokenized (009): ['num', '=', 'len', '(', 'tweets', ')', ')', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "list_of_groups = [ tweets [ i : i + group_size ] for i in range ( 0 , len ( tweets ) , group_size ) ] \n"
Original    (027): ['list_of_groups', '=', '[', 'tweets', '[', 'i', ':', 'i', '+', 'group_size', ']', 'for', 'i', 'in', 'range', '(', '0', ',', 'len', '(', 'tweets', ')', ',', 'group_size', ')', ']', '\\n']
Tokenized   (038): ['<s>', 'list', '_', 'of', '_', 'groups', '=', '[', 'tweets', '[', 'i', ':', 'i', '+', 'group', '_', 'size', ']', 'for', 'i', 'in', 'range', '(', '0', ',', 'len', '(', 'tweets', ')', ',', 'group', '_', 'size', ')', ']', '\\', 'n', '</s>']
Filtered   (036): ['list', '_', 'of', '_', 'groups', '=', '[', 'tweets', '[', 'i', ':', 'i', '+', 'group', '_', 'size', ']', 'for', 'i', 'in', 'range', '(', '0', ',', 'len', '(', 'tweets', ')', ',', 'group', '_', 'size', ')', ']', '\\', 'n']
Detokenized (027): ['list_of_groups', '=', '[', 'tweets', '[', 'i', ':', 'i', '+', 'group_size', ']', 'for', 'i', 'in', 'range', '(', '0', ',', 'len', '(', 'tweets', ')', ',', 'group_size', ')', ']', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "handle_all_tweets ( response . data , tweet_subset ) \n"
Original    (009): ['handle_all_tweets', '(', 'response', '.', 'data', ',', 'tweet_subset', ')', '\\n']
Tokenized   (021): ['<s>', 'handle', '_', 'all', '_', 't', 'we', 'ets', '(', 'response', '.', 'data', ',', 'tweet', '_', 'sub', 'set', ')', '\\', 'n', '</s>']
Filtered   (019): ['handle', '_', 'all', '_', 't', 'we', 'ets', '(', 'response', '.', 'data', ',', 'tweet', '_', 'sub', 'set', ')', '\\', 'n']
Detokenized (009): ['handle_all_tweets', '(', 'response', '.', 'data', ',', 'tweet_subset', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "tweets = Tweet . query . filter ( Tweet . profile_id == profile_id ) \n"
Original    (015): ['tweets', '=', 'Tweet', '.', 'query', '.', 'filter', '(', 'Tweet', '.', 'profile_id', '==', 'profile_id', ')', '\\n']
Tokenized   (024): ['<s>', 't', 'we', 'ets', '=', 'Tweet', '.', 'query', '.', 'filter', '(', 'Tweet', '.', 'profile', '_', 'id', '==', 'profile', '_', 'id', ')', '\\', 'n', '</s>']
Filtered   (022): ['t', 'we', 'ets', '=', 'Tweet', '.', 'query', '.', 'filter', '(', 'Tweet', '.', 'profile', '_', 'id', '==', 'profile', '_', 'id', ')', '\\', 'n']
Detokenized (015): ['tweets', '=', 'Tweet', '.', 'query', '.', 'filter', '(', 'Tweet', '.', 'profile_id', '==', 'profile_id', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "tweet_dict = dict ( [ ( ( tweet . tweet_id , tweet . tiid ) , tweet ) for tweet in tweets ] ) \n"
Original    (025): ['tweet_dict', '=', 'dict', '(', '[', '(', '(', 'tweet', '.', 'tweet_id', ',', 'tweet', '.', 'tiid', ')', ',', 'tweet', ')', 'for', 'tweet', 'in', 'tweets', ']', ')', '\\n']
Tokenized   (034): ['<s>', 't', 'weet', '_', 'dict', '=', 'dict', '(', '[', '(', '(', 'tweet', '.', 'tweet', '_', 'id', ',', 'tweet', '.', 'ti', 'id', ')', ',', 'tweet', ')', 'for', 'tweet', 'in', 'tweets', ']', ')', '\\', 'n', '</s>']
Filtered   (032): ['t', 'weet', '_', 'dict', '=', 'dict', '(', '[', '(', '(', 'tweet', '.', 'tweet', '_', 'id', ',', 'tweet', '.', 'ti', 'id', ')', ',', 'tweet', ')', 'for', 'tweet', 'in', 'tweets', ']', ')', '\\', 'n']
Detokenized (025): ['tweet_dict', '=', 'dict', '(', '[', '(', '(', 'tweet', '.', 'tweet_id', ',', 'tweet', '.', 'tiid', ')', ',', 'tweet', ')', 'for', 'tweet', 'in', 'tweets', ']', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "tweet . profile_id = profile_id \n"
Original    (006): ['tweet', '.', 'profile_id', '=', 'profile_id', '\\n']
Tokenized   (014): ['<s>', 't', 'weet', '.', 'profile', '_', 'id', '=', 'profile', '_', 'id', '\\', 'n', '</s>']
Filtered   (012): ['t', 'weet', '.', 'profile', '_', 'id', '=', 'profile', '_', 'id', '\\', 'n']
Detokenized (006): ['tweet', '.', 'profile_id', '=', 'profile_id', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "tweet_ids = [ tweet . tweet_id for tweet in tweets_to_hydrate_from_twitter ] \n"
Original    (012): ['tweet_ids', '=', '[', 'tweet', '.', 'tweet_id', 'for', 'tweet', 'in', 'tweets_to_hydrate_from_twitter', ']', '\\n']
Tokenized   (029): ['<s>', 't', 'weet', '_', 'ids', '=', '[', 'tweet', '.', 'tweet', '_', 'id', 'for', 'tweet', 'in', 'tweets', '_', 'to', '_', 'hyd', 'rate', '_', 'from', '_', 'twitter', ']', '\\', 'n', '</s>']
Filtered   (027): ['t', 'weet', '_', 'ids', '=', '[', 'tweet', '.', 'tweet', '_', 'id', 'for', 'tweet', 'in', 'tweets', '_', 'to', '_', 'hyd', 'rate', '_', 'from', '_', 'twitter', ']', '\\', 'n']
Detokenized (012): ['tweet_ids', '=', '[', 'tweet', '.', 'tweet_id', 'for', 'tweet', 'in', 'tweets_to_hydrate_from_twitter', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "display_url = url_info [ "display_url" ] \n"
Original    (007): ['display_url', '=', 'url_info', '[', '"display_url"', ']', '\\n']
Tokenized   (018): ['<s>', 'display', '_', 'url', '=', 'url', '_', 'info', '[', '"', 'display', '_', 'url', '"', ']', '\\', 'n', '</s>']
Filtered   (016): ['display', '_', 'url', '=', 'url', '_', 'info', '[', '"', 'display', '_', 'url', '"', ']', '\\', 'n']
Detokenized (007): ['display_url', '=', 'url_info', '[', '"display_url"', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "tweet_id = self . tweet_id , \n"
Original    (007): ['tweet_id', '=', 'self', '.', 'tweet_id', ',', '\\n']
Tokenized   (015): ['<s>', 't', 'weet', '_', 'id', '=', 'self', '.', 'tweet', '_', 'id', ',', '\\', 'n', '</s>']
Filtered   (013): ['t', 'weet', '_', 'id', '=', 'self', '.', 'tweet', '_', 'id', ',', '\\', 'n']
Detokenized (007): ['tweet_id', '=', 'self', '.', 'tweet_id', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "file_loc = os . path . dirname ( os . path . realpath ( __file__ ) ) \n"
Original    (018): ['file_loc', '=', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'realpath', '(', '__file__', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'file', '_', 'loc', '=', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'real', 'path', '(', '__', 'file', '__', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['file', '_', 'loc', '=', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'real', 'path', '(', '__', 'file', '__', ')', ')', '\\', 'n']
Detokenized (018): ['file_loc', '=', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'realpath', '(', '__file__', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "urllib . urlretrieve ( url + fname , fname ) \n"
Original    (011): ['urllib', '.', 'urlretrieve', '(', 'url', '+', 'fname', ',', 'fname', ')', '\\n']
Tokenized   (020): ['<s>', 'ur', 'll', 'ib', '.', 'url', 'ret', 'rieve', '(', 'url', '+', 'f', 'name', ',', 'f', 'name', ')', '\\', 'n', '</s>']
Filtered   (018): ['ur', 'll', 'ib', '.', 'url', 'ret', 'rieve', '(', 'url', '+', 'f', 'name', ',', 'f', 'name', ')', '\\', 'n']
Detokenized (011): ['urllib', '.', 'urlretrieve', '(', 'url', '+', 'fname', ',', 'fname', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "loaded = np . fromstring ( fd . read ( ) , dtype = np . uint8 ) \n"
Original    (019): ['loaded', '=', 'np', '.', 'fromstring', '(', 'fd', '.', 'read', '(', ')', ',', 'dtype', '=', 'np', '.', 'uint8', ')', '\\n']
Tokenized   (026): ['<s>', 'loaded', '=', 'np', '.', 'from', 'string', '(', 'f', 'd', '.', 'read', '(', ')', ',', 'd', 'type', '=', 'np', '.', 'uint', '8', ')', '\\', 'n', '</s>']
Filtered   (024): ['loaded', '=', 'np', '.', 'from', 'string', '(', 'f', 'd', '.', 'read', '(', ')', ',', 'd', 'type', '=', 'np', '.', 'uint', '8', ')', '\\', 'n']
Detokenized (019): ['loaded', '=', 'np', '.', 'fromstring', '(', 'fd', '.', 'read', '(', ')', ',', 'dtype', '=', 'np', '.', 'uint8', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "fd = gzip . open ( os . path . join ( data_dir , ) ) \n"
Original    (017): ['fd', '=', 'gzip', '.', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'data_dir', ',', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'fd', '=', 'g', 'zip', '.', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'data', '_', 'dir', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['fd', '=', 'g', 'zip', '.', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'data', '_', 'dir', ',', ')', ')', '\\', 'n']
Detokenized (017): ['fd', '=', 'gzip', '.', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'data_dir', ',', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "trY = loaded [ 8 : ] . reshape ( ( 60000 ) ) \n"
Original    (015): ['trY', '=', 'loaded', '[', '8', ':', ']', '.', 'reshape', '(', '(', '60000', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'tr', 'Y', '=', 'loaded', '[', '8', ':', ']', '.', 'resh', 'ape', '(', '(', '6', '0000', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['tr', 'Y', '=', 'loaded', '[', '8', ':', ']', '.', 'resh', 'ape', '(', '(', '6', '0000', ')', ')', '\\', 'n']
Detokenized (015): ['trY', '=', 'loaded', '[', '8', ':', ']', '.', 'reshape', '(', '(', '60000', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "trX = trX . reshape ( - 1 , 28 , 28 ) \n"
Original    (014): ['trX', '=', 'trX', '.', 'reshape', '(', '-', '1', ',', '28', ',', '28', ')', '\\n']
Tokenized   (020): ['<s>', 'tr', 'X', '=', 'tr', 'X', '.', 'resh', 'ape', '(', '-', '1', ',', '28', ',', '28', ')', '\\', 'n', '</s>']
Filtered   (018): ['tr', 'X', '=', 'tr', 'X', '.', 'resh', 'ape', '(', '-', '1', ',', '28', ',', '28', ')', '\\', 'n']
Detokenized (014): ['trX', '=', 'trX', '.', 'reshape', '(', '-', '1', ',', '28', ',', '28', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "dirpath = os . path . join ( self . repo . path , "unused_directory" ) \n"
Original    (017): ['dirpath', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"unused_directory"', ')', '\\n']
Tokenized   (026): ['<s>', 'dir', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"', 'un', 'used', '_', 'directory', '"', ')', '\\', 'n', '</s>']
Filtered   (024): ['dir', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"', 'un', 'used', '_', 'directory', '"', ')', '\\', 'n']
Detokenized (017): ['dirpath', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"unused_directory"', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "subpath = os . path . join ( self . repo . path , "a" , "b" , "c" ) \n"
Original    (021): ['subpath', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"a"', ',', '"b"', ',', '"c"', ')', '\\n']
Tokenized   (031): ['<s>', 'sub', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"', 'a', '"', ',', '"', 'b', '"', ',', '"', 'c', '"', ')', '\\', 'n', '</s>']
Filtered   (029): ['sub', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"', 'a', '"', ',', '"', 'b', '"', ',', '"', 'c', '"', ')', '\\', 'n']
Detokenized (021): ['subpath', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'repo', '.', 'path', ',', '"a"', ',', '"b"', ',', '"c"', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "handle = self . profile . username , \n"
Original    (009): ['handle', '=', 'self', '.', 'profile', '.', 'username', ',', '\\n']
Tokenized   (012): ['<s>', 'handle', '=', 'self', '.', 'profile', '.', 'username', ',', '\\', 'n', '</s>']
Filtered   (010): ['handle', '=', 'self', '.', 'profile', '.', 'username', ',', '\\', 'n']
Detokenized (009): ['handle', '=', 'self', '.', 'profile', '.', 'username', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "id_key = ) \n"
Original    (004): ['id_key', '=', ')', '\\n']
Tokenized   (009): ['<s>', 'id', '_', 'key', '=', ')', '\\', 'n', '</s>']
Filtered   (007): ['id', '_', 'key', '=', ')', '\\', 'n']
Detokenized (004): ['id_key', '=', ')', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "message_thread = model . MessageThread ( okc_id = self . thread . id , \n"
Original    (015): ['message_thread', '=', 'model', '.', 'MessageThread', '(', 'okc_id', '=', 'self', '.', 'thread', '.', 'id', ',', '\\n']
Tokenized   (024): ['<s>', 'message', '_', 'thread', '=', 'model', '.', 'Message', 'Thread', '(', 'ok', 'c', '_', 'id', '=', 'self', '.', 'thread', '.', 'id', ',', '\\', 'n', '</s>']
Filtered   (022): ['message', '_', 'thread', '=', 'model', '.', 'Message', 'Thread', '(', 'ok', 'c', '_', 'id', '=', 'self', '.', 'thread', '.', 'id', ',', '\\', 'n']
Detokenized (015): ['message_thread', '=', 'model', '.', 'MessageThread', '(', 'okc_id', '=', 'self', '.', 'thread', '.', 'id', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "new_messages = [ message for message in self . thread . messages \n"
Original    (013): ['new_messages', '=', '[', 'message', 'for', 'message', 'in', 'self', '.', 'thread', '.', 'messages', '\\n']
Tokenized   (019): ['<s>', 'new', '_', 'mess', 'ages', '=', '[', 'message', 'for', 'message', 'in', 'self', '.', 'thread', '.', 'messages', '\\', 'n', '</s>']
Filtered   (017): ['new', '_', 'mess', 'ages', '=', '[', 'message', 'for', 'message', 'in', 'self', '.', 'thread', '.', 'messages', '\\', 'n']
Detokenized (013): ['new_messages', '=', '[', 'message', 'for', 'message', 'in', 'self', '.', 'thread', '.', 'messages', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "new_message_model = model . Message ( okc_id = new_message . id , \n"
Original    (013): ['new_message_model', '=', 'model', '.', 'Message', '(', 'okc_id', '=', 'new_message', '.', 'id', ',', '\\n']
Tokenized   (025): ['<s>', 'new', '_', 'message', '_', 'model', '=', 'model', '.', 'Message', '(', 'ok', 'c', '_', 'id', '=', 'new', '_', 'message', '.', 'id', ',', '\\', 'n', '</s>']
Filtered   (023): ['new', '_', 'message', '_', 'model', '=', 'model', '.', 'Message', '(', 'ok', 'c', '_', 'id', '=', 'new', '_', 'message', '.', 'id', ',', '\\', 'n']
Detokenized (013): ['new_message_model', '=', 'model', '.', 'Message', '(', 'okc_id', '=', 'new_message', '.', 'id', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "time_sent = new_message . time_sent ) \n"
Original    (007): ['time_sent', '=', 'new_message', '.', 'time_sent', ')', '\\n']
Tokenized   (016): ['<s>', 'time', '_', 'sent', '=', 'new', '_', 'message', '.', 'time', '_', 'sent', ')', '\\', 'n', '</s>']
Filtered   (014): ['time', '_', 'sent', '=', 'new', '_', 'message', '.', 'time', '_', 'sent', ')', '\\', 'n']
Detokenized (007): ['time_sent', '=', 'new_message', '.', 'time_sent', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "mailbox . Sync ( user ) . all ( ) \n"
Original    (011): ['mailbox', '.', 'Sync', '(', 'user', ')', '.', 'all', '(', ')', '\\n']
Tokenized   (015): ['<s>', 'mail', 'box', '.', 'Sync', '(', 'user', ')', '.', 'all', '(', ')', '\\', 'n', '</s>']
Filtered   (013): ['mail', 'box', '.', 'Sync', '(', 'user', ')', '.', 'all', '(', ')', '\\', 'n']
Detokenized (011): ['mailbox', '.', 'Sync', '(', 'user', ')', '.', 'all', '(', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "user_model . upsert_model ( id_key = ) \n"
Original    (008): ['user_model', '.', 'upsert_model', '(', 'id_key', '=', ')', '\\n']
Tokenized   (018): ['<s>', 'user', '_', 'model', '.', 'ups', 'ert', '_', 'model', '(', 'id', '_', 'key', '=', ')', '\\', 'n', '</s>']
Filtered   (016): ['user', '_', 'model', '.', 'ups', 'ert', '_', 'model', '(', 'id', '_', 'key', '=', ')', '\\', 'n']
Detokenized (008): ['user_model', '.', 'upsert_model', '(', 'id_key', '=', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "response_dict = user . photo . upload_and_confirm ( user . quickmatch ( ) . photo_infos [ 0 ] ) \n"
Original    (020): ['response_dict', '=', 'user', '.', 'photo', '.', 'upload_and_confirm', '(', 'user', '.', 'quickmatch', '(', ')', '.', 'photo_infos', '[', '0', ']', ')', '\\n']
Tokenized   (034): ['<s>', 'response', '_', 'dict', '=', 'user', '.', 'photo', '.', 'upload', '_', 'and', '_', 'conf', 'irm', '(', 'user', '.', 'quick', 'match', '(', ')', '.', 'photo', '_', 'inf', 'os', '[', '0', ']', ')', '\\', 'n', '</s>']
Filtered   (032): ['response', '_', 'dict', '=', 'user', '.', 'photo', '.', 'upload', '_', 'and', '_', 'conf', 'irm', '(', 'user', '.', 'quick', 'match', '(', ')', '.', 'photo', '_', 'inf', 'os', '[', '0', ']', ')', '\\', 'n']
Detokenized (020): ['response_dict', '=', 'user', '.', 'photo', '.', 'upload_and_confirm', '(', 'user', '.', 'quickmatch', '(', ')', '.', 'photo_infos', '[', '0', ']', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "vcr_live_sleep ( 2 ) \n"
Original    (005): ['vcr_live_sleep', '(', '2', ')', '\\n']
Tokenized   (013): ['<s>', 'v', 'cr', '_', 'live', '_', 'sleep', '(', '2', ')', '\\', 'n', '</s>']
Filtered   (011): ['v', 'cr', '_', 'live', '_', 'sleep', '(', '2', ')', '\\', 'n']
Detokenized (005): ['vcr_live_sleep', '(', '2', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "b2_h = shared_zeros ( ( self . hp . batch_size , n_h ) ) \n"
Original    (015): ['b2_h', '=', 'shared_zeros', '(', '(', 'self', '.', 'hp', '.', 'batch_size', ',', 'n_h', ')', ')', '\\n']
Tokenized   (028): ['<s>', 'b', '2', '_', 'h', '=', 'shared', '_', 'zer', 'os', '(', '(', 'self', '.', 'hp', '.', 'batch', '_', 'size', ',', 'n', '_', 'h', ')', ')', '\\', 'n', '</s>']
Filtered   (026): ['b', '2', '_', 'h', '=', 'shared', '_', 'zer', 'os', '(', '(', 'self', '.', 'hp', '.', 'batch', '_', 'size', ',', 'n', '_', 'h', ')', ')', '\\', 'n']
Detokenized (015): ['b2_h', '=', 'shared_zeros', '(', '(', 'self', '.', 'hp', '.', 'batch_size', ',', 'n_h', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "W1 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n"
Original    (019): ['W1', '=', 'shared_normal', '(', '(', 'n_h', ',', 'n_h', '*', 'gates', ')', ',', 'scale', '=', 'scale', '*', '1.5', ')', '\\n']
Tokenized   (031): ['<s>', 'W', '1', '=', 'shared', '_', 'normal', '(', '(', 'n', '_', 'h', ',', 'n', '_', 'h', '*', 'gates', ')', ',', 'scale', '=', 'scale', '*', '1', '.', '5', ')', '\\', 'n', '</s>']
Filtered   (029): ['W', '1', '=', 'shared', '_', 'normal', '(', '(', 'n', '_', 'h', ',', 'n', '_', 'h', '*', 'gates', ')', ',', 'scale', '=', 'scale', '*', '1', '.', '5', ')', '\\', 'n']
Detokenized (019): ['W1', '=', 'shared_normal', '(', '(', 'n_h', ',', 'n_h', '*', 'gates', ')', ',', 'scale', '=', 'scale', '*', '1.5', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "b1 = shared_zeros ( ( n_h * gates ) ) \n"
Original    (011): ['b1', '=', 'shared_zeros', '(', '(', 'n_h', '*', 'gates', ')', ')', '\\n']
Tokenized   (020): ['<s>', 'b', '1', '=', 'shared', '_', 'zer', 'os', '(', '(', 'n', '_', 'h', '*', 'gates', ')', ')', '\\', 'n', '</s>']
Filtered   (018): ['b', '1', '=', 'shared', '_', 'zer', 'os', '(', '(', 'n', '_', 'h', '*', 'gates', ')', ')', '\\', 'n']
Detokenized (011): ['b1', '=', 'shared_zeros', '(', '(', 'n_h', '*', 'gates', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "b2 = shared_zeros ( ( n_h * gates , ) ) \n"
Original    (012): ['b2', '=', 'shared_zeros', '(', '(', 'n_h', '*', 'gates', ',', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'b', '2', '=', 'shared', '_', 'zer', 'os', '(', '(', 'n', '_', 'h', '*', 'gates', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['b', '2', '=', 'shared', '_', 'zer', 'os', '(', '(', 'n', '_', 'h', '*', 'gates', ',', ')', ')', '\\', 'n']
Detokenized (012): ['b2', '=', 'shared_zeros', '(', '(', 'n_h', '*', 'gates', ',', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "i_on = T . nnet . sigmoid ( g_on [ : , : n_h ] ) \n"
Original    (017): ['i_on', '=', 'T', '.', 'nnet', '.', 'sigmoid', '(', 'g_on', '[', ':', ',', ':', 'n_h', ']', ')', '\\n']
Tokenized   (029): ['<s>', 'i', '_', 'on', '=', 'T', '.', 'n', 'net', '.', 's', 'igm', 'oid', '(', 'g', '_', 'on', '[', ':', ',', ':', 'n', '_', 'h', ']', ')', '\\', 'n', '</s>']
Filtered   (027): ['i', '_', 'on', '=', 'T', '.', 'n', 'net', '.', 's', 'igm', 'oid', '(', 'g', '_', 'on', '[', ':', ',', ':', 'n', '_', 'h', ']', ')', '\\', 'n']
Detokenized (017): ['i_on', '=', 'T', '.', 'nnet', '.', 'sigmoid', '(', 'g_on', '[', ':', ',', ':', 'n_h', ']', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "f_on = T . nnet . sigmoid ( g_on [ : , n_h : 2 * n_h ] ) \n"
Original    (020): ['f_on', '=', 'T', '.', 'nnet', '.', 'sigmoid', '(', 'g_on', '[', ':', ',', 'n_h', ':', '2', '*', 'n_h', ']', ')', '\\n']
Tokenized   (034): ['<s>', 'f', '_', 'on', '=', 'T', '.', 'n', 'net', '.', 's', 'igm', 'oid', '(', 'g', '_', 'on', '[', ':', ',', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '\\', 'n', '</s>']
Filtered   (032): ['f', '_', 'on', '=', 'T', '.', 'n', 'net', '.', 's', 'igm', 'oid', '(', 'g', '_', 'on', '[', ':', ',', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '\\', 'n']
Detokenized (020): ['f_on', '=', 'T', '.', 'nnet', '.', 'sigmoid', '(', 'g_on', '[', ':', ',', 'n_h', ':', '2', '*', 'n_h', ']', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "o_on = T . nnet . sigmoid ( g_on [ : , 2 * n_h : 3 * n_h ] ) \n"
Original    (022): ['o_on', '=', 'T', '.', 'nnet', '.', 'sigmoid', '(', 'g_on', '[', ':', ',', '2', '*', 'n_h', ':', '3', '*', 'n_h', ']', ')', '\\n']
Tokenized   (036): ['<s>', 'o', '_', 'on', '=', 'T', '.', 'n', 'net', '.', 's', 'igm', 'oid', '(', 'g', '_', 'on', '[', ':', ',', '2', '*', 'n', '_', 'h', ':', '3', '*', 'n', '_', 'h', ']', ')', '\\', 'n', '</s>']
Filtered   (034): ['o', '_', 'on', '=', 'T', '.', 'n', 'net', '.', 's', 'igm', 'oid', '(', 'g', '_', 'on', '[', ':', ',', '2', '*', 'n', '_', 'h', ':', '3', '*', 'n', '_', 'h', ']', ')', '\\', 'n']
Detokenized (022): ['o_on', '=', 'T', '.', 'nnet', '.', 'sigmoid', '(', 'g_on', '[', ':', ',', '2', '*', 'n_h', ':', '3', '*', 'n_h', ']', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "h_t = T . tanh ( T . dot ( X , W [ : , 1 * n_h : 2 * n_h ] ) + T . dot ( h , U [ : , 1 * n_h : 2 * n_h ] ) + b [ 1 * n_h : 2 * n_h ] ) \n"
Original    (058): ['h_t', '=', 'T', '.', 'tanh', '(', 'T', '.', 'dot', '(', 'X', ',', 'W', '[', ':', ',', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '+', 'T', '.', 'dot', '(', 'h', ',', 'U', '[', ':', ',', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '+', 'b', '[', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '\\n']
Tokenized   (076): ['<s>', 'h', '_', 't', '=', 'T', '.', 'tan', 'h', '(', 'T', '.', 'dot', '(', 'X', ',', 'W', '[', ':', ',', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '+', 'T', '.', 'dot', '(', 'h', ',', 'U', '[', ':', ',', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '+', 'b', '[', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '\\', 'n', '</s>']
Filtered   (074): ['h', '_', 't', '=', 'T', '.', 'tan', 'h', '(', 'T', '.', 'dot', '(', 'X', ',', 'W', '[', ':', ',', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '+', 'T', '.', 'dot', '(', 'h', ',', 'U', '[', ':', ',', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '+', 'b', '[', '1', '*', 'n', '_', 'h', ':', '2', '*', 'n', '_', 'h', ']', ')', '\\', 'n']
Detokenized (058): ['h_t', '=', 'T', '.', 'tanh', '(', 'T', '.', 'dot', '(', 'X', ',', 'W', '[', ':', ',', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '+', 'T', '.', 'dot', '(', 'h', ',', 'U', '[', ':', ',', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '+', 'b', '[', '1', '*', 'n_h', ':', '2', '*', 'n_h', ']', ')', '\\n']
Counter: 74
===================================================================
Hidden states:  (13, 58, 768)
# Extracted words:  58
Sentence         : "te_cost , te_h_updates = model ( self . X , self . params , 0. ) \n"
Original    (017): ['te_cost', ',', 'te_h_updates', '=', 'model', '(', 'self', '.', 'X', ',', 'self', '.', 'params', ',', '0.', ')', '\\n']
Tokenized   (028): ['<s>', 'te', '_', 'cost', ',', 'te', '_', 'h', '_', 'up', 'dates', '=', 'model', '(', 'self', '.', 'X', ',', 'self', '.', 'params', ',', '0', '.', ')', '\\', 'n', '</s>']
Filtered   (026): ['te', '_', 'cost', ',', 'te', '_', 'h', '_', 'up', 'dates', '=', 'model', '(', 'self', '.', 'X', ',', 'self', '.', 'params', ',', '0', '.', ')', '\\', 'n']
Detokenized (017): ['te_cost', ',', 'te_h_updates', '=', 'model', '(', 'self', '.', 'X', ',', 'self', '.', 'params', ',', '0.', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "csvWriter = csv . writer ( sys . stdout , delimiter = separator , quotechar = quote , \n"
Original    (019): ['csvWriter', '=', 'csv', '.', 'writer', '(', 'sys', '.', 'stdout', ',', 'delimiter', '=', 'separator', ',', 'quotechar', '=', 'quote', ',', '\\n']
Tokenized   (028): ['<s>', 'csv', 'Writer', '=', 'c', 'sv', '.', 'writer', '(', 'sys', '.', 'std', 'out', ',', 'delim', 'iter', '=', 'separ', 'ator', ',', 'quote', 'char', '=', 'quote', ',', '\\', 'n', '</s>']
Filtered   (026): ['csv', 'Writer', '=', 'c', 'sv', '.', 'writer', '(', 'sys', '.', 'std', 'out', ',', 'delim', 'iter', '=', 'separ', 'ator', ',', 'quote', 'char', '=', 'quote', ',', '\\', 'n']
Detokenized (019): ['csvWriter', '=', 'csv', '.', 'writer', '(', 'sys', '.', 'stdout', ',', 'delimiter', '=', 'separator', ',', 'quotechar', '=', 'quote', ',', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "IColumnProvider_Methods = IPersist_Methods + [ "Initialize" , "GetColumnInfo" , "GetItemData" ] \n"
Original    (012): ['IColumnProvider_Methods', '=', 'IPersist_Methods', '+', '[', '"Initialize"', ',', '"GetColumnInfo"', ',', '"GetItemData"', ']', '\\n']
Tokenized   (035): ['<s>', 'IC', 'ol', 'umn', 'Provider', '_', 'Methods', '=', 'IP', 'ers', 'ist', '_', 'Methods', '+', '[', '"', 'Initial', 'ize', '"', ',', '"', 'Get', 'Column', 'Info', '"', ',', '"', 'Get', 'Item', 'Data', '"', ']', '\\', 'n', '</s>']
Filtered   (033): ['IC', 'ol', 'umn', 'Provider', '_', 'Methods', '=', 'IP', 'ers', 'ist', '_', 'Methods', '+', '[', '"', 'Initial', 'ize', '"', ',', '"', 'Get', 'Column', 'Info', '"', ',', '"', 'Get', 'Item', 'Data', '"', ']', '\\', 'n']
Detokenized (012): ['IColumnProvider_Methods', '=', 'IPersist_Methods', '+', '[', '"Initialize"', ',', '"GetColumnInfo"', ',', '"GetItemData"', ']', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "_com_interfaces_ = [ pythoncom . IID_IPersist , \n"
Original    (008): ['_com_interfaces_', '=', '[', 'pythoncom', '.', 'IID_IPersist', ',', '\\n']
Tokenized   (022): ['<s>', '_', 'com', '_', 'inter', 'faces', '_', '=', '[', 'python', 'com', '.', 'I', 'ID', '_', 'IP', 'ers', 'ist', ',', '\\', 'n', '</s>']
Filtered   (020): ['_', 'com', '_', 'inter', 'faces', '_', '=', '[', 'python', 'com', '.', 'I', 'ID', '_', 'IP', 'ers', 'ist', ',', '\\', 'n']
Detokenized (008): ['_com_interfaces_', '=', '[', 'pythoncom', '.', 'IID_IPersist', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "20 , #cChars \n"
Original    (004): ['20', ',', '#cChars', '\\n']
Tokenized   (010): ['<s>', '20', ',', '#', 'c', 'Ch', 'ars', '\\', 'n', '</s>']
Filtered   (008): ['20', ',', '#', 'c', 'Ch', 'ars', '\\', 'n']
Detokenized (004): ['20', ',', '#cChars', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "fmt_id == self . _reg_clsid_ \n"
Original    (006): ['fmt_id', '==', 'self', '.', '_reg_clsid_', '\\n']
Tokenized   (017): ['<s>', 'f', 'mt', '_', 'id', '==', 'self', '.', '_', 'reg', '_', 'cl', 'sid', '_', '\\', 'n', '</s>']
Filtered   (015): ['f', 'mt', '_', 'id', '==', 'self', '.', '_', 'reg', '_', 'cl', 'sid', '_', '\\', 'n']
Detokenized (006): ['fmt_id', '==', 'self', '.', '_reg_clsid_', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : ""Folder\\\\ShellEx\\\\ColumnHandlers\\\\" + str ( ColumnProvider . _reg_clsid_ ) ) \n"
Original    (010): ['"Folder\\\\\\\\ShellEx\\\\\\\\ColumnHandlers\\\\\\\\"', '+', 'str', '(', 'ColumnProvider', '.', '_reg_clsid_', ')', ')', '\\n']
Tokenized   (029): ['<s>', '"', 'Folder', '\\\\\\\\', 'Shell', 'Ex', '\\\\\\\\', 'Column', 'Hand', 'lers', '\\\\\\\\', '"', '+', 'str', '(', 'Column', 'Provider', '.', '_', 'reg', '_', 'cl', 'sid', '_', ')', ')', '\\', 'n', '</s>']
Filtered   (027): ['"', 'Folder', '\\\\\\\\', 'Shell', 'Ex', '\\\\\\\\', 'Column', 'Hand', 'lers', '\\\\\\\\', '"', '+', 'str', '(', 'Column', 'Provider', '.', '_', 'reg', '_', 'cl', 'sid', '_', ')', ')', '\\', 'n']
Detokenized (010): ['"Folder\\\\\\\\ShellEx\\\\\\\\ColumnHandlers\\\\\\\\"', '+', 'str', '(', 'ColumnProvider', '.', '_reg_clsid_', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "_winreg . SetValueEx ( key , None , 0 , _winreg . REG_SZ , ColumnProvider . _reg_desc_ ) \n"
Original    (019): ['_winreg', '.', 'SetValueEx', '(', 'key', ',', 'None', ',', '0', ',', '_winreg', '.', 'REG_SZ', ',', 'ColumnProvider', '.', '_reg_desc_', ')', '\\n']
Tokenized   (036): ['<s>', '_', 'win', 'reg', '.', 'Set', 'Value', 'Ex', '(', 'key', ',', 'None', ',', '0', ',', '_', 'win', 'reg', '.', 'REG', '_', 'S', 'Z', ',', 'Column', 'Provider', '.', '_', 'reg', '_', 'desc', '_', ')', '\\', 'n', '</s>']
Filtered   (034): ['_', 'win', 'reg', '.', 'Set', 'Value', 'Ex', '(', 'key', ',', 'None', ',', '0', ',', '_', 'win', 'reg', '.', 'REG', '_', 'S', 'Z', ',', 'Column', 'Provider', '.', '_', 'reg', '_', 'desc', '_', ')', '\\', 'n']
Detokenized (019): ['_winreg', '.', 'SetValueEx', '(', 'key', ',', 'None', ',', '0', ',', '_winreg', '.', 'REG_SZ', ',', 'ColumnProvider', '.', '_reg_desc_', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "register . UseCommandLine ( ColumnProvider , \n"
Original    (007): ['register', '.', 'UseCommandLine', '(', 'ColumnProvider', ',', '\\n']
Tokenized   (013): ['<s>', 'register', '.', 'Use', 'Command', 'Line', '(', 'Column', 'Provider', ',', '\\', 'n', '</s>']
Filtered   (011): ['register', '.', 'Use', 'Command', 'Line', '(', 'Column', 'Provider', ',', '\\', 'n']
Detokenized (007): ['register', '.', 'UseCommandLine', '(', 'ColumnProvider', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "aliases = MultipleValueField ( required = False ) \n"
Original    (009): ['aliases', '=', 'MultipleValueField', '(', 'required', '=', 'False', ')', '\\n']
Tokenized   (015): ['<s>', 'ali', 'ases', '=', 'Multiple', 'Value', 'Field', '(', 'required', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (013): ['ali', 'ases', '=', 'Multiple', 'Value', 'Field', '(', 'required', '=', 'False', ')', '\\', 'n']
Detokenized (009): ['aliases', '=', 'MultipleValueField', '(', 'required', '=', 'False', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "StoredQueryParameter = namedtuple ( "StoredQueryParameter" , ( , , , , \n"
Original    (012): ['StoredQueryParameter', '=', 'namedtuple', '(', '"StoredQueryParameter"', ',', '(', ',', ',', ',', ',', '\\n']
Tokenized   (025): ['<s>', 'St', 'ored', 'Query', 'Parameter', '=', 'named', 't', 'uple', '(', '"', 'St', 'ored', 'Query', 'Parameter', '"', ',', '(', ',', ',', ',', ',', '\\', 'n', '</s>']
Filtered   (023): ['St', 'ored', 'Query', 'Parameter', '=', 'named', 't', 'uple', '(', '"', 'St', 'ored', 'Query', 'Parameter', '"', ',', '(', ',', ',', ',', ',', '\\', 'n']
Detokenized (012): ['StoredQueryParameter', '=', 'namedtuple', '(', '"StoredQueryParameter"', ',', '(', ',', ',', ',', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "fts = list ( self . models . keys ( ) ) \n"
Original    (013): ['fts', '=', 'list', '(', 'self', '.', 'models', '.', 'keys', '(', ')', ')', '\\n']
Tokenized   (016): ['<s>', 'fts', '=', 'list', '(', 'self', '.', 'models', '.', 'keys', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (014): ['fts', '=', 'list', '(', 'self', '.', 'models', '.', 'keys', '(', ')', ')', '\\', 'n']
Detokenized (013): ['fts', '=', 'list', '(', 'self', '.', 'models', '.', 'keys', '(', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "sort_by = parms . cleaned_data [ ] \n"
Original    (008): ['sort_by', '=', 'parms', '.', 'cleaned_data', '[', ']', '\\n']
Tokenized   (016): ['<s>', 'sort', '_', 'by', '=', 'par', 'ms', '.', 'cleaned', '_', 'data', '[', ']', '\\', 'n', '</s>']
Filtered   (014): ['sort', '_', 'by', '=', 'par', 'ms', '.', 'cleaned', '_', 'data', '[', ']', '\\', 'n']
Detokenized (008): ['sort_by', '=', 'parms', '.', 'cleaned_data', '[', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "geometry_field = self . geometries [ type_names [ 0 ] ] \n"
Original    (012): ['geometry_field', '=', 'self', '.', 'geometries', '[', 'type_names', '[', '0', ']', ']', '\\n']
Tokenized   (022): ['<s>', 'ge', 'ometry', '_', 'field', '=', 'self', '.', 'ge', 'omet', 'ries', '[', 'type', '_', 'names', '[', '0', ']', ']', '\\', 'n', '</s>']
Filtered   (020): ['ge', 'ometry', '_', 'field', '=', 'self', '.', 'ge', 'omet', 'ries', '[', 'type', '_', 'names', '[', '0', ']', ']', '\\', 'n']
Detokenized (012): ['geometry_field', '=', 'self', '.', 'geometries', '[', 'type_names', '[', '0', ']', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "mxy = mxy ) \n"
Original    (005): ['mxy', '=', 'mxy', ')', '\\n']
Tokenized   (010): ['<s>', 'm', 'xy', '=', 'm', 'xy', ')', '\\', 'n', '</s>']
Filtered   (008): ['m', 'xy', '=', 'm', 'xy', ')', '\\', 'n']
Detokenized (005): ['mxy', '=', 'mxy', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "query_set = query_set . order_by ( * sort_by ) \n"
Original    (010): ['query_set', '=', 'query_set', '.', 'order_by', '(', '*', 'sort_by', ')', '\\n']
Tokenized   (021): ['<s>', 'query', '_', 'set', '=', 'query', '_', 'set', '.', 'order', '_', 'by', '(', '*', 'sort', '_', 'by', ')', '\\', 'n', '</s>']
Filtered   (019): ['query', '_', 'set', '=', 'query', '_', 'set', '.', 'order', '_', 'by', '(', '*', 'sort', '_', 'by', ')', '\\', 'n']
Detokenized (010): ['query_set', '=', 'query_set', '.', 'order_by', '(', '*', 'sort_by', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "output_format = root . get ( , ) \n"
Original    (009): ['output_format', '=', 'root', '.', 'get', '(', ',', ')', '\\n']
Tokenized   (014): ['<s>', 'output', '_', 'format', '=', 'root', '.', 'get', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (012): ['output', '_', 'format', '=', 'root', '.', 'get', '(', ',', ')', '\\', 'n']
Detokenized (009): ['output_format', '=', 'root', '.', 'get', '(', ',', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "type_names . append ( ( namespace , name ) ) \n"
Original    (011): ['type_names', '.', 'append', '(', '(', 'namespace', ',', 'name', ')', ')', '\\n']
Tokenized   (016): ['<s>', 'type', '_', 'names', '.', 'append', '(', '(', 'namespace', ',', 'name', ')', ')', '\\', 'n', '</s>']
Filtered   (014): ['type', '_', 'names', '.', 'append', '(', '(', 'namespace', ',', 'name', ')', ')', '\\', 'n']
Detokenized (011): ['type_names', '.', 'append', '(', '(', 'namespace', ',', 'name', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""schema" : feature_type . schema , \n"
Original    (007): ['"schema"', ':', 'feature_type', '.', 'schema', ',', '\\n']
Tokenized   (015): ['<s>', '"', 'sche', 'ma', '"', ':', 'feature', '_', 'type', '.', 'schema', ',', '\\', 'n', '</s>']
Filtered   (013): ['"', 'sche', 'ma', '"', ':', 'feature', '_', 'type', '.', 'schema', ',', '\\', 'n']
Detokenized (007): ['"schema"', ':', 'feature_type', '.', 'schema', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""ns_name" : feature_type . ns_name \n"
Original    (006): ['"ns_name"', ':', 'feature_type', '.', 'ns_name', '\\n']
Tokenized   (017): ['<s>', '"', 'ns', '_', 'name', '"', ':', 'feature', '_', 'type', '.', 'ns', '_', 'name', '\\', 'n', '</s>']
Filtered   (015): ['"', 'ns', '_', 'name', '"', ':', 'feature', '_', 'type', '.', 'ns', '_', 'name', '\\', 'n']
Detokenized (006): ['"ns_name"', ':', 'feature_type', '.', 'ns_name', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "db_params = settings . DATABASES [ response . db ] \n"
Original    (011): ['db_params', '=', 'settings', '.', 'DATABASES', '[', 'response', '.', 'db', ']', '\\n']
Tokenized   (020): ['<s>', 'db', '_', 'params', '=', 'settings', '.', 'D', 'AT', 'AB', 'AS', 'ES', '[', 'response', '.', 'db', ']', '\\', 'n', '</s>']
Filtered   (018): ['db', '_', 'params', '=', 'settings', '.', 'D', 'AT', 'AB', 'AS', 'ES', '[', 'response', '.', 'db', ']', '\\', 'n']
Detokenized (011): ['db_params', '=', 'settings', '.', 'DATABASES', '[', 'response', '.', 'db', ']', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "parameters = tuple ( [ adapt ( p ) for p in parameters ] ) \n"
Original    (016): ['parameters', '=', 'tuple', '(', '[', 'adapt', '(', 'p', ')', 'for', 'p', 'in', 'parameters', ']', ')', '\\n']
Tokenized   (020): ['<s>', 'param', 'eters', '=', 'tuple', '(', '[', 'adapt', '(', 'p', ')', 'for', 'p', 'in', 'parameters', ']', ')', '\\', 'n', '</s>']
Filtered   (018): ['param', 'eters', '=', 'tuple', '(', '[', 'adapt', '(', 'p', ')', 'for', 'p', 'in', 'parameters', ']', ')', '\\', 'n']
Detokenized (016): ['parameters', '=', 'tuple', '(', '[', 'adapt', '(', 'p', ')', 'for', 'p', 'in', 'parameters', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "connection_string = "PG:dbname=\'{db}\'" . format ( db = db_params [ ] ) \n"
Original    (013): ['connection_string', '=', '"PG:dbname=\\\'{db}\\\'"', '.', 'format', '(', 'db', '=', 'db_params', '[', ']', ')', '\\n']
Tokenized   (030): ['<s>', 'connection', '_', 'string', '=', '"', 'PG', ':', 'db', 'name', '=', "\\'", '{', 'db', '}\\', '\'"', '.', 'format', '(', 'db', '=', 'db', '_', 'params', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (028): ['connection', '_', 'string', '=', '"', 'PG', ':', 'db', 'name', '=', "\\'", '{', 'db', '}\\', '\'"', '.', 'format', '(', 'db', '=', 'db', '_', 'params', '[', ']', ')', '\\', 'n']
Detokenized (013): ['connection_string', '=', '"PG:dbname=\\\'{db}\\\'"', '.', 'format', '(', 'db', '=', 'db_params', '[', ']', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "etree . SubElement ( p , ) . text = parameter . abstractS \n"
Original    (014): ['etree', '.', 'SubElement', '(', 'p', ',', ')', '.', 'text', '=', 'parameter', '.', 'abstractS', '\\n']
Tokenized   (020): ['<s>', 'et', 'ree', '.', 'Sub', 'Element', '(', 'p', ',', ')', '.', 'text', '=', 'parameter', '.', 'abstract', 'S', '\\', 'n', '</s>']
Filtered   (018): ['et', 'ree', '.', 'Sub', 'Element', '(', 'p', ',', ')', '.', 'text', '=', 'parameter', '.', 'abstract', 'S', '\\', 'n']
Detokenized (014): ['etree', '.', 'SubElement', '(', 'p', ',', ')', '.', 'text', '=', 'parameter', '.', 'abstractS', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : ""isPrivate" : parameter . query_expression . private == True , \n"
Original    (011): ['"isPrivate"', ':', 'parameter', '.', 'query_expression', '.', 'private', '==', 'True', ',', '\\n']
Tokenized   (019): ['<s>', '"', 'is', 'Private', '"', ':', 'parameter', '.', 'query', '_', 'expression', '.', 'private', '==', 'True', ',', '\\', 'n', '</s>']
Filtered   (017): ['"', 'is', 'Private', '"', ':', 'parameter', '.', 'query', '_', 'expression', '.', 'private', '==', 'True', ',', '\\', 'n']
Detokenized (011): ['"isPrivate"', ':', 'parameter', '.', 'query_expression', '.', 'private', '==', 'True', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""language" : parameter . query_expression . language , \n"
Original    (009): ['"language"', ':', 'parameter', '.', 'query_expression', '.', 'language', ',', '\\n']
Tokenized   (016): ['<s>', '"', 'language', '"', ':', 'parameter', '.', 'query', '_', 'expression', '.', 'language', ',', '\\', 'n', '</s>']
Filtered   (014): ['"', 'language', '"', ':', 'parameter', '.', 'query', '_', 'expression', '.', 'language', ',', '\\', 'n']
Detokenized (009): ['"language"', ':', 'parameter', '.', 'query_expression', '.', 'language', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ""returnFeatureTypes" : . join ( parameter . query_expression . return_feature_types } ) . text = parameter . query_expression . text \n"
Original    (021): ['"returnFeatureTypes"', ':', '.', 'join', '(', 'parameter', '.', 'query_expression', '.', 'return_feature_types', '}', ')', '.', 'text', '=', 'parameter', '.', 'query_expression', '.', 'text', '\\n']
Tokenized   (036): ['<s>', '"', 'return', 'Feature', 'Types', '"', ':', '.', 'join', '(', 'parameter', '.', 'query', '_', 'expression', '.', 'return', '_', 'feature', '_', 'types', '}', ')', '.', 'text', '=', 'parameter', '.', 'query', '_', 'expression', '.', 'text', '\\', 'n', '</s>']
Filtered   (034): ['"', 'return', 'Feature', 'Types', '"', ':', '.', 'join', '(', 'parameter', '.', 'query', '_', 'expression', '.', 'return', '_', 'feature', '_', 'types', '}', ')', '.', 'text', '=', 'parameter', '.', 'query', '_', 'expression', '.', 'text', '\\', 'n']
Detokenized (021): ['"returnFeatureTypes"', ':', '.', 'join', '(', 'parameter', '.', 'query_expression', '.', 'return_feature_types', '}', ')', '.', 'text', '=', 'parameter', '.', 'query_expression', '.', 'text', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : ""endpoint" : request . build_absolute_uri ( ) . split ( ) [ 0 ] , \n"
Original    (016): ['"endpoint"', ':', 'request', '.', 'build_absolute_uri', '(', ')', '.', 'split', '(', ')', '[', '0', ']', ',', '\\n']
Tokenized   (026): ['<s>', '"', 'end', 'point', '"', ':', 'request', '.', 'build', '_', 'absolute', '_', 'uri', '(', ')', '.', 'split', '(', ')', '[', '0', ']', ',', '\\', 'n', '</s>']
Filtered   (024): ['"', 'end', 'point', '"', ':', 'request', '.', 'build', '_', 'absolute', '_', 'uri', '(', ')', '.', 'split', '(', ')', '[', '0', ']', ',', '\\', 'n']
Detokenized (016): ['"endpoint"', ':', 'request', '.', 'build_absolute_uri', '(', ')', '.', 'split', '(', ')', '[', '0', ']', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : ""output_formats" : [ ogr . GetDriver ( drv ) . GetName ( ) for drv in range ( ogr . GetDriverCount ( ) ) "addr_street" : self . addr_street , \n"
Original    (031): ['"output_formats"', ':', '[', 'ogr', '.', 'GetDriver', '(', 'drv', ')', '.', 'GetName', '(', ')', 'for', 'drv', 'in', 'range', '(', 'ogr', '.', 'GetDriverCount', '(', ')', ')', '"addr_street"', ':', 'self', '.', 'addr_street', ',', '\\n']
Tokenized   (053): ['<s>', '"', 'output', '_', 'form', 'ats', '"', ':', '[', 'o', 'gr', '.', 'Get', 'Driver', '(', 'dr', 'v', ')', '.', 'Get', 'Name', '(', ')', 'for', 'dr', 'v', 'in', 'range', '(', 'o', 'gr', '.', 'Get', 'Driver', 'Count', '(', ')', ')', '"', 'addr', '_', 'street', '"', ':', 'self', '.', 'addr', '_', 'street', ',', '\\', 'n', '</s>']
Filtered   (051): ['"', 'output', '_', 'form', 'ats', '"', ':', '[', 'o', 'gr', '.', 'Get', 'Driver', '(', 'dr', 'v', ')', '.', 'Get', 'Name', '(', ')', 'for', 'dr', 'v', 'in', 'range', '(', 'o', 'gr', '.', 'Get', 'Driver', 'Count', '(', ')', ')', '"', 'addr', '_', 'street', '"', ':', 'self', '.', 'addr', '_', 'street', ',', '\\', 'n']
Detokenized (031): ['"output_formats"', ':', '[', 'ogr', '.', 'GetDriver', '(', 'drv', ')', '.', 'GetName', '(', ')', 'for', 'drv', 'in', 'range', '(', 'ogr', '.', 'GetDriverCount', '(', ')', ')', '"addr_street"', ':', 'self', '.', 'addr_street', ',', '\\n']
Counter: 51
===================================================================
Hidden states:  (13, 31, 768)
# Extracted words:  31
Sentence         : ""feature_versioning" : self . adapter . supports_feature_versioning ( ) , \n"
Original    (011): ['"feature_versioning"', ':', 'self', '.', 'adapter', '.', 'supports_feature_versioning', '(', ')', ',', '\\n']
Tokenized   (024): ['<s>', '"', 'feature', '_', 'version', 'ing', '"', ':', 'self', '.', 'adapter', '.', 'supports', '_', 'feature', '_', 'version', 'ing', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (022): ['"', 'feature', '_', 'version', 'ing', '"', ':', 'self', '.', 'adapter', '.', 'supports', '_', 'feature', '_', 'version', 'ing', '(', ')', ',', '\\', 'n']
Detokenized (011): ['"feature_versioning"', ':', 'self', '.', 'adapter', '.', 'supports_feature_versioning', '(', ')', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""date" : datetime . now ( ) , \n"
Original    (009): ['"date"', ':', 'datetime', '.', 'now', '(', ')', ',', '\\n']
Tokenized   (015): ['<s>', '"', 'date', '"', ':', 'dat', 'etime', '.', 'now', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (013): ['"', 'date', '"', ':', 'dat', 'etime', '.', 'now', '(', ')', ',', '\\', 'n']
Detokenized (009): ['"date"', ':', 'datetime', '.', 'now', '(', ')', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "matchItem . setText ( 3 , unicode ( self . data [ "Matches" ] ) ) \n"
Original    (017): ['matchItem', '.', 'setText', '(', '3', ',', 'unicode', '(', 'self', '.', 'data', '[', '"Matches"', ']', ')', ')', '\\n']
Tokenized   (026): ['<s>', 'match', 'Item', '.', 'set', 'Text', '(', '3', ',', 'unic', 'ode', '(', 'self', '.', 'data', '[', '"', 'Mat', 'ches', '"', ']', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['match', 'Item', '.', 'set', 'Text', '(', '3', ',', 'unic', 'ode', '(', 'self', '.', 'data', '[', '"', 'Mat', 'ches', '"', ']', ')', ')', '\\', 'n']
Detokenized (017): ['matchItem', '.', 'setText', '(', '3', ',', 'unicode', '(', 'self', '.', 'data', '[', '"Matches"', ']', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "roundItem . setText ( 3 , unicode ( opponent [ 2 ] ) ) \n"
Original    (015): ['roundItem', '.', 'setText', '(', '3', ',', 'unicode', '(', 'opponent', '[', '2', ']', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'round', 'Item', '.', 'set', 'Text', '(', '3', ',', 'unic', 'ode', '(', 'opponent', '[', '2', ']', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['round', 'Item', '.', 'set', 'Text', '(', '3', ',', 'unic', 'ode', '(', 'opponent', '[', '2', ']', ')', ')', '\\', 'n']
Detokenized (015): ['roundItem', '.', 'setText', '(', '3', ',', 'unicode', '(', 'opponent', '[', '2', ']', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "opponent [ 3 ] = roundItem \n"
Original    (007): ['opponent', '[', '3', ']', '=', 'roundItem', '\\n']
Tokenized   (012): ['<s>', 'opp', 'onent', '[', '3', ']', '=', 'round', 'Item', '\\', 'n', '</s>']
Filtered   (010): ['opp', 'onent', '[', '3', ']', '=', 'round', 'Item', '\\', 'n']
Detokenized (007): ['opponent', '[', '3', ']', '=', 'roundItem', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "M = np . matrix ( [ [ 2 , 3 , 4 ] , \n"
Original    (016): ['M', '=', 'np', '.', 'matrix', '(', '[', '[', '2', ',', '3', ',', '4', ']', ',', '\\n']
Tokenized   (019): ['<s>', 'M', '=', 'np', '.', 'matrix', '(', '[', '[', '2', ',', '3', ',', '4', ']', ',', '\\', 'n', '</s>']
Filtered   (017): ['M', '=', 'np', '.', 'matrix', '(', '[', '[', '2', ',', '3', ',', '4', ']', ',', '\\', 'n']
Detokenized (016): ['M', '=', 'np', '.', 'matrix', '(', '[', '[', '2', ',', '3', ',', '4', ']', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "matrix = Matrix ( M , mtype = ) \n"
Original    (010): ['matrix', '=', 'Matrix', '(', 'M', ',', 'mtype', '=', ')', '\\n']
Tokenized   (015): ['<s>', 'mat', 'rix', '=', 'Matrix', '(', 'M', ',', 'm', 'type', '=', ')', '\\', 'n', '</s>']
Filtered   (013): ['mat', 'rix', '=', 'Matrix', '(', 'M', ',', 'm', 'type', '=', ')', '\\', 'n']
Detokenized (010): ['matrix', '=', 'Matrix', '(', 'M', ',', 'mtype', '=', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "math = Math ( data = [ , vec_name , , Matrix ( M * a ) ] ) \n"
Original    (020): ['math', '=', 'Math', '(', 'data', '=', '[', ',', 'vec_name', ',', ',', 'Matrix', '(', 'M', '*', 'a', ')', ']', ')', '\\n']
Tokenized   (025): ['<s>', 'math', '=', 'Math', '(', 'data', '=', '[', ',', 'vec', '_', 'name', ',', ',', 'Matrix', '(', 'M', '*', 'a', ')', ']', ')', '\\', 'n', '</s>']
Filtered   (023): ['math', '=', 'Math', '(', 'data', '=', '[', ',', 'vec', '_', 'name', ',', ',', 'Matrix', '(', 'M', '*', 'a', ')', ']', ')', '\\', 'n']
Detokenized (020): ['math', '=', 'Math', '(', 'data', '=', '[', ',', 'vec_name', ',', ',', 'Matrix', '(', 'M', '*', 'a', ')', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "q2 = Quantity ( v , format_cb = lambda x : str ( int ( x ) ) ) \n"
Original    (020): ['q2', '=', 'Quantity', '(', 'v', ',', 'format_cb', '=', 'lambda', 'x', ':', 'str', '(', 'int', '(', 'x', ')', ')', ')', '\\n']
Tokenized   (026): ['<s>', 'q', '2', '=', 'Quantity', '(', 'v', ',', 'format', '_', 'cb', '=', 'lambda', 'x', ':', 'str', '(', 'int', '(', 'x', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['q', '2', '=', 'Quantity', '(', 'v', ',', 'format', '_', 'cb', '=', 'lambda', 'x', ':', 'str', '(', 'int', '(', 'x', ')', ')', ')', '\\', 'n']
Detokenized (020): ['q2', '=', 'Quantity', '(', 'v', ',', 'format_cb', '=', 'lambda', 'x', ':', 'str', '(', 'int', '(', 'x', ')', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "q3 = Quantity ( v , options = { : } ) \n"
Original    (013): ['q3', '=', 'Quantity', '(', 'v', ',', 'options', '=', '{', ':', '}', ')', '\\n']
Tokenized   (017): ['<s>', 'q', '3', '=', 'Quantity', '(', 'v', ',', 'options', '=', '{', ':', '}', ')', '\\', 'n', '</s>']
Filtered   (015): ['q', '3', '=', 'Quantity', '(', 'v', ',', 'options', '=', '{', ':', '}', ')', '\\', 'n']
Detokenized (013): ['q3', '=', 'Quantity', '(', 'v', ',', 'options', '=', '{', ':', '}', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "test_dimensionality_to_siunitx ( ) \n"
Original    (004): ['test_dimensionality_to_siunitx', '(', ')', '\\n']
Tokenized   (016): ['<s>', 'test', '_', 'dimension', 'ality', '_', 'to', '_', 'si', 'unit', 'x', '(', ')', '\\', 'n', '</s>']
Filtered   (014): ['test', '_', 'dimension', 'ality', '_', 'to', '_', 'si', 'unit', 'x', '(', ')', '\\', 'n']
Detokenized (004): ['test_dimensionality_to_siunitx', '(', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "ph = put_handler . put_handler ( fs , ) \n"
Original    (010): ['ph', '=', 'put_handler', '.', 'put_handler', '(', 'fs', ',', ')', '\\n']
Tokenized   (017): ['<s>', 'ph', '=', 'put', '_', 'handler', '.', 'put', '_', 'handler', '(', 'fs', ',', ')', '\\', 'n', '</s>']
Filtered   (015): ['ph', '=', 'put', '_', 'handler', '.', 'put', '_', 'handler', '(', 'fs', ',', ')', '\\', 'n']
Detokenized (010): ['ph', '=', 'put_handler', '.', 'put_handler', '(', 'fs', ',', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "hs = http_server . http_server ( ip = , port = 8080 ) \n"
Original    (014): ['hs', '=', 'http_server', '.', 'http_server', '(', 'ip', '=', ',', 'port', '=', '8080', ')', '\\n']
Tokenized   (022): ['<s>', 'hs', '=', 'http', '_', 'server', '.', 'http', '_', 'server', '(', 'ip', '=', ',', 'port', '=', '80', '80', ')', '\\', 'n', '</s>']
Filtered   (020): ['hs', '=', 'http', '_', 'server', '.', 'http', '_', 'server', '(', 'ip', '=', ',', 'port', '=', '80', '80', ')', '\\', 'n']
Detokenized (014): ['hs', '=', 'http_server', '.', 'http_server', '(', 'ip', '=', ',', 'port', '=', '8080', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "num_trans = num_requests * num_conns \n"
Original    (006): ['num_trans', '=', 'num_requests', '*', 'num_conns', '\\n']
Tokenized   (017): ['<s>', 'num', '_', 'trans', '=', 'num', '_', 'requ', 'ests', '*', 'num', '_', 'con', 'ns', '\\', 'n', '</s>']
Filtered   (015): ['num', '_', 'trans', '=', 'num', '_', 'requ', 'ests', '*', 'num', '_', 'con', 'ns', '\\', 'n']
Detokenized (006): ['num_trans', '=', 'num_requests', '*', 'num_conns', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "trans_per_sec = num_trans / total_time \n"
Original    (006): ['trans_per_sec', '=', 'num_trans', '/', 'total_time', '\\n']
Tokenized   (017): ['<s>', 'trans', '_', 'per', '_', 'sec', '=', 'num', '_', 'trans', '/', 'total', '_', 'time', '\\', 'n', '</s>']
Filtered   (015): ['trans', '_', 'per', '_', 'sec', '=', 'num', '_', 'trans', '/', 'total', '_', 'time', '\\', 'n']
Detokenized (006): ['trans_per_sec', '=', 'num_trans', '/', 'total_time', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "map ( str , ( num_conns , num_requests , request_size , throughput , trans_per_sec ) \n"
Original    (016): ['map', '(', 'str', ',', '(', 'num_conns', ',', 'num_requests', ',', 'request_size', ',', 'throughput', ',', 'trans_per_sec', ')', '\\n']
Tokenized   (031): ['<s>', 'map', '(', 'str', ',', '(', 'num', '_', 'con', 'ns', ',', 'num', '_', 'requ', 'ests', ',', 'request', '_', 'size', ',', 'throughput', ',', 'trans', '_', 'per', '_', 'sec', ')', '\\', 'n', '</s>']
Filtered   (029): ['map', '(', 'str', ',', '(', 'num', '_', 'con', 'ns', ',', 'num', '_', 'requ', 'ests', ',', 'request', '_', 'size', ',', 'throughput', ',', 'trans', '_', 'per', '_', 'sec', ')', '\\', 'n']
Detokenized (016): ['map', '(', 'str', ',', '(', 'num_conns', ',', 'num_requests', ',', 'request_size', ',', 'throughput', ',', 'trans_per_sec', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "queue . add_task ( task , 3 ) \n"
Original    (009): ['queue', '.', 'add_task', '(', 'task', ',', '3', ')', '\\n']
Tokenized   (014): ['<s>', 'queue', '.', 'add', '_', 'task', '(', 'task', ',', '3', ')', '\\', 'n', '</s>']
Filtered   (012): ['queue', '.', 'add', '_', 'task', '(', 'task', ',', '3', ')', '\\', 'n']
Detokenized (009): ['queue', '.', 'add_task', '(', 'task', ',', '3', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "futures . append ( queue . yield_task ( task , 3 ) ) \n"
Original    (014): ['futures', '.', 'append', '(', 'queue', '.', 'yield_task', '(', 'task', ',', '3', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'f', 'ut', 'ures', '.', 'append', '(', 'queue', '.', 'yield', '_', 'task', '(', 'task', ',', '3', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['f', 'ut', 'ures', '.', 'append', '(', 'queue', '.', 'yield', '_', 'task', '(', 'task', ',', '3', ')', ')', '\\', 'n']
Detokenized (014): ['futures', '.', 'append', '(', 'queue', '.', 'yield_task', '(', 'task', ',', '3', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "task_results [ : ] = res \n"
Original    (007): ['task_results', '[', ':', ']', '=', 'res', '\\n']
Tokenized   (012): ['<s>', 'task', '_', 'results', '[', ':', ']', '=', 'res', '\\', 'n', '</s>']
Filtered   (010): ['task', '_', 'results', '[', ':', ']', '=', 'res', '\\', 'n']
Detokenized (007): ['task_results', '[', ':', ']', '=', 'res', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "shuffle ( self . __queued_servers ) \n"
Original    (007): ['shuffle', '(', 'self', '.', '__queued_servers', ')', '\\n']
Tokenized   (016): ['<s>', 'sh', 'uffle', '(', 'self', '.', '__', 'que', 'ued', '_', 'ser', 'vers', ')', '\\', 'n', '</s>']
Filtered   (014): ['sh', 'uffle', '(', 'self', '.', '__', 'que', 'ued', '_', 'ser', 'vers', ')', '\\', 'n']
Detokenized (007): ['shuffle', '(', 'self', '.', '__queued_servers', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "event_name = event [ ] \n"
Original    (006): ['event_name', '=', 'event', '[', ']', '\\n']
Tokenized   (011): ['<s>', 'event', '_', 'name', '=', 'event', '[', ']', '\\', 'n', '</s>']
Filtered   (009): ['event', '_', 'name', '=', 'event', '[', ']', '\\', 'n']
Detokenized (006): ['event_name', '=', 'event', '[', ']', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "event_data = zlib . compress ( pickle . dumps ( event ) ) \n"
Original    (014): ['event_data', '=', 'zlib', '.', 'compress', '(', 'pickle', '.', 'dumps', '(', 'event', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'event', '_', 'data', '=', 'z', 'lib', '.', 'compress', '(', 'pick', 'le', '.', 'dumps', '(', 'event', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['event', '_', 'data', '=', 'z', 'lib', '.', 'compress', '(', 'pick', 'le', '.', 'dumps', '(', 'event', ')', ')', '\\', 'n']
Detokenized (014): ['event_data', '=', 'zlib', '.', 'compress', '(', 'pickle', '.', 'dumps', '(', 'event', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "path_only , query = self . _split_path ( path ) \n"
Original    (011): ['path_only', ',', 'query', '=', 'self', '.', '_split_path', '(', 'path', ')', '\\n']
Tokenized   (019): ['<s>', 'path', '_', 'only', ',', 'query', '=', 'self', '.', '_', 'split', '_', 'path', '(', 'path', ')', '\\', 'n', '</s>']
Filtered   (017): ['path', '_', 'only', ',', 'query', '=', 'self', '.', '_', 'split', '_', 'path', '(', 'path', ')', '\\', 'n']
Detokenized (011): ['path_only', ',', 'query', '=', 'self', '.', '_split_path', '(', 'path', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "break ; \n"
Original    (003): ['break', ';', '\\n']
Tokenized   (006): ['<s>', 'break', ';', '\\', 'n', '</s>']
Filtered   (004): ['break', ';', '\\', 'n']
Detokenized (003): ['break', ';', '\\n']
Counter: 4
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "u . email = user [ 2 ] \n"
Original    (009): ['u', '.', 'email', '=', 'user', '[', '2', ']', '\\n']
Tokenized   (012): ['<s>', 'u', '.', 'email', '=', 'user', '[', '2', ']', '\\', 'n', '</s>']
Filtered   (010): ['u', '.', 'email', '=', 'user', '[', '2', ']', '\\', 'n']
Detokenized (009): ['u', '.', 'email', '=', 'user', '[', '2', ']', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "trac_components = list ( [ ] ) \n"
Original    (008): ['trac_components', '=', 'list', '(', '[', ']', ')', '\\n']
Tokenized   (015): ['<s>', 'tr', 'ac', '_', 'comp', 'onents', '=', 'list', '(', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (013): ['tr', 'ac', '_', 'comp', 'onents', '=', 'list', '(', '[', ']', ')', '\\', 'n']
Detokenized (008): ['trac_components', '=', 'list', '(', '[', ']', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "component . owner = self . _get_user_login ( component . owner ) \n"
Original    (013): ['component', '.', 'owner', '=', 'self', '.', '_get_user_login', '(', 'component', '.', 'owner', ')', '\\n']
Tokenized   (021): ['<s>', 'component', '.', 'owner', '=', 'self', '.', '_', 'get', '_', 'user', '_', 'login', '(', 'component', '.', 'owner', ')', '\\', 'n', '</s>']
Filtered   (019): ['component', '.', 'owner', '=', 'self', '.', '_', 'get', '_', 'user', '_', 'login', '(', 'component', '.', 'owner', ')', '\\', 'n']
Detokenized (013): ['component', '.', 'owner', '=', 'self', '.', '_get_user_login', '(', 'component', '.', 'owner', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "networks [ pkt . pduSource ] . append ( pkt . wirtnNetwork ) \n"
Original    (014): ['networks', '[', 'pkt', '.', 'pduSource', ']', '.', 'append', '(', 'pkt', '.', 'wirtnNetwork', ')', '\\n']
Tokenized   (025): ['<s>', 'net', 'works', '[', 'p', 'kt', '.', 'p', 'du', 'Source', ']', '.', 'append', '(', 'p', 'kt', '.', 'w', 'irt', 'n', 'Network', ')', '\\', 'n', '</s>']
Filtered   (023): ['net', 'works', '[', 'p', 'kt', '.', 'p', 'du', 'Source', ']', '.', 'append', '(', 'p', 'kt', '.', 'w', 'irt', 'n', 'Network', ')', '\\', 'n']
Detokenized (014): ['networks', '[', 'pkt', '.', 'pduSource', ']', '.', 'append', '(', 'pkt', '.', 'wirtnNetwork', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "filterSource = Address ( sys . argv [ i + 1 ] ) \n"
Original    (014): ['filterSource', '=', 'Address', '(', 'sys', '.', 'argv', '[', 'i', '+', '1', ']', ')', '\\n']
Tokenized   (019): ['<s>', 'filter', 'Source', '=', 'Address', '(', 'sys', '.', 'arg', 'v', '[', 'i', '+', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (017): ['filter', 'Source', '=', 'Address', '(', 'sys', '.', 'arg', 'v', '[', 'i', '+', '1', ']', ')', '\\', 'n']
Detokenized (014): ['filterSource', '=', 'Address', '(', 'sys', '.', 'argv', '[', 'i', '+', '1', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "net_count . sort ( lambda x , y : cmp ( y [ 1 ] , x [ 1 ] ) ) \n"
Original    (023): ['net_count', '.', 'sort', '(', 'lambda', 'x', ',', 'y', ':', 'cmp', '(', 'y', '[', '1', ']', ',', 'x', '[', '1', ']', ')', ')', '\\n']
Tokenized   (029): ['<s>', 'net', '_', 'count', '.', 'sort', '(', 'lambda', 'x', ',', 'y', ':', 'c', 'mp', '(', 'y', '[', '1', ']', ',', 'x', '[', '1', ']', ')', ')', '\\', 'n', '</s>']
Filtered   (027): ['net', '_', 'count', '.', 'sort', '(', 'lambda', 'x', ',', 'y', ':', 'c', 'mp', '(', 'y', '[', '1', ']', ',', 'x', '[', '1', ']', ')', ')', '\\', 'n']
Detokenized (023): ['net_count', '.', 'sort', '(', 'lambda', 'x', ',', 'y', ':', 'cmp', '(', 'y', '[', '1', ']', ',', 'x', '[', '1', ']', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "strm = StringIO ( self . pickleBuffer ) \n"
Original    (009): ['strm', '=', 'StringIO', '(', 'self', '.', 'pickleBuffer', ')', '\\n']
Tokenized   (016): ['<s>', 'str', 'm', '=', 'String', 'IO', '(', 'self', '.', 'pick', 'le', 'Buffer', ')', '\\', 'n', '</s>']
Filtered   (014): ['str', 'm', '=', 'String', 'IO', '(', 'self', '.', 'pick', 'le', 'Buffer', ')', '\\', 'n']
Detokenized (009): ['strm', '=', 'StringIO', '(', 'self', '.', 'pickleBuffer', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "pdu . pduSource = self . peer \n"
Original    (008): ['pdu', '.', 'pduSource', '=', 'self', '.', 'peer', '\\n']
Tokenized   (014): ['<s>', 'p', 'du', '.', 'p', 'du', 'Source', '=', 'self', '.', 'peer', '\\', 'n', '</s>']
Filtered   (012): ['p', 'du', '.', 'p', 'du', 'Source', '=', 'self', '.', 'peer', '\\', 'n']
Detokenized (008): ['pdu', '.', 'pduSource', '=', 'self', '.', 'peer', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "connect_task . install_task ( _time ( ) + self . reconnect [ actor . peer ] ) \n"
Original    (018): ['connect_task', '.', 'install_task', '(', '_time', '(', ')', '+', 'self', '.', 'reconnect', '[', 'actor', '.', 'peer', ']', ')', '\\n']
Tokenized   (026): ['<s>', 'connect', '_', 'task', '.', 'install', '_', 'task', '(', '_', 'time', '(', ')', '+', 'self', '.', 'reconnect', '[', 'actor', '.', 'peer', ']', ')', '\\', 'n', '</s>']
Filtered   (024): ['connect', '_', 'task', '.', 'install', '_', 'task', '(', '_', 'time', '(', ')', '+', 'self', '.', 'reconnect', '[', 'actor', '.', 'peer', ']', ')', '\\', 'n']
Detokenized (018): ['connect_task', '.', 'install_task', '(', '_time', '(', ')', '+', 'self', '.', 'reconnect', '[', 'actor', '.', 'peer', ']', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "asyncore . dispatcher . __init__ ( self , sock ) \n"
Original    (011): ['asyncore', '.', 'dispatcher', '.', '__init__', '(', 'self', ',', 'sock', ')', '\\n']
Tokenized   (018): ['<s>', 'as', 'yn', 'core', '.', 'dispatcher', '.', '__', 'init', '__', '(', 'self', ',', 'sock', ')', '\\', 'n', '</s>']
Filtered   (016): ['as', 'yn', 'core', '.', 'dispatcher', '.', '__', 'init', '__', '(', 'self', ',', 'sock', ')', '\\', 'n']
Detokenized (011): ['asyncore', '.', 'dispatcher', '.', '__init__', '(', 'self', ',', 'sock', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "TCPServerDirector . _warning ( , err ) \n"
Original    (008): ['TCPServerDirector', '.', '_warning', '(', ',', 'err', ')', '\\n']
Tokenized   (015): ['<s>', 'TC', 'PS', 'erver', 'Director', '.', '_', 'warning', '(', ',', 'err', ')', '\\', 'n', '</s>']
Filtered   (013): ['TC', 'PS', 'erver', 'Director', '.', '_', 'warning', '(', ',', 'err', ')', '\\', 'n']
Detokenized (008): ['TCPServerDirector', '.', '_warning', '(', ',', 'err', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "buff = packet [ 1 ] \n"
Original    (007): ['buff', '=', 'packet', '[', '1', ']', '\\n']
Tokenized   (010): ['<s>', 'buff', '=', 'packet', '[', '1', ']', '\\', 'n', '</s>']
Filtered   (008): ['buff', '=', 'packet', '[', '1', ']', '\\', 'n']
Detokenized (007): ['buff', '=', 'packet', '[', '1', ']', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "fileIdentifier = ( obj_type , obj_inst ) , \n"
Original    (009): ['fileIdentifier', '=', '(', 'obj_type', ',', 'obj_inst', ')', ',', '\\n']
Tokenized   (018): ['<s>', 'file', 'Ident', 'ifier', '=', '(', 'obj', '_', 'type', ',', 'obj', '_', 'inst', ')', ',', '\\', 'n', '</s>']
Filtered   (016): ['file', 'Ident', 'ifier', '=', '(', 'obj', '_', 'type', ',', 'obj', '_', 'inst', ')', ',', '\\', 'n']
Detokenized (009): ['fileIdentifier', '=', '(', 'obj_type', ',', 'obj_inst', ')', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "record_data = list ( args [ 4 : ] ) \n"
Original    (011): ['record_data', '=', 'list', '(', 'args', '[', '4', ':', ']', ')', '\\n']
Tokenized   (016): ['<s>', 'record', '_', 'data', '=', 'list', '(', 'args', '[', '4', ':', ']', ')', '\\', 'n', '</s>']
Filtered   (014): ['record', '_', 'data', '=', 'list', '(', 'args', '[', '4', ':', ']', ')', '\\', 'n']
Detokenized (011): ['record_data', '=', 'list', '(', 'args', '[', '4', ':', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "accessMethod = AtomicWriteFileRequestAccessMethodChoice ( \n"
Original    (005): ['accessMethod', '=', 'AtomicWriteFileRequestAccessMethodChoice', '(', '\\n']
Tokenized   (015): ['<s>', 'access', 'Method', '=', 'Atomic', 'Write', 'File', 'Request', 'Access', 'Method', 'Choice', '(', '\\', 'n', '</s>']
Filtered   (013): ['access', 'Method', '=', 'Atomic', 'Write', 'File', 'Request', 'Access', 'Method', 'Choice', '(', '\\', 'n']
Detokenized (005): ['accessMethod', '=', 'AtomicWriteFileRequestAccessMethodChoice', '(', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "objectIdentifier = int ( args . ini . objectidentifier ) , \n"
Original    (012): ['objectIdentifier', '=', 'int', '(', 'args', '.', 'ini', '.', 'objectidentifier', ')', ',', '\\n']
Tokenized   (020): ['<s>', 'object', 'Ident', 'ifier', '=', 'int', '(', 'args', '.', 'in', 'i', '.', 'object', 'ident', 'ifier', ')', ',', '\\', 'n', '</s>']
Filtered   (018): ['object', 'Ident', 'ifier', '=', 'int', '(', 'args', '.', 'in', 'i', '.', 'object', 'ident', 'ifier', ')', ',', '\\', 'n']
Detokenized (012): ['objectIdentifier', '=', 'int', '(', 'args', '.', 'ini', '.', 'objectidentifier', ')', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "this_application = TestApplication ( this_device , args . ini . address ) \n"
Original    (013): ['this_application', '=', 'TestApplication', '(', 'this_device', ',', 'args', '.', 'ini', '.', 'address', ')', '\\n']
Tokenized   (022): ['<s>', 'this', '_', 'application', '=', 'Test', 'Application', '(', 'this', '_', 'device', ',', 'args', '.', 'in', 'i', '.', 'address', ')', '\\', 'n', '</s>']
Filtered   (020): ['this', '_', 'application', '=', 'Test', 'Application', '(', 'this', '_', 'device', ',', 'args', '.', 'in', 'i', '.', 'address', ')', '\\', 'n']
Detokenized (013): ['this_application', '=', 'TestApplication', '(', 'this_device', ',', 'args', '.', 'ini', '.', 'address', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "_log . debug ( "running" ) \n"
Original    (007): ['_log', '.', 'debug', '(', '"running"', ')', '\\n']
Tokenized   (013): ['<s>', '_', 'log', '.', 'debug', '(', '"', 'running', '"', ')', '\\', 'n', '</s>']
Filtered   (011): ['_', 'log', '.', 'debug', '(', '"', 'running', '"', ')', '\\', 'n']
Detokenized (007): ['_log', '.', 'debug', '(', '"running"', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "Status . FAILED_TO_ADD_TO_CLIENT : % ( COLOR_FAILED_TO_ADD_TO_CLIENT , Color . ENDC ) , \n"
Original    (014): ['Status', '.', 'FAILED_TO_ADD_TO_CLIENT', ':', '%', '(', 'COLOR_FAILED_TO_ADD_TO_CLIENT', ',', 'Color', '.', 'ENDC', ')', ',', '\\n']
Tokenized   (043): ['<s>', 'Status', '.', 'FA', 'IL', 'ED', '_', 'TO', '_', 'ADD', '_', 'TO', '_', 'CL', 'IENT', ':', '%', '(', 'COL', 'OR', '_', 'FA', 'IL', 'ED', '_', 'TO', '_', 'ADD', '_', 'TO', '_', 'CL', 'IENT', ',', 'Color', '.', 'END', 'C', ')', ',', '\\', 'n', '</s>']
Filtered   (041): ['Status', '.', 'FA', 'IL', 'ED', '_', 'TO', '_', 'ADD', '_', 'TO', '_', 'CL', 'IENT', ':', '%', '(', 'COL', 'OR', '_', 'FA', 'IL', 'ED', '_', 'TO', '_', 'ADD', '_', 'TO', '_', 'CL', 'IENT', ',', 'Color', '.', 'END', 'C', ')', ',', '\\', 'n']
Detokenized (014): ['Status', '.', 'FAILED_TO_ADD_TO_CLIENT', ':', '%', '(', 'COLOR_FAILED_TO_ADD_TO_CLIENT', ',', 'Color', '.', 'ENDC', ')', ',', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "end_size += f [ ] \n"
Original    (006): ['end_size', '+=', 'f', '[', ']', '\\n']
Tokenized   (011): ['<s>', 'end', '_', 'size', '+=', 'f', '[', ']', '\\', 'n', '</s>']
Filtered   (009): ['end', '_', 'size', '+=', 'f', '[', ']', '\\', 'n']
Detokenized (006): ['end_size', '+=', 'f', '[', ']', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "files_to_check += self . db . find_hash_varying_size ( f [ ] ) \n"
Original    (013): ['files_to_check', '+=', 'self', '.', 'db', '.', 'find_hash_varying_size', '(', 'f', '[', ']', ')', '\\n']
Tokenized   (028): ['<s>', 'files', '_', 'to', '_', 'check', '+=', 'self', '.', 'db', '.', 'find', '_', 'hash', '_', 'v', 'ary', 'ing', '_', 'size', '(', 'f', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (026): ['files', '_', 'to', '_', 'check', '+=', 'self', '.', 'db', '.', 'find', '_', 'hash', '_', 'v', 'ary', 'ing', '_', 'size', '(', 'f', '[', ']', ')', '\\', 'n']
Detokenized (013): ['files_to_check', '+=', 'self', '.', 'db', '.', 'find_hash_varying_size', '(', 'f', '[', ']', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "match_start , match_end = pieces . match_file ( db_file , start_size , end_size ) \n"
Original    (015): ['match_start', ',', 'match_end', '=', 'pieces', '.', 'match_file', '(', 'db_file', ',', 'start_size', ',', 'end_size', ')', '\\n']
Tokenized   (030): ['<s>', 'match', '_', 'start', ',', 'match', '_', 'end', '=', 'pieces', '.', 'match', '_', 'file', '(', 'db', '_', 'file', ',', 'start', '_', 'size', ',', 'end', '_', 'size', ')', '\\', 'n', '</s>']
Filtered   (028): ['match', '_', 'start', ',', 'match', '_', 'end', '=', 'pieces', '.', 'match', '_', 'file', '(', 'db', '_', 'file', ',', 'start', '_', 'size', ',', 'end', '_', 'size', ')', '\\', 'n']
Detokenized (015): ['match_start', ',', 'match_end', '=', 'pieces', '.', 'match_file', '(', 'db_file', ',', 'start_size', ',', 'end_size', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "path_files [ os . path . join ( * path ) ] . append ( { \n"
Original    (017): ['path_files', '[', 'os', '.', 'path', '.', 'join', '(', '*', 'path', ')', ']', '.', 'append', '(', '{', '\\n']
Tokenized   (022): ['<s>', 'path', '_', 'files', '[', 'os', '.', 'path', '.', 'join', '(', '*', 'path', ')', ']', '.', 'append', '(', '{', '\\', 'n', '</s>']
Filtered   (020): ['path', '_', 'files', '[', 'os', '.', 'path', '.', 'join', '(', '*', 'path', ')', ']', '.', 'append', '(', '{', '\\', 'n']
Detokenized (017): ['path_files', '[', 'os', '.', 'path', '.', 'join', '(', '*', 'path', ')', ']', '.', 'append', '(', '{', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "files_sorted [ . join ( orig_path ) ] = i \n"
Original    (011): ['files_sorted', '[', '.', 'join', '(', 'orig_path', ')', ']', '=', 'i', '\\n']
Tokenized   (019): ['<s>', 'files', '_', 's', 'orted', '[', '.', 'join', '(', 'orig', '_', 'path', ')', ']', '=', 'i', '\\', 'n', '</s>']
Filtered   (017): ['files', '_', 's', 'orted', '[', '.', 'join', '(', 'orig', '_', 'path', ')', ']', '=', 'i', '\\', 'n']
Detokenized (011): ['files_sorted', '[', '.', 'join', '(', 'orig_path', ')', ']', '=', 'i', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "found_size , missing_size = 0 , 0 \n"
Original    (008): ['found_size', ',', 'missing_size', '=', '0', ',', '0', '\\n']
Tokenized   (015): ['<s>', 'found', '_', 'size', ',', 'missing', '_', 'size', '=', '0', ',', '0', '\\', 'n', '</s>']
Filtered   (013): ['found', '_', 'size', ',', 'missing', '_', 'size', '=', '0', ',', '0', '\\', 'n']
Detokenized (008): ['found_size', ',', 'missing_size', '=', '0', ',', '0', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "output_fp . write ( * write_bytes ) \n"
Original    (008): ['output_fp', '.', 'write', '(', '*', 'write_bytes', ')', '\\n']
Tokenized   (015): ['<s>', 'output', '_', 'fp', '.', 'write', '(', '*', 'write', '_', 'bytes', ')', '\\', 'n', '</s>']
Filtered   (013): ['output', '_', 'fp', '.', 'write', '(', '*', 'write', '_', 'bytes', ')', '\\', 'n']
Detokenized (008): ['output_fp', '.', 'write', '(', '*', 'write_bytes', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "bytes_written += read_bytes \n"
Original    (004): ['bytes_written', '+=', 'read_bytes', '\\n']
Tokenized   (011): ['<s>', 'bytes', '_', 'written', '+=', 'read', '_', 'bytes', '\\', 'n', '</s>']
Filtered   (009): ['bytes', '_', 'written', '+=', 'read', '_', 'bytes', '\\', 'n']
Detokenized (004): ['bytes_written', '+=', 'read_bytes', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "missing_percent = ( missing_size / ( found_size + missing_size ) ) * 100 \n"
Original    (014): ['missing_percent', '=', '(', 'missing_size', '/', '(', 'found_size', '+', 'missing_size', ')', ')', '*', '100', '\\n']
Tokenized   (025): ['<s>', 'missing', '_', 'percent', '=', '(', 'missing', '_', 'size', '/', '(', 'found', '_', 'size', '+', 'missing', '_', 'size', ')', ')', '*', '100', '\\', 'n', '</s>']
Filtered   (023): ['missing', '_', 'percent', '=', '(', 'missing', '_', 'size', '/', '(', 'found', '_', 'size', '+', 'missing', '_', 'size', ')', ')', '*', '100', '\\', 'n']
Detokenized (014): ['missing_percent', '=', '(', 'missing_size', '/', '(', 'found_size', '+', 'missing_size', ')', ')', '*', '100', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "found_percent = 100 - missing_percent \n"
Original    (006): ['found_percent', '=', '100', '-', 'missing_percent', '\\n']
Tokenized   (013): ['<s>', 'found', '_', 'percent', '=', '100', '-', 'missing', '_', 'percent', '\\', 'n', '</s>']
Filtered   (011): ['found', '_', 'percent', '=', '100', '-', 'missing', '_', 'percent', '\\', 'n']
Detokenized (006): ['found_percent', '=', '100', '-', 'missing_percent', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "would_not_add = missing_size and missing_percent > self . add_limit_percent or missing_size > \n"
Original    (013): ['would_not_add', '=', 'missing_size', 'and', 'missing_percent', '>', 'self', '.', 'add_limit_percent', 'or', 'missing_size', '>', '\\n']
Tokenized   (030): ['<s>', 'would', '_', 'not', '_', 'add', '=', 'missing', '_', 'size', 'and', 'missing', '_', 'percent', '>', 'self', '.', 'add', '_', 'limit', '_', 'percent', 'or', 'missing', '_', 'size', '>', '\\', 'n', '</s>']
Filtered   (028): ['would', '_', 'not', '_', 'add', '=', 'missing', '_', 'size', 'and', 'missing', '_', 'percent', '>', 'self', '.', 'add', '_', 'limit', '_', 'percent', 'or', 'missing', '_', 'size', '>', '\\', 'n']
Detokenized (013): ['would_not_add', '=', 'missing_size', 'and', 'missing_percent', '>', 'self', '.', 'add_limit_percent', 'or', 'missing_size', '>', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "LEGO_PALETTE = ( , , , , , , ) \n"
Original    (011): ['LEGO_PALETTE', '=', '(', ',', ',', ',', ',', ',', ',', ')', '\\n']
Tokenized   (020): ['<s>', 'LE', 'GO', '_', 'P', 'AL', 'ET', 'TE', '=', '(', ',', ',', ',', ',', ',', ',', ')', '\\', 'n', '</s>']
Filtered   (018): ['LE', 'GO', '_', 'P', 'AL', 'ET', 'TE', '=', '(', ',', ',', ',', ',', ',', ',', ')', '\\', 'n']
Detokenized (011): ['LEGO_PALETTE', '=', '(', ',', ',', ',', ',', ',', ',', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "Draft4Validator , RefResolver , create , extend , validator_for , validate , \n"
Original    (013): ['Draft4Validator', ',', 'RefResolver', ',', 'create', ',', 'extend', ',', 'validator_for', ',', 'validate', ',', '\\n']
Tokenized   (024): ['<s>', 'Draft', '4', 'Valid', 'ator', ',', 'Ref', 'Res', 'olver', ',', 'create', ',', 'extend', ',', 'valid', 'ator', '_', 'for', ',', 'validate', ',', '\\', 'n', '</s>']
Filtered   (022): ['Draft', '4', 'Valid', 'ator', ',', 'Ref', 'Res', 'olver', ',', 'create', ',', 'extend', ',', 'valid', 'ator', '_', 'for', ',', 'validate', ',', '\\', 'n']
Detokenized (013): ['Draft4Validator', ',', 'RefResolver', ',', 'create', ',', 'extend', ',', 'validator_for', ',', 'validate', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "u"enum" : [ [ "a" , "b" , "c" ] , [ "d" , "e" , "f" ] ] , \n"
Original    (021): ['u"enum"', ':', '[', '[', '"a"', ',', '"b"', ',', '"c"', ']', ',', '[', '"d"', ',', '"e"', ',', '"f"', ']', ']', ',', '\\n']
Tokenized   (039): ['<s>', 'u', '"', 'enum', '"', ':', '[', '[', '"', 'a', '"', ',', '"', 'b', '"', ',', '"', 'c', '"', ']', ',', '[', '"', 'd', '"', ',', '"', 'e', '"', ',', '"', 'f', '"', ']', ']', ',', '\\', 'n', '</s>']
Filtered   (037): ['u', '"', 'enum', '"', ':', '[', '[', '"', 'a', '"', ',', '"', 'b', '"', ',', '"', 'c', '"', ']', ',', '[', '"', 'd', '"', ',', '"', 'e', '"', ',', '"', 'f', '"', ']', ']', ',', '\\', 'n']
Detokenized (021): ['u"enum"', ':', '[', '[', '"a"', ',', '"b"', ',', '"c"', ']', ',', '[', '"d"', ',', '"e"', ',', '"f"', ']', ']', ',', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "got = ( e . message for e in self . validator . iter_errors ( instance , schema ) ) \n"
Original    (021): ['got', '=', '(', 'e', '.', 'message', 'for', 'e', 'in', 'self', '.', 'validator', '.', 'iter_errors', '(', 'instance', ',', 'schema', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'got', '=', '(', 'e', '.', 'message', 'for', 'e', 'in', 'self', '.', 'valid', 'ator', '.', 'iter', '_', 'errors', '(', 'instance', ',', 'schema', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['got', '=', '(', 'e', '.', 'message', 'for', 'e', 'in', 'self', '.', 'valid', 'ator', '.', 'iter', '_', 'errors', '(', 'instance', ',', 'schema', ')', ')', '\\', 'n']
Detokenized (021): ['got', '=', '(', 'e', '.', 'message', 'for', 'e', 'in', 'self', '.', 'validator', '.', 'iter_errors', '(', 'instance', ',', 'schema', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "checker . checks ( u"thing" ) ( check_fn ) \n"
Original    (010): ['checker', '.', 'checks', '(', 'u"thing"', ')', '(', 'check_fn', ')', '\\n']
Tokenized   (019): ['<s>', 'check', 'er', '.', 'checks', '(', 'u', '"', 'thing', '"', ')', '(', 'check', '_', 'fn', ')', '\\', 'n', '</s>']
Filtered   (017): ['check', 'er', '.', 'checks', '(', 'u', '"', 'thing', '"', ')', '(', 'check', '_', 'fn', ')', '\\', 'n']
Detokenized (010): ['checker', '.', 'checks', '(', 'u"thing"', ')', '(', 'check_fn', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "deque ( [ "type" , 1 , "properties" , "foo" , "enum" ] ) , \n"
Original    (016): ['deque', '(', '[', '"type"', ',', '1', ',', '"properties"', ',', '"foo"', ',', '"enum"', ']', ')', ',', '\\n']
Tokenized   (028): ['<s>', 'de', 'que', '(', '[', '"', 'type', '"', ',', '1', ',', '"', 'properties', '"', ',', '"', 'foo', '"', ',', '"', 'enum', '"', ']', ')', ',', '\\', 'n', '</s>']
Filtered   (026): ['de', 'que', '(', '[', '"', 'type', '"', ',', '1', ',', '"', 'properties', '"', ',', '"', 'foo', '"', ',', '"', 'enum', '"', ']', ')', ',', '\\', 'n']
Detokenized (016): ['deque', '(', '[', '"type"', ',', '1', ',', '"properties"', ',', '"foo"', ',', '"enum"', ']', ')', ',', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : ""baz" : { "minItems" : 2 } , \n"
Original    (009): ['"baz"', ':', '{', '"minItems"', ':', '2', '}', ',', '\\n']
Tokenized   (018): ['<s>', '"', 'b', 'az', '"', ':', '{', '"', 'min', 'Items', '"', ':', '2', '}', ',', '\\', 'n', '</s>']
Filtered   (016): ['"', 'b', 'az', '"', ':', '{', '"', 'min', 'Items', '"', ':', '2', '}', ',', '\\', 'n']
Detokenized (009): ['"baz"', ':', '{', '"minItems"', ':', '2', '}', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ""required" : [ "root" ] , \n"
Original    (007): ['"required"', ':', '[', '"root"', ']', ',', '\\n']
Tokenized   (014): ['<s>', '"', 'required', '"', ':', '[', '"', 'root', '"', ']', ',', '\\', 'n', '</s>']
Filtered   (012): ['"', 'required', '"', ':', '[', '"', 'root', '"', ']', ',', '\\', 'n']
Detokenized (007): ['"required"', ':', '[', '"root"', ']', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "e2 . absolute_schema_path , deque ( \n"
Original    (007): ['e2', '.', 'absolute_schema_path', ',', 'deque', '(', '\\n']
Tokenized   (017): ['<s>', 'e', '2', '.', 'absolute', '_', 'sche', 'ma', '_', 'path', ',', 'de', 'que', '(', '\\', 'n', '</s>']
Filtered   (015): ['e', '2', '.', 'absolute', '_', 'sche', 'ma', '_', 'path', ',', 'de', 'que', '(', '\\', 'n']
Detokenized (007): ['e2', '.', 'absolute_schema_path', ',', 'deque', '(', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ""additionalProperties" : { "type" : "integer" , "minimum" : 5 } \n"
Original    (012): ['"additionalProperties"', ':', '{', '"type"', ':', '"integer"', ',', '"minimum"', ':', '5', '}', '\\n']
Tokenized   (026): ['<s>', '"', 'add', 'itional', 'Pro', 'perties', '"', ':', '{', '"', 'type', '"', ':', '"', 'integer', '"', ',', '"', 'minimum', '"', ':', '5', '}', '\\', 'n', '</s>']
Filtered   (024): ['"', 'add', 'itional', 'Pro', 'perties', '"', ':', '{', '"', 'type', '"', ':', '"', 'integer', '"', ',', '"', 'minimum', '"', ':', '5', '}', '\\', 'n']
Detokenized (012): ['"additionalProperties"', ':', '{', '"type"', ':', '"integer"', ',', '"minimum"', ':', '5', '}', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : ""bar" : { "type" : "string" } , \n"
Original    (009): ['"bar"', ':', '{', '"type"', ':', '"string"', '}', ',', '\\n']
Tokenized   (018): ['<s>', '"', 'bar', '"', ':', '{', '"', 'type', '"', ':', '"', 'string', '"', '}', ',', '\\', 'n', '</s>']
Filtered   (016): ['"', 'bar', '"', ':', '{', '"', 'type', '"', ':', '"', 'string', '"', '}', ',', '\\', 'n']
Detokenized (009): ['"bar"', ':', '{', '"type"', ':', '"string"', '}', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ""foo" : { "minimum" : 5 } \n"
Original    (008): ['"foo"', ':', '{', '"minimum"', ':', '5', '}', '\\n']
Tokenized   (015): ['<s>', '"', 'foo', '"', ':', '{', '"', 'minimum', '"', ':', '5', '}', '\\', 'n', '</s>']
Filtered   (013): ['"', 'foo', '"', ':', '{', '"', 'minimum', '"', ':', '5', '}', '\\', 'n']
Detokenized (008): ['"foo"', ':', '{', '"minimum"', ':', '5', '}', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : ""items" : [ { } ] , \n"
Original    (008): ['"items"', ':', '[', '{', '}', ']', ',', '\\n']
Tokenized   (013): ['<s>', '"', 'items', '"', ':', '[', '{', '}', ']', ',', '\\', 'n', '</s>']
Filtered   (011): ['"', 'items', '"', ':', '[', '{', '}', ']', ',', '\\', 'n']
Detokenized (008): ['"items"', ':', '[', '{', '}', ']', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "validate ( instance = instance , schema = { my_property : my_value } ) \n"
Original    (015): ['validate', '(', 'instance', '=', 'instance', ',', 'schema', '=', '{', 'my_property', ':', 'my_value', '}', ')', '\\n']
Tokenized   (023): ['<s>', 'valid', 'ate', '(', 'instance', '=', 'instance', ',', 'schema', '=', '{', 'my', '_', 'property', ':', 'my', '_', 'value', '}', ')', '\\', 'n', '</s>']
Filtered   (021): ['valid', 'ate', '(', 'instance', '=', 'instance', ',', 'schema', '=', '{', 'my', '_', 'property', ':', 'my', '_', 'value', '}', ')', '\\', 'n']
Detokenized (015): ['validate', '(', 'instance', '=', 'instance', ',', 'schema', '=', '{', 'my_property', ':', 'my_value', '}', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "chk_schema . assert_called_once_with ( { } ) \n"
Original    (008): ['chk_schema', '.', 'assert_called_once_with', '(', '{', '}', ')', '\\n']
Tokenized   (021): ['<s>', 'ch', 'k', '_', 'sche', 'ma', '.', 'assert', '_', 'called', '_', 'once', '_', 'with', '(', '{', '}', ')', '\\', 'n', '</s>']
Filtered   (019): ['ch', 'k', '_', 'sche', 'ma', '.', 'assert', '_', 'called', '_', 'once', '_', 'with', '(', '{', '}', ')', '\\', 'n']
Detokenized (008): ['chk_schema', '.', 'assert_called_once_with', '(', '{', '}', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "stored_schema = { "stored" : "schema" } \n"
Original    (008): ['stored_schema', '=', '{', '"stored"', ':', '"schema"', '}', '\\n']
Tokenized   (021): ['<s>', 'st', 'ored', '_', 'sche', 'ma', '=', '{', '"', 'st', 'ored', '"', ':', '"', 'sche', 'ma', '"', '}', '\\', 'n', '</s>']
Filtered   (019): ['st', 'ored', '_', 'sche', 'ma', '=', '{', '"', 'st', 'ored', '"', ':', '"', 'sche', 'ma', '"', '}', '\\', 'n']
Detokenized (008): ['stored_schema', '=', '{', '"stored"', ':', '"schema"', '}', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : ""ports" : \n"
Original    (003): ['"ports"', ':', '\\n']
Tokenized   (008): ['<s>', '"', 'ports', '"', ':', '\\', 'n', '</s>']
Filtered   (006): ['"', 'ports', '"', ':', '\\', 'n']
Detokenized (003): ['"ports"', ':', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "l2Report . generateReport ( pod . id , True , False ) \n"
Original    (013): ['l2Report', '.', 'generateReport', '(', 'pod', '.', 'id', ',', 'True', ',', 'False', ')', '\\n']
Tokenized   (019): ['<s>', 'l', '2', 'Report', '.', 'generate', 'Report', '(', 'pod', '.', 'id', ',', 'True', ',', 'False', ')', '\\', 'n', '</s>']
Filtered   (017): ['l', '2', 'Report', '.', 'generate', 'Report', '(', 'pod', '.', 'id', ',', 'True', ',', 'False', ')', '\\', 'n']
Detokenized (013): ['l2Report', '.', 'generateReport', '(', 'pod', '.', 'id', ',', 'True', ',', 'False', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "_YAML_ = splitext ( __file__ ) [ 0 ] + \n"
Original    (011): ['_YAML_', '=', 'splitext', '(', '__file__', ')', '[', '0', ']', '+', '\\n']
Tokenized   (022): ['<s>', '_', 'Y', 'AM', 'L', '_', '=', 'spl', 'ite', 'xt', '(', '__', 'file', '__', ')', '[', '0', ']', '+', '\\', 'n', '</s>']
Filtered   (020): ['_', 'Y', 'AM', 'L', '_', '=', 'spl', 'ite', 'xt', '(', '__', 'file', '__', ')', '[', '0', ']', '+', '\\', 'n']
Detokenized (011): ['_YAML_', '=', 'splitext', '(', '__file__', ')', '[', '0', ']', '+', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "globals ( ) . update ( loadyaml ( _YAML_ ) ) \n"
Original    (012): ['globals', '(', ')', '.', 'update', '(', 'loadyaml', '(', '_YAML_', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'gl', 'ob', 'als', '(', ')', '.', 'update', '(', 'load', 'y', 'aml', '(', '_', 'Y', 'AM', 'L', '_', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['gl', 'ob', 'als', '(', ')', '.', 'update', '(', 'load', 'y', 'aml', '(', '_', 'Y', 'AM', 'L', '_', ')', ')', '\\', 'n']
Detokenized (012): ['globals', '(', ')', '.', 'update', '(', 'loadyaml', '(', '_YAML_', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "gather_facts = False ) \n"
Original    (005): ['gather_facts', '=', 'False', ')', '\\n']
Tokenized   (011): ['<s>', 'g', 'ather', '_', 'facts', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (009): ['g', 'ather', '_', 'facts', '=', 'False', ')', '\\', 'n']
Detokenized (005): ['gather_facts', '=', 'False', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "DEFAULT_API_URLS = ( , \n"
Original    (005): ['DEFAULT_API_URLS', '=', '(', ',', '\\n']
Tokenized   (014): ['<s>', 'DE', 'FAULT', '_', 'API', '_', 'UR', 'LS', '=', '(', ',', '\\', 'n', '</s>']
Filtered   (012): ['DE', 'FAULT', '_', 'API', '_', 'UR', 'LS', '=', '(', ',', '\\', 'n']
Detokenized (005): ['DEFAULT_API_URLS', '=', '(', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "BAD_STATUS_CODES = [ , , , \n"
Original    (007): ['BAD_STATUS_CODES', '=', '[', ',', ',', ',', '\\n']
Tokenized   (018): ['<s>', 'B', 'AD', '_', 'STAT', 'US', '_', 'C', 'OD', 'ES', '=', '[', ',', ',', ',', '\\', 'n', '</s>']
Filtered   (016): ['B', 'AD', '_', 'STAT', 'US', '_', 'C', 'OD', 'ES', '=', '[', ',', ',', ',', '\\', 'n']
Detokenized (007): ['BAD_STATUS_CODES', '=', '[', ',', ',', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "translate_otp = True , api_urls = DEFAULT_API_URLS , \n"
Original    (009): ['translate_otp', '=', 'True', ',', 'api_urls', '=', 'DEFAULT_API_URLS', ',', '\\n']
Tokenized   (025): ['<s>', 'trans', 'late', '_', 'ot', 'p', '=', 'True', ',', 'api', '_', 'url', 's', '=', 'DE', 'FAULT', '_', 'API', '_', 'UR', 'LS', ',', '\\', 'n', '</s>']
Filtered   (023): ['trans', 'late', '_', 'ot', 'p', '=', 'True', ',', 'api', '_', 'url', 's', '=', 'DE', 'FAULT', '_', 'API', '_', 'UR', 'LS', ',', '\\', 'n']
Detokenized (009): ['translate_otp', '=', 'True', ',', 'api_urls', '=', 'DEFAULT_API_URLS', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "rand_str = b ( os . urandom ( 30 ) ) \n"
Original    (012): ['rand_str', '=', 'b', '(', 'os', '.', 'urandom', '(', '30', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'rand', '_', 'str', '=', 'b', '(', 'os', '.', 'ur', 'andom', '(', '30', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['rand', '_', 'str', '=', 'b', '(', 'os', '.', 'ur', 'andom', '(', '30', ')', ')', '\\', 'n']
Detokenized (012): ['rand_str', '=', 'b', '(', 'os', '.', 'urandom', '(', '30', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "nonce = base64 . b64encode ( rand_str , b ( ) ) [ : 25 ] . decode ( ) \n"
Original    (021): ['nonce', '=', 'base64', '.', 'b64encode', '(', 'rand_str', ',', 'b', '(', ')', ')', '[', ':', '25', ']', '.', 'decode', '(', ')', '\\n']
Tokenized   (031): ['<s>', 'non', 'ce', '=', 'base', '64', '.', 'b', '64', 'en', 'code', '(', 'rand', '_', 'str', ',', 'b', '(', ')', ')', '[', ':', '25', ']', '.', 'decode', '(', ')', '\\', 'n', '</s>']
Filtered   (029): ['non', 'ce', '=', 'base', '64', '.', 'b', '64', 'en', 'code', '(', 'rand', '_', 'str', ',', 'b', '(', ')', ')', '[', ':', '25', ']', '.', 'decode', '(', ')', '\\', 'n']
Detokenized (021): ['nonce', '=', 'base64', '.', 'b64encode', '(', 'rand_str', ',', 'b', '(', ')', ')', '[', ':', '25', ']', '.', 'decode', '(', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "otp . otp , nonce , \n"
Original    (007): ['otp', '.', 'otp', ',', 'nonce', ',', '\\n']
Tokenized   (013): ['<s>', 'ot', 'p', '.', 'ot', 'p', ',', 'non', 'ce', ',', '\\', 'n', '</s>']
Filtered   (011): ['ot', 'p', '.', 'ot', 'p', ',', 'non', 'ce', ',', '\\', 'n']
Detokenized (007): ['otp', '.', 'otp', ',', 'nonce', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "pairs_string = . join ( [ . join ( pair ) for pair in pairs_sorted ] ) \n"
Original    (018): ['pairs_string', '=', '.', 'join', '(', '[', '.', 'join', '(', 'pair', ')', 'for', 'pair', 'in', 'pairs_sorted', ']', ')', '\\n']
Tokenized   (027): ['<s>', 'p', 'airs', '_', 'string', '=', '.', 'join', '(', '[', '.', 'join', '(', 'pair', ')', 'for', 'pair', 'in', 'pairs', '_', 's', 'orted', ']', ')', '\\', 'n', '</s>']
Filtered   (025): ['p', 'airs', '_', 'string', '=', '.', 'join', '(', '[', '.', 'join', '(', 'pair', ')', 'for', 'pair', 'in', 'pairs', '_', 's', 'orted', ']', ')', '\\', 'n']
Detokenized (018): ['pairs_string', '=', '.', 'join', '(', '[', '.', 'join', '(', 'pair', ')', 'for', 'pair', 'in', 'pairs_sorted', ']', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "digest = hmac . new ( self . key , b ( pairs_string ) , hashlib . sha1 ) . digest ( ) \n"
Original    (024): ['digest', '=', 'hmac', '.', 'new', '(', 'self', '.', 'key', ',', 'b', '(', 'pairs_string', ')', ',', 'hashlib', '.', 'sha1', ')', '.', 'digest', '(', ')', '\\n']
Tokenized   (034): ['<s>', 'dig', 'est', '=', 'h', 'mac', '.', 'new', '(', 'self', '.', 'key', ',', 'b', '(', 'pairs', '_', 'string', ')', ',', 'hash', 'lib', '.', 'sh', 'a', '1', ')', '.', 'digest', '(', ')', '\\', 'n', '</s>']
Filtered   (032): ['dig', 'est', '=', 'h', 'mac', '.', 'new', '(', 'self', '.', 'key', ',', 'b', '(', 'pairs', '_', 'string', ')', ',', 'hash', 'lib', '.', 'sh', 'a', '1', ')', '.', 'digest', '(', ')', '\\', 'n']
Detokenized (024): ['digest', '=', 'hmac', '.', 'new', '(', 'self', '.', 'key', ',', 'b', '(', 'pairs_string', ')', ',', 'hashlib', '.', 'sha1', ')', '.', 'digest', '(', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "signature = ( [ unquote ( v ) for k , v in pairs if k == ] or [ None ] ) [ 0 ] \n"
Original    (027): ['signature', '=', '(', '[', 'unquote', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '==', ']', 'or', '[', 'None', ']', ')', '[', '0', ']', '\\n']
Tokenized   (032): ['<s>', 'sign', 'ature', '=', '(', '[', 'un', 'quote', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '==', ']', 'or', '[', 'None', ']', ')', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (030): ['sign', 'ature', '=', '(', '[', 'un', 'quote', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '==', ']', 'or', '[', 'None', ']', ')', '[', '0', ']', '\\', 'n']
Detokenized (027): ['signature', '=', '(', '[', 'unquote', '(', 'v', ')', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '==', ']', 'or', '[', 'None', ']', ')', '[', '0', ']', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "query_string = . join ( [ k + + v for k , v in pairs if k != ] ) \n"
Original    (022): ['query_string', '=', '.', 'join', '(', '[', 'k', '+', '+', 'v', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '!=', ']', ')', '\\n']
Tokenized   (027): ['<s>', 'query', '_', 'string', '=', '.', 'join', '(', '[', 'k', '+', '+', 'v', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '!=', ']', ')', '\\', 'n', '</s>']
Filtered   (025): ['query', '_', 'string', '=', '.', 'join', '(', '[', 'k', '+', '+', 'v', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '!=', ']', ')', '\\', 'n']
Detokenized (022): ['query_string', '=', '.', 'join', '(', '[', 'k', '+', '+', 'v', 'for', 'k', ',', 'v', 'in', 'pairs', 'if', 'k', '!=', ']', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "pairs = ( x . split ( , 1 ) for x in query_string . split ( ) ) \n"
Original    (020): ['pairs', '=', '(', 'x', '.', 'split', '(', ',', '1', ')', 'for', 'x', 'in', 'query_string', '.', 'split', '(', ')', ')', '\\n']
Tokenized   (026): ['<s>', 'p', 'airs', '=', '(', 'x', '.', 'split', '(', ',', '1', ')', 'for', 'x', 'in', 'query', '_', 'string', '.', 'split', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['p', 'airs', '=', '(', 'x', '.', 'split', '(', ',', '1', ')', 'for', 'x', 'in', 'query', '_', 'string', '.', 'split', '(', ')', ')', '\\', 'n']
Detokenized (020): ['pairs', '=', '(', 'x', '.', 'split', '(', ',', '1', ')', 'for', 'x', 'in', 'query_string', '.', 'split', '(', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "py_modules = [ ] , \n"
Original    (006): ['py_modules', '=', '[', ']', ',', '\\n']
Tokenized   (011): ['<s>', 'py', '_', 'modules', '=', '[', ']', ',', '\\', 'n', '</s>']
Filtered   (009): ['py', '_', 'modules', '=', '[', ']', ',', '\\', 'n']
Detokenized (006): ['py_modules', '=', '[', ']', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "submitter , msg = result [ 0 ] \n"
Original    (009): ['submitter', ',', 'msg', '=', 'result', '[', '0', ']', '\\n']
Tokenized   (013): ['<s>', 'sub', 'mitter', ',', 'msg', '=', 'result', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (011): ['sub', 'mitter', ',', 'msg', '=', 'result', '[', '0', ']', '\\', 'n']
Detokenized (009): ['submitter', ',', 'msg', '=', 'result', '[', '0', ']', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "contact = self . line_interface . _get_contact_by_id ( me . id ) \n"
Original    (013): ['contact', '=', 'self', '.', 'line_interface', '.', '_get_contact_by_id', '(', 'me', '.', 'id', ')', '\\n']
Tokenized   (025): ['<s>', 'contact', '=', 'self', '.', 'line', '_', 'interface', '.', '_', 'get', '_', 'contact', '_', 'by', '_', 'id', '(', 'me', '.', 'id', ')', '\\', 'n', '</s>']
Filtered   (023): ['contact', '=', 'self', '.', 'line', '_', 'interface', '.', '_', 'get', '_', 'contact', '_', 'by', '_', 'id', '(', 'me', '.', 'id', ')', '\\', 'n']
Detokenized (013): ['contact', '=', 'self', '.', 'line_interface', '.', '_get_contact_by_id', '(', 'me', '.', 'id', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "ok_ ( me_display_name == me . name ) \n"
Original    (009): ['ok_', '(', 'me_display_name', '==', 'me', '.', 'name', ')', '\\n']
Tokenized   (017): ['<s>', 'ok', '_', '(', 'me', '_', 'display', '_', 'name', '==', 'me', '.', 'name', ')', '\\', 'n', '</s>']
Filtered   (015): ['ok', '_', '(', 'me', '_', 'display', '_', 'name', '==', 'me', '.', 'name', ')', '\\', 'n']
Detokenized (009): ['ok_', '(', 'me_display_name', '==', 'me', '.', 'name', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "transport . get_extra_info . return_value = None \n"
Original    (008): ['transport', '.', 'get_extra_info', '.', 'return_value', '=', 'None', '\\n']
Tokenized   (018): ['<s>', 'trans', 'port', '.', 'get', '_', 'extra', '_', 'info', '.', 'return', '_', 'value', '=', 'None', '\\', 'n', '</s>']
Filtered   (016): ['trans', 'port', '.', 'get', '_', 'extra', '_', 'info', '.', 'return', '_', 'value', '=', 'None', '\\', 'n']
Detokenized (008): ['transport', '.', 'get_extra_info', '.', 'return_value', '=', 'None', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "ShortenerSettings = namedtuple ( , [ \n"
Original    (007): ['ShortenerSettings', '=', 'namedtuple', '(', ',', '[', '\\n']
Tokenized   (014): ['<s>', 'Short', 'ener', 'Settings', '=', 'named', 't', 'uple', '(', ',', '[', '\\', 'n', '</s>']
Filtered   (012): ['Short', 'ener', 'Settings', '=', 'named', 't', 'uple', '(', ',', '[', '\\', 'n']
Detokenized (007): ['ShortenerSettings', '=', 'namedtuple', '(', ',', '[', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "right_to_left = [ , ] , \n"
Original    (007): ['right_to_left', '=', '[', ',', ']', ',', '\\n']
Tokenized   (014): ['<s>', 'right', '_', 'to', '_', 'left', '=', '[', ',', ']', ',', '\\', 'n', '</s>']
Filtered   (012): ['right', '_', 'to', '_', 'left', '=', '[', ',', ']', ',', '\\', 'n']
Detokenized (007): ['right_to_left', '=', '[', ',', ']', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "shortener = { } , \n"
Original    (006): ['shortener', '=', '{', '}', ',', '\\n']
Tokenized   (010): ['<s>', 'short', 'ener', '=', '{', '}', ',', '\\', 'n', '</s>']
Filtered   (008): ['short', 'ener', '=', '{', '}', ',', '\\', 'n']
Detokenized (006): ['shortener', '=', '{', '}', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "workers_pool = 10 , \n"
Original    (005): ['workers_pool', '=', '10', ',', '\\n']
Tokenized   (010): ['<s>', 'workers', '_', 'pool', '=', '10', ',', '\\', 'n', '</s>']
Filtered   (008): ['workers', '_', 'pool', '=', '10', ',', '\\', 'n']
Detokenized (005): ['workers_pool', '=', '10', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "cms_service_host = "http://localhost:5001" \n"
Original    (004): ['cms_service_host', '=', '"http://localhost:5001"', '\\n']
Tokenized   (018): ['<s>', 'cms', '_', 'service', '_', 'host', '=', '"', 'http', '://', 'localhost', ':', '500', '1', '"', '\\', 'n', '</s>']
Filtered   (016): ['cms', '_', 'service', '_', 'host', '=', '"', 'http', '://', 'localhost', ':', '500', '1', '"', '\\', 'n']
Detokenized (004): ['cms_service_host', '=', '"http://localhost:5001"', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "subparsers = args . add_subparsers ( help = , dest = ) \n"
Original    (013): ['subparsers', '=', 'args', '.', 'add_subparsers', '(', 'help', '=', ',', 'dest', '=', ')', '\\n']
Tokenized   (024): ['<s>', 'sub', 'p', 'ars', 'ers', '=', 'args', '.', 'add', '_', 'sub', 'p', 'ars', 'ers', '(', 'help', '=', ',', 'dest', '=', ')', '\\', 'n', '</s>']
Filtered   (022): ['sub', 'p', 'ars', 'ers', '=', 'args', '.', 'add', '_', 'sub', 'p', 'ars', 'ers', '(', 'help', '=', ',', 'dest', '=', ')', '\\', 'n']
Detokenized (013): ['subparsers', '=', 'args', '.', 'add_subparsers', '(', 'help', '=', ',', 'dest', '=', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "template_parser . add_argument ( , \n"
Original    (006): ['template_parser', '.', 'add_argument', '(', ',', '\\n']
Tokenized   (013): ['<s>', 'template', '_', 'parser', '.', 'add', '_', 'argument', '(', ',', '\\', 'n', '</s>']
Filtered   (011): ['template', '_', 'parser', '.', 'add', '_', 'argument', '(', ',', '\\', 'n']
Detokenized (006): ['template_parser', '.', 'add_argument', '(', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "config_parser . add_argument ( , help = ) \n"
Original    (009): ['config_parser', '.', 'add_argument', '(', ',', 'help', '=', ')', '\\n']
Tokenized   (016): ['<s>', 'config', '_', 'parser', '.', 'add', '_', 'argument', '(', ',', 'help', '=', ')', '\\', 'n', '</s>']
Filtered   (014): ['config', '_', 'parser', '.', 'add', '_', 'argument', '(', ',', 'help', '=', ')', '\\', 'n']
Detokenized (009): ['config_parser', '.', 'add_argument', '(', ',', 'help', '=', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "gui_parser . add_argument ( , , type = str , help = ) \n"
Original    (014): ['gui_parser', '.', 'add_argument', '(', ',', ',', 'type', '=', 'str', ',', 'help', '=', ')', '\\n']
Tokenized   (021): ['<s>', 'gui', '_', 'parser', '.', 'add', '_', 'argument', '(', ',', ',', 'type', '=', 'str', ',', 'help', '=', ')', '\\', 'n', '</s>']
Filtered   (019): ['gui', '_', 'parser', '.', 'add', '_', 'argument', '(', ',', ',', 'type', '=', 'str', ',', 'help', '=', ')', '\\', 'n']
Detokenized (014): ['gui_parser', '.', 'add_argument', '(', ',', ',', 'type', '=', 'str', ',', 'help', '=', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "changePwdResult = conn . extend . microsoft . modify_password ( user_dn , newpassword ) \n"
Original    (015): ['changePwdResult', '=', 'conn', '.', 'extend', '.', 'microsoft', '.', 'modify_password', '(', 'user_dn', ',', 'newpassword', ')', '\\n']
Tokenized   (027): ['<s>', 'change', 'P', 'wd', 'Result', '=', 'conn', '.', 'extend', '.', 'micro', 'soft', '.', 'modify', '_', 'password', '(', 'user', '_', 'dn', ',', 'new', 'password', ')', '\\', 'n', '</s>']
Filtered   (025): ['change', 'P', 'wd', 'Result', '=', 'conn', '.', 'extend', '.', 'micro', 'soft', '.', 'modify', '_', 'password', '(', 'user', '_', 'dn', ',', 'new', 'password', ')', '\\', 'n']
Detokenized (015): ['changePwdResult', '=', 'conn', '.', 'extend', '.', 'microsoft', '.', 'modify_password', '(', 'user_dn', ',', 'newpassword', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "cap_path = os . path . join ( caps_directory , ) \n"
Original    (012): ['cap_path', '=', 'os', '.', 'path', '.', 'join', '(', 'caps_directory', ',', ')', '\\n']
Tokenized   (019): ['<s>', 'cap', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'caps', '_', 'directory', ',', ')', '\\', 'n', '</s>']
Filtered   (017): ['cap', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'caps', '_', 'directory', ',', ')', '\\', 'n']
Detokenized (012): ['cap_path', '=', 'os', '.', 'path', '.', 'join', '(', 'caps_directory', ',', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "cap . eventloop . stop ( ) \n"
Original    (008): ['cap', '.', 'eventloop', '.', 'stop', '(', ')', '\\n']
Tokenized   (012): ['<s>', 'cap', '.', 'event', 'loop', '.', 'stop', '(', ')', '\\', 'n', '</s>']
Filtered   (010): ['cap', '.', 'event', 'loop', '.', 'stop', '(', ')', '\\', 'n']
Detokenized (008): ['cap', '.', 'eventloop', '.', 'stop', '(', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "flush = Service ( name = , \n"
Original    (008): ['flush', '=', 'Service', '(', 'name', '=', ',', '\\n']
Tokenized   (011): ['<s>', 'flush', '=', 'Service', '(', 'name', '=', ',', '\\', 'n', '</s>']
Filtered   (009): ['flush', '=', 'Service', '(', 'name', '=', ',', '\\', 'n']
Detokenized (008): ['flush', '=', 'Service', '(', 'name', '=', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "sourceIds = [ d [ ] for d in response . json ] \n"
Original    (014): ['sourceIds', '=', '[', 'd', '[', ']', 'for', 'd', 'in', 'response', '.', 'json', ']', '\\n']
Tokenized   (019): ['<s>', 'source', 'Id', 's', '=', '[', 'd', '[', ']', 'for', 'd', 'in', 'response', '.', 'json', ']', '\\', 'n', '</s>']
Filtered   (017): ['source', 'Id', 's', '=', '[', 'd', '[', ']', 'for', 'd', 'in', 'response', '.', 'json', ']', '\\', 'n']
Detokenized (014): ['sourceIds', '=', '[', 'd', '[', ']', 'for', 'd', 'in', 'response', '.', 'json', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : ""folder." ) \n"
Original    (003): ['"folder."', ')', '\\n']
Tokenized   (008): ['<s>', '"', 'folder', '."', ')', '\\', 'n', '</s>']
Filtered   (006): ['"', 'folder', '."', ')', '\\', 'n']
Detokenized (003): ['"folder."', ')', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "minerva_metadata [ ] = { \n"
Original    (006): ['minerva_metadata', '[', ']', '=', '{', '\\n']
Tokenized   (012): ['<s>', 'min', 'erva', '_', 'metadata', '[', ']', '=', '{', '\\', 'n', '</s>']
Filtered   (010): ['min', 'erva', '_', 'metadata', '[', ']', '=', '{', '\\', 'n']
Detokenized (006): ['minerva_metadata', '[', ']', '=', '{', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "Description ( ) \n"
Original    (004): ['Description', '(', ')', '\\n']
Tokenized   (007): ['<s>', 'Description', '(', ')', '\\', 'n', '</s>']
Filtered   (005): ['Description', '(', ')', '\\', 'n']
Detokenized (004): ['Description', '(', ')', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "matches = re . findall ( "(\'|\\")(\\S+)(\'|\\")" , text ) \n"
Original    (011): ['matches', '=', 're', '.', 'findall', '(', '"(\\\'|\\\\")(\\\\S+)(\\\'|\\\\")"', ',', 'text', ')', '\\n']
Tokenized   (030): ['<s>', 'mat', 'ches', '=', 're', '.', 'find', 'all', '(', '"(', "\\'", '|', '\\\\', '")', '(', '\\\\', 'S', '+', ')(', "\\'", '|', '\\\\', '")', '"', ',', 'text', ')', '\\', 'n', '</s>']
Filtered   (028): ['mat', 'ches', '=', 're', '.', 'find', 'all', '(', '"(', "\\'", '|', '\\\\', '")', '(', '\\\\', 'S', '+', ')(', "\\'", '|', '\\\\', '")', '"', ',', 'text', ')', '\\', 'n']
Detokenized (011): ['matches', '=', 're', '.', 'findall', '(', '"(\\\'|\\\\")(\\\\S+)(\\\'|\\\\")"', ',', 'text', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "package_data = { : [ ] } , \n"
Original    (009): ['package_data', '=', '{', ':', '[', ']', '}', ',', '\\n']
Tokenized   (014): ['<s>', 'package', '_', 'data', '=', '{', ':', '[', ']', '}', ',', '\\', 'n', '</s>']
Filtered   (012): ['package', '_', 'data', '=', '{', ':', '[', ']', '}', ',', '\\', 'n']
Detokenized (009): ['package_data', '=', '{', ':', '[', ']', '}', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "list_permissions = [ , , , ] \n"
Original    (008): ['list_permissions', '=', '[', ',', ',', ',', ']', '\\n']
Tokenized   (014): ['<s>', 'list', '_', 'per', 'missions', '=', '[', ',', ',', ',', ']', '\\', 'n', '</s>']
Filtered   (012): ['list', '_', 'per', 'missions', '=', '[', ',', ',', ',', ']', '\\', 'n']
Detokenized (008): ['list_permissions', '=', '[', ',', ',', ',', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "option_list = BaseCommand . option_list + ( \n"
Original    (008): ['option_list', '=', 'BaseCommand', '.', 'option_list', '+', '(', '\\n']
Tokenized   (016): ['<s>', 'option', '_', 'list', '=', 'Base', 'Command', '.', 'option', '_', 'list', '+', '(', '\\', 'n', '</s>']
Filtered   (014): ['option', '_', 'list', '=', 'Base', 'Command', '.', 'option', '_', 'list', '+', '(', '\\', 'n']
Detokenized (008): ['option_list', '=', 'BaseCommand', '.', 'option_list', '+', '(', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "make_option ( , \n"
Original    (004): ['make_option', '(', ',', '\\n']
Tokenized   (009): ['<s>', 'make', '_', 'option', '(', ',', '\\', 'n', '</s>']
Filtered   (007): ['make', '_', 'option', '(', ',', '\\', 'n']
Detokenized (004): ['make_option', '(', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "confirm_token = Column ( Unicode ( 100 ) ) \n"
Original    (010): ['confirm_token', '=', 'Column', '(', 'Unicode', '(', '100', ')', ')', '\\n']
Tokenized   (016): ['<s>', 'conf', 'irm', '_', 'token', '=', 'Column', '(', 'Unicode', '(', '100', ')', ')', '\\', 'n', '</s>']
Filtered   (014): ['conf', 'irm', '_', 'token', '=', 'Column', '(', 'Unicode', '(', '100', ')', ')', '\\', 'n']
Detokenized (010): ['confirm_token', '=', 'Column', '(', 'Unicode', '(', '100', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "creation_date = Column ( DateTime ( ) , nullable = False ) \n"
Original    (013): ['creation_date', '=', 'Column', '(', 'DateTime', '(', ')', ',', 'nullable', '=', 'False', ')', '\\n']
Tokenized   (020): ['<s>', 'creation', '_', 'date', '=', 'Column', '(', 'Date', 'Time', '(', ')', ',', 'null', 'able', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (018): ['creation', '_', 'date', '=', 'Column', '(', 'Date', 'Time', '(', ')', ',', 'null', 'able', '=', 'False', ')', '\\', 'n']
Detokenized (013): ['creation_date', '=', 'Column', '(', 'DateTime', '(', ')', ',', 'nullable', '=', 'False', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "last_login_date = Column ( DateTime ( ) ) \n"
Original    (009): ['last_login_date', '=', 'Column', '(', 'DateTime', '(', ')', ')', '\\n']
Tokenized   (017): ['<s>', 'last', '_', 'login', '_', 'date', '=', 'Column', '(', 'Date', 'Time', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (015): ['last', '_', 'login', '_', 'date', '=', 'Column', '(', 'Date', 'Time', '(', ')', ')', '\\', 'n']
Detokenized (009): ['last_login_date', '=', 'Column', '(', 'DateTime', '(', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "SHARING_ROLES = [ , , ] \n"
Original    (007): ['SHARING_ROLES', '=', '[', ',', ',', ']', '\\n']
Tokenized   (016): ['<s>', 'SH', 'AR', 'ING', '_', 'R', 'OL', 'ES', '=', '[', ',', ',', ']', '\\', 'n', '</s>']
Filtered   (014): ['SH', 'AR', 'ING', '_', 'R', 'OL', 'ES', '=', '[', ',', ',', ']', '\\', 'n']
Detokenized (007): ['SHARING_ROLES', '=', '[', ',', ',', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "USER_MANAGEMENT_ROLES = SHARING_ROLES + [ ] \n"
Original    (007): ['USER_MANAGEMENT_ROLES', '=', 'SHARING_ROLES', '+', '[', ']', '\\n']
Tokenized   (023): ['<s>', 'USER', '_', 'MAN', 'AG', 'EMENT', '_', 'R', 'OL', 'ES', '=', 'SHAR', 'ING', '_', 'R', 'OL', 'ES', '+', '[', ']', '\\', 'n', '</s>']
Filtered   (021): ['USER', '_', 'MAN', 'AG', 'EMENT', '_', 'R', 'OL', 'ES', '=', 'SHAR', 'ING', '_', 'R', 'OL', 'ES', '+', '[', ']', '\\', 'n']
Detokenized (007): ['USER_MANAGEMENT_ROLES', '=', 'SHARING_ROLES', '+', '[', ']', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "_DEFAULT_SHARING_ROLES = SHARING_ROLES [ : ] \n"
Original    (007): ['_DEFAULT_SHARING_ROLES', '=', 'SHARING_ROLES', '[', ':', ']', '\\n']
Tokenized   (025): ['<s>', '_', 'DE', 'FAULT', '_', 'SH', 'AR', 'ING', '_', 'R', 'OL', 'ES', '=', 'SHAR', 'ING', '_', 'R', 'OL', 'ES', '[', ':', ']', '\\', 'n', '</s>']
Filtered   (023): ['_', 'DE', 'FAULT', '_', 'SH', 'AR', 'ING', '_', 'R', 'OL', 'ES', '=', 'SHAR', 'ING', '_', 'R', 'OL', 'ES', '[', ':', ']', '\\', 'n']
Detokenized (007): ['_DEFAULT_SHARING_ROLES', '=', 'SHARING_ROLES', '[', ':', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "principal = get_principals ( ) . get ( name ) \n"
Original    (011): ['principal', '=', 'get_principals', '(', ')', '.', 'get', '(', 'name', ')', '\\n']
Tokenized   (021): ['<s>', 'pr', 'inc', 'ipal', '=', 'get', '_', 'pr', 'inc', 'ip', 'als', '(', ')', '.', 'get', '(', 'name', ')', '\\', 'n', '</s>']
Filtered   (019): ['pr', 'inc', 'ipal', '=', 'get', '_', 'pr', 'inc', 'ip', 'als', '(', ')', '.', 'get', '(', 'name', ')', '\\', 'n']
Detokenized (011): ['principal', '=', 'get_principals', '(', ')', '.', 'get', '(', 'name', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "lg for lg in context . local_groups \n"
Original    (008): ['lg', 'for', 'lg', 'in', 'context', '.', 'local_groups', '\\n']
Tokenized   (015): ['<s>', 'l', 'g', 'for', 'l', 'g', 'in', 'context', '.', 'local', '_', 'groups', '\\', 'n', '</s>']
Filtered   (013): ['l', 'g', 'for', 'l', 'g', 'in', 'context', '.', 'local', '_', 'groups', '\\', 'n']
Detokenized (008): ['lg', 'for', 'lg', 'in', 'context', '.', 'local_groups', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "LocalGroup ( context , name , unicode ( group_name ) ) \n"
Original    (012): ['LocalGroup', '(', 'context', ',', 'name', ',', 'unicode', '(', 'group_name', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'Local', 'Group', '(', 'context', ',', 'name', ',', 'unic', 'ode', '(', 'group', '_', 'name', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['Local', 'Group', '(', 'context', ',', 'name', ',', 'unic', 'ode', '(', 'group', '_', 'name', ')', ')', '\\', 'n']
Detokenized (012): ['LocalGroup', '(', 'context', ',', 'name', ',', 'unicode', '(', 'group_name', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "filters . append ( func . lower ( col ) . like ( value ) ) \n"
Original    (017): ['filters', '.', 'append', '(', 'func', '.', 'lower', '(', 'col', ')', '.', 'like', '(', 'value', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'fil', 'ters', '.', 'append', '(', 'func', '.', 'lower', '(', 'col', ')', '.', 'like', '(', 'value', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['fil', 'ters', '.', 'append', '(', 'func', '.', 'lower', '(', 'col', ')', '.', 'like', '(', 'value', ')', ')', '\\', 'n']
Detokenized (017): ['filters', '.', 'append', '(', 'func', '.', 'lower', '(', 'col', ')', '.', 'like', '(', 'value', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "bcrypt . hashpw ( password . encode ( ) , hashed . encode ( ) ) ) \n"
Original    (018): ['bcrypt', '.', 'hashpw', '(', 'password', '.', 'encode', '(', ')', ',', 'hashed', '.', 'encode', '(', ')', ')', ')', '\\n']
Tokenized   (025): ['<s>', 'bc', 'rypt', '.', 'hash', 'p', 'w', '(', 'password', '.', 'encode', '(', ')', ',', 'has', 'hed', '.', 'encode', '(', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (023): ['bc', 'rypt', '.', 'hash', 'p', 'w', '(', 'password', '.', 'encode', '(', ')', ',', 'has', 'hed', '.', 'encode', '(', ')', ')', ')', '\\', 'n']
Detokenized (018): ['bcrypt', '.', 'hashpw', '(', 'password', '.', 'encode', '(', ')', ',', 'hashed', '.', 'encode', '(', ')', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "browser . open ( . format ( BASE_URL ) ) \n"
Original    (011): ['browser', '.', 'open', '(', '.', 'format', '(', 'BASE_URL', ')', ')', '\\n']
Tokenized   (016): ['<s>', 'browser', '.', 'open', '(', '.', 'format', '(', 'BASE', '_', 'URL', ')', ')', '\\', 'n', '</s>']
Filtered   (014): ['browser', '.', 'open', '(', '.', 'format', '(', 'BASE', '_', 'URL', ')', ')', '\\', 'n']
Detokenized (011): ['browser', '.', 'open', '(', '.', 'format', '(', 'BASE_URL', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "AUTHORS = open ( os . path . join ( here , ) ) . read ( ) \n"
Original    (019): ['AUTHORS', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ')', '.', 'read', '(', ')', '\\n']
Tokenized   (024): ['<s>', 'A', 'UTH', 'ORS', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ')', '.', 'read', '(', ')', '\\', 'n', '</s>']
Filtered   (022): ['A', 'UTH', 'ORS', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ')', '.', 'read', '(', ')', '\\', 'n']
Detokenized (019): ['AUTHORS', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ')', '.', 'read', '(', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "fixed_boxes ) : \n"
Original    (004): ['fixed_boxes', ')', ':', '\\n']
Tokenized   (009): ['<s>', 'fixed', '_', 'boxes', ')', ':', '\\', 'n', '</s>']
Filtered   (007): ['fixed', '_', 'boxes', ')', ':', '\\', 'n']
Detokenized (004): ['fixed_boxes', ')', ':', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "resolve_percentages ( box , ( containing_block . width , containing_block . height ) ) \n"
Original    (015): ['resolve_percentages', '(', 'box', ',', '(', 'containing_block', '.', 'width', ',', 'containing_block', '.', 'height', ')', ')', '\\n']
Tokenized   (026): ['<s>', 'res', 'olve', '_', 'percent', 'ages', '(', 'box', ',', '(', 'containing', '_', 'block', '.', 'width', ',', 'containing', '_', 'block', '.', 'height', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['res', 'olve', '_', 'percent', 'ages', '(', 'box', ',', '(', 'containing', '_', 'block', '.', 'width', ',', 'containing', '_', 'block', '.', 'height', ')', ')', '\\', 'n']
Detokenized (015): ['resolve_percentages', '(', 'box', ',', '(', 'containing_block', '.', 'width', ',', 'containing_block', '.', 'height', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "box , _ , _ , _ , _ = block_container_layout ( \n"
Original    (013): ['box', ',', '_', ',', '_', ',', '_', ',', '_', '=', 'block_container_layout', '(', '\\n']
Tokenized   (020): ['<s>', 'box', ',', '_', ',', '_', ',', '_', ',', '_', '=', 'block', '_', 'container', '_', 'layout', '(', '\\', 'n', '</s>']
Filtered   (018): ['box', ',', '_', ',', '_', ',', '_', ',', '_', '=', 'block', '_', 'container', '_', 'layout', '(', '\\', 'n']
Detokenized (013): ['box', ',', '_', ',', '_', ',', '_', ',', '_', '=', 'block_container_layout', '(', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "skip_stack = None , device_size = device_size , page_is_empty = False , \n"
Original    (013): ['skip_stack', '=', 'None', ',', 'device_size', '=', 'device_size', ',', 'page_is_empty', '=', 'False', ',', '\\n']
Tokenized   (026): ['<s>', 'skip', '_', 'stack', '=', 'None', ',', 'device', '_', 'size', '=', 'device', '_', 'size', ',', 'page', '_', 'is', '_', 'empty', '=', 'False', ',', '\\', 'n', '</s>']
Filtered   (024): ['skip', '_', 'stack', '=', 'None', ',', 'device', '_', 'size', '=', 'device', '_', 'size', ',', 'page', '_', 'is', '_', 'empty', '=', 'False', ',', '\\', 'n']
Detokenized (013): ['skip_stack', '=', 'None', ',', 'device_size', '=', 'device_size', ',', 'page_is_empty', '=', 'False', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "list_marker_layout ( context , box ) \n"
Original    (007): ['list_marker_layout', '(', 'context', ',', 'box', ')', '\\n']
Tokenized   (015): ['<s>', 'list', '_', 'mark', 'er', '_', 'layout', '(', 'context', ',', 'box', ')', '\\', 'n', '</s>']
Filtered   (013): ['list', '_', 'mark', 'er', '_', 'layout', '(', 'context', ',', 'box', ')', '\\', 'n']
Detokenized (007): ['list_marker_layout', '(', 'context', ',', 'box', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "hypothetical_position = box . position_y + collapsed_margin \n"
Original    (008): ['hypothetical_position', '=', 'box', '.', 'position_y', '+', 'collapsed_margin', '\\n']
Tokenized   (019): ['<s>', 'hyp', 'ot', 'hetical', '_', 'position', '=', 'box', '.', 'position', '_', 'y', '+', 'collapsed', '_', 'margin', '\\', 'n', '</s>']
Filtered   (017): ['hyp', 'ot', 'hetical', '_', 'position', '=', 'box', '.', 'position', '_', 'y', '+', 'collapsed', '_', 'margin', '\\', 'n']
Detokenized (008): ['hypothetical_position', '=', 'box', '.', 'position_y', '+', 'collapsed_margin', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "box_width = box . margin_width ( ) if outer else box . border_width ( ) \n"
Original    (016): ['box_width', '=', 'box', '.', 'margin_width', '(', ')', 'if', 'outer', 'else', 'box', '.', 'border_width', '(', ')', '\\n']
Tokenized   (025): ['<s>', 'box', '_', 'width', '=', 'box', '.', 'margin', '_', 'width', '(', ')', 'if', 'outer', 'else', 'box', '.', 'border', '_', 'width', '(', ')', '\\', 'n', '</s>']
Filtered   (023): ['box', '_', 'width', '=', 'box', '.', 'margin', '_', 'width', '(', ')', 'if', 'outer', 'else', 'box', '.', 'border', '_', 'width', '(', ')', '\\', 'n']
Detokenized (016): ['box_width', '=', 'box', '.', 'margin_width', '(', ')', 'if', 'outer', 'else', 'box', '.', 'border_width', '(', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "max_right_bound -= box . margin_right \n"
Original    (006): ['max_right_bound', '-=', 'box', '.', 'margin_right', '\\n']
Tokenized   (015): ['<s>', 'max', '_', 'right', '_', 'bound', '-=', 'box', '.', 'margin', '_', 'right', '\\', 'n', '</s>']
Filtered   (013): ['max', '_', 'right', '_', 'bound', '-=', 'box', '.', 'margin', '_', 'right', '\\', 'n']
Detokenized (006): ['max_right_bound', '-=', 'box', '.', 'margin_right', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "shape . position_y + shape . margin_height ( ) \n"
Original    (010): ['shape', '.', 'position_y', '+', 'shape', '.', 'margin_height', '(', ')', '\\n']
Tokenized   (017): ['<s>', 'shape', '.', 'position', '_', 'y', '+', 'shape', '.', 'margin', '_', 'height', '(', ')', '\\', 'n', '</s>']
Filtered   (015): ['shape', '.', 'position', '_', 'y', '+', 'shape', '.', 'margin', '_', 'height', '(', ')', '\\', 'n']
Detokenized (010): ['shape', '.', 'position_y', '+', 'shape', '.', 'margin_height', '(', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "urlpatterns = patterns ( , \n"
Original    (006): ['urlpatterns', '=', 'patterns', '(', ',', '\\n']
Tokenized   (011): ['<s>', 'url', 'pattern', 's', '=', 'patterns', '(', ',', '\\', 'n', '</s>']
Filtered   (009): ['url', 'pattern', 's', '=', 'patterns', '(', ',', '\\', 'n']
Detokenized (006): ['urlpatterns', '=', 'patterns', '(', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "obj1 , obj2 = qs \n"
Original    (006): ['obj1', ',', 'obj2', '=', 'qs', '\\n']
Tokenized   (012): ['<s>', 'obj', '1', ',', 'obj', '2', '=', 'q', 's', '\\', 'n', '</s>']
Filtered   (010): ['obj', '1', ',', 'obj', '2', '=', 'q', 's', '\\', 'n']
Detokenized (006): ['obj1', ',', 'obj2', '=', 'qs', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "n1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n"
Original    (022): ['n1', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'pk', '=', 'self', '.', 'normal_id', '[', '1', ']', ')', '\\n']
Tokenized   (029): ['<s>', 'n', '1', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'p', 'k', '=', 'self', '.', 'normal', '_', 'id', '[', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (027): ['n', '1', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'p', 'k', '=', 'self', '.', 'normal', '_', 'id', '[', '1', ']', ')', '\\', 'n']
Detokenized (022): ['n1', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'pk', '=', 'self', '.', 'normal_id', '[', '1', ']', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "shared_field = NEW_SHARED , translated_field = NEW_TRANSLATED \n"
Original    (008): ['shared_field', '=', 'NEW_SHARED', ',', 'translated_field', '=', 'NEW_TRANSLATED', '\\n']
Tokenized   (024): ['<s>', 'shared', '_', 'field', '=', 'NEW', '_', 'SH', 'AR', 'ED', ',', 'translated', '_', 'field', '=', 'NEW', '_', 'TR', 'AN', 'SL', 'ATED', '\\', 'n', '</s>']
Filtered   (022): ['shared', '_', 'field', '=', 'NEW', '_', 'SH', 'AR', 'ED', ',', 'translated', '_', 'field', '=', 'NEW', '_', 'TR', 'AN', 'SL', 'ATED', '\\', 'n']
Detokenized (008): ['shared_field', '=', 'NEW_SHARED', ',', 'translated_field', '=', 'NEW_TRANSLATED', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "NORMAL [ 2 ] . shared_field ] ) \n"
Original    (009): ['NORMAL', '[', '2', ']', '.', 'shared_field', ']', ')', '\\n']
Tokenized   (015): ['<s>', 'NOR', 'MAL', '[', '2', ']', '.', 'shared', '_', 'field', ']', ')', '\\', 'n', '</s>']
Filtered   (013): ['NOR', 'MAL', '[', '2', ']', '.', 'shared', '_', 'field', ']', ')', '\\', 'n']
Detokenized (009): ['NORMAL', '[', '2', ']', '.', 'shared_field', ']', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "ja = Normal . objects . language ( ) . get ( pk = en . pk ) \n"
Original    (019): ['ja', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'pk', '=', 'en', '.', 'pk', ')', '\\n']
Tokenized   (024): ['<s>', 'ja', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'p', 'k', '=', 'en', '.', 'p', 'k', ')', '\\', 'n', '</s>']
Filtered   (022): ['ja', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'p', 'k', '=', 'en', '.', 'p', 'k', ')', '\\', 'n']
Detokenized (019): ['ja', '=', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'get', '(', 'pk', '=', 'en', '.', 'pk', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "AggregateModel . objects . language ( "en" ) . create ( number = 0 , translated_number = 0 ) \n"
Original    (020): ['AggregateModel', '.', 'objects', '.', 'language', '(', '"en"', ')', '.', 'create', '(', 'number', '=', '0', ',', 'translated_number', '=', '0', ')', '\\n']
Tokenized   (029): ['<s>', 'Agg', 'regate', 'Model', '.', 'objects', '.', 'language', '(', '"', 'en', '"', ')', '.', 'create', '(', 'number', '=', '0', ',', 'translated', '_', 'number', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (027): ['Agg', 'regate', 'Model', '.', 'objects', '.', 'language', '(', '"', 'en', '"', ')', '.', 'create', '(', 'number', '=', '0', ',', 'translated', '_', 'number', '=', '0', ')', '\\', 'n']
Detokenized (020): ['AggregateModel', '.', 'objects', '.', 'language', '(', '"en"', ')', '.', 'create', '(', 'number', '=', '0', ',', 'translated_number', '=', '0', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "shared_contains_two = Q ( shared_field__contains = ) \n"
Original    (008): ['shared_contains_two', '=', 'Q', '(', 'shared_field__contains', '=', ')', '\\n']
Tokenized   (021): ['<s>', 'shared', '_', 'cont', 'ains', '_', 'two', '=', 'Q', '(', 'shared', '_', 'field', '__', 'cont', 'ains', '=', ')', '\\', 'n', '</s>']
Filtered   (019): ['shared', '_', 'cont', 'ains', '_', 'two', '=', 'Q', '(', 'shared', '_', 'field', '__', 'cont', 'ains', '=', ')', '\\', 'n']
Detokenized (008): ['shared_contains_two', '=', 'Q', '(', 'shared_field__contains', '=', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "normal_one = Q ( normal_field = STANDARD [ 1 ] . normal_field ) \n"
Original    (014): ['normal_one', '=', 'Q', '(', 'normal_field', '=', 'STANDARD', '[', '1', ']', '.', 'normal_field', ')', '\\n']
Tokenized   (024): ['<s>', 'normal', '_', 'one', '=', 'Q', '(', 'normal', '_', 'field', '=', 'STAND', 'ARD', '[', '1', ']', '.', 'normal', '_', 'field', ')', '\\', 'n', '</s>']
Filtered   (022): ['normal', '_', 'one', '=', 'Q', '(', 'normal', '_', 'field', '=', 'STAND', 'ARD', '[', '1', ']', '.', 'normal', '_', 'field', ')', '\\', 'n']
Detokenized (014): ['normal_one', '=', 'Q', '(', 'normal_field', '=', 'STANDARD', '[', '1', ']', '.', 'normal_field', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "shared_one = Q ( normal__shared_field = NORMAL [ STANDARD [ 1 ] . normal ] . shared_field ) \n"
Original    (019): ['shared_one', '=', 'Q', '(', 'normal__shared_field', '=', 'NORMAL', '[', 'STANDARD', '[', '1', ']', '.', 'normal', ']', '.', 'shared_field', ')', '\\n']
Tokenized   (032): ['<s>', 'shared', '_', 'one', '=', 'Q', '(', 'normal', '__', 'shared', '_', 'field', '=', 'NOR', 'MAL', '[', 'STAND', 'ARD', '[', '1', ']', '.', 'normal', ']', '.', 'shared', '_', 'field', ')', '\\', 'n', '</s>']
Filtered   (030): ['shared', '_', 'one', '=', 'Q', '(', 'normal', '__', 'shared', '_', 'field', '=', 'NOR', 'MAL', '[', 'STAND', 'ARD', '[', '1', ']', '.', 'normal', ']', '.', 'shared', '_', 'field', ')', '\\', 'n']
Detokenized (019): ['shared_one', '=', 'Q', '(', 'normal__shared_field', '=', 'NORMAL', '[', 'STANDARD', '[', '1', ']', '.', 'normal', ']', '.', 'shared_field', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "translated_one_en = Q ( normal__translated_field = NORMAL [ STANDARD [ 1 ] . normal ] . translated_field [ translated_two_en = Q ( normal__translated_field = NORMAL [ STANDARD [ 2 ] . normal ] . translated_field [ \n"
Original    (037): ['translated_one_en', '=', 'Q', '(', 'normal__translated_field', '=', 'NORMAL', '[', 'STANDARD', '[', '1', ']', '.', 'normal', ']', '.', 'translated_field', '[', 'translated_two_en', '=', 'Q', '(', 'normal__translated_field', '=', 'NORMAL', '[', 'STANDARD', '[', '2', ']', '.', 'normal', ']', '.', 'translated_field', '[', '\\n']
Tokenized   (067): ['<s>', 'trans', 'lated', '_', 'one', '_', 'en', '=', 'Q', '(', 'normal', '__', 'trans', 'lated', '_', 'field', '=', 'NOR', 'MAL', '[', 'STAND', 'ARD', '[', '1', ']', '.', 'normal', ']', '.', 'translated', '_', 'field', '[', 'translated', '_', 'two', '_', 'en', '=', 'Q', '(', 'normal', '__', 'trans', 'lated', '_', 'field', '=', 'NOR', 'MAL', '[', 'STAND', 'ARD', '[', '2', ']', '.', 'normal', ']', '.', 'translated', '_', 'field', '[', '\\', 'n', '</s>']
Filtered   (065): ['trans', 'lated', '_', 'one', '_', 'en', '=', 'Q', '(', 'normal', '__', 'trans', 'lated', '_', 'field', '=', 'NOR', 'MAL', '[', 'STAND', 'ARD', '[', '1', ']', '.', 'normal', ']', '.', 'translated', '_', 'field', '[', 'translated', '_', 'two', '_', 'en', '=', 'Q', '(', 'normal', '__', 'trans', 'lated', '_', 'field', '=', 'NOR', 'MAL', '[', 'STAND', 'ARD', '[', '2', ']', '.', 'normal', ']', '.', 'translated', '_', 'field', '[', '\\', 'n']
Detokenized (037): ['translated_one_en', '=', 'Q', '(', 'normal__translated_field', '=', 'NORMAL', '[', 'STANDARD', '[', '1', ']', '.', 'normal', ']', '.', 'translated_field', '[', 'translated_two_en', '=', 'Q', '(', 'normal__translated_field', '=', 'NORMAL', '[', 'STANDARD', '[', '2', ']', '.', 'normal', ']', '.', 'translated_field', '[', '\\n']
Counter: 65
===================================================================
Hidden states:  (13, 37, 768)
# Extracted words:  37
Sentence         : "qs = manager . filter ( shared_one & ~ translated_two_en ) \n"
Original    (012): ['qs', '=', 'manager', '.', 'filter', '(', 'shared_one', '&', '~', 'translated_two_en', ')', '\\n']
Tokenized   (021): ['<s>', 'qs', '=', 'manager', '.', 'filter', '(', 'shared', '_', 'one', '&', '~', 'translated', '_', 'two', '_', 'en', ')', '\\', 'n', '</s>']
Filtered   (019): ['qs', '=', 'manager', '.', 'filter', '(', 'shared', '_', 'one', '&', '~', 'translated', '_', 'two', '_', 'en', ')', '\\', 'n']
Detokenized (012): ['qs', '=', 'manager', '.', 'filter', '(', 'shared_one', '&', '~', 'translated_two_en', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "Normal . objects . language ( ) . complex_filter , \n"
Original    (011): ['Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'complex_filter', ',', '\\n']
Tokenized   (016): ['<s>', 'Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'complex', '_', 'filter', ',', '\\', 'n', '</s>']
Filtered   (014): ['Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'complex', '_', 'filter', ',', '\\', 'n']
Detokenized (011): ['Normal', '.', 'objects', '.', 'language', '(', ')', '.', 'complex_filter', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "analytics . track ( user_id , "Activate" , { \n"
Original    (010): ['analytics', '.', 'track', '(', 'user_id', ',', '"Activate"', ',', '{', '\\n']
Tokenized   (019): ['<s>', 'analy', 'tics', '.', 'track', '(', 'user', '_', 'id', ',', '"', 'Activ', 'ate', '"', ',', '{', '\\', 'n', '</s>']
Filtered   (017): ['analy', 'tics', '.', 'track', '(', 'user', '_', 'id', ',', '"', 'Activ', 'ate', '"', ',', '{', '\\', 'n']
Detokenized (010): ['analytics', '.', 'track', '(', 'user_id', ',', '"Activate"', ',', '{', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sublime_plugin . WindowCommand . __init__ ( self , * args , ** kwargs ) \n"
Original    (015): ['sublime_plugin', '.', 'WindowCommand', '.', '__init__', '(', 'self', ',', '*', 'args', ',', '**', 'kwargs', ')', '\\n']
Tokenized   (026): ['<s>', 'sub', 'lime', '_', 'plugin', '.', 'Window', 'Command', '.', '__', 'init', '__', '(', 'self', ',', '*', 'args', ',', '**', 'k', 'w', 'args', ')', '\\', 'n', '</s>']
Filtered   (024): ['sub', 'lime', '_', 'plugin', '.', 'Window', 'Command', '.', '__', 'init', '__', '(', 'self', ',', '*', 'args', ',', '**', 'k', 'w', 'args', ')', '\\', 'n']
Detokenized (015): ['sublime_plugin', '.', 'WindowCommand', '.', '__init__', '(', 'self', ',', '*', 'args', ',', '**', 'kwargs', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : ""show_response" , { "title" : title , "text" : text } ) \n"
Original    (013): ['"show_response"', ',', '{', '"title"', ':', 'title', ',', '"text"', ':', 'text', '}', ')', '\\n']
Tokenized   (024): ['<s>', '"', 'show', '_', 'response', '"', ',', '{', '"', 'title', '"', ':', 'title', ',', '"', 'text', '"', ':', 'text', '}', ')', '\\', 'n', '</s>']
Filtered   (022): ['"', 'show', '_', 'response', '"', ',', '{', '"', 'title', '"', ':', 'title', ',', '"', 'text', '"', ':', 'text', '}', ')', '\\', 'n']
Detokenized (013): ['"show_response"', ',', '{', '"title"', ':', 'title', ',', '"text"', ':', 'text', '}', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "ensure_ascii = False \n"
Original    (004): ['ensure_ascii', '=', 'False', '\\n']
Tokenized   (012): ['<s>', 'ens', 'ure', '_', 'as', 'ci', 'i', '=', 'False', '\\', 'n', '</s>']
Filtered   (010): ['ens', 'ure', '_', 'as', 'ci', 'i', '=', 'False', '\\', 'n']
Detokenized (004): ['ensure_ascii', '=', 'False', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "syntax = "Packages/JavaScript/JSON.tmLanguage" ) \n"
Original    (005): ['syntax', '=', '"Packages/JavaScript/JSON.tmLanguage"', ')', '\\n']
Tokenized   (020): ['<s>', 'sy', 'ntax', '=', '"', 'Pack', 'ages', '/', 'Java', 'Script', '/', 'JSON', '.', 'tm', 'Language', '"', ')', '\\', 'n', '</s>']
Filtered   (018): ['sy', 'ntax', '=', '"', 'Pack', 'ages', '/', 'Java', 'Script', '/', 'JSON', '.', 'tm', 'Language', '"', ')', '\\', 'n']
Detokenized (005): ['syntax', '=', '"Packages/JavaScript/JSON.tmLanguage"', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "scroll = self . settings . scroll_size \n"
Original    (008): ['scroll', '=', 'self', '.', 'settings', '.', 'scroll_size', '\\n']
Tokenized   (013): ['<s>', 'scroll', '=', 'self', '.', 'settings', '.', 'scroll', '_', 'size', '\\', 'n', '</s>']
Filtered   (011): ['scroll', '=', 'self', '.', 'settings', '.', 'scroll', '_', 'size', '\\', 'n']
Detokenized (008): ['scroll', '=', 'self', '.', 'settings', '.', 'scroll_size', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : ""show_panel" , { "panel" : "output.elasticsearch" } ) \n"
Original    (009): ['"show_panel"', ',', '{', '"panel"', ':', '"output.elasticsearch"', '}', ')', '\\n']
Tokenized   (024): ['<s>', '"', 'show', '_', 'panel', '"', ',', '{', '"', 'panel', '"', ':', '"', 'output', '.', 'el', 'astic', 'search', '"', '}', ')', '\\', 'n', '</s>']
Filtered   (022): ['"', 'show', '_', 'panel', '"', ',', '{', '"', 'panel', '"', ':', '"', 'output', '.', 'el', 'astic', 'search', '"', '}', ')', '\\', 'n']
Detokenized (009): ['"show_panel"', ',', '{', '"panel"', ':', '"output.elasticsearch"', '}', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "panel . set_read_only ( True ) \n"
Original    (007): ['panel', '.', 'set_read_only', '(', 'True', ')', '\\n']
Tokenized   (014): ['<s>', 'panel', '.', 'set', '_', 'read', '_', 'only', '(', 'True', ')', '\\', 'n', '</s>']
Filtered   (012): ['panel', '.', 'set', '_', 'read', '_', 'only', '(', 'True', ')', '\\', 'n']
Detokenized (007): ['panel', '.', 'set_read_only', '(', 'True', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "400 : RequestError , \n"
Original    (005): ['400', ':', 'RequestError', ',', '\\n']
Tokenized   (009): ['<s>', '400', ':', 'Request', 'Error', ',', '\\', 'n', '</s>']
Filtered   (007): ['400', ':', 'Request', 'Error', ',', '\\', 'n']
Detokenized (005): ['400', ':', 'RequestError', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "TestConfigFileSource . ConcreteConfigFileSource ) \n"
Original    (005): ['TestConfigFileSource', '.', 'ConcreteConfigFileSource', ')', '\\n']
Tokenized   (015): ['<s>', 'Test', 'Config', 'File', 'Source', '.', 'Con', 'crete', 'Config', 'File', 'Source', ')', '\\', 'n', '</s>']
Filtered   (013): ['Test', 'Config', 'File', 'Source', '.', 'Con', 'crete', 'Config', 'File', 'Source', ')', '\\', 'n']
Detokenized (005): ['TestConfigFileSource', '.', 'ConcreteConfigFileSource', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "ReferenceReachabilityTester . TwoWayScopeReferenceAttacher . attach ( self . _scope_tree ) \n"
Original    (011): ['ReferenceReachabilityTester', '.', 'TwoWayScopeReferenceAttacher', '.', 'attach', '(', 'self', '.', '_scope_tree', ')', '\\n']
Tokenized   (027): ['<s>', 'Reference', 'Re', 'ach', 'ability', 'T', 'ester', '.', 'Two', 'Way', 'Scope', 'Reference', 'Att', 'acher', '.', 'attach', '(', 'self', '.', '_', 'scope', '_', 'tree', ')', '\\', 'n', '</s>']
Filtered   (025): ['Reference', 'Re', 'ach', 'ability', 'T', 'ester', '.', 'Two', 'Way', 'Scope', 'Reference', 'Att', 'acher', '.', 'attach', '(', 'self', '.', '_', 'scope', '_', 'tree', ')', '\\', 'n']
Detokenized (011): ['ReferenceReachabilityTester', '.', 'TwoWayScopeReferenceAttacher', '.', 'attach', '(', 'self', '.', '_scope_tree', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "declaring_id_node [ REFERECED_FLAG ] = True \n"
Original    (007): ['declaring_id_node', '[', 'REFERECED_FLAG', ']', '=', 'True', '\\n']
Tokenized   (020): ['<s>', 'decl', 'aring', '_', 'id', '_', 'node', '[', 'RE', 'FER', 'EC', 'ED', '_', 'FLAG', ']', '=', 'True', '\\', 'n', '</s>']
Filtered   (018): ['decl', 'aring', '_', 'id', '_', 'node', '[', 'RE', 'FER', 'EC', 'ED', '_', 'FLAG', ']', '=', 'True', '\\', 'n']
Detokenized (007): ['declaring_id_node', '[', 'REFERECED_FLAG', ']', '=', 'True', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "node_type = NodeType ( node [ ] ) \n"
Original    (009): ['node_type', '=', 'NodeType', '(', 'node', '[', ']', ')', '\\n']
Tokenized   (015): ['<s>', 'node', '_', 'type', '=', 'Node', 'Type', '(', 'node', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (013): ['node', '_', 'type', '=', 'Node', 'Type', '(', 'node', '[', ']', ')', '\\', 'n']
Detokenized (009): ['node_type', '=', 'NodeType', '(', 'node', '[', ']', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "is_set_cmd = excmd_node [ ] [ ] . get ( ) in SetCommandFamily \n"
Original    (014): ['is_set_cmd', '=', 'excmd_node', '[', ']', '[', ']', '.', 'get', '(', ')', 'in', 'SetCommandFamily', '\\n']
Tokenized   (026): ['<s>', 'is', '_', 'set', '_', 'cmd', '=', 'exc', 'md', '_', 'node', '[', ']', '[', ']', '.', 'get', '(', ')', 'in', 'Set', 'Command', 'Family', '\\', 'n', '</s>']
Filtered   (024): ['is', '_', 'set', '_', 'cmd', '=', 'exc', 'md', '_', 'node', '[', ']', '[', ']', '.', 'get', '(', ')', 'in', 'Set', 'Command', 'Family', '\\', 'n']
Detokenized (014): ['is_set_cmd', '=', 'excmd_node', '[', ']', '[', ']', '.', 'get', '(', ')', 'in', 'SetCommandFamily', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "option_name = re . match ( , option_expr ) . group ( 0 ) \n"
Original    (015): ['option_name', '=', 're', '.', 'match', '(', ',', 'option_expr', ')', '.', 'group', '(', '0', ')', '\\n']
Tokenized   (022): ['<s>', 'option', '_', 'name', '=', 're', '.', 'match', '(', ',', 'option', '_', 'expr', ')', '.', 'group', '(', '0', ')', '\\', 'n', '</s>']
Filtered   (020): ['option', '_', 'name', '=', 're', '.', 'match', '(', ',', 'option', '_', 'expr', ')', '.', 'group', '(', '0', ')', '\\', 'n']
Detokenized (015): ['option_name', '=', 're', '.', 'match', '(', ',', 'option_expr', ')', '.', 'group', '(', '0', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "is_valid = option_name not in AbbreviationsIncludingInvertPrefix \n"
Original    (007): ['is_valid', '=', 'option_name', 'not', 'in', 'AbbreviationsIncludingInvertPrefix', '\\n']
Tokenized   (023): ['<s>', 'is', '_', 'valid', '=', 'option', '_', 'name', 'not', 'in', 'Ab', 'bre', 'vi', 'ations', 'In', 'cluding', 'In', 'vert', 'Pref', 'ix', '\\', 'n', '</s>']
Filtered   (021): ['is', '_', 'valid', '=', 'option', '_', 'name', 'not', 'in', 'Ab', 'bre', 'vi', 'ations', 'In', 'cluding', 'In', 'vert', 'Pref', 'ix', '\\', 'n']
Detokenized (007): ['is_valid', '=', 'option_name', 'not', 'in', 'AbbreviationsIncludingInvertPrefix', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "stderr . setFormatter ( logging . Formatter ( \n"
Original    (009): ['stderr', '.', 'setFormatter', '(', 'logging', '.', 'Formatter', '(', '\\n']
Tokenized   (017): ['<s>', 'st', 'der', 'r', '.', 'set', 'Form', 'atter', '(', 'logging', '.', 'Form', 'atter', '(', '\\', 'n', '</s>']
Filtered   (015): ['st', 'der', 'r', '.', 'set', 'Form', 'atter', '(', 'logging', '.', 'Form', 'atter', '(', '\\', 'n']
Detokenized (009): ['stderr', '.', 'setFormatter', '(', 'logging', '.', 'Formatter', '(', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "level = level if level else os . environ . get ( , ) \n"
Original    (015): ['level', '=', 'level', 'if', 'level', 'else', 'os', '.', 'environ', '.', 'get', '(', ',', ')', '\\n']
Tokenized   (019): ['<s>', 'level', '=', 'level', 'if', 'level', 'else', 'os', '.', 'en', 'viron', '.', 'get', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (017): ['level', '=', 'level', 'if', 'level', 'else', 'os', '.', 'en', 'viron', '.', 'get', '(', ',', ')', '\\', 'n']
Detokenized (015): ['level', '=', 'level', 'if', 'level', 'else', 'os', '.', 'environ', '.', 'get', '(', ',', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "g_s = g0 * g_c * g_R * g_D * g_T * g_M \n"
Original    (014): ['g_s', '=', 'g0', '*', 'g_c', '*', 'g_R', '*', 'g_D', '*', 'g_T', '*', 'g_M', '\\n']
Tokenized   (030): ['<s>', 'g', '_', 's', '=', 'g', '0', '*', 'g', '_', 'c', '*', 'g', '_', 'R', '*', 'g', '_', 'D', '*', 'g', '_', 'T', '*', 'g', '_', 'M', '\\', 'n', '</s>']
Filtered   (028): ['g', '_', 's', '=', 'g', '0', '*', 'g', '_', 'c', '*', 'g', '_', 'R', '*', 'g', '_', 'D', '*', 'g', '_', 'T', '*', 'g', '_', 'M', '\\', 'n']
Detokenized (014): ['g_s', '=', 'g0', '*', 'g_c', '*', 'g_R', '*', 'g_D', '*', 'g_T', '*', 'g_M', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "g_T = ( ( TK - TL ) * ( TH - TK ) ** alpha_T ) / ( ( T0 - TL ) * ( TH - T0 ) ** alpha_T ) \n"
Original    (034): ['g_T', '=', '(', '(', 'TK', '-', 'TL', ')', '*', '(', 'TH', '-', 'TK', ')', '**', 'alpha_T', ')', '/', '(', '(', 'T0', '-', 'TL', ')', '*', '(', 'TH', '-', 'T0', ')', '**', 'alpha_T', ')', '\\n']
Tokenized   (047): ['<s>', 'g', '_', 'T', '=', '(', '(', 'T', 'K', '-', 'TL', ')', '*', '(', 'TH', '-', 'T', 'K', ')', '**', 'alpha', '_', 'T', ')', '/', '(', '(', 'T', '0', '-', 'TL', ')', '*', '(', 'TH', '-', 'T', '0', ')', '**', 'alpha', '_', 'T', ')', '\\', 'n', '</s>']
Filtered   (045): ['g', '_', 'T', '=', '(', '(', 'T', 'K', '-', 'TL', ')', '*', '(', 'TH', '-', 'T', 'K', ')', '**', 'alpha', '_', 'T', ')', '/', '(', '(', 'T', '0', '-', 'TL', ')', '*', '(', 'TH', '-', 'T', '0', ')', '**', 'alpha', '_', 'T', ')', '\\', 'n']
Detokenized (034): ['g_T', '=', '(', '(', 'TK', '-', 'TL', ')', '*', '(', 'TH', '-', 'TK', ')', '**', 'alpha_T', ')', '/', '(', '(', 'T0', '-', 'TL', ')', '*', '(', 'TH', '-', 'T0', ')', '**', 'alpha_T', ')', '\\n']
Counter: 45
===================================================================
Hidden states:  (13, 34, 768)
# Extracted words:  34
Sentence         : "r_a = AeroReist ( um , zm , z0 , d ) \n"
Original    (013): ['r_a', '=', 'AeroReist', '(', 'um', ',', 'zm', ',', 'z0', ',', 'd', ')', '\\n']
Tokenized   (022): ['<s>', 'r', '_', 'a', '=', 'Aero', 'Re', 'ist', '(', 'um', ',', 'z', 'm', ',', 'z', '0', ',', 'd', ')', '\\', 'n', '</s>']
Filtered   (020): ['r', '_', 'a', '=', 'Aero', 'Re', 'ist', '(', 'um', ',', 'z', 'm', ',', 'z', '0', ',', 'd', ')', '\\', 'n']
Detokenized (013): ['r_a', '=', 'AeroReist', '(', 'um', ',', 'zm', ',', 'z0', ',', 'd', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "r_s = SurfResist ( g0 , S , D , Tc , SM , SM0 ) \n"
Original    (017): ['r_s', '=', 'SurfResist', '(', 'g0', ',', 'S', ',', 'D', ',', 'Tc', ',', 'SM', ',', 'SM0', ')', '\\n']
Tokenized   (027): ['<s>', 'r', '_', 's', '=', 'Surf', 'Res', 'ist', '(', 'g', '0', ',', 'S', ',', 'D', ',', 'T', 'c', ',', 'SM', ',', 'SM', '0', ')', '\\', 'n', '</s>']
Filtered   (025): ['r', '_', 's', '=', 'Surf', 'Res', 'ist', '(', 'g', '0', ',', 'S', ',', 'D', ',', 'T', 'c', ',', 'SM', ',', 'SM', '0', ')', '\\', 'n']
Detokenized (017): ['r_s', '=', 'SurfResist', '(', 'g0', ',', 'S', ',', 'D', ',', 'Tc', ',', 'SM', ',', 'SM0', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "LE = ( delta * Rn + ( rho_a * cP * D ) / r_a ) / ( delta + gamma * ( 1.0 + r_s / r_a ) ) \n"
Original    (032): ['LE', '=', '(', 'delta', '*', 'Rn', '+', '(', 'rho_a', '*', 'cP', '*', 'D', ')', '/', 'r_a', ')', '/', '(', 'delta', '+', 'gamma', '*', '(', '1.0', '+', 'r_s', '/', 'r_a', ')', ')', '\\n']
Tokenized   (048): ['<s>', 'LE', '=', '(', 'delta', '*', 'R', 'n', '+', '(', 'r', 'ho', '_', 'a', '*', 'c', 'P', '*', 'D', ')', '/', 'r', '_', 'a', ')', '/', '(', 'delta', '+', 'gamma', '*', '(', '1', '.', '0', '+', 'r', '_', 's', '/', 'r', '_', 'a', ')', ')', '\\', 'n', '</s>']
Filtered   (046): ['LE', '=', '(', 'delta', '*', 'R', 'n', '+', '(', 'r', 'ho', '_', 'a', '*', 'c', 'P', '*', 'D', ')', '/', 'r', '_', 'a', ')', '/', '(', 'delta', '+', 'gamma', '*', '(', '1', '.', '0', '+', 'r', '_', 's', '/', 'r', '_', 'a', ')', ')', '\\', 'n']
Detokenized (032): ['LE', '=', '(', 'delta', '*', 'Rn', '+', '(', 'rho_a', '*', 'cP', '*', 'D', ')', '/', 'r_a', ')', '/', '(', 'delta', '+', 'gamma', '*', '(', '1.0', '+', 'r_s', '/', 'r_a', ')', ')', '\\n']
Counter: 46
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "glClearColor ( * background_color ) \n"
Original    (006): ['glClearColor', '(', '*', 'background_color', ')', '\\n']
Tokenized   (013): ['<s>', 'gl', 'Clear', 'Color', '(', '*', 'background', '_', 'color', ')', '\\', 'n', '</s>']
Filtered   (011): ['gl', 'Clear', 'Color', '(', '*', 'background', '_', 'color', ')', '\\', 'n']
Detokenized (006): ['glClearColor', '(', '*', 'background_color', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "glScissor ( x , y , width , height ) \n"
Original    (011): ['glScissor', '(', 'x', ',', 'y', ',', 'width', ',', 'height', ')', '\\n']
Tokenized   (017): ['<s>', 'gl', 'Sc', 'iss', 'or', '(', 'x', ',', 'y', ',', 'width', ',', 'height', ')', '\\', 'n', '</s>']
Filtered   (015): ['gl', 'Sc', 'iss', 'or', '(', 'x', ',', 'y', ',', 'width', ',', 'height', ')', '\\', 'n']
Detokenized (011): ['glScissor', '(', 'x', ',', 'y', ',', 'width', ',', 'height', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "glOrtho ( x , x + width , y , y + height , - 1 , 1 ) \n"
Original    (020): ['glOrtho', '(', 'x', ',', 'x', '+', 'width', ',', 'y', ',', 'y', '+', 'height', ',', '-', '1', ',', '1', ')', '\\n']
Tokenized   (026): ['<s>', 'gl', 'Or', 'th', 'o', '(', 'x', ',', 'x', '+', 'width', ',', 'y', ',', 'y', '+', 'height', ',', '-', '1', ',', '1', ')', '\\', 'n', '</s>']
Filtered   (024): ['gl', 'Or', 'th', 'o', '(', 'x', ',', 'x', '+', 'width', ',', 'y', ',', 'y', '+', 'height', ',', '-', '1', ',', '1', ')', '\\', 'n']
Detokenized (020): ['glOrtho', '(', 'x', ',', 'x', '+', 'width', ',', 'y', ',', 'y', '+', 'height', ',', '-', '1', ',', '1', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "glNormal3f ( 0 , 1. , 0 ) \n"
Original    (009): ['glNormal3f', '(', '0', ',', '1.', ',', '0', ')', '\\n']
Tokenized   (016): ['<s>', 'gl', 'Normal', '3', 'f', '(', '0', ',', '1', '.', ',', '0', ')', '\\', 'n', '</s>']
Filtered   (014): ['gl', 'Normal', '3', 'f', '(', '0', ',', '1', '.', ',', '0', ')', '\\', 'n']
Detokenized (009): ['glNormal3f', '(', '0', ',', '1.', ',', '0', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "glVertex ( n , n , p ) \n"
Original    (009): ['glVertex', '(', 'n', ',', 'n', ',', 'p', ')', '\\n']
Tokenized   (014): ['<s>', 'gl', 'Ver', 'tex', '(', 'n', ',', 'n', ',', 'p', ')', '\\', 'n', '</s>']
Filtered   (012): ['gl', 'Ver', 'tex', '(', 'n', ',', 'n', ',', 'p', ')', '\\', 'n']
Detokenized (009): ['glVertex', '(', 'n', ',', 'n', ',', 'p', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "companies = [ path for path in paths \n"
Original    (009): ['companies', '=', '[', 'path', 'for', 'path', 'in', 'paths', '\\n']
Tokenized   (013): ['<s>', 'compan', 'ies', '=', '[', 'path', 'for', 'path', 'in', 'paths', '\\', 'n', '</s>']
Filtered   (011): ['compan', 'ies', '=', '[', 'path', 'for', 'path', 'in', 'paths', '\\', 'n']
Detokenized (009): ['companies', '=', '[', 'path', 'for', 'path', 'in', 'paths', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "and os . path . exists ( os . path . join ( folder , path , ) ) ] \n"
Original    (021): ['and', 'os', '.', 'path', '.', 'exists', '(', 'os', '.', 'path', '.', 'join', '(', 'folder', ',', 'path', ',', ')', ')', ']', '\\n']
Tokenized   (024): ['<s>', 'and', 'os', '.', 'path', '.', 'exists', '(', 'os', '.', 'path', '.', 'join', '(', 'folder', ',', 'path', ',', ')', ')', ']', '\\', 'n', '</s>']
Filtered   (022): ['and', 'os', '.', 'path', '.', 'exists', '(', 'os', '.', 'path', '.', 'join', '(', 'folder', ',', 'path', ',', ')', ')', ']', '\\', 'n']
Detokenized (021): ['and', 'os', '.', 'path', '.', 'exists', '(', 'os', '.', 'path', '.', 'join', '(', 'folder', ',', 'path', ',', ')', ')', ']', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "folder = os . path . join ( root_folder , , , ) \n"
Original    (014): ['folder', '=', 'os', '.', 'path', '.', 'join', '(', 'root_folder', ',', ',', ',', ')', '\\n']
Tokenized   (019): ['<s>', 'folder', '=', 'os', '.', 'path', '.', 'join', '(', 'root', '_', 'folder', ',', ',', ',', ')', '\\', 'n', '</s>']
Filtered   (017): ['folder', '=', 'os', '.', 'path', '.', 'join', '(', 'root', '_', 'folder', ',', ',', ',', ')', '\\', 'n']
Detokenized (014): ['folder', '=', 'os', '.', 'path', '.', 'join', '(', 'root_folder', ',', ',', ',', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "scripts = [ , \n"
Original    (005): ['scripts', '=', '[', ',', '\\n']
Tokenized   (008): ['<s>', 'scripts', '=', '[', ',', '\\', 'n', '</s>']
Filtered   (006): ['scripts', '=', '[', ',', '\\', 'n']
Detokenized (005): ['scripts', '=', '[', ',', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "pro2 . predict ( ) from collections import OrderedDict \n"
Original    (010): ['pro2', '.', 'predict', '(', ')', 'from', 'collections', 'import', 'OrderedDict', '\\n']
Tokenized   (017): ['<s>', 'pro', '2', '.', 'predict', '(', ')', 'from', 'collections', 'import', 'Ord', 'ered', 'D', 'ict', '\\', 'n', '</s>']
Filtered   (015): ['pro', '2', '.', 'predict', '(', ')', 'from', 'collections', 'import', 'Ord', 'ered', 'D', 'ict', '\\', 'n']
Detokenized (010): ['pro2', '.', 'predict', '(', ')', 'from', 'collections', 'import', 'OrderedDict', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : ""dimensions." % ( self . __class__ . __name__ , shape ) ) \n"
Original    (013): ['"dimensions."', '%', '(', 'self', '.', '__class__', '.', '__name__', ',', 'shape', ')', ')', '\\n']
Tokenized   (023): ['<s>', '"', 'dim', 'ensions', '."', '%', '(', 'self', '.', '__', 'class', '__', '.', '__', 'name', '__', ',', 'shape', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['"', 'dim', 'ensions', '."', '%', '(', 'self', '.', '__', 'class', '__', '.', '__', 'name', '__', ',', 'shape', ')', ')', '\\', 'n']
Detokenized (013): ['"dimensions."', '%', '(', 'self', '.', '__class__', '.', '__name__', ',', 'shape', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "only = set ( tag for tag , value in tags . items ( ) if value ) \n"
Original    (019): ['only', '=', 'set', '(', 'tag', 'for', 'tag', ',', 'value', 'in', 'tags', '.', 'items', '(', ')', 'if', 'value', ')', '\\n']
Tokenized   (022): ['<s>', 'only', '=', 'set', '(', 'tag', 'for', 'tag', ',', 'value', 'in', 'tags', '.', 'items', '(', ')', 'if', 'value', ')', '\\', 'n', '</s>']
Filtered   (020): ['only', '=', 'set', '(', 'tag', 'for', 'tag', ',', 'value', 'in', 'tags', '.', 'items', '(', ')', 'if', 'value', ')', '\\', 'n']
Detokenized (019): ['only', '=', 'set', '(', 'tag', 'for', 'tag', ',', 'value', 'in', 'tags', '.', 'items', '(', ')', 'if', 'value', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "crop1 = [ None , , , ] \n"
Original    (009): ['crop1', '=', '[', 'None', ',', ',', ',', ']', '\\n']
Tokenized   (013): ['<s>', 'crop', '1', '=', '[', 'None', ',', ',', ',', ']', '\\', 'n', '</s>']
Filtered   (011): ['crop', '1', '=', '[', 'None', ',', ',', ',', ']', '\\', 'n']
Detokenized (009): ['crop1', '=', '[', 'None', ',', ',', ',', ']', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "outs = [ o . eval ( ) for o in outs ] \n"
Original    (014): ['outs', '=', '[', 'o', '.', 'eval', '(', ')', 'for', 'o', 'in', 'outs', ']', '\\n']
Tokenized   (017): ['<s>', 'outs', '=', '[', 'o', '.', 'eval', '(', ')', 'for', 'o', 'in', 'outs', ']', '\\', 'n', '</s>']
Filtered   (015): ['outs', '=', '[', 'o', '.', 'eval', '(', ')', 'for', 'o', 'in', 'outs', ']', '\\', 'n']
Detokenized (014): ['outs', '=', '[', 'o', '.', 'eval', '(', ')', 'for', 'o', 'in', 'outs', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "crop_test ( crop_x , [ x0 , x1 , x2 , x0 , x1 , x2 ] , \n"
Original    (019): ['crop_test', '(', 'crop_x', ',', '[', 'x0', ',', 'x1', ',', 'x2', ',', 'x0', ',', 'x1', ',', 'x2', ']', ',', '\\n']
Tokenized   (032): ['<s>', 'crop', '_', 'test', '(', 'crop', '_', 'x', ',', '[', 'x', '0', ',', 'x', '1', ',', 'x', '2', ',', 'x', '0', ',', 'x', '1', ',', 'x', '2', ']', ',', '\\', 'n', '</s>']
Filtered   (030): ['crop', '_', 'test', '(', 'crop', '_', 'x', ',', '[', 'x', '0', ',', 'x', '1', ',', 'x', '2', ',', 'x', '0', ',', 'x', '1', ',', 'x', '2', ']', ',', '\\', 'n']
Detokenized (019): ['crop_test', '(', 'crop_x', ',', '[', 'x0', ',', 'x1', ',', 'x2', ',', 'x0', ',', 'x1', ',', 'x2', ']', ',', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "cropping = [ ] * 2 ) \n"
Original    (008): ['cropping', '=', '[', ']', '*', '2', ')', '\\n']
Tokenized   (012): ['<s>', 'cro', 'pping', '=', '[', ']', '*', '2', ')', '\\', 'n', '</s>']
Filtered   (010): ['cro', 'pping', '=', '[', ']', '*', '2', ')', '\\', 'n']
Detokenized (008): ['cropping', '=', '[', ']', '*', '2', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "desired_result_0 = numpy . concatenate ( [ x0 [ : , : 2 ] , x1 [ : , : 2 ] ] , axis = 0 ) \n"
Original    (029): ['desired_result_0', '=', 'numpy', '.', 'concatenate', '(', '[', 'x0', '[', ':', ',', ':', '2', ']', ',', 'x1', '[', ':', ',', ':', '2', ']', ']', ',', 'axis', '=', '0', ')', '\\n']
Tokenized   (042): ['<s>', 'des', 'ired', '_', 'result', '_', '0', '=', 'n', 'umpy', '.', 'conc', 'aten', 'ate', '(', '[', 'x', '0', '[', ':', ',', ':', '2', ']', ',', 'x', '1', '[', ':', ',', ':', '2', ']', ']', ',', 'axis', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (040): ['des', 'ired', '_', 'result', '_', '0', '=', 'n', 'umpy', '.', 'conc', 'aten', 'ate', '(', '[', 'x', '0', '[', ':', ',', ':', '2', ']', ',', 'x', '1', '[', ':', ',', ':', '2', ']', ']', ',', 'axis', '=', '0', ')', '\\', 'n']
Detokenized (029): ['desired_result_0', '=', 'numpy', '.', 'concatenate', '(', '[', 'x0', '[', ':', ',', ':', '2', ']', ',', 'x1', '[', ':', ',', ':', '2', ']', ']', ',', 'axis', '=', '0', ')', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "desired_result_1 = numpy . concatenate ( [ x0 [ : 4 , : ] , x1 [ : 4 , : ] ] , axis = 1 ) \n"
Original    (029): ['desired_result_1', '=', 'numpy', '.', 'concatenate', '(', '[', 'x0', '[', ':', '4', ',', ':', ']', ',', 'x1', '[', ':', '4', ',', ':', ']', ']', ',', 'axis', '=', '1', ')', '\\n']
Tokenized   (042): ['<s>', 'des', 'ired', '_', 'result', '_', '1', '=', 'n', 'umpy', '.', 'conc', 'aten', 'ate', '(', '[', 'x', '0', '[', ':', '4', ',', ':', ']', ',', 'x', '1', '[', ':', '4', ',', ':', ']', ']', ',', 'axis', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (040): ['des', 'ired', '_', 'result', '_', '1', '=', 'n', 'umpy', '.', 'conc', 'aten', 'ate', '(', '[', 'x', '0', '[', ':', '4', ',', ':', ']', ',', 'x', '1', '[', ':', '4', ',', ':', ']', ']', ',', 'axis', '=', '1', ')', '\\', 'n']
Detokenized (029): ['desired_result_1', '=', 'numpy', '.', 'concatenate', '(', '[', 'x0', '[', ':', '4', ',', ':', ']', ',', 'x1', '[', ':', '4', ',', ':', ']', ']', ',', 'axis', '=', '1', ')', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "inputs = [ theano . shared ( a ) , \n"
Original    (011): ['inputs', '=', '[', 'theano', '.', 'shared', '(', 'a', ')', ',', '\\n']
Tokenized   (016): ['<s>', 'input', 's', '=', '[', 'the', 'ano', '.', 'shared', '(', 'a', ')', ',', '\\', 'n', '</s>']
Filtered   (014): ['input', 's', '=', '[', 'the', 'ano', '.', 'shared', '(', 'a', ')', ',', '\\', 'n']
Detokenized (011): ['inputs', '=', '[', 'theano', '.', 'shared', '(', 'a', ')', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "theano . shared ( b ) ] \n"
Original    (008): ['theano', '.', 'shared', '(', 'b', ')', ']', '\\n']
Tokenized   (012): ['<s>', 'the', 'ano', '.', 'shared', '(', 'b', ')', ']', '\\', 'n', '</s>']
Filtered   (010): ['the', 'ano', '.', 'shared', '(', 'b', ')', ']', '\\', 'n']
Detokenized (008): ['theano', '.', 'shared', '(', 'b', ')', ']', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "mask = os . urandom ( 4 ) if mask else None \n"
Original    (013): ['mask', '=', 'os', '.', 'urandom', '(', '4', ')', 'if', 'mask', 'else', 'None', '\\n']
Tokenized   (017): ['<s>', 'mask', '=', 'os', '.', 'ur', 'andom', '(', '4', ')', 'if', 'mask', 'else', 'None', '\\', 'n', '</s>']
Filtered   (015): ['mask', '=', 'os', '.', 'ur', 'andom', '(', '4', ')', 'if', 'mask', 'else', 'None', '\\', 'n']
Detokenized (013): ['mask', '=', 'os', '.', 'urandom', '(', '4', ')', 'if', 'mask', 'else', 'None', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "masking_key = mask , fin = 1 ) . build ( ) \n"
Original    (013): ['masking_key', '=', 'mask', ',', 'fin', '=', '1', ')', '.', 'build', '(', ')', '\\n']
Tokenized   (019): ['<s>', 'mask', 'ing', '_', 'key', '=', 'mask', ',', 'fin', '=', '1', ')', '.', 'build', '(', ')', '\\', 'n', '</s>']
Filtered   (017): ['mask', 'ing', '_', 'key', '=', 'mask', ',', 'fin', '=', '1', ')', '.', 'build', '(', ')', '\\', 'n']
Detokenized (013): ['masking_key', '=', 'mask', ',', 'fin', '=', '1', ')', '.', 'build', '(', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "fin = fin ) . build ( ) \n"
Original    (009): ['fin', '=', 'fin', ')', '.', 'build', '(', ')', '\\n']
Tokenized   (012): ['<s>', 'fin', '=', 'fin', ')', '.', 'build', '(', ')', '\\', 'n', '</s>']
Filtered   (010): ['fin', '=', 'fin', ')', '.', 'build', '(', ')', '\\', 'n']
Detokenized (009): ['fin', '=', 'fin', ')', '.', 'build', '(', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "flags = unpack ( , msg [ idx : idx + 4 ] ) [ 0 ] \n"
Original    (018): ['flags', '=', 'unpack', '(', ',', 'msg', '[', 'idx', ':', 'idx', '+', '4', ']', ')', '[', '0', ']', '\\n']
Tokenized   (024): ['<s>', 'flags', '=', 'un', 'pack', '(', ',', 'msg', '[', 'id', 'x', ':', 'id', 'x', '+', '4', ']', ')', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (022): ['flags', '=', 'un', 'pack', '(', ',', 'msg', '[', 'id', 'x', ':', 'id', 'x', '+', '4', ']', ')', '[', '0', ']', '\\', 'n']
Detokenized (018): ['flags', '=', 'unpack', '(', ',', 'msg', '[', 'idx', ':', 'idx', '+', '4', ']', ')', '[', '0', ']', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "pdc = req . get_options ( ) [ ] \n"
Original    (010): ['pdc', '=', 'req', '.', 'get_options', '(', ')', '[', ']', '\\n']
Tokenized   (016): ['<s>', 'p', 'dc', '=', 'req', '.', 'get', '_', 'options', '(', ')', '[', ']', '\\', 'n', '</s>']
Filtered   (014): ['p', 'dc', '=', 'req', '.', 'get', '_', 'options', '(', ')', '[', ']', '\\', 'n']
Detokenized (010): ['pdc', '=', 'req', '.', 'get_options', '(', ')', '[', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "bdc = req . get_options ( ) . get ( , False ) \n"
Original    (014): ['bdc', '=', 'req', '.', 'get_options', '(', ')', '.', 'get', '(', ',', 'False', ')', '\\n']
Tokenized   (020): ['<s>', 'bd', 'c', '=', 'req', '.', 'get', '_', 'options', '(', ')', '.', 'get', '(', ',', 'False', ')', '\\', 'n', '</s>']
Filtered   (018): ['bd', 'c', '=', 'req', '.', 'get', '_', 'options', '(', ')', '.', 'get', '(', ',', 'False', ')', '\\', 'n']
Detokenized (014): ['bdc', '=', 'req', '.', 'get_options', '(', ')', '.', 'get', '(', ',', 'False', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "decoded_path = urllib . unquote ( url . path ) [ 1 : ] \n"
Original    (015): ['decoded_path', '=', 'urllib', '.', 'unquote', '(', 'url', '.', 'path', ')', '[', '1', ':', ']', '\\n']
Tokenized   (024): ['<s>', 'dec', 'oded', '_', 'path', '=', 'ur', 'll', 'ib', '.', 'un', 'quote', '(', 'url', '.', 'path', ')', '[', '1', ':', ']', '\\', 'n', '</s>']
Filtered   (022): ['dec', 'oded', '_', 'path', '=', 'ur', 'll', 'ib', '.', 'un', 'quote', '(', 'url', '.', 'path', ')', '[', '1', ':', ']', '\\', 'n']
Detokenized (015): ['decoded_path', '=', 'urllib', '.', 'unquote', '(', 'url', '.', 'path', ')', '[', '1', ':', ']', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "rules = . join ( req . requires ( ) ) . strip ( ) \n"
Original    (016): ['rules', '=', '.', 'join', '(', 'req', '.', 'requires', '(', ')', ')', '.', 'strip', '(', ')', '\\n']
Tokenized   (019): ['<s>', 'rules', '=', '.', 'join', '(', 'req', '.', 'requires', '(', ')', ')', '.', 'strip', '(', ')', '\\', 'n', '</s>']
Filtered   (017): ['rules', '=', '.', 'join', '(', 'req', '.', 'requires', '(', ')', ')', '.', 'strip', '(', ')', '\\', 'n']
Detokenized (016): ['rules', '=', '.', 'join', '(', 'req', '.', 'requires', '(', ')', ')', '.', 'strip', '(', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n"
Original    (018): ['domain', '=', 'req', '.', 'get_options', '(', ')', '.', 'get', '(', ',', 'req', '.', 'auth_name', '(', ')', ')', '\\n']
Tokenized   (025): ['<s>', 'domain', '=', 'req', '.', 'get', '_', 'options', '(', ')', '.', 'get', '(', ',', 'req', '.', 'auth', '_', 'name', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (023): ['domain', '=', 'req', '.', 'get', '_', 'options', '(', ')', '.', 'get', '(', ',', 'req', '.', 'auth', '_', 'name', '(', ')', ')', '\\', 'n']
Detokenized (018): ['domain', '=', 'req', '.', 'get_options', '(', ')', '.', 'get', '(', ',', 'req', '.', 'auth_name', '(', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "auth_headers = req . headers_in . get ( , [ ] ) \n"
Original    (013): ['auth_headers', '=', 'req', '.', 'headers_in', '.', 'get', '(', ',', '[', ']', ')', '\\n']
Tokenized   (020): ['<s>', 'auth', '_', 'headers', '=', 'req', '.', 'headers', '_', 'in', '.', 'get', '(', ',', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (018): ['auth', '_', 'headers', '=', 'req', '.', 'headers', '_', 'in', '.', 'get', '(', ',', '[', ']', ')', '\\', 'n']
Detokenized (013): ['auth_headers', '=', 'req', '.', 'headers_in', '.', 'get', '(', ',', '[', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "set_remote_user ( req , ah_data [ 1 ] , domain ) \n"
Original    (012): ['set_remote_user', '(', 'req', ',', 'ah_data', '[', '1', ']', ',', 'domain', ')', '\\n']
Tokenized   (021): ['<s>', 'set', '_', 'remote', '_', 'user', '(', 'req', ',', 'ah', '_', 'data', '[', '1', ']', ',', 'domain', ')', '\\', 'n', '</s>']
Filtered   (019): ['set', '_', 'remote', '_', 'user', '(', 'req', ',', 'ah', '_', 'data', '[', '1', ']', ',', 'domain', ')', '\\', 'n']
Detokenized (012): ['set_remote_user', '(', 'req', ',', 'ah_data', '[', '1', ']', ',', 'domain', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "dict = json . loads ( request . data . decode ( ) ) \n"
Original    (015): ['dict', '=', 'json', '.', 'loads', '(', 'request', '.', 'data', '.', 'decode', '(', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'dict', '=', 'json', '.', 'loads', '(', 'request', '.', 'data', '.', 'decode', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['dict', '=', 'json', '.', 'loads', '(', 'request', '.', 'data', '.', 'decode', '(', ')', ')', '\\', 'n']
Detokenized (015): ['dict', '=', 'json', '.', 'loads', '(', 'request', '.', 'data', '.', 'decode', '(', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "rv = self . app . delete ( . format ( id ) ) \n"
Original    (015): ['rv', '=', 'self', '.', 'app', '.', 'delete', '(', '.', 'format', '(', 'id', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'r', 'v', '=', 'self', '.', 'app', '.', 'delete', '(', '.', 'format', '(', 'id', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['r', 'v', '=', 'self', '.', 'app', '.', 'delete', '(', '.', 'format', '(', 'id', ')', ')', '\\', 'n']
Detokenized (015): ['rv', '=', 'self', '.', 'app', '.', 'delete', '(', '.', 'format', '(', 'id', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "configure_logging ( "index_pages_logging.config" , "index_pages.log" ) \n"
Original    (007): ['configure_logging', '(', '"index_pages_logging.config"', ',', '"index_pages.log"', ')', '\\n']
Tokenized   (029): ['<s>', 'config', 'ure', '_', 'log', 'ging', '(', '"', 'index', '_', 'pages', '_', 'log', 'ging', '.', 'config', '"', ',', '"', 'index', '_', 'pages', '.', 'log', '"', ')', '\\', 'n', '</s>']
Filtered   (027): ['config', 'ure', '_', 'log', 'ging', '(', '"', 'index', '_', 'pages', '_', 'log', 'ging', '.', 'config', '"', ',', '"', 'index', '_', 'pages', '.', 'log', '"', ')', '\\', 'n']
Detokenized (007): ['configure_logging', '(', '"index_pages_logging.config"', ',', '"index_pages.log"', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "ocr_file = join ( dir , ) \n"
Original    (008): ['ocr_file', '=', 'join', '(', 'dir', ',', ')', '\\n']
Tokenized   (013): ['<s>', 'ocr', '_', 'file', '=', 'join', '(', 'dir', ',', ')', '\\', 'n', '</s>']
Filtered   (011): ['ocr', '_', 'file', '=', 'join', '(', 'dir', ',', ')', '\\', 'n']
Detokenized (008): ['ocr_file', '=', 'join', '(', 'dir', ',', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "expected_text = { "eng" : file ( join ( dir , ) ) . read ( ) . decode ( ) } \n"
Original    (023): ['expected_text', '=', '{', '"eng"', ':', 'file', '(', 'join', '(', 'dir', ',', ')', ')', '.', 'read', '(', ')', '.', 'decode', '(', ')', '}', '\\n']
Tokenized   (030): ['<s>', 'expected', '_', 'text', '=', '{', '"', 'eng', '"', ':', 'file', '(', 'join', '(', 'dir', ',', ')', ')', '.', 'read', '(', ')', '.', 'decode', '(', ')', '}', '\\', 'n', '</s>']
Filtered   (028): ['expected', '_', 'text', '=', '{', '"', 'eng', '"', ':', 'file', '(', 'join', '(', 'dir', ',', ')', ')', '.', 'read', '(', ')', '.', 'decode', '(', ')', '}', '\\', 'n']
Detokenized (023): ['expected_text', '=', '{', '"eng"', ':', 'file', '(', 'join', '(', 'dir', ',', ')', ')', '.', 'read', '(', ')', '.', 'decode', '(', ')', '}', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "tuples . append ( ( os . path . join ( root , pair [ 0 ] ) , np . int32 ( pair [ 1 ] ) ) ) \n"
Original    (031): ['tuples', '.', 'append', '(', '(', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'pair', '[', '0', ']', ')', ',', 'np', '.', 'int32', '(', 'pair', '[', '1', ']', ')', ')', ')', '\\n']
Tokenized   (036): ['<s>', 'tu', 'ples', '.', 'append', '(', '(', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'pair', '[', '0', ']', ')', ',', 'np', '.', 'int', '32', '(', 'pair', '[', '1', ']', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (034): ['tu', 'ples', '.', 'append', '(', '(', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'pair', '[', '0', ']', ')', ',', 'np', '.', 'int', '32', '(', 'pair', '[', '1', ']', ')', ')', ')', '\\', 'n']
Detokenized (031): ['tuples', '.', 'append', '(', '(', 'os', '.', 'path', '.', 'join', '(', 'root', ',', 'pair', '[', '0', ']', ')', ',', 'np', '.', 'int32', '(', 'pair', '[', '1', ']', ')', ')', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 31, 768)
# Extracted words:  31
Sentence         : "val_list = load_image_list ( args . val , args . root ) \n"
Original    (013): ['val_list', '=', 'load_image_list', '(', 'args', '.', 'val', ',', 'args', '.', 'root', ')', '\\n']
Tokenized   (022): ['<s>', 'val', '_', 'list', '=', 'load', '_', 'image', '_', 'list', '(', 'args', '.', 'val', ',', 'args', '.', 'root', ')', '\\', 'n', '</s>']
Filtered   (020): ['val', '_', 'list', '=', 'load', '_', 'image', '_', 'list', '(', 'args', '.', 'val', ',', 'args', '.', 'root', ')', '\\', 'n']
Detokenized (013): ['val_list', '=', 'load_image_list', '(', 'args', '.', 'val', ',', 'args', '.', 'root', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "mean_image = pickle . load ( open ( args . mean , ) ) \n"
Original    (015): ['mean_image', '=', 'pickle', '.', 'load', '(', 'open', '(', 'args', '.', 'mean', ',', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'mean', '_', 'image', '=', 'pick', 'le', '.', 'load', '(', 'open', '(', 'args', '.', 'mean', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['mean', '_', 'image', '=', 'pick', 'le', '.', 'load', '(', 'open', '(', 'args', '.', 'mean', ',', ')', ')', '\\', 'n']
Detokenized (015): ['mean_image', '=', 'pickle', '.', 'load', '(', 'open', '(', 'args', '.', 'mean', ',', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "cropwidth = 256 - model . insize \n"
Original    (008): ['cropwidth', '=', '256', '-', 'model', '.', 'insize', '\\n']
Tokenized   (013): ['<s>', 'crop', 'width', '=', '256', '-', 'model', '.', 'ins', 'ize', '\\', 'n', '</s>']
Filtered   (011): ['crop', 'width', '=', '256', '-', 'model', '.', 'ins', 'ize', '\\', 'n']
Detokenized (008): ['cropwidth', '=', '256', '-', 'model', '.', 'insize', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "left = random . randint ( 0 , cropwidth - 1 ) \n"
Original    (013): ['left', '=', 'random', '.', 'randint', '(', '0', ',', 'cropwidth', '-', '1', ')', '\\n']
Tokenized   (018): ['<s>', 'left', '=', 'random', '.', 'rand', 'int', '(', '0', ',', 'crop', 'width', '-', '1', ')', '\\', 'n', '</s>']
Filtered   (016): ['left', '=', 'random', '.', 'rand', 'int', '(', '0', ',', 'crop', 'width', '-', '1', ')', '\\', 'n']
Detokenized (013): ['left', '=', 'random', '.', 'randint', '(', '0', ',', 'cropwidth', '-', '1', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "image /= 255 \n"
Original    (004): ['image', '/=', '255', '\\n']
Tokenized   (008): ['<s>', 'image', '/', '=', '255', '\\', 'n', '</s>']
Filtered   (006): ['image', '/', '=', '255', '\\', 'n']
Detokenized (004): ['image', '/=', '255', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "val_batch_pool = [ None ] * args . val_batchsize \n"
Original    (010): ['val_batch_pool', '=', '[', 'None', ']', '*', 'args', '.', 'val_batchsize', '\\n']
Tokenized   (020): ['<s>', 'val', '_', 'batch', '_', 'pool', '=', '[', 'None', ']', '*', 'args', '.', 'val', '_', 'batch', 'size', '\\', 'n', '</s>']
Filtered   (018): ['val', '_', 'batch', '_', 'pool', '=', '[', 'None', ']', '*', 'args', '.', 'val', '_', 'batch', 'size', '\\', 'n']
Detokenized (010): ['val_batch_pool', '=', '[', 'None', ']', '*', 'args', '.', 'val_batchsize', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "perm = np . random . permutation ( len ( train_list ) ) \n"
Original    (014): ['perm', '=', 'np', '.', 'random', '.', 'permutation', '(', 'len', '(', 'train_list', ')', ')', '\\n']
Tokenized   (020): ['<s>', 'perm', '=', 'np', '.', 'random', '.', 'perm', 'utation', '(', 'len', '(', 'train', '_', 'list', ')', ')', '\\', 'n', '</s>']
Filtered   (018): ['perm', '=', 'np', '.', 'random', '.', 'perm', 'utation', '(', 'len', '(', 'train', '_', 'list', ')', ')', '\\', 'n']
Detokenized (014): ['perm', '=', 'np', '.', 'random', '.', 'permutation', '(', 'len', '(', 'train_list', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "batch_pool [ i ] = pool . apply_async ( read_image , ( path , False , True ) ) \n"
Original    (020): ['batch_pool', '[', 'i', ']', '=', 'pool', '.', 'apply_async', '(', 'read_image', ',', '(', 'path', ',', 'False', ',', 'True', ')', ')', '\\n']
Tokenized   (030): ['<s>', 'batch', '_', 'pool', '[', 'i', ']', '=', 'pool', '.', 'apply', '_', 'as', 'ync', '(', 'read', '_', 'image', ',', '(', 'path', ',', 'False', ',', 'True', ')', ')', '\\', 'n', '</s>']
Filtered   (028): ['batch', '_', 'pool', '[', 'i', ']', '=', 'pool', '.', 'apply', '_', 'as', 'ync', '(', 'read', '_', 'image', ',', '(', 'path', ',', 'False', ',', 'True', ')', ')', '\\', 'n']
Detokenized (020): ['batch_pool', '[', 'i', ']', '=', 'pool', '.', 'apply_async', '(', 'read_image', ',', '(', 'path', ',', 'False', ',', 'True', ')', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "read_image , ( path , True , False ) ) \n"
Original    (011): ['read_image', ',', '(', 'path', ',', 'True', ',', 'False', ')', ')', '\\n']
Tokenized   (016): ['<s>', 'read', '_', 'image', ',', '(', 'path', ',', 'True', ',', 'False', ')', ')', '\\', 'n', '</s>']
Filtered   (014): ['read', '_', 'image', ',', '(', 'path', ',', 'True', ',', 'False', ')', ')', '\\', 'n']
Detokenized (011): ['read_image', ',', '(', 'path', ',', 'True', ',', 'False', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "val_count = val_loss = val_accuracy = 0 \n"
Original    (008): ['val_count', '=', 'val_loss', '=', 'val_accuracy', '=', '0', '\\n']
Tokenized   (018): ['<s>', 'val', '_', 'count', '=', 'val', '_', 'loss', '=', 'val', '_', 'acc', 'uracy', '=', '0', '\\', 'n', '</s>']
Filtered   (016): ['val', '_', 'count', '=', 'val', '_', 'loss', '=', 'val', '_', 'acc', 'uracy', '=', '0', '\\', 'n']
Detokenized (008): ['val_count', '=', 'val_loss', '=', 'val_accuracy', '=', '0', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "duration = time . time ( ) - val_begin_at \n"
Original    (010): ['duration', '=', 'time', '.', 'time', '(', ')', '-', 'val_begin_at', '\\n']
Tokenized   (017): ['<s>', 'duration', '=', 'time', '.', 'time', '(', ')', '-', 'val', '_', 'begin', '_', 'at', '\\', 'n', '</s>']
Filtered   (015): ['duration', '=', 'time', '.', 'time', '(', ')', '-', 'val', '_', 'begin', '_', 'at', '\\', 'n']
Detokenized (010): ['duration', '=', 'time', '.', 'time', '(', ')', '-', 'val_begin_at', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "mean_error = 1 - val_accuracy * args . val_batchsize / 50000 \n"
Original    (012): ['mean_error', '=', '1', '-', 'val_accuracy', '*', 'args', '.', 'val_batchsize', '/', '50000', '\\n']
Tokenized   (024): ['<s>', 'mean', '_', 'error', '=', '1', '-', 'val', '_', 'acc', 'uracy', '*', 'args', '.', 'val', '_', 'batch', 'size', '/', '5', '0000', '\\', 'n', '</s>']
Filtered   (022): ['mean', '_', 'error', '=', '1', '-', 'val', '_', 'acc', 'uracy', '*', 'args', '.', 'val', '_', 'batch', 'size', '/', '5', '0000', '\\', 'n']
Detokenized (012): ['mean_error', '=', '1', '-', 'val_accuracy', '*', 'args', '.', 'val_batchsize', '/', '50000', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "serializers . save_hdf5 ( args . outstate , optimizer ) \n"
Original    (011): ['serializers', '.', 'save_hdf5', '(', 'args', '.', 'outstate', ',', 'optimizer', ')', '\\n']
Tokenized   (021): ['<s>', 'serial', 'izers', '.', 'save', '_', 'h', 'df', '5', '(', 'args', '.', 'out', 'state', ',', 'optim', 'izer', ')', '\\', 'n', '</s>']
Filtered   (019): ['serial', 'izers', '.', 'save', '_', 'h', 'df', '5', '(', 'args', '.', 'out', 'state', ',', 'optim', 'izer', ')', '\\', 'n']
Detokenized (011): ['serializers', '.', 'save_hdf5', '(', 'args', '.', 'outstate', ',', 'optimizer', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "boards_name = [ slugify ( b [ ] ) for b in SETTINGS . get ( , { } ) . values ( ) ] \n"
Original    (026): ['boards_name', '=', '[', 'slugify', '(', 'b', '[', ']', ')', 'for', 'b', 'in', 'SETTINGS', '.', 'get', '(', ',', '{', '}', ')', '.', 'values', '(', ')', ']', '\\n']
Tokenized   (034): ['<s>', 'boards', '_', 'name', '=', '[', 'slug', 'ify', '(', 'b', '[', ']', ')', 'for', 'b', 'in', 'SET', 'T', 'INGS', '.', 'get', '(', ',', '{', '}', ')', '.', 'values', '(', ')', ']', '\\', 'n', '</s>']
Filtered   (032): ['boards', '_', 'name', '=', '[', 'slug', 'ify', '(', 'b', '[', ']', ')', 'for', 'b', 'in', 'SET', 'T', 'INGS', '.', 'get', '(', ',', '{', '}', ')', '.', 'values', '(', ')', ']', '\\', 'n']
Detokenized (026): ['boards_name', '=', '[', 'slugify', '(', 'b', '[', ']', ')', 'for', 'b', 'in', 'SETTINGS', '.', 'get', '(', ',', '{', '}', ')', '.', 'values', '(', ')', ']', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "LOGGING . info ( . format ( board_name , result ) ) \n"
Original    (013): ['LOGGING', '.', 'info', '(', '.', 'format', '(', 'board_name', ',', 'result', ')', ')', '\\n']
Tokenized   (020): ['<s>', 'LOG', 'G', 'ING', '.', 'info', '(', '.', 'format', '(', 'board', '_', 'name', ',', 'result', ')', ')', '\\', 'n', '</s>']
Filtered   (018): ['LOG', 'G', 'ING', '.', 'info', '(', '.', 'format', '(', 'board', '_', 'name', ',', 'result', ')', ')', '\\', 'n']
Detokenized (013): ['LOGGING', '.', 'info', '(', '.', 'format', '(', 'board_name', ',', 'result', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "is_py2 = _ver [ 0 ] == 2 \n"
Original    (009): ['is_py2', '=', '_ver', '[', '0', ']', '==', '2', '\\n']
Tokenized   (016): ['<s>', 'is', '_', 'py', '2', '=', '_', 'ver', '[', '0', ']', '==', '2', '\\', 'n', '</s>']
Filtered   (014): ['is', '_', 'py', '2', '=', '_', 'ver', '[', '0', ']', '==', '2', '\\', 'n']
Detokenized (009): ['is_py2', '=', '_ver', '[', '0', ']', '==', '2', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "is_py2_7_9_or_later = _ver [ 0 ] >= 2 and _ver [ 1 ] >= 7 and _ver [ 2 ] >= 9 \n"
Original    (023): ['is_py2_7_9_or_later', '=', '_ver', '[', '0', ']', '>=', '2', 'and', '_ver', '[', '1', ']', '>=', '7', 'and', '_ver', '[', '2', ']', '>=', '9', '\\n']
Tokenized   (040): ['<s>', 'is', '_', 'py', '2', '_', '7', '_', '9', '_', 'or', '_', 'later', '=', '_', 'ver', '[', '0', ']', '>=', '2', 'and', '_', 'ver', '[', '1', ']', '>=', '7', 'and', '_', 'ver', '[', '2', ']', '>=', '9', '\\', 'n', '</s>']
Filtered   (038): ['is', '_', 'py', '2', '_', '7', '_', '9', '_', 'or', '_', 'later', '=', '_', 'ver', '[', '0', ']', '>=', '2', 'and', '_', 'ver', '[', '1', ']', '>=', '7', 'and', '_', 'ver', '[', '2', ']', '>=', '9', '\\', 'n']
Detokenized (023): ['is_py2_7_9_or_later', '=', '_ver', '[', '0', ']', '>=', '2', 'and', '_ver', '[', '1', ']', '>=', '7', 'and', '_ver', '[', '2', ']', '>=', '9', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "is_py3_3 = is_py3 and _ver [ 1 ] == 3 \n"
Original    (011): ['is_py3_3', '=', 'is_py3', 'and', '_ver', '[', '1', ']', '==', '3', '\\n']
Tokenized   (023): ['<s>', 'is', '_', 'py', '3', '_', '3', '=', 'is', '_', 'py', '3', 'and', '_', 'ver', '[', '1', ']', '==', '3', '\\', 'n', '</s>']
Filtered   (021): ['is', '_', 'py', '3', '_', '3', '=', 'is', '_', 'py', '3', 'and', '_', 'ver', '[', '1', ']', '==', '3', '\\', 'n']
Detokenized (011): ['is_py3_3', '=', 'is_py3', 'and', '_ver', '[', '1', ']', '==', '3', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "strategy = zlib . Z_DEFAULT_STRATEGY ) : \n"
Original    (008): ['strategy', '=', 'zlib', '.', 'Z_DEFAULT_STRATEGY', ')', ':', '\\n']
Tokenized   (020): ['<s>', 'str', 'ategy', '=', 'z', 'lib', '.', 'Z', '_', 'DE', 'FAULT', '_', 'STR', 'ATE', 'GY', ')', ':', '\\', 'n', '</s>']
Filtered   (018): ['str', 'ategy', '=', 'z', 'lib', '.', 'Z', '_', 'DE', 'FAULT', '_', 'STR', 'ATE', 'GY', ')', ':', '\\', 'n']
Detokenized (008): ['strategy', '=', 'zlib', '.', 'Z_DEFAULT_STRATEGY', ')', ':', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "secure = self . secure \n"
Original    (006): ['secure', '=', 'self', '.', 'secure', '\\n']
Tokenized   (009): ['<s>', 'secure', '=', 'self', '.', 'secure', '\\', 'n', '</s>']
Filtered   (007): ['secure', '=', 'self', '.', 'secure', '\\', 'n']
Detokenized (006): ['secure', '=', 'self', '.', 'secure', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "e . huffman_coder = HuffmanEncoder ( REQUEST_CODES , REQUEST_CODES_LENGTH ) \n"
Original    (011): ['e', '.', 'huffman_coder', '=', 'HuffmanEncoder', '(', 'REQUEST_CODES', ',', 'REQUEST_CODES_LENGTH', ')', '\\n']
Tokenized   (035): ['<s>', 'e', '.', 'h', 'uff', 'man', '_', 'c', 'oder', '=', 'Huff', 'man', 'Enc', 'oder', '(', 'RE', 'QUEST', '_', 'C', 'OD', 'ES', ',', 'RE', 'QUEST', '_', 'C', 'OD', 'ES', '_', 'L', 'ENGTH', ')', '\\', 'n', '</s>']
Filtered   (033): ['e', '.', 'h', 'uff', 'man', '_', 'c', 'oder', '=', 'Huff', 'man', 'Enc', 'oder', '(', 'RE', 'QUEST', '_', 'C', 'OD', 'ES', ',', 'RE', 'QUEST', '_', 'C', 'OD', 'ES', '_', 'L', 'ENGTH', ')', '\\', 'n']
Detokenized (011): ['e', '.', 'huffman_coder', '=', 'HuffmanEncoder', '(', 'REQUEST_CODES', ',', 'REQUEST_CODES_LENGTH', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "train_seq = corpus . read_sequence_list_conll ( input_data , max_sent_len = 15 , max_nr_sent = 1000 ) \n"
Original    (017): ['train_seq', '=', 'corpus', '.', 'read_sequence_list_conll', '(', 'input_data', ',', 'max_sent_len', '=', '15', ',', 'max_nr_sent', '=', '1000', ')', '\\n']
Tokenized   (039): ['<s>', 'train', '_', 'seq', '=', 'corpus', '.', 'read', '_', 'sequence', '_', 'list', '_', 'con', 'll', '(', 'input', '_', 'data', ',', 'max', '_', 'sent', '_', 'len', '=', '15', ',', 'max', '_', 'nr', '_', 'sent', '=', '1000', ')', '\\', 'n', '</s>']
Filtered   (037): ['train', '_', 'seq', '=', 'corpus', '.', 'read', '_', 'sequence', '_', 'list', '_', 'con', 'll', '(', 'input', '_', 'data', ',', 'max', '_', 'sent', '_', 'len', '=', '15', ',', 'max', '_', 'nr', '_', 'sent', '=', '1000', ')', '\\', 'n']
Detokenized (017): ['train_seq', '=', 'corpus', '.', 'read_sequence_list_conll', '(', 'input_data', ',', 'max_sent_len', '=', '15', ',', 'max_nr_sent', '=', '1000', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "pickle . dump ( ( corpus . word_dict , corpus . tag_dict ) , open ( , ) ) \n"
Original    (020): ['pickle', '.', 'dump', '(', '(', 'corpus', '.', 'word_dict', ',', 'corpus', '.', 'tag_dict', ')', ',', 'open', '(', ',', ')', ')', '\\n']
Tokenized   (028): ['<s>', 'pick', 'le', '.', 'dump', '(', '(', 'corpus', '.', 'word', '_', 'dict', ',', 'corpus', '.', 'tag', '_', 'dict', ')', ',', 'open', '(', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (026): ['pick', 'le', '.', 'dump', '(', '(', 'corpus', '.', 'word', '_', 'dict', ',', 'corpus', '.', 'tag', '_', 'dict', ')', ',', 'open', '(', ',', ')', ')', '\\', 'n']
Detokenized (020): ['pickle', '.', 'dump', '(', '(', 'corpus', '.', 'word_dict', ',', 'corpus', '.', 'tag_dict', ')', ',', 'open', '(', ',', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "total = np . zeros ( self . features . n_feats ) \n"
Original    (013): ['total', '=', 'np', '.', 'zeros', '(', 'self', '.', 'features', '.', 'n_feats', ')', '\\n']
Tokenized   (020): ['<s>', 'total', '=', 'np', '.', 'z', 'eros', '(', 'self', '.', 'features', '.', 'n', '_', 'fe', 'ats', ')', '\\', 'n', '</s>']
Filtered   (018): ['total', '=', 'np', '.', 'z', 'eros', '(', 'self', '.', 'features', '.', 'n', '_', 'fe', 'ats', ')', '\\', 'n']
Detokenized (013): ['total', '=', 'np', '.', 'zeros', '(', 'self', '.', 'features', '.', 'n_feats', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "scores = self . features . compute_scores ( feats , self . weights ) \n"
Original    (015): ['scores', '=', 'self', '.', 'features', '.', 'compute_scores', '(', 'feats', ',', 'self', '.', 'weights', ')', '\\n']
Tokenized   (022): ['<s>', 'sc', 'ores', '=', 'self', '.', 'features', '.', 'compute', '_', 'sc', 'ores', '(', 'feats', ',', 'self', '.', 'weights', ')', '\\', 'n', '</s>']
Filtered   (020): ['sc', 'ores', '=', 'self', '.', 'features', '.', 'compute', '_', 'sc', 'ores', '(', 'feats', ',', 'self', '.', 'weights', ')', '\\', 'n']
Detokenized (015): ['scores', '=', 'self', '.', 'features', '.', 'compute_scores', '(', 'feats', ',', 'self', '.', 'weights', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "t0 = 1.0 / ( sigma * eta0 ) \n"
Original    (010): ['t0', '=', '1.0', '/', '(', 'sigma', '*', 'eta0', ')', '\\n']
Tokenized   (019): ['<s>', 't', '0', '=', '1', '.', '0', '/', '(', 's', 'igma', '*', 'et', 'a', '0', ')', '\\', 'n', '</s>']
Filtered   (017): ['t', '0', '=', '1', '.', '0', '/', '(', 's', 'igma', '*', 'et', 'a', '0', ')', '\\', 'n']
Detokenized (010): ['t0', '=', '1.0', '/', '(', 'sigma', '*', 'eta0', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "marginals , logZ = self . decoder . parse_marginals_nonproj ( scores ) \n"
Original    (013): ['marginals', ',', 'logZ', '=', 'self', '.', 'decoder', '.', 'parse_marginals_nonproj', '(', 'scores', ')', '\\n']
Tokenized   (026): ['<s>', 'marg', 'inals', ',', 'log', 'Z', '=', 'self', '.', 'dec', 'oder', '.', 'parse', '_', 'marg', 'inals', '_', 'non', 'pro', 'j', '(', 'scores', ')', '\\', 'n', '</s>']
Filtered   (024): ['marg', 'inals', ',', 'log', 'Z', '=', 'self', '.', 'dec', 'oder', '.', 'parse', '_', 'marg', 'inals', '_', 'non', 'pro', 'j', '(', 'scores', ')', '\\', 'n']
Detokenized (013): ['marginals', ',', 'logZ', '=', 'self', '.', 'decoder', '.', 'parse_marginals_nonproj', '(', 'scores', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "score_corr += scores [ h , m ] \n"
Original    (009): ['score_corr', '+=', 'scores', '[', 'h', ',', 'm', ']', '\\n']
Tokenized   (015): ['<s>', 'score', '_', 'cor', 'r', '+=', 'scores', '[', 'h', ',', 'm', ']', '\\', 'n', '</s>']
Filtered   (013): ['score', '_', 'cor', 'r', '+=', 'scores', '[', 'h', ',', 'm', ']', '\\', 'n']
Detokenized (009): ['score_corr', '+=', 'scores', '[', 'h', ',', 'm', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "features = self . add_final_features ( sequence , sequence . y [ - 1 ] , features ) \n"
Original    (019): ['features', '=', 'self', '.', 'add_final_features', '(', 'sequence', ',', 'sequence', '.', 'y', '[', '-', '1', ']', ',', 'features', ')', '\\n']
Tokenized   (026): ['<s>', 'features', '=', 'self', '.', 'add', '_', 'final', '_', 'features', '(', 'sequence', ',', 'sequence', '.', 'y', '[', '-', '1', ']', ',', 'features', ')', '\\', 'n', '</s>']
Filtered   (024): ['features', '=', 'self', '.', 'add', '_', 'final', '_', 'features', '(', 'sequence', ',', 'sequence', '.', 'y', '[', '-', '1', ']', ',', 'features', ')', '\\', 'n']
Detokenized (019): ['features', '=', 'self', '.', 'add_final_features', '(', 'sequence', ',', 'sequence', '.', 'y', '[', '-', '1', ']', ',', 'features', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "node_idx = self . add_emission_features ( sequence , pos , y , node_idx ) \n"
Original    (015): ['node_idx', '=', 'self', '.', 'add_emission_features', '(', 'sequence', ',', 'pos', ',', 'y', ',', 'node_idx', ')', '\\n']
Tokenized   (029): ['<s>', 'node', '_', 'id', 'x', '=', 'self', '.', 'add', '_', 'em', 'ission', '_', 'features', '(', 'sequence', ',', 'pos', ',', 'y', ',', 'node', '_', 'id', 'x', ')', '\\', 'n', '</s>']
Filtered   (027): ['node', '_', 'id', 'x', '=', 'self', '.', 'add', '_', 'em', 'ission', '_', 'features', '(', 'sequence', ',', 'pos', ',', 'y', ',', 'node', '_', 'id', 'x', ')', '\\', 'n']
Detokenized (015): ['node_idx', '=', 'self', '.', 'add_emission_features', '(', 'sequence', ',', 'pos', ',', 'y', ',', 'node_idx', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "edge_idx = self . add_final_features ( sequence , y_prev , edge_idx ) \n"
Original    (013): ['edge_idx', '=', 'self', '.', 'add_final_features', '(', 'sequence', ',', 'y_prev', ',', 'edge_idx', ')', '\\n']
Tokenized   (028): ['<s>', 'edge', '_', 'id', 'x', '=', 'self', '.', 'add', '_', 'final', '_', 'features', '(', 'sequence', ',', 'y', '_', 'prev', ',', 'edge', '_', 'id', 'x', ')', '\\', 'n', '</s>']
Filtered   (026): ['edge', '_', 'id', 'x', '=', 'self', '.', 'add', '_', 'final', '_', 'features', '(', 'sequence', ',', 'y', '_', 'prev', ',', 'edge', '_', 'id', 'x', ')', '\\', 'n']
Detokenized (013): ['edge_idx', '=', 'self', '.', 'add_final_features', '(', 'sequence', ',', 'y_prev', ',', 'edge_idx', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "y_name = self . dataset . y_dict . get_label_name ( y ) \n"
Original    (013): ['y_name', '=', 'self', '.', 'dataset', '.', 'y_dict', '.', 'get_label_name', '(', 'y', ')', '\\n']
Tokenized   (024): ['<s>', 'y', '_', 'name', '=', 'self', '.', 'dataset', '.', 'y', '_', 'dict', '.', 'get', '_', 'label', '_', 'name', '(', 'y', ')', '\\', 'n', '</s>']
Filtered   (022): ['y', '_', 'name', '=', 'self', '.', 'dataset', '.', 'y', '_', 'dict', '.', 'get', '_', 'label', '_', 'name', '(', 'y', ')', '\\', 'n']
Detokenized (013): ['y_name', '=', 'self', '.', 'dataset', '.', 'y_dict', '.', 'get_label_name', '(', 'y', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "feat_name = "prev_tag:%s::%s" % ( y_prev_name , y_name ) \n"
Original    (010): ['feat_name', '=', '"prev_tag:%s::%s"', '%', '(', 'y_prev_name', ',', 'y_name', ')', '\\n']
Tokenized   (031): ['<s>', 'feat', '_', 'name', '=', '"', 'prev', '_', 'tag', ':', '%', 's', '::', '%', 's', '"', '%', '(', 'y', '_', 'prev', '_', 'name', ',', 'y', '_', 'name', ')', '\\', 'n', '</s>']
Filtered   (029): ['feat', '_', 'name', '=', '"', 'prev', '_', 'tag', ':', '%', 's', '::', '%', 's', '"', '%', '(', 'y', '_', 'prev', '_', 'name', ',', 'y', '_', 'name', ')', '\\', 'n']
Detokenized (010): ['feat_name', '=', '"prev_tag:%s::%s"', '%', '(', 'y_prev_name', ',', 'y_name', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "point_geom . AddPoint ( point [ 0 ] , point [ 1 ] ) \n"
Original    (015): ['point_geom', '.', 'AddPoint', '(', 'point', '[', '0', ']', ',', 'point', '[', '1', ']', ')', '\\n']
Tokenized   (022): ['<s>', 'point', '_', 'ge', 'om', '.', 'Add', 'Point', '(', 'point', '[', '0', ']', ',', 'point', '[', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (020): ['point', '_', 'ge', 'om', '.', 'Add', 'Point', '(', 'point', '[', '0', ']', ',', 'point', '[', '1', ']', ')', '\\', 'n']
Detokenized (015): ['point_geom', '.', 'AddPoint', '(', 'point', '[', '0', ']', ',', 'point', '[', '1', ']', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "longitudes = [ 100 , 110 , 120 , 130 , 140 ] \n"
Original    (014): ['longitudes', '=', '[', '100', ',', '110', ',', '120', ',', '130', ',', '140', ']', '\\n']
Tokenized   (018): ['<s>', 'long', 'itudes', '=', '[', '100', ',', '110', ',', '120', ',', '130', ',', '140', ']', '\\', 'n', '</s>']
Filtered   (016): ['long', 'itudes', '=', '[', '100', ',', '110', ',', '120', ',', '130', ',', '140', ']', '\\', 'n']
Detokenized (014): ['longitudes', '=', '[', '100', ',', '110', ',', '120', ',', '130', ',', '140', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "point_1 . AddPoint ( longitudes [ 0 ] , latitudes [ 0 ] , elevation ) \n"
Original    (017): ['point_1', '.', 'AddPoint', '(', 'longitudes', '[', '0', ']', ',', 'latitudes', '[', '0', ']', ',', 'elevation', ')', '\\n']
Tokenized   (025): ['<s>', 'point', '_', '1', '.', 'Add', 'Point', '(', 'long', 'itudes', '[', '0', ']', ',', 'lat', 'itudes', '[', '0', ']', ',', 'elevation', ')', '\\', 'n', '</s>']
Filtered   (023): ['point', '_', '1', '.', 'Add', 'Point', '(', 'long', 'itudes', '[', '0', ']', ',', 'lat', 'itudes', '[', '0', ']', ',', 'elevation', ')', '\\', 'n']
Detokenized (017): ['point_1', '.', 'AddPoint', '(', 'longitudes', '[', '0', ']', ',', 'latitudes', '[', '0', ']', ',', 'elevation', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "di = np . array ( [ x for x in range ( projected_X . shape [ 1 ] ) ] ) \n"
Original    (023): ['di', '=', 'np', '.', 'array', '(', '[', 'x', 'for', 'x', 'in', 'range', '(', 'projected_X', '.', 'shape', '[', '1', ']', ')', ']', ')', '\\n']
Tokenized   (028): ['<s>', 'di', '=', 'np', '.', 'array', '(', '[', 'x', 'for', 'x', 'in', 'range', '(', 'projected', '_', 'X', '.', 'shape', '[', '1', ']', ')', ']', ')', '\\', 'n', '</s>']
Filtered   (026): ['di', '=', 'np', '.', 'array', '(', '[', 'x', 'for', 'x', 'in', 'range', '(', 'projected', '_', 'X', '.', 'shape', '[', '1', ']', ')', ']', ')', '\\', 'n']
Detokenized (023): ['di', '=', 'np', '.', 'array', '(', '[', 'x', 'for', 'x', 'in', 'range', '(', 'projected_X', '.', 'shape', '[', '1', ']', ')', ']', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "projected_X = np . c_ [ ids , projected_X ] \n"
Original    (011): ['projected_X', '=', 'np', '.', 'c_', '[', 'ids', ',', 'projected_X', ']', '\\n']
Tokenized   (021): ['<s>', 'project', 'ed', '_', 'X', '=', 'np', '.', 'c', '_', '[', '', 'ids', ',', 'projected', '_', 'X', ']', '\\', 'n', '</s>']
Filtered   (019): ['project', 'ed', '_', 'X', '=', 'np', '.', 'c', '_', '[', '', 'ids', ',', 'projected', '_', 'X', ']', '\\', 'n']
Detokenized (011): ['projected_X', '=', 'np', '.', 'c_', '[', 'ids', ',', 'projected_X', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "clusterer . fit ( inverse_x [ : , 1 : ] ) \n"
Original    (013): ['clusterer', '.', 'fit', '(', 'inverse_x', '[', ':', ',', '1', ':', ']', ')', '\\n']
Tokenized   (020): ['<s>', 'cl', 'ust', 'erer', '.', 'fit', '(', 'inverse', '_', 'x', '[', ':', ',', '1', ':', ']', ')', '\\', 'n', '</s>']
Filtered   (018): ['cl', 'ust', 'erer', '.', 'fit', '(', 'inverse', '_', 'x', '[', ':', ',', '1', ':', ']', ')', '\\', 'n']
Detokenized (013): ['clusterer', '.', 'fit', '(', 'inverse_x', '[', ':', ',', '1', ':', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "complex [ "meta" ] = self . projection \n"
Original    (009): ['complex', '[', '"meta"', ']', '=', 'self', '.', 'projection', '\\n']
Tokenized   (014): ['<s>', 'complex', '[', '"', 'meta', '"', ']', '=', 'self', '.', 'projection', '\\', 'n', '</s>']
Filtered   (012): ['complex', '[', '"', 'meta', '"', ']', '=', 'self', '.', 'projection', '\\', 'n']
Detokenized (009): ['complex', '[', '"meta"', ']', '=', 'self', '.', 'projection', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( complex ~~ k2e [ k ] = e \n"
Original    (040): ['json_s', '[', '"nodes"', ']', '.', 'append', '(', '{', '"name"', ':', 'str', '(', 'k', ')', ',', '"tooltip"', ':', 'tooltip_s', ',', '"group"', ':', '2', '*', 'int', '(', 'np', '.', 'log', '(', 'len', '(', 'complex', '~~', 'k2e', '[', 'k', ']', '=', 'e', '\\n']
Tokenized   (060): ['<s>', 'json', '_', 's', '[', '"', 'n', 'odes', '"', ']', '.', 'append', '(', '{', '"', 'name', '"', ':', 'str', '(', 'k', ')', ',', '"', 'tool', 'tip', '"', ':', 'tooltip', '_', 's', ',', '"', 'group', '"', ':', '2', '*', 'int', '(', 'np', '.', 'log', '(', 'len', '(', 'complex', '', '~~', 'k', '2', 'e', '[', 'k', ']', '=', 'e', '\\', 'n', '</s>']
Filtered   (058): ['json', '_', 's', '[', '"', 'n', 'odes', '"', ']', '.', 'append', '(', '{', '"', 'name', '"', ':', 'str', '(', 'k', ')', ',', '"', 'tool', 'tip', '"', ':', 'tooltip', '_', 's', ',', '"', 'group', '"', ':', '2', '*', 'int', '(', 'np', '.', 'log', '(', 'len', '(', 'complex', '', '~~', 'k', '2', 'e', '[', 'k', ']', '=', 'e', '\\', 'n']
Detokenized (040): ['json_s', '[', '"nodes"', ']', '.', 'append', '(', '{', '"name"', ':', 'str', '(', 'k', ')', ',', '"tooltip"', ':', 'tooltip_s', ',', '"group"', ':', '2', '*', 'int', '(', 'np', '.', 'log', '(', 'len', '(', 'complex', '~~', 'k2e', '[', 'k', ']', '=', 'e', '\\n']
Counter: 58
===================================================================
Hidden states:  (13, 40, 768)
# Extracted words:  40
Sentence         : "width_js = "%s" % width_html \n"
Original    (006): ['width_js', '=', '"%s"', '%', 'width_html', '\\n']
Tokenized   (015): ['<s>', 'width', '_', 'js', '=', '"%', 's', '"', '%', 'width', '_', 'html', '\\', 'n', '</s>']
Filtered   (013): ['width', '_', 'js', '=', '"%', 's', '"', '%', 'width', '_', 'html', '\\', 'n']
Detokenized (006): ['width_js', '=', '"%s"', '%', 'width_html', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "new_settings [ interface ] [ ] [ % protocol ] = server \n"
Original    (013): ['new_settings', '[', 'interface', ']', '[', ']', '[', '%', 'protocol', ']', '=', 'server', '\\n']
Tokenized   (018): ['<s>', 'new', '_', 'settings', '[', 'interface', ']', '[', ']', '[', '%', 'protocol', ']', '=', 'server', '\\', 'n', '</s>']
Filtered   (016): ['new', '_', 'settings', '[', 'interface', ']', '[', ']', '[', '%', 'protocol', ']', '=', 'server', '\\', 'n']
Detokenized (013): ['new_settings', '[', 'interface', ']', '[', ']', '[', '%', 'protocol', ']', '=', 'server', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "setups = ( less_setup , sass_setup , stylus_setup , sass_erb_setup ) \n"
Original    (012): ['setups', '=', '(', 'less_setup', ',', 'sass_setup', ',', 'stylus_setup', ',', 'sass_erb_setup', ')', '\\n']
Tokenized   (029): ['<s>', 'set', 'ups', '=', '(', 'less', '_', 'setup', ',', 's', 'ass', '_', 'setup', ',', 'styl', 'us', '_', 'setup', ',', 's', 'ass', '_', 'erb', '_', 'setup', ')', '\\', 'n', '</s>']
Filtered   (027): ['set', 'ups', '=', '(', 'less', '_', 'setup', ',', 's', 'ass', '_', 'setup', ',', 'styl', 'us', '_', 'setup', ',', 's', 'ass', '_', 'erb', '_', 'setup', ')', '\\', 'n']
Detokenized (012): ['setups', '=', '(', 'less_setup', ',', 'sass_setup', ',', 'stylus_setup', ',', 'sass_erb_setup', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "fn = self . view . file_name ( ) . encode ( "utf_8" ) \n"
Original    (015): ['fn', '=', 'self', '.', 'view', '.', 'file_name', '(', ')', '.', 'encode', '(', '"utf_8"', ')', '\\n']
Tokenized   (024): ['<s>', 'fn', '=', 'self', '.', 'view', '.', 'file', '_', 'name', '(', ')', '.', 'encode', '(', '"', 'utf', '_', '8', '"', ')', '\\', 'n', '</s>']
Filtered   (022): ['fn', '=', 'self', '.', 'view', '.', 'file', '_', 'name', '(', ')', '.', 'encode', '(', '"', 'utf', '_', '8', '"', ')', '\\', 'n']
Detokenized (015): ['fn', '=', 'self', '.', 'view', '.', 'file_name', '(', ')', '.', 'encode', '(', '"utf_8"', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "compiled_regex = re . compile ( chosen_setup . regex , re . MULTILINE ) \n"
Original    (015): ['compiled_regex', '=', 're', '.', 'compile', '(', 'chosen_setup', '.', 'regex', ',', 're', '.', 'MULTILINE', ')', '\\n']
Tokenized   (027): ['<s>', 'comp', 'iled', '_', 're', 'gex', '=', 're', '.', 'compile', '(', 'chosen', '_', 'setup', '.', 'regex', ',', 're', '.', 'M', 'ULT', 'IL', 'INE', ')', '\\', 'n', '</s>']
Filtered   (025): ['comp', 'iled', '_', 're', 'gex', '=', 're', '.', 'compile', '(', 'chosen', '_', 'setup', '.', 'regex', ',', 're', '.', 'M', 'ULT', 'IL', 'INE', ')', '\\', 'n']
Detokenized (015): ['compiled_regex', '=', 're', '.', 'compile', '(', 'chosen_setup', '.', 'regex', ',', 're', '.', 'MULTILINE', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "file_dir = os . path . dirname ( fn ) . decode ( "utf-8" ) \n"
Original    (016): ['file_dir', '=', 'os', '.', 'path', '.', 'dirname', '(', 'fn', ')', '.', 'decode', '(', '"utf-8"', ')', '\\n']
Tokenized   (026): ['<s>', 'file', '_', 'dir', '=', 'os', '.', 'path', '.', 'dir', 'name', '(', 'fn', ')', '.', 'decode', '(', '"', 'utf', '-', '8', '"', ')', '\\', 'n', '</s>']
Filtered   (024): ['file', '_', 'dir', '=', 'os', '.', 'path', '.', 'dir', 'name', '(', 'fn', ')', '.', 'decode', '(', '"', 'utf', '-', '8', '"', ')', '\\', 'n']
Detokenized (016): ['file_dir', '=', 'os', '.', 'path', '.', 'dirname', '(', 'fn', ')', '.', 'decode', '(', '"utf-8"', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "partial_filename = fn_split [ 0 ] + "/_" + fn_split [ 1 ] \n"
Original    (014): ['partial_filename', '=', 'fn_split', '[', '0', ']', '+', '"/_"', '+', 'fn_split', '[', '1', ']', '\\n']
Tokenized   (025): ['<s>', 'partial', '_', 'filename', '=', 'fn', '_', 'split', '[', '0', ']', '+', '"/', '_', '"', '+', 'fn', '_', 'split', '[', '1', ']', '\\', 'n', '</s>']
Filtered   (023): ['partial', '_', 'filename', '=', 'fn', '_', 'split', '[', '0', ']', '+', '"/', '_', '"', '+', 'fn', '_', 'split', '[', '1', ']', '\\', 'n']
Detokenized (014): ['partial_filename', '=', 'fn_split', '[', '0', ']', '+', '"/_"', '+', 'fn_split', '[', '1', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "imported_vars = imported_vars + m \n"
Original    (006): ['imported_vars', '=', 'imported_vars', '+', 'm', '\\n']
Tokenized   (016): ['<s>', 'im', 'ported', '_', 'v', 'ars', '=', 'imported', '_', 'v', 'ars', '+', 'm', '\\', 'n', '</s>']
Filtered   (014): ['im', 'ported', '_', 'v', 'ars', '=', 'imported', '_', 'v', 'ars', '+', 'm', '\\', 'n']
Detokenized (006): ['imported_vars', '=', 'imported_vars', '+', 'm', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : ""params" : [ { \n"
Original    (005): ['"params"', ':', '[', '{', '\\n']
Tokenized   (010): ['<s>', '"', 'params', '"', ':', '[', '{', '\\', 'n', '</s>']
Filtered   (008): ['"', 'params', '"', ':', '[', '{', '\\', 'n']
Detokenized (005): ['"params"', ':', '[', '{', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "LAT_MAX = + 90.0 \n"
Original    (005): ['LAT_MAX', '=', '+', '90.0', '\\n']
Tokenized   (013): ['<s>', 'L', 'AT', '_', 'MAX', '=', '+', '90', '.', '0', '\\', 'n', '</s>']
Filtered   (011): ['L', 'AT', '_', 'MAX', '=', '+', '90', '.', '0', '\\', 'n']
Detokenized (005): ['LAT_MAX', '=', '+', '90.0', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : ""purple" , "teal" , "lightgray" ] \n"
Original    (007): ['"purple"', ',', '"teal"', ',', '"lightgray"', ']', '\\n']
Tokenized   (019): ['<s>', '"', 'pur', 'ple', '"', ',', '"', 'te', 'al', '"', ',', '"', 'light', 'gray', '"', ']', '\\', 'n', '</s>']
Filtered   (017): ['"', 'pur', 'ple', '"', ',', '"', 'te', 'al', '"', ',', '"', 'light', 'gray', '"', ']', '\\', 'n']
Detokenized (007): ['"purple"', ',', '"teal"', ',', '"lightgray"', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "numrange = None , default = None , max_width = 72 ) : \n"
Original    (014): ['numrange', '=', 'None', ',', 'default', '=', 'None', ',', 'max_width', '=', '72', ')', ':', '\\n']
Tokenized   (020): ['<s>', 'num', 'range', '=', 'None', ',', 'default', '=', 'None', ',', 'max', '_', 'width', '=', '72', ')', ':', '\\', 'n', '</s>']
Filtered   (018): ['num', 'range', '=', 'None', ',', 'default', '=', 'None', ',', 'max', '_', 'width', '=', '72', ')', ':', '\\', 'n']
Detokenized (014): ['numrange', '=', 'None', ',', 'default', '=', 'None', ',', 'max_width', '=', '72', ')', ':', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "found_letter . lower ( ) == default . lower ( ) ) ) : \n"
Original    (015): ['found_letter', '.', 'lower', '(', ')', '==', 'default', '.', 'lower', '(', ')', ')', ')', ':', '\\n']
Tokenized   (020): ['<s>', 'found', '_', 'letter', '.', 'lower', '(', ')', '==', 'default', '.', 'lower', '(', ')', ')', ')', ':', '\\', 'n', '</s>']
Filtered   (018): ['found', '_', 'letter', '.', 'lower', '(', ')', '==', 'default', '.', 'lower', '(', ')', ')', ')', ':', '\\', 'n']
Detokenized (015): ['found_letter', '.', 'lower', '(', ')', '==', 'default', '.', 'lower', '(', ')', ')', ')', ':', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "option [ : index ] + show_letter + option [ index + 1 : ] \n"
Original    (016): ['option', '[', ':', 'index', ']', '+', 'show_letter', '+', 'option', '[', 'index', '+', '1', ':', ']', '\\n']
Tokenized   (021): ['<s>', 'option', '[', ':', 'index', ']', '+', 'show', '_', 'letter', '+', 'option', '[', 'index', '+', '1', ':', ']', '\\', 'n', '</s>']
Filtered   (019): ['option', '[', ':', 'index', ']', '+', 'show', '_', 'letter', '+', 'option', '[', 'index', '+', '1', ':', ']', '\\', 'n']
Detokenized (016): ['option', '[', ':', 'index', ']', '+', 'show_letter', '+', 'option', '[', 'index', '+', '1', ':', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "display_letters . append ( found_letter . upper ( ) ) \n"
Original    (011): ['display_letters', '.', 'append', '(', 'found_letter', '.', 'upper', '(', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'display', '_', 'letters', '.', 'append', '(', 'found', '_', 'letter', '.', 'upper', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['display', '_', 'letters', '.', 'append', '(', 'found', '_', 'letter', '.', 'upper', '(', ')', ')', '\\', 'n']
Detokenized (011): ['display_letters', '.', 'append', '(', 'found_letter', '.', 'upper', '(', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "default_name = self . colorize ( , default_name ) \n"
Original    (010): ['default_name', '=', 'self', '.', 'colorize', '(', ',', 'default_name', ')', '\\n']
Tokenized   (018): ['<s>', 'default', '_', 'name', '=', 'self', '.', 'color', 'ize', '(', ',', 'default', '_', 'name', ')', '\\', 'n', '</s>']
Filtered   (016): ['default', '_', 'name', '=', 'self', '.', 'color', 'ize', '(', ',', 'default', '_', 'name', ')', '\\', 'n']
Detokenized (010): ['default_name', '=', 'self', '.', 'colorize', '(', ',', 'default_name', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "prompt_parts . append ( tmpl % default_name ) \n"
Original    (009): ['prompt_parts', '.', 'append', '(', 'tmpl', '%', 'default_name', ')', '\\n']
Tokenized   (019): ['<s>', 'prom', 'pt', '_', 'parts', '.', 'append', '(', 't', 'm', 'pl', '%', 'default', '_', 'name', ')', '\\', 'n', '</s>']
Filtered   (017): ['prom', 'pt', '_', 'parts', '.', 'append', '(', 't', 'm', 'pl', '%', 'default', '_', 'name', ')', '\\', 'n']
Detokenized (009): ['prompt_parts', '.', 'append', '(', 'tmpl', '%', 'default_name', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "matcher = SequenceMatcher ( lambda x : False , a , b ) \n"
Original    (014): ['matcher', '=', 'SequenceMatcher', '(', 'lambda', 'x', ':', 'False', ',', 'a', ',', 'b', ')', '\\n']
Tokenized   (020): ['<s>', 'mat', 'cher', '=', 'Sequence', 'Mat', 'cher', '(', 'lambda', 'x', ':', 'False', ',', 'a', ',', 'b', ')', '\\', 'n', '</s>']
Filtered   (018): ['mat', 'cher', '=', 'Sequence', 'Mat', 'cher', '(', 'lambda', 'x', ':', 'False', ',', 'a', ',', 'b', ')', '\\', 'n']
Detokenized (014): ['matcher', '=', 'SequenceMatcher', '(', 'lambda', 'x', ':', 'False', ',', 'a', ',', 'b', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "b_out . append ( self . colorize ( color , b [ b_start : b_end ] ) ) \n"
Original    (019): ['b_out', '.', 'append', '(', 'self', '.', 'colorize', '(', 'color', ',', 'b', '[', 'b_start', ':', 'b_end', ']', ')', ')', '\\n']
Tokenized   (029): ['<s>', 'b', '_', 'out', '.', 'append', '(', 'self', '.', 'color', 'ize', '(', 'color', ',', 'b', '[', 'b', '_', 'start', ':', 'b', '_', 'end', ']', ')', ')', '\\', 'n', '</s>']
Filtered   (027): ['b', '_', 'out', '.', 'append', '(', 'self', '.', 'color', 'ize', '(', 'color', ',', 'b', '[', 'b', '_', 'start', ':', 'b', '_', 'end', ']', ')', ')', '\\', 'n']
Detokenized (019): ['b_out', '.', 'append', '(', 'self', '.', 'colorize', '(', 'color', ',', 'b', '[', 'b_start', ':', 'b_end', ']', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "variable = % varname \n"
Original    (005): ['variable', '=', '%', 'varname', '\\n']
Tokenized   (009): ['<s>', 'variable', '=', '%', 'var', 'name', '\\', 'n', '</s>']
Filtered   (007): ['variable', '=', '%', 'var', 'name', '\\', 'n']
Detokenized (005): ['variable', '=', '%', 'varname', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "62 : , \n"
Original    (004): ['62', ':', ',', '\\n']
Tokenized   (007): ['<s>', '62', ':', ',', '\\', 'n', '</s>']
Filtered   (005): ['62', ':', ',', '\\', 'n']
Detokenized (004): ['62', ':', ',', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "_push . update ( { \n"
Original    (006): ['_push', '.', 'update', '(', '{', '\\n']
Tokenized   (010): ['<s>', '_', 'push', '.', 'update', '(', '{', '\\', 'n', '</s>']
Filtered   (008): ['_', 'push', '.', 'update', '(', '{', '\\', 'n']
Detokenized (006): ['_push', '.', 'update', '(', '{', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "_readonly = Entity . _readonly | { , } \n"
Original    (010): ['_readonly', '=', 'Entity', '.', '_readonly', '|', '{', ',', '}', '\\n']
Tokenized   (017): ['<s>', '_', 'read', 'only', '=', 'Entity', '.', '_', 'read', 'only', '|', '{', ',', '}', '\\', 'n', '</s>']
Filtered   (015): ['_', 'read', 'only', '=', 'Entity', '.', '_', 'read', 'only', '|', '{', ',', '}', '\\', 'n']
Detokenized (010): ['_readonly', '=', 'Entity', '.', '_readonly', '|', '{', ',', '}', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "remove_ids = [ 6 , 7 ] \n"
Original    (008): ['remove_ids', '=', '[', '6', ',', '7', ']', '\\n']
Tokenized   (013): ['<s>', 'remove', '_', 'ids', '=', '[', '6', ',', '7', ']', '\\', 'n', '</s>']
Filtered   (011): ['remove', '_', 'ids', '=', '[', '6', ',', '7', ']', '\\', 'n']
Detokenized (008): ['remove_ids', '=', '[', '6', ',', '7', ']', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "remove_advertiser_ids = [ 8 , 9 , 10 ] \n"
Original    (010): ['remove_advertiser_ids', '=', '[', '8', ',', '9', ',', '10', ']', '\\n']
Tokenized   (019): ['<s>', 'remove', '_', 'ad', 'vertis', 'er', '_', 'ids', '=', '[', '8', ',', '9', ',', '10', ']', '\\', 'n', '</s>']
Filtered   (017): ['remove', '_', 'ad', 'vertis', 'er', '_', 'ids', '=', '[', '8', ',', '9', ',', '10', ']', '\\', 'n']
Detokenized (010): ['remove_advertiser_ids', '=', '[', '8', ',', '9', ',', '10', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "num_users , num_items = dataset . shape \n"
Original    (008): ['num_users', ',', 'num_items', '=', 'dataset', '.', 'shape', '\\n']
Tokenized   (015): ['<s>', 'num', '_', 'users', ',', 'num', '_', 'items', '=', 'dataset', '.', 'shape', '\\', 'n', '</s>']
Filtered   (013): ['num', '_', 'users', ',', 'num', '_', 'items', '=', 'dataset', '.', 'shape', '\\', 'n']
Detokenized (008): ['num_users', ',', 'num_items', '=', 'dataset', '.', 'shape', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "async_job = view . map_async ( process , tasks , retries = 2 ) \n"
Original    (015): ['async_job', '=', 'view', '.', 'map_async', '(', 'process', ',', 'tasks', ',', 'retries', '=', '2', ')', '\\n']
Tokenized   (025): ['<s>', 'as', 'ync', '_', 'job', '=', 'view', '.', 'map', '_', 'as', 'ync', '(', 'process', ',', 'tasks', ',', 'ret', 'ries', '=', '2', ')', '\\', 'n', '</s>']
Filtered   (023): ['as', 'ync', '_', 'job', '=', 'view', '.', 'map', '_', 'as', 'ync', '(', 'process', ',', 'tasks', ',', 'ret', 'ries', '=', '2', ')', '\\', 'n']
Detokenized (015): ['async_job', '=', 'view', '.', 'map_async', '(', 'process', ',', 'tasks', ',', 'retries', '=', '2', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "remaining = len ( tasks ) - len ( done ) \n"
Original    (012): ['remaining', '=', 'len', '(', 'tasks', ')', '-', 'len', '(', 'done', ')', '\\n']
Tokenized   (016): ['<s>', 'rem', 'aining', '=', 'len', '(', 'tasks', ')', '-', 'len', '(', 'done', ')', '\\', 'n', '</s>']
Filtered   (014): ['rem', 'aining', '=', 'len', '(', 'tasks', ')', '-', 'len', '(', 'done', ')', '\\', 'n']
Detokenized (012): ['remaining', '=', 'len', '(', 'tasks', ')', '-', 'len', '(', 'done', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "num_items , type ( model ) . __name__ , simsfile ) \n"
Original    (012): ['num_items', ',', 'type', '(', 'model', ')', '.', '__name__', ',', 'simsfile', ')', '\\n']
Tokenized   (021): ['<s>', 'num', '_', 'items', ',', 'type', '(', 'model', ')', '.', '__', 'name', '__', ',', 'sim', 's', 'file', ')', '\\', 'n', '</s>']
Filtered   (019): ['num', '_', 'items', ',', 'type', '(', 'model', ')', '.', '__', 'name', '__', ',', 'sim', 's', 'file', ')', '\\', 'n']
Detokenized (012): ['num_items', ',', 'type', '(', 'model', ')', '.', '__name__', ',', 'simsfile', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "before = text [ : len ( text ) - len ( like ) ] \n"
Original    (016): ['before', '=', 'text', '[', ':', 'len', '(', 'text', ')', '-', 'len', '(', 'like', ')', ']', '\\n']
Tokenized   (019): ['<s>', 'before', '=', 'text', '[', ':', 'len', '(', 'text', ')', '-', 'len', '(', 'like', ')', ']', '\\', 'n', '</s>']
Filtered   (017): ['before', '=', 'text', '[', ':', 'len', '(', 'text', ')', '-', 'len', '(', 'like', ')', ']', '\\', 'n']
Detokenized (016): ['before', '=', 'text', '[', ':', 'len', '(', 'text', ')', '-', 'len', '(', 'like', ')', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "Version = namedtuple ( , ) \n"
Original    (007): ['Version', '=', 'namedtuple', '(', ',', ')', '\\n']
Tokenized   (012): ['<s>', 'Version', '=', 'named', 't', 'uple', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (010): ['Version', '=', 'named', 't', 'uple', '(', ',', ')', '\\', 'n']
Detokenized (007): ['Version', '=', 'namedtuple', '(', ',', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "_defaults = collections . OrderedDict ( [ \n"
Original    (008): ['_defaults', '=', 'collections', '.', 'OrderedDict', '(', '[', '\\n']
Tokenized   (016): ['<s>', '_', 'default', 's', '=', 'collections', '.', 'Ord', 'ered', 'D', 'ict', '(', '[', '\\', 'n', '</s>']
Filtered   (014): ['_', 'default', 's', '=', 'collections', '.', 'Ord', 'ered', 'D', 'ict', '(', '[', '\\', 'n']
Detokenized (008): ['_defaults', '=', 'collections', '.', 'OrderedDict', '(', '[', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "rootDirectory = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , , ) \n"
Original    (027): ['rootDirectory', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'realpath', '(', '__file__', ')', ')', ',', ',', ')', '\\n']
Tokenized   (035): ['<s>', 'root', 'Directory', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'real', 'path', '(', '__', 'file', '__', ')', ')', ',', ',', ')', '\\', 'n', '</s>']
Filtered   (033): ['root', 'Directory', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'real', 'path', '(', '__', 'file', '__', ')', ')', ',', ',', ')', '\\', 'n']
Detokenized (027): ['rootDirectory', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'realpath', '(', '__file__', ')', ')', ',', ',', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "httpd . handle_request ( ) from . import TestEnable # \n"
Original    (011): ['httpd', '.', 'handle_request', '(', ')', 'from', '.', 'import', 'TestEnable', '#', '\\n']
Tokenized   (018): ['<s>', 'http', 'd', '.', 'handle', '_', 'request', '(', ')', 'from', '.', 'import', 'Test', 'Enable', '#', '\\', 'n', '</s>']
Filtered   (016): ['http', 'd', '.', 'handle', '_', 'request', '(', ')', 'from', '.', 'import', 'Test', 'Enable', '#', '\\', 'n']
Detokenized (011): ['httpd', '.', 'handle_request', '(', ')', 'from', '.', 'import', 'TestEnable', '#', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "field_names = tuple ( field_names ) , \n"
Original    (008): ['field_names', '=', 'tuple', '(', 'field_names', ')', ',', '\\n']
Tokenized   (015): ['<s>', 'field', '_', 'names', '=', 'tuple', '(', 'field', '_', 'names', ')', ',', '\\', 'n', '</s>']
Filtered   (013): ['field', '_', 'names', '=', 'tuple', '(', 'field', '_', 'names', ')', ',', '\\', 'n']
Detokenized (008): ['field_names', '=', 'tuple', '(', 'field_names', ')', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "arg_list = repr ( tuple ( field_names ) ) . replace ( "\'" , "" ) [ 1 : - 1 ] , \n"
Original    (024): ['arg_list', '=', 'repr', '(', 'tuple', '(', 'field_names', ')', ')', '.', 'replace', '(', '"\\\'"', ',', '""', ')', '[', '1', ':', '-', '1', ']', ',', '\\n']
Tokenized   (032): ['<s>', 'arg', '_', 'list', '=', 'repr', '(', 'tuple', '(', 'field', '_', 'names', ')', ')', '.', 'replace', '(', '"\\', '\'"', ',', '""', ')', '[', '1', ':', '-', '1', ']', ',', '\\', 'n', '</s>']
Filtered   (030): ['arg', '_', 'list', '=', 'repr', '(', 'tuple', '(', 'field', '_', 'names', ')', ')', '.', 'replace', '(', '"\\', '\'"', ',', '""', ')', '[', '1', ':', '-', '1', ']', ',', '\\', 'n']
Detokenized (024): ['arg_list', '=', 'repr', '(', 'tuple', '(', 'field_names', ')', ')', '.', 'replace', '(', '"\\\'"', ',', '""', ')', '[', '1', ':', '-', '1', ']', ',', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "repr_fmt = . join ( _repr_template . format ( name = name ) \n"
Original    (014): ['repr_fmt', '=', '.', 'join', '(', '_repr_template', '.', 'format', '(', 'name', '=', 'name', ')', '\\n']
Tokenized   (025): ['<s>', 're', 'pr', '_', 'f', 'mt', '=', '.', 'join', '(', '_', 're', 'pr', '_', 'template', '.', 'format', '(', 'name', '=', 'name', ')', '\\', 'n', '</s>']
Filtered   (023): ['re', 'pr', '_', 'f', 'mt', '=', '.', 'join', '(', '_', 're', 'pr', '_', 'template', '.', 'format', '(', 'name', '=', 'name', ')', '\\', 'n']
Detokenized (014): ['repr_fmt', '=', '.', 'join', '(', '_repr_template', '.', 'format', '(', 'name', '=', 'name', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "field_defs = . join ( _field_template . format ( index = index , name = name ) \n"
Original    (018): ['field_defs', '=', '.', 'join', '(', '_field_template', '.', 'format', '(', 'index', '=', 'index', ',', 'name', '=', 'name', ')', '\\n']
Tokenized   (027): ['<s>', 'field', '_', 'def', 's', '=', '.', 'join', '(', '_', 'field', '_', 'template', '.', 'format', '(', 'index', '=', 'index', ',', 'name', '=', 'name', ')', '\\', 'n', '</s>']
Filtered   (025): ['field', '_', 'def', 's', '=', '.', 'join', '(', '_', 'field', '_', 'template', '.', 'format', '(', 'index', '=', 'index', ',', 'name', '=', 'name', ')', '\\', 'n']
Detokenized (018): ['field_defs', '=', '.', 'join', '(', '_field_template', '.', 'format', '(', 'index', '=', 'index', ',', 'name', '=', 'name', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "OrderedDict = OrderedDict , _property = property , _tuple = tuple ) \n"
Original    (013): ['OrderedDict', '=', 'OrderedDict', ',', '_property', '=', 'property', ',', '_tuple', '=', 'tuple', ')', '\\n']
Tokenized   (025): ['<s>', 'Ord', 'ered', 'D', 'ict', '=', 'Ord', 'ered', 'D', 'ict', ',', '_', 'property', '=', 'property', ',', '_', 't', 'uple', '=', 'tuple', ')', '\\', 'n', '</s>']
Filtered   (023): ['Ord', 'ered', 'D', 'ict', '=', 'Ord', 'ered', 'D', 'ict', ',', '_', 'property', '=', 'property', ',', '_', 't', 'uple', '=', 'tuple', ')', '\\', 'n']
Detokenized (013): ['OrderedDict', '=', 'OrderedDict', ',', '_property', '=', 'property', ',', '_tuple', '=', 'tuple', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "xx = Xdf [ ] . values \n"
Original    (008): ['xx', '=', 'Xdf', '[', ']', '.', 'values', '\\n']
Tokenized   (012): ['<s>', 'xx', '=', 'X', 'df', '[', ']', '.', 'values', '\\', 'n', '</s>']
Filtered   (010): ['xx', '=', 'X', 'df', '[', ']', '.', 'values', '\\', 'n']
Detokenized (008): ['xx', '=', 'Xdf', '[', ']', '.', 'values', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "X_CD13 , Y_CD13 = util . get_data ( cd13 , y_names = [ , ] ) \n"
Original    (017): ['X_CD13', ',', 'Y_CD13', '=', 'util', '.', 'get_data', '(', 'cd13', ',', 'y_names', '=', '[', ',', ']', ')', '\\n']
Tokenized   (031): ['<s>', 'X', '_', 'CD', '13', ',', 'Y', '_', 'CD', '13', '=', 'util', '.', 'get', '_', 'data', '(', 'cd', '13', ',', 'y', '_', 'names', '=', '[', ',', ']', ')', '\\', 'n', '</s>']
Filtered   (029): ['X', '_', 'CD', '13', ',', 'Y', '_', 'CD', '13', '=', 'util', '.', 'get', '_', 'data', '(', 'cd', '13', ',', 'y', '_', 'names', '=', '[', ',', ']', ')', '\\', 'n']
Detokenized (017): ['X_CD13', ',', 'Y_CD13', '=', 'util', '.', 'get_data', '(', 'cd13', ',', 'y_names', '=', '[', ',', ']', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "cd33 = human_data . xs ( , level = , drop_level = False ) \n"
Original    (015): ['cd33', '=', 'human_data', '.', 'xs', '(', ',', 'level', '=', ',', 'drop_level', '=', 'False', ')', '\\n']
Tokenized   (024): ['<s>', 'cd', '33', '=', 'human', '_', 'data', '.', 'x', 's', '(', ',', 'level', '=', ',', 'drop', '_', 'level', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (022): ['cd', '33', '=', 'human', '_', 'data', '.', 'x', 's', '(', ',', 'level', '=', ',', 'drop', '_', 'level', '=', 'False', ')', '\\', 'n']
Detokenized (015): ['cd33', '=', 'human_data', '.', 'xs', '(', ',', 'level', '=', ',', 'drop_level', '=', 'False', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "X_CD33 , Y_CD33 = util . get_data ( cd33 , y_names = [ , , ] ) \n"
Original    (018): ['X_CD33', ',', 'Y_CD33', '=', 'util', '.', 'get_data', '(', 'cd33', ',', 'y_names', '=', '[', ',', ',', ']', ')', '\\n']
Tokenized   (032): ['<s>', 'X', '_', 'CD', '33', ',', 'Y', '_', 'CD', '33', '=', 'util', '.', 'get', '_', 'data', '(', 'cd', '33', ',', 'y', '_', 'names', '=', '[', ',', ',', ']', ')', '\\', 'n', '</s>']
Filtered   (030): ['X', '_', 'CD', '33', ',', 'Y', '_', 'CD', '33', '=', 'util', '.', 'get', '_', 'data', '(', 'cd', '33', ',', 'y', '_', 'names', '=', '[', ',', ',', ']', ')', '\\', 'n']
Detokenized (018): ['X_CD33', ',', 'Y_CD33', '=', 'util', '.', 'get_data', '(', 'cd33', ',', 'y_names', '=', '[', ',', ',', ']', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "X_CD15 , Y_CD15 = util . get_data ( cd15 , y_names = [ ] ) \n"
Original    (016): ['X_CD15', ',', 'Y_CD15', '=', 'util', '.', 'get_data', '(', 'cd15', ',', 'y_names', '=', '[', ']', ')', '\\n']
Tokenized   (030): ['<s>', 'X', '_', 'CD', '15', ',', 'Y', '_', 'CD', '15', '=', 'util', '.', 'get', '_', 'data', '(', 'cd', '15', ',', 'y', '_', 'names', '=', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (028): ['X', '_', 'CD', '15', ',', 'Y', '_', 'CD', '15', '=', 'util', '.', 'get', '_', 'data', '(', 'cd', '15', ',', 'y', '_', 'names', '=', '[', ']', ')', '\\', 'n']
Detokenized (016): ['X_CD15', ',', 'Y_CD15', '=', 'util', '.', 'get_data', '(', 'cd15', ',', 'y_names', '=', '[', ']', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "mouse_Y = pandas . concat ( [ mouse_Y , Y ] , axis = 0 ) \n"
Original    (017): ['mouse_Y', '=', 'pandas', '.', 'concat', '(', '[', 'mouse_Y', ',', 'Y', ']', ',', 'axis', '=', '0', ')', '\\n']
Tokenized   (026): ['<s>', 'mouse', '_', 'Y', '=', 'pand', 'as', '.', 'conc', 'at', '(', '[', 'mouse', '_', 'Y', ',', 'Y', ']', ',', 'axis', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (024): ['mouse', '_', 'Y', '=', 'pand', 'as', '.', 'conc', 'at', '(', '[', 'mouse', '_', 'Y', ',', 'Y', ']', ',', 'axis', '=', '0', ')', '\\', 'n']
Detokenized (017): ['mouse_Y', '=', 'pandas', '.', 'concat', '(', '[', 'mouse_Y', ',', 'Y', ']', ',', 'axis', '=', '0', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "mouse_data = pandas . read_excel ( data_file , sheetname = 1 , index_col = [ 0 , 1 ] ) \n"
Original    (021): ['mouse_data', '=', 'pandas', '.', 'read_excel', '(', 'data_file', ',', 'sheetname', '=', '1', ',', 'index_col', '=', '[', '0', ',', '1', ']', ')', '\\n']
Tokenized   (035): ['<s>', 'mouse', '_', 'data', '=', 'pand', 'as', '.', 'read', '_', 'ex', 'cel', '(', 'data', '_', 'file', ',', 'sheet', 'name', '=', '1', ',', 'index', '_', 'col', '=', '[', '0', ',', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (033): ['mouse', '_', 'data', '=', 'pand', 'as', '.', 'read', '_', 'ex', 'cel', '(', 'data', '_', 'file', ',', 'sheet', 'name', '=', '1', ',', 'index', '_', 'col', '=', '[', '0', ',', '1', ']', ')', '\\', 'n']
Detokenized (021): ['mouse_data', '=', 'pandas', '.', 'read_excel', '(', 'data_file', ',', 'sheetname', '=', '1', ',', 'index_col', '=', '[', '0', ',', '1', ']', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "data_efficient [ ] = 1. \n"
Original    (006): ['data_efficient', '[', ']', '=', '1.', '\\n']
Tokenized   (012): ['<s>', 'data', '_', 'efficient', '[', ']', '=', '1', '.', '\\', 'n', '</s>']
Filtered   (010): ['data', '_', 'efficient', '[', ']', '=', '1', '.', '\\', 'n']
Detokenized (006): ['data_efficient', '[', ']', '=', '1.', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "exp_data [ ] = exp_data . groupby ( ) [ ] . transform ( exp_data [ ] = exp_data . groupby ( ) [ ] . transform ( \n"
Original    (029): ['exp_data', '[', ']', '=', 'exp_data', '.', 'groupby', '(', ')', '[', ']', '.', 'transform', '(', 'exp_data', '[', ']', '=', 'exp_data', '.', 'groupby', '(', ')', '[', ']', '.', 'transform', '(', '\\n']
Tokenized   (042): ['<s>', 'exp', '_', 'data', '[', ']', '=', 'exp', '_', 'data', '.', 'group', 'by', '(', ')', '[', ']', '.', 'transform', '(', 'exp', '_', 'data', '[', ']', '=', 'exp', '_', 'data', '.', 'group', 'by', '(', ')', '[', ']', '.', 'transform', '(', '\\', 'n', '</s>']
Filtered   (040): ['exp', '_', 'data', '[', ']', '=', 'exp', '_', 'data', '.', 'group', 'by', '(', ')', '[', ']', '.', 'transform', '(', 'exp', '_', 'data', '[', ']', '=', 'exp', '_', 'data', '.', 'group', 'by', '(', ')', '[', ']', '.', 'transform', '(', '\\', 'n']
Detokenized (029): ['exp_data', '[', ']', '=', 'exp_data', '.', 'groupby', '(', ')', '[', ']', '.', 'transform', '(', 'exp_data', '[', ']', '=', 'exp_data', '.', 'groupby', '(', ')', '[', ']', '.', 'transform', '(', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "aggregated [ ] = aggregated [ [ , ] ] . mean ( axis = 1 ) \n"
Original    (018): ['aggregated', '[', ']', '=', 'aggregated', '[', '[', ',', ']', ']', '.', 'mean', '(', 'axis', '=', '1', ')', '\\n']
Tokenized   (024): ['<s>', 'agg', 'reg', 'ated', '[', ']', '=', 'aggreg', 'ated', '[', '[', ',', ']', ']', '.', 'mean', '(', 'axis', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (022): ['agg', 'reg', 'ated', '[', ']', '=', 'aggreg', 'ated', '[', '[', ',', ']', ']', '.', 'mean', '(', 'axis', '=', '1', ')', '\\', 'n']
Detokenized (018): ['aggregated', '[', ']', '=', 'aggregated', '[', '[', ',', ']', ']', '.', 'mean', '(', 'axis', '=', '1', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "known_pairs = { : [ , , , ] , \n"
Original    (011): ['known_pairs', '=', '{', ':', '[', ',', ',', ',', ']', ',', '\\n']
Tokenized   (017): ['<s>', 'known', '_', 'p', 'airs', '=', '{', ':', '[', ',', ',', ',', ']', ',', '\\', 'n', '</s>']
Filtered   (015): ['known', '_', 'p', 'airs', '=', '{', ':', '[', ',', ',', ',', ']', ',', '\\', 'n']
Detokenized (011): ['known_pairs', '=', '{', ':', '[', ',', ',', ',', ']', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "drugs_to_genes [ ] . extend ( [ , , , , , \n"
Original    (013): ['drugs_to_genes', '[', ']', '.', 'extend', '(', '[', ',', ',', ',', ',', ',', '\\n']
Tokenized   (022): ['<s>', 'drug', 's', '_', 'to', '_', 'gen', 'es', '[', ']', '.', 'extend', '(', '[', ',', ',', ',', ',', ',', '\\', 'n', '</s>']
Filtered   (020): ['drug', 's', '_', 'to', '_', 'gen', 'es', '[', ']', '.', 'extend', '(', '[', ',', ',', ',', ',', ',', '\\', 'n']
Detokenized (013): ['drugs_to_genes', '[', ']', '.', 'extend', '(', '[', ',', ',', ',', ',', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "Xtmp [ ] = drug \n"
Original    (006): ['Xtmp', '[', ']', '=', 'drug', '\\n']
Tokenized   (010): ['<s>', 'X', 'tmp', '[', ']', '=', 'drug', '\\', 'n', '</s>']
Filtered   (008): ['X', 'tmp', '[', ']', '=', 'drug', '\\', 'n']
Detokenized (006): ['Xtmp', '[', ']', '=', 'drug', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "y_rank = pandas . concat ( ( y_rank , y_ranktmp ) , axis = 0 ) \n"
Original    (017): ['y_rank', '=', 'pandas', '.', 'concat', '(', '(', 'y_rank', ',', 'y_ranktmp', ')', ',', 'axis', '=', '0', ')', '\\n']
Tokenized   (029): ['<s>', 'y', '_', 'rank', '=', 'pand', 'as', '.', 'conc', 'at', '(', '(', 'y', '_', 'rank', ',', 'y', '_', 'rank', 'tmp', ')', ',', 'axis', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (027): ['y', '_', 'rank', '=', 'pand', 'as', '.', 'conc', 'at', '(', '(', 'y', '_', 'rank', ',', 'y', '_', 'rank', 'tmp', ')', ',', 'axis', '=', '0', ')', '\\', 'n']
Detokenized (017): ['y_rank', '=', 'pandas', '.', 'concat', '(', '(', 'y_rank', ',', 'y_ranktmp', ')', ',', 'axis', '=', '0', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "experiments [ ] = [ , , , ] \n"
Original    (010): ['experiments', '[', ']', '=', '[', ',', ',', ',', ']', '\\n']
Tokenized   (014): ['<s>', 'exper', 'iments', '[', ']', '=', '[', ',', ',', ',', ']', '\\', 'n', '</s>']
Filtered   (012): ['exper', 'iments', '[', ']', '=', '[', ',', ',', ',', ']', '\\', 'n']
Detokenized (010): ['experiments', '[', ']', '=', '[', ',', ',', ',', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "data_tmp [ "variance" ] = np . var ( data_tmp . values , axis = 1 ) \n"
Original    (018): ['data_tmp', '[', '"variance"', ']', '=', 'np', '.', 'var', '(', 'data_tmp', '.', 'values', ',', 'axis', '=', '1', ')', '\\n']
Tokenized   (028): ['<s>', 'data', '_', 'tmp', '[', '"', 'vari', 'ance', '"', ']', '=', 'np', '.', 'var', '(', 'data', '_', 'tmp', '.', 'values', ',', 'axis', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (026): ['data', '_', 'tmp', '[', '"', 'vari', 'ance', '"', ']', '=', 'np', '.', 'var', '(', 'data', '_', 'tmp', '.', 'values', ',', 'axis', '=', '1', ')', '\\', 'n']
Detokenized (018): ['data_tmp', '[', '"variance"', ']', '=', 'np', '.', 'var', '(', 'data_tmp', '.', 'values', ',', 'axis', '=', '1', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "gene_position_xu , target_genes_xu , Xdf_xu , Y_xu = read_xu_et_al ( data_file3 , learn_options ) \n"
Original    (015): ['gene_position_xu', ',', 'target_genes_xu', ',', 'Xdf_xu', ',', 'Y_xu', '=', 'read_xu_et_al', '(', 'data_file3', ',', 'learn_options', ')', '\\n']
Tokenized   (049): ['<s>', 'g', 'ene', '_', 'position', '_', 'x', 'u', ',', 'target', '_', 'gen', 'es', '_', 'x', 'u', ',', 'X', 'df', '_', 'x', 'u', ',', 'Y', '_', 'x', 'u', '=', 'read', '_', 'x', 'u', '_', 'et', '_', 'al', '(', 'data', '_', 'file', '3', ',', 'learn', '_', 'options', ')', '\\', 'n', '</s>']
Filtered   (047): ['g', 'ene', '_', 'position', '_', 'x', 'u', ',', 'target', '_', 'gen', 'es', '_', 'x', 'u', ',', 'X', 'df', '_', 'x', 'u', ',', 'Y', '_', 'x', 'u', '=', 'read', '_', 'x', 'u', '_', 'et', '_', 'al', '(', 'data', '_', 'file', '3', ',', 'learn', '_', 'options', ')', '\\', 'n']
Detokenized (015): ['gene_position_xu', ',', 'target_genes_xu', ',', 'Xdf_xu', ',', 'Y_xu', '=', 'read_xu_et_al', '(', 'data_file3', ',', 'learn_options', ')', '\\n']
Counter: 47
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "annotations , gene_position1 , target_genes1 , Xdf1 , Y1 = read_V1_data ( data_file , learn_options ) \n"
Original    (017): ['annotations', ',', 'gene_position1', ',', 'target_genes1', ',', 'Xdf1', ',', 'Y1', '=', 'read_V1_data', '(', 'data_file', ',', 'learn_options', ')', '\\n']
Tokenized   (040): ['<s>', 'annot', 'ations', ',', 'gene', '_', 'position', '1', ',', 'target', '_', 'gen', 'es', '1', ',', 'X', 'df', '1', ',', 'Y', '1', '=', 'read', '_', 'V', '1', '_', 'data', '(', 'data', '_', 'file', ',', 'learn', '_', 'options', ')', '\\', 'n', '</s>']
Filtered   (038): ['annot', 'ations', ',', 'gene', '_', 'position', '1', ',', 'target', '_', 'gen', 'es', '1', ',', 'X', 'df', '1', ',', 'Y', '1', '=', 'read', '_', 'V', '1', '_', 'data', '(', 'data', '_', 'file', ',', 'learn', '_', 'options', ')', '\\', 'n']
Detokenized (017): ['annotations', ',', 'gene_position1', ',', 'target_genes1', ',', 'Xdf1', ',', 'Y1', '=', 'read_V1_data', '(', 'data_file', ',', 'learn_options', ')', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "Y_cols_to_keep = np . unique ( [ , , , \n"
Original    (011): ['Y_cols_to_keep', '=', 'np', '.', 'unique', '(', '[', ',', ',', ',', '\\n']
Tokenized   (021): ['<s>', 'Y', '_', 'col', 's', '_', 'to', '_', 'keep', '=', 'np', '.', 'unique', '(', '[', ',', ',', ',', '\\', 'n', '</s>']
Filtered   (019): ['Y', '_', 'col', 's', '_', 'to', '_', 'keep', '=', 'np', '.', 'unique', '(', '[', ',', ',', ',', '\\', 'n']
Detokenized (011): ['Y_cols_to_keep', '=', 'np', '.', 'unique', '(', '[', ',', ',', ',', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "gene_position = pandas . concat ( ( gene_position1 , gene_position2 ) ) \n"
Original    (013): ['gene_position', '=', 'pandas', '.', 'concat', '(', '(', 'gene_position1', ',', 'gene_position2', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'g', 'ene', '_', 'position', '=', 'pand', 'as', '.', 'conc', 'at', '(', '(', 'gene', '_', 'position', '1', ',', 'gene', '_', 'position', '2', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['g', 'ene', '_', 'position', '=', 'pand', 'as', '.', 'conc', 'at', '(', '(', 'gene', '_', 'position', '1', ',', 'gene', '_', 'position', '2', ')', ')', '\\', 'n']
Detokenized (013): ['gene_position', '=', 'pandas', '.', 'concat', '(', '(', 'gene_position1', ',', 'gene_position2', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "onedupind = np . where ( Y . index . duplicated ( ) ) [ 0 ] [ 0 ] \n"
Original    (021): ['onedupind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'duplicated', '(', ')', ')', '[', '0', ']', '[', '0', ']', '\\n']
Tokenized   (027): ['<s>', 'oned', 'up', 'ind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'dupl', 'icated', '(', ')', ')', '[', '0', ']', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (025): ['oned', 'up', 'ind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'dupl', 'icated', '(', ')', ')', '[', '0', ']', '[', '0', ']', '\\', 'n']
Detokenized (021): ['onedupind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'duplicated', '(', ')', ')', '[', '0', ']', '[', '0', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "alldupind = np . where ( Y . index . get_level_values ( 0 ) . values == Y . index [ onedupind ] [ 0 ] ) [ 0 ] \n"
Original    (031): ['alldupind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'get_level_values', '(', '0', ')', '.', 'values', '==', 'Y', '.', 'index', '[', 'onedupind', ']', '[', '0', ']', ')', '[', '0', ']', '\\n']
Tokenized   (044): ['<s>', 'al', 'ld', 'up', 'ind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'get', '_', 'level', '_', 'values', '(', '0', ')', '.', 'values', '==', 'Y', '.', 'index', '[', 'on', 'ed', 'up', 'ind', ']', '[', '0', ']', ')', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (042): ['al', 'ld', 'up', 'ind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'get', '_', 'level', '_', 'values', '(', '0', ')', '.', 'values', '==', 'Y', '.', 'index', '[', 'on', 'ed', 'up', 'ind', ']', '[', '0', ']', ')', '[', '0', ']', '\\', 'n']
Detokenized (031): ['alldupind', '=', 'np', '.', 'where', '(', 'Y', '.', 'index', '.', 'get_level_values', '(', '0', ')', '.', 'values', '==', 'Y', '.', 'index', '[', 'onedupind', ']', '[', '0', ']', ')', '[', '0', ']', '\\n']
Counter: 42
===================================================================
Hidden states:  (13, 31, 768)
# Extracted words:  31
Sentence         : "newindex [ onedupind ] = ( newindex [ onedupind ] [ 0 ] , newindex [ onedupind ] [ 1 ] , "nodrug2" ) \n"
Original    (025): ['newindex', '[', 'onedupind', ']', '=', '(', 'newindex', '[', 'onedupind', ']', '[', '0', ']', ',', 'newindex', '[', 'onedupind', ']', '[', '1', ']', ',', '"nodrug2"', ')', '\\n']
Tokenized   (045): ['<s>', 'new', 'index', '[', 'on', 'ed', 'up', 'ind', ']', '=', '(', 'new', 'index', '[', 'on', 'ed', 'up', 'ind', ']', '[', '0', ']', ',', 'new', 'index', '[', 'on', 'ed', 'up', 'ind', ']', '[', '1', ']', ',', '"', 'n', 'od', 'rug', '2', '"', ')', '\\', 'n', '</s>']
Filtered   (043): ['new', 'index', '[', 'on', 'ed', 'up', 'ind', ']', '=', '(', 'new', 'index', '[', 'on', 'ed', 'up', 'ind', ']', '[', '0', ']', ',', 'new', 'index', '[', 'on', 'ed', 'up', 'ind', ']', '[', '1', ']', ',', '"', 'n', 'od', 'rug', '2', '"', ')', '\\', 'n']
Detokenized (025): ['newindex', '[', 'onedupind', ']', '=', '(', 'newindex', '[', 'onedupind', ']', '[', '0', ']', ',', 'newindex', '[', 'onedupind', ']', '[', '1', ']', ',', '"nodrug2"', ')', '\\n']
Counter: 43
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "Xdf . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n"
Original    (021): ['Xdf', '.', 'index', '=', 'pandas', '.', 'MultiIndex', '.', 'from_tuples', '(', 'newindex', ',', 'names', '=', 'Y', '.', 'index', '.', 'names', ')', '\\n']
Tokenized   (031): ['<s>', 'X', 'df', '.', 'index', '=', 'pand', 'as', '.', 'Multi', 'Index', '.', 'from', '_', 'tu', 'ples', '(', 'new', 'index', ',', 'names', '=', 'Y', '.', 'index', '.', 'names', ')', '\\', 'n', '</s>']
Filtered   (029): ['X', 'df', '.', 'index', '=', 'pand', 'as', '.', 'Multi', 'Index', '.', 'from', '_', 'tu', 'ples', '(', 'new', 'index', ',', 'names', '=', 'Y', '.', 'index', '.', 'names', ')', '\\', 'n']
Detokenized (021): ['Xdf', '.', 'index', '=', 'pandas', '.', 'MultiIndex', '.', 'from_tuples', '(', 'newindex', ',', 'names', '=', 'Y', '.', 'index', '.', 'names', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "mouse_genes = Xdf [ Xdf [ ] == ] [ ] . unique ( ) \n"
Original    (016): ['mouse_genes', '=', 'Xdf', '[', 'Xdf', '[', ']', '==', ']', '[', ']', '.', 'unique', '(', ')', '\\n']
Tokenized   (024): ['<s>', 'mouse', '_', 'gen', 'es', '=', 'X', 'df', '[', 'X', 'df', '[', ']', '==', ']', '[', ']', '.', 'unique', '(', ')', '\\', 'n', '</s>']
Filtered   (022): ['mouse', '_', 'gen', 'es', '=', 'X', 'df', '[', 'X', 'df', '[', ']', '==', ']', '[', ']', '.', 'unique', '(', ')', '\\', 'n']
Detokenized (016): ['mouse_genes', '=', 'Xdf', '[', 'Xdf', '[', ']', '==', ']', '[', ']', '.', 'unique', '(', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "all_genes = get_V3_genes ( None , None ) return np . setdiff1d ( all_genes , mouse_genes ) \n"
Original    (018): ['all_genes', '=', 'get_V3_genes', '(', 'None', ',', 'None', ')', 'return', 'np', '.', 'setdiff1d', '(', 'all_genes', ',', 'mouse_genes', ')', '\\n']
Tokenized   (039): ['<s>', 'all', '_', 'gen', 'es', '=', 'get', '_', 'V', '3', '_', 'gen', 'es', '(', 'None', ',', 'None', ')', 'return', 'np', '.', 'set', 'diff', '1', 'd', '(', 'all', '_', 'gen', 'es', ',', 'mouse', '_', 'gen', 'es', ')', '\\', 'n', '</s>']
Filtered   (037): ['all', '_', 'gen', 'es', '=', 'get', '_', 'V', '3', '_', 'gen', 'es', '(', 'None', ',', 'None', ')', 'return', 'np', '.', 'set', 'diff', '1', 'd', '(', 'all', '_', 'gen', 'es', ',', 'mouse', '_', 'gen', 'es', ')', '\\', 'n']
Detokenized (018): ['all_genes', '=', 'get_V3_genes', '(', 'None', ',', 'None', ')', 'return', 'np', '.', 'setdiff1d', '(', 'all_genes', ',', 'mouse_genes', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "option_value = self . cfg . migrate [ self . option_name ] \n"
Original    (013): ['option_value', '=', 'self', '.', 'cfg', '.', 'migrate', '[', 'self', '.', 'option_name', ']', '\\n']
Tokenized   (021): ['<s>', 'option', '_', 'value', '=', 'self', '.', 'cf', 'g', '.', 'migrate', '[', 'self', '.', 'option', '_', 'name', ']', '\\', 'n', '</s>']
Filtered   (019): ['option', '_', 'value', '=', 'self', '.', 'cf', 'g', '.', 'migrate', '[', 'self', '.', 'option', '_', 'name', ']', '\\', 'n']
Detokenized (013): ['option_value', '=', 'self', '.', 'cfg', '.', 'migrate', '[', 'self', '.', 'option_name', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "search_opts_tenant = kwargs . get ( , { } ) \n"
Original    (011): ['search_opts_tenant', '=', 'kwargs', '.', 'get', '(', ',', '{', '}', ')', '\\n']
Tokenized   (022): ['<s>', 'search', '_', 'op', 'ts', '_', 'ten', 'ant', '=', 'k', 'w', 'args', '.', 'get', '(', ',', '{', '}', ')', '\\', 'n', '</s>']
Filtered   (020): ['search', '_', 'op', 'ts', '_', 'ten', 'ant', '=', 'k', 'w', 'args', '.', 'get', '(', ',', '{', '}', ')', '\\', 'n']
Detokenized (011): ['search_opts_tenant', '=', 'kwargs', '.', 'get', '(', ',', '{', '}', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "tenants_without_quotas = self . get_tenants_without_quotas ( tenants_src , \n"
Original    (009): ['tenants_without_quotas', '=', 'self', '.', 'get_tenants_without_quotas', '(', 'tenants_src', ',', '\\n']
Tokenized   (030): ['<s>', 'ten', 'ants', '_', 'without', '_', 'qu', 'ot', 'as', '=', 'self', '.', 'get', '_', 'ten', 'ants', '_', 'without', '_', 'qu', 'ot', 'as', '(', 'tenants', '_', 'src', ',', '\\', 'n', '</s>']
Filtered   (028): ['ten', 'ants', '_', 'without', '_', 'qu', 'ot', 'as', '=', 'self', '.', 'get', '_', 'ten', 'ants', '_', 'without', '_', 'qu', 'ot', 'as', '(', 'tenants', '_', 'src', ',', '\\', 'n']
Detokenized (009): ['tenants_without_quotas', '=', 'self', '.', 'get_tenants_without_quotas', '(', 'tenants_src', ',', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "quot = network_src . show_quota ( tenants_without_quotas [ 0 ] ) \n"
Original    (012): ['quot', '=', 'network_src', '.', 'show_quota', '(', 'tenants_without_quotas', '[', '0', ']', ')', '\\n']
Tokenized   (027): ['<s>', 'qu', 'ot', '=', 'network', '_', 'src', '.', 'show', '_', 'qu', 'ota', '(', 'tenants', '_', 'without', '_', 'qu', 'ot', 'as', '[', '0', ']', ')', '\\', 'n', '</s>']
Filtered   (025): ['qu', 'ot', '=', 'network', '_', 'src', '.', 'show', '_', 'qu', 'ota', '(', 'tenants', '_', 'without', '_', 'qu', 'ot', 'as', '[', '0', ']', ')', '\\', 'n']
Detokenized (012): ['quot', '=', 'network_src', '.', 'show_quota', '(', 'tenants_without_quotas', '[', '0', ']', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "quot_default_dst [ item_quot ] ) \n"
Original    (006): ['quot_default_dst', '[', 'item_quot', ']', ')', '\\n']
Tokenized   (018): ['<s>', 'qu', 'ot', '_', 'default', '_', 'd', 'st', '[', 'item', '_', 'qu', 'ot', ']', ')', '\\', 'n', '</s>']
Filtered   (016): ['qu', 'ot', '_', 'default', '_', 'd', 'st', '[', 'item', '_', 'qu', 'ot', ']', ')', '\\', 'n']
Detokenized (006): ['quot_default_dst', '[', 'item_quot', ']', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "tenants = [ identity_src . keystone_client . tenants . find ( id = tnt_id ) for \n"
Original    (017): ['tenants', '=', '[', 'identity_src', '.', 'keystone_client', '.', 'tenants', '.', 'find', '(', 'id', '=', 'tnt_id', ')', 'for', '\\n']
Tokenized   (029): ['<s>', 'ten', 'ants', '=', '[', 'identity', '_', 'src', '.', 'key', 'stone', '_', 'client', '.', 'tenants', '.', 'find', '(', 'id', '=', 't', 'nt', '_', 'id', ')', 'for', '\\', 'n', '</s>']
Filtered   (027): ['ten', 'ants', '=', '[', 'identity', '_', 'src', '.', 'key', 'stone', '_', 'client', '.', 'tenants', '.', 'find', '(', 'id', '=', 't', 'nt', '_', 'id', ')', 'for', '\\', 'n']
Detokenized (017): ['tenants', '=', '[', 'identity_src', '.', 'keystone_client', '.', 'tenants', '.', 'find', '(', 'id', '=', 'tnt_id', ')', 'for', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "tnt_id in filter_tenants_ids_list ] \n"
Original    (005): ['tnt_id', 'in', 'filter_tenants_ids_list', ']', '\\n']
Tokenized   (018): ['<s>', 't', 'nt', '_', 'id', 'in', 'filter', '_', 'ten', 'ants', '_', 'ids', '_', 'list', ']', '\\', 'n', '</s>']
Filtered   (016): ['t', 'nt', '_', 'id', 'in', 'filter', '_', 'ten', 'ants', '_', 'ids', '_', 'list', ']', '\\', 'n']
Detokenized (005): ['tnt_id', 'in', 'filter_tenants_ids_list', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "instance [ ] [ ] , instance [ ] [ ] ) \n"
Original    (013): ['instance', '[', ']', '[', ']', ',', 'instance', '[', ']', '[', ']', ')', '\\n']
Tokenized   (016): ['<s>', 'instance', '[', ']', '[', ']', ',', 'instance', '[', ']', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (014): ['instance', '[', ']', '[', ']', ',', 'instance', '[', ']', '[', ']', ')', '\\', 'n']
Detokenized (013): ['instance', '[', ']', '[', ']', ',', 'instance', '[', ']', '[', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "vol [ ] , storage_resource . get_status , , \n"
Original    (010): ['vol', '[', ']', ',', 'storage_resource', '.', 'get_status', ',', ',', '\\n']
Tokenized   (017): ['<s>', 'vol', '[', ']', ',', 'storage', '_', 'resource', '.', 'get', '_', 'status', ',', ',', '\\', 'n', '</s>']
Filtered   (015): ['vol', '[', ']', ',', 'storage', '_', 'resource', '.', 'get', '_', 'status', ',', ',', '\\', 'n']
Detokenized (010): ['vol', '[', ']', ',', 'storage_resource', '.', 'get_status', ',', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "inst_name = libvirt_instance_name ) ) \n"
Original    (006): ['inst_name', '=', 'libvirt_instance_name', ')', ')', '\\n']
Tokenized   (016): ['<s>', 'inst', '_', 'name', '=', 'lib', 'virt', '_', 'instance', '_', 'name', ')', ')', '\\', 'n', '</s>']
Filtered   (014): ['inst', '_', 'name', '=', 'lib', 'virt', '_', 'instance', '_', 'name', ')', ')', '\\', 'n']
Detokenized (006): ['inst_name', '=', 'libvirt_instance_name', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "dst = instance_image_path ( instance_id ) ) \n"
Original    (008): ['dst', '=', 'instance_image_path', '(', 'instance_id', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'd', 'st', '=', 'instance', '_', 'image', '_', 'path', '(', 'instance', '_', 'id', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['d', 'st', '=', 'instance', '_', 'image', '_', 'path', '(', 'instance', '_', 'id', ')', ')', '\\', 'n']
Detokenized (008): ['dst', '=', 'instance_image_path', '(', 'instance_id', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "interface . find ( ) ) \n"
Original    (007): ['interface', '.', 'find', '(', ')', ')', '\\n']
Tokenized   (010): ['<s>', 'interface', '.', 'find', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (008): ['interface', '.', 'find', '(', ')', ')', '\\', 'n']
Detokenized (007): ['interface', '.', 'find', '(', ')', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "mac = self . mac , src = self . source_iface , dst = self . target_iface ) \n"
Original    (019): ['mac', '=', 'self', '.', 'mac', ',', 'src', '=', 'self', '.', 'source_iface', ',', 'dst', '=', 'self', '.', 'target_iface', ')', '\\n']
Tokenized   (028): ['<s>', 'mac', '=', 'self', '.', 'mac', ',', 'src', '=', 'self', '.', 'source', '_', 'if', 'ace', ',', 'dst', '=', 'self', '.', 'target', '_', 'if', 'ace', ')', '\\', 'n', '</s>']
Filtered   (026): ['mac', '=', 'self', '.', 'mac', ',', 'src', '=', 'self', '.', 'source', '_', 'if', 'ace', ',', 'dst', '=', 'self', '.', 'target', '_', 'if', 'ace', ')', '\\', 'n']
Detokenized (019): ['mac', '=', 'self', '.', 'mac', ',', 'src', '=', 'self', '.', 'source_iface', ',', 'dst', '=', 'self', '.', 'target_iface', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "element . attrib = { attr : value } \n"
Original    (010): ['element', '.', 'attrib', '=', '{', 'attr', ':', 'value', '}', '\\n']
Tokenized   (015): ['<s>', 'element', '.', 'att', 'rib', '=', '{', 'att', 'r', ':', 'value', '}', '\\', 'n', '</s>']
Filtered   (013): ['element', '.', 'att', 'rib', '=', '{', 'att', 'r', ':', 'value', '}', '\\', 'n']
Detokenized (010): ['element', '.', 'attrib', '=', '{', 'attr', ':', 'value', '}', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "rr . run ( copy . format ( src_file = source_object . path , \n"
Original    (015): ['rr', '.', 'run', '(', 'copy', '.', 'format', '(', 'src_file', '=', 'source_object', '.', 'path', ',', '\\n']
Tokenized   (022): ['<s>', 'rr', '.', 'run', '(', 'copy', '.', 'format', '(', 'src', '_', 'file', '=', 'source', '_', 'object', '.', 'path', ',', '\\', 'n', '</s>']
Filtered   (020): ['rr', '.', 'run', '(', 'copy', '.', 'format', '(', 'src', '_', 'file', '=', 'source', '_', 'object', '.', 'path', ',', '\\', 'n']
Detokenized (015): ['rr', '.', 'run', '(', 'copy', '.', 'format', '(', 'src_file', '=', 'source_object', '.', 'path', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "max_bytes = sizeof_format . parse_size ( kwargs . pop ( , 0 ) ) \n"
Original    (015): ['max_bytes', '=', 'sizeof_format', '.', 'parse_size', '(', 'kwargs', '.', 'pop', '(', ',', '0', ')', ')', '\\n']
Tokenized   (026): ['<s>', 'max', '_', 'bytes', '=', 'sizeof', '_', 'format', '.', 'parse', '_', 'size', '(', 'k', 'w', 'args', '.', 'pop', '(', ',', '0', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['max', '_', 'bytes', '=', 'sizeof', '_', 'format', '.', 'parse', '_', 'size', '(', 'k', 'w', 'args', '.', 'pop', '(', ',', '0', ')', ')', '\\', 'n']
Detokenized (015): ['max_bytes', '=', 'sizeof_format', '.', 'parse_size', '(', 'kwargs', '.', 'pop', '(', ',', '0', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "scenario = os . path . splitext ( scenario_filename ) [ 0 ] \n"
Original    (014): ['scenario', '=', 'os', '.', 'path', '.', 'splitext', '(', 'scenario_filename', ')', '[', '0', ']', '\\n']
Tokenized   (022): ['<s>', 'sc', 'enario', '=', 'os', '.', 'path', '.', 'spl', 'ite', 'xt', '(', 'scenario', '_', 'filename', ')', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (020): ['sc', 'enario', '=', 'os', '.', 'path', '.', 'spl', 'ite', 'xt', '(', 'scenario', '_', 'filename', ')', '[', '0', ']', '\\', 'n']
Detokenized (014): ['scenario', '=', 'os', '.', 'path', '.', 'splitext', '(', 'scenario_filename', ')', '[', '0', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "file_name = config . rollback_params [ ] [ ] \n"
Original    (010): ['file_name', '=', 'config', '.', 'rollback_params', '[', ']', '[', ']', '\\n']
Tokenized   (018): ['<s>', 'file', '_', 'name', '=', 'config', '.', 'roll', 'back', '_', 'params', '[', ']', '[', ']', '\\', 'n', '</s>']
Filtered   (016): ['file', '_', 'name', '=', 'config', '.', 'roll', 'back', '_', 'params', '[', ']', '[', ']', '\\', 'n']
Detokenized (010): ['file_name', '=', 'config', '.', 'rollback_params', '[', ']', '[', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "pre_file_path = os . path . join ( self . cloudferry_dir , file_name ) \n"
Original    (015): ['pre_file_path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'cloudferry_dir', ',', 'file_name', ')', '\\n']
Tokenized   (028): ['<s>', 'pre', '_', 'file', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'cloud', 'fer', 'ry', '_', 'dir', ',', 'file', '_', 'name', ')', '\\', 'n', '</s>']
Filtered   (026): ['pre', '_', 'file', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'cloud', 'fer', 'ry', '_', 'dir', ',', 'file', '_', 'name', ')', '\\', 'n']
Detokenized (015): ['pre_file_path', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'cloudferry_dir', ',', 'file_name', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "o2 = C ( 2 ) \n"
Original    (007): ['o2', '=', 'C', '(', '2', ')', '\\n']
Tokenized   (011): ['<s>', 'o', '2', '=', 'C', '(', '2', ')', '\\', 'n', '</s>']
Filtered   (009): ['o', '2', '=', 'C', '(', '2', ')', '\\', 'n']
Detokenized (007): ['o2', '=', 'C', '(', '2', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "org_tag = request . user . get_profile ( ) . org_tag \n"
Original    (012): ['org_tag', '=', 'request', '.', 'user', '.', 'get_profile', '(', ')', '.', 'org_tag', '\\n']
Tokenized   (021): ['<s>', 'org', '_', 'tag', '=', 'request', '.', 'user', '.', 'get', '_', 'profile', '(', ')', '.', 'org', '_', 'tag', '\\', 'n', '</s>']
Filtered   (019): ['org', '_', 'tag', '=', 'request', '.', 'user', '.', 'get', '_', 'profile', '(', ')', '.', 'org', '_', 'tag', '\\', 'n']
Detokenized (012): ['org_tag', '=', 'request', '.', 'user', '.', 'get_profile', '(', ')', '.', 'org_tag', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "featureset = Recording . objects . filter ( lat__lt = ne_lat , lat__gt = sw_lat , lon__lt = ne_lon , lon__gt = sw_lon \n"
Original    (024): ['featureset', '=', 'Recording', '.', 'objects', '.', 'filter', '(', 'lat__lt', '=', 'ne_lat', ',', 'lat__gt', '=', 'sw_lat', ',', 'lon__lt', '=', 'ne_lon', ',', 'lon__gt', '=', 'sw_lon', '\\n']
Tokenized   (046): ['<s>', 'features', 'et', '=', 'Recording', '.', 'objects', '.', 'filter', '(', 'lat', '__', 'lt', '=', 'ne', '_', 'lat', ',', 'lat', '__', 'gt', '=', 'sw', '_', 'lat', ',', 'l', 'on', '__', 'lt', '=', 'ne', '_', 'lon', ',', 'l', 'on', '__', 'gt', '=', 'sw', '_', 'lon', '\\', 'n', '</s>']
Filtered   (044): ['features', 'et', '=', 'Recording', '.', 'objects', '.', 'filter', '(', 'lat', '__', 'lt', '=', 'ne', '_', 'lat', ',', 'lat', '__', 'gt', '=', 'sw', '_', 'lat', ',', 'l', 'on', '__', 'lt', '=', 'ne', '_', 'lon', ',', 'l', 'on', '__', 'gt', '=', 'sw', '_', 'lon', '\\', 'n']
Detokenized (024): ['featureset', '=', 'Recording', '.', 'objects', '.', 'filter', '(', 'lat__lt', '=', 'ne_lat', ',', 'lat__gt', '=', 'sw_lat', ',', 'lon__lt', '=', 'ne_lon', ',', 'lon__gt', '=', 'sw_lon', '\\n']
Counter: 44
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "httpresponse_kwargs = { : kwargs . pop ( , None ) } \n"
Original    (013): ['httpresponse_kwargs', '=', '{', ':', 'kwargs', '.', 'pop', '(', ',', 'None', ')', '}', '\\n']
Tokenized   (022): ['<s>', 'http', 'response', '_', 'kw', 'args', '=', '{', ':', 'k', 'w', 'args', '.', 'pop', '(', ',', 'None', ')', '}', '\\', 'n', '</s>']
Filtered   (020): ['http', 'response', '_', 'kw', 'args', '=', '{', ':', 'k', 'w', 'args', '.', 'pop', '(', ',', 'None', ')', '}', '\\', 'n']
Detokenized (013): ['httpresponse_kwargs', '=', '{', ':', 'kwargs', '.', 'pop', '(', ',', 'None', ')', '}', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "is_testing = in sys . argv \n"
Original    (007): ['is_testing', '=', 'in', 'sys', '.', 'argv', '\\n']
Tokenized   (013): ['<s>', 'is', '_', 'testing', '=', 'in', 'sys', '.', 'arg', 'v', '\\', 'n', '</s>']
Filtered   (011): ['is', '_', 'testing', '=', 'in', 'sys', '.', 'arg', 'v', '\\', 'n']
Detokenized (007): ['is_testing', '=', 'in', 'sys', '.', 'argv', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "up_time = end_time - self . start_time \n"
Original    (008): ['up_time', '=', 'end_time', '-', 'self', '.', 'start_time', '\\n']
Tokenized   (017): ['<s>', 'up', '_', 'time', '=', 'end', '_', 'time', '-', 'self', '.', 'start', '_', 'time', '\\', 'n', '</s>']
Filtered   (015): ['up', '_', 'time', '=', 'end', '_', 'time', '-', 'self', '.', 'start', '_', 'time', '\\', 'n']
Detokenized (008): ['up_time', '=', 'end_time', '-', 'self', '.', 'start_time', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "remaining_time = self . count_down_total - datetime . timedelta ( seconds = ( int ( up_time ) ) ) \n"
Original    (020): ['remaining_time', '=', 'self', '.', 'count_down_total', '-', 'datetime', '.', 'timedelta', '(', 'seconds', '=', '(', 'int', '(', 'up_time', ')', ')', ')', '\\n']
Tokenized   (034): ['<s>', 'rem', 'aining', '_', 'time', '=', 'self', '.', 'count', '_', 'down', '_', 'total', '-', 'dat', 'etime', '.', 'timed', 'elta', '(', 'seconds', '=', '(', 'int', '(', 'up', '_', 'time', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (032): ['rem', 'aining', '_', 'time', '=', 'self', '.', 'count', '_', 'down', '_', 'total', '-', 'dat', 'etime', '.', 'timed', 'elta', '(', 'seconds', '=', '(', 'int', '(', 'up', '_', 'time', ')', ')', ')', '\\', 'n']
Detokenized (020): ['remaining_time', '=', 'self', '.', 'count_down_total', '-', 'datetime', '.', 'timedelta', '(', 'seconds', '=', '(', 'int', '(', 'up_time', ')', ')', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "abort_time = time . time ( ) + timeout \n"
Original    (010): ['abort_time', '=', 'time', '.', 'time', '(', ')', '+', 'timeout', '\\n']
Tokenized   (016): ['<s>', 'ab', 'ort', '_', 'time', '=', 'time', '.', 'time', '(', ')', '+', 'timeout', '\\', 'n', '</s>']
Filtered   (014): ['ab', 'ort', '_', 'time', '=', 'time', '.', 'time', '(', ')', '+', 'timeout', '\\', 'n']
Detokenized (010): ['abort_time', '=', 'time', '.', 'time', '(', ')', '+', 'timeout', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "elif not stanza . getID ( ) : \n"
Original    (009): ['elif', 'not', 'stanza', '.', 'getID', '(', ')', ':', '\\n']
Tokenized   (015): ['<s>', 'el', 'if', 'not', 'st', 'anza', '.', 'get', 'ID', '(', ')', ':', '\\', 'n', '</s>']
Filtered   (013): ['el', 'if', 'not', 'st', 'anza', '.', 'get', 'ID', '(', ')', ':', '\\', 'n']
Detokenized (009): ['elif', 'not', 'stanza', '.', 'getID', '(', ')', ':', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "_ID = ` ID ` \n"
Original    (006): ['_ID', '=', '`', 'ID', '`', '\\n']
Tokenized   (010): ['<s>', '_', 'ID', '=', '`', 'ID', '`', '\\', 'n', '</s>']
Filtered   (008): ['_', 'ID', '=', '`', 'ID', '`', '\\', 'n']
Detokenized (006): ['_ID', '=', '`', 'ID', '`', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "__description__ = , \n"
Original    (004): ['__description__', '=', ',', '\\n']
Tokenized   (009): ['<s>', '__', 'description', '__', '=', ',', '\\', 'n', '</s>']
Filtered   (007): ['__', 'description', '__', '=', ',', '\\', 'n']
Detokenized (004): ['__description__', '=', ',', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "REQUIRES = [ i . strip ( ) for i in open ( "requirements.txt" ) . readlines ( ) ] \n"
Original    (021): ['REQUIRES', '=', '[', 'i', '.', 'strip', '(', ')', 'for', 'i', 'in', 'open', '(', '"requirements.txt"', ')', '.', 'readlines', '(', ')', ']', '\\n']
Tokenized   (033): ['<s>', 'RE', 'QU', 'IR', 'ES', '=', '[', 'i', '.', 'strip', '(', ')', 'for', 'i', 'in', 'open', '(', '"', 'requ', 'irements', '.', 'txt', '"', ')', '.', 'read', 'lines', '(', ')', ']', '\\', 'n', '</s>']
Filtered   (031): ['RE', 'QU', 'IR', 'ES', '=', '[', 'i', '.', 'strip', '(', ')', 'for', 'i', 'in', 'open', '(', '"', 'requ', 'irements', '.', 'txt', '"', ')', '.', 'read', 'lines', '(', ')', ']', '\\', 'n']
Detokenized (021): ['REQUIRES', '=', '[', 'i', '.', 'strip', '(', ')', 'for', 'i', 'in', 'open', '(', '"requirements.txt"', ')', '.', 'readlines', '(', ')', ']', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "BaseField . __init__ ( self , ** kwargs ) \n"
Original    (010): ['BaseField', '.', '__init__', '(', 'self', ',', '**', 'kwargs', ')', '\\n']
Tokenized   (018): ['<s>', 'Base', 'Field', '.', '__', 'init', '__', '(', 'self', ',', '**', 'k', 'w', 'args', ')', '\\', 'n', '</s>']
Filtered   (016): ['Base', 'Field', '.', '__', 'init', '__', '(', 'self', ',', '**', 'k', 'w', 'args', ')', '\\', 'n']
Detokenized (010): ['BaseField', '.', '__init__', '(', 'self', ',', '**', 'kwargs', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "is_list = not hasattr ( items , ) \n"
Original    (009): ['is_list', '=', 'not', 'hasattr', '(', 'items', ',', ')', '\\n']
Tokenized   (015): ['<s>', 'is', '_', 'list', '=', 'not', 'has', 'attr', '(', 'items', ',', ')', '\\', 'n', '</s>']
Filtered   (013): ['is', '_', 'list', '=', 'not', 'has', 'attr', '(', 'items', ',', ')', '\\', 'n']
Detokenized (009): ['is_list', '=', 'not', 'hasattr', '(', 'items', ',', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "object_map [ ( collection , doc . id ) ] = doc \n"
Original    (013): ['object_map', '[', '(', 'collection', ',', 'doc', '.', 'id', ')', ']', '=', 'doc', '\\n']
Tokenized   (018): ['<s>', 'object', '_', 'map', '[', '(', 'collection', ',', 'doc', '.', 'id', ')', ']', '=', 'doc', '\\', 'n', '</s>']
Filtered   (016): ['object', '_', 'map', '[', '(', 'collection', ',', 'doc', '.', 'id', ')', ']', '=', 'doc', '\\', 'n']
Detokenized (013): ['object_map', '[', '(', 'collection', ',', 'doc', '.', 'id', ')', ']', '=', 'doc', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "_cls = doc . _data . pop ( , None ) \n"
Original    (012): ['_cls', '=', 'doc', '.', '_data', '.', 'pop', '(', ',', 'None', ')', '\\n']
Tokenized   (018): ['<s>', '_', 'cl', 's', '=', 'doc', '.', '_', 'data', '.', 'pop', '(', ',', 'None', ')', '\\', 'n', '</s>']
Filtered   (016): ['_', 'cl', 's', '=', 'doc', '.', '_', 'data', '.', 'pop', '(', ',', 'None', ')', '\\', 'n']
Detokenized (012): ['_cls', '=', 'doc', '.', '_data', '.', 'pop', '(', ',', 'None', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "81.4471435546875 , \n"
Original    (003): ['81.4471435546875', ',', '\\n']
Tokenized   (013): ['<s>', '81', '.', '447', '14', '35', '54', '68', '75', ',', '\\', 'n', '</s>']
Filtered   (011): ['81', '.', '447', '14', '35', '54', '68', '75', ',', '\\', 'n']
Detokenized (003): ['81.4471435546875', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "23.61432859499169 \n"
Original    (002): ['23.61432859499169', '\\n']
Tokenized   (012): ['<s>', '23', '.', '6', '143', '28', '59', '499', '169', '\\', 'n', '</s>']
Filtered   (010): ['23', '.', '6', '143', '28', '59', '499', '169', '\\', 'n']
Detokenized (002): ['23.61432859499169', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 2, 768)
# Extracted words:  2
Sentence         : "invalid_coords = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] ] ] ] \n"
Original    (020): ['invalid_coords', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ']', ']', ']', '\\n']
Tokenized   (027): ['<s>', 'in', 'valid', '_', 'co', 'ords', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ']', ']', ']', '\\', 'n', '</s>']
Filtered   (025): ['in', 'valid', '_', 'co', 'ords', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ']', ']', ']', '\\', 'n']
Detokenized (020): ['invalid_coords', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ']', ']', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "Location ( loc = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ] ) . validate ( ) \n"
Original    (039): ['Location', '(', 'loc', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ',', '[', '5', ',', '6', ']', ',', '[', '1', ',', '2', ']', ']', ']', ']', ')', '.', 'validate', '(', ')', '\\n']
Tokenized   (042): ['<s>', 'Location', '(', 'loc', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ',', '[', '5', ',', '6', ']', ',', '[', '1', ',', '2', ']', ']', ']', ']', ')', '.', 'validate', '(', ')', '\\', 'n', '</s>']
Filtered   (040): ['Location', '(', 'loc', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ',', '[', '5', ',', '6', ']', ',', '[', '1', ',', '2', ']', ']', ']', ']', ')', '.', 'validate', '(', ')', '\\', 'n']
Detokenized (039): ['Location', '(', 'loc', '=', '[', '[', '[', '[', '1', ',', '2', ']', ',', '[', '3', ',', '4', ']', ',', '[', '5', ',', '6', ']', ',', '[', '1', ',', '2', ']', ']', ']', ']', ')', '.', 'validate', '(', ')', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 39, 768)
# Extracted words:  39
Sentence         : "Parent ( name = ) . save ( ) \n"
Original    (010): ['Parent', '(', 'name', '=', ')', '.', 'save', '(', ')', '\\n']
Tokenized   (013): ['<s>', 'Parent', '(', 'name', '=', ')', '.', 'save', '(', ')', '\\', 'n', '</s>']
Filtered   (011): ['Parent', '(', 'name', '=', ')', '.', 'save', '(', ')', '\\', 'n']
Detokenized (010): ['Parent', '(', 'name', '=', ')', '.', 'save', '(', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "echo_payload = Struct ( "echo_payload" , \n"
Original    (007): ['echo_payload', '=', 'Struct', '(', '"echo_payload"', ',', '\\n']
Tokenized   (018): ['<s>', 'echo', '_', 'pay', 'load', '=', 'Struct', '(', '"', 'echo', '_', 'pay', 'load', '"', ',', '\\', 'n', '</s>']
Filtered   (016): ['echo', '_', 'pay', 'load', '=', 'Struct', '(', '"', 'echo', '_', 'pay', 'load', '"', ',', '\\', 'n']
Detokenized (007): ['echo_payload', '=', 'Struct', '(', '"echo_payload"', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "Padding ( 2 ) , \n"
Original    (006): ['Padding', '(', '2', ')', ',', '\\n']
Tokenized   (010): ['<s>', 'P', 'adding', '(', '2', ')', ',', '\\', 'n', '</s>']
Filtered   (008): ['P', 'adding', '(', '2', ')', ',', '\\', 'n']
Detokenized (006): ['Padding', '(', '2', ')', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "IpAddress ( "host" ) , \n"
Original    (006): ['IpAddress', '(', '"host"', ')', ',', '\\n']
Tokenized   (013): ['<s>', 'I', 'p', 'Address', '(', '"', 'host', '"', ')', ',', '\\', 'n', '</s>']
Filtered   (011): ['I', 'p', 'Address', '(', '"', 'host', '"', ')', ',', '\\', 'n']
Detokenized (006): ['IpAddress', '(', '"host"', ')', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "Bytes ( "echo" , 8 ) , \n"
Original    (008): ['Bytes', '(', '"echo"', ',', '8', ')', ',', '\\n']
Tokenized   (013): ['<s>', 'Bytes', '(', '"', 'echo', '"', ',', '8', ')', ',', '\\', 'n', '</s>']
Filtered   (011): ['Bytes', '(', '"', 'echo', '"', ',', '8', ')', ',', '\\', 'n']
Detokenized (008): ['Bytes', '(', '"echo"', ',', '8', ')', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "dest_unreachable_code = Enum ( Byte ( "code" ) , \n"
Original    (010): ['dest_unreachable_code', '=', 'Enum', '(', 'Byte', '(', '"code"', ')', ',', '\\n']
Tokenized   (022): ['<s>', 'dest', '_', 'un', 'reach', 'able', '_', 'code', '=', 'En', 'um', '(', 'Byte', '(', '"', 'code', '"', ')', ',', '\\', 'n', '</s>']
Filtered   (020): ['dest', '_', 'un', 'reach', 'able', '_', 'code', '=', 'En', 'um', '(', 'Byte', '(', '"', 'code', '"', ')', ',', '\\', 'n']
Detokenized (010): ['dest_unreachable_code', '=', 'Enum', '(', 'Byte', '(', '"code"', ')', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "Enum ( Byte ( "type" ) , \n"
Original    (008): ['Enum', '(', 'Byte', '(', '"type"', ')', ',', '\\n']
Tokenized   (014): ['<s>', 'En', 'um', '(', 'Byte', '(', '"', 'type', '"', ')', ',', '\\', 'n', '</s>']
Filtered   (012): ['En', 'um', '(', 'Byte', '(', '"', 'type', '"', ')', ',', '\\', 'n']
Detokenized (008): ['Enum', '(', 'Byte', '(', '"type"', ')', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "Switch ( "payload" , lambda ctx : ctx . type , \n"
Original    (012): ['Switch', '(', '"payload"', ',', 'lambda', 'ctx', ':', 'ctx', '.', 'type', ',', '\\n']
Tokenized   (020): ['<s>', 'Switch', '(', '"', 'pay', 'load', '"', ',', 'lambda', 'c', 'tx', ':', 'c', 'tx', '.', 'type', ',', '\\', 'n', '</s>']
Filtered   (018): ['Switch', '(', '"', 'pay', 'load', '"', ',', 'lambda', 'c', 'tx', ':', 'c', 'tx', '.', 'type', ',', '\\', 'n']
Detokenized (012): ['Switch', '(', '"payload"', ',', 'lambda', 'ctx', ':', 'ctx', '.', 'type', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : ""63646566676869" ) . decode ( "hex" ) \n"
Original    (008): ['"63646566676869"', ')', '.', 'decode', '(', '"hex"', ')', '\\n']
Tokenized   (021): ['<s>', '"', '63', '64', '65', '66', '67', '68', '69', '"', ')', '.', 'decode', '(', '"', 'hex', '"', ')', '\\', 'n', '</s>']
Filtered   (019): ['"', '63', '64', '65', '66', '67', '68', '69', '"', ')', '.', 'decode', '(', '"', 'hex', '"', ')', '\\', 'n']
Detokenized (008): ['"63646566676869"', ')', '.', 'decode', '(', '"hex"', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "cap2 = ( "0000385c02001b006162636465666768696a6b6c6d6e6f70717273747576776162" \n"
Original    (005): ['cap2', '=', '(', '"0000385c02001b006162636465666768696a6b6c6d6e6f70717273747576776162"', '\\n']
Tokenized   (044): ['<s>', 'cap', '2', '=', '(', '"', '0000', '385', 'c', '0', '2001', 'b', '006', '16', '26', '364', '65', '66', '67', '68', '696', 'a', '6', 'b', '6', 'c', '6', 'd', '6', 'e', '6', 'f', '707', '17', '27', '374', '75', '767', '76', '162', '"', '\\', 'n', '</s>']
Filtered   (042): ['cap', '2', '=', '(', '"', '0000', '385', 'c', '0', '2001', 'b', '006', '16', '26', '364', '65', '66', '67', '68', '696', 'a', '6', 'b', '6', 'c', '6', 'd', '6', 'e', '6', 'f', '707', '17', '27', '374', '75', '767', '76', '162', '"', '\\', 'n']
Detokenized (005): ['cap2', '=', '(', '"0000385c02001b006162636465666768696a6b6c6d6e6f70717273747576776162"', '\\n']
Counter: 42
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "cap3 = ( "0301000000001122aabbccdd0102030405060708" ) . decode ( "hex" ) \n"
Original    (011): ['cap3', '=', '(', '"0301000000001122aabbccdd0102030405060708"', ')', '.', 'decode', '(', '"hex"', ')', '\\n']
Tokenized   (034): ['<s>', 'cap', '3', '=', '(', '"', '03', '01', '00000000', '112', '2', 'a', 'abb', 'cc', 'dd', '010', '20', '30', '40', '50', '60', '708', '"', ')', '.', 'decode', '(', '"', 'hex', '"', ')', '\\', 'n', '</s>']
Filtered   (032): ['cap', '3', '=', '(', '"', '03', '01', '00000000', '112', '2', 'a', 'abb', 'cc', 'dd', '010', '20', '30', '40', '50', '60', '708', '"', ')', '.', 'decode', '(', '"', 'hex', '"', ')', '\\', 'n']
Detokenized (011): ['cap3', '=', '(', '"0301000000001122aabbccdd0102030405060708"', ')', '.', 'decode', '(', '"hex"', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "intps [ ] = dest_target . vlan \n"
Original    (008): ['intps', '[', ']', '=', 'dest_target', '.', 'vlan', '\\n']
Tokenized   (015): ['<s>', 'int', 'ps', '[', ']', '=', 'dest', '_', 'target', '.', 'v', 'lan', '\\', 'n', '</s>']
Filtered   (013): ['int', 'ps', '[', ']', '=', 'dest', '_', 'target', '.', 'v', 'lan', '\\', 'n']
Detokenized (008): ['intps', '[', ']', '=', 'dest_target', '.', 'vlan', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "router , interface = ri . split ( ) \n"
Original    (010): ['router', ',', 'interface', '=', 'ri', '.', 'split', '(', ')', '\\n']
Tokenized   (015): ['<s>', 'rou', 'ter', ',', 'interface', '=', 'r', 'i', '.', 'split', '(', ')', '\\', 'n', '</s>']
Filtered   (013): ['rou', 'ter', ',', 'interface', '=', 'r', 'i', '.', 'split', '(', ')', '\\', 'n']
Detokenized (010): ['router', ',', 'interface', '=', 'ri', '.', 'split', '(', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "cm = NCSVPNConnectionManager ( ncs_services_url , user , password , port_map , name ) \n"
Original    (015): ['cm', '=', 'NCSVPNConnectionManager', '(', 'ncs_services_url', ',', 'user', ',', 'password', ',', 'port_map', ',', 'name', ')', '\\n']
Tokenized   (029): ['<s>', 'cm', '=', 'N', 'CS', 'VPN', 'Connection', 'Manager', '(', 'n', 'cs', '_', 'services', '_', 'url', ',', 'user', ',', 'password', ',', 'port', '_', 'map', ',', 'name', ')', '\\', 'n', '</s>']
Filtered   (027): ['cm', '=', 'N', 'CS', 'VPN', 'Connection', 'Manager', '(', 'n', 'cs', '_', 'services', '_', 'url', ',', 'user', ',', 'password', ',', 'port', '_', 'map', ',', 'name', ')', '\\', 'n']
Detokenized (015): ['cm', '=', 'NCSVPNConnectionManager', '(', 'ncs_services_url', ',', 'user', ',', 'password', ',', 'port_map', ',', 'name', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "soap_resource . registerDecoder ( actions . QUERY_RECURSIVE , self . queryRecursive ) \n"
Original    (013): ['soap_resource', '.', 'registerDecoder', '(', 'actions', '.', 'QUERY_RECURSIVE', ',', 'self', '.', 'queryRecursive', ')', '\\n']
Tokenized   (029): ['<s>', 'so', 'ap', '_', 'resource', '.', 'register', 'Dec', 'oder', '(', 'actions', '.', 'QU', 'ERY', '_', 'REC', 'UR', 'S', 'IVE', ',', 'self', '.', 'query', 'Rec', 'ursive', ')', '\\', 'n', '</s>']
Filtered   (027): ['so', 'ap', '_', 'resource', '.', 'register', 'Dec', 'oder', '(', 'actions', '.', 'QU', 'ERY', '_', 'REC', 'UR', 'S', 'IVE', ',', 'self', '.', 'query', 'Rec', 'ursive', ')', '\\', 'n']
Detokenized (013): ['soap_resource', '.', 'registerDecoder', '(', 'actions', '.', 'QUERY_RECURSIVE', ',', 'self', '.', 'queryRecursive', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "soap_fault = soapresource . SOAPFault ( err . getErrorMessage ( ) , ex_element ) \n"
Original    (015): ['soap_fault', '=', 'soapresource', '.', 'SOAPFault', '(', 'err', '.', 'getErrorMessage', '(', ')', ',', 'ex_element', ')', '\\n']
Tokenized   (030): ['<s>', 'so', 'ap', '_', 'f', 'ault', '=', 'soap', 'resource', '.', 'SO', 'AP', 'F', 'ault', '(', 'err', '.', 'get', 'Error', 'Message', '(', ')', ',', 'ex', '_', 'element', ')', '\\', 'n', '</s>']
Filtered   (028): ['so', 'ap', '_', 'f', 'ault', '=', 'soap', 'resource', '.', 'SO', 'AP', 'F', 'ault', '(', 'err', '.', 'get', 'Error', 'Message', '(', ')', ',', 'ex', '_', 'element', ')', '\\', 'n']
Detokenized (015): ['soap_fault', '=', 'soapresource', '.', 'SOAPFault', '(', 'err', '.', 'getErrorMessage', '(', ')', ',', 'ex_element', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "symmetric = p2ps . symmetricPath or False sd = nsa . Point2PointService ( src_stp , dst_stp , p2ps . capacity , p2ps . directionality , symmetric , \n"
Original    (028): ['symmetric', '=', 'p2ps', '.', 'symmetricPath', 'or', 'False', 'sd', '=', 'nsa', '.', 'Point2PointService', '(', 'src_stp', ',', 'dst_stp', ',', 'p2ps', '.', 'capacity', ',', 'p2ps', '.', 'directionality', ',', 'symmetric', ',', '\\n']
Tokenized   (053): ['<s>', 'sy', 'mm', 'etric', '=', 'p', '2', 'ps', '.', 'symm', 'etric', 'Path', 'or', 'False', 'sd', '=', 'n', 'sa', '.', 'Point', '2', 'Point', 'Service', '(', 'src', '_', 'st', 'p', ',', 'dst', '_', 'st', 'p', ',', 'p', '2', 'ps', '.', 'capacity', ',', 'p', '2', 'ps', '.', 'direction', 'ality', ',', 'symm', 'etric', ',', '\\', 'n', '</s>']
Filtered   (051): ['sy', 'mm', 'etric', '=', 'p', '2', 'ps', '.', 'symm', 'etric', 'Path', 'or', 'False', 'sd', '=', 'n', 'sa', '.', 'Point', '2', 'Point', 'Service', '(', 'src', '_', 'st', 'p', ',', 'dst', '_', 'st', 'p', ',', 'p', '2', 'ps', '.', 'capacity', ',', 'p', '2', 'ps', '.', 'direction', 'ality', ',', 'symm', 'etric', ',', '\\', 'n']
Detokenized (028): ['symmetric', '=', 'p2ps', '.', 'symmetricPath', 'or', 'False', 'sd', '=', 'nsa', '.', 'Point2PointService', '(', 'src_stp', ',', 'dst_stp', ',', 'p2ps', '.', 'capacity', ',', 'p2ps', '.', 'directionality', ',', 'symmetric', ',', '\\n']
Counter: 51
===================================================================
Hidden states:  (13, 28, 768)
# Extracted words:  28
Sentence         : "crt = nsa . Criteria ( criteria . version , schedule , sd ) \n"
Original    (015): ['crt', '=', 'nsa', '.', 'Criteria', '(', 'criteria', '.', 'version', ',', 'schedule', ',', 'sd', ')', '\\n']
Tokenized   (021): ['<s>', 'cr', 't', '=', 'n', 'sa', '.', 'Crit', 'eria', '(', 'criteria', '.', 'version', ',', 'schedule', ',', 'sd', ')', '\\', 'n', '</s>']
Filtered   (019): ['cr', 't', '=', 'n', 'sa', '.', 'Crit', 'eria', '(', 'criteria', '.', 'version', ',', 'schedule', ',', 'sd', ')', '\\', 'n']
Detokenized (015): ['crt', '=', 'nsa', '.', 'Criteria', '(', 'criteria', '.', 'version', ',', 'schedule', ',', 'sd', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "tc = json . load ( open ( tcf ) ) \n"
Original    (012): ['tc', '=', 'json', '.', 'load', '(', 'open', '(', 'tcf', ')', ')', '\\n']
Tokenized   (016): ['<s>', 'tc', '=', 'json', '.', 'load', '(', 'open', '(', 't', 'cf', ')', ')', '\\', 'n', '</s>']
Filtered   (014): ['tc', '=', 'json', '.', 'load', '(', 'open', '(', 't', 'cf', ')', ')', '\\', 'n']
Detokenized (012): ['tc', '=', 'json', '.', 'load', '(', 'open', '(', 'tcf', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "source_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , dest_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , start_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 2 ) \n"
Original    (057): ['source_stp', '=', 'nsa', '.', 'STP', '(', ',', ',', 'labels', '=', '[', 'nsa', '.', 'Label', '(', 'nml', '.', 'ETHERNET_VLAN', ',', 'dest_stp', '=', 'nsa', '.', 'STP', '(', ',', ',', 'labels', '=', '[', 'nsa', '.', 'Label', '(', 'nml', '.', 'ETHERNET_VLAN', ',', 'start_time', '=', 'datetime', '.', 'datetime', '.', 'utcnow', '(', ')', '+', 'datetime', '.', 'timedelta', '(', 'seconds', '=', '2', ')', '\\n']
Tokenized   (092): ['<s>', 'source', '_', 'st', 'p', '=', 'n', 'sa', '.', 'ST', 'P', '(', ',', ',', 'labels', '=', '[', 'n', 'sa', '.', 'Label', '(', 'n', 'ml', '.', 'E', 'THER', 'NET', '_', 'V', 'LAN', ',', 'dest', '_', 'st', 'p', '=', 'n', 'sa', '.', 'ST', 'P', '(', ',', ',', 'labels', '=', '[', 'n', 'sa', '.', 'Label', '(', 'n', 'ml', '.', 'E', 'THER', 'NET', '_', 'V', 'LAN', ',', 'start', '_', 'time', '=', 'dat', 'etime', '.', 'dat', 'etime', '.', 'ut', 'c', 'now', '(', ')', '+', 'dat', 'etime', '.', 'timed', 'elta', '(', 'seconds', '=', '2', ')', '\\', 'n', '</s>']
Filtered   (090): ['source', '_', 'st', 'p', '=', 'n', 'sa', '.', 'ST', 'P', '(', ',', ',', 'labels', '=', '[', 'n', 'sa', '.', 'Label', '(', 'n', 'ml', '.', 'E', 'THER', 'NET', '_', 'V', 'LAN', ',', 'dest', '_', 'st', 'p', '=', 'n', 'sa', '.', 'ST', 'P', '(', ',', ',', 'labels', '=', '[', 'n', 'sa', '.', 'Label', '(', 'n', 'ml', '.', 'E', 'THER', 'NET', '_', 'V', 'LAN', ',', 'start', '_', 'time', '=', 'dat', 'etime', '.', 'dat', 'etime', '.', 'ut', 'c', 'now', '(', ')', '+', 'dat', 'etime', '.', 'timed', 'elta', '(', 'seconds', '=', '2', ')', '\\', 'n']
Detokenized (057): ['source_stp', '=', 'nsa', '.', 'STP', '(', ',', ',', 'labels', '=', '[', 'nsa', '.', 'Label', '(', 'nml', '.', 'ETHERNET_VLAN', ',', 'dest_stp', '=', 'nsa', '.', 'STP', '(', ',', ',', 'labels', '=', '[', 'nsa', '.', 'Label', '(', 'nml', '.', 'ETHERNET_VLAN', ',', 'start_time', '=', 'datetime', '.', 'datetime', '.', 'utcnow', '(', ')', '+', 'datetime', '.', 'timedelta', '(', 'seconds', '=', '2', ')', '\\n']
Counter: 90
===================================================================
Hidden states:  (13, 57, 768)
# Extracted words:  57
Sentence         : "end_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 30 ) \n"
Original    (019): ['end_time', '=', 'datetime', '.', 'datetime', '.', 'utcnow', '(', ')', '+', 'datetime', '.', 'timedelta', '(', 'seconds', '=', '30', ')', '\\n']
Tokenized   (030): ['<s>', 'end', '_', 'time', '=', 'dat', 'etime', '.', 'dat', 'etime', '.', 'ut', 'c', 'now', '(', ')', '+', 'dat', 'etime', '.', 'timed', 'elta', '(', 'seconds', '=', '30', ')', '\\', 'n', '</s>']
Filtered   (028): ['end', '_', 'time', '=', 'dat', 'etime', '.', 'dat', 'etime', '.', 'ut', 'c', 'now', '(', ')', '+', 'dat', 'etime', '.', 'timed', 'elta', '(', 'seconds', '=', '30', ')', '\\', 'n']
Detokenized (019): ['end_time', '=', 'datetime', '.', 'datetime', '.', 'utcnow', '(', ')', '+', 'datetime', '.', 'timedelta', '(', 'seconds', '=', '30', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "connection_id , active , version_consistent , version , timestamp = yield d_down \n"
Original    (013): ['connection_id', ',', 'active', ',', 'version_consistent', ',', 'version', ',', 'timestamp', '=', 'yield', 'd_down', '\\n']
Tokenized   (023): ['<s>', 'connection', '_', 'id', ',', 'active', ',', 'version', '_', 'cons', 'istent', ',', 'version', ',', 'timestamp', '=', 'yield', 'd', '_', 'down', '\\', 'n', '</s>']
Filtered   (021): ['connection', '_', 'id', ',', 'active', ',', 'version', '_', 'cons', 'istent', ',', 'version', ',', 'timestamp', '=', 'yield', 'd', '_', 'down', '\\', 'n']
Detokenized (013): ['connection_id', ',', 'active', ',', 'version_consistent', ',', 'version', ',', 'timestamp', '=', 'yield', 'd_down', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "scheduler = digits . scheduler . Scheduler ( config_value ( ) , True ) \n"
Original    (015): ['scheduler', '=', 'digits', '.', 'scheduler', '.', 'Scheduler', '(', 'config_value', '(', ')', ',', 'True', ')', '\\n']
Tokenized   (024): ['<s>', 'sc', 'hed', 'uler', '=', 'digits', '.', 'sched', 'uler', '.', 'Sched', 'uler', '(', 'config', '_', 'value', '(', ')', ',', 'True', ')', '\\', 'n', '</s>']
Filtered   (022): ['sc', 'hed', 'uler', '=', 'digits', '.', 'sched', 'uler', '.', 'Sched', 'uler', '(', 'config', '_', 'value', '(', ')', ',', 'True', ')', '\\', 'n']
Detokenized (015): ['scheduler', '=', 'digits', '.', 'scheduler', '.', 'Scheduler', '(', 'config_value', '(', ')', ',', 'True', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "row_index = int ( params [ ] [ 0 ] ) \n"
Original    (012): ['row_index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '\\n']
Tokenized   (017): ['<s>', 'row', '_', 'index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '\\', 'n', '</s>']
Filtered   (015): ['row', '_', 'index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '\\', 'n']
Detokenized (012): ['row_index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "char_index = int ( params [ ] [ 0 ] ) - 1 \n"
Original    (014): ['char_index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\n']
Tokenized   (019): ['<s>', 'char', '_', 'index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\', 'n', '</s>']
Filtered   (017): ['char', '_', 'index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\', 'n']
Detokenized (014): ['char_index', '=', 'int', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "comparator = comparators . index ( params [ ] [ 0 ] ) - 1 \n"
Original    (016): ['comparator', '=', 'comparators', '.', 'index', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\n']
Tokenized   (022): ['<s>', 'com', 'par', 'ator', '=', 'compar', 'ators', '.', 'index', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\', 'n', '</s>']
Filtered   (020): ['com', 'par', 'ator', '=', 'compar', 'ators', '.', 'index', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\', 'n']
Detokenized (016): ['comparator', '=', 'comparators', '.', 'index', '(', 'params', '[', ']', '[', '0', ']', ')', '-', '1', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "truth = ( cmp ( ord ( current_character ) , test_char ) == comparator ) \n"
Original    (016): ['truth', '=', '(', 'cmp', '(', 'ord', '(', 'current_character', ')', ',', 'test_char', ')', '==', 'comparator', ')', '\\n']
Tokenized   (025): ['<s>', 'truth', '=', '(', 'c', 'mp', '(', 'ord', '(', 'current', '_', 'character', ')', ',', 'test', '_', 'char', ')', '==', 'compar', 'ator', ')', '\\', 'n', '</s>']
Filtered   (023): ['truth', '=', '(', 'c', 'mp', '(', 'ord', '(', 'current', '_', 'character', ')', ',', 'test', '_', 'char', ')', '==', 'compar', 'ator', ')', '\\', 'n']
Detokenized (016): ['truth', '=', '(', 'cmp', '(', 'ord', '(', 'current_character', ')', ',', 'test_char', ')', '==', 'comparator', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "start_response ( , [ ( , ) ] ) \n"
Original    (010): ['start_response', '(', ',', '[', '(', ',', ')', ']', ')', '\\n']
Tokenized   (015): ['<s>', 'start', '_', 'response', '(', ',', '[', '(', ',', ')', ']', ')', '\\', 'n', '</s>']
Filtered   (013): ['start', '_', 'response', '(', ',', '[', '(', ',', ')', ']', ')', '\\', 'n']
Detokenized (010): ['start_response', '(', ',', '[', '(', ',', ')', ']', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "CHARSET = [ chr ( x ) for x in xrange ( 32 , 127 ) ] \n"
Original    (018): ['CHARSET', '=', '[', 'chr', '(', 'x', ')', 'for', 'x', 'in', 'xrange', '(', '32', ',', '127', ')', ']', '\\n']
Tokenized   (025): ['<s>', 'CH', 'ARS', 'ET', '=', '[', 'ch', 'r', '(', 'x', ')', 'for', 'x', 'in', 'x', 'range', '(', '32', ',', '127', ')', ']', '\\', 'n', '</s>']
Filtered   (023): ['CH', 'ARS', 'ET', '=', '[', 'ch', 'r', '(', 'x', ')', 'for', 'x', 'in', 'x', 'range', '(', '32', ',', '127', ')', ']', '\\', 'n']
Detokenized (018): ['CHARSET', '=', '[', 'chr', '(', 'x', ')', 'for', 'x', 'in', 'xrange', '(', '32', ',', '127', ')', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "obj_struct [ ] = [ int ( bbox . find ( ) . text ) , \n"
Original    (017): ['obj_struct', '[', ']', '=', '[', 'int', '(', 'bbox', '.', 'find', '(', ')', '.', 'text', ')', ',', '\\n']
Tokenized   (023): ['<s>', 'obj', '_', 'struct', '[', ']', '=', '[', 'int', '(', 'b', 'box', '.', 'find', '(', ')', '.', 'text', ')', ',', '\\', 'n', '</s>']
Filtered   (021): ['obj', '_', 'struct', '[', ']', '=', '[', 'int', '(', 'b', 'box', '.', 'find', '(', ')', '.', 'text', ')', ',', '\\', 'n']
Detokenized (017): ['obj_struct', '[', ']', '=', '[', 'int', '(', 'bbox', '.', 'find', '(', ')', '.', 'text', ')', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "int ( bbox . find ( ) . text ) ] \n"
Original    (012): ['int', '(', 'bbox', '.', 'find', '(', ')', '.', 'text', ')', ']', '\\n']
Tokenized   (016): ['<s>', 'int', '(', 'b', 'box', '.', 'find', '(', ')', '.', 'text', ')', ']', '\\', 'n', '</s>']
Filtered   (014): ['int', '(', 'b', 'box', '.', 'find', '(', ')', '.', 'text', ')', ']', '\\', 'n']
Detokenized (012): ['int', '(', 'bbox', '.', 'find', '(', ')', '.', 'text', ')', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "mpre = np . concatenate ( ( [ 0. ] , prec , [ 0. ] ) ) \n"
Original    (019): ['mpre', '=', 'np', '.', 'concatenate', '(', '(', '[', '0.', ']', ',', 'prec', ',', '[', '0.', ']', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'mp', 're', '=', 'np', '.', 'conc', 'aten', 'ate', '(', '(', '[', '0', '.', ']', ',', 'prec', ',', '[', '0', '.', ']', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['mp', 're', '=', 'np', '.', 'conc', 'aten', 'ate', '(', '(', '[', '0', '.', ']', ',', 'prec', ',', '[', '0', '.', ']', ')', ')', '\\', 'n']
Detokenized (019): ['mpre', '=', 'np', '.', 'concatenate', '(', '(', '[', '0.', ']', ',', 'prec', ',', '[', '0.', ']', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "difficult = np . array ( [ x [ ] for x in R ] ) . astype ( np . bool ) \n"
Original    (024): ['difficult', '=', 'np', '.', 'array', '(', '[', 'x', '[', ']', 'for', 'x', 'in', 'R', ']', ')', '.', 'astype', '(', 'np', '.', 'bool', ')', '\\n']
Tokenized   (029): ['<s>', 'diff', 'icult', '=', 'np', '.', 'array', '(', '[', 'x', '[', ']', 'for', 'x', 'in', 'R', ']', ')', '.', 'ast', 'ype', '(', 'np', '.', 'bool', ')', '\\', 'n', '</s>']
Filtered   (027): ['diff', 'icult', '=', 'np', '.', 'array', '(', '[', 'x', '[', ']', 'for', 'x', 'in', 'R', ']', ')', '.', 'ast', 'ype', '(', 'np', '.', 'bool', ')', '\\', 'n']
Detokenized (024): ['difficult', '=', 'np', '.', 'array', '(', '[', 'x', '[', ']', 'for', 'x', 'in', 'R', ']', ')', '.', 'astype', '(', 'np', '.', 'bool', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "det = [ False ] * len ( R ) \n"
Original    (011): ['det', '=', '[', 'False', ']', '*', 'len', '(', 'R', ')', '\\n']
Tokenized   (014): ['<s>', 'det', '=', '[', 'False', ']', '*', 'len', '(', 'R', ')', '\\', 'n', '</s>']
Filtered   (012): ['det', '=', '[', 'False', ']', '*', 'len', '(', 'R', ')', '\\', 'n']
Detokenized (011): ['det', '=', '[', 'False', ']', '*', 'len', '(', 'R', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "npos = npos + sum ( ~ difficult ) \n"
Original    (010): ['npos', '=', 'npos', '+', 'sum', '(', '~', 'difficult', ')', '\\n']
Tokenized   (015): ['<s>', 'n', 'pos', '=', 'n', 'pos', '+', 'sum', '(', '~', 'difficult', ')', '\\', 'n', '</s>']
Filtered   (013): ['n', 'pos', '=', 'n', 'pos', '+', 'sum', '(', '~', 'difficult', ')', '\\', 'n']
Detokenized (010): ['npos', '=', 'npos', '+', 'sum', '(', '~', 'difficult', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "class_recs [ imagename ] = { : bbox , \n"
Original    (010): ['class_recs', '[', 'imagename', ']', '=', '{', ':', 'bbox', ',', '\\n']
Tokenized   (018): ['<s>', 'class', '_', 're', 'cs', '[', 'imag', 'ename', ']', '=', '{', ':', 'b', 'box', ',', '\\', 'n', '</s>']
Filtered   (016): ['class', '_', 're', 'cs', '[', 'imag', 'ename', ']', '=', '{', ':', 'b', 'box', ',', '\\', 'n']
Detokenized (010): ['class_recs', '[', 'imagename', ']', '=', '{', ':', 'bbox', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "confidence = np . array ( [ float ( x [ 1 ] ) for x in splitlines ] ) \n"
Original    (021): ['confidence', '=', 'np', '.', 'array', '(', '[', 'float', '(', 'x', '[', '1', ']', ')', 'for', 'x', 'in', 'splitlines', ']', ')', '\\n']
Tokenized   (025): ['<s>', 'confidence', '=', 'np', '.', 'array', '(', '[', 'float', '(', 'x', '[', '1', ']', ')', 'for', 'x', 'in', 'split', 'lines', ']', ')', '\\', 'n', '</s>']
Filtered   (023): ['confidence', '=', 'np', '.', 'array', '(', '[', 'float', '(', 'x', '[', '1', ']', ')', 'for', 'x', 'in', 'split', 'lines', ']', ')', '\\', 'n']
Detokenized (021): ['confidence', '=', 'np', '.', 'array', '(', '[', 'float', '(', 'x', '[', '1', ']', ')', 'for', 'x', 'in', 'splitlines', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "sorted_ind = np . argsort ( - confidence ) \n"
Original    (010): ['sorted_ind', '=', 'np', '.', 'argsort', '(', '-', 'confidence', ')', '\\n']
Tokenized   (017): ['<s>', 's', 'orted', '_', 'ind', '=', 'np', '.', 'args', 'ort', '(', '-', 'confidence', ')', '\\', 'n', '</s>']
Filtered   (015): ['s', 'orted', '_', 'ind', '=', 'np', '.', 'args', 'ort', '(', '-', 'confidence', ')', '\\', 'n']
Detokenized (010): ['sorted_ind', '=', 'np', '.', 'argsort', '(', '-', 'confidence', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "BB = BB [ sorted_ind , : ] \n"
Original    (009): ['BB', '=', 'BB', '[', 'sorted_ind', ',', ':', ']', '\\n']
Tokenized   (014): ['<s>', 'BB', '=', 'BB', '[', 'sorted', '_', 'ind', ',', ':', ']', '\\', 'n', '</s>']
Filtered   (012): ['BB', '=', 'BB', '[', 'sorted', '_', 'ind', ',', ':', ']', '\\', 'n']
Detokenized (009): ['BB', '=', 'BB', '[', 'sorted_ind', ',', ':', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "image_ids = [ image_ids [ x ] for x in sorted_ind ] \n"
Original    (013): ['image_ids', '=', '[', 'image_ids', '[', 'x', ']', 'for', 'x', 'in', 'sorted_ind', ']', '\\n']
Tokenized   (022): ['<s>', 'image', '_', 'ids', '=', '[', 'image', '_', 'ids', '[', 'x', ']', 'for', 'x', 'in', 'sorted', '_', 'ind', ']', '\\', 'n', '</s>']
Filtered   (020): ['image', '_', 'ids', '=', '[', 'image', '_', 'ids', '[', 'x', ']', 'for', 'x', 'in', 'sorted', '_', 'ind', ']', '\\', 'n']
Detokenized (013): ['image_ids', '=', '[', 'image_ids', '[', 'x', ']', 'for', 'x', 'in', 'sorted_ind', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "bb = BB [ d , : ] . astype ( float ) \n"
Original    (014): ['bb', '=', 'BB', '[', 'd', ',', ':', ']', '.', 'astype', '(', 'float', ')', '\\n']
Tokenized   (018): ['<s>', 'bb', '=', 'BB', '[', 'd', ',', ':', ']', '.', 'ast', 'ype', '(', 'float', ')', '\\', 'n', '</s>']
Filtered   (016): ['bb', '=', 'BB', '[', 'd', ',', ':', ']', '.', 'ast', 'ype', '(', 'float', ')', '\\', 'n']
Detokenized (014): ['bb', '=', 'BB', '[', 'd', ',', ':', ']', '.', 'astype', '(', 'float', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "BBGT = R [ ] . astype ( float ) \n"
Original    (011): ['BBGT', '=', 'R', '[', ']', '.', 'astype', '(', 'float', ')', '\\n']
Tokenized   (016): ['<s>', 'BB', 'GT', '=', 'R', '[', ']', '.', 'ast', 'ype', '(', 'float', ')', '\\', 'n', '</s>']
Filtered   (014): ['BB', 'GT', '=', 'R', '[', ']', '.', 'ast', 'ype', '(', 'float', ')', '\\', 'n']
Detokenized (011): ['BBGT', '=', 'R', '[', ']', '.', 'astype', '(', 'float', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "iymin = np . maximum ( BBGT [ : , 1 ] , bb [ 1 ] ) \n"
Original    (019): ['iymin', '=', 'np', '.', 'maximum', '(', 'BBGT', '[', ':', ',', '1', ']', ',', 'bb', '[', '1', ']', ')', '\\n']
Tokenized   (025): ['<s>', 'iy', 'min', '=', 'np', '.', 'maximum', '(', 'BB', 'GT', '[', ':', ',', '1', ']', ',', 'b', 'b', '[', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (023): ['iy', 'min', '=', 'np', '.', 'maximum', '(', 'BB', 'GT', '[', ':', ',', '1', ']', ',', 'b', 'b', '[', '1', ']', ')', '\\', 'n']
Detokenized (019): ['iymin', '=', 'np', '.', 'maximum', '(', 'BBGT', '[', ':', ',', '1', ']', ',', 'bb', '[', '1', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "iw = np . maximum ( ixmax - ixmin + 1. , 0. ) \n"
Original    (015): ['iw', '=', 'np', '.', 'maximum', '(', 'ixmax', '-', 'ixmin', '+', '1.', ',', '0.', ')', '\\n']
Tokenized   (024): ['<s>', 'iw', '=', 'np', '.', 'maximum', '(', '', 'ix', 'max', '-', '', 'ix', 'min', '+', '1', '.', ',', '0', '.', ')', '\\', 'n', '</s>']
Filtered   (022): ['iw', '=', 'np', '.', 'maximum', '(', '', 'ix', 'max', '-', '', 'ix', 'min', '+', '1', '.', ',', '0', '.', ')', '\\', 'n']
Detokenized (015): ['iw', '=', 'np', '.', 'maximum', '(', 'ixmax', '-', 'ixmin', '+', '1.', ',', '0.', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "uni = ( ( bb [ 2 ] - bb [ 0 ] + 1. ) * ( bb [ 3 ] - bb [ 1 ] + 1. ) + \n"
Original    (032): ['uni', '=', '(', '(', 'bb', '[', '2', ']', '-', 'bb', '[', '0', ']', '+', '1.', ')', '*', '(', 'bb', '[', '3', ']', '-', 'bb', '[', '1', ']', '+', '1.', ')', '+', '\\n']
Tokenized   (041): ['<s>', 'uni', '=', '(', '(', 'b', 'b', '[', '2', ']', '-', 'b', 'b', '[', '0', ']', '+', '1', '.', ')', '*', '(', 'b', 'b', '[', '3', ']', '-', 'b', 'b', '[', '1', ']', '+', '1', '.', ')', '+', '\\', 'n', '</s>']
Filtered   (039): ['uni', '=', '(', '(', 'b', 'b', '[', '2', ']', '-', 'b', 'b', '[', '0', ']', '+', '1', '.', ')', '*', '(', 'b', 'b', '[', '3', ']', '-', 'b', 'b', '[', '1', ']', '+', '1', '.', ')', '+', '\\', 'n']
Detokenized (032): ['uni', '=', '(', '(', 'bb', '[', '2', ']', '-', 'bb', '[', '0', ']', '+', '1.', ')', '*', '(', 'bb', '[', '3', ']', '-', 'bb', '[', '1', ']', '+', '1.', ')', '+', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "rec = tp / float ( npos ) \n"
Original    (009): ['rec', '=', 'tp', '/', 'float', '(', 'npos', ')', '\\n']
Tokenized   (014): ['<s>', 'rec', '=', 't', 'p', '/', 'float', '(', 'n', 'pos', ')', '\\', 'n', '</s>']
Filtered   (012): ['rec', '=', 't', 'p', '/', 'float', '(', 'n', 'pos', ')', '\\', 'n']
Detokenized (009): ['rec', '=', 'tp', '/', 'float', '(', 'npos', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "prec = tp / ( tp + fp + 1e-10 ) \n"
Original    (012): ['prec', '=', 'tp', '/', '(', 'tp', '+', 'fp', '+', '1e-10', ')', '\\n']
Tokenized   (022): ['<s>', 'pre', 'c', '=', 't', 'p', '/', '(', 't', 'p', '+', 'f', 'p', '+', '1', 'e', '-', '10', ')', '\\', 'n', '</s>']
Filtered   (020): ['pre', 'c', '=', 't', 'p', '/', '(', 't', 'p', '+', 'f', 'p', '+', '1', 'e', '-', '10', ')', '\\', 'n']
Detokenized (012): ['prec', '=', 'tp', '/', '(', 'tp', '+', 'fp', '+', '1e-10', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "scale = strip_mantissa ( maxval ) / float ( 1 << ( bits - sign - 1 ) ) \n"
Original    (020): ['scale', '=', 'strip_mantissa', '(', 'maxval', ')', '/', 'float', '(', '1', '<<', '(', 'bits', '-', 'sign', '-', '1', ')', ')', '\\n']
Tokenized   (028): ['<s>', 'scale', '=', 'strip', '_', 'm', 'ant', 'issa', '(', 'max', 'val', ')', '/', 'float', '(', '1', '<<', '(', 'bits', '-', 'sign', '-', '1', ')', ')', '\\', 'n', '</s>']
Filtered   (026): ['scale', '=', 'strip', '_', 'm', 'ant', 'issa', '(', 'max', 'val', ')', '/', 'float', '(', '1', '<<', '(', 'bits', '-', 'sign', '-', '1', ')', ')', '\\', 'n']
Detokenized (020): ['scale', '=', 'strip_mantissa', '(', 'maxval', ')', '/', 'float', '(', '1', '<<', '(', 'bits', '-', 'sign', '-', '1', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "ary = np . around ( ary * ( 1.0 / scale ) ) . astype ( np . int64 ) \n"
Original    (022): ['ary', '=', 'np', '.', 'around', '(', 'ary', '*', '(', '1.0', '/', 'scale', ')', ')', '.', 'astype', '(', 'np', '.', 'int64', ')', '\\n']
Tokenized   (030): ['<s>', 'ary', '=', 'np', '.', 'around', '(', 'a', 'ry', '*', '(', '1', '.', '0', '/', 'scale', ')', ')', '.', 'ast', 'ype', '(', 'np', '.', 'int', '64', ')', '\\', 'n', '</s>']
Filtered   (028): ['ary', '=', 'np', '.', 'around', '(', 'a', 'ry', '*', '(', '1', '.', '0', '/', 'scale', ')', ')', '.', 'ast', 'ype', '(', 'np', '.', 'int', '64', ')', '\\', 'n']
Detokenized (022): ['ary', '=', 'np', '.', 'around', '(', 'ary', '*', '(', '1.0', '/', 'scale', ')', ')', '.', 'astype', '(', 'np', '.', 'int64', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "f2 -= dif \n"
Original    (004): ['f2', '-=', 'dif', '\\n']
Tokenized   (009): ['<s>', 'f', '2', '-=', 'd', 'if', '\\', 'n', '</s>']
Filtered   (007): ['f', '2', '-=', 'd', 'if', '\\', 'n']
Detokenized (004): ['f2', '-=', 'dif', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "slicedF = F [ : , sliceR , sliceS , : ] . reshape ( ( - 1 , K ) ) \n"
Original    (023): ['slicedF', '=', 'F', '[', ':', ',', 'sliceR', ',', 'sliceS', ',', ':', ']', '.', 'reshape', '(', '(', '-', '1', ',', 'K', ')', ')', '\\n']
Tokenized   (032): ['<s>', 's', 'lic', 'ed', 'F', '=', 'F', '[', ':', ',', 'slice', 'R', ',', 'slice', 'S', ',', ':', ']', '.', 'resh', 'ape', '(', '(', '-', '1', ',', 'K', ')', ')', '\\', 'n', '</s>']
Filtered   (030): ['s', 'lic', 'ed', 'F', '=', 'F', '[', ':', ',', 'slice', 'R', ',', 'slice', 'S', ',', ':', ']', '.', 'resh', 'ape', '(', '(', '-', '1', ',', 'K', ')', ')', '\\', 'n']
Detokenized (023): ['slicedF', '=', 'F', '[', ':', ',', 'sliceR', ',', 'sliceS', ',', ':', ']', '.', 'reshape', '(', '(', '-', '1', ',', 'K', ')', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "K , P , Q , N = E . shape \n"
Original    (012): ['K', ',', 'P', ',', 'Q', ',', 'N', '=', 'E', '.', 'shape', '\\n']
Tokenized   (015): ['<s>', 'K', ',', 'P', ',', 'Q', ',', 'N', '=', 'E', '.', 'shape', '\\', 'n', '</s>']
Filtered   (013): ['K', ',', 'P', ',', 'Q', ',', 'N', '=', 'E', '.', 'shape', '\\', 'n']
Detokenized (012): ['K', ',', 'P', ',', 'Q', ',', 'N', '=', 'E', '.', 'shape', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "qSlice = [ fconv_slice ( q , S , X , padding [ 0 ] , strides [ 0 ] ) for q in range ( Q ) ] \n"
Original    (030): ['qSlice', '=', '[', 'fconv_slice', '(', 'q', ',', 'S', ',', 'X', ',', 'padding', '[', '0', ']', ',', 'strides', '[', '0', ']', ')', 'for', 'q', 'in', 'range', '(', 'Q', ')', ']', '\\n']
Tokenized   (038): ['<s>', 'q', 'Sl', 'ice', '=', '[', 'f', 'conv', '_', 'slice', '(', 'q', ',', 'S', ',', 'X', ',', 'padding', '[', '0', ']', ',', 'strides', '[', '0', ']', ')', 'for', 'q', 'in', 'range', '(', 'Q', ')', ']', '\\', 'n', '</s>']
Filtered   (036): ['q', 'Sl', 'ice', '=', '[', 'f', 'conv', '_', 'slice', '(', 'q', ',', 'S', ',', 'X', ',', 'padding', '[', '0', ']', ',', 'strides', '[', '0', ']', ')', 'for', 'q', 'in', 'range', '(', 'Q', ')', ']', '\\', 'n']
Detokenized (030): ['qSlice', '=', '[', 'fconv_slice', '(', 'q', ',', 'S', ',', 'X', ',', 'padding', '[', '0', ']', ',', 'strides', '[', '0', ']', ')', 'for', 'q', 'in', 'range', '(', 'Q', ')', ']', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 30, 768)
# Extracted words:  30
Sentence         : "slicedE = E [ : , p , q , : ] \n"
Original    (013): ['slicedE', '=', 'E', '[', ':', ',', 'p', ',', 'q', ',', ':', ']', '\\n']
Tokenized   (019): ['<s>', 's', 'lic', 'ed', 'E', '=', 'E', '[', ':', ',', 'p', ',', 'q', ',', ':', ']', '\\', 'n', '</s>']
Filtered   (017): ['s', 'lic', 'ed', 'E', '=', 'E', '[', ':', ',', 'p', ',', 'q', ',', ':', ']', '\\', 'n']
Detokenized (013): ['slicedE', '=', 'E', '[', ':', ',', 'p', ',', 'q', ',', ':', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "rcp3 = 1.0 / 3.0 \n"
Original    (006): ['rcp3', '=', '1.0', '/', '3.0', '\\n']
Tokenized   (015): ['<s>', 'rc', 'p', '3', '=', '1', '.', '0', '/', '3', '.', '0', '\\', 'n', '</s>']
Filtered   (013): ['rc', 'p', '3', '=', '1', '.', '0', '/', '3', '.', '0', '\\', 'n']
Detokenized (006): ['rcp3', '=', '1.0', '/', '3.0', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "t3 = I [ 1 , : ] + I [ 3 , : ] * 4.0 \n"
Original    (018): ['t3', '=', 'I', '[', '1', ',', ':', ']', '+', 'I', '[', '3', ',', ':', ']', '*', '4.0', '\\n']
Tokenized   (024): ['<s>', 't', '3', '=', 'I', '[', '1', ',', ':', ']', '+', 'I', '[', '3', ',', ':', ']', '*', '4', '.', '0', '\\', 'n', '</s>']
Filtered   (022): ['t', '3', '=', 'I', '[', '1', ',', ':', ']', '+', 'I', '[', '3', ',', ':', ']', '*', '4', '.', '0', '\\', 'n']
Detokenized (018): ['t3', '=', 'I', '[', '1', ',', ':', ']', '+', 'I', '[', '3', ',', ':', ']', '*', '4.0', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "T1 = np . empty ( ( 3 , 3 ) ) \n"
Original    (013): ['T1', '=', 'np', '.', 'empty', '(', '(', '3', ',', '3', ')', ')', '\\n']
Tokenized   (017): ['<s>', 'T', '1', '=', 'np', '.', 'empty', '(', '(', '3', ',', '3', ')', ')', '\\', 'n', '</s>']
Filtered   (015): ['T', '1', '=', 'np', '.', 'empty', '(', '(', '3', ',', '3', ')', ')', '\\', 'n']
Detokenized (013): ['T1', '=', 'np', '.', 'empty', '(', '(', '3', ',', '3', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "Fw = np . empty ( ( D , D , C , K ) ) \n"
Original    (017): ['Fw', '=', 'np', '.', 'empty', '(', '(', 'D', ',', 'D', ',', 'C', ',', 'K', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'F', 'w', '=', 'np', '.', 'empty', '(', '(', 'D', ',', 'D', ',', 'C', ',', 'K', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['F', 'w', '=', 'np', '.', 'empty', '(', '(', 'D', ',', 'D', ',', 'C', ',', 'K', ')', ')', '\\', 'n']
Detokenized (017): ['Fw', '=', 'np', '.', 'empty', '(', '(', 'D', ',', 'D', ',', 'C', ',', 'K', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "sliceI = I [ : , start_y : stop_y , start_x : stop_x , : ] \n"
Original    (017): ['sliceI', '=', 'I', '[', ':', ',', 'start_y', ':', 'stop_y', ',', 'start_x', ':', 'stop_x', ',', ':', ']', '\\n']
Tokenized   (029): ['<s>', 'slice', 'I', '=', 'I', '[', ':', ',', 'start', '_', 'y', ':', 'stop', '_', 'y', ',', 'start', '_', 'x', ':', 'stop', '_', 'x', ',', ':', ']', '\\', 'n', '</s>']
Filtered   (027): ['slice', 'I', '=', 'I', '[', ':', ',', 'start', '_', 'y', ':', 'stop', '_', 'y', ',', 'start', '_', 'x', ':', 'stop', '_', 'x', ',', ':', ']', '\\', 'n']
Detokenized (017): ['sliceI', '=', 'I', '[', ':', ',', 'start_y', ':', 'stop_y', ',', 'start_x', ':', 'stop_x', ',', ':', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "O [ k , p0 : p1 , q0 : q1 , n ] = Out [ 0 : plen , 0 : qlen ] \n"
Original    (026): ['O', '[', 'k', ',', 'p0', ':', 'p1', ',', 'q0', ':', 'q1', ',', 'n', ']', '=', 'Out', '[', '0', ':', 'plen', ',', '0', ':', 'qlen', ']', '\\n']
Tokenized   (035): ['<s>', 'O', '[', 'k', ',', 'p', '0', ':', 'p', '1', ',', 'q', '0', ':', 'q', '1', ',', 'n', ']', '=', 'Out', '[', '0', ':', 'pl', 'en', ',', '0', ':', 'q', 'len', ']', '\\', 'n', '</s>']
Filtered   (033): ['O', '[', 'k', ',', 'p', '0', ':', 'p', '1', ',', 'q', '0', ':', 'q', '1', ',', 'n', ']', '=', 'Out', '[', '0', ':', 'pl', 'en', ',', '0', ':', 'q', 'len', ']', '\\', 'n']
Detokenized (026): ['O', '[', 'k', ',', 'p0', ':', 'p1', ',', 'q0', ':', 'q1', ',', 'n', ']', '=', 'Out', '[', '0', ':', 'plen', ',', '0', ':', 'qlen', ']', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "start_p , stop_p , pad_p = image_slice ( y , P , B , B ) \n"
Original    (017): ['start_p', ',', 'stop_p', ',', 'pad_p', '=', 'image_slice', '(', 'y', ',', 'P', ',', 'B', ',', 'B', ')', '\\n']
Tokenized   (028): ['<s>', 'start', '_', 'p', ',', 'stop', '_', 'p', ',', 'pad', '_', 'p', '=', 'image', '_', 'slice', '(', 'y', ',', 'P', ',', 'B', ',', 'B', ')', '\\', 'n', '</s>']
Filtered   (026): ['start', '_', 'p', ',', 'stop', '_', 'p', ',', 'pad', '_', 'p', '=', 'image', '_', 'slice', '(', 'y', ',', 'P', ',', 'B', ',', 'B', ')', '\\', 'n']
Detokenized (017): ['start_p', ',', 'stop_p', ',', 'pad_p', '=', 'image_slice', '(', 'y', ',', 'P', ',', 'B', ',', 'B', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "trans = ( 2 , 2 ) \n"
Original    (008): ['trans', '=', '(', '2', ',', '2', ')', '\\n']
Tokenized   (011): ['<s>', 'trans', '=', '(', '2', ',', '2', ')', '\\', 'n', '</s>']
Filtered   (009): ['trans', '=', '(', '2', ',', '2', ')', '\\', 'n']
Detokenized (008): ['trans', '=', '(', '2', ',', '2', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "E = np . random . uniform ( - 1.0 , 1.0 , dimO ) \n"
Original    (016): ['E', '=', 'np', '.', 'random', '.', 'uniform', '(', '-', '1.0', ',', '1.0', ',', 'dimO', ')', '\\n']
Tokenized   (024): ['<s>', 'E', '=', 'np', '.', 'random', '.', 'uniform', '(', '-', '1', '.', '0', ',', '1', '.', '0', ',', 'dim', 'O', ')', '\\', 'n', '</s>']
Filtered   (022): ['E', '=', 'np', '.', 'random', '.', 'uniform', '(', '-', '1', '.', '0', ',', '1', '.', '0', ',', 'dim', 'O', ')', '\\', 'n']
Detokenized (016): ['E', '=', 'np', '.', 'random', '.', 'uniform', '(', '-', '1.0', ',', '1.0', ',', 'dimO', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "xprop_direct ( E , F , Bd , padding , strides , backward = True ) \n"
Original    (017): ['xprop_direct', '(', 'E', ',', 'F', ',', 'Bd', ',', 'padding', ',', 'strides', ',', 'backward', '=', 'True', ')', '\\n']
Tokenized   (024): ['<s>', 'x', 'prop', '_', 'direct', '(', 'E', ',', 'F', ',', 'B', 'd', ',', 'padding', ',', 'strides', ',', 'backward', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (022): ['x', 'prop', '_', 'direct', '(', 'E', ',', 'F', ',', 'B', 'd', ',', 'padding', ',', 'strides', ',', 'backward', '=', 'True', ')', '\\', 'n']
Detokenized (017): ['xprop_direct', '(', 'E', ',', 'F', ',', 'Bd', ',', 'padding', ',', 'strides', ',', 'backward', '=', 'True', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "xprop_winograd ( E , F , Bw , padding , minimal = minimal , trans = trans , backward = True ) \n"
Original    (023): ['xprop_winograd', '(', 'E', ',', 'F', ',', 'Bw', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ',', 'backward', '=', 'True', ')', '\\n']
Tokenized   (032): ['<s>', 'x', 'prop', '_', 'win', 'og', 'rad', '(', 'E', ',', 'F', ',', 'B', 'w', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ',', 'backward', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (030): ['x', 'prop', '_', 'win', 'og', 'rad', '(', 'E', ',', 'F', ',', 'B', 'w', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ',', 'backward', '=', 'True', ')', '\\', 'n']
Detokenized (023): ['xprop_winograd', '(', 'E', ',', 'F', ',', 'Bw', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ',', 'backward', '=', 'True', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "updat_direct ( I , E , Ud , padding , strides ) \n"
Original    (013): ['updat_direct', '(', 'I', ',', 'E', ',', 'Ud', ',', 'padding', ',', 'strides', ')', '\\n']
Tokenized   (019): ['<s>', 'up', 'dat', '_', 'direct', '(', 'I', ',', 'E', ',', 'Ud', ',', 'padding', ',', 'strides', ')', '\\', 'n', '</s>']
Filtered   (017): ['up', 'dat', '_', 'direct', '(', 'I', ',', 'E', ',', 'Ud', ',', 'padding', ',', 'strides', ')', '\\', 'n']
Detokenized (013): ['updat_direct', '(', 'I', ',', 'E', ',', 'Ud', ',', 'padding', ',', 'strides', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "updat_winograd ( I , E , Uw , padding , minimal = minimal , trans = trans ) \n"
Original    (019): ['updat_winograd', '(', 'I', ',', 'E', ',', 'Uw', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ')', '\\n']
Tokenized   (028): ['<s>', 'up', 'dat', '_', 'win', 'og', 'rad', '(', 'I', ',', 'E', ',', 'U', 'w', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ')', '\\', 'n', '</s>']
Filtered   (026): ['up', 'dat', '_', 'win', 'og', 'rad', '(', 'I', ',', 'E', ',', 'U', 'w', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ')', '\\', 'n']
Detokenized (019): ['updat_winograd', '(', 'I', ',', 'E', ',', 'Uw', ',', 'padding', ',', 'minimal', '=', 'minimal', ',', 'trans', '=', 'trans', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "BranchNode , SkipNode , LRN , ColorNoise ) \n"
Original    (009): ['BranchNode', ',', 'SkipNode', ',', 'LRN', ',', 'ColorNoise', ')', '\\n']
Tokenized   (018): ['<s>', 'B', 'ranch', 'Node', ',', 'Skip', 'Node', ',', 'LR', 'N', ',', 'Color', 'No', 'ise', ')', '\\', 'n', '</s>']
Filtered   (016): ['B', 'ranch', 'Node', ',', 'Skip', 'Node', ',', 'LR', 'N', ',', 'Color', 'No', 'ise', ')', '\\', 'n']
Detokenized (009): ['BranchNode', ',', 'SkipNode', ',', 'LRN', ',', 'ColorNoise', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "img_set_options = dict ( repo_dir = args . data_dir , \n"
Original    (011): ['img_set_options', '=', 'dict', '(', 'repo_dir', '=', 'args', '.', 'data_dir', ',', '\\n']
Tokenized   (022): ['<s>', 'img', '_', 'set', '_', 'options', '=', 'dict', '(', 'repo', '_', 'dir', '=', 'args', '.', 'data', '_', 'dir', ',', '\\', 'n', '</s>']
Filtered   (020): ['img', '_', 'set', '_', 'options', '=', 'dict', '(', 'repo', '_', 'dir', '=', 'args', '.', 'data', '_', 'dir', ',', '\\', 'n']
Detokenized (011): ['img_set_options', '=', 'dict', '(', 'repo_dir', '=', 'args', '.', 'data_dir', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "subset_pct = 0.09990891117239205 ) \n"
Original    (005): ['subset_pct', '=', '0.09990891117239205', ')', '\\n']
Tokenized   (020): ['<s>', 'sub', 'set', '_', 'p', 'ct', '=', '0', '.', '0', '999', '08', '911', '17', '239', '205', ')', '\\', 'n', '</s>']
Filtered   (018): ['sub', 'set', '_', 'p', 'ct', '=', '0', '.', '0', '999', '08', '911', '17', '239', '205', ')', '\\', 'n']
Detokenized (005): ['subset_pct', '=', '0.09990891117239205', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "do_transforms = False , ** img_set_options ) \n"
Original    (008): ['do_transforms', '=', 'False', ',', '**', 'img_set_options', ')', '\\n']
Tokenized   (018): ['<s>', 'do', '_', 'trans', 'forms', '=', 'False', ',', '**', 'img', '_', 'set', '_', 'options', ')', '\\', 'n', '</s>']
Filtered   (016): ['do', '_', 'trans', 'forms', '=', 'False', ',', '**', 'img', '_', 'set', '_', 'options', ')', '\\', 'n']
Detokenized (008): ['do_transforms', '=', 'False', ',', '**', 'img_set_options', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "test = ImageLoader ( set_name = , scale_range = ( 256 , 384 ) , shuffle = False , \n"
Original    (020): ['test', '=', 'ImageLoader', '(', 'set_name', '=', ',', 'scale_range', '=', '(', '256', ',', '384', ')', ',', 'shuffle', '=', 'False', ',', '\\n']
Tokenized   (028): ['<s>', 'test', '=', 'Image', 'Loader', '(', 'set', '_', 'name', '=', ',', 'scale', '_', 'range', '=', '(', '256', ',', '384', ')', ',', 'shuffle', '=', 'False', ',', '\\', 'n', '</s>']
Filtered   (026): ['test', '=', 'Image', 'Loader', '(', 'set', '_', 'name', '=', ',', 'scale', '_', 'range', '=', '(', '256', ',', '384', ')', ',', 'shuffle', '=', 'False', ',', '\\', 'n']
Detokenized (020): ['test', '=', 'ImageLoader', '(', 'set_name', '=', ',', 'scale_range', '=', '(', '256', ',', '384', ')', ',', 'shuffle', '=', 'False', ',', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "Pooling ( 3 , strides = 2 ) , \n"
Original    (010): ['Pooling', '(', '3', ',', 'strides', '=', '2', ')', ',', '\\n']
Tokenized   (014): ['<s>', 'Pool', 'ing', '(', '3', ',', 'strides', '=', '2', ')', ',', '\\', 'n', '</s>']
Filtered   (012): ['Pool', 'ing', '(', '3', ',', 'strides', '=', '2', ')', ',', '\\', 'n']
Detokenized (010): ['Pooling', '(', '3', ',', 'strides', '=', '2', ')', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "activation = Rectlin ( ) , padding = 1 ) , \n"
Original    (012): ['activation', '=', 'Rectlin', '(', ')', ',', 'padding', '=', '1', ')', ',', '\\n']
Tokenized   (016): ['<s>', 'activation', '=', 'Rect', 'lin', '(', ')', ',', 'padding', '=', '1', ')', ',', '\\', 'n', '</s>']
Filtered   (014): ['activation', '=', 'Rect', 'lin', '(', ')', ',', 'padding', '=', '1', ')', ',', '\\', 'n']
Detokenized (012): ['activation', '=', 'Rectlin', '(', ')', ',', 'padding', '=', '1', ')', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "Conv ( ( 3 , 3 , 256 ) , init = Gaussian ( scale = 0.03 ) , bias = Constant ( 1 ) , \n"
Original    (027): ['Conv', '(', '(', '3', ',', '3', ',', '256', ')', ',', 'init', '=', 'Gaussian', '(', 'scale', '=', '0.03', ')', ',', 'bias', '=', 'Constant', '(', '1', ')', ',', '\\n']
Tokenized   (034): ['<s>', 'Con', 'v', '(', '(', '3', ',', '3', ',', '256', ')', ',', 'init', '=', 'Ga', 'ussian', '(', 'scale', '=', '0', '.', '03', ')', ',', 'bias', '=', 'Constant', '(', '1', ')', ',', '\\', 'n', '</s>']
Filtered   (032): ['Con', 'v', '(', '(', '3', ',', '3', ',', '256', ')', ',', 'init', '=', 'Ga', 'ussian', '(', 'scale', '=', '0', '.', '03', ')', ',', 'bias', '=', 'Constant', '(', '1', ')', ',', '\\', 'n']
Detokenized (027): ['Conv', '(', '(', '3', ',', '3', ',', '256', ')', ',', 'init', '=', 'Gaussian', '(', 'scale', '=', '0.03', ')', ',', 'bias', '=', 'Constant', '(', '1', ')', ',', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "Dropout ( keep = 1.0 ) , \n"
Original    (008): ['Dropout', '(', 'keep', '=', '1.0', ')', ',', '\\n']
Tokenized   (014): ['<s>', 'Drop', 'out', '(', 'keep', '=', '1', '.', '0', ')', ',', '\\', 'n', '</s>']
Filtered   (012): ['Drop', 'out', '(', 'keep', '=', '1', '.', '0', ')', ',', '\\', 'n']
Detokenized (008): ['Dropout', '(', 'keep', '=', '1.0', ')', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "Affine ( nout = 1000 , init = Gaussian ( scale = 0.01 ) , bias = Constant ( - 7 ) , activation = Softmax ( ) ) ] \n"
Original    (031): ['Affine', '(', 'nout', '=', '1000', ',', 'init', '=', 'Gaussian', '(', 'scale', '=', '0.01', ')', ',', 'bias', '=', 'Constant', '(', '-', '7', ')', ',', 'activation', '=', 'Softmax', '(', ')', ')', ']', '\\n']
Tokenized   (040): ['<s>', 'Aff', 'ine', '(', 'n', 'out', '=', '1000', ',', 'init', '=', 'Ga', 'ussian', '(', 'scale', '=', '0', '.', '01', ')', ',', 'bias', '=', 'Constant', '(', '-', '7', ')', ',', 'activation', '=', 'Soft', 'max', '(', ')', ')', ']', '\\', 'n', '</s>']
Filtered   (038): ['Aff', 'ine', '(', 'n', 'out', '=', '1000', ',', 'init', '=', 'Ga', 'ussian', '(', 'scale', '=', '0', '.', '01', ')', ',', 'bias', '=', 'Constant', '(', '-', '7', ')', ',', 'activation', '=', 'Soft', 'max', '(', ')', ')', ']', '\\', 'n']
Detokenized (031): ['Affine', '(', 'nout', '=', '1000', ',', 'init', '=', 'Gaussian', '(', 'scale', '=', '0.01', ')', ',', 'bias', '=', 'Constant', '(', '-', '7', ')', ',', 'activation', '=', 'Softmax', '(', ')', ')', ']', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 31, 768)
# Extracted words:  31
Sentence         : "weight_sched = Schedule ( [ 22 , 44 , 65 ] , ( 1 / 250. ) ** ( 1 / 3. ) ) \n"
Original    (025): ['weight_sched', '=', 'Schedule', '(', '[', '22', ',', '44', ',', '65', ']', ',', '(', '1', '/', '250.', ')', '**', '(', '1', '/', '3.', ')', ')', '\\n']
Tokenized   (033): ['<s>', 'weight', '_', 'sc', 'hed', '=', 'Schedule', '(', '[', '22', ',', '44', ',', '65', ']', ',', '(', '1', '/', '250', '.', ')', '**', '(', '1', '/', '3', '.', ')', ')', '\\', 'n', '</s>']
Filtered   (031): ['weight', '_', 'sc', 'hed', '=', 'Schedule', '(', '[', '22', ',', '44', ',', '65', ']', ',', '(', '1', '/', '250', '.', ')', '**', '(', '1', '/', '3', '.', ')', ')', '\\', 'n']
Detokenized (025): ['weight_sched', '=', 'Schedule', '(', '[', '22', ',', '44', ',', '65', ']', ',', '(', '1', '/', '250.', ')', '**', '(', '1', '/', '3.', ')', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "opt_gdm = GradientDescentMomentum ( 0.01 / 10 , 0.9 , wdecay = 0.0005 , schedule = weight_sched , \n"
Original    (019): ['opt_gdm', '=', 'GradientDescentMomentum', '(', '0.01', '/', '10', ',', '0.9', ',', 'wdecay', '=', '0.0005', ',', 'schedule', '=', 'weight_sched', ',', '\\n']
Tokenized   (043): ['<s>', 'opt', '_', 'gd', 'm', '=', 'Grad', 'ient', 'Des', 'cent', 'Mom', 'ent', 'um', '(', '0', '.', '01', '/', '10', ',', '0', '.', '9', ',', 'w', 'dec', 'ay', '=', '0', '.', '000', '5', ',', 'schedule', '=', 'weight', '_', 'sc', 'hed', ',', '\\', 'n', '</s>']
Filtered   (041): ['opt', '_', 'gd', 'm', '=', 'Grad', 'ient', 'Des', 'cent', 'Mom', 'ent', 'um', '(', '0', '.', '01', '/', '10', ',', '0', '.', '9', ',', 'w', 'dec', 'ay', '=', '0', '.', '000', '5', ',', 'schedule', '=', 'weight', '_', 'sc', 'hed', ',', '\\', 'n']
Detokenized (019): ['opt_gdm', '=', 'GradientDescentMomentum', '(', '0.01', '/', '10', ',', '0.9', ',', 'wdecay', '=', '0.0005', ',', 'schedule', '=', 'weight_sched', ',', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "opt_biases = GradientDescentMomentum ( 0.02 / 10 , 0.9 , schedule = Schedule ( [ 44 ] , 0.1 ) , \n"
Original    (022): ['opt_biases', '=', 'GradientDescentMomentum', '(', '0.02', '/', '10', ',', '0.9', ',', 'schedule', '=', 'Schedule', '(', '[', '44', ']', ',', '0.1', ')', ',', '\\n']
Tokenized   (040): ['<s>', 'opt', '_', 'bi', 'ases', '=', 'Grad', 'ient', 'Des', 'cent', 'Mom', 'ent', 'um', '(', '0', '.', '02', '/', '10', ',', '0', '.', '9', ',', 'schedule', '=', 'Schedule', '(', '[', '44', ']', ',', '0', '.', '1', ')', ',', '\\', 'n', '</s>']
Filtered   (038): ['opt', '_', 'bi', 'ases', '=', 'Grad', 'ient', 'Des', 'cent', 'Mom', 'ent', 'um', '(', '0', '.', '02', '/', '10', ',', '0', '.', '9', ',', 'schedule', '=', 'Schedule', '(', '[', '44', ']', ',', '0', '.', '1', ')', ',', '\\', 'n']
Detokenized (022): ['opt_biases', '=', 'GradientDescentMomentum', '(', '0.02', '/', '10', ',', '0.9', ',', 'schedule', '=', 'Schedule', '(', '[', '44', ']', ',', '0.1', ')', ',', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "valmetric = TopKMisclassification ( k = 5 ) \n"
Original    (009): ['valmetric', '=', 'TopKMisclassification', '(', 'k', '=', '5', ')', '\\n']
Tokenized   (018): ['<s>', 'val', 'met', 'ric', '=', 'Top', 'K', 'Mis', 'class', 'ification', '(', 'k', '=', '5', ')', '\\', 'n', '</s>']
Filtered   (016): ['val', 'met', 'ric', '=', 'Top', 'K', 'Mis', 'class', 'ification', '(', 'k', '=', '5', ')', '\\', 'n']
Detokenized (009): ['valmetric', '=', 'TopKMisclassification', '(', 'k', '=', '5', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "nifm_rng = [ 8 ] \n"
Original    (006): ['nifm_rng', '=', '[', '8', ']', '\\n']
Tokenized   (014): ['<s>', 'n', 'if', 'm', '_', 'r', 'ng', '=', '[', '8', ']', '\\', 'n', '</s>']
Filtered   (012): ['n', 'if', 'm', '_', 'r', 'ng', '=', '[', '8', ']', '\\', 'n']
Detokenized (006): ['nifm_rng', '=', '[', '8', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "fargs_ . append ( itt . product ( fs_rng , nifm_rng , pad_rng , stride_rng , in_sz_rng , bsz_rng ) ) \n"
Original    (022): ['fargs_', '.', 'append', '(', 'itt', '.', 'product', '(', 'fs_rng', ',', 'nifm_rng', ',', 'pad_rng', ',', 'stride_rng', ',', 'in_sz_rng', ',', 'bsz_rng', ')', ')', '\\n']
Tokenized   (053): ['<s>', 'f', 'args', '_', '.', 'append', '(', 'it', 't', '.', 'product', '(', 'fs', '_', 'r', 'ng', ',', 'n', 'if', 'm', '_', 'r', 'ng', ',', 'pad', '_', 'r', 'ng', ',', 'stride', '_', 'r', 'ng', ',', 'in', '_', 's', 'z', '_', 'r', 'ng', ',', 'b', 's', 'z', '_', 'r', 'ng', ')', ')', '\\', 'n', '</s>']
Filtered   (051): ['f', 'args', '_', '.', 'append', '(', 'it', 't', '.', 'product', '(', 'fs', '_', 'r', 'ng', ',', 'n', 'if', 'm', '_', 'r', 'ng', ',', 'pad', '_', 'r', 'ng', ',', 'stride', '_', 'r', 'ng', ',', 'in', '_', 's', 'z', '_', 'r', 'ng', ',', 'b', 's', 'z', '_', 'r', 'ng', ')', ')', '\\', 'n']
Detokenized (022): ['fargs_', '.', 'append', '(', 'itt', '.', 'product', '(', 'fs_rng', ',', 'nifm_rng', ',', 'pad_rng', ',', 'stride_rng', ',', 'in_sz_rng', ',', 'bsz_rng', ')', ')', '\\n']
Counter: 51
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "bsz = inp . shape [ - 1 ] \n"
Original    (010): ['bsz', '=', 'inp', '.', 'shape', '[', '-', '1', ']', '\\n']
Tokenized   (015): ['<s>', 'bs', 'z', '=', 'in', 'p', '.', 'shape', '[', '-', '1', ']', '\\', 'n', '</s>']
Filtered   (013): ['bs', 'z', '=', 'in', 'p', '.', 'shape', '[', '-', '1', ']', '\\', 'n']
Detokenized (010): ['bsz', '=', 'inp', '.', 'shape', '[', '-', '1', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "check_inds = check_inds [ 0 : ncheck ] \n"
Original    (009): ['check_inds', '=', 'check_inds', '[', '0', ':', 'ncheck', ']', '\\n']
Tokenized   (019): ['<s>', 'check', '_', 'ind', 's', '=', 'check', '_', 'ind', 's', '[', '0', ':', 'n', 'check', ']', '\\', 'n', '</s>']
Filtered   (017): ['check', '_', 'ind', 's', '=', 'check', '_', 'ind', 's', '[', '0', ':', 'n', 'check', ']', '\\', 'n']
Detokenized (009): ['check_inds', '=', 'check_inds', '[', '0', ':', 'ncheck', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "inpa = inp . get ( ) . reshape ( inp_lshape ) \n"
Original    (013): ['inpa', '=', 'inp', '.', 'get', '(', ')', '.', 'reshape', '(', 'inp_lshape', ')', '\\n']
Tokenized   (023): ['<s>', 'in', 'pa', '=', 'in', 'p', '.', 'get', '(', ')', '.', 'resh', 'ape', '(', 'in', 'p', '_', 'l', 'shape', ')', '\\', 'n', '</s>']
Filtered   (021): ['in', 'pa', '=', 'in', 'p', '.', 'get', '(', ')', '.', 'resh', 'ape', '(', 'in', 'p', '_', 'l', 'shape', ')', '\\', 'n']
Detokenized (013): ['inpa', '=', 'inp', '.', 'get', '(', ')', '.', 'reshape', '(', 'inp_lshape', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "outshape = ( inp_lshape [ 0 ] , \n"
Original    (009): ['outshape', '=', '(', 'inp_lshape', '[', '0', ']', ',', '\\n']
Tokenized   (017): ['<s>', 'out', 'shape', '=', '(', 'in', 'p', '_', 'l', 'shape', '[', '0', ']', ',', '\\', 'n', '</s>']
Filtered   (015): ['out', 'shape', '=', '(', 'in', 'p', '_', 'l', 'shape', '[', '0', ']', ',', '\\', 'n']
Detokenized (009): ['outshape', '=', '(', 'inp_lshape', '[', '0', ']', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "be . output_dim ( inp_lshape [ 2 ] , fshape [ 1 ] , padding , strides [ 1 ] , pooling = True ) , \n"
Original    (027): ['be', '.', 'output_dim', '(', 'inp_lshape', '[', '2', ']', ',', 'fshape', '[', '1', ']', ',', 'padding', ',', 'strides', '[', '1', ']', ',', 'pooling', '=', 'True', ')', ',', '\\n']
Tokenized   (038): ['<s>', 'be', '.', 'output', '_', 'dim', '(', 'in', 'p', '_', 'l', 'shape', '[', '2', ']', ',', 'f', 'shape', '[', '1', ']', ',', 'padding', ',', 'strides', '[', '1', ']', ',', 'pool', 'ing', '=', 'True', ')', ',', '\\', 'n', '</s>']
Filtered   (036): ['be', '.', 'output', '_', 'dim', '(', 'in', 'p', '_', 'l', 'shape', '[', '2', ']', ',', 'f', 'shape', '[', '1', ']', ',', 'padding', ',', 'strides', '[', '1', ']', ',', 'pool', 'ing', '=', 'True', ')', ',', '\\', 'n']
Detokenized (027): ['be', '.', 'output_dim', '(', 'inp_lshape', '[', '2', ']', ',', 'fshape', '[', '1', ']', ',', 'padding', ',', 'strides', '[', '1', ']', ',', 'pooling', '=', 'True', ')', ',', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "inp_lshape [ - 1 ] ) \n"
Original    (007): ['inp_lshape', '[', '-', '1', ']', ')', '\\n']
Tokenized   (014): ['<s>', 'in', 'p', '_', 'l', 'shape', '[', '-', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (012): ['in', 'p', '_', 'l', 'shape', '[', '-', '1', ']', ')', '\\', 'n']
Detokenized (007): ['inp_lshape', '[', '-', '1', ']', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "inp_pad [ : , padding : - padding , padding : - padding , : ] = inpa [ : , 0 : , 0 : , : ] \n"
Original    (030): ['inp_pad', '[', ':', ',', 'padding', ':', '-', 'padding', ',', 'padding', ':', '-', 'padding', ',', ':', ']', '=', 'inpa', '[', ':', ',', '0', ':', ',', '0', ':', ',', ':', ']', '\\n']
Tokenized   (037): ['<s>', 'in', 'p', '_', 'pad', '[', ':', ',', 'padding', ':', '-', 'padding', ',', 'padding', ':', '-', 'padding', ',', ':', ']', '=', 'in', 'pa', '[', ':', ',', '0', ':', ',', '0', ':', ',', ':', ']', '\\', 'n', '</s>']
Filtered   (035): ['in', 'p', '_', 'pad', '[', ':', ',', 'padding', ':', '-', 'padding', ',', 'padding', ':', '-', 'padding', ',', ':', ']', '=', 'in', 'pa', '[', ':', ',', '0', ':', ',', '0', ':', ',', ':', ']', '\\', 'n']
Detokenized (030): ['inp_pad', '[', ':', ',', 'padding', ':', '-', 'padding', ',', 'padding', ':', '-', 'padding', ',', ':', ']', '=', 'inpa', '[', ':', ',', '0', ':', ',', '0', ':', ',', ':', ']', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 30, 768)
# Extracted words:  30
Sentence         : "out_exp [ indC , indh , indw , cnt ] = np . max ( inp_check ) \n"
Original    (018): ['out_exp', '[', 'indC', ',', 'indh', ',', 'indw', ',', 'cnt', ']', '=', 'np', '.', 'max', '(', 'inp_check', ')', '\\n']
Tokenized   (030): ['<s>', 'out', '_', 'exp', '[', 'ind', 'C', ',', 'ind', 'h', ',', 'ind', 'w', ',', 'c', 'nt', ']', '=', 'np', '.', 'max', '(', 'in', 'p', '_', 'check', ')', '\\', 'n', '</s>']
Filtered   (028): ['out', '_', 'exp', '[', 'ind', 'C', ',', 'ind', 'h', ',', 'ind', 'w', ',', 'c', 'nt', ']', '=', 'np', '.', 'max', '(', 'in', 'p', '_', 'check', ')', '\\', 'n']
Detokenized (018): ['out_exp', '[', 'indC', ',', 'indh', ',', 'indw', ',', 'cnt', ']', '=', 'np', '.', 'max', '(', 'inp_check', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "NervanaObject . be . bsz = batch_size \n"
Original    (008): ['NervanaObject', '.', 'be', '.', 'bsz', '=', 'batch_size', '\\n']
Tokenized   (018): ['<s>', 'N', 'erv', 'ana', 'Object', '.', 'be', '.', 'b', 's', 'z', '=', 'batch', '_', 'size', '\\', 'n', '</s>']
Filtered   (016): ['N', 'erv', 'ana', 'Object', '.', 'be', '.', 'b', 's', 'z', '=', 'batch', '_', 'size', '\\', 'n']
Detokenized (008): ['NervanaObject', '.', 'be', '.', 'bsz', '=', 'batch_size', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "inshape = ( nifm , in_sz , in_sz ) \n"
Original    (010): ['inshape', '=', '(', 'nifm', ',', 'in_sz', ',', 'in_sz', ')', '\\n']
Tokenized   (023): ['<s>', 'ins', 'h', 'ape', '=', '(', 'n', 'if', 'm', ',', 'in', '_', 's', 'z', ',', 'in', '_', 's', 'z', ')', '\\', 'n', '</s>']
Filtered   (021): ['ins', 'h', 'ape', '=', '(', 'n', 'if', 'm', ',', 'in', '_', 's', 'z', ',', 'in', '_', 's', 'z', ')', '\\', 'n']
Detokenized (010): ['inshape', '=', '(', 'nifm', ',', 'in_sz', ',', 'in_sz', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "src = "img/file-icon.jpg" , ** kw ) ] \n"
Original    (009): ['src', '=', '"img/file-icon.jpg"', ',', '**', 'kw', ')', ']', '\\n']
Tokenized   (021): ['<s>', 'src', '=', '"', 'img', '/', 'file', '-', 'icon', '.', 'jpg', '"', ',', '**', 'k', 'w', ')', ']', '\\', 'n', '</s>']
Filtered   (019): ['src', '=', '"', 'img', '/', 'file', '-', 'icon', '.', 'jpg', '"', ',', '**', 'k', 'w', ')', ']', '\\', 'n']
Detokenized (009): ['src', '=', '"img/file-icon.jpg"', ',', '**', 'kw', ')', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "render = lambda r : r . div ( comp . render ( r , model = None ) , r . script ( component_to_update = + self . comp_id , \n"
Original    (032): ['render', '=', 'lambda', 'r', ':', 'r', '.', 'div', '(', 'comp', '.', 'render', '(', 'r', ',', 'model', '=', 'None', ')', ',', 'r', '.', 'script', '(', 'component_to_update', '=', '+', 'self', '.', 'comp_id', ',', '\\n']
Tokenized   (041): ['<s>', 'render', '=', 'lambda', 'r', ':', 'r', '.', 'div', '(', 'comp', '.', 'render', '(', 'r', ',', 'model', '=', 'None', ')', ',', 'r', '.', 'script', '(', 'component', '_', 'to', '_', 'update', '=', '+', 'self', '.', 'comp', '_', 'id', ',', '\\', 'n', '</s>']
Filtered   (039): ['render', '=', 'lambda', 'r', ':', 'r', '.', 'div', '(', 'comp', '.', 'render', '(', 'r', ',', 'model', '=', 'None', ')', ',', 'r', '.', 'script', '(', 'component', '_', 'to', '_', 'update', '=', '+', 'self', '.', 'comp', '_', 'id', ',', '\\', 'n']
Detokenized (032): ['render', '=', 'lambda', 'r', ':', 'r', '.', 'div', '(', 'comp', '.', 'render', '(', 'r', ',', 'model', '=', 'None', ')', ',', 'r', '.', 'script', '(', 'component_to_update', '=', '+', 'self', '.', 'comp_id', ',', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "ajax . py2js ( self . crop_height ( ) ) \n"
Original    (011): ['ajax', '.', 'py2js', '(', 'self', '.', 'crop_height', '(', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'aj', 'ax', '.', 'py', '2', 'js', '(', 'self', '.', 'crop', '_', 'height', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['aj', 'ax', '.', 'py', '2', 'js', '(', 'self', '.', 'crop', '_', 'height', '(', ')', ')', '\\', 'n']
Detokenized (011): ['ajax', '.', 'py2js', '(', 'self', '.', 'crop_height', '(', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "local_handler = getattr ( self , , None ) \n"
Original    (010): ['local_handler', '=', 'getattr', '(', 'self', ',', ',', 'None', ')', '\\n']
Tokenized   (016): ['<s>', 'local', '_', 'handler', '=', 'get', 'attr', '(', 'self', ',', ',', 'None', ')', '\\', 'n', '</s>']
Filtered   (014): ['local', '_', 'handler', '=', 'get', 'attr', '(', 'self', ',', ',', 'None', ')', '\\', 'n']
Detokenized (010): ['local_handler', '=', 'getattr', '(', 'self', ',', ',', 'None', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "genie2 . client . wrapper . RetryPolicy ( \n"
Original    (009): ['genie2', '.', 'client', '.', 'wrapper', '.', 'RetryPolicy', '(', '\\n']
Tokenized   (016): ['<s>', 'gen', 'ie', '2', '.', 'client', '.', 'wrapper', '.', 'Ret', 'ry', 'Policy', '(', '\\', 'n', '</s>']
Filtered   (014): ['gen', 'ie', '2', '.', 'client', '.', 'wrapper', '.', 'Ret', 'ry', 'Policy', '(', '\\', 'n']
Detokenized (009): ['genie2', '.', 'client', '.', 'wrapper', '.', 'RetryPolicy', '(', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "tries = 8 , none_on_404 = True , no_retry_http_codes = range ( 400 , 500 ) ) \n"
Original    (018): ['tries', '=', '8', ',', 'none_on_404', '=', 'True', ',', 'no_retry_http_codes', '=', 'range', '(', '400', ',', '500', ')', ')', '\\n']
Tokenized   (033): ['<s>', 't', 'ries', '=', '8', ',', 'none', '_', 'on', '_', '404', '=', 'True', ',', 'no', '_', 'ret', 'ry', '_', 'http', '_', 'codes', '=', 'range', '(', '400', ',', '500', ')', ')', '\\', 'n', '</s>']
Filtered   (031): ['t', 'ries', '=', '8', ',', 'none', '_', 'on', '_', '404', '=', 'True', ',', 'no', '_', 'ret', 'ry', '_', 'http', '_', 'codes', '=', 'range', '(', '400', ',', '500', ')', ')', '\\', 'n']
Detokenized (018): ['tries', '=', '8', ',', 'none_on_404', '=', 'True', ',', 'no_retry_http_codes', '=', 'range', '(', '400', ',', '500', ')', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "tagging . add_argument ( , , dest = , action = conf_action ( context . ami ) , help = \n"
Original    (021): ['tagging', '.', 'add_argument', '(', ',', ',', 'dest', '=', ',', 'action', '=', 'conf_action', '(', 'context', '.', 'ami', ')', ',', 'help', '=', '\\n']
Tokenized   (030): ['<s>', 'tag', 'ging', '.', 'add', '_', 'argument', '(', ',', ',', 'dest', '=', ',', 'action', '=', 'conf', '_', 'action', '(', 'context', '.', 'am', 'i', ')', ',', 'help', '=', '\\', 'n', '</s>']
Filtered   (028): ['tag', 'ging', '.', 'add', '_', 'argument', '(', ',', ',', 'dest', '=', ',', 'action', '=', 'conf', '_', 'action', '(', 'context', '.', 'am', 'i', ')', ',', 'help', '=', '\\', 'n']
Detokenized (021): ['tagging', '.', 'add_argument', '(', ',', ',', 'dest', '=', ',', 'action', '=', 'conf_action', '(', 'context', '.', 'ami', ')', ',', 'help', '=', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "BASE_URL = . format ( FQDN ) \n"
Original    (008): ['BASE_URL', '=', '.', 'format', '(', 'FQDN', ')', '\\n']
Tokenized   (016): ['<s>', 'B', 'ASE', '_', 'URL', '=', '.', 'format', '(', 'F', 'Q', 'DN', ')', '\\', 'n', '</s>']
Filtered   (014): ['B', 'ASE', '_', 'URL', '=', '.', 'format', '(', 'F', 'Q', 'DN', ')', '\\', 'n']
Detokenized (008): ['BASE_URL', '=', '.', 'format', '(', 'FQDN', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test" , config ElasticSearchServiceItem ( region = "us-west-2" , account = "TEST_ACCOUNT" , name = "es_test_2" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_3" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_4" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_5" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_6" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_7" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_8" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_9" , config ] \n"
Original    (137): ['ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-west-2"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_2"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_3"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_4"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_5"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_6"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_7"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_8"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_9"', ',', 'config', ']', '\\n']
Tokenized   (328): ['<s>', 'El', 'astic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'west', '-', '2', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '2', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '3', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '4', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '5', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '6', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '7', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '8', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '9', '"', ',', 'config', ']', '\\', 'n', '</s>']
Filtered   (326): ['El', 'astic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'west', '-', '2', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '2', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '3', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '4', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '5', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '6', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '7', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'eu', '-', 'west', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '8', '"', ',', 'config', 'Elastic', 'Search', 'Service', 'Item', '(', 'region', '=', '"', 'us', '-', 'east', '-', '1', '"', ',', 'account', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', ',', 'name', '=', '"', 'es', '_', 'test', '_', '9', '"', ',', 'config', ']', '\\', 'n']
Detokenized (137): ['ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-west-2"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_2"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_3"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_4"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_5"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_6"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_7"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"eu-west-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_8"', ',', 'config', 'ElasticSearchServiceItem', '(', 'region', '=', '"us-east-1"', ',', 'account', '=', '"TEST_ACCOUNT"', ',', 'name', '=', '"es_test_9"', ',', 'config', ']', '\\n']
Counter: 326
===================================================================
Hidden states:  (13, 137, 768)
# Extracted words:  137
Sentence         : "test_account . role_name = "TEST_ACCOUNT" \n"
Original    (006): ['test_account', '.', 'role_name', '=', '"TEST_ACCOUNT"', '\\n']
Tokenized   (019): ['<s>', 'test', '_', 'account', '.', 'role', '_', 'name', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', '\\', 'n', '</s>']
Filtered   (017): ['test', '_', 'account', '.', 'role', '_', 'name', '=', '"', 'T', 'EST', '_', 'ACC', 'OUNT', '"', '\\', 'n']
Detokenized (006): ['test_account', '.', 'role_name', '=', '"TEST_ACCOUNT"', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "all_clusters . extend ( response [ ] [ if response [ ] [ ] [ ] ~~~ marker = response [ ] [ ] [ ~~ else : \n"
Original    (029): ['all_clusters', '.', 'extend', '(', 'response', '[', ']', '[', 'if', 'response', '[', ']', '[', ']', '[', ']', '~~~', 'marker', '=', 'response', '[', ']', '[', ']', '[', '~~', 'else', ':', '\\n']
Tokenized   (038): ['<s>', 'all', '_', 'cl', 'usters', '.', 'extend', '(', 'response', '[', ']', '[', 'if', 'response', '[', ']', '[', ']', '[', ']', '', '~~', '~', 'marker', '=', 'response', '[', ']', '[', ']', '[', '', '~~', 'else', ':', '\\', 'n', '</s>']
Filtered   (036): ['all', '_', 'cl', 'usters', '.', 'extend', '(', 'response', '[', ']', '[', 'if', 'response', '[', ']', '[', ']', '[', ']', '', '~~', '~', 'marker', '=', 'response', '[', ']', '[', ']', '[', '', '~~', 'else', ':', '\\', 'n']
Detokenized (029): ['all_clusters', '.', 'extend', '(', 'response', '[', ']', '[', 'if', 'response', '[', ']', '[', ']', '[', ']', '~~~', 'marker', '=', 'response', '[', ']', '[', ']', '[', '~~', 'else', ':', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "_container_child_objects = ( , ) \n"
Original    (006): ['_container_child_objects', '=', '(', ',', ')', '\\n']
Tokenized   (014): ['<s>', '_', 'container', '_', 'child', '_', 'objects', '=', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (012): ['_', 'container', '_', 'child', '_', 'objects', '=', '(', ',', ')', '\\', 'n']
Detokenized (006): ['_container_child_objects', '=', '(', ',', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "_recommended_attrs = ( ( ( , np . ndarray , 1 , np . dtype ( ) ) , \n"
Original    (020): ['_recommended_attrs', '=', '(', '(', '(', ',', 'np', '.', 'ndarray', ',', '1', ',', 'np', '.', 'dtype', '(', ')', ')', ',', '\\n']
Tokenized   (031): ['<s>', '_', 'recomm', 'ended', '_', 'att', 'rs', '=', '(', '(', '(', ',', 'np', '.', 'n', 'd', 'array', ',', '1', ',', 'np', '.', 'd', 'type', '(', ')', ')', ',', '\\', 'n', '</s>']
Filtered   (029): ['_', 'recomm', 'ended', '_', 'att', 'rs', '=', '(', '(', '(', ',', 'np', '.', 'n', 'd', 'array', ',', '1', ',', 'np', '.', 'd', 'type', '(', ')', ')', ',', '\\', 'n']
Detokenized (020): ['_recommended_attrs', '=', '(', '(', '(', ',', 'np', '.', 'ndarray', ',', '1', ',', 'np', '.', 'dtype', '(', ')', ')', ',', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "errstr = . format ( errno , pszMsgBuffer . value ) \n"
Original    (012): ['errstr', '=', '.', 'format', '(', 'errno', ',', 'pszMsgBuffer', '.', 'value', ')', '\\n']
Tokenized   (020): ['<s>', 'err', 'str', '=', '.', 'format', '(', 'err', 'no', ',', 'ps', 'z', 'Msg', 'Buffer', '.', 'value', ')', '\\', 'n', '</s>']
Filtered   (018): ['err', 'str', '=', '.', 'format', '(', 'err', 'no', ',', 'ps', 'z', 'Msg', 'Buffer', '.', 'value', ')', '\\', 'n']
Detokenized (012): ['errstr', '=', '.', 'format', '(', 'errno', ',', 'pszMsgBuffer', '.', 'value', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "supported_objects = [ Segment , AnalogSignal , EventArray , SpikeTrain ] \n"
Original    (012): ['supported_objects', '=', '[', 'Segment', ',', 'AnalogSignal', ',', 'EventArray', ',', 'SpikeTrain', ']', '\\n']
Tokenized   (022): ['<s>', 'supported', '_', 'objects', '=', '[', 'Se', 'gment', ',', 'Analog', 'Sign', 'al', ',', 'Event', 'Array', ',', 'Spike', 'Train', ']', '\\', 'n', '</s>']
Filtered   (020): ['supported', '_', 'objects', '=', '[', 'Se', 'gment', ',', 'Analog', 'Sign', 'al', ',', 'Event', 'Array', ',', 'Spike', 'Train', ']', '\\', 'n']
Detokenized (012): ['supported_objects', '=', '[', 'Segment', ',', 'AnalogSignal', ',', 'EventArray', ',', 'SpikeTrain', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "readable_objects = [ Segment ] \n"
Original    (006): ['readable_objects', '=', '[', 'Segment', ']', '\\n']
Tokenized   (012): ['<s>', 'readable', '_', 'objects', '=', '[', 'Se', 'gment', ']', '\\', 'n', '</s>']
Filtered   (010): ['readable', '_', 'objects', '=', '[', 'Se', 'gment', ']', '\\', 'n']
Detokenized (006): ['readable_objects', '=', '[', 'Segment', ']', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "read_params = { Segment : [ ] } \n"
Original    (009): ['read_params', '=', '{', 'Segment', ':', '[', ']', '}', '\\n']
Tokenized   (015): ['<s>', 'read', '_', 'params', '=', '{', 'Se', 'gment', ':', '[', ']', '}', '\\', 'n', '</s>']
Filtered   (013): ['read', '_', 'params', '=', '{', 'Se', 'gment', ':', '[', ']', '}', '\\', 'n']
Detokenized (009): ['read_params', '=', '{', 'Segment', ':', '[', ']', '}', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "labels . append ( str ( pData . value ) ) \n"
Original    (012): ['labels', '.', 'append', '(', 'str', '(', 'pData', '.', 'value', ')', ')', '\\n']
Tokenized   (017): ['<s>', 'lab', 'els', '.', 'append', '(', 'str', '(', 'p', 'Data', '.', 'value', ')', ')', '\\', 'n', '</s>']
Filtered   (015): ['lab', 'els', '.', 'append', '(', 'str', '(', 'p', 'Data', '.', 'value', ')', ')', '\\', 'n']
Detokenized (012): ['labels', '.', 'append', '(', 'str', '(', 'pData', '.', 'value', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "ea . labels = np . array ( labels , dtype = ) \n"
Original    (014): ['ea', '.', 'labels', '=', 'np', '.', 'array', '(', 'labels', ',', 'dtype', '=', ')', '\\n']
Tokenized   (018): ['<s>', 'ea', '.', 'labels', '=', 'np', '.', 'array', '(', 'labels', ',', 'd', 'type', '=', ')', '\\', 'n', '</s>']
Filtered   (016): ['ea', '.', 'labels', '=', 'np', '.', 'array', '(', 'labels', ',', 'd', 'type', '=', ')', '\\', 'n']
Detokenized (014): ['ea', '.', 'labels', '=', 'np', '.', 'array', '(', 'labels', ',', 'dtype', '=', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "dwStopIndex , ctypes . byref ( pdwContCount ) , pData [ total_read : ] . ctypes total_read += pdwContCount . value \n"
Original    (022): ['dwStopIndex', ',', 'ctypes', '.', 'byref', '(', 'pdwContCount', ')', ',', 'pData', '[', 'total_read', ':', ']', '.', 'ctypes', 'total_read', '+=', 'pdwContCount', '.', 'value', '\\n']
Tokenized   (044): ['<s>', 'd', 'w', 'Stop', 'Index', ',', 'c', 'types', '.', 'by', 'ref', '(', 'p', 'd', 'w', 'Cont', 'Count', ')', ',', 'p', 'Data', '[', 'total', '_', 'read', ':', ']', '.', 'c', 'types', 'total', '_', 'read', '+=', 'p', 'd', 'w', 'Cont', 'Count', '.', 'value', '\\', 'n', '</s>']
Filtered   (042): ['d', 'w', 'Stop', 'Index', ',', 'c', 'types', '.', 'by', 'ref', '(', 'p', 'd', 'w', 'Cont', 'Count', ')', ',', 'p', 'Data', '[', 'total', '_', 'read', ':', ']', '.', 'c', 'types', 'total', '_', 'read', '+=', 'p', 'd', 'w', 'Cont', 'Count', '.', 'value', '\\', 'n']
Detokenized (022): ['dwStopIndex', ',', 'ctypes', '.', 'byref', '(', 'pdwContCount', ')', ',', 'pData', '[', 'total_read', ':', ']', '.', 'ctypes', 'total_read', '+=', 'pdwContCount', '.', 'value', '\\n']
Counter: 42
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "anaSig . annotate ( probe_info = str ( pAnalogInfo . szProbeInfo ) ) \n"
Original    (014): ['anaSig', '.', 'annotate', '(', 'probe_info', '=', 'str', '(', 'pAnalogInfo', '.', 'szProbeInfo', ')', ')', '\\n']
Tokenized   (029): ['<s>', 'ana', 'S', 'ig', '.', 'annot', 'ate', '(', 'probe', '_', 'info', '=', 'str', '(', 'p', 'An', 'alog', 'Info', '.', 's', 'z', 'Pro', 'be', 'Info', ')', ')', '\\', 'n', '</s>']
Filtered   (027): ['ana', 'S', 'ig', '.', 'annot', 'ate', '(', 'probe', '_', 'info', '=', 'str', '(', 'p', 'An', 'alog', 'Info', '.', 's', 'z', 'Pro', 'be', 'Info', ')', ')', '\\', 'n']
Detokenized (014): ['anaSig', '.', 'annotate', '(', 'probe_info', '=', 'str', '(', 'pAnalogInfo', '.', 'szProbeInfo', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "pData = np . zeros ( ( dwDataBufferSize ) , dtype = ) \n"
Original    (014): ['pData', '=', 'np', '.', 'zeros', '(', '(', 'dwDataBufferSize', ')', ',', 'dtype', '=', ')', '\\n']
Tokenized   (023): ['<s>', 'p', 'Data', '=', 'np', '.', 'z', 'eros', '(', '(', 'dw', 'Data', 'Buffer', 'Size', ')', ',', 'd', 'type', '=', ')', '\\', 'n', '</s>']
Filtered   (021): ['p', 'Data', '=', 'np', '.', 'z', 'eros', '(', '(', 'dw', 'Data', 'Buffer', 'Size', ')', ',', 'd', 'type', '=', ')', '\\', 'n']
Detokenized (014): ['pData', '=', 'np', '.', 'zeros', '(', '(', 'dwDataBufferSize', ')', ',', 'dtype', '=', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "waveforms = pq . Quantity ( waveforms , units = str ( pdwSegmentInfo left_sweep = nsample / 2. / float ( pdwSegmentInfo . dSampleRate ) * pq sampling_rate = float ( pdwSegmentInfo . dSampleRate ) * pq . Hz , \n"
Original    (041): ['waveforms', '=', 'pq', '.', 'Quantity', '(', 'waveforms', ',', 'units', '=', 'str', '(', 'pdwSegmentInfo', 'left_sweep', '=', 'nsample', '/', '2.', '/', 'float', '(', 'pdwSegmentInfo', '.', 'dSampleRate', ')', '*', 'pq', 'sampling_rate', '=', 'float', '(', 'pdwSegmentInfo', '.', 'dSampleRate', ')', '*', 'pq', '.', 'Hz', ',', '\\n']
Tokenized   (075): ['<s>', 'wave', 'forms', '=', 'p', 'q', '.', 'Quantity', '(', 'wave', 'forms', ',', 'units', '=', 'str', '(', 'p', 'd', 'w', 'Seg', 'ment', 'Info', 'left', '_', 'swe', 'ep', '=', 'ns', 'ample', '/', '2', '.', '/', 'float', '(', 'p', 'd', 'w', 'Seg', 'ment', 'Info', '.', 'd', 'Sample', 'Rate', ')', '*', 'p', 'q', 'sampling', '_', 'rate', '=', 'float', '(', 'p', 'd', 'w', 'Seg', 'ment', 'Info', '.', 'd', 'Sample', 'Rate', ')', '*', 'p', 'q', '.', 'Hz', ',', '\\', 'n', '</s>']
Filtered   (073): ['wave', 'forms', '=', 'p', 'q', '.', 'Quantity', '(', 'wave', 'forms', ',', 'units', '=', 'str', '(', 'p', 'd', 'w', 'Seg', 'ment', 'Info', 'left', '_', 'swe', 'ep', '=', 'ns', 'ample', '/', '2', '.', '/', 'float', '(', 'p', 'd', 'w', 'Seg', 'ment', 'Info', '.', 'd', 'Sample', 'Rate', ')', '*', 'p', 'q', 'sampling', '_', 'rate', '=', 'float', '(', 'p', 'd', 'w', 'Seg', 'ment', 'Info', '.', 'd', 'Sample', 'Rate', ')', '*', 'p', 'q', '.', 'Hz', ',', '\\', 'n']
Detokenized (041): ['waveforms', '=', 'pq', '.', 'Quantity', '(', 'waveforms', ',', 'units', '=', 'str', '(', 'pdwSegmentInfo', 'left_sweep', '=', 'nsample', '/', '2.', '/', 'float', '(', 'pdwSegmentInfo', '.', 'dSampleRate', ')', '*', 'pq', 'sampling_rate', '=', 'float', '(', 'pdwSegmentInfo', '.', 'dSampleRate', ')', '*', 'pq', '.', 'Hz', ',', '\\n']
Counter: 73
===================================================================
Hidden states:  (13, 41, 768)
# Extracted words:  41
Sentence         : "ctypes . byref ( pNeuralInfo ) , ctypes . sizeof ( pNeuralInfo ) ) \n"
Original    (015): ['ctypes', '.', 'byref', '(', 'pNeuralInfo', ')', ',', 'ctypes', '.', 'sizeof', '(', 'pNeuralInfo', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'ct', 'ypes', '.', 'by', 'ref', '(', 'p', 'Ne', 'ural', 'Info', ')', ',', 'c', 'types', '.', 'sizeof', '(', 'p', 'Ne', 'ural', 'Info', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['ct', 'ypes', '.', 'by', 'ref', '(', 'p', 'Ne', 'ural', 'Info', ')', ',', 'c', 'types', '.', 'sizeof', '(', 'p', 'Ne', 'ural', 'Info', ')', ')', '\\', 'n']
Detokenized (015): ['ctypes', '.', 'byref', '(', 'pNeuralInfo', ')', ',', 'ctypes', '.', 'sizeof', '(', 'pNeuralInfo', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "neuroshare . ns_GetNeuralData ( hFile , dwEntityID , dwStartIndex , \n"
Original    (011): ['neuroshare', '.', 'ns_GetNeuralData', '(', 'hFile', ',', 'dwEntityID', ',', 'dwStartIndex', ',', '\\n']
Tokenized   (026): ['<s>', 'ne', 'uro', 'share', '.', 'ns', '_', 'Get', 'Ne', 'ural', 'Data', '(', 'h', 'File', ',', 'dw', 'Entity', 'ID', ',', 'dw', 'Start', 'Index', ',', '\\', 'n', '</s>']
Filtered   (024): ['ne', 'uro', 'share', '.', 'ns', '_', 'Get', 'Ne', 'ural', 'Data', '(', 'h', 'File', ',', 'dw', 'Entity', 'ID', ',', 'dw', 'Start', 'Index', ',', '\\', 'n']
Detokenized (011): ['neuroshare', '.', 'ns_GetNeuralData', '(', 'hFile', ',', 'dwEntityID', ',', 'dwStartIndex', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "dwIndexCount , pData . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) ) \n"
Original    (019): ['dwIndexCount', ',', 'pData', '.', 'ctypes', '.', 'data_as', '(', 'ctypes', '.', 'POINTER', '(', 'ctypes', '.', 'c_double', ')', ')', ')', '\\n']
Tokenized   (034): ['<s>', 'd', 'w', 'Index', 'Count', ',', 'p', 'Data', '.', 'c', 'types', '.', 'data', '_', 'as', '(', 'c', 'types', '.', 'PO', 'INTER', '(', 'c', 'types', '.', 'c', '_', 'double', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (032): ['d', 'w', 'Index', 'Count', ',', 'p', 'Data', '.', 'c', 'types', '.', 'data', '_', 'as', '(', 'c', 'types', '.', 'PO', 'INTER', '(', 'c', 'types', '.', 'c', '_', 'double', ')', ')', ')', '\\', 'n']
Detokenized (019): ['dwIndexCount', ',', 'pData', '.', 'ctypes', '.', 'data_as', '(', 'ctypes', '.', 'POINTER', '(', 'ctypes', '.', 'c_double', ')', ')', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "times = pData * pq . s \n"
Original    (008): ['times', '=', 'pData', '*', 'pq', '.', 's', '\\n']
Tokenized   (013): ['<s>', 'times', '=', 'p', 'Data', '*', 'p', 'q', '.', 's', '\\', 'n', '</s>']
Filtered   (011): ['times', '=', 'p', 'Data', '*', 'p', 'q', '.', 's', '\\', 'n']
Detokenized (008): ['times', '=', 'pData', '*', 'pq', '.', 's', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "clone_object , TEST_ANNOTATIONS ) \n"
Original    (005): ['clone_object', ',', 'TEST_ANNOTATIONS', ')', '\\n']
Tokenized   (014): ['<s>', 'clone', '_', 'object', ',', 'TEST', '_', 'AN', 'NOT', 'ATIONS', ')', '\\', 'n', '</s>']
Filtered   (012): ['clone', '_', 'object', ',', 'TEST', '_', 'AN', 'NOT', 'ATIONS', ')', '\\', 'n']
Detokenized (005): ['clone_object', ',', 'TEST_ANNOTATIONS', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "range ( len ( TEST_ANNOTATIONS ) ) ] ) \n"
Original    (010): ['range', '(', 'len', '(', 'TEST_ANNOTATIONS', ')', ')', ']', ')', '\\n']
Tokenized   (017): ['<s>', 'range', '(', 'len', '(', 'TEST', '_', 'AN', 'NOT', 'ATIONS', ')', ')', ']', ')', '\\', 'n', '</s>']
Filtered   (015): ['range', '(', 'len', '(', 'TEST', '_', 'AN', 'NOT', 'ATIONS', ')', ')', ']', ')', '\\', 'n']
Detokenized (010): ['range', '(', 'len', '(', 'TEST_ANNOTATIONS', ')', ')', ']', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "file_datetime = get_fake_value ( , datetime , seed = 0 ) \n"
Original    (012): ['file_datetime', '=', 'get_fake_value', '(', ',', 'datetime', ',', 'seed', '=', '0', ')', '\\n']
Tokenized   (023): ['<s>', 'file', '_', 'dat', 'etime', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'dat', 'etime', ',', 'seed', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (021): ['file', '_', 'dat', 'etime', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'dat', 'etime', ',', 'seed', '=', '0', ')', '\\', 'n']
Detokenized (012): ['file_datetime', '=', 'get_fake_value', '(', ',', 'datetime', ',', 'seed', '=', '0', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "file_origin = get_fake_value ( , str ) \n"
Original    (008): ['file_origin', '=', 'get_fake_value', '(', ',', 'str', ')', '\\n']
Tokenized   (017): ['<s>', 'file', '_', 'origin', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'str', ')', '\\', 'n', '</s>']
Filtered   (015): ['file', '_', 'origin', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'str', ')', '\\', 'n']
Detokenized (008): ['file_origin', '=', 'get_fake_value', '(', ',', 'str', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "attrs1 = { : file_datetime , \n"
Original    (007): ['attrs1', '=', '{', ':', 'file_datetime', ',', '\\n']
Tokenized   (015): ['<s>', 'att', 'rs', '1', '=', '{', ':', 'file', '_', 'dat', 'etime', ',', '\\', 'n', '</s>']
Filtered   (013): ['att', 'rs', '1', '=', '{', ':', 'file', '_', 'dat', 'etime', ',', '\\', 'n']
Detokenized (007): ['attrs1', '=', '{', ':', 'file_datetime', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "res21 = get_fake_values ( Segment , annotate = True , seed = 0 ) \n"
Original    (015): ['res21', '=', 'get_fake_values', '(', 'Segment', ',', 'annotate', '=', 'True', ',', 'seed', '=', '0', ')', '\\n']
Tokenized   (025): ['<s>', 'res', '21', '=', 'get', '_', 'fake', '_', 'values', '(', 'Se', 'gment', ',', 'annot', 'ate', '=', 'True', ',', 'seed', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (023): ['res', '21', '=', 'get', '_', 'fake', '_', 'values', '(', 'Se', 'gment', ',', 'annot', 'ate', '=', 'True', ',', 'seed', '=', '0', ')', '\\', 'n']
Detokenized (015): ['res21', '=', 'get_fake_values', '(', 'Segment', ',', 'annotate', '=', 'True', ',', 'seed', '=', '0', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "res22 = get_fake_values ( , annotate = True , seed = 0 ) \n"
Original    (014): ['res22', '=', 'get_fake_values', '(', ',', 'annotate', '=', 'True', ',', 'seed', '=', '0', ')', '\\n']
Tokenized   (023): ['<s>', 'res', '22', '=', 'get', '_', 'fake', '_', 'values', '(', ',', 'annot', 'ate', '=', 'True', ',', 'seed', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (021): ['res', '22', '=', 'get', '_', 'fake', '_', 'values', '(', ',', 'annot', 'ate', '=', 'True', ',', 'seed', '=', '0', ')', '\\', 'n']
Detokenized (014): ['res22', '=', 'get_fake_values', '(', ',', 'annotate', '=', 'True', ',', 'seed', '=', '0', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "targ0 = get_fake_value ( , datetime , seed = seed + 0 ) \n"
Original    (014): ['targ0', '=', 'get_fake_value', '(', ',', 'datetime', ',', 'seed', '=', 'seed', '+', '0', ')', '\\n']
Tokenized   (024): ['<s>', 't', 'arg', '0', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'dat', 'etime', ',', 'seed', '=', 'seed', '+', '0', ')', '\\', 'n', '</s>']
Filtered   (022): ['t', 'arg', '0', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'dat', 'etime', ',', 'seed', '=', 'seed', '+', '0', ')', '\\', 'n']
Detokenized (014): ['targ0', '=', 'get_fake_value', '(', ',', 'datetime', ',', 'seed', '=', 'seed', '+', '0', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "targ4 = get_fake_value ( , str , \n"
Original    (008): ['targ4', '=', 'get_fake_value', '(', ',', 'str', ',', '\\n']
Tokenized   (017): ['<s>', 't', 'arg', '4', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'str', ',', '\\', 'n', '</s>']
Filtered   (015): ['t', 'arg', '4', '=', 'get', '_', 'fake', '_', 'value', '(', ',', 'str', ',', '\\', 'n']
Detokenized (008): ['targ4', '=', 'get_fake_value', '(', ',', 'str', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "seed = seed + 4 , obj = Segment ) \n"
Original    (011): ['seed', '=', 'seed', '+', '4', ',', 'obj', '=', 'Segment', ')', '\\n']
Tokenized   (015): ['<s>', 'seed', '=', 'seed', '+', '4', ',', 'obj', '=', 'Se', 'gment', ')', '\\', 'n', '</s>']
Filtered   (013): ['seed', '=', 'seed', '+', '4', ',', 'obj', '=', 'Se', 'gment', ')', '\\', 'n']
Detokenized (011): ['seed', '=', 'seed', '+', '4', ',', 'obj', '=', 'Segment', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "childobjs = ( , , \n"
Original    (006): ['childobjs', '=', '(', ',', ',', '\\n']
Tokenized   (011): ['<s>', 'child', 'ob', 'js', '=', '(', ',', ',', '\\', 'n', '</s>']
Filtered   (009): ['child', 'ob', 'js', '=', '(', ',', ',', '\\', 'n']
Detokenized (006): ['childobjs', '=', '(', ',', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "children = ( self . sigs1a + self . sigarrs1a + \n"
Original    (012): ['children', '=', '(', 'self', '.', 'sigs1a', '+', 'self', '.', 'sigarrs1a', '+', '\\n']
Tokenized   (022): ['<s>', 'children', '=', '(', 'self', '.', 's', 'igs', '1', 'a', '+', 'self', '.', 'sig', 'arr', 's', '1', 'a', '+', '\\', 'n', '</s>']
Filtered   (020): ['children', '=', '(', 'self', '.', 's', 'igs', '1', 'a', '+', 'self', '.', 'sig', 'arr', 's', '1', 'a', '+', '\\', 'n']
Detokenized (012): ['children', '=', '(', 'self', '.', 'sigs1a', '+', 'self', '.', 'sigarrs1a', '+', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : ""analogsignals" : self . nchildren ** 2 , \n"
Original    (009): ['"analogsignals"', ':', 'self', '.', 'nchildren', '**', '2', ',', '\\n']
Tokenized   (018): ['<s>', '"', 'an', 'alog', 'sign', 'als', '"', ':', 'self', '.', 'n', 'children', '**', '2', ',', '\\', 'n', '</s>']
Filtered   (016): ['"', 'an', 'alog', 'sign', 'als', '"', ':', 'self', '.', 'n', 'children', '**', '2', ',', '\\', 'n']
Detokenized (009): ['"analogsignals"', ':', 'self', '.', 'nchildren', '**', '2', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ""epocharrays" : self . nchildren , "eventarrays" : self . nchildren , \n"
Original    (013): ['"epocharrays"', ':', 'self', '.', 'nchildren', ',', '"eventarrays"', ':', 'self', '.', 'nchildren', ',', '\\n']
Tokenized   (027): ['<s>', '"', 'ep', 'och', 'arr', 'ays', '"', ':', 'self', '.', 'n', 'children', ',', '"', 'event', 'arr', 'ays', '"', ':', 'self', '.', 'n', 'children', ',', '\\', 'n', '</s>']
Filtered   (025): ['"', 'ep', 'och', 'arr', 'ays', '"', ':', 'self', '.', 'n', 'children', ',', '"', 'event', 'arr', 'ays', '"', ':', 'self', '.', 'n', 'children', ',', '\\', 'n']
Detokenized (013): ['"epocharrays"', ':', 'self', '.', 'nchildren', ',', '"eventarrays"', ':', 'self', '.', 'nchildren', ',', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : ""analogsignalarrays" : self . nchildren } \n"
Original    (007): ['"analogsignalarrays"', ':', 'self', '.', 'nchildren', '}', '\\n']
Tokenized   (018): ['<s>', '"', 'an', 'alog', 'sign', 'al', 'arr', 'ays', '"', ':', 'self', '.', 'n', 'children', '}', '\\', 'n', '</s>']
Filtered   (016): ['"', 'an', 'alog', 'sign', 'al', 'arr', 'ays', '"', ':', 'self', '.', 'n', 'children', '}', '\\', 'n']
Detokenized (007): ['"analogsignalarrays"', ':', 'self', '.', 'nchildren', '}', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "targdict = { : 5 } ) \n"
Original    (008): ['targdict', '=', '{', ':', '5', '}', ')', '\\n']
Tokenized   (013): ['<s>', 't', 'arg', 'dict', '=', '{', ':', '5', '}', ')', '\\', 'n', '</s>']
Filtered   (011): ['t', 'arg', 'dict', '=', '{', ':', '5', '}', ')', '\\', 'n']
Detokenized (008): ['targdict', '=', '{', ':', '5', '}', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "res6 = filterdata ( data , name = self . epcs2 [ 0 ] . name , j = 5 ) \n"
Original    (022): ['res6', '=', 'filterdata', '(', 'data', ',', 'name', '=', 'self', '.', 'epcs2', '[', '0', ']', '.', 'name', ',', 'j', '=', '5', ')', '\\n']
Tokenized   (029): ['<s>', 'res', '6', '=', 'filter', 'data', '(', 'data', ',', 'name', '=', 'self', '.', 'ep', 'cs', '2', '[', '0', ']', '.', 'name', ',', 'j', '=', '5', ')', '\\', 'n', '</s>']
Filtered   (027): ['res', '6', '=', 'filter', 'data', '(', 'data', ',', 'name', '=', 'self', '.', 'ep', 'cs', '2', '[', '0', ']', '.', 'name', ',', 'j', '=', '5', ')', '\\', 'n']
Detokenized (022): ['res6', '=', 'filterdata', '(', 'data', ',', 'name', '=', 'self', '.', 'epcs2', '[', '0', ']', '.', 'name', ',', 'j', '=', '5', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "res7 = filterdata ( data , { : self . epcs2 [ 1 ] . name , : 5 } ) \n"
Original    (022): ['res7', '=', 'filterdata', '(', 'data', ',', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\n']
Tokenized   (029): ['<s>', 'res', '7', '=', 'filter', 'data', '(', 'data', ',', '{', ':', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\', 'n', '</s>']
Filtered   (027): ['res', '7', '=', 'filter', 'data', '(', 'data', ',', '{', ':', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\', 'n']
Detokenized (022): ['res7', '=', 'filterdata', '(', 'data', ',', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "res8 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name , : 5 } ) \n"
Original    (024): ['res8', '=', 'filterdata', '(', 'data', ',', 'targdict', '=', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\n']
Tokenized   (032): ['<s>', 'res', '8', '=', 'filter', 'data', '(', 'data', ',', 'targ', 'dict', '=', '{', ':', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\', 'n', '</s>']
Filtered   (030): ['res', '8', '=', 'filter', 'data', '(', 'data', ',', 'targ', 'dict', '=', '{', ':', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\', 'n']
Detokenized (024): ['res8', '=', 'filterdata', '(', 'data', ',', 'targdict', '=', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', ',', ':', '5', '}', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "res9 = filterdata ( data , { : self . epcs2 [ 1 ] . name } , j = 5 ) \n"
Original    (023): ['res9', '=', 'filterdata', '(', 'data', ',', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\n']
Tokenized   (030): ['<s>', 'res', '9', '=', 'filter', 'data', '(', 'data', ',', '{', ':', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\', 'n', '</s>']
Filtered   (028): ['res', '9', '=', 'filter', 'data', '(', 'data', ',', '{', ':', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\', 'n']
Detokenized (023): ['res9', '=', 'filterdata', '(', 'data', ',', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "res10 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name } , j = 5 ) \n"
Original    (025): ['res10', '=', 'filterdata', '(', 'data', ',', 'targdict', '=', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\n']
Tokenized   (033): ['<s>', 'res', '10', '=', 'filter', 'data', '(', 'data', ',', 'targ', 'dict', '=', '{', ':', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\', 'n', '</s>']
Filtered   (031): ['res', '10', '=', 'filter', 'data', '(', 'data', ',', 'targ', 'dict', '=', '{', ':', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\', 'n']
Detokenized (025): ['res10', '=', 'filterdata', '(', 'data', ',', 'targdict', '=', '{', ':', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', '}', ',', 'j', '=', '5', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "res11 = filterdata ( data , name = self . epcs2 [ 1 ] . name , targdict = { : 5 } ) \n"
Original    (025): ['res11', '=', 'filterdata', '(', 'data', ',', 'name', '=', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', ',', 'targdict', '=', '{', ':', '5', '}', ')', '\\n']
Tokenized   (033): ['<s>', 'res', '11', '=', 'filter', 'data', '(', 'data', ',', 'name', '=', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', ',', 'targ', 'dict', '=', '{', ':', '5', '}', ')', '\\', 'n', '</s>']
Filtered   (031): ['res', '11', '=', 'filter', 'data', '(', 'data', ',', 'name', '=', 'self', '.', 'ep', 'cs', '2', '[', '1', ']', '.', 'name', ',', 'targ', 'dict', '=', '{', ':', '5', '}', ')', '\\', 'n']
Detokenized (025): ['res11', '=', 'filterdata', '(', 'data', ',', 'name', '=', 'self', '.', 'epcs2', '[', '1', ']', '.', 'name', ',', 'targdict', '=', '{', ':', '5', '}', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "targ = [ self . epcs1a [ 1 ] ] \n"
Original    (011): ['targ', '=', '[', 'self', '.', 'epcs1a', '[', '1', ']', ']', '\\n']
Tokenized   (018): ['<s>', 't', 'arg', '=', '[', 'self', '.', 'ep', 'cs', '1', 'a', '[', '1', ']', ']', '\\', 'n', '</s>']
Filtered   (016): ['t', 'arg', '=', '[', 'self', '.', 'ep', 'cs', '1', 'a', '[', '1', ']', ']', '\\', 'n']
Detokenized (011): ['targ', '=', '[', 'self', '.', 'epcs1a', '[', '1', ']', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "res3 = filterdata ( data , [ { : 1 } , { : 2 } ] ) \n"
Original    (019): ['res3', '=', 'filterdata', '(', 'data', ',', '[', '{', ':', '1', '}', ',', '{', ':', '2', '}', ']', ')', '\\n']
Tokenized   (024): ['<s>', 'res', '3', '=', 'filter', 'data', '(', 'data', ',', '[', '{', ':', '1', '}', ',', '{', ':', '2', '}', ']', ')', '\\', 'n', '</s>']
Filtered   (022): ['res', '3', '=', 'filter', 'data', '(', 'data', ',', '[', '{', ':', '1', '}', ',', '{', ':', '2', '}', ']', ')', '\\', 'n']
Detokenized (019): ['res3', '=', 'filterdata', '(', 'data', ',', '[', '{', ':', '1', '}', ',', '{', ':', '2', '}', ']', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "res4 = filterdata ( data , { : 1 } , i = 2 ) \n"
Original    (016): ['res4', '=', 'filterdata', '(', 'data', ',', '{', ':', '1', '}', ',', 'i', '=', '2', ')', '\\n']
Tokenized   (021): ['<s>', 'res', '4', '=', 'filter', 'data', '(', 'data', ',', '{', ':', '1', '}', ',', 'i', '=', '2', ')', '\\', 'n', '</s>']
Filtered   (019): ['res', '4', '=', 'filter', 'data', '(', 'data', ',', '{', ':', '1', '}', ',', 'i', '=', '2', ')', '\\', 'n']
Detokenized (016): ['res4', '=', 'filterdata', '(', 'data', ',', '{', ':', '1', '}', ',', 'i', '=', '2', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "res5 = filterdata ( data , [ { : 1 } ] , i = 2 ) \n"
Original    (018): ['res5', '=', 'filterdata', '(', 'data', ',', '[', '{', ':', '1', '}', ']', ',', 'i', '=', '2', ')', '\\n']
Tokenized   (023): ['<s>', 'res', '5', '=', 'filter', 'data', '(', 'data', ',', '[', '{', ':', '1', '}', ']', ',', 'i', '=', '2', ')', '\\', 'n', '</s>']
Filtered   (021): ['res', '5', '=', 'filter', 'data', '(', 'data', ',', '[', '{', ':', '1', '}', ']', ',', 'i', '=', '2', ')', '\\', 'n']
Detokenized (018): ['res5', '=', 'filterdata', '(', 'data', ',', '[', '{', ':', '1', '}', ']', ',', 'i', '=', '2', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "ann = pretty ( ann ) . replace ( , ) \n"
Original    (012): ['ann', '=', 'pretty', '(', 'ann', ')', '.', 'replace', '(', ',', ')', '\\n']
Tokenized   (015): ['<s>', 'ann', '=', 'pretty', '(', 'ann', ')', '.', 'replace', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (013): ['ann', '=', 'pretty', '(', 'ann', ')', '.', 'replace', '(', ',', ')', '\\', 'n']
Detokenized (012): ['ann', '=', 'pretty', '(', 'ann', ')', '.', 'replace', '(', ',', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "unit_with_sig = np . array ( [ 0 , 2 , 5 ] ) \n"
Original    (015): ['unit_with_sig', '=', 'np', '.', 'array', '(', '[', '0', ',', '2', ',', '5', ']', ')', '\\n']
Tokenized   (023): ['<s>', 'unit', '_', 'with', '_', 's', 'ig', '=', 'np', '.', 'array', '(', '[', '0', ',', '2', ',', '5', ']', ')', '\\', 'n', '</s>']
Filtered   (021): ['unit', '_', 'with', '_', 's', 'ig', '=', 'np', '.', 'array', '(', '[', '0', ',', '2', ',', '5', ']', ')', '\\', 'n']
Detokenized (015): ['unit_with_sig', '=', 'np', '.', 'array', '(', '[', '0', ',', '2', ',', '5', ']', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "rcgs = [ RecordingChannelGroup ( name = , \n"
Original    (009): ['rcgs', '=', '[', 'RecordingChannelGroup', '(', 'name', '=', ',', '\\n']
Tokenized   (015): ['<s>', 'rc', 'gs', '=', '[', 'Recording', 'Channel', 'Group', '(', 'name', '=', ',', '\\', 'n', '</s>']
Filtered   (013): ['rc', 'gs', '=', '[', 'Recording', 'Channel', 'Group', '(', 'name', '=', ',', '\\', 'n']
Detokenized (009): ['rcgs', '=', '[', 'RecordingChannelGroup', '(', 'name', '=', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "RecordingChannelGroup ( name = , \n"
Original    (006): ['RecordingChannelGroup', '(', 'name', '=', ',', '\\n']
Tokenized   (012): ['<s>', 'Rec', 'ording', 'Channel', 'Group', '(', 'name', '=', ',', '\\', 'n', '</s>']
Filtered   (010): ['Rec', 'ording', 'Channel', 'Group', '(', 'name', '=', ',', '\\', 'n']
Detokenized (006): ['RecordingChannelGroup', '(', 'name', '=', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "t_start = 0. , t_stop = 10 ) \n"
Original    (009): ['t_start', '=', '0.', ',', 't_stop', '=', '10', ')', '\\n']
Tokenized   (017): ['<s>', 't', '_', 'start', '=', '0', '.', ',', 't', '_', 'stop', '=', '10', ')', '\\', 'n', '</s>']
Filtered   (015): ['t', '_', 'start', '=', '0', '.', ',', 't', '_', 'stop', '=', '10', ')', '\\', 'n']
Detokenized (009): ['t_start', '=', '0.', ',', 't_stop', '=', '10', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "st . unit = all_unit [ j ] \n"
Original    (009): ['st', '.', 'unit', '=', 'all_unit', '[', 'j', ']', '\\n']
Tokenized   (014): ['<s>', 'st', '.', 'unit', '=', 'all', '_', 'unit', '[', 'j', ']', '\\', 'n', '</s>']
Filtered   (012): ['st', '.', 'unit', '=', 'all', '_', 'unit', '[', 'j', ']', '\\', 'n']
Detokenized (009): ['st', '.', 'unit', '=', 'all_unit', '[', 'j', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "sampling_rate = 1000. * pq . Hz , \n"
Original    (009): ['sampling_rate', '=', '1000.', '*', 'pq', '.', 'Hz', ',', '\\n']
Tokenized   (017): ['<s>', 'sam', 'pling', '_', 'rate', '=', '1000', '.', '*', 'p', 'q', '.', 'Hz', ',', '\\', 'n', '</s>']
Filtered   (015): ['sam', 'pling', '_', 'rate', '=', '1000', '.', '*', 'p', 'q', '.', 'Hz', ',', '\\', 'n']
Detokenized (009): ['sampling_rate', '=', '1000.', '*', 'pq', '.', 'Hz', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "newseg = seg . construct_subsegment_by_unit ( all_unit [ : 4 ] ) \n"
Original    (013): ['newseg', '=', 'seg', '.', 'construct_subsegment_by_unit', '(', 'all_unit', '[', ':', '4', ']', ')', '\\n']
Tokenized   (029): ['<s>', 'new', 'se', 'g', '=', 'se', 'g', '.', 'construct', '_', 'sub', 'se', 'gment', '_', 'by', '_', 'unit', '(', 'all', '_', 'unit', '[', ':', '4', ']', ')', '\\', 'n', '</s>']
Filtered   (027): ['new', 'se', 'g', '=', 'se', 'g', '.', 'construct', '_', 'sub', 'se', 'gment', '_', 'by', '_', 'unit', '(', 'all', '_', 'unit', '[', ':', '4', ']', ')', '\\', 'n']
Detokenized (013): ['newseg', '=', 'seg', '.', 'construct_subsegment_by_unit', '(', 'all_unit', '[', ':', '4', ']', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "ind2 = self . unit2 . channel_indexes [ 0 ] \n"
Original    (011): ['ind2', '=', 'self', '.', 'unit2', '.', 'channel_indexes', '[', '0', ']', '\\n']
Tokenized   (019): ['<s>', 'ind', '2', '=', 'self', '.', 'unit', '2', '.', 'channel', '_', 'index', 'es', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (017): ['ind', '2', '=', 'self', '.', 'unit', '2', '.', 'channel', '_', 'index', 'es', '[', '0', ']', '\\', 'n']
Detokenized (011): ['ind2', '=', 'self', '.', 'unit2', '.', 'channel_indexes', '[', '0', ']', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "result22 = self . seg1 . take_analogsignal_by_channelindex ( [ ind2 ] ) \n"
Original    (013): ['result22', '=', 'self', '.', 'seg1', '.', 'take_analogsignal_by_channelindex', '(', '[', 'ind2', ']', ')', '\\n']
Tokenized   (030): ['<s>', 'result', '22', '=', 'self', '.', 'se', 'g', '1', '.', 'take', '_', 'an', 'alog', 'sign', 'al', '_', 'by', '_', 'channel', 'index', '(', '[', 'ind', '2', ']', ')', '\\', 'n', '</s>']
Filtered   (028): ['result', '22', '=', 'self', '.', 'se', 'g', '1', '.', 'take', '_', 'an', 'alog', 'sign', 'al', '_', 'by', '_', 'channel', 'index', '(', '[', 'ind', '2', ']', ')', '\\', 'n']
Detokenized (013): ['result22', '=', 'self', '.', 'seg1', '.', 'take_analogsignal_by_channelindex', '(', '[', 'ind2', ']', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "targ1 = [ self . sigarrs1a [ 0 ] [ : , np . array ( [ True ] ) ] , \n"
Original    (023): ['targ1', '=', '[', 'self', '.', 'sigarrs1a', '[', '0', ']', '[', ':', ',', 'np', '.', 'array', '(', '[', 'True', ']', ')', ']', ',', '\\n']
Tokenized   (032): ['<s>', 't', 'arg', '1', '=', '[', 'self', '.', 'sig', 'arr', 's', '1', 'a', '[', '0', ']', '[', ':', ',', 'np', '.', 'array', '(', '[', 'True', ']', ')', ']', ',', '\\', 'n', '</s>']
Filtered   (030): ['t', 'arg', '1', '=', '[', 'self', '.', 'sig', 'arr', 's', '1', 'a', '[', '0', ']', '[', ':', ',', 'np', '.', 'array', '(', '[', 'True', ']', ')', ']', ',', '\\', 'n']
Detokenized (023): ['targ1', '=', '[', 'self', '.', 'sigarrs1a', '[', '0', ']', '[', ':', ',', 'np', '.', 'array', '(', '[', 'True', ']', ')', ']', ',', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "result21 = seg . take_slice_of_analogsignalarray_by_channelindex ( [ ind1 ] ) \n"
Original    (011): ['result21', '=', 'seg', '.', 'take_slice_of_analogsignalarray_by_channelindex', '(', '[', 'ind1', ']', ')', '\\n']
Tokenized   (032): ['<s>', 'result', '21', '=', 'se', 'g', '.', 'take', '_', 'slice', '_', 'of', '_', 'an', 'alog', 'sign', 'al', 'array', '_', 'by', '_', 'channel', 'index', '(', '[', 'ind', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (030): ['result', '21', '=', 'se', 'g', '.', 'take', '_', 'slice', '_', 'of', '_', 'an', 'alog', 'sign', 'al', 'array', '_', 'by', '_', 'channel', 'index', '(', '[', 'ind', '1', ']', ')', '\\', 'n']
Detokenized (011): ['result21', '=', 'seg', '.', 'take_slice_of_analogsignalarray_by_channelindex', '(', '[', 'ind1', ']', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "json_content = json_content . decode ( "utf-8" ) . replace ( , ) \n"
Original    (014): ['json_content', '=', 'json_content', '.', 'decode', '(', '"utf-8"', ')', '.', 'replace', '(', ',', ')', '\\n']
Tokenized   (025): ['<s>', 'json', '_', 'content', '=', 'json', '_', 'content', '.', 'decode', '(', '"', 'utf', '-', '8', '"', ')', '.', 'replace', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (023): ['json', '_', 'content', '=', 'json', '_', 'content', '.', 'decode', '(', '"', 'utf', '-', '8', '"', ')', '.', 'replace', '(', ',', ')', '\\', 'n']
Detokenized (014): ['json_content', '=', 'json_content', '.', 'decode', '(', '"utf-8"', ')', '.', 'replace', '(', ',', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "Image1 = StatisticMap ( name = , collection = self . Collection1 , file = , Image1 . file = SimpleUploadedFile ( , file ( os . path . join ( self . test_path , Image1 . save ( ) \n"
Original    (041): ['Image1', '=', 'StatisticMap', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection1', ',', 'file', '=', ',', 'Image1', '.', 'file', '=', 'SimpleUploadedFile', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', 'Image1', '.', 'save', '(', ')', '\\n']
Tokenized   (055): ['<s>', 'Image', '1', '=', 'Stat', 'istic', 'Map', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection', '1', ',', 'file', '=', ',', 'Image', '1', '.', 'file', '=', 'Simple', 'Upload', 'ed', 'File', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', 'Image', '1', '.', 'save', '(', ')', '\\', 'n', '</s>']
Filtered   (053): ['Image', '1', '=', 'Stat', 'istic', 'Map', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection', '1', ',', 'file', '=', ',', 'Image', '1', '.', 'file', '=', 'Simple', 'Upload', 'ed', 'File', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', 'Image', '1', '.', 'save', '(', ')', '\\', 'n']
Detokenized (041): ['Image1', '=', 'StatisticMap', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection1', ',', 'file', '=', ',', 'Image1', '.', 'file', '=', 'SimpleUploadedFile', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', 'Image1', '.', 'save', '(', ')', '\\n']
Counter: 53
===================================================================
Hidden states:  (13, 41, 768)
# Extracted words:  41
Sentence         : "fname = os . path . basename ( os . path . join ( self . test_path , ) ) file_dict = { : SimpleUploadedFile ( fname , zip_file . read ( ) ) } \n"
Original    (036): ['fname', '=', 'os', '.', 'path', '.', 'basename', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', ')', ')', 'file_dict', '=', '{', ':', 'SimpleUploadedFile', '(', 'fname', ',', 'zip_file', '.', 'read', '(', ')', ')', '}', '\\n']
Tokenized   (051): ['<s>', 'f', 'name', '=', 'os', '.', 'path', '.', 'bas', 'ename', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', ')', ')', 'file', '_', 'dict', '=', '{', ':', 'Simple', 'Upload', 'ed', 'File', '(', 'f', 'name', ',', 'zip', '_', 'file', '.', 'read', '(', ')', ')', '}', '\\', 'n', '</s>']
Filtered   (049): ['f', 'name', '=', 'os', '.', 'path', '.', 'bas', 'ename', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', ')', ')', 'file', '_', 'dict', '=', '{', ':', 'Simple', 'Upload', 'ed', 'File', '(', 'f', 'name', ',', 'zip', '_', 'file', '.', 'read', '(', ')', ')', '}', '\\', 'n']
Detokenized (036): ['fname', '=', 'os', '.', 'path', '.', 'basename', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', ')', ')', 'file_dict', '=', '{', ':', 'SimpleUploadedFile', '(', 'fname', ',', 'zip_file', '.', 'read', '(', ')', ')', '}', '\\n']
Counter: 49
===================================================================
Hidden states:  (13, 36, 768)
# Extracted words:  36
Sentence         : "Image2ss = StatisticMap ( name = , collection = self . Collection3 , file = Image2ss . file = SimpleUploadedFile ( , file ( os . path . join ( self . test_path , Image2ss . save ( ) \n"
Original    (040): ['Image2ss', '=', 'StatisticMap', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection3', ',', 'file', '=', 'Image2ss', '.', 'file', '=', 'SimpleUploadedFile', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', 'Image2ss', '.', 'save', '(', ')', '\\n']
Tokenized   (057): ['<s>', 'Image', '2', 'ss', '=', 'Stat', 'istic', 'Map', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection', '3', ',', 'file', '=', 'Image', '2', 'ss', '.', 'file', '=', 'Simple', 'Upload', 'ed', 'File', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', 'Image', '2', 'ss', '.', 'save', '(', ')', '\\', 'n', '</s>']
Filtered   (055): ['Image', '2', 'ss', '=', 'Stat', 'istic', 'Map', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection', '3', ',', 'file', '=', 'Image', '2', 'ss', '.', 'file', '=', 'Simple', 'Upload', 'ed', 'File', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test', '_', 'path', ',', 'Image', '2', 'ss', '.', 'save', '(', ')', '\\', 'n']
Detokenized (040): ['Image2ss', '=', 'StatisticMap', '(', 'name', '=', ',', 'collection', '=', 'self', '.', 'Collection3', ',', 'file', '=', 'Image2ss', '.', 'file', '=', 'SimpleUploadedFile', '(', ',', 'file', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'test_path', ',', 'Image2ss', '.', 'save', '(', ')', '\\n']
Counter: 55
===================================================================
Hidden states:  (13, 40, 768)
# Extracted words:  40
Sentence         : "acc_new = rho * acc + ( 1 - rho ) * g ** 2 \n"
Original    (016): ['acc_new', '=', 'rho', '*', 'acc', '+', '(', '1', '-', 'rho', ')', '*', 'g', '**', '2', '\\n']
Tokenized   (023): ['<s>', 'acc', '_', 'new', '=', 'r', 'ho', '*', 'acc', '+', '(', '1', '-', 'r', 'ho', ')', '*', 'g', '**', '2', '\\', 'n', '</s>']
Filtered   (021): ['acc', '_', 'new', '=', 'r', 'ho', '*', 'acc', '+', '(', '1', '-', 'r', 'ho', ')', '*', 'g', '**', '2', '\\', 'n']
Detokenized (016): ['acc_new', '=', 'rho', '*', 'acc', '+', '(', '1', '-', 'rho', ')', '*', 'g', '**', '2', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "gradient_scaling = T . sqrt ( acc_new + epsilon ) \n"
Original    (011): ['gradient_scaling', '=', 'T', '.', 'sqrt', '(', 'acc_new', '+', 'epsilon', ')', '\\n']
Tokenized   (022): ['<s>', 'gradient', '_', 'sc', 'aling', '=', 'T', '.', 'sq', 'rt', '(', 'acc', '_', 'new', '+', 'e', 'ps', 'ilon', ')', '\\', 'n', '</s>']
Filtered   (020): ['gradient', '_', 'sc', 'aling', '=', 'T', '.', 'sq', 'rt', '(', 'acc', '_', 'new', '+', 'e', 'ps', 'ilon', ')', '\\', 'n']
Detokenized (011): ['gradient_scaling', '=', 'T', '.', 'sqrt', '(', 'acc_new', '+', 'epsilon', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "py_x = softmax ( T . dot ( h2 , w_o ) ) \n"
Original    (014): ['py_x', '=', 'softmax', '(', 'T', '.', 'dot', '(', 'h2', ',', 'w_o', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'py', '_', 'x', '=', 'soft', 'max', '(', 'T', '.', 'dot', '(', 'h', '2', ',', 'w', '_', 'o', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['py', '_', 'x', '=', 'soft', 'max', '(', 'T', '.', 'dot', '(', 'h', '2', ',', 'w', '_', 'o', ')', ')', '\\', 'n']
Detokenized (014): ['py_x', '=', 'softmax', '(', 'T', '.', 'dot', '(', 'h2', ',', 'w_o', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "w_h = init_weights ( ( 784 , 625 ) ) \n"
Original    (011): ['w_h', '=', 'init_weights', '(', '(', '784', ',', '625', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'w', '_', 'h', '=', 'init', '_', 'weights', '(', '(', '7', '84', ',', '625', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['w', '_', 'h', '=', 'init', '_', 'weights', '(', '(', '7', '84', ',', '625', ')', ')', '\\', 'n']
Detokenized (011): ['w_h', '=', 'init_weights', '(', '(', '784', ',', '625', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "noise_h , noise_h2 , noise_py_x = model ( X , w_h , w_h2 , w_o , 0.2 , 0.5 ) \n"
Original    (021): ['noise_h', ',', 'noise_h2', ',', 'noise_py_x', '=', 'model', '(', 'X', ',', 'w_h', ',', 'w_h2', ',', 'w_o', ',', '0.2', ',', '0.5', ')', '\\n']
Tokenized   (045): ['<s>', 'no', 'ise', '_', 'h', ',', 'noise', '_', 'h', '2', ',', 'noise', '_', 'py', '_', 'x', '=', 'model', '(', 'X', ',', 'w', '_', 'h', ',', 'w', '_', 'h', '2', ',', 'w', '_', 'o', ',', '0', '.', '2', ',', '0', '.', '5', ')', '\\', 'n', '</s>']
Filtered   (043): ['no', 'ise', '_', 'h', ',', 'noise', '_', 'h', '2', ',', 'noise', '_', 'py', '_', 'x', '=', 'model', '(', 'X', ',', 'w', '_', 'h', ',', 'w', '_', 'h', '2', ',', 'w', '_', 'o', ',', '0', '.', '2', ',', '0', '.', '5', ')', '\\', 'n']
Detokenized (021): ['noise_h', ',', 'noise_h2', ',', 'noise_py_x', '=', 'model', '(', 'X', ',', 'w_h', ',', 'w_h2', ',', 'w_o', ',', '0.2', ',', '0.5', ')', '\\n']
Counter: 43
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "y_x = T . argmax ( py_x , axis = 1 ) \n"
Original    (013): ['y_x', '=', 'T', '.', 'argmax', '(', 'py_x', ',', 'axis', '=', '1', ')', '\\n']
Tokenized   (021): ['<s>', 'y', '_', 'x', '=', 'T', '.', 'arg', 'max', '(', 'py', '_', 'x', ',', 'axis', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (019): ['y', '_', 'x', '=', 'T', '.', 'arg', 'max', '(', 'py', '_', 'x', ',', 'axis', '=', '1', ')', '\\', 'n']
Detokenized (013): ['y_x', '=', 'T', '.', 'argmax', '(', 'py_x', ',', 'axis', '=', '1', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "cost = T . mean ( T . nnet . categorical_crossentropy ( noise_py_x , Y ) ) \n"
Original    (018): ['cost', '=', 'T', '.', 'mean', '(', 'T', '.', 'nnet', '.', 'categorical_crossentropy', '(', 'noise_py_x', ',', 'Y', ')', ')', '\\n']
Tokenized   (031): ['<s>', 'cost', '=', 'T', '.', 'mean', '(', 'T', '.', 'n', 'net', '.', 'categ', 'orical', '_', 'cross', 'ent', 'ropy', '(', 'noise', '_', 'py', '_', 'x', ',', 'Y', ')', ')', '\\', 'n', '</s>']
Filtered   (029): ['cost', '=', 'T', '.', 'mean', '(', 'T', '.', 'n', 'net', '.', 'categ', 'orical', '_', 'cross', 'ent', 'ropy', '(', 'noise', '_', 'py', '_', 'x', ',', 'Y', ')', ')', '\\', 'n']
Detokenized (018): ['cost', '=', 'T', '.', 'mean', '(', 'T', '.', 'nnet', '.', 'categorical_crossentropy', '(', 'noise_py_x', ',', 'Y', ')', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "updates = RMSprop ( cost , params , lr = 0.001 ) \n"
Original    (013): ['updates', '=', 'RMSprop', '(', 'cost', ',', 'params', ',', 'lr', '=', '0.001', ')', '\\n']
Tokenized   (022): ['<s>', 'up', 'dates', '=', 'R', 'MS', 'prop', '(', 'cost', ',', 'params', ',', 'l', 'r', '=', '0', '.', '001', ')', '\\', 'n', '</s>']
Filtered   (020): ['up', 'dates', '=', 'R', 'MS', 'prop', '(', 'cost', ',', 'params', ',', 'l', 'r', '=', '0', '.', '001', ')', '\\', 'n']
Detokenized (013): ['updates', '=', 'RMSprop', '(', 'cost', ',', 'params', ',', 'lr', '=', '0.001', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "train = theano . function ( inputs = [ X , Y ] , outputs = cost , updates = updates , allow_input_downcast = True ) \n"
Original    (027): ['train', '=', 'theano', '.', 'function', '(', 'inputs', '=', '[', 'X', ',', 'Y', ']', ',', 'outputs', '=', 'cost', ',', 'updates', '=', 'updates', ',', 'allow_input_downcast', '=', 'True', ')', '\\n']
Tokenized   (036): ['<s>', 'train', '=', 'the', 'ano', '.', 'function', '(', 'inputs', '=', '[', 'X', ',', 'Y', ']', ',', 'outputs', '=', 'cost', ',', 'updates', '=', 'updates', ',', 'allow', '_', 'input', '_', 'down', 'cast', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (034): ['train', '=', 'the', 'ano', '.', 'function', '(', 'inputs', '=', '[', 'X', ',', 'Y', ']', ',', 'outputs', '=', 'cost', ',', 'updates', '=', 'updates', ',', 'allow', '_', 'input', '_', 'down', 'cast', '=', 'True', ')', '\\', 'n']
Detokenized (027): ['train', '=', 'theano', '.', 'function', '(', 'inputs', '=', '[', 'X', ',', 'Y', ']', ',', 'outputs', '=', 'cost', ',', 'updates', '=', 'updates', ',', 'allow_input_downcast', '=', 'True', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "predict = theano . function ( inputs = [ X ] , outputs = y_x , allow_input_downcast = True ) \n"
Original    (021): ['predict', '=', 'theano', '.', 'function', '(', 'inputs', '=', '[', 'X', ']', ',', 'outputs', '=', 'y_x', ',', 'allow_input_downcast', '=', 'True', ')', '\\n']
Tokenized   (033): ['<s>', 'p', 'redict', '=', 'the', 'ano', '.', 'function', '(', 'inputs', '=', '[', 'X', ']', ',', 'outputs', '=', 'y', '_', 'x', ',', 'allow', '_', 'input', '_', 'down', 'cast', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (031): ['p', 'redict', '=', 'the', 'ano', '.', 'function', '(', 'inputs', '=', '[', 'X', ']', ',', 'outputs', '=', 'y', '_', 'x', ',', 'allow', '_', 'input', '_', 'down', 'cast', '=', 'True', ')', '\\', 'n']
Detokenized (021): ['predict', '=', 'theano', '.', 'function', '(', 'inputs', '=', '[', 'X', ']', ',', 'outputs', '=', 'y_x', ',', 'allow_input_downcast', '=', 'True', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "settings . DATABASE_CONFIG_DICT [ ] ) \n"
Original    (007): ['settings', '.', 'DATABASE_CONFIG_DICT', '[', ']', ')', '\\n']
Tokenized   (019): ['<s>', 'settings', '.', 'D', 'AT', 'AB', 'ASE', '_', 'CON', 'FIG', '_', 'D', 'ICT', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (017): ['settings', '.', 'D', 'AT', 'AB', 'ASE', '_', 'CON', 'FIG', '_', 'D', 'ICT', '[', ']', ')', '\\', 'n']
Detokenized (007): ['settings', '.', 'DATABASE_CONFIG_DICT', '[', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "TEMPLATES [ 0 ] [ ] [ ] = DEBUG \n"
Original    (011): ['TEMPLATES', '[', '0', ']', '[', ']', '[', ']', '=', 'DEBUG', '\\n']
Tokenized   (017): ['<s>', 'T', 'EM', 'PL', 'ATES', '[', '0', ']', '[', ']', '[', ']', '=', 'DEBUG', '\\', 'n', '</s>']
Filtered   (015): ['T', 'EM', 'PL', 'ATES', '[', '0', ']', '[', ']', '[', ']', '=', 'DEBUG', '\\', 'n']
Detokenized (011): ['TEMPLATES', '[', '0', ']', '[', ']', '[', ']', '=', 'DEBUG', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "SECRET_KEY = env ( "DJANGO_SECRET_KEY" , default = ) \n"
Original    (010): ['SECRET_KEY', '=', 'env', '(', '"DJANGO_SECRET_KEY"', ',', 'default', '=', ')', '\\n']
Tokenized   (025): ['<s>', 'SEC', 'RET', '_', 'KEY', '=', 'env', '(', '"', 'DJ', 'AN', 'GO', '_', 'SEC', 'RET', '_', 'KEY', '"', ',', 'default', '=', ')', '\\', 'n', '</s>']
Filtered   (023): ['SEC', 'RET', '_', 'KEY', '=', 'env', '(', '"', 'DJ', 'AN', 'GO', '_', 'SEC', 'RET', '_', 'KEY', '"', ',', 'default', '=', ')', '\\', 'n']
Detokenized (010): ['SECRET_KEY', '=', 'env', '(', '"DJANGO_SECRET_KEY"', ',', 'default', '=', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "MIDDLEWARE_CLASSES += ( , ) \n"
Original    (006): ['MIDDLEWARE_CLASSES', '+=', '(', ',', ')', '\\n']
Tokenized   (016): ['<s>', 'M', 'ID', 'D', 'LE', 'WARE', '_', 'CLASS', 'ES', '+=', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (014): ['M', 'ID', 'D', 'LE', 'WARE', '_', 'CLASS', 'ES', '+=', '(', ',', ')', '\\', 'n']
Detokenized (006): ['MIDDLEWARE_CLASSES', '+=', '(', ',', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "INTERNAL_IPS = ( , , ) \n"
Original    (007): ['INTERNAL_IPS', '=', '(', ',', ',', ')', '\\n']
Tokenized   (014): ['<s>', 'IN', 'TERN', 'AL', '_', 'IPS', '=', '(', ',', ',', ')', '\\', 'n', '</s>']
Filtered   (012): ['IN', 'TERN', 'AL', '_', 'IPS', '=', '(', ',', ',', ')', '\\', 'n']
Detokenized (007): ['INTERNAL_IPS', '=', '(', ',', ',', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "redirect_url = request . POST . get ( ) or \n"
Original    (011): ['redirect_url', '=', 'request', '.', 'POST', '.', 'get', '(', ')', 'or', '\\n']
Tokenized   (017): ['<s>', 'red', 'irect', '_', 'url', '=', 'request', '.', 'POST', '.', 'get', '(', ')', 'or', '\\', 'n', '</s>']
Filtered   (015): ['red', 'irect', '_', 'url', '=', 'request', '.', 'POST', '.', 'get', '(', ')', 'or', '\\', 'n']
Detokenized (011): ['redirect_url', '=', 'request', '.', 'POST', '.', 'get', '(', ')', 'or', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "icon = self . get_plugin_icon ( ) , \n"
Original    (009): ['icon', '=', 'self', '.', 'get_plugin_icon', '(', ')', ',', '\\n']
Tokenized   (016): ['<s>', 'icon', '=', 'self', '.', 'get', '_', 'plugin', '_', 'icon', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (014): ['icon', '=', 'self', '.', 'get', '_', 'plugin', '_', 'icon', '(', ')', ',', '\\', 'n']
Detokenized (009): ['icon', '=', 'self', '.', 'get_plugin_icon', '(', ')', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "memoryprofiler_act . setEnabled ( is_memoryprofiler_installed ( ) ) \n"
Original    (009): ['memoryprofiler_act', '.', 'setEnabled', '(', 'is_memoryprofiler_installed', '(', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'memory', 'prof', 'iler', '_', 'act', '.', 'set', 'Enabled', '(', 'is', '_', 'memory', 'prof', 'iler', '_', 'installed', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['memory', 'prof', 'iler', '_', 'act', '.', 'set', 'Enabled', '(', 'is', '_', 'memory', 'prof', 'iler', '_', 'installed', '(', ')', ')', '\\', 'n']
Detokenized (009): ['memoryprofiler_act', '.', 'setEnabled', '(', 'is_memoryprofiler_installed', '(', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "wdir , args = None , None \n"
Original    (008): ['wdir', ',', 'args', '=', 'None', ',', 'None', '\\n']
Tokenized   (012): ['<s>', 'w', 'dir', ',', 'args', '=', 'None', ',', 'None', '\\', 'n', '</s>']
Filtered   (010): ['w', 'dir', ',', 'args', '=', 'None', ',', 'None', '\\', 'n']
Detokenized (008): ['wdir', ',', 'args', '=', 'None', ',', 'None', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "use_colors = self . get_option ( , True ) ) \n"
Original    (011): ['use_colors', '=', 'self', '.', 'get_option', '(', ',', 'True', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'use', '_', 'col', 'ors', '=', 'self', '.', 'get', '_', 'option', '(', ',', 'True', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['use', '_', 'col', 'ors', '=', 'self', '.', 'get', '_', 'option', '(', ',', 'True', ')', ')', '\\', 'n']
Detokenized (011): ['use_colors', '=', 'self', '.', 'get_option', '(', ',', 'True', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "message_type = None , enum_type = None , containing_type = None , \n"
Original    (013): ['message_type', '=', 'None', ',', 'enum_type', '=', 'None', ',', 'containing_type', '=', 'None', ',', '\\n']
Tokenized   (022): ['<s>', 'message', '_', 'type', '=', 'None', ',', 'enum', '_', 'type', '=', 'None', ',', 'containing', '_', 'type', '=', 'None', ',', '\\', 'n', '</s>']
Filtered   (020): ['message', '_', 'type', '=', 'None', ',', 'enum', '_', 'type', '=', 'None', ',', 'containing', '_', 'type', '=', 'None', ',', '\\', 'n']
Detokenized (013): ['message_type', '=', 'None', ',', 'enum_type', '=', 'None', ',', 'containing_type', '=', 'None', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "is_extension = False , extension_scope = None , \n"
Original    (009): ['is_extension', '=', 'False', ',', 'extension_scope', '=', 'None', ',', '\\n']
Tokenized   (017): ['<s>', 'is', '_', 'ext', 'ension', '=', 'False', ',', 'extension', '_', 'scope', '=', 'None', ',', '\\', 'n', '</s>']
Filtered   (015): ['is', '_', 'ext', 'ension', '=', 'False', ',', 'extension', '_', 'scope', '=', 'None', ',', '\\', 'n']
Detokenized (009): ['is_extension', '=', 'False', ',', 'extension_scope', '=', 'None', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "_BATCHNOTIFICATIONREQUEST . fields_by_name [ ] . message_type = _PUSHNOTIFICATION \n"
Original    (010): ['_BATCHNOTIFICATIONREQUEST', '.', 'fields_by_name', '[', ']', '.', 'message_type', '=', '_PUSHNOTIFICATION', '\\n']
Tokenized   (031): ['<s>', '_', 'B', 'ATCH', 'NOT', 'IFIC', 'ATION', 'RE', 'QUEST', '.', 'fields', '_', 'by', '_', 'name', '[', ']', '.', 'message', '_', 'type', '=', '_', 'P', 'USH', 'NOT', 'IFIC', 'ATION', '\\', 'n', '</s>']
Filtered   (029): ['_', 'B', 'ATCH', 'NOT', 'IFIC', 'ATION', 'RE', 'QUEST', '.', 'fields', '_', 'by', '_', 'name', '[', ']', '.', 'message', '_', 'type', '=', '_', 'P', 'USH', 'NOT', 'IFIC', 'ATION', '\\', 'n']
Detokenized (010): ['_BATCHNOTIFICATIONREQUEST', '.', 'fields_by_name', '[', ']', '.', 'message_type', '=', '_PUSHNOTIFICATION', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "expected_zip_content = [ "manifest.json" , "sd_bl.bin" , "sd_bl.dat" ] \n"
Original    (010): ['expected_zip_content', '=', '[', '"manifest.json"', ',', '"sd_bl.bin"', ',', '"sd_bl.dat"', ']', '\\n']
Tokenized   (034): ['<s>', 'expected', '_', 'zip', '_', 'content', '=', '[', '"', 'man', 'ifest', '.', 'json', '"', ',', '"', 'sd', '_', 'bl', '.', 'bin', '"', ',', '"', 'sd', '_', 'bl', '.', 'dat', '"', ']', '\\', 'n', '</s>']
Filtered   (032): ['expected', '_', 'zip', '_', 'content', '=', '[', '"', 'man', 'ifest', '.', 'json', '"', ',', '"', 'sd', '_', 'bl', '.', 'bin', '"', ',', '"', 'sd', '_', 'bl', '.', 'dat', '"', ']', '\\', 'n']
Detokenized (010): ['expected_zip_content', '=', '[', '"manifest.json"', ',', '"sd_bl.bin"', ',', '"sd_bl.dat"', ']', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sd_req = [ 0x1000 , 0xffff ] , \n"
Original    (009): ['sd_req', '=', '[', '0x1000', ',', '0xffff', ']', ',', '\\n']
Tokenized   (018): ['<s>', 'sd', '_', 'req', '=', '[', '0', 'x', '1000', ',', '0', 'x', 'ffff', ']', ',', '\\', 'n', '</s>']
Filtered   (016): ['sd', '_', 'req', '=', '[', '0', 'x', '1000', ',', '0', 'x', 'ffff', ']', ',', '\\', 'n']
Detokenized (009): ['sd_req', '=', '[', '0x1000', ',', '0xffff', ']', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "pkg_name = os . path . join ( self . work_directory , "mypackage.zip" ) \n"
Original    (015): ['pkg_name', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work_directory', ',', '"mypackage.zip"', ')', '\\n']
Tokenized   (027): ['<s>', 'pkg', '_', 'name', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work', '_', 'directory', ',', '"', 'my', 'package', '.', 'zip', '"', ')', '\\', 'n', '</s>']
Filtered   (025): ['pkg', '_', 'name', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work', '_', 'directory', ',', '"', 'my', 'package', '.', 'zip', '"', ')', '\\', 'n']
Detokenized (015): ['pkg_name', '=', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work_directory', ',', '"mypackage.zip"', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "manifest = self . p . unpack_package ( os . path . join ( self . work_directory , pkg_name ) , unpacked_dir ) \n"
Original    (024): ['manifest', '=', 'self', '.', 'p', '.', 'unpack_package', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work_directory', ',', 'pkg_name', ')', ',', 'unpacked_dir', ')', '\\n']
Tokenized   (039): ['<s>', 'man', 'ifest', '=', 'self', '.', 'p', '.', 'un', 'pack', '_', 'package', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work', '_', 'directory', ',', 'p', 'kg', '_', 'name', ')', ',', 'unp', 'acked', '_', 'dir', ')', '\\', 'n', '</s>']
Filtered   (037): ['man', 'ifest', '=', 'self', '.', 'p', '.', 'un', 'pack', '_', 'package', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work', '_', 'directory', ',', 'p', 'kg', '_', 'name', ')', ',', 'unp', 'acked', '_', 'dir', ')', '\\', 'n']
Detokenized (024): ['manifest', '=', 'self', '.', 'p', '.', 'unpack_package', '(', 'os', '.', 'path', '.', 'join', '(', 'self', '.', 'work_directory', ',', 'pkg_name', ')', ',', 'unpacked_dir', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "style = wx . richtext . RE_MULTILINE , value = ) \n"
Original    (012): ['style', '=', 'wx', '.', 'richtext', '.', 'RE_MULTILINE', ',', 'value', '=', ')', '\\n']
Tokenized   (022): ['<s>', 'style', '=', 'w', 'x', '.', 'rich', 'text', '.', 'RE', '_', 'M', 'ULT', 'IL', 'INE', ',', 'value', '=', ')', '\\', 'n', '</s>']
Filtered   (020): ['style', '=', 'w', 'x', '.', 'rich', 'text', '.', 'RE', '_', 'M', 'ULT', 'IL', 'INE', ',', 'value', '=', ')', '\\', 'n']
Detokenized (012): ['style', '=', 'wx', '.', 'richtext', '.', 'RE_MULTILINE', ',', 'value', '=', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "fgSizer1 . Add ( self . lblChangelt , 0 , wx . ALL , 5 ) \n"
Original    (017): ['fgSizer1', '.', 'Add', '(', 'self', '.', 'lblChangelt', ',', '0', ',', 'wx', '.', 'ALL', ',', '5', ')', '\\n']
Tokenized   (028): ['<s>', 'fg', 'S', 'izer', '1', '.', 'Add', '(', 'self', '.', 'l', 'bl', 'Ch', 'ang', 'elt', ',', '0', ',', 'w', 'x', '.', 'ALL', ',', '5', ')', '\\', 'n', '</s>']
Filtered   (026): ['fg', 'S', 'izer', '1', '.', 'Add', '(', 'self', '.', 'l', 'bl', 'Ch', 'ang', 'elt', ',', '0', ',', 'w', 'x', '.', 'ALL', ',', '5', ')', '\\', 'n']
Detokenized (017): ['fgSizer1', '.', 'Add', '(', 'self', '.', 'lblChangelt', ',', '0', ',', 'wx', '.', 'ALL', ',', '5', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "sbThreshold . Add ( fgSizer1 , 0 , wx . EXPAND , 5 ) \n"
Original    (015): ['sbThreshold', '.', 'Add', '(', 'fgSizer1', ',', '0', ',', 'wx', '.', 'EXPAND', ',', '5', ')', '\\n']
Tokenized   (026): ['<s>', 'sb', 'Th', 'reshold', '.', 'Add', '(', 'f', 'g', 'S', 'izer', '1', ',', '0', ',', 'w', 'x', '.', 'EXP', 'AND', ',', '5', ')', '\\', 'n', '</s>']
Filtered   (024): ['sb', 'Th', 'reshold', '.', 'Add', '(', 'f', 'g', 'S', 'izer', '1', ',', '0', ',', 'w', 'x', '.', 'EXP', 'AND', ',', '5', ')', '\\', 'n']
Detokenized (015): ['sbThreshold', '.', 'Add', '(', 'fgSizer1', ',', '0', ',', 'wx', '.', 'EXPAND', ',', '5', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "bsValueThresh . Add ( sbThreshold , 1 , 0 , 5 ) \n"
Original    (013): ['bsValueThresh', '.', 'Add', '(', 'sbThreshold', ',', '1', ',', '0', ',', '5', ')', '\\n']
Tokenized   (022): ['<s>', 'bs', 'Value', 'Th', 'resh', '.', 'Add', '(', 's', 'b', 'Th', 'reshold', ',', '1', ',', '0', ',', '5', ')', '\\', 'n', '</s>']
Filtered   (020): ['bs', 'Value', 'Th', 'resh', '.', 'Add', '(', 's', 'b', 'Th', 'reshold', ',', '1', ',', '0', ',', '5', ')', '\\', 'n']
Detokenized (013): ['bsValueThresh', '.', 'Add', '(', 'sbThreshold', ',', '1', ',', '0', ',', '5', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "cbGapTimeChoices = [ u"second" , u"minute" , u"hour" , u"day" ] \n"
Original    (012): ['cbGapTimeChoices', '=', '[', 'u"second"', ',', 'u"minute"', ',', 'u"hour"', ',', 'u"day"', ']', '\\n']
Tokenized   (032): ['<s>', 'cb', 'G', 'ap', 'Time', 'Cho', 'ices', '=', '[', 'u', '"', 'second', '"', ',', 'u', '"', 'minute', '"', ',', 'u', '"', 'hour', '"', ',', 'u', '"', 'day', '"', ']', '\\', 'n', '</s>']
Filtered   (030): ['cb', 'G', 'ap', 'Time', 'Cho', 'ices', '=', '[', 'u', '"', 'second', '"', ',', 'u', '"', 'minute', '"', ',', 'u', '"', 'hour', '"', ',', 'u', '"', 'day', '"', ']', '\\', 'n']
Detokenized (012): ['cbGapTimeChoices', '=', '[', 'u"second"', ',', 'u"minute"', ',', 'u"hour"', ',', 'u"day"', ']', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "fmt24hr = True , spinButton = self . sbBefore , oob_color = ) \n"
Original    (014): ['fmt24hr', '=', 'True', ',', 'spinButton', '=', 'self', '.', 'sbBefore', ',', 'oob_color', '=', ')', '\\n']
Tokenized   (026): ['<s>', 'f', 'mt', '24', 'hr', '=', 'True', ',', 'spin', 'Button', '=', 'self', '.', 's', 'b', 'Before', ',', 'o', 'ob', '_', 'color', '=', ')', '\\', 'n', '</s>']
Filtered   (024): ['f', 'mt', '24', 'hr', '=', 'True', ',', 'spin', 'Button', '=', 'self', '.', 's', 'b', 'Before', ',', 'o', 'ob', '_', 'color', '=', ')', '\\', 'n']
Detokenized (014): ['fmt24hr', '=', 'True', ',', 'spinButton', '=', 'self', '.', 'sbBefore', ',', 'oob_color', '=', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "bsButtons . Add ( self . btnOK , 1 , wx . ALL | wx . EXPAND , 5 ) \n"
Original    (021): ['bsButtons', '.', 'Add', '(', 'self', '.', 'btnOK', ',', '1', ',', 'wx', '.', 'ALL', '|', 'wx', '.', 'EXPAND', ',', '5', ')', '\\n']
Tokenized   (031): ['<s>', 'bs', 'But', 'tons', '.', 'Add', '(', 'self', '.', 'b', 'tn', 'OK', ',', '1', ',', 'w', 'x', '.', 'ALL', '|', 'w', 'x', '.', 'EXP', 'AND', ',', '5', ')', '\\', 'n', '</s>']
Filtered   (029): ['bs', 'But', 'tons', '.', 'Add', '(', 'self', '.', 'b', 'tn', 'OK', ',', '1', ',', 'w', 'x', '.', 'ALL', '|', 'w', 'x', '.', 'EXP', 'AND', ',', '5', ')', '\\', 'n']
Detokenized (021): ['bsButtons', '.', 'Add', '(', 'self', '.', 'btnOK', ',', '1', ',', 'wx', '.', 'ALL', '|', 'wx', '.', 'EXPAND', ',', '5', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "mat ( op2 . INC , ( elem_node [ op2 . i [ 0 ] ] , \n"
Original    (018): ['mat', '(', 'op2', '.', 'INC', ',', '(', 'elem_node', '[', 'op2', '.', 'i', '[', '0', ']', ']', ',', '\\n']
Tokenized   (026): ['<s>', 'mat', '(', 'op', '2', '.', 'INC', ',', '(', 'ele', 'm', '_', 'node', '[', 'op', '2', '.', 'i', '[', '0', ']', ']', ',', '\\', 'n', '</s>']
Filtered   (024): ['mat', '(', 'op', '2', '.', 'INC', ',', '(', 'ele', 'm', '_', 'node', '[', 'op', '2', '.', 'i', '[', '0', ']', ']', ',', '\\', 'n']
Detokenized (018): ['mat', '(', 'op2', '.', 'INC', ',', '(', 'elem_node', '[', 'op2', '.', 'i', '[', '0', ']', ']', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "elem_node [ op2 . i [ 1 ] ] ) ) , \n"
Original    (013): ['elem_node', '[', 'op2', '.', 'i', '[', '1', ']', ']', ')', ')', ',', '\\n']
Tokenized   (020): ['<s>', 'e', 'lem', '_', 'node', '[', 'op', '2', '.', 'i', '[', '1', ']', ']', ')', ')', ',', '\\', 'n', '</s>']
Filtered   (018): ['e', 'lem', '_', 'node', '[', 'op', '2', '.', 'i', '[', '1', ']', ']', ')', ')', ',', '\\', 'n']
Detokenized (013): ['elem_node', '[', 'op2', '.', 'i', '[', '1', ']', ']', ')', ')', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "partition_size = NUM_ELE / 2 , \n"
Original    (007): ['partition_size', '=', 'NUM_ELE', '/', '2', ',', '\\n']
Tokenized   (016): ['<s>', 'part', 'ition', '_', 'size', '=', 'NUM', '_', 'E', 'LE', '/', '2', ',', '\\', 'n', '</s>']
Filtered   (014): ['part', 'ition', '_', 'size', '=', 'NUM', '_', 'E', 'LE', '/', '2', ',', '\\', 'n']
Detokenized (007): ['partition_size', '=', 'NUM_ELE', '/', '2', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "statusBarAx . barh ( [ 0 ] , [ 100.0 * expNumber / len ( data . getPaths ( ) ) ] , \n"
Original    (024): ['statusBarAx', '.', 'barh', '(', '[', '0', ']', ',', '[', '100.0', '*', 'expNumber', '/', 'len', '(', 'data', '.', 'getPaths', '(', ')', ')', ']', ',', '\\n']
Tokenized   (035): ['<s>', 'status', 'Bar', 'Ax', '.', 'bar', 'h', '(', '[', '0', ']', ',', '[', '100', '.', '0', '*', 'exp', 'Number', '/', 'len', '(', 'data', '.', 'get', 'Path', 's', '(', ')', ')', ']', ',', '\\', 'n', '</s>']
Filtered   (033): ['status', 'Bar', 'Ax', '.', 'bar', 'h', '(', '[', '0', ']', ',', '[', '100', '.', '0', '*', 'exp', 'Number', '/', 'len', '(', 'data', '.', 'get', 'Path', 's', '(', ')', ')', ']', ',', '\\', 'n']
Detokenized (024): ['statusBarAx', '.', 'barh', '(', '[', '0', ']', ',', '[', '100.0', '*', 'expNumber', '/', 'len', '(', 'data', '.', 'getPaths', '(', ')', ')', ']', ',', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "fluxes , errors , photFlags = photometry . multirad ( image , x , y , \n"
Original    (017): ['fluxes', ',', 'errors', ',', 'photFlags', '=', 'photometry', '.', 'multirad', '(', 'image', ',', 'x', ',', 'y', ',', '\\n']
Tokenized   (026): ['<s>', 'f', 'lux', 'es', ',', 'errors', ',', 'phot', 'Flags', '=', 'phot', 'ometry', '.', 'mult', 'ir', 'ad', '(', 'image', ',', 'x', ',', 'y', ',', '\\', 'n', '</s>']
Filtered   (024): ['f', 'lux', 'es', ',', 'errors', ',', 'phot', 'Flags', '=', 'phot', 'ometry', '.', 'mult', 'ir', 'ad', '(', 'image', ',', 'x', ',', 'y', ',', '\\', 'n']
Detokenized (017): ['fluxes', ',', 'errors', ',', 'photFlags', '=', 'photometry', '.', 'multirad', '(', 'image', ',', 'x', ',', 'y', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "meanComparisonStars , meanComparisonStarErrors = data . calcMeanComparison_multirad ( ccdGain = data . ccdGain ) \n"
Original    (015): ['meanComparisonStars', ',', 'meanComparisonStarErrors', '=', 'data', '.', 'calcMeanComparison_multirad', '(', 'ccdGain', '=', 'data', '.', 'ccdGain', ')', '\\n']
Tokenized   (040): ['<s>', 'mean', 'Compar', 'ison', 'Stars', ',', 'mean', 'Compar', 'ison', 'Star', 'Er', 'rors', '=', 'data', '.', 'calc', 'Me', 'an', 'Compar', 'ison', '_', 'mult', 'ir', 'ad', '(', 'c', 'cd', 'G', 'ain', '=', 'data', '.', 'c', 'cd', 'G', 'ain', ')', '\\', 'n', '</s>']
Filtered   (038): ['mean', 'Compar', 'ison', 'Stars', ',', 'mean', 'Compar', 'ison', 'Star', 'Er', 'rors', '=', 'data', '.', 'calc', 'Me', 'an', 'Compar', 'ison', '_', 'mult', 'ir', 'ad', '(', 'c', 'cd', 'G', 'ain', '=', 'data', '.', 'c', 'cd', 'G', 'ain', ')', '\\', 'n']
Detokenized (015): ['meanComparisonStars', ',', 'meanComparisonStarErrors', '=', 'data', '.', 'calcMeanComparison_multirad', '(', 'ccdGain', '=', 'data', '.', 'ccdGain', ')', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "lightCurves , lightCurveErrors = data . computeLightCurve_multirad ( meanComparisonStars , \n"
Original    (011): ['lightCurves', ',', 'lightCurveErrors', '=', 'data', '.', 'computeLightCurve_multirad', '(', 'meanComparisonStars', ',', '\\n']
Tokenized   (030): ['<s>', 'light', 'Cur', 'ves', ',', 'light', 'Cur', 've', 'Er', 'rors', '=', 'data', '.', 'compute', 'Light', 'Cur', 've', '_', 'mult', 'ir', 'ad', '(', 'mean', 'Compar', 'ison', 'Stars', ',', '\\', 'n', '</s>']
Filtered   (028): ['light', 'Cur', 'ves', ',', 'light', 'Cur', 've', 'Er', 'rors', '=', 'data', '.', 'compute', 'Light', 'Cur', 've', '_', 'mult', 'ir', 'ad', '(', 'mean', 'Compar', 'ison', 'Stars', ',', '\\', 'n']
Detokenized (011): ['lightCurves', ',', 'lightCurveErrors', '=', 'data', '.', 'computeLightCurve_multirad', '(', 'meanComparisonStars', ',', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "json_data = self . _send_request ( , url , params = params ) \n"
Original    (014): ['json_data', '=', 'self', '.', '_send_request', '(', ',', 'url', ',', 'params', '=', 'params', ')', '\\n']
Tokenized   (022): ['<s>', 'json', '_', 'data', '=', 'self', '.', '_', 'send', '_', 'request', '(', ',', 'url', ',', 'params', '=', 'params', ')', '\\', 'n', '</s>']
Filtered   (020): ['json', '_', 'data', '=', 'self', '.', '_', 'send', '_', 'request', '(', ',', 'url', ',', 'params', '=', 'params', ')', '\\', 'n']
Detokenized (014): ['json_data', '=', 'self', '.', '_send_request', '(', ',', 'url', ',', 'params', '=', 'params', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "mkdir ( env . hosts_data . log_path ( ) ) \n"
Original    (011): ['mkdir', '(', 'env', '.', 'hosts_data', '.', 'log_path', '(', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'mk', 'dir', '(', 'env', '.', 'hosts', '_', 'data', '.', 'log', '_', 'path', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['mk', 'dir', '(', 'env', '.', 'hosts', '_', 'data', '.', 'log', '_', 'path', '(', ')', ')', '\\', 'n']
Detokenized (011): ['mkdir', '(', 'env', '.', 'hosts_data', '.', 'log_path', '(', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "StringIO ( env . hosts_data . celery_supervisor_config ( ) ) , \n"
Original    (012): ['StringIO', '(', 'env', '.', 'hosts_data', '.', 'celery_supervisor_config', '(', ')', ')', ',', '\\n']
Tokenized   (024): ['<s>', 'String', 'IO', '(', 'env', '.', 'hosts', '_', 'data', '.', 'cel', 'ery', '_', 'super', 'visor', '_', 'config', '(', ')', ')', ',', '\\', 'n', '</s>']
Filtered   (022): ['String', 'IO', '(', 'env', '.', 'hosts', '_', 'data', '.', 'cel', 'ery', '_', 'super', 'visor', '_', 'config', '(', ')', ')', ',', '\\', 'n']
Detokenized (012): ['StringIO', '(', 'env', '.', 'hosts_data', '.', 'celery_supervisor_config', '(', ')', ')', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "env . hosts_data . celery_supervisor_config_path ( ) , \n"
Original    (009): ['env', '.', 'hosts_data', '.', 'celery_supervisor_config_path', '(', ')', ',', '\\n']
Tokenized   (022): ['<s>', 'env', '.', 'hosts', '_', 'data', '.', 'cel', 'ery', '_', 'super', 'visor', '_', 'config', '_', 'path', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (020): ['env', '.', 'hosts', '_', 'data', '.', 'cel', 'ery', '_', 'super', 'visor', '_', 'config', '_', 'path', '(', ')', ',', '\\', 'n']
Detokenized (009): ['env', '.', 'hosts_data', '.', 'celery_supervisor_config_path', '(', ')', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "rmdir ( env . hosts_data . celery_supervisor_config_path ( ) , sudo_access = True ) \n"
Original    (015): ['rmdir', '(', 'env', '.', 'hosts_data', '.', 'celery_supervisor_config_path', '(', ')', ',', 'sudo_access', '=', 'True', ')', '\\n']
Tokenized   (032): ['<s>', 'r', 'md', 'ir', '(', 'env', '.', 'hosts', '_', 'data', '.', 'cel', 'ery', '_', 'super', 'visor', '_', 'config', '_', 'path', '(', ')', ',', 'sudo', '_', 'access', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (030): ['r', 'md', 'ir', '(', 'env', '.', 'hosts', '_', 'data', '.', 'cel', 'ery', '_', 'super', 'visor', '_', 'config', '_', 'path', '(', ')', ',', 'sudo', '_', 'access', '=', 'True', ')', '\\', 'n']
Detokenized (015): ['rmdir', '(', 'env', '.', 'hosts_data', '.', 'celery_supervisor_config_path', '(', ')', ',', 'sudo_access', '=', 'True', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "sudo ( . format ( env . hosts_data . application_name ( ) ) ) \n"
Original    (015): ['sudo', '(', '.', 'format', '(', 'env', '.', 'hosts_data', '.', 'application_name', '(', ')', ')', ')', '\\n']
Tokenized   (022): ['<s>', 'sudo', '(', '.', 'format', '(', 'env', '.', 'hosts', '_', 'data', '.', 'application', '_', 'name', '(', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (020): ['sudo', '(', '.', 'format', '(', 'env', '.', 'hosts', '_', 'data', '.', 'application', '_', 'name', '(', ')', ')', ')', '\\', 'n']
Detokenized (015): ['sudo', '(', '.', 'format', '(', 'env', '.', 'hosts_data', '.', 'application_name', '(', ')', ')', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "collection_response = SharedCollectionResponse ( json . loads ( self . send ( ) . content ) ) \n"
Original    (018): ['collection_response', '=', 'SharedCollectionResponse', '(', 'json', '.', 'loads', '(', 'self', '.', 'send', '(', ')', '.', 'content', ')', ')', '\\n']
Tokenized   (025): ['<s>', 'collection', '_', 'response', '=', 'Shared', 'Collection', 'Response', '(', 'json', '.', 'loads', '(', 'self', '.', 'send', '(', ')', '.', 'content', ')', ')', '\\', 'n', '</s>']
Filtered   (023): ['collection', '_', 'response', '=', 'Shared', 'Collection', 'Response', '(', 'json', '.', 'loads', '(', 'self', '.', 'send', '(', ')', '.', 'content', ')', ')', '\\', 'n']
Detokenized (018): ['collection_response', '=', 'SharedCollectionResponse', '(', 'json', '.', 'loads', '(', 'self', '.', 'send', '(', ')', '.', 'content', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "libraries = [ "sodium" ] , \n"
Original    (007): ['libraries', '=', '[', '"sodium"', ']', ',', '\\n']
Tokenized   (014): ['<s>', 'l', 'ibraries', '=', '[', '"', 's', 'odium', '"', ']', ',', '\\', 'n', '</s>']
Filtered   (012): ['l', 'ibraries', '=', '[', '"', 's', 'odium', '"', ']', ',', '\\', 'n']
Detokenized (007): ['libraries', '=', '[', '"sodium"', ']', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "number = 2 , type = 12 , cpp_type = 9 , label = 2 , \n"
Original    (017): ['number', '=', '2', ',', 'type', '=', '12', ',', 'cpp_type', '=', '9', ',', 'label', '=', '2', ',', '\\n']
Tokenized   (023): ['<s>', 'number', '=', '2', ',', 'type', '=', '12', ',', 'c', 'pp', '_', 'type', '=', '9', ',', 'label', '=', '2', ',', '\\', 'n', '</s>']
Filtered   (021): ['number', '=', '2', ',', 'type', '=', '12', ',', 'c', 'pp', '_', 'type', '=', '9', ',', 'label', '=', '2', ',', '\\', 'n']
Detokenized (017): ['number', '=', '2', ',', 'type', '=', '12', ',', 'cpp_type', '=', '9', ',', 'label', '=', '2', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "has_default_value = False , default_value = _b ( "" ) , \n"
Original    (012): ['has_default_value', '=', 'False', ',', 'default_value', '=', '_b', '(', '""', ')', ',', '\\n']
Tokenized   (022): ['<s>', 'has', '_', 'default', '_', 'value', '=', 'False', ',', 'default', '_', 'value', '=', '_', 'b', '(', '""', ')', ',', '\\', 'n', '</s>']
Filtered   (020): ['has', '_', 'default', '_', 'value', '=', 'False', ',', 'default', '_', 'value', '=', '_', 'b', '(', '""', ')', ',', '\\', 'n']
Detokenized (012): ['has_default_value', '=', 'False', ',', 'default_value', '=', '_b', '(', '""', ')', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "PeerSeeds = _reflection . GeneratedProtocolMessageType ( , ( _message . Message , ) , dict ( \n"
Original    (017): ['PeerSeeds', '=', '_reflection', '.', 'GeneratedProtocolMessageType', '(', ',', '(', '_message', '.', 'Message', ',', ')', ',', 'dict', '(', '\\n']
Tokenized   (031): ['<s>', 'Pe', 'er', 'S', 'eeds', '=', '_', 'ref', 'lection', '.', 'Gener', 'ated', 'Prot', 'ocol', 'Message', 'Type', '(', ',', '(', '_', 'message', '.', 'Message', ',', ')', ',', 'dict', '(', '\\', 'n', '</s>']
Filtered   (029): ['Pe', 'er', 'S', 'eeds', '=', '_', 'ref', 'lection', '.', 'Gener', 'ated', 'Prot', 'ocol', 'Message', 'Type', '(', ',', '(', '_', 'message', '.', 'Message', ',', ')', ',', 'dict', '(', '\\', 'n']
Detokenized (017): ['PeerSeeds', '=', '_reflection', '.', 'GeneratedProtocolMessageType', '(', ',', '(', '_message', '.', 'Message', ',', ')', ',', 'dict', '(', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "__module__ = \n"
Original    (003): ['__module__', '=', '\\n']
Tokenized   (008): ['<s>', '__', 'module', '__', '=', '\\', 'n', '</s>']
Filtered   (006): ['__', 'module', '__', '=', '\\', 'n']
Detokenized (003): ['__module__', '=', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "tstream = BytearrayStream ( istream . read ( self . length ) ) \n"
Original    (014): ['tstream', '=', 'BytearrayStream', '(', 'istream', '.', 'read', '(', 'self', '.', 'length', ')', ')', '\\n']
Tokenized   (024): ['<s>', 't', 'stream', '=', 'By', 't', 'ear', 'ray', 'Stream', '(', 'is', 't', 'ream', '.', 'read', '(', 'self', '.', 'length', ')', ')', '\\', 'n', '</s>']
Filtered   (022): ['t', 'stream', '=', 'By', 't', 'ear', 'ray', 'Stream', '(', 'is', 't', 'ream', '.', 'read', '(', 'self', '.', 'length', ')', ')', '\\', 'n']
Detokenized (014): ['tstream', '=', 'BytearrayStream', '(', 'istream', '.', 'read', '(', 'self', '.', 'length', ')', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "opts , args = parser . parse_args ( sys . argv [ 1 : ] ) \n"
Original    (017): ['opts', ',', 'args', '=', 'parser', '.', 'parse_args', '(', 'sys', '.', 'argv', '[', '1', ':', ']', ')', '\\n']
Tokenized   (024): ['<s>', 'op', 'ts', ',', 'args', '=', 'parser', '.', 'parse', '_', 'args', '(', 'sys', '.', 'arg', 'v', '[', '1', ':', ']', ')', '\\', 'n', '</s>']
Filtered   (022): ['op', 'ts', ',', 'args', '=', 'parser', '.', 'parse', '_', 'args', '(', 'sys', '.', 'arg', 'v', '[', '1', ':', ']', ')', '\\', 'n']
Detokenized (017): ['opts', ',', 'args', '=', 'parser', '.', 'parse_args', '(', 'sys', '.', 'argv', '[', '1', ':', ']', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : ""{0}" . format ( uid ) ) \n"
Original    (008): ['"{0}"', '.', 'format', '(', 'uid', ')', ')', '\\n']
Tokenized   (015): ['<s>', '"', '{', '0', '}"', '.', 'format', '(', 'u', 'id', ')', ')', '\\', 'n', '</s>']
Filtered   (013): ['"', '{', '0', '}"', '.', 'format', '(', 'u', 'id', ')', ')', '\\', 'n']
Detokenized (008): ['"{0}"', '.', 'format', '(', 'uid', ')', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : ""located." . format ( path ) \n"
Original    (007): ['"located."', '.', 'format', '(', 'path', ')', '\\n']
Tokenized   (013): ['<s>', '"', 'l', 'ocated', '."', '.', 'format', '(', 'path', ')', '\\', 'n', '</s>']
Filtered   (011): ['"', 'l', 'ocated', '."', '.', 'format', '(', 'path', ')', '\\', 'n']
Detokenized (007): ['"located."', '.', 'format', '(', 'path', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "discover_versions . DiscoverVersionsResponsePayload , ** kwargs ) \n"
Original    (008): ['discover_versions', '.', 'DiscoverVersionsResponsePayload', ',', '**', 'kwargs', ')', '\\n']
Tokenized   (020): ['<s>', 'd', 'iscover', '_', 'versions', '.', 'Discover', 'Versions', 'Response', 'Pay', 'load', ',', '**', 'k', 'w', 'args', ')', '\\', 'n', '</s>']
Filtered   (018): ['d', 'iscover', '_', 'versions', '.', 'Discover', 'Versions', 'Response', 'Pay', 'load', ',', '**', 'k', 'w', 'args', ')', '\\', 'n']
Detokenized (008): ['discover_versions', '.', 'DiscoverVersionsResponsePayload', ',', '**', 'kwargs', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "sqltypes . Base . metadata . create_all ( self . engine ) \n"
Original    (013): ['sqltypes', '.', 'Base', '.', 'metadata', '.', 'create_all', '(', 'self', '.', 'engine', ')', '\\n']
Tokenized   (019): ['<s>', 'sql', 'types', '.', 'Base', '.', 'metadata', '.', 'create', '_', 'all', '(', 'self', '.', 'engine', ')', '\\', 'n', '</s>']
Filtered   (017): ['sql', 'types', '.', 'Base', '.', 'metadata', '.', 'create', '_', 'all', '(', 'self', '.', 'engine', ')', '\\', 'n']
Detokenized (013): ['sqltypes', '.', 'Base', '.', 'metadata', '.', 'create_all', '(', 'self', '.', 'engine', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "enums . OpaqueDataType . NONE , \n"
Original    (007): ['enums', '.', 'OpaqueDataType', '.', 'NONE', ',', '\\n']
Tokenized   (015): ['<s>', 'en', 'ums', '.', 'Op', 'aque', 'Data', 'Type', '.', 'N', 'ONE', ',', '\\', 'n', '</s>']
Filtered   (013): ['en', 'ums', '.', 'Op', 'aque', 'Data', 'Type', '.', 'N', 'ONE', ',', '\\', 'n']
Detokenized (007): ['enums', '.', 'OpaqueDataType', '.', 'NONE', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "binascii . hexlify ( self . bytes_a ) , enums . OpaqueDataType . NONE ) \n"
Original    (016): ['binascii', '.', 'hexlify', '(', 'self', '.', 'bytes_a', ')', ',', 'enums', '.', 'OpaqueDataType', '.', 'NONE', ')', '\\n']
Tokenized   (031): ['<s>', 'bin', 'as', 'ci', 'i', '.', 'hex', 'l', 'ify', '(', 'self', '.', 'bytes', '_', 'a', ')', ',', 'en', 'ums', '.', 'Op', 'aque', 'Data', 'Type', '.', 'N', 'ONE', ')', '\\', 'n', '</s>']
Filtered   (029): ['bin', 'as', 'ci', 'i', '.', 'hex', 'l', 'ify', '(', 'self', '.', 'bytes', '_', 'a', ')', ',', 'en', 'ums', '.', 'Op', 'aque', 'Data', 'Type', '.', 'N', 'ONE', ')', '\\', 'n']
Detokenized (016): ['binascii', '.', 'hexlify', '(', 'self', '.', 'bytes_a', ')', ',', 'enums', '.', 'OpaqueDataType', '.', 'NONE', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "Session = sessionmaker ( bind = self . engine ) \n"
Original    (011): ['Session', '=', 'sessionmaker', '(', 'bind', '=', 'self', '.', 'engine', ')', '\\n']
Tokenized   (015): ['<s>', 'Session', '=', 'session', 'maker', '(', 'bind', '=', 'self', '.', 'engine', ')', '\\', 'n', '</s>']
Filtered   (013): ['Session', '=', 'session', 'maker', '(', 'bind', '=', 'self', '.', 'engine', ')', '\\', 'n']
Detokenized (011): ['Session', '=', 'sessionmaker', '(', 'bind', '=', 'self', '.', 'engine', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "get_obj = session . query ( OpaqueObject ) . filter ( \n"
Original    (012): ['get_obj', '=', 'session', '.', 'query', '(', 'OpaqueObject', ')', '.', 'filter', '(', '\\n']
Tokenized   (019): ['<s>', 'get', '_', 'obj', '=', 'session', '.', 'query', '(', 'Op', 'aque', 'Object', ')', '.', 'filter', '(', '\\', 'n', '</s>']
Filtered   (017): ['get', '_', 'obj', '=', 'session', '.', 'query', '(', 'Op', 'aque', 'Object', ')', '.', 'filter', '(', '\\', 'n']
Detokenized (012): ['get_obj', '=', 'session', '.', 'query', '(', 'OpaqueObject', ')', '.', 'filter', '(', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "ManagedObject . unique_identifier == obj . unique_identifier \n"
Original    (008): ['ManagedObject', '.', 'unique_identifier', '==', 'obj', '.', 'unique_identifier', '\\n']
Tokenized   (019): ['<s>', 'Man', 'aged', 'Object', '.', 'unique', '_', 'ident', 'ifier', '==', 'obj', '.', 'unique', '_', 'ident', 'ifier', '\\', 'n', '</s>']
Filtered   (017): ['Man', 'aged', 'Object', '.', 'unique', '_', 'ident', 'ifier', '==', 'obj', '.', 'unique', '_', 'ident', 'ifier', '\\', 'n']
Detokenized (008): ['ManagedObject', '.', 'unique_identifier', '==', 'obj', '.', 'unique_identifier', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "0 ) ) \n"
Original    (004): ['0', ')', ')', '\\n']
Tokenized   (007): ['<s>', '0', ')', ')', '\\', 'n', '</s>']
Filtered   (005): ['0', ')', ')', '\\', 'n']
Detokenized (004): ['0', ')', ')', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "expected_mo_names . append ( sqltypes . ManagedObjectName ( expected_names [ 1 ] , \n"
Original    (014): ['expected_mo_names', '.', 'append', '(', 'sqltypes', '.', 'ManagedObjectName', '(', 'expected_names', '[', '1', ']', ',', '\\n']
Tokenized   (027): ['<s>', 'expected', '_', 'mo', '_', 'names', '.', 'append', '(', 'sql', 'types', '.', 'Man', 'aged', 'Object', 'Name', '(', 'expected', '_', 'names', '[', '1', ']', ',', '\\', 'n', '</s>']
Filtered   (025): ['expected', '_', 'mo', '_', 'names', '.', 'append', '(', 'sql', 'types', '.', 'Man', 'aged', 'Object', 'Name', '(', 'expected', '_', 'names', '[', '1', ']', ',', '\\', 'n']
Detokenized (014): ['expected_mo_names', '.', 'append', '(', 'sqltypes', '.', 'ManagedObjectName', '(', 'expected_names', '[', '1', ']', ',', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "expected_names = [ first_name , added_name ] \n"
Original    (008): ['expected_names', '=', '[', 'first_name', ',', 'added_name', ']', '\\n']
Tokenized   (017): ['<s>', 'expected', '_', 'names', '=', '[', 'first', '_', 'name', ',', 'added', '_', 'name', ']', '\\', 'n', '</s>']
Filtered   (015): ['expected', '_', 'names', '=', '[', 'first', '_', 'name', ',', 'added', '_', 'name', ']', '\\', 'n']
Detokenized (008): ['expected_names', '=', '[', 'first_name', ',', 'added_name', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "] } , \n"
Original    (004): [']', '}', ',', '\\n']
Tokenized   (007): ['<s>', ']', '}', ',', '\\', 'n', '</s>']
Filtered   (005): [']', '}', ',', '\\', 'n']
Detokenized (004): [']', '}', ',', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "types . MethodType ( _lib_dir_option , None , MSVCCompiler ) ) \n"
Original    (012): ['types', '.', 'MethodType', '(', '_lib_dir_option', ',', 'None', ',', 'MSVCCompiler', ')', ')', '\\n']
Tokenized   (025): ['<s>', 'types', '.', 'Method', 'Type', '(', '_', 'lib', '_', 'dir', '_', 'option', ',', 'None', ',', 'MS', 'V', 'CC', 'omp', 'iler', ')', ')', '\\', 'n', '</s>']
Filtered   (023): ['types', '.', 'Method', 'Type', '(', '_', 'lib', '_', 'dir', '_', 'option', ',', 'None', ',', 'MS', 'V', 'CC', 'omp', 'iler', ')', ')', '\\', 'n']
Detokenized (012): ['types', '.', 'MethodType', '(', '_lib_dir_option', ',', 'None', ',', 'MSVCCompiler', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "setup ( ** kwds ) \n"
Original    (006): ['setup', '(', '**', 'kwds', ')', '\\n']
Tokenized   (011): ['<s>', 'setup', '(', '**', 'k', 'w', 'ds', ')', '\\', 'n', '</s>']
Filtered   (009): ['setup', '(', '**', 'k', 'w', 'ds', ')', '\\', 'n']
Detokenized (006): ['setup', '(', '**', 'kwds', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "intersphinx_mapping = { : None } \n"
Original    (007): ['intersphinx_mapping', '=', '{', ':', 'None', '}', '\\n']
Tokenized   (015): ['<s>', 'inters', 'ph', 'inx', '_', 'm', 'apping', '=', '{', ':', 'None', '}', '\\', 'n', '</s>']
Filtered   (013): ['inters', 'ph', 'inx', '_', 'm', 'apping', '=', '{', ':', 'None', '}', '\\', 'n']
Detokenized (007): ['intersphinx_mapping', '=', '{', ':', 'None', '}', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "new_w = int ( width * wrat ) \n"
Original    (009): ['new_w', '=', 'int', '(', 'width', '*', 'wrat', ')', '\\n']
Tokenized   (015): ['<s>', 'new', '_', 'w', '=', 'int', '(', 'width', '*', 'wr', 'at', ')', '\\', 'n', '</s>']
Filtered   (013): ['new', '_', 'w', '=', 'int', '(', 'width', '*', 'wr', 'at', ')', '\\', 'n']
Detokenized (009): ['new_w', '=', 'int', '(', 'width', '*', 'wrat', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "im . getbbox ( ) , Image . BICUBIC ) \n"
Original    (011): ['im', '.', 'getbbox', '(', ')', ',', 'Image', '.', 'BICUBIC', ')', '\\n']
Tokenized   (019): ['<s>', 'im', '.', 'get', 'b', 'box', '(', ')', ',', 'Image', '.', 'B', 'IC', 'UB', 'IC', ')', '\\', 'n', '</s>']
Filtered   (017): ['im', '.', 'get', 'b', 'box', '(', ')', ',', 'Image', '.', 'B', 'IC', 'UB', 'IC', ')', '\\', 'n']
Detokenized (011): ['im', '.', 'getbbox', '(', ')', ',', 'Image', '.', 'BICUBIC', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "resize_image ( os . path . abspath ( os . path . join ( , dest + ) ) ) \n"
Original    (021): ['resize_image', '(', 'os', '.', 'path', '.', 'abspath', '(', 'os', '.', 'path', '.', 'join', '(', ',', 'dest', '+', ')', ')', ')', '\\n']
Tokenized   (028): ['<s>', 'res', 'ize', '_', 'image', '(', 'os', '.', 'path', '.', 'abs', 'path', '(', 'os', '.', 'path', '.', 'join', '(', ',', 'dest', '+', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (026): ['res', 'ize', '_', 'image', '(', 'os', '.', 'path', '.', 'abs', 'path', '(', 'os', '.', 'path', '.', 'join', '(', ',', 'dest', '+', ')', ')', ')', '\\', 'n']
Detokenized (021): ['resize_image', '(', 'os', '.', 'path', '.', 'abspath', '(', 'os', '.', 'path', '.', 'join', '(', ',', 'dest', '+', ')', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "f_x = Float ( 0.0 , iotype = "out" ) \n"
Original    (011): ['f_x', '=', 'Float', '(', '0.0', ',', 'iotype', '=', '"out"', ')', '\\n']
Tokenized   (021): ['<s>', 'f', '_', 'x', '=', 'Float', '(', '0', '.', '0', ',', 'i', 'otype', '=', '"', 'out', '"', ')', '\\', 'n', '</s>']
Filtered   (019): ['f', '_', 'x', '=', 'Float', '(', '0', '.', '0', ',', 'i', 'otype', '=', '"', 'out', '"', ')', '\\', 'n']
Detokenized (011): ['f_x', '=', 'Float', '(', '0.0', ',', 'iotype', '=', '"out"', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "doe_c = [ 0.1 , 0.2 , 0.3 , 0.5 , 0.7 , 0.8 , 0.9 ] + doe_e \n"
Original    (020): ['doe_c', '=', '[', '0.1', ',', '0.2', ',', '0.3', ',', '0.5', ',', '0.7', ',', '0.8', ',', '0.9', ']', '+', 'doe_e', '\\n']
Tokenized   (043): ['<s>', 'd', 'oe', '_', 'c', '=', '[', '0', '.', '1', ',', '0', '.', '2', ',', '0', '.', '3', ',', '0', '.', '5', ',', '0', '.', '7', ',', '0', '.', '8', ',', '0', '.', '9', ']', '+', 'do', 'e', '_', 'e', '\\', 'n', '</s>']
Filtered   (041): ['d', 'oe', '_', 'c', '=', '[', '0', '.', '1', ',', '0', '.', '2', ',', '0', '.', '3', ',', '0', '.', '5', ',', '0', '.', '7', ',', '0', '.', '8', ',', '0', '.', '9', ']', '+', 'do', 'e', '_', 'e', '\\', 'n']
Detokenized (020): ['doe_c', '=', '[', '0.1', ',', '0.2', ',', '0.3', ',', '0.5', ',', '0.7', ',', '0.8', ',', '0.9', ']', '+', 'doe_e', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "responses = ( , ) , nfi = self . nfi ) ) \n"
Original    (014): ['responses', '=', '(', ',', ')', ',', 'nfi', '=', 'self', '.', 'nfi', ')', ')', '\\n']
Tokenized   (020): ['<s>', 'respons', 'es', '=', '(', ',', ')', ',', 'n', 'fi', '=', 'self', '.', 'n', 'fi', ')', ')', '\\', 'n', '</s>']
Filtered   (018): ['respons', 'es', '=', '(', ',', ')', ',', 'n', 'fi', '=', 'self', '.', 'n', 'fi', ')', ')', '\\', 'n']
Detokenized (014): ['responses', '=', '(', ',', ')', ',', 'nfi', '=', 'self', '.', 'nfi', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "sigma_cok = np . array ( [ d . sigma for d in sim_cok . mm_checker . case_outputs . meta_model . f_x ] ) \n"
Original    (025): ['sigma_cok', '=', 'np', '.', 'array', '(', '[', 'd', '.', 'sigma', 'for', 'd', 'in', 'sim_cok', '.', 'mm_checker', '.', 'case_outputs', '.', 'meta_model', '.', 'f_x', ']', ')', '\\n']
Tokenized   (046): ['<s>', 's', 'igma', '_', 'c', 'ok', '=', 'np', '.', 'array', '(', '[', 'd', '.', 's', 'igma', 'for', 'd', 'in', 'sim', '_', 'c', 'ok', '.', 'mm', '_', 'check', 'er', '.', 'case', '_', 'output', 's', '.', 'meta', '_', 'model', '.', 'f', '_', 'x', ']', ')', '\\', 'n', '</s>']
Filtered   (044): ['s', 'igma', '_', 'c', 'ok', '=', 'np', '.', 'array', '(', '[', 'd', '.', 's', 'igma', 'for', 'd', 'in', 'sim', '_', 'c', 'ok', '.', 'mm', '_', 'check', 'er', '.', 'case', '_', 'output', 's', '.', 'meta', '_', 'model', '.', 'f', '_', 'x', ']', ')', '\\', 'n']
Detokenized (025): ['sigma_cok', '=', 'np', '.', 'array', '(', '[', 'd', '.', 'sigma', 'for', 'd', 'in', 'sim_cok', '.', 'mm_checker', '.', 'case_outputs', '.', 'meta_model', '.', 'f_x', ']', ')', '\\n']
Counter: 44
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "actual = sim_k . mm_checker . case_outputs . model . f_x \n"
Original    (012): ['actual', '=', 'sim_k', '.', 'mm_checker', '.', 'case_outputs', '.', 'model', '.', 'f_x', '\\n']
Tokenized   (025): ['<s>', 'actual', '=', 'sim', '_', 'k', '.', 'mm', '_', 'check', 'er', '.', 'case', '_', 'output', 's', '.', 'model', '.', 'f', '_', 'x', '\\', 'n', '</s>']
Filtered   (023): ['actual', '=', 'sim', '_', 'k', '.', 'mm', '_', 'check', 'er', '.', 'case', '_', 'output', 's', '.', 'model', '.', 'f', '_', 'x', '\\', 'n']
Detokenized (012): ['actual', '=', 'sim_k', '.', 'mm_checker', '.', 'case_outputs', '.', 'model', '.', 'f_x', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "predicted_cok - 2 * sigma_cok , facecolor = , alpha = 0.2 ) \n"
Original    (014): ['predicted_cok', '-', '2', '*', 'sigma_cok', ',', 'facecolor', '=', ',', 'alpha', '=', '0.2', ')', '\\n']
Tokenized   (028): ['<s>', 'pred', 'icted', '_', 'c', 'ok', '-', '2', '*', 's', 'igma', '_', 'c', 'ok', ',', 'face', 'color', '=', ',', 'alpha', '=', '0', '.', '2', ')', '\\', 'n', '</s>']
Filtered   (026): ['pred', 'icted', '_', 'c', 'ok', '-', '2', '*', 's', 'igma', '_', 'c', 'ok', ',', 'face', 'color', '=', ',', 'alpha', '=', '0', '.', '2', ')', '\\', 'n']
Detokenized (014): ['predicted_cok', '-', '2', '*', 'sigma_cok', ',', 'facecolor', '=', ',', 'alpha', '=', '0.2', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "newsetupfile = os . path . join ( os . path . dirname ( setupfile ) , \n"
Original    (018): ['newsetupfile', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dirname', '(', 'setupfile', ')', ',', '\\n']
Tokenized   (025): ['<s>', 'new', 'setup', 'file', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', 'setup', 'file', ')', ',', '\\', 'n', '</s>']
Filtered   (023): ['new', 'setup', 'file', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', 'setup', 'file', ')', ',', '\\', 'n']
Detokenized (018): ['newsetupfile', '=', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'dirname', '(', 'setupfile', ')', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "srcdir = os . path . abspath ( os . path . expanduser ( srcdir ) ) . replace ( , ) \n"
Original    (023): ['srcdir', '=', 'os', '.', 'path', '.', 'abspath', '(', 'os', '.', 'path', '.', 'expanduser', '(', 'srcdir', ')', ')', '.', 'replace', '(', ',', ')', '\\n']
Tokenized   (030): ['<s>', 'src', 'dir', '=', 'os', '.', 'path', '.', 'abs', 'path', '(', 'os', '.', 'path', '.', 'expand', 'user', '(', 'src', 'dir', ')', ')', '.', 'replace', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (028): ['src', 'dir', '=', 'os', '.', 'path', '.', 'abs', 'path', '(', 'os', '.', 'path', '.', 'expand', 'user', '(', 'src', 'dir', ')', ')', '.', 'replace', '(', ',', ')', '\\', 'n']
Detokenized (023): ['srcdir', '=', 'os', '.', 'path', '.', 'abspath', '(', 'os', '.', 'path', '.', 'expanduser', '(', 'srcdir', ')', ')', '.', 'replace', '(', ',', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "cmd . extend ( [ , destdir ] ) \n"
Original    (010): ['cmd', '.', 'extend', '(', '[', ',', 'destdir', ']', ')', '\\n']
Tokenized   (014): ['<s>', 'cmd', '.', 'extend', '(', '[', ',', 'dest', 'dir', ']', ')', '\\', 'n', '</s>']
Filtered   (012): ['cmd', '.', 'extend', '(', '[', ',', 'dest', 'dir', ']', ')', '\\', 'n']
Detokenized (010): ['cmd', '.', 'extend', '(', '[', ',', 'destdir', ']', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "list ( newfiles ) ) \n"
Original    (006): ['list', '(', 'newfiles', ')', ')', '\\n']
Tokenized   (010): ['<s>', 'list', '(', 'new', 'files', ')', ')', '\\', 'n', '</s>']
Filtered   (008): ['list', '(', 'new', 'files', ')', ')', '\\', 'n']
Detokenized (006): ['list', '(', 'newfiles', ')', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "destdir = os . path . abspath ( os . path . expanduser ( options . destdir ) ) \n"
Original    (020): ['destdir', '=', 'os', '.', 'path', '.', 'abspath', '(', 'os', '.', 'path', '.', 'expanduser', '(', 'options', '.', 'destdir', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'dest', 'dir', '=', 'os', '.', 'path', '.', 'abs', 'path', '(', 'os', '.', 'path', '.', 'expand', 'user', '(', 'options', '.', 'dest', 'dir', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['dest', 'dir', '=', 'os', '.', 'path', '.', 'abs', 'path', '(', 'os', '.', 'path', '.', 'expand', 'user', '(', 'options', '.', 'dest', 'dir', ')', ')', '\\', 'n']
Detokenized (020): ['destdir', '=', 'os', '.', 'path', '.', 'abspath', '(', 'os', '.', 'path', '.', 'expanduser', '(', 'options', '.', 'destdir', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "all_names . extend ( [ prefix + name \n"
Original    (009): ['all_names', '.', 'extend', '(', '[', 'prefix', '+', 'name', '\\n']
Tokenized   (014): ['<s>', 'all', '_', 'names', '.', 'extend', '(', '[', 'prefix', '+', 'name', '\\', 'n', '</s>']
Filtered   (012): ['all', '_', 'names', '.', 'extend', '(', '[', 'prefix', '+', 'name', '\\', 'n']
Detokenized (009): ['all_names', '.', 'extend', '(', '[', 'prefix', '+', 'name', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "lnames = [ prefix + rec for rec in driver [ ] ] \n"
Original    (014): ['lnames', '=', '[', 'prefix', '+', 'rec', 'for', 'rec', 'in', 'driver', '[', ']', ']', '\\n']
Tokenized   (018): ['<s>', 'l', 'names', '=', '[', 'prefix', '+', 'rec', 'for', 'rec', 'in', 'driver', '[', ']', ']', '\\', 'n', '</s>']
Filtered   (016): ['l', 'names', '=', '[', 'prefix', '+', 'rec', 'for', 'rec', 'in', 'driver', '[', ']', ']', '\\', 'n']
Detokenized (014): ['lnames', '=', '[', 'prefix', '+', 'rec', 'for', 'rec', 'in', 'driver', '[', ']', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "driver_grp = self . _inp [ ] [ driver_name ] \n"
Original    (011): ['driver_grp', '=', 'self', '.', '_inp', '[', ']', '[', 'driver_name', ']', '\\n']
Tokenized   (021): ['<s>', 'driver', '_', 'gr', 'p', '=', 'self', '.', '_', 'in', 'p', '[', ']', '[', 'driver', '_', 'name', ']', '\\', 'n', '</s>']
Filtered   (019): ['driver', '_', 'gr', 'p', '=', 'self', '.', '_', 'in', 'p', '[', ']', '[', 'driver', '_', 'name', ']', '\\', 'n']
Detokenized (011): ['driver_grp', '=', 'self', '.', '_inp', '[', ']', '[', 'driver_name', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "iteration_grp = self . _inp [ ] [ driver_name ] [ iteration_case_name ] \n"
Original    (014): ['iteration_grp', '=', 'self', '.', '_inp', '[', ']', '[', 'driver_name', ']', '[', 'iteration_case_name', ']', '\\n']
Tokenized   (029): ['<s>', 'iter', 'ation', '_', 'gr', 'p', '=', 'self', '.', '_', 'in', 'p', '[', ']', '[', 'driver', '_', 'name', ']', '[', 'iteration', '_', 'case', '_', 'name', ']', '\\', 'n', '</s>']
Filtered   (027): ['iter', 'ation', '_', 'gr', 'p', '=', 'self', '.', '_', 'in', 'p', '[', ']', '[', 'driver', '_', 'name', ']', '[', 'iteration', '_', 'case', '_', 'name', ']', '\\', 'n']
Detokenized (014): ['iteration_grp', '=', 'self', '.', '_inp', '[', ']', '[', 'driver_name', ']', '[', 'iteration_case_name', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "info = self . read_iteration_case_from_hdf5 ( self . _inp , driver_name , iteration_case_name ) yield info \n"
Original    (017): ['info', '=', 'self', '.', 'read_iteration_case_from_hdf5', '(', 'self', '.', '_inp', ',', 'driver_name', ',', 'iteration_case_name', ')', 'yield', 'info', '\\n']
Tokenized   (039): ['<s>', 'info', '=', 'self', '.', 'read', '_', 'iter', 'ation', '_', 'case', '_', 'from', '_', 'h', 'df', '5', '(', 'self', '.', '_', 'in', 'p', ',', 'driver', '_', 'name', ',', 'iteration', '_', 'case', '_', 'name', ')', 'yield', 'info', '\\', 'n', '</s>']
Filtered   (037): ['info', '=', 'self', '.', 'read', '_', 'iter', 'ation', '_', 'case', '_', 'from', '_', 'h', 'df', '5', '(', 'self', '.', '_', 'in', 'p', ',', 'driver', '_', 'name', ',', 'iteration', '_', 'case', '_', 'name', ')', 'yield', 'info', '\\', 'n']
Detokenized (017): ['info', '=', 'self', '.', 'read_iteration_case_from_hdf5', '(', 'self', '.', '_inp', ',', 'driver_name', ',', 'iteration_case_name', ')', 'yield', 'info', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "sleep_time = Float ( 0.0 , iotype = , desc = ) \n"
Original    (013): ['sleep_time', '=', 'Float', '(', '0.0', ',', 'iotype', '=', ',', 'desc', '=', ')', '\\n']
Tokenized   (021): ['<s>', 'sleep', '_', 'time', '=', 'Float', '(', '0', '.', '0', ',', 'i', 'otype', '=', ',', 'desc', '=', ')', '\\', 'n', '</s>']
Filtered   (019): ['sleep', '_', 'time', '=', 'Float', '(', '0', '.', '0', ',', 'i', 'otype', '=', ',', 'desc', '=', ')', '\\', 'n']
Detokenized (013): ['sleep_time', '=', 'Float', '(', '0.0', ',', 'iotype', '=', ',', 'desc', '=', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "accuracy = Float ( 1.0e-6 , iotype = , \n"
Original    (010): ['accuracy', '=', 'Float', '(', '1.0e-6', ',', 'iotype', '=', ',', '\\n']
Tokenized   (020): ['<s>', 'acc', 'uracy', '=', 'Float', '(', '1', '.', '0', 'e', '-', '6', ',', 'i', 'otype', '=', ',', '\\', 'n', '</s>']
Filtered   (018): ['acc', 'uracy', '=', 'Float', '(', '1', '.', '0', 'e', '-', '6', ',', 'i', 'otype', '=', ',', '\\', 'n']
Detokenized (010): ['accuracy', '=', 'Float', '(', '1.0e-6', ',', 'iotype', '=', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "iprint = Enum ( 0 , [ 0 , 1 , 2 , 3 ] , iotype = , \n"
Original    (020): ['iprint', '=', 'Enum', '(', '0', ',', '[', '0', ',', '1', ',', '2', ',', '3', ']', ',', 'iotype', '=', ',', '\\n']
Tokenized   (026): ['<s>', 'ip', 'rint', '=', 'En', 'um', '(', '0', ',', '[', '0', ',', '1', ',', '2', ',', '3', ']', ',', 'i', 'otype', '=', ',', '\\', 'n', '</s>']
Filtered   (024): ['ip', 'rint', '=', 'En', 'um', '(', '0', ',', '[', '0', ',', '1', ',', '2', ',', '3', ']', ',', 'i', 'otype', '=', ',', '\\', 'n']
Detokenized (020): ['iprint', '=', 'Enum', '(', '0', ',', '[', '0', ',', '1', ',', '2', ',', '3', ']', ',', 'iotype', '=', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "output_filename = Str ( , iotype = , \n"
Original    (009): ['output_filename', '=', 'Str', '(', ',', 'iotype', '=', ',', '\\n']
Tokenized   (015): ['<s>', 'output', '_', 'filename', '=', 'Str', '(', ',', 'i', 'otype', '=', ',', '\\', 'n', '</s>']
Filtered   (013): ['output', '_', 'filename', '=', 'Str', '(', ',', 'i', 'otype', '=', ',', '\\', 'n']
Detokenized (009): ['output_filename', '=', 'Str', '(', ',', 'iotype', '=', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "la = max ( m , 1 ) \n"
Original    (009): ['la', '=', 'max', '(', 'm', ',', '1', ')', '\\n']
Tokenized   (012): ['<s>', 'la', '=', 'max', '(', 'm', ',', '1', ')', '\\', 'n', '</s>']
Filtered   (010): ['la', '=', 'max', '(', 'm', ',', '1', ')', '\\', 'n']
Detokenized (009): ['la', '=', 'max', '(', 'm', ',', '1', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "gg = zeros ( [ la ] , ) \n"
Original    (010): ['gg', '=', 'zeros', '(', '[', 'la', ']', ',', ')', '\\n']
Tokenized   (014): ['<s>', 'gg', '=', 'z', 'eros', '(', '[', 'la', ']', ',', ')', '\\', 'n', '</s>']
Filtered   (012): ['gg', '=', 'z', 'eros', '(', '[', 'la', ']', ',', ')', '\\', 'n']
Detokenized (010): ['gg', '=', 'zeros', '(', '[', 'la', ']', ',', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "dg = zeros ( [ la , n + 1 ] , ) \n"
Original    (014): ['dg', '=', 'zeros', '(', '[', 'la', ',', 'n', '+', '1', ']', ',', ')', '\\n']
Tokenized   (019): ['<s>', 'd', 'g', '=', 'z', 'eros', '(', '[', 'la', ',', 'n', '+', '1', ']', ',', ')', '\\', 'n', '</s>']
Filtered   (017): ['d', 'g', '=', 'z', 'eros', '(', '[', 'la', ',', 'n', '+', '1', ']', ',', ')', '\\', 'n']
Detokenized (014): ['dg', '=', 'zeros', '(', '[', 'la', ',', 'n', '+', '1', ']', ',', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "mineq = m - meq + 2 * ( n + 1 ) \n"
Original    (014): ['mineq', '=', 'm', '-', 'meq', '+', '2', '*', '(', 'n', '+', '1', ')', '\\n']
Tokenized   (019): ['<s>', 'mine', 'q', '=', 'm', '-', 'me', 'q', '+', '2', '*', '(', 'n', '+', '1', ')', '\\', 'n', '</s>']
Filtered   (017): ['mine', 'q', '=', 'm', '-', 'me', 'q', '+', '2', '*', '(', 'n', '+', '1', ')', '\\', 'n']
Detokenized (014): ['mineq', '=', 'm', '-', 'meq', '+', '2', '*', '(', 'n', '+', '1', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "lsq = ( n + 1 ) * ( ( n + 1 ) + 1 ) + meq * ( ( n + 1 ) + 1 ) + mineq * ( ( n + 1 ) + 1 ) \n"
Original    (042): ['lsq', '=', '(', 'n', '+', '1', ')', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'meq', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'mineq', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '\\n']
Tokenized   (048): ['<s>', 'ls', 'q', '=', '(', 'n', '+', '1', ')', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'me', 'q', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'mine', 'q', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '\\', 'n', '</s>']
Filtered   (046): ['ls', 'q', '=', '(', 'n', '+', '1', ')', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'me', 'q', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'mine', 'q', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '\\', 'n']
Detokenized (042): ['lsq', '=', '(', 'n', '+', '1', ')', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'meq', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '+', 'mineq', '*', '(', '(', 'n', '+', '1', ')', '+', '1', ')', '\\n']
Counter: 46
===================================================================
Hidden states:  (13, 42, 768)
# Extracted words:  42
Sentence         : "lsi = ( ( n + 1 ) - meq + 1 ) * ( mineq + 2 ) + 2 * mineq \n"
Original    (024): ['lsi', '=', '(', '(', 'n', '+', '1', ')', '-', 'meq', '+', '1', ')', '*', '(', 'mineq', '+', '2', ')', '+', '2', '*', 'mineq', '\\n']
Tokenized   (031): ['<s>', 'ls', 'i', '=', '(', '(', 'n', '+', '1', ')', '-', 'me', 'q', '+', '1', ')', '*', '(', 'mine', 'q', '+', '2', ')', '+', '2', '*', 'mine', 'q', '\\', 'n', '</s>']
Filtered   (029): ['ls', 'i', '=', '(', '(', 'n', '+', '1', ')', '-', 'me', 'q', '+', '1', ')', '*', '(', 'mine', 'q', '+', '2', ')', '+', '2', '*', 'mine', 'q', '\\', 'n']
Detokenized (024): ['lsi', '=', '(', '(', 'n', '+', '1', ')', '-', 'meq', '+', '1', ')', '*', '(', 'mineq', '+', '2', ')', '+', '2', '*', 'mineq', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "lsei = ( ( n + 1 ) + mineq ) * ( ( n + 1 ) - meq ) + 2 * meq + ( n + 1 ) \n"
Original    (032): ['lsei', '=', '(', '(', 'n', '+', '1', ')', '+', 'mineq', ')', '*', '(', '(', 'n', '+', '1', ')', '-', 'meq', ')', '+', '2', '*', 'meq', '+', '(', 'n', '+', '1', ')', '\\n']
Tokenized   (039): ['<s>', 'l', 'sei', '=', '(', '(', 'n', '+', '1', ')', '+', 'mine', 'q', ')', '*', '(', '(', 'n', '+', '1', ')', '-', 'me', 'q', ')', '+', '2', '*', 'me', 'q', '+', '(', 'n', '+', '1', ')', '\\', 'n', '</s>']
Filtered   (037): ['l', 'sei', '=', '(', '(', 'n', '+', '1', ')', '+', 'mine', 'q', ')', '*', '(', '(', 'n', '+', '1', ')', '-', 'me', 'q', ')', '+', '2', '*', 'me', 'q', '+', '(', 'n', '+', '1', ')', '\\', 'n']
Detokenized (032): ['lsei', '=', '(', '(', 'n', '+', '1', ')', '+', 'mineq', ')', '*', '(', '(', 'n', '+', '1', ')', '-', 'meq', ')', '+', '2', '*', 'meq', '+', '(', 'n', '+', '1', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "slsqpb = ( n + 1 ) * ( n / 2 ) + 2 * m + 3 * n + 3 * ( n + 1 ) + 1 \n"
Original    (032): ['slsqpb', '=', '(', 'n', '+', '1', ')', '*', '(', 'n', '/', '2', ')', '+', '2', '*', 'm', '+', '3', '*', 'n', '+', '3', '*', '(', 'n', '+', '1', ')', '+', '1', '\\n']
Tokenized   (037): ['<s>', 'sl', 'sq', 'pb', '=', '(', 'n', '+', '1', ')', '*', '(', 'n', '/', '2', ')', '+', '2', '*', 'm', '+', '3', '*', 'n', '+', '3', '*', '(', 'n', '+', '1', ')', '+', '1', '\\', 'n', '</s>']
Filtered   (035): ['sl', 'sq', 'pb', '=', '(', 'n', '+', '1', ')', '*', '(', 'n', '/', '2', ')', '+', '2', '*', 'm', '+', '3', '*', 'n', '+', '3', '*', '(', 'n', '+', '1', ')', '+', '1', '\\', 'n']
Detokenized (032): ['slsqpb', '=', '(', 'n', '+', '1', ')', '*', '(', 'n', '/', '2', ')', '+', '2', '*', 'm', '+', '3', '*', 'n', '+', '3', '*', '(', 'n', '+', '1', ')', '+', '1', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "lw = lsq + lsi + lsei + slsqpb + n + m \n"
Original    (014): ['lw', '=', 'lsq', '+', 'lsi', '+', 'lsei', '+', 'slsqpb', '+', 'n', '+', 'm', '\\n']
Tokenized   (023): ['<s>', 'l', 'w', '=', 'l', 'sq', '+', 'l', 'si', '+', 'l', 'sei', '+', 'sl', 'sq', 'pb', '+', 'n', '+', 'm', '\\', 'n', '</s>']
Filtered   (021): ['l', 'w', '=', 'l', 'sq', '+', 'l', 'si', '+', 'l', 'sei', '+', 'sl', 'sq', 'pb', '+', 'n', '+', 'm', '\\', 'n']
Detokenized (014): ['lw', '=', 'lsq', '+', 'lsi', '+', 'lsei', '+', 'slsqpb', '+', 'n', '+', 'm', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "ljw = max ( mineq , ( n + 1 ) - meq ) \n"
Original    (015): ['ljw', '=', 'max', '(', 'mineq', ',', '(', 'n', '+', '1', ')', '-', 'meq', ')', '\\n']
Tokenized   (022): ['<s>', 'l', 'j', 'w', '=', 'max', '(', 'mine', 'q', ',', '(', 'n', '+', '1', ')', '-', 'me', 'q', ')', '\\', 'n', '</s>']
Filtered   (020): ['l', 'j', 'w', '=', 'max', '(', 'mine', 'q', ',', '(', 'n', '+', '1', ')', '-', 'me', 'q', ')', '\\', 'n']
Detokenized (015): ['ljw', '=', 'max', '(', 'mineq', ',', '(', 'n', '+', '1', ')', '-', 'meq', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "_iodict = { : , : } \n"
Original    (008): ['_iodict', '=', '{', ':', ',', ':', '}', '\\n']
Tokenized   (013): ['<s>', '_', 'iod', 'ict', '=', '{', ':', ',', ':', '}', '\\', 'n', '</s>']
Filtered   (011): ['_', 'iod', 'ict', '=', '{', ':', ',', ':', '}', '\\', 'n']
Detokenized (008): ['_iodict', '=', '{', ':', ',', ':', '}', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "state [ ] = { } \n"
Original    (007): ['state', '[', ']', '=', '{', '}', '\\n']
Tokenized   (010): ['<s>', 'state', '[', ']', '=', '{', '}', '\\', 'n', '</s>']
Filtered   (008): ['state', '[', ']', '=', '{', '}', '\\', 'n']
Detokenized (007): ['state', '[', ']', '=', '{', '}', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "key = ( addr_type , addr , proxy . _authkey ) \n"
Original    (012): ['key', '=', '(', 'addr_type', ',', 'addr', ',', 'proxy', '.', '_authkey', ')', '\\n']
Tokenized   (019): ['<s>', 'key', '=', '(', 'addr', '_', 'type', ',', 'addr', ',', 'proxy', '.', '_', 'auth', 'key', ')', '\\', 'n', '</s>']
Filtered   (017): ['key', '=', '(', 'addr', '_', 'type', ',', 'addr', ',', 'proxy', '.', '_', 'auth', 'key', ')', '\\', 'n']
Detokenized (012): ['key', '=', '(', 'addr_type', ',', 'addr', ',', 'proxy', '.', '_authkey', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "address = ( ip_addr , 0 ) \n"
Original    (008): ['address', '=', '(', 'ip_addr', ',', '0', ')', '\\n']
Tokenized   (013): ['<s>', 'address', '=', '(', 'ip', '_', 'addr', ',', '0', ')', '\\', 'n', '</s>']
Filtered   (011): ['address', '=', '(', 'ip', '_', 'addr', ',', '0', ')', '\\', 'n']
Detokenized (008): ['address', '=', '(', 'ip_addr', ',', '0', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "access = addr if addr_type == else addr_type \n"
Original    (009): ['access', '=', 'addr', 'if', 'addr_type', '==', 'else', 'addr_type', '\\n']
Tokenized   (016): ['<s>', 'access', '=', 'addr', 'if', 'addr', '_', 'type', '==', 'else', 'addr', '_', 'type', '\\', 'n', '</s>']
Filtered   (014): ['access', '=', 'addr', 'if', 'addr', '_', 'type', '==', 'else', 'addr', '_', 'type', '\\', 'n']
Detokenized (009): ['access', '=', 'addr', 'if', 'addr_type', '==', 'else', 'addr_type', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "manager = ObjectManager ( self , address , authkey = proxy . _authkey , \n"
Original    (015): ['manager', '=', 'ObjectManager', '(', 'self', ',', 'address', ',', 'authkey', '=', 'proxy', '.', '_authkey', ',', '\\n']
Tokenized   (022): ['<s>', 'manager', '=', 'Object', 'Manager', '(', 'self', ',', 'address', ',', 'auth', 'key', '=', 'proxy', '.', '_', 'auth', 'key', ',', '\\', 'n', '</s>']
Filtered   (020): ['manager', '=', 'Object', 'Manager', '(', 'self', ',', 'address', ',', 'auth', 'key', '=', 'proxy', '.', '_', 'auth', 'key', ',', '\\', 'n']
Detokenized (015): ['manager', '=', 'ObjectManager', '(', 'self', ',', 'address', ',', 'authkey', '=', 'proxy', '.', '_authkey', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "match_dict = self . _alltraits ( ** metadata ) \n"
Original    (010): ['match_dict', '=', 'self', '.', '_alltraits', '(', '**', 'metadata', ')', '\\n']
Tokenized   (018): ['<s>', 'match', '_', 'dict', '=', 'self', '.', '_', 'all', 'tra', 'its', '(', '**', 'metadata', ')', '\\', 'n', '</s>']
Filtered   (016): ['match', '_', 'dict', '=', 'self', '.', '_', 'all', 'tra', 'its', '(', '**', 'metadata', ')', '\\', 'n']
Detokenized (010): ['match_dict', '=', 'self', '.', '_alltraits', '(', '**', 'metadata', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "childname , _ , restofpath = traitpath . partition ( ) \n"
Original    (012): ['childname', ',', '_', ',', 'restofpath', '=', 'traitpath', '.', 'partition', '(', ')', '\\n']
Tokenized   (019): ['<s>', 'child', 'name', ',', '_', ',', 'rest', 'of', 'path', '=', 'trait', 'path', '.', 'partition', '(', ')', '\\', 'n', '</s>']
Filtered   (017): ['child', 'name', ',', '_', ',', 'rest', 'of', 'path', '=', 'trait', 'path', '.', 'partition', '(', ')', '\\', 'n']
Detokenized (012): ['childname', ',', '_', ',', 'restofpath', '=', 'traitpath', '.', 'partition', '(', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "mdict . setdefault ( , t . __class__ . __name__ ) \n"
Original    (012): ['mdict', '.', 'setdefault', '(', ',', 't', '.', '__class__', '.', '__name__', ')', '\\n']
Tokenized   (021): ['<s>', 'md', 'ict', '.', 'set', 'default', '(', ',', 't', '.', '__', 'class', '__', '.', '__', 'name', '__', ')', '\\', 'n', '</s>']
Filtered   (019): ['md', 'ict', '.', 'set', 'default', '(', ',', 't', '.', '__', 'class', '__', '.', '__', 'name', '__', ')', '\\', 'n']
Detokenized (012): ['mdict', '.', 'setdefault', '(', ',', 't', '.', '__class__', '.', '__name__', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "expr = compile ( assign , assign , mode = ) \n"
Original    (012): ['expr', '=', 'compile', '(', 'assign', ',', 'assign', ',', 'mode', '=', ')', '\\n']
Tokenized   (015): ['<s>', 'expr', '=', 'compile', '(', 'assign', ',', 'assign', ',', 'mode', '=', ')', '\\', 'n', '</s>']
Filtered   (013): ['expr', '=', 'compile', '(', 'assign', ',', 'assign', ',', 'mode', '=', ')', '\\', 'n']
Detokenized (012): ['expr', '=', 'compile', '(', 'assign', ',', 'assign', ',', 'mode', '=', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "tstamp = % ( now . year , now . month , now . day , now . hour , now . minute ) \n"
Original    (025): ['tstamp', '=', '%', '(', 'now', '.', 'year', ',', 'now', '.', 'month', ',', 'now', '.', 'day', ',', 'now', '.', 'hour', ',', 'now', '.', 'minute', ')', '\\n']
Tokenized   (030): ['<s>', 't', 'st', 'amp', '=', '%', '(', 'now', '.', 'year', ',', 'now', '.', 'month', ',', 'now', '.', 'day', ',', 'now', '.', 'hour', ',', 'now', '.', 'minute', ')', '\\', 'n', '</s>']
Filtered   (028): ['t', 'st', 'amp', '=', '%', '(', 'now', '.', 'year', ',', 'now', '.', 'month', ',', 'now', '.', 'day', ',', 'now', '.', 'hour', ',', 'now', '.', 'minute', ')', '\\', 'n']
Detokenized (025): ['tstamp', '=', '%', '(', 'now', '.', 'year', ',', 'now', '.', 'month', ',', 'now', '.', 'day', ',', 'now', '.', 'hour', ',', 'now', '.', 'minute', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "entry_pts = [ ( self , name , _get_entry_group ( self ) ) ] \n"
Original    (015): ['entry_pts', '=', '[', '(', 'self', ',', 'name', ',', '_get_entry_group', '(', 'self', ')', ')', ']', '\\n']
Tokenized   (026): ['<s>', 'entry', '_', 'pt', 's', '=', '[', '(', 'self', ',', 'name', ',', '_', 'get', '_', 'entry', '_', 'group', '(', 'self', ')', ')', ']', '\\', 'n', '</s>']
Filtered   (024): ['entry', '_', 'pt', 's', '=', '[', '(', 'self', ',', 'name', ',', '_', 'get', '_', 'entry', '_', 'group', '(', 'self', ')', ')', ']', '\\', 'n']
Detokenized (015): ['entry_pts', '=', '[', '(', 'self', ',', 'name', ',', '_get_entry_group', '(', 'self', ')', ')', ']', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "root_start = root_start + 1 if root_start >= 0 else 0 \n"
Original    (012): ['root_start', '=', 'root_start', '+', '1', 'if', 'root_start', '>=', '0', 'else', '0', '\\n']
Tokenized   (021): ['<s>', 'root', '_', 'start', '=', 'root', '_', 'start', '+', '1', 'if', 'root', '_', 'start', '>=', '0', 'else', '0', '\\', 'n', '</s>']
Filtered   (019): ['root', '_', 'start', '=', 'root', '_', 'start', '+', '1', 'if', 'root', '_', 'start', '>=', '0', 'else', '0', '\\', 'n']
Detokenized (012): ['root_start', '=', 'root_start', '+', '1', 'if', 'root_start', '>=', '0', 'else', '0', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "root_pathname += \n"
Original    (003): ['root_pathname', '+=', '\\n']
Tokenized   (009): ['<s>', 'root', '_', 'path', 'name', '+=', '\\', 'n', '</s>']
Filtered   (007): ['root', '_', 'path', 'name', '+=', '\\', 'n']
Detokenized (003): ['root_pathname', '+=', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "Container . _bases ( type ( obj ) , names ) \n"
Original    (012): ['Container', '.', '_bases', '(', 'type', '(', 'obj', ')', ',', 'names', ')', '\\n']
Tokenized   (017): ['<s>', 'Container', '.', '_', 'b', 'ases', '(', 'type', '(', 'obj', ')', ',', 'names', ')', '\\', 'n', '</s>']
Filtered   (015): ['Container', '.', '_', 'b', 'ases', '(', 'type', '(', 'obj', ')', ',', 'names', ')', '\\', 'n']
Detokenized (012): ['Container', '.', '_bases', '(', 'type', '(', 'obj', ')', ',', 'names', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "names . append ( % ( cls . __module__ , cls . __name__ ) ) \n"
Original    (016): ['names', '.', 'append', '(', '%', '(', 'cls', '.', '__module__', ',', 'cls', '.', '__name__', ')', ')', '\\n']
Tokenized   (025): ['<s>', 'names', '.', 'append', '(', '%', '(', 'cl', 's', '.', '__', 'module', '__', ',', 'cl', 's', '.', '__', 'name', '__', ')', ')', '\\', 'n', '</s>']
Filtered   (023): ['names', '.', 'append', '(', '%', '(', 'cl', 's', '.', '__', 'module', '__', ',', 'cl', 's', '.', '__', 'name', '__', ')', ')', '\\', 'n']
Detokenized (016): ['names', '.', 'append', '(', '%', '(', 'cls', '.', '__module__', ',', 'cls', '.', '__name__', ')', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "_get_entry_group . group_map = [ \n"
Original    (006): ['_get_entry_group', '.', 'group_map', '=', '[', '\\n']
Tokenized   (016): ['<s>', '_', 'get', '_', 'entry', '_', 'group', '.', 'group', '_', 'map', '=', '[', '\\', 'n', '</s>']
Filtered   (014): ['_', 'get', '_', 'entry', '_', 'group', '.', 'group', '_', 'map', '=', '[', '\\', 'n']
Detokenized (006): ['_get_entry_group', '.', 'group_map', '=', '[', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "pprint . pprint ( dict ( [ ( n , str ( v ) ) \n"
Original    (016): ['pprint', '.', 'pprint', '(', 'dict', '(', '[', '(', 'n', ',', 'str', '(', 'v', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'pp', 'rint', '.', 'p', 'print', '(', 'dict', '(', '[', '(', 'n', ',', 'str', '(', 'v', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['pp', 'rint', '.', 'p', 'print', '(', 'dict', '(', '[', '(', 'n', ',', 'str', '(', 'v', ')', ')', '\\', 'n']
Detokenized (016): ['pprint', '.', 'pprint', '(', 'dict', '(', '[', '(', 'n', ',', 'str', '(', 'v', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "** metadata ) ] ) , \n"
Original    (007): ['**', 'metadata', ')', ']', ')', ',', '\\n']
Tokenized   (010): ['<s>', '**', 'metadata', ')', ']', ')', ',', '\\', 'n', '</s>']
Filtered   (008): ['**', 'metadata', ')', ']', ')', ',', '\\', 'n']
Detokenized (007): ['**', 'metadata', ')', ']', ')', ',', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "io_attr [ ] = \n"
Original    (005): ['io_attr', '[', ']', '=', '\\n']
Tokenized   (010): ['<s>', 'io', '_', 'attr', '[', ']', '=', '\\', 'n', '</s>']
Filtered   (008): ['io', '_', 'attr', '[', ']', '=', '\\', 'n']
Detokenized (005): ['io_attr', '[', ']', '=', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "_redirect_streams ( ofile . fileno ( ) ) \n"
Original    (009): ['_redirect_streams', '(', 'ofile', '.', 'fileno', '(', ')', ')', '\\n']
Tokenized   (019): ['<s>', '_', 'red', 'irect', '_', 'stream', 's', '(', 'of', 'ile', '.', 'fil', 'eno', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['_', 'red', 'irect', '_', 'stream', 's', '(', 'of', 'ile', '.', 'fil', 'eno', '(', ')', ')', '\\', 'n']
Detokenized (009): ['_redirect_streams', '(', 'ofile', '.', 'fileno', '(', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "leftover = arr_size % num_divisions \n"
Original    (006): ['leftover', '=', 'arr_size', '%', 'num_divisions', '\\n']
Tokenized   (015): ['<s>', 'left', 'over', '=', 'arr', '_', 'size', '%', 'num', '_', 'div', 'isions', '\\', 'n', '</s>']
Filtered   (013): ['left', 'over', '=', 'arr', '_', 'size', '%', 'num', '_', 'div', 'isions', '\\', 'n']
Detokenized (006): ['leftover', '=', 'arr_size', '%', 'num_divisions', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "sizes [ : leftover ] += 1 \n"
Original    (008): ['sizes', '[', ':', 'leftover', ']', '+=', '1', '\\n']
Tokenized   (012): ['<s>', 's', 'izes', '[', ':', 'leftover', ']', '+=', '1', '\\', 'n', '</s>']
Filtered   (010): ['s', 'izes', '[', ':', 'leftover', ']', '+=', '1', '\\', 'n']
Detokenized (008): ['sizes', '[', ':', 'leftover', ']', '+=', '1', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "offsets [ 1 : ] = numpy . cumsum ( sizes ) [ : - 1 ] \n"
Original    (018): ['offsets', '[', '1', ':', ']', '=', 'numpy', '.', 'cumsum', '(', 'sizes', ')', '[', ':', '-', '1', ']', '\\n']
Tokenized   (025): ['<s>', 'offs', 'ets', '[', '1', ':', ']', '=', 'n', 'umpy', '.', 'c', 'ums', 'um', '(', 'sizes', ')', '[', ':', '-', '1', ']', '\\', 'n', '</s>']
Filtered   (023): ['offs', 'ets', '[', '1', ':', ']', '=', 'n', 'umpy', '.', 'c', 'ums', 'um', '(', 'sizes', ')', '[', ':', '-', '1', ']', '\\', 'n']
Detokenized (018): ['offsets', '[', '1', ':', ']', '=', 'numpy', '.', 'cumsum', '(', 'sizes', ')', '[', ':', '-', '1', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "z1 = Float ( 0. , iotype = ) \n"
Original    (010): ['z1', '=', 'Float', '(', '0.', ',', 'iotype', '=', ')', '\\n']
Tokenized   (016): ['<s>', 'z', '1', '=', 'Float', '(', '0', '.', ',', 'i', 'otype', '=', ')', '\\', 'n', '</s>']
Filtered   (014): ['z', '1', '=', 'Float', '(', '0', '.', ',', 'i', 'otype', '=', ')', '\\', 'n']
Detokenized (010): ['z1', '=', 'Float', '(', '0.', ',', 'iotype', '=', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "z_store = Array ( [ 0. , 0. ] , iotype = ) \n"
Original    (014): ['z_store', '=', 'Array', '(', '[', '0.', ',', '0.', ']', ',', 'iotype', '=', ')', '\\n']
Tokenized   (022): ['<s>', 'z', '_', 'store', '=', 'Array', '(', '[', '0', '.', ',', '0', '.', ']', ',', 'i', 'otype', '=', ')', '\\', 'n', '</s>']
Filtered   (020): ['z', '_', 'store', '=', 'Array', '(', '[', '0', '.', ',', '0', '.', ']', ',', 'i', 'otype', '=', ')', '\\', 'n']
Detokenized (014): ['z_store', '=', 'Array', '(', '[', '0.', ',', '0.', ']', ',', 'iotype', '=', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "ssa_F = Array ( [ 0.0 ] , iotype = ) \n"
Original    (012): ['ssa_F', '=', 'Array', '(', '[', '0.0', ']', ',', 'iotype', '=', ')', '\\n']
Tokenized   (021): ['<s>', 'ss', 'a', '_', 'F', '=', 'Array', '(', '[', '0', '.', '0', ']', ',', 'i', 'otype', '=', ')', '\\', 'n', '</s>']
Filtered   (019): ['ss', 'a', '_', 'F', '=', 'Array', '(', '[', '0', '.', '0', ']', ',', 'i', 'otype', '=', ')', '\\', 'n']
Detokenized (012): ['ssa_F', '=', 'Array', '(', '[', '0.0', ']', ',', 'iotype', '=', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "ssa_dG = Array ( [ [ 0.0 , 0.0 ] , [ 0.0 , 0.0 ] ] , iotype = ) \n"
Original    (022): ['ssa_dG', '=', 'Array', '(', '[', '[', '0.0', ',', '0.0', ']', ',', '[', '0.0', ',', '0.0', ']', ']', ',', 'iotype', '=', ')', '\\n']
Tokenized   (038): ['<s>', 'ss', 'a', '_', 'd', 'G', '=', 'Array', '(', '[', '[', '0', '.', '0', ',', '0', '.', '0', ']', ',', '[', '0', '.', '0', ',', '0', '.', '0', ']', ']', ',', 'i', 'otype', '=', ')', '\\', 'n', '</s>']
Filtered   (036): ['ss', 'a', '_', 'd', 'G', '=', 'Array', '(', '[', '[', '0', '.', '0', ',', '0', '.', '0', ']', ',', '[', '0', '.', '0', ',', '0', '.', '0', ']', ']', ',', 'i', 'otype', '=', ')', '\\', 'n']
Detokenized (022): ['ssa_dG', '=', 'Array', '(', '[', '[', '0.0', ',', '0.0', ']', ',', '[', '0.0', ',', '0.0', ']', ']', ',', 'iotype', '=', ')', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "arr_out = Array ( [ 1. , 2. , 3. ] , iotype = , units = ) \n"
Original    (019): ['arr_out', '=', 'Array', '(', '[', '1.', ',', '2.', ',', '3.', ']', ',', 'iotype', '=', ',', 'units', '=', ')', '\\n']
Tokenized   (028): ['<s>', 'arr', '_', 'out', '=', 'Array', '(', '[', '1', '.', ',', '2', '.', ',', '3', '.', ']', ',', 'i', 'otype', '=', ',', 'units', '=', ')', '\\', 'n', '</s>']
Filtered   (026): ['arr', '_', 'out', '=', 'Array', '(', '[', '1', '.', ',', '2', '.', ',', '3', '.', ']', ',', 'i', 'otype', '=', ',', 'units', '=', ')', '\\', 'n']
Detokenized (019): ['arr_out', '=', 'Array', '(', '[', '1.', ',', '2.', ',', '3.', ']', ',', 'iotype', '=', ',', 'units', '=', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "arg [ ] = np . array ( [ 3.1 ] ) \n"
Original    (013): ['arg', '[', ']', '=', 'np', '.', 'array', '(', '[', '3.1', ']', ')', '\\n']
Tokenized   (018): ['<s>', 'arg', '[', ']', '=', 'np', '.', 'array', '(', '[', '3', '.', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (016): ['arg', '[', ']', '=', 'np', '.', 'array', '(', '[', '3', '.', '1', ']', ')', '\\', 'n']
Detokenized (013): ['arg', '[', ']', '=', 'np', '.', 'array', '(', '[', '3.1', ']', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "jacs [ ] = np . array ( [ [ 100.0 , 101 , 102 , 103 ] , \n"
Original    (020): ['jacs', '[', ']', '=', 'np', '.', 'array', '(', '[', '[', '100.0', ',', '101', ',', '102', ',', '103', ']', ',', '\\n']
Tokenized   (026): ['<s>', 'j', 'acs', '[', ']', '=', 'np', '.', 'array', '(', '[', '[', '100', '.', '0', ',', '101', ',', '102', ',', '103', ']', ',', '\\', 'n', '</s>']
Filtered   (024): ['j', 'acs', '[', ']', '=', 'np', '.', 'array', '(', '[', '[', '100', '.', '0', ',', '101', ',', '102', ',', '103', ']', ',', '\\', 'n']
Detokenized (020): ['jacs', '[', ']', '=', 'np', '.', 'array', '(', '[', '[', '100.0', ',', '101', ',', '102', ',', '103', ']', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "assert_rel_error ( self , J [ 3 ] [ 0 ] , 3.0 , 1e-5 ) \n"
Original    (017): ['assert_rel_error', '(', 'self', ',', 'J', '[', '3', ']', '[', '0', ']', ',', '3.0', ',', '1e-5', ')', '\\n']
Tokenized   (029): ['<s>', 'assert', '_', 'rel', '_', 'error', '(', 'self', ',', 'J', '[', '3', ']', '[', '0', ']', ',', '3', '.', '0', ',', '1', 'e', '-', '5', ')', '\\', 'n', '</s>']
Filtered   (027): ['assert', '_', 'rel', '_', 'error', '(', 'self', ',', 'J', '[', '3', ']', '[', '0', ']', ',', '3', '.', '0', ',', '1', 'e', '-', '5', ')', '\\', 'n']
Detokenized (017): ['assert_rel_error', '(', 'self', ',', 'J', '[', '3', ']', '[', '0', ']', ',', '3.0', ',', '1e-5', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "newval = _getformat ( val ) % val \n"
Original    (009): ['newval', '=', '_getformat', '(', 'val', ')', '%', 'val', '\\n']
Tokenized   (015): ['<s>', 'new', 'val', '=', '_', 'get', 'format', '(', 'val', ')', '%', 'val', '\\', 'n', '</s>']
Filtered   (013): ['new', 'val', '=', '_', 'get', 'format', '(', 'val', ')', '%', 'val', '\\', 'n']
Detokenized (009): ['newval', '=', '_getformat', '(', 'val', ')', '%', 'val', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "newline = re . sub ( self . reg , sub . replace_array , line ) \n"
Original    (017): ['newline', '=', 're', '.', 'sub', '(', 'self', '.', 'reg', ',', 'sub', '.', 'replace_array', ',', 'line', ')', '\\n']
Tokenized   (023): ['<s>', 'new', 'line', '=', 're', '.', 'sub', '(', 'self', '.', 'reg', ',', 'sub', '.', 'replace', '_', 'array', ',', 'line', ')', '\\', 'n', '</s>']
Filtered   (021): ['new', 'line', '=', 're', '.', 'sub', '(', 'self', '.', 'reg', ',', 'sub', '.', 'replace', '_', 'array', ',', 'line', ')', '\\', 'n']
Detokenized (017): ['newline', '=', 're', '.', 'sub', '(', 'self', '.', 'reg', ',', 'sub', '.', 'replace_array', ',', 'line', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "fields = self . _parse_line ( ) . parseString ( line . replace ( key , "KeyField" ) ) \n"
Original    (020): ['fields', '=', 'self', '.', '_parse_line', '(', ')', '.', 'parseString', '(', 'line', '.', 'replace', '(', 'key', ',', '"KeyField"', ')', ')', '\\n']
Tokenized   (030): ['<s>', 'fields', '=', 'self', '.', '_', 'parse', '_', 'line', '(', ')', '.', 'parse', 'String', '(', 'line', '.', 'replace', '(', 'key', ',', '"', 'Key', 'Field', '"', ')', ')', '\\', 'n', '</s>']
Filtered   (028): ['fields', '=', 'self', '.', '_', 'parse', '_', 'line', '(', ')', '.', 'parse', 'String', '(', 'line', '.', 'replace', '(', 'key', ',', '"', 'Key', 'Field', '"', ')', ')', '\\', 'n']
Detokenized (020): ['fields', '=', 'self', '.', '_parse_line', '(', ')', '.', 'parseString', '(', 'line', '.', 'replace', '(', 'key', ',', '"KeyField"', ')', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "j2 = self . current_row + rowend + 1 \n"
Original    (010): ['j2', '=', 'self', '.', 'current_row', '+', 'rowend', '+', '1', '\\n']
Tokenized   (017): ['<s>', 'j', '2', '=', 'self', '.', 'current', '_', 'row', '+', 'row', 'end', '+', '1', '\\', 'n', '</s>']
Filtered   (015): ['j', '2', '=', 'self', '.', 'current', '_', 'row', '+', 'row', 'end', '+', '1', '\\', 'n']
Detokenized (010): ['j2', '=', 'self', '.', 'current_row', '+', 'rowend', '+', '1', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "ee = CaselessLiteral ( ) | CaselessLiteral ( ) \n"
Original    (010): ['ee', '=', 'CaselessLiteral', '(', ')', '|', 'CaselessLiteral', '(', ')', '\\n']
Tokenized   (021): ['<s>', 'ee', '=', 'Cas', 'eless', 'L', 'it', 'eral', '(', ')', '|', 'Cas', 'eless', 'L', 'it', 'eral', '(', ')', '\\', 'n', '</s>']
Filtered   (019): ['ee', '=', 'Cas', 'eless', 'L', 'it', 'eral', '(', ')', '|', 'Cas', 'eless', 'L', 'it', 'eral', '(', ')', '\\', 'n']
Detokenized (010): ['ee', '=', 'CaselessLiteral', '(', ')', '|', 'CaselessLiteral', '(', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "num_int = ToInteger ( Combine ( Optional ( sign ) + digits ) ) \n"
Original    (015): ['num_int', '=', 'ToInteger', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'num', '_', 'int', '=', 'To', 'Integer', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['num', '_', 'int', '=', 'To', 'Integer', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\', 'n']
Detokenized (015): ['num_int', '=', 'ToInteger', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "num_float = ToFloat ( Combine ( Optional ( sign ) + \n"
Original    (012): ['num_float', '=', 'ToFloat', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', '\\n']
Tokenized   (018): ['<s>', 'num', '_', 'float', '=', 'To', 'Float', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', '\\', 'n', '</s>']
Filtered   (016): ['num', '_', 'float', '=', 'To', 'Float', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', '\\', 'n']
Detokenized (012): ['num_float', '=', 'ToFloat', '(', 'Combine', '(', 'Optional', '(', 'sign', ')', '+', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "Optional ( ee + Optional ( sign ) + digits ) \n"
Original    (012): ['Optional', '(', 'ee', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', '\\n']
Tokenized   (016): ['<s>', 'Optional', '(', 'e', 'e', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', '\\', 'n', '</s>']
Filtered   (014): ['Optional', '(', 'e', 'e', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', '\\', 'n']
Detokenized (012): ['Optional', '(', 'ee', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "mixed_exp = ToFloat ( Combine ( digits + ee + Optional ( sign ) + digits ) ) \n"
Original    (019): ['mixed_exp', '=', 'ToFloat', '(', 'Combine', '(', 'digits', '+', 'ee', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'm', 'ixed', '_', 'exp', '=', 'To', 'Float', '(', 'Combine', '(', 'digits', '+', 'e', 'e', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['m', 'ixed', '_', 'exp', '=', 'To', 'Float', '(', 'Combine', '(', 'digits', '+', 'e', 'e', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\', 'n']
Detokenized (019): ['mixed_exp', '=', 'ToFloat', '(', 'Combine', '(', 'digits', '+', 'ee', '+', 'Optional', '(', 'sign', ')', '+', 'digits', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "string_text ) ) ) \n"
Original    (005): ['string_text', ')', ')', ')', '\\n']
Tokenized   (010): ['<s>', 'string', '_', 'text', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (008): ['string', '_', 'text', ')', ')', ')', '\\', 'n']
Detokenized (005): ['string_text', ')', ')', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "J [ , ] = - 1.0 \n"
Original    (008): ['J', '[', ',', ']', '=', '-', '1.0', '\\n']
Tokenized   (013): ['<s>', 'J', '[', ',', ']', '=', '-', '1', '.', '0', '\\', 'n', '</s>']
Filtered   (011): ['J', '[', ',', ']', '=', '-', '1', '.', '0', '\\', 'n']
Detokenized (008): ['J', '[', ',', ']', '=', '-', '1.0', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "top [ ] = - 7.0 \n"
Original    (007): ['top', '[', ']', '=', '-', '7.0', '\\n']
Tokenized   (012): ['<s>', 'top', '[', ']', '=', '-', '7', '.', '0', '\\', 'n', '</s>']
Filtered   (010): ['top', '[', ']', '=', '-', '7', '.', '0', '\\', 'n']
Detokenized (007): ['top', '[', ']', '=', '-', '7.0', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "lhs , op , rhs = _parse_constraint ( expr ) \n"
Original    (011): ['lhs', ',', 'op', ',', 'rhs', '=', '_parse_constraint', '(', 'expr', ')', '\\n']
Tokenized   (021): ['<s>', 'l', 'hs', ',', 'op', ',', 'rh', 's', '=', '_', 'parse', '_', 'con', 'str', 'aint', '(', 'expr', ')', '\\', 'n', '</s>']
Filtered   (019): ['l', 'hs', ',', 'op', ',', 'rh', 's', '=', '_', 'parse', '_', 'con', 'str', 'aint', '(', 'expr', ')', '\\', 'n']
Detokenized (011): ['lhs', ',', 'op', ',', 'rhs', '=', '_parse_constraint', '(', 'expr', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "first , second = ( rhs , lhs ) if op . startswith ( ) else ( lhs , rhs ) \n"
Original    (022): ['first', ',', 'second', '=', '(', 'rhs', ',', 'lhs', ')', 'if', 'op', '.', 'startswith', '(', ')', 'else', '(', 'lhs', ',', 'rhs', ')', '\\n']
Tokenized   (031): ['<s>', 'first', ',', 'second', '=', '(', 'rh', 's', ',', 'l', 'hs', ')', 'if', 'op', '.', 'start', 'sw', 'ith', '(', ')', 'else', '(', 'l', 'hs', ',', 'rh', 's', ')', '\\', 'n', '</s>']
Filtered   (029): ['first', ',', 'second', '=', '(', 'rh', 's', ',', 'l', 'hs', ')', 'if', 'op', '.', 'start', 'sw', 'ith', '(', ')', 'else', '(', 'l', 'hs', ',', 'rh', 's', ')', '\\', 'n']
Detokenized (022): ['first', ',', 'second', '=', '(', 'rhs', ',', 'lhs', ')', 'if', 'op', '.', 'startswith', '(', ')', 'else', '(', 'lhs', ',', 'rhs', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "input_graph . add_edges_from ( ( ( start , p ) for p in plist [ 1 : ] ) , \n"
Original    (021): ['input_graph', '.', 'add_edges_from', '(', '(', '(', 'start', ',', 'p', ')', 'for', 'p', 'in', 'plist', '[', '1', ':', ']', ')', ',', '\\n']
Tokenized   (032): ['<s>', 'input', '_', 'graph', '.', 'add', '_', 'ed', 'ges', '_', 'from', '(', '(', '(', 'start', ',', 'p', ')', 'for', 'p', 'in', 'pl', 'ist', '[', '1', ':', ']', ')', ',', '\\', 'n', '</s>']
Filtered   (030): ['input', '_', 'graph', '.', 'add', '_', 'ed', 'ges', '_', 'from', '(', '(', '(', 'start', ',', 'p', ')', 'for', 'p', 'in', 'pl', 'ist', '[', '1', ':', ']', ')', ',', '\\', 'n']
Detokenized (021): ['input_graph', '.', 'add_edges_from', '(', '(', '(', 'start', ',', 'p', ')', 'for', 'p', 'in', 'plist', '[', '1', ':', ']', ')', ',', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "src_idxs = { src : None } \n"
Original    (008): ['src_idxs', '=', '{', 'src', ':', 'None', '}', '\\n']
Tokenized   (014): ['<s>', 'src', '_', 'id', 'xs', '=', '{', 'src', ':', 'None', '}', '\\', 'n', '</s>']
Filtered   (012): ['src', '_', 'id', 'xs', '=', '{', 'src', ':', 'None', '}', '\\', 'n']
Detokenized (008): ['src_idxs', '=', '{', 'src', ':', 'None', '}', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "units = [ params_dict [ n ] . get ( ) for n in connected_inputs ] \n"
Original    (017): ['units', '=', '[', 'params_dict', '[', 'n', ']', '.', 'get', '(', ')', 'for', 'n', 'in', 'connected_inputs', ']', '\\n']
Tokenized   (025): ['<s>', 'units', '=', '[', 'params', '_', 'dict', '[', 'n', ']', '.', 'get', '(', ')', 'for', 'n', 'in', 'connected', '_', 'input', 's', ']', '\\', 'n', '</s>']
Filtered   (023): ['units', '=', '[', 'params', '_', 'dict', '[', 'n', ']', '.', 'get', '(', ')', 'for', 'n', 'in', 'connected', '_', 'input', 's', ']', '\\', 'n']
Detokenized (017): ['units', '=', '[', 'params_dict', '[', 'n', ']', '.', 'get', '(', ')', 'for', 'n', 'in', 'connected_inputs', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "vals = [ params_dict [ n ] [ ] for n in connected_inputs ] \n"
Original    (015): ['vals', '=', '[', 'params_dict', '[', 'n', ']', '[', ']', 'for', 'n', 'in', 'connected_inputs', ']', '\\n']
Tokenized   (023): ['<s>', 'vals', '=', '[', 'params', '_', 'dict', '[', 'n', ']', '[', ']', 'for', 'n', 'in', 'connected', '_', 'input', 's', ']', '\\', 'n', '</s>']
Filtered   (021): ['vals', '=', '[', 'params', '_', 'dict', '[', 'n', ']', '[', ']', 'for', 'n', 'in', 'connected', '_', 'input', 's', ']', '\\', 'n']
Detokenized (015): ['vals', '=', '[', 'params_dict', '[', 'n', ']', '[', ']', 'for', 'n', 'in', 'connected_inputs', ']', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "tname , t = connected_inputs [ i ] , u \n"
Original    (011): ['tname', ',', 't', '=', 'connected_inputs', '[', 'i', ']', ',', 'u', '\\n']
Tokenized   (018): ['<s>', 't', 'name', ',', 't', '=', 'connected', '_', 'input', 's', '[', 'i', ']', ',', 'u', '\\', 'n', '</s>']
Filtered   (016): ['t', 'name', ',', 't', '=', 'connected', '_', 'input', 's', '[', 'i', ']', ',', 'u', '\\', 'n']
Detokenized (011): ['tname', ',', 't', '=', 'connected_inputs', '[', 'i', ']', ',', 'u', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "correct_src = params_dict [ connected_inputs [ 0 ] ] [ ] \n"
Original    (012): ['correct_src', '=', 'params_dict', '[', 'connected_inputs', '[', '0', ']', ']', '[', ']', '\\n']
Tokenized   (022): ['<s>', 'correct', '_', 'src', '=', 'params', '_', 'dict', '[', 'connected', '_', 'input', 's', '[', '0', ']', ']', '[', ']', '\\', 'n', '</s>']
Filtered   (020): ['correct', '_', 'src', '=', 'params', '_', 'dict', '[', 'connected', '_', 'input', 's', '[', '0', ']', ']', '[', ']', '\\', 'n']
Detokenized (012): ['correct_src', '=', 'params_dict', '[', 'connected_inputs', '[', '0', ']', ']', '[', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "sorted ( [ ( v , k ) for k , v in forms . items ( ) ] ) ) ) \n"
Original    (023): ['sorted', '(', '[', '(', 'v', ',', 'k', ')', 'for', 'k', ',', 'v', 'in', 'forms', '.', 'items', '(', ')', ']', ')', ')', ')', '\\n']
Tokenized   (027): ['<s>', 's', 'orted', '(', '[', '(', 'v', ',', 'k', ')', 'for', 'k', ',', 'v', 'in', 'forms', '.', 'items', '(', ')', ']', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['s', 'orted', '(', '[', '(', 'v', ',', 'k', ')', 'for', 'k', ',', 'v', 'in', 'forms', '.', 'items', '(', ')', ']', ')', ')', ')', '\\', 'n']
Detokenized (023): ['sorted', '(', '[', '(', 'v', ',', 'k', ')', 'for', 'k', ',', 'v', 'in', 'forms', '.', 'items', '(', ')', ']', ')', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "full_order = { s . pathname : i for i , s in \n"
Original    (014): ['full_order', '=', '{', 's', '.', 'pathname', ':', 'i', 'for', 'i', ',', 's', 'in', '\\n']
Tokenized   (020): ['<s>', 'full', '_', 'order', '=', '{', 's', '.', 'path', 'name', ':', 'i', 'for', 'i', ',', 's', 'in', '\\', 'n', '</s>']
Filtered   (018): ['full', '_', 'order', '=', '{', 's', '.', 'path', 'name', ':', 'i', 'for', 'i', ',', 's', 'in', '\\', 'n']
Detokenized (014): ['full_order', '=', '{', 's', '.', 'pathname', ':', 'i', 'for', 'i', ',', 's', 'in', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "enumerate ( self . root . subsystems ( recurse = True ) ) } \n"
Original    (015): ['enumerate', '(', 'self', '.', 'root', '.', 'subsystems', '(', 'recurse', '=', 'True', ')', ')', '}', '\\n']
Tokenized   (022): ['<s>', 'en', 'umer', 'ate', '(', 'self', '.', 'root', '.', 'subsystem', 's', '(', 'rec', 'urse', '=', 'True', ')', ')', '}', '\\', 'n', '</s>']
Filtered   (020): ['en', 'umer', 'ate', '(', 'self', '.', 'root', '.', 'subsystem', 's', '(', 'rec', 'urse', '=', 'True', ')', ')', '}', '\\', 'n']
Detokenized (015): ['enumerate', '(', 'self', '.', 'root', '.', 'subsystems', '(', 'recurse', '=', 'True', ')', ')', '}', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "ssys = srcs [ 0 ] . rsplit ( , 1 ) [ 0 ] \n"
Original    (016): ['ssys', '=', 'srcs', '[', '0', ']', '.', 'rsplit', '(', ',', '1', ')', '[', '0', ']', '\\n']
Tokenized   (022): ['<s>', 'ss', 'ys', '=', 'src', 's', '[', '0', ']', '.', 'r', 'split', '(', ',', '1', ')', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (020): ['ss', 'ys', '=', 'src', 's', '[', '0', ']', '.', 'r', 'split', '(', ',', '1', ')', '[', '0', ']', '\\', 'n']
Detokenized (016): ['ssys', '=', 'srcs', '[', '0', ']', '.', 'rsplit', '(', ',', '1', ')', '[', '0', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "params_dict , unknowns_dict = self . root . _setup_variables ( ) \n"
Original    (012): ['params_dict', ',', 'unknowns_dict', '=', 'self', '.', 'root', '.', '_setup_variables', '(', ')', '\\n']
Tokenized   (024): ['<s>', 'params', '_', 'dict', ',', 'unknown', 's', '_', 'dict', '=', 'self', '.', 'root', '.', '_', 'setup', '_', 'vari', 'ables', '(', ')', '\\', 'n', '</s>']
Filtered   (022): ['params', '_', 'dict', ',', 'unknown', 's', '_', 'dict', '=', 'self', '.', 'root', '.', '_', 'setup', '_', 'vari', 'ables', '(', ')', '\\', 'n']
Detokenized (012): ['params_dict', ',', 'unknowns_dict', '=', 'self', '.', 'root', '.', '_setup_variables', '(', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "is not Component . setup_distrib ) ) : \n"
Original    (009): ['is', 'not', 'Component', '.', 'setup_distrib', ')', ')', ':', '\\n']
Tokenized   (015): ['<s>', 'is', 'not', 'Component', '.', 'setup', '_', 'dist', 'rib', ')', ')', ':', '\\', 'n', '</s>']
Filtered   (013): ['is', 'not', 'Component', '.', 'setup', '_', 'dist', 'rib', ')', ')', ':', '\\', 'n']
Detokenized (009): ['is', 'not', 'Component', '.', 'setup_distrib', ')', ')', ':', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "alloc_derivs = not self . root . fd_options [ ] \n"
Original    (011): ['alloc_derivs', '=', 'not', 'self', '.', 'root', '.', 'fd_options', '[', ']', '\\n']
Tokenized   (021): ['<s>', 'alloc', '_', 'der', 'iv', 's', '=', 'not', 'self', '.', 'root', '.', 'f', 'd', '_', 'options', '[', ']', '\\', 'n', '</s>']
Filtered   (019): ['alloc', '_', 'der', 'iv', 's', '=', 'not', 'self', '.', 'root', '.', 'f', 'd', '_', 'options', '[', ']', '\\', 'n']
Detokenized (011): ['alloc_derivs', '=', 'not', 'self', '.', 'root', '.', 'fd_options', '[', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "dangling_params = sorted ( set ( [ \n"
Original    (008): ['dangling_params', '=', 'sorted', '(', 'set', '(', '[', '\\n']
Tokenized   (014): ['<s>', 'd', 'angling', '_', 'params', '=', 'sorted', '(', 'set', '(', '[', '\\', 'n', '</s>']
Filtered   (012): ['d', 'angling', '_', 'params', '=', 'sorted', '(', 'set', '(', '[', '\\', 'n']
Detokenized (008): ['dangling_params', '=', 'sorted', '(', 'set', '(', '[', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "nocomps = sorted ( [ c . pathname for c in self . root . components ( recurse = True , \n"
Original    (022): ['nocomps', '=', 'sorted', '(', '[', 'c', '.', 'pathname', 'for', 'c', 'in', 'self', '.', 'root', '.', 'components', '(', 'recurse', '=', 'True', ',', '\\n']
Tokenized   (029): ['<s>', 'n', 'ocom', 'ps', '=', 'sorted', '(', '[', 'c', '.', 'path', 'name', 'for', 'c', 'in', 'self', '.', 'root', '.', 'components', '(', 'rec', 'urse', '=', 'True', ',', '\\', 'n', '</s>']
Filtered   (027): ['n', 'ocom', 'ps', '=', 'sorted', '(', '[', 'c', '.', 'path', 'name', 'for', 'c', 'in', 'self', '.', 'root', '.', 'components', '(', 'rec', 'urse', '=', 'True', ',', '\\', 'n']
Detokenized (022): ['nocomps', '=', 'sorted', '(', '[', 'c', '.', 'pathname', 'for', 'c', 'in', 'self', '.', 'root', '.', 'components', '(', 'recurse', '=', 'True', ',', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "local = True ) \n"
Original    (005): ['local', '=', 'True', ')', '\\n']
Tokenized   (008): ['<s>', 'local', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (006): ['local', '=', 'True', ')', '\\', 'n']
Detokenized (005): ['local', '=', 'True', ')', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "recorders . extend ( grp . ln_solver . recorders ) \n"
Original    (011): ['recorders', '.', 'extend', '(', 'grp', '.', 'ln_solver', '.', 'recorders', ')', '\\n']
Tokenized   (021): ['<s>', 'rec', 'orders', '.', 'extend', '(', 'gr', 'p', '.', 'l', 'n', '_', 's', 'olver', '.', 'record', 'ers', ')', '\\', 'n', '</s>']
Filtered   (019): ['rec', 'orders', '.', 'extend', '(', 'gr', 'p', '.', 'l', 'n', '_', 's', 'olver', '.', 'record', 'ers', ')', '\\', 'n']
Detokenized (011): ['recorders', '.', 'extend', '(', 'grp', '.', 'ln_solver', '.', 'recorders', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "conn_comps . update ( [ s . rsplit ( , 1 ) [ 0 ] \n"
Original    (016): ['conn_comps', '.', 'update', '(', '[', 's', '.', 'rsplit', '(', ',', '1', ')', '[', '0', ']', '\\n']
Tokenized   (023): ['<s>', 'conn', '_', 'com', 'ps', '.', 'update', '(', '[', 's', '.', 'r', 'split', '(', ',', '1', ')', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (021): ['conn', '_', 'com', 'ps', '.', 'update', '(', '[', 's', '.', 'r', 'split', '(', ',', '1', ')', '[', '0', ']', '\\', 'n']
Detokenized (016): ['conn_comps', '.', 'update', '(', '[', 's', '.', 'rsplit', '(', ',', '1', ')', '[', '0', ']', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "noconn_comps = sorted ( [ c . pathname \n"
Original    (009): ['noconn_comps', '=', 'sorted', '(', '[', 'c', '.', 'pathname', '\\n']
Tokenized   (018): ['<s>', 'n', 'ocon', 'n', '_', 'com', 'ps', '=', 'sorted', '(', '[', 'c', '.', 'path', 'name', '\\', 'n', '</s>']
Filtered   (016): ['n', 'ocon', 'n', '_', 'com', 'ps', '=', 'sorted', '(', '[', 'c', '.', 'path', 'name', '\\', 'n']
Detokenized (009): ['noconn_comps', '=', 'sorted', '(', '[', 'c', '.', 'pathname', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "strong = [ s for s in nx . strongly_connected_components ( graph ) \n"
Original    (014): ['strong', '=', '[', 's', 'for', 's', 'in', 'nx', '.', 'strongly_connected_components', '(', 'graph', ')', '\\n']
Tokenized   (023): ['<s>', 'strong', '=', '[', 's', 'for', 's', 'in', 'n', 'x', '.', 'strongly', '_', 'connected', '_', 'comp', 'onents', '(', 'graph', ')', '\\', 'n', '</s>']
Filtered   (021): ['strong', '=', '[', 's', 'for', 's', 'in', 'n', 'x', '.', 'strongly', '_', 'connected', '_', 'comp', 'onents', '(', 'graph', ')', '\\', 'n']
Detokenized (014): ['strong', '=', '[', 's', 'for', 's', 'in', 'nx', '.', 'strongly_connected_components', '(', 'graph', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "subs = [ s for s in grp . _subsystems ] \n"
Original    (012): ['subs', '=', '[', 's', 'for', 's', 'in', 'grp', '.', '_subsystems', ']', '\\n']
Tokenized   (020): ['<s>', 'sub', 's', '=', '[', 's', 'for', 's', 'in', 'gr', 'p', '.', '_', 'sub', 'system', 's', ']', '\\', 'n', '</s>']
Filtered   (018): ['sub', 's', '=', '[', 's', 'for', 's', 'in', 'gr', 'p', '.', '_', 'sub', 'system', 's', ']', '\\', 'n']
Detokenized (012): ['subs', '=', '[', 's', 'for', 's', 'in', 'grp', '.', '_subsystems', ']', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "tups = sorted ( [ ( subs . index ( s ) , s ) for s in relstrong [ - 1 ] ] ) \n"
Original    (026): ['tups', '=', 'sorted', '(', '[', '(', 'subs', '.', 'index', '(', 's', ')', ',', 's', ')', 'for', 's', 'in', 'relstrong', '[', '-', '1', ']', ']', ')', '\\n']
Tokenized   (031): ['<s>', 't', 'ups', '=', 'sorted', '(', '[', '(', 'subs', '.', 'index', '(', 's', ')', ',', 's', ')', 'for', 's', 'in', 'rel', 'strong', '[', '-', '1', ']', ']', ')', '\\', 'n', '</s>']
Filtered   (029): ['t', 'ups', '=', 'sorted', '(', '[', '(', 'subs', '.', 'index', '(', 's', ')', ',', 's', ')', 'for', 's', 'in', 'rel', 'strong', '[', '-', '1', ']', ']', ')', '\\', 'n']
Detokenized (026): ['tups', '=', 'sorted', '(', '[', '(', 'subs', '.', 'index', '(', 's', ')', ',', 's', ')', 'for', 's', 'in', 'relstrong', '[', '-', '1', ']', ']', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "relstrong [ - 1 ] = [ t [ 1 ] for t in tups ] \n"
Original    (017): ['relstrong', '[', '-', '1', ']', '=', '[', 't', '[', '1', ']', 'for', 't', 'in', 'tups', ']', '\\n']
Tokenized   (022): ['<s>', 'rel', 'strong', '[', '-', '1', ']', '=', '[', 't', '[', '1', ']', 'for', 't', 'in', 't', 'ups', ']', '\\', 'n', '</s>']
Filtered   (020): ['rel', 'strong', '[', '-', '1', ']', '=', '[', 't', '[', '1', ']', 'for', 't', 'in', 't', 'ups', ']', '\\', 'n']
Detokenized (017): ['relstrong', '[', '-', '1', ']', '=', '[', 't', '[', '1', ']', 'for', 't', 'in', 'tups', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "nearest_child ( grp . pathname , n ) for n in out_of_order [ name ] \n"
Original    (016): ['nearest_child', '(', 'grp', '.', 'pathname', ',', 'n', ')', 'for', 'n', 'in', 'out_of_order', '[', 'name', ']', '\\n']
Tokenized   (028): ['<s>', 'ne', 'arest', '_', 'child', '(', 'gr', 'p', '.', 'path', 'name', ',', 'n', ')', 'for', 'n', 'in', 'out', '_', 'of', '_', 'order', '[', 'name', ']', '\\', 'n', '</s>']
Filtered   (026): ['ne', 'arest', '_', 'child', '(', 'gr', 'p', '.', 'path', 'name', ',', 'n', ')', 'for', 'n', 'in', 'out', '_', 'of', '_', 'order', '[', 'name', ']', '\\', 'n']
Detokenized (016): ['nearest_child', '(', 'grp', '.', 'pathname', ',', 'n', ')', 'for', 'n', 'in', 'out_of_order', '[', 'name', ']', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "pbos = [ var for var in vec if vec . metadata ( var ) . get ( ) ] \n"
Original    (021): ['pbos', '=', '[', 'var', 'for', 'var', 'in', 'vec', 'if', 'vec', '.', 'metadata', '(', 'var', ')', '.', 'get', '(', ')', ']', '\\n']
Tokenized   (025): ['<s>', 'p', 'bos', '=', '[', 'var', 'for', 'var', 'in', 'vec', 'if', 'vec', '.', 'metadata', '(', 'var', ')', '.', 'get', '(', ')', ']', '\\', 'n', '</s>']
Filtered   (023): ['p', 'bos', '=', '[', 'var', 'for', 'var', 'in', 'vec', 'if', 'vec', '.', 'metadata', '(', 'var', ')', '.', 'get', '(', ')', ']', '\\', 'n']
Detokenized (021): ['pbos', '=', '[', 'var', 'for', 'var', 'in', 'vec', 'if', 'vec', '.', 'metadata', '(', 'var', ')', '.', 'get', '(', ')', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "iteritems ( self . root . _params_dict ) ) : \n"
Original    (011): ['iteritems', '(', 'self', '.', 'root', '.', '_params_dict', ')', ')', ':', '\\n']
Tokenized   (018): ['<s>', 'iter', 'items', '(', 'self', '.', 'root', '.', '_', 'params', '_', 'dict', ')', ')', ':', '\\', 'n', '</s>']
Filtered   (016): ['iter', 'items', '(', 'self', '.', 'root', '.', '_', 'params', '_', 'dict', ')', ')', ':', '\\', 'n']
Detokenized (011): ['iteritems', '(', 'self', '.', 'root', '.', '_params_dict', ')', ')', ':', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "dv_scale = None , cn_scale = None , sparsity = None ) : \n"
Original    (014): ['dv_scale', '=', 'None', ',', 'cn_scale', '=', 'None', ',', 'sparsity', '=', 'None', ')', ':', '\\n']
Tokenized   (024): ['<s>', 'd', 'v', '_', 'scale', '=', 'None', ',', 'c', 'n', '_', 'scale', '=', 'None', ',', 'sp', 'arsity', '=', 'None', ')', ':', '\\', 'n', '</s>']
Filtered   (022): ['d', 'v', '_', 'scale', '=', 'None', ',', 'c', 'n', '_', 'scale', '=', 'None', ',', 'sp', 'arsity', '=', 'None', ')', ':', '\\', 'n']
Detokenized (014): ['dv_scale', '=', 'None', ',', 'cn_scale', '=', 'None', ',', 'sparsity', '=', 'None', ')', ':', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "fd_unknowns = [ var for var in unknown_list if var not in indep_list ] \n"
Original    (015): ['fd_unknowns', '=', '[', 'var', 'for', 'var', 'in', 'unknown_list', 'if', 'var', 'not', 'in', 'indep_list', ']', '\\n']
Tokenized   (026): ['<s>', 'fd', '_', 'unknown', 's', '=', '[', 'var', 'for', 'var', 'in', 'unknown', '_', 'list', 'if', 'var', 'not', 'in', 'ind', 'ep', '_', 'list', ']', '\\', 'n', '</s>']
Filtered   (024): ['fd', '_', 'unknown', 's', '=', '[', 'var', 'for', 'var', 'in', 'unknown', '_', 'list', 'if', 'var', 'not', 'in', 'ind', 'ep', '_', 'list', ']', '\\', 'n']
Detokenized (015): ['fd_unknowns', '=', '[', 'var', 'for', 'var', 'in', 'unknown_list', 'if', 'var', 'not', 'in', 'indep_list', ']', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "usize += len ( idx ) \n"
Original    (007): ['usize', '+=', 'len', '(', 'idx', ')', '\\n']
Tokenized   (012): ['<s>', 'us', 'ize', '+=', 'len', '(', 'id', 'x', ')', '\\', 'n', '</s>']
Filtered   (010): ['us', 'ize', '+=', 'len', '(', 'id', 'x', ')', '\\', 'n']
Detokenized (007): ['usize', '+=', 'len', '(', 'idx', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "fwd = mode == \n"
Original    (005): ['fwd', '=', 'mode', '==', '\\n']
Tokenized   (009): ['<s>', 'f', 'wd', '=', 'mode', '==', '\\', 'n', '</s>']
Filtered   (007): ['f', 'wd', '=', 'mode', '==', '\\', 'n']
Detokenized (005): ['fwd', '=', 'mode', '==', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "poi_indices , qoi_indices = self . _poi_indices , self . _qoi_indices \n"
Original    (012): ['poi_indices', ',', 'qoi_indices', '=', 'self', '.', '_poi_indices', ',', 'self', '.', '_qoi_indices', '\\n']
Tokenized   (033): ['<s>', 'po', 'i', '_', 'ind', 'ices', ',', 'q', 'oi', '_', 'ind', 'ices', '=', 'self', '.', '_', 'po', 'i', '_', 'ind', 'ices', ',', 'self', '.', '_', 'q', 'oi', '_', 'ind', 'ices', '\\', 'n', '</s>']
Filtered   (031): ['po', 'i', '_', 'ind', 'ices', ',', 'q', 'oi', '_', 'ind', 'ices', '=', 'self', '.', '_', 'po', 'i', '_', 'ind', 'ices', ',', 'self', '.', '_', 'q', 'oi', '_', 'ind', 'ices', '\\', 'n']
Detokenized (012): ['poi_indices', ',', 'qoi_indices', '=', 'self', '.', '_poi_indices', ',', 'self', '.', '_qoi_indices', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "in_scale , un_scale = cn_scale , dv_scale \n"
Original    (008): ['in_scale', ',', 'un_scale', '=', 'cn_scale', ',', 'dv_scale', '\\n']
Tokenized   (021): ['<s>', 'in', '_', 'scale', ',', 'un', '_', 'scale', '=', 'c', 'n', '_', 'scale', ',', 'd', 'v', '_', 'scale', '\\', 'n', '</s>']
Filtered   (019): ['in', '_', 'scale', ',', 'un', '_', 'scale', '=', 'c', 'n', '_', 'scale', ',', 'd', 'v', '_', 'scale', '\\', 'n']
Detokenized (008): ['in_scale', ',', 'un_scale', '=', 'cn_scale', ',', 'dv_scale', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "duvec = self . root . dumat [ vkey ] \n"
Original    (011): ['duvec', '=', 'self', '.', 'root', '.', 'dumat', '[', 'vkey', ']', '\\n']
Tokenized   (017): ['<s>', 'du', 'vec', '=', 'self', '.', 'root', '.', 'd', 'umat', '[', 'v', 'key', ']', '\\', 'n', '</s>']
Filtered   (015): ['du', 'vec', '=', 'self', '.', 'root', '.', 'd', 'umat', '[', 'v', 'key', ']', '\\', 'n']
Detokenized (011): ['duvec', '=', 'self', '.', 'root', '.', 'dumat', '[', 'vkey', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "rhs [ vkey ] [ : ] = 0.0 \n"
Original    (010): ['rhs', '[', 'vkey', ']', '[', ':', ']', '=', '0.0', '\\n']
Tokenized   (017): ['<s>', 'r', 'hs', '[', 'v', 'key', ']', '[', ':', ']', '=', '0', '.', '0', '\\', 'n', '</s>']
Filtered   (015): ['r', 'hs', '[', 'v', 'key', ']', '[', ':', ']', '=', '0', '.', '0', '\\', 'n']
Detokenized (010): ['rhs', '[', 'vkey', ']', '[', ':', ']', '=', '0.0', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "isinstance ( self . root . ln_solver , LinearGaussSeidel ) ) : \n"
Original    (013): ['isinstance', '(', 'self', '.', 'root', '.', 'ln_solver', ',', 'LinearGaussSeidel', ')', ')', ':', '\\n']
Tokenized   (025): ['<s>', 'is', 'instance', '(', 'self', '.', 'root', '.', 'l', 'n', '_', 's', 'olver', ',', 'Linear', 'Ga', 'uss', 'Se', 'idel', ')', ')', ':', '\\', 'n', '</s>']
Filtered   (023): ['is', 'instance', '(', 'self', '.', 'root', '.', 'l', 'n', '_', 's', 'olver', ',', 'Linear', 'Ga', 'uss', 'Se', 'idel', ')', ')', ':', '\\', 'n']
Detokenized (013): ['isinstance', '(', 'self', '.', 'root', '.', 'ln_solver', ',', 'LinearGaussSeidel', ')', ')', ':', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "unkn_list = [ item for item in dunknowns if not dunknowns . metadata ( item ) . get ( ) ] \n"
Original    (022): ['unkn_list', '=', '[', 'item', 'for', 'item', 'in', 'dunknowns', 'if', 'not', 'dunknowns', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\n']
Tokenized   (032): ['<s>', 'unk', 'n', '_', 'list', '=', '[', 'item', 'for', 'item', 'in', 'd', 'unknown', 's', 'if', 'not', 'd', 'unknown', 's', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\', 'n', '</s>']
Filtered   (030): ['unk', 'n', '_', 'list', '=', '[', 'item', 'for', 'item', 'in', 'd', 'unknown', 's', 'if', 'not', 'd', 'unknown', 's', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\', 'n']
Detokenized (022): ['unkn_list', '=', '[', 'item', 'for', 'item', 'in', 'dunknowns', 'if', 'not', 'dunknowns', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "p_size = np . size ( dinputs [ p_name ] ) \n"
Original    (012): ['p_size', '=', 'np', '.', 'size', '(', 'dinputs', '[', 'p_name', ']', ')', '\\n']
Tokenized   (021): ['<s>', 'p', '_', 'size', '=', 'np', '.', 'size', '(', 'd', 'input', 's', '[', 'p', '_', 'name', ']', ')', '\\', 'n', '</s>']
Filtered   (019): ['p', '_', 'size', '=', 'np', '.', 'size', '(', 'd', 'input', 's', '[', 'p', '_', 'name', ']', ')', '\\', 'n']
Detokenized (012): ['p_size', '=', 'np', '.', 'size', '(', 'dinputs', '[', 'p_name', ']', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "dresids . _dat [ u_name ] . val [ idx ] = 1.0 \n"
Original    (014): ['dresids', '.', '_dat', '[', 'u_name', ']', '.', 'val', '[', 'idx', ']', '=', '1.0', '\\n']
Tokenized   (025): ['<s>', 'd', 'res', 'ids', '.', '_', 'dat', '[', 'u', '_', 'name', ']', '.', 'val', '[', 'id', 'x', ']', '=', '1', '.', '0', '\\', 'n', '</s>']
Filtered   (023): ['d', 'res', 'ids', '.', '_', 'dat', '[', 'u', '_', 'name', ']', '.', 'val', '[', 'id', 'x', ']', '=', '1', '.', '0', '\\', 'n']
Detokenized (014): ['dresids', '.', '_dat', '[', 'u_name', ']', '.', 'val', '[', 'idx', ']', '=', '1.0', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "dunknowns , dresids , ) \n"
Original    (006): ['dunknowns', ',', 'dresids', ',', ')', '\\n']
Tokenized   (013): ['<s>', 'd', 'unknown', 's', ',', 'd', 'res', 'ids', ',', ')', '\\', 'n', '</s>']
Filtered   (011): ['d', 'unknown', 's', ',', 'd', 'res', 'ids', ',', ')', '\\', 'n']
Detokenized (006): ['dunknowns', ',', 'dresids', ',', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "jac_rev [ ( u_name , p_name ) ] [ idx , : ] = dinputs . _dat [ p_name ] . val \n"
Original    (023): ['jac_rev', '[', '(', 'u_name', ',', 'p_name', ')', ']', '[', 'idx', ',', ':', ']', '=', 'dinputs', '.', '_dat', '[', 'p_name', ']', '.', 'val', '\\n']
Tokenized   (038): ['<s>', 'jac', '_', 'rev', '[', '(', 'u', '_', 'name', ',', 'p', '_', 'name', ')', ']', '[', 'id', 'x', ',', ':', ']', '=', 'd', 'input', 's', '.', '_', 'dat', '[', 'p', '_', 'name', ']', '.', 'val', '\\', 'n', '</s>']
Filtered   (036): ['jac', '_', 'rev', '[', '(', 'u', '_', 'name', ',', 'p', '_', 'name', ')', ']', '[', 'id', 'x', ',', ':', ']', '=', 'd', 'input', 's', '.', '_', 'dat', '[', 'p', '_', 'name', ']', '.', 'val', '\\', 'n']
Detokenized (023): ['jac_rev', '[', '(', 'u_name', ',', 'p_name', ')', ']', '[', 'idx', ',', ':', ']', '=', 'dinputs', '.', '_dat', '[', 'p_name', ']', '.', 'val', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "c_name = cname , jac_fd2 = jac_fd2 , fd_desc = fd_desc , \n"
Original    (013): ['c_name', '=', 'cname', ',', 'jac_fd2', '=', 'jac_fd2', ',', 'fd_desc', '=', 'fd_desc', ',', '\\n']
Tokenized   (033): ['<s>', 'c', '_', 'name', '=', 'c', 'name', ',', 'j', 'ac', '_', 'fd', '2', '=', 'j', 'ac', '_', 'fd', '2', ',', 'f', 'd', '_', 'desc', '=', 'f', 'd', '_', 'desc', ',', '\\', 'n', '</s>']
Filtered   (031): ['c', '_', 'name', '=', 'c', 'name', ',', 'j', 'ac', '_', 'fd', '2', '=', 'j', 'ac', '_', 'fd', '2', ',', 'f', 'd', '_', 'desc', '=', 'f', 'd', '_', 'desc', ',', '\\', 'n']
Detokenized (013): ['c_name', '=', 'cname', ',', 'jac_fd2', '=', 'jac_fd2', ',', 'fd_desc', '=', 'fd_desc', ',', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "param_srcs = [ root . connections [ p ] for p in abs_indep_list if not root . _params_dict [ p ] . get ( ) ] \n"
Original    (027): ['param_srcs', '=', '[', 'root', '.', 'connections', '[', 'p', ']', 'for', 'p', 'in', 'abs_indep_list', 'if', 'not', 'root', '.', '_params_dict', '[', 'p', ']', '.', 'get', '(', ')', ']', '\\n']
Tokenized   (041): ['<s>', 'param', '_', 'src', 's', '=', '[', 'root', '.', 'connections', '[', 'p', ']', 'for', 'p', 'in', 'abs', '_', 'ind', 'ep', '_', 'list', 'if', 'not', 'root', '.', '_', 'params', '_', 'dict', '[', 'p', ']', '.', 'get', '(', ')', ']', '\\', 'n', '</s>']
Filtered   (039): ['param', '_', 'src', 's', '=', '[', 'root', '.', 'connections', '[', 'p', ']', 'for', 'p', 'in', 'abs', '_', 'ind', 'ep', '_', 'list', 'if', 'not', 'root', '.', '_', 'params', '_', 'dict', '[', 'p', ']', '.', 'get', '(', ')', ']', '\\', 'n']
Detokenized (027): ['param_srcs', '=', '[', 'root', '.', 'connections', '[', 'p', ']', 'for', 'p', 'in', 'abs_indep_list', 'if', 'not', 'root', '.', '_params_dict', '[', 'p', ']', '.', 'get', '(', ')', ']', '\\n']
Counter: 39
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "to_prom_name [ p ] for p , idxs in param_srcs \n"
Original    (011): ['to_prom_name', '[', 'p', ']', 'for', 'p', ',', 'idxs', 'in', 'param_srcs', '\\n']
Tokenized   (022): ['<s>', 'to', '_', 'prom', '_', 'name', '[', 'p', ']', 'for', 'p', ',', 'id', 'xs', 'in', 'param', '_', 'src', 's', '\\', 'n', '</s>']
Filtered   (020): ['to', '_', 'prom', '_', 'name', '[', 'p', ']', 'for', 'p', ',', 'id', 'xs', 'in', 'param', '_', 'src', 's', '\\', 'n']
Detokenized (011): ['to_prom_name', '[', 'p', ']', 'for', 'p', ',', 'idxs', 'in', 'param_srcs', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "unknown_list = [ item for item in unknown_list if not root . unknowns . metadata ( item ) . get ( ) ] \n"
Original    (024): ['unknown_list', '=', '[', 'item', 'for', 'item', 'in', 'unknown_list', 'if', 'not', 'root', '.', 'unknowns', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\n']
Tokenized   (032): ['<s>', 'unknown', '_', 'list', '=', '[', 'item', 'for', 'item', 'in', 'unknown', '_', 'list', 'if', 'not', 'root', '.', 'unknown', 's', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\', 'n', '</s>']
Filtered   (030): ['unknown', '_', 'list', '=', '[', 'item', 'for', 'item', 'in', 'unknown', '_', 'list', 'if', 'not', 'root', '.', 'unknown', 's', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\', 'n']
Detokenized (024): ['unknown_list', '=', '[', 'item', 'for', 'item', 'in', 'unknown_list', 'if', 'not', 'root', '.', 'unknowns', '.', 'metadata', '(', 'item', ')', '.', 'get', '(', ')', ']', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "_assemble_deriv_data ( indep_list , unknown_list , data , \n"
Original    (009): ['_assemble_deriv_data', '(', 'indep_list', ',', 'unknown_list', ',', 'data', ',', '\\n']
Tokenized   (024): ['<s>', '_', 'as', 'semble', '_', 'der', 'iv', '_', 'data', '(', 'ind', 'ep', '_', 'list', ',', 'unknown', '_', 'list', ',', 'data', ',', '\\', 'n', '</s>']
Filtered   (022): ['_', 'as', 'semble', '_', 'der', 'iv', '_', 'data', '(', 'ind', 'ep', '_', 'list', ',', 'unknown', '_', 'list', ',', 'data', ',', '\\', 'n']
Detokenized (009): ['_assemble_deriv_data', '(', 'indep_list', ',', 'unknown_list', ',', 'data', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "_both_names ( tmeta , to_prom_name ) ) \n"
Original    (008): ['_both_names', '(', 'tmeta', ',', 'to_prom_name', ')', ')', '\\n']
Tokenized   (019): ['<s>', '_', 'both', '_', 'names', '(', 't', 'meta', ',', 'to', '_', 'prom', '_', 'name', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['_', 'both', '_', 'names', '(', 't', 'meta', ',', 'to', '_', 'prom', '_', 'name', ')', ')', '\\', 'n']
Detokenized (008): ['_both_names', '(', 'tmeta', ',', 'to_prom_name', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "abs_unames = self . root . _sysdata . to_abs_uname \n"
Original    (010): ['abs_unames', '=', 'self', '.', 'root', '.', '_sysdata', '.', 'to_abs_uname', '\\n']
Tokenized   (023): ['<s>', 'abs', '_', 'un', 'ames', '=', 'self', '.', 'root', '.', '_', 'sys', 'data', '.', 'to', '_', 'abs', '_', 'un', 'ame', '\\', 'n', '</s>']
Filtered   (021): ['abs', '_', 'un', 'ames', '=', 'self', '.', 'root', '.', '_', 'sys', 'data', '.', 'to', '_', 'abs', '_', 'un', 'ame', '\\', 'n']
Detokenized (010): ['abs_unames', '=', 'self', '.', 'root', '.', '_sysdata', '.', 'to_abs_uname', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "out_str = tmp1 . format ( _pad_name ( ) , _pad_name ( ) , \n"
Original    (015): ['out_str', '=', 'tmp1', '.', 'format', '(', '_pad_name', '(', ')', ',', '_pad_name', '(', ')', ',', '\\n']
Tokenized   (027): ['<s>', 'out', '_', 'str', '=', 'tmp', '1', '.', 'format', '(', '_', 'pad', '_', 'name', '(', ')', ',', '_', 'pad', '_', 'name', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (025): ['out', '_', 'str', '=', 'tmp', '1', '.', 'format', '(', '_', 'pad', '_', 'name', '(', ')', ',', '_', 'pad', '_', 'name', '(', ')', ',', '\\', 'n']
Detokenized (015): ['out_str', '=', 'tmp1', '.', 'format', '(', '_pad_name', '(', ')', ',', '_pad_name', '(', ')', ',', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "magfor , magrev , magfd , abs1 , abs2 , \n"
Original    (011): ['magfor', ',', 'magrev', ',', 'magfd', ',', 'abs1', ',', 'abs2', ',', '\\n']
Tokenized   (019): ['<s>', 'mag', 'for', ',', 'mag', 'rev', ',', 'mag', 'fd', ',', 'abs', '1', ',', 'abs', '2', ',', '\\', 'n', '</s>']
Filtered   (017): ['mag', 'for', ',', 'mag', 'rev', ',', 'mag', 'fd', ',', 'abs', '1', ',', 'abs', '2', ',', '\\', 'n']
Detokenized (011): ['magfor', ',', 'magrev', ',', 'magfd', ',', 'abs1', ',', 'abs2', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "rel1 , rel2 ) ) \n"
Original    (006): ['rel1', ',', 'rel2', ')', ')', '\\n']
Tokenized   (011): ['<s>', 'rel', '1', ',', 'rel', '2', ')', ')', '\\', 'n', '</s>']
Filtered   (009): ['rel', '1', ',', 'rel', '2', ')', ')', '\\', 'n']
Detokenized (006): ['rel1', ',', 'rel2', ')', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "_pad_name ( , 12 , quotes = False ) \n"
Original    (010): ['_pad_name', '(', ',', '12', ',', 'quotes', '=', 'False', ')', '\\n']
Tokenized   (016): ['<s>', '_', 'pad', '_', 'name', '(', ',', '12', ',', 'quotes', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (014): ['_', 'pad', '_', 'name', '(', ',', '12', ',', 'quotes', '=', 'False', ')', '\\', 'n']
Detokenized (010): ['_pad_name', '(', ',', '12', ',', 'quotes', '=', 'False', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "magfd , magfd2 , abs4 , rel4 ) ) \n"
Original    (010): ['magfd', ',', 'magfd2', ',', 'abs4', ',', 'rel4', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'mag', 'fd', ',', 'mag', 'fd', '2', ',', 'abs', '4', ',', 'rel', '4', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['mag', 'fd', ',', 'mag', 'fd', '2', ',', 'abs', '4', ',', 'rel', '4', ')', ')', '\\', 'n']
Detokenized (010): ['magfd', ',', 'magfd2', ',', 'abs4', ',', 'rel4', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "out_stream . write ( str ( Jsub_fd2 ) ) \n"
Original    (010): ['out_stream', '.', 'write', '(', 'str', '(', 'Jsub_fd2', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'out', '_', 'stream', '.', 'write', '(', 'str', '(', 'J', 'sub', '_', 'fd', '2', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['out', '_', 'stream', '.', 'write', '(', 'str', '(', 'J', 'sub', '_', 'fd', '2', ')', ')', '\\', 'n']
Detokenized (010): ['out_stream', '.', 'write', '(', 'str', '(', 'Jsub_fd2', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sqlite_dict_args . setdefault ( , ) \n"
Original    (007): ['sqlite_dict_args', '.', 'setdefault', '(', ',', ')', '\\n']
Tokenized   (016): ['<s>', 'sql', 'ite', '_', 'dict', '_', 'args', '.', 'set', 'default', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (014): ['sql', 'ite', '_', 'dict', '_', 'args', '.', 'set', 'default', '(', ',', ')', '\\', 'n']
Detokenized (007): ['sqlite_dict_args', '.', 'setdefault', '(', ',', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "ll_1 = ll_0 + n_samples - k - 1 \n"
Original    (010): ['ll_1', '=', 'll_0', '+', 'n_samples', '-', 'k', '-', '1', '\\n']
Tokenized   (020): ['<s>', 'll', '_', '1', '=', 'll', '_', '0', '+', 'n', '_', 's', 'amples', '-', 'k', '-', '1', '\\', 'n', '</s>']
Filtered   (018): ['ll', '_', '1', '=', 'll', '_', '0', '+', 'n', '_', 's', 'amples', '-', 'k', '-', '1', '\\', 'n']
Detokenized (010): ['ll_1', '=', 'll_0', '+', 'n_samples', '-', 'k', '-', '1', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "D = self . D [ lvl ] \n"
Original    (009): ['D', '=', 'self', '.', 'D', '[', 'lvl', ']', '\\n']
Tokenized   (012): ['<s>', 'D', '=', 'self', '.', 'D', '[', 'lvl', ']', '\\', 'n', '</s>']
Filtered   (010): ['D', '=', 'self', '.', 'D', '[', 'lvl', ']', '\\', 'n']
Detokenized (009): ['D', '=', 'self', '.', 'D', '[', 'lvl', ']', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "initial_range = INITIAL_RANGE_DEFAULT , tol = TOLERANCE_DEFAULT ) : \n"
Original    (010): ['initial_range', '=', 'INITIAL_RANGE_DEFAULT', ',', 'tol', '=', 'TOLERANCE_DEFAULT', ')', ':', '\\n']
Tokenized   (030): ['<s>', 'initial', '_', 'range', '=', 'IN', 'IT', 'IAL', '_', 'R', 'ANGE', '_', 'DE', 'FAULT', ',', 'to', 'l', '=', 'T', 'OL', 'ER', 'ANCE', '_', 'DE', 'FAULT', ')', ':', '\\', 'n', '</s>']
Filtered   (028): ['initial', '_', 'range', '=', 'IN', 'IT', 'IAL', '_', 'R', 'ANGE', '_', 'DE', 'FAULT', ',', 'to', 'l', '=', 'T', 'OL', 'ER', 'ANCE', '_', 'DE', 'FAULT', ')', ':', '\\', 'n']
Detokenized (010): ['initial_range', '=', 'INITIAL_RANGE_DEFAULT', ',', 'tol', '=', 'TOLERANCE_DEFAULT', ')', ':', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "y_best = y [ nlevel - 1 ] \n"
Original    (009): ['y_best', '=', 'y', '[', 'nlevel', '-', '1', ']', '\\n']
Tokenized   (015): ['<s>', 'y', '_', 'best', '=', 'y', '[', 'n', 'level', '-', '1', ']', '\\', 'n', '</s>']
Filtered   (013): ['y', '_', 'best', '=', 'y', '[', 'n', 'level', '-', '1', ']', '\\', 'n']
Detokenized (009): ['y_best', '=', 'y', '[', 'nlevel', '-', '1', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "+ str ( theta ) ) \n"
Original    (007): ['+', 'str', '(', 'theta', ')', ')', '\\n']
Tokenized   (011): ['<s>', '+', 'str', '(', 'the', 'ta', ')', ')', '\\', 'n', '</s>']
Filtered   (009): ['+', 'str', '(', 'the', 'ta', ')', ')', '\\', 'n']
Detokenized (007): ['+', 'str', '(', 'theta', ')', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "Yt = solve_triangular ( C , y , lower = True ) \n"
Original    (013): ['Yt', '=', 'solve_triangular', '(', 'C', ',', 'y', ',', 'lower', '=', 'True', ')', '\\n']
Tokenized   (020): ['<s>', 'Y', 't', '=', 'solve', '_', 'tri', 'angular', '(', 'C', ',', 'y', ',', 'lower', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (018): ['Y', 't', '=', 'solve', '_', 'tri', 'angular', '(', 'C', ',', 'y', ',', 'lower', '=', 'True', ')', '\\', 'n']
Detokenized (013): ['Yt', '=', 'solve_triangular', '(', 'C', ',', 'y', ',', 'lower', '=', 'True', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "err2 = np . dot ( err . T , err ) [ 0 , 0 ] \n"
Original    (018): ['err2', '=', 'np', '.', 'dot', '(', 'err', '.', 'T', ',', 'err', ')', '[', '0', ',', '0', ']', '\\n']
Tokenized   (022): ['<s>', 'err', '2', '=', 'np', '.', 'dot', '(', 'err', '.', 'T', ',', 'err', ')', '[', '0', ',', '0', ']', '\\', 'n', '</s>']
Filtered   (020): ['err', '2', '=', 'np', '.', 'dot', '(', 'err', '.', 'T', ',', 'err', ')', '[', '0', ',', '0', ']', '\\', 'n']
Detokenized (018): ['err2', '=', 'np', '.', 'dot', '(', 'err', '.', 'T', ',', 'err', ')', '[', '0', ',', '0', ']', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "sigma2 = err2 / ( n_samples - p - q ) \n"
Original    (012): ['sigma2', '=', 'err2', '/', '(', 'n_samples', '-', 'p', '-', 'q', ')', '\\n']
Tokenized   (021): ['<s>', 's', 'igma', '2', '=', 'err', '2', '/', '(', 'n', '_', 's', 'amples', '-', 'p', '-', 'q', ')', '\\', 'n', '</s>']
Filtered   (019): ['s', 'igma', '2', '=', 'err', '2', '/', '(', 'n', '_', 's', 'amples', '-', 'p', '-', 'q', ')', '\\', 'n']
Detokenized (012): ['sigma2', '=', 'err2', '/', '(', 'n_samples', '-', 'p', '-', 'q', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "detR = ( ( np . diag ( C ) ) ** ( 2. / n_samples ) ) . prod ( ) \n"
Original    (023): ['detR', '=', '(', '(', 'np', '.', 'diag', '(', 'C', ')', ')', '**', '(', '2.', '/', 'n_samples', ')', ')', '.', 'prod', '(', ')', '\\n']
Tokenized   (032): ['<s>', 'det', 'R', '=', '(', '(', 'np', '.', 'di', 'ag', '(', 'C', ')', ')', '**', '(', '2', '.', '/', 'n', '_', 's', 'amples', ')', ')', '.', 'prod', '(', ')', '\\', 'n', '</s>']
Filtered   (030): ['det', 'R', '=', '(', '(', 'np', '.', 'di', 'ag', '(', 'C', ')', ')', '**', '(', '2', '.', '/', 'n', '_', 's', 'amples', ')', ')', '.', 'prod', '(', ')', '\\', 'n']
Detokenized (023): ['detR', '=', '(', '(', 'np', '.', 'diag', '(', 'C', ')', ')', '**', '(', '2.', '/', 'n_samples', ')', ')', '.', 'prod', '(', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "rlf_value = ( n_samples - p - q ) * np . log10 ( sigma2 ) + n_samples * np . log10 ( detR ) \n"
Original    (026): ['rlf_value', '=', '(', 'n_samples', '-', 'p', '-', 'q', ')', '*', 'np', '.', 'log10', '(', 'sigma2', ')', '+', 'n_samples', '*', 'np', '.', 'log10', '(', 'detR', ')', '\\n']
Tokenized   (043): ['<s>', 'r', 'lf', '_', 'value', '=', '(', 'n', '_', 's', 'amples', '-', 'p', '-', 'q', ')', '*', 'np', '.', 'log', '10', '(', 's', 'igma', '2', ')', '+', 'n', '_', 's', 'amples', '*', 'np', '.', 'log', '10', '(', 'det', 'R', ')', '\\', 'n', '</s>']
Filtered   (041): ['r', 'lf', '_', 'value', '=', '(', 'n', '_', 's', 'amples', '-', 'p', '-', 'q', ')', '*', 'np', '.', 'log', '10', '(', 's', 'igma', '2', ')', '+', 'n', '_', 's', 'amples', '*', 'np', '.', 'log', '10', '(', 'det', 'R', ')', '\\', 'n']
Detokenized (026): ['rlf_value', '=', '(', 'n_samples', '-', 'p', '-', 'q', ')', '*', 'np', '.', 'log10', '(', 'sigma2', ')', '+', 'n_samples', '*', 'np', '.', 'log10', '(', 'detR', ')', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "log10t [ i ] - np . log10 ( thetaL [ 0 ] [ i ] ) } ) \n"
Original    (020): ['log10t', '[', 'i', ']', '-', 'np', '.', 'log10', '(', 'thetaL', '[', '0', ']', '[', 'i', ']', ')', '}', ')', '\\n']
Tokenized   (028): ['<s>', 'log', '10', 't', '[', 'i', ']', '-', 'np', '.', 'log', '10', '(', 'the', 'ta', 'L', '[', '0', ']', '[', 'i', ']', ')', '}', ')', '\\', 'n', '</s>']
Filtered   (026): ['log', '10', 't', '[', 'i', ']', '-', 'np', '.', 'log', '10', '(', 'the', 'ta', 'L', '[', '0', ']', '[', 'i', ']', ')', '}', ')', '\\', 'n']
Detokenized (020): ['log10t', '[', 'i', ']', '-', 'np', '.', 'log10', '(', 'thetaL', '[', '0', ']', '[', 'i', ']', ')', '}', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "np . log10 ( thetaU [ 0 ] [ i ] ) - log10t [ i ] } ) \n"
Original    (020): ['np', '.', 'log10', '(', 'thetaU', '[', '0', ']', '[', 'i', ']', ')', '-', 'log10t', '[', 'i', ']', '}', ')', '\\n']
Tokenized   (028): ['<s>', 'np', '.', 'log', '10', '(', 'the', 'ta', 'U', '[', '0', ']', '[', 'i', ']', ')', '-', 'log', '10', 't', '[', 'i', ']', '}', ')', '\\', 'n', '</s>']
Filtered   (026): ['np', '.', 'log', '10', '(', 'the', 'ta', 'U', '[', '0', ']', '[', 'i', ']', ')', '-', 'log', '10', 't', '[', 'i', ']', '}', ')', '\\', 'n']
Detokenized (020): ['np', '.', 'log10', '(', 'thetaU', '[', '0', ']', '[', 'i', ']', ')', '-', 'log10t', '[', 'i', ']', '}', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "sol = minimize ( rlf_transform , x0 , method = , \n"
Original    (012): ['sol', '=', 'minimize', '(', 'rlf_transform', ',', 'x0', ',', 'method', '=', ',', '\\n']
Tokenized   (019): ['<s>', 'sol', '=', 'minimize', '(', 'r', 'lf', '_', 'transform', ',', 'x', '0', ',', 'method', '=', ',', '\\', 'n', '</s>']
Filtered   (017): ['sol', '=', 'minimize', '(', 'r', 'lf', '_', 'transform', ',', 'x', '0', ',', 'method', '=', ',', '\\', 'n']
Detokenized (012): ['sol', '=', 'minimize', '(', 'rlf_transform', ',', 'x0', ',', 'method', '=', ',', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "optimal_theta = 10. ** log10_optimal_x \n"
Original    (006): ['optimal_theta', '=', '10.', '**', 'log10_optimal_x', '\\n']
Tokenized   (020): ['<s>', 'opt', 'imal', '_', 'the', 'ta', '=', '10', '.', '**', 'log', '10', '_', 'opt', 'imal', '_', 'x', '\\', 'n', '</s>']
Filtered   (018): ['opt', 'imal', '_', 'the', 'ta', '=', '10', '.', '**', 'log', '10', '_', 'opt', 'imal', '_', 'x', '\\', 'n']
Detokenized (006): ['optimal_theta', '=', '10.', '**', 'log10_optimal_x', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "r_t = solve_triangular ( C , r_ . T , lower = True ) \n"
Original    (015): ['r_t', '=', 'solve_triangular', '(', 'C', ',', 'r_', '.', 'T', ',', 'lower', '=', 'True', ')', '\\n']
Tokenized   (024): ['<s>', 'r', '_', 't', '=', 'solve', '_', 'tri', 'angular', '(', 'C', ',', 'r', '_', '.', 'T', ',', 'lower', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (022): ['r', '_', 't', '=', 'solve', '_', 'tri', 'angular', '(', 'C', ',', 'r', '_', '.', 'T', ',', 'lower', '=', 'True', ')', '\\', 'n']
Detokenized (015): ['r_t', '=', 'solve_triangular', '(', 'C', ',', 'r_', '.', 'T', ',', 'lower', '=', 'True', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "dx = l1_cross_distances ( X , Y = self . X [ i ] ) \n"
Original    (016): ['dx', '=', 'l1_cross_distances', '(', 'X', ',', 'Y', '=', 'self', '.', 'X', '[', 'i', ']', ')', '\\n']
Tokenized   (025): ['<s>', 'dx', '=', 'l', '1', '_', 'cross', '_', 'dist', 'ances', '(', 'X', ',', 'Y', '=', 'self', '.', 'X', '[', 'i', ']', ')', '\\', 'n', '</s>']
Filtered   (023): ['dx', '=', 'l', '1', '_', 'cross', '_', 'dist', 'ances', '(', 'X', ',', 'Y', '=', 'self', '.', 'X', '[', 'i', ']', ')', '\\', 'n']
Detokenized (016): ['dx', '=', 'l1_cross_distances', '(', 'X', ',', 'Y', '=', 'self', '.', 'X', '[', 'i', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "r_ = self . corr ( self . theta [ i ] , dx ) . reshape ( n_eval , self . n_samples [ i ] ) \n"
Original    (028): ['r_', '=', 'self', '.', 'corr', '(', 'self', '.', 'theta', '[', 'i', ']', ',', 'dx', ')', '.', 'reshape', '(', 'n_eval', ',', 'self', '.', 'n_samples', '[', 'i', ']', ')', '\\n']
Tokenized   (040): ['<s>', 'r', '_', '=', 'self', '.', 'cor', 'r', '(', 'self', '.', 'the', 'ta', '[', 'i', ']', ',', 'dx', ')', '.', 'resh', 'ape', '(', 'n', '_', 'eval', ',', 'self', '.', 'n', '_', 's', 'amples', '[', 'i', ']', ')', '\\', 'n', '</s>']
Filtered   (038): ['r', '_', '=', 'self', '.', 'cor', 'r', '(', 'self', '.', 'the', 'ta', '[', 'i', ']', ',', 'dx', ')', '.', 'resh', 'ape', '(', 'n', '_', 'eval', ',', 'self', '.', 'n', '_', 's', 'amples', '[', 'i', ']', ')', '\\', 'n']
Detokenized (028): ['r_', '=', 'self', '.', 'corr', '(', 'self', '.', 'theta', '[', 'i', ']', ',', 'dx', ')', '.', 'reshape', '(', 'n_eval', ',', 'self', '.', 'n_samples', '[', 'i', ']', ')', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 28, 768)
# Extracted words:  28
Sentence         : "yt = solve_triangular ( C , self . y [ i ] , lower = True ) \n"
Original    (018): ['yt', '=', 'solve_triangular', '(', 'C', ',', 'self', '.', 'y', '[', 'i', ']', ',', 'lower', '=', 'True', ')', '\\n']
Tokenized   (024): ['<s>', 'yt', '=', 'solve', '_', 'tri', 'angular', '(', 'C', ',', 'self', '.', 'y', '[', 'i', ']', ',', 'lower', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (022): ['yt', '=', 'solve', '_', 'tri', 'angular', '(', 'C', ',', 'self', '.', 'y', '[', 'i', ']', ',', 'lower', '=', 'True', ')', '\\', 'n']
Detokenized (018): ['yt', '=', 'solve_triangular', '(', 'C', ',', 'self', '.', 'y', '[', 'i', ']', ',', 'lower', '=', 'True', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "mu [ : , i ] = ( np . dot ( f . T , beta ) + np . dot ( r_t . T , yt - np . dot ( Ft , beta ) ) ) . ravel ( ) \n"
Original    (044): ['mu', '[', ':', ',', 'i', ']', '=', '(', 'np', '.', 'dot', '(', 'f', '.', 'T', ',', 'beta', ')', '+', 'np', '.', 'dot', '(', 'r_t', '.', 'T', ',', 'yt', '-', 'np', '.', 'dot', '(', 'Ft', ',', 'beta', ')', ')', ')', '.', 'ravel', '(', ')', '\\n']
Tokenized   (051): ['<s>', 'mu', '[', ':', ',', 'i', ']', '=', '(', 'np', '.', 'dot', '(', 'f', '.', 'T', ',', 'beta', ')', '+', 'np', '.', 'dot', '(', 'r', '_', 't', '.', 'T', ',', 'y', 't', '-', 'np', '.', 'dot', '(', 'Ft', ',', 'beta', ')', ')', ')', '.', 'ra', 'vel', '(', ')', '\\', 'n', '</s>']
Filtered   (049): ['mu', '[', ':', ',', 'i', ']', '=', '(', 'np', '.', 'dot', '(', 'f', '.', 'T', ',', 'beta', ')', '+', 'np', '.', 'dot', '(', 'r', '_', 't', '.', 'T', ',', 'y', 't', '-', 'np', '.', 'dot', '(', 'Ft', ',', 'beta', ')', ')', ')', '.', 'ra', 'vel', '(', ')', '\\', 'n']
Detokenized (044): ['mu', '[', ':', ',', 'i', ']', '=', '(', 'np', '.', 'dot', '(', 'f', '.', 'T', ',', 'beta', ')', '+', 'np', '.', 'dot', '(', 'r_t', '.', 'T', ',', 'yt', '-', 'np', '.', 'dot', '(', 'Ft', ',', 'beta', ')', ')', ')', '.', 'ravel', '(', ')', '\\n']
Counter: 49
===================================================================
Hidden states:  (13, 44, 768)
# Extracted words:  44
Sentence         : "u_ = solve_triangular ( G . T , f - np . dot ( Ft . T , r_t ) , lower = True ) \n"
Original    (026): ['u_', '=', 'solve_triangular', '(', 'G', '.', 'T', ',', 'f', '-', 'np', '.', 'dot', '(', 'Ft', '.', 'T', ',', 'r_t', ')', ',', 'lower', '=', 'True', ')', '\\n']
Tokenized   (035): ['<s>', 'u', '_', '=', 'solve', '_', 'tri', 'angular', '(', 'G', '.', 'T', ',', 'f', '-', 'np', '.', 'dot', '(', 'Ft', '.', 'T', ',', 'r', '_', 't', ')', ',', 'lower', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (033): ['u', '_', '=', 'solve', '_', 'tri', 'angular', '(', 'G', '.', 'T', ',', 'f', '-', 'np', '.', 'dot', '(', 'Ft', '.', 'T', ',', 'r', '_', 't', ')', ',', 'lower', '=', 'True', ')', '\\', 'n']
Detokenized (026): ['u_', '=', 'solve_triangular', '(', 'G', '.', 'T', ',', 'f', '-', 'np', '.', 'dot', '(', 'Ft', '.', 'T', ',', 'r_t', ')', ',', 'lower', '=', 'True', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "sigma2_rho = ( sigma2_rho * g ) . sum ( axis = 1 ) \n"
Original    (015): ['sigma2_rho', '=', '(', 'sigma2_rho', '*', 'g', ')', '.', 'sum', '(', 'axis', '=', '1', ')', '\\n']
Tokenized   (028): ['<s>', 's', 'igma', '2', '_', 'r', 'ho', '=', '(', 's', 'igma', '2', '_', 'r', 'ho', '*', 'g', ')', '.', 'sum', '(', 'axis', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (026): ['s', 'igma', '2', '_', 'r', 'ho', '=', '(', 's', 'igma', '2', '_', 'r', 'ho', '*', 'g', ')', '.', 'sum', '(', 'axis', '=', '1', ')', '\\', 'n']
Detokenized (015): ['sigma2_rho', '=', '(', 'sigma2_rho', '*', 'g', ')', '.', 'sum', '(', 'axis', '=', '1', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "MSE [ : , i ] = sigma2_rho * MSE [ : , i - 1 ] + Q_ / ( 2 * ( self . n_samples [ i ] - self . p [ i ] - self . q [ i ] ) ) * ( 1 - ( r_t ** 2 ) . sum ( axis = 0 ) ) + self . sigma2 [ i ] * ( u_ ** 2 ) . sum ( axis = 0 ) \n"
Original    (084): ['MSE', '[', ':', ',', 'i', ']', '=', 'sigma2_rho', '*', 'MSE', '[', ':', ',', 'i', '-', '1', ']', '+', 'Q_', '/', '(', '2', '*', '(', 'self', '.', 'n_samples', '[', 'i', ']', '-', 'self', '.', 'p', '[', 'i', ']', '-', 'self', '.', 'q', '[', 'i', ']', ')', ')', '*', '(', '1', '-', '(', 'r_t', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', ')', '+', 'self', '.', 'sigma2', '[', 'i', ']', '*', '(', 'u_', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', '\\n']
Tokenized   (103): ['<s>', 'M', 'SE', '[', ':', ',', 'i', ']', '=', 's', 'igma', '2', '_', 'r', 'ho', '*', 'M', 'SE', '[', ':', ',', 'i', '-', '1', ']', '+', 'Q', '_', '/', '(', '2', '*', '(', 'self', '.', 'n', '_', 's', 'amples', '[', 'i', ']', '-', 'self', '.', 'p', '[', 'i', ']', '-', 'self', '.', 'q', '[', 'i', ']', ')', ')', '*', '(', '1', '-', '(', 'r', '_', 't', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', ')', '+', 'self', '.', 's', 'igma', '2', '[', 'i', ']', '*', '(', 'u', '_', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (101): ['M', 'SE', '[', ':', ',', 'i', ']', '=', 's', 'igma', '2', '_', 'r', 'ho', '*', 'M', 'SE', '[', ':', ',', 'i', '-', '1', ']', '+', 'Q', '_', '/', '(', '2', '*', '(', 'self', '.', 'n', '_', 's', 'amples', '[', 'i', ']', '-', 'self', '.', 'p', '[', 'i', ']', '-', 'self', '.', 'q', '[', 'i', ']', ')', ')', '*', '(', '1', '-', '(', 'r', '_', 't', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', ')', '+', 'self', '.', 's', 'igma', '2', '[', 'i', ']', '*', '(', 'u', '_', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', '\\', 'n']
Detokenized (084): ['MSE', '[', ':', ',', 'i', ']', '=', 'sigma2_rho', '*', 'MSE', '[', ':', ',', 'i', '-', '1', ']', '+', 'Q_', '/', '(', '2', '*', '(', 'self', '.', 'n_samples', '[', 'i', ']', '-', 'self', '.', 'p', '[', 'i', ']', '-', 'self', '.', 'q', '[', 'i', ']', ')', ')', '*', '(', '1', '-', '(', 'r_t', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', ')', '+', 'self', '.', 'sigma2', '[', 'i', ']', '*', '(', 'u_', '**', '2', ')', '.', 'sum', '(', 'axis', '=', '0', ')', '\\n']
Counter: 101
===================================================================
Hidden states:  (13, 84, 768)
# Extracted words:  84
Sentence         : "n_features = np . zeros ( nlevel , dtype = int ) \n"
Original    (013): ['n_features', '=', 'np', '.', 'zeros', '(', 'nlevel', ',', 'dtype', '=', 'int', ')', '\\n']
Tokenized   (021): ['<s>', 'n', '_', 'features', '=', 'np', '.', 'z', 'eros', '(', 'n', 'level', ',', 'd', 'type', '=', 'int', ')', '\\', 'n', '</s>']
Filtered   (019): ['n', '_', 'features', '=', 'np', '.', 'z', 'eros', '(', 'n', 'level', ',', 'd', 'type', '=', 'int', ')', '\\', 'n']
Detokenized (013): ['n_features', '=', 'np', '.', 'zeros', '(', 'nlevel', ',', 'dtype', '=', 'int', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "n_samples_y [ i ] = y [ i ] . shape [ 0 ] \n"
Original    (015): ['n_samples_y', '[', 'i', ']', '=', 'y', '[', 'i', ']', '.', 'shape', '[', '0', ']', '\\n']
Tokenized   (023): ['<s>', 'n', '_', 's', 'amples', '_', 'y', '[', 'i', ']', '=', 'y', '[', 'i', ']', '.', 'shape', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (021): ['n', '_', 's', 'amples', '_', 'y', '[', 'i', ']', '=', 'y', '[', 'i', ']', '.', 'shape', '[', '0', ']', '\\', 'n']
Detokenized (015): ['n_samples_y', '[', 'i', ']', '=', 'y', '[', 'i', ']', '.', 'shape', '[', '0', ']', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "Y_pred , MSE = self . model . predict ( [ new_x ] ) \n"
Original    (015): ['Y_pred', ',', 'MSE', '=', 'self', '.', 'model', '.', 'predict', '(', '[', 'new_x', ']', ')', '\\n']
Tokenized   (023): ['<s>', 'Y', '_', 'pred', ',', 'M', 'SE', '=', 'self', '.', 'model', '.', 'predict', '(', '[', 'new', '_', 'x', ']', ')', '\\', 'n', '</s>']
Filtered   (021): ['Y', '_', 'pred', ',', 'M', 'SE', '=', 'self', '.', 'model', '.', 'predict', '(', '[', 'new', '_', 'x', ']', ')', '\\', 'n']
Detokenized (015): ['Y_pred', ',', 'MSE', '=', 'self', '.', 'model', '.', 'predict', '(', '[', 'new_x', ']', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "X , Y = self . _fit_adapter ( X , Y ) \n"
Original    (013): ['X', ',', 'Y', '=', 'self', '.', '_fit_adapter', '(', 'X', ',', 'Y', ')', '\\n']
Tokenized   (020): ['<s>', 'X', ',', 'Y', '=', 'self', '.', '_', 'fit', '_', 'ad', 'apter', '(', 'X', ',', 'Y', ')', '\\', 'n', '</s>']
Filtered   (018): ['X', ',', 'Y', '=', 'self', '.', '_', 'fit', '_', 'ad', 'apter', '(', 'X', ',', 'Y', ')', '\\', 'n']
Detokenized (013): ['X', ',', 'Y', '=', 'self', '.', '_fit_adapter', '(', 'X', ',', 'Y', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "Y = [ np . array ( y ) for y in reversed ( Y ) ] \n"
Original    (018): ['Y', '=', '[', 'np', '.', 'array', '(', 'y', ')', 'for', 'y', 'in', 'reversed', '(', 'Y', ')', ']', '\\n']
Tokenized   (021): ['<s>', 'Y', '=', '[', 'np', '.', 'array', '(', 'y', ')', 'for', 'y', 'in', 'reversed', '(', 'Y', ')', ']', '\\', 'n', '</s>']
Filtered   (019): ['Y', '=', '[', 'np', '.', 'array', '(', 'y', ')', 'for', 'y', 'in', 'reversed', '(', 'Y', ')', ']', '\\', 'n']
Detokenized (018): ['Y', '=', '[', 'np', '.', 'array', '(', 'y', ')', 'for', 'y', 'in', 'reversed', '(', 'Y', ')', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "newdata = np . array ( parsed [ : ] ) \n"
Original    (012): ['newdata', '=', 'np', '.', 'array', '(', 'parsed', '[', ':', ']', ')', '\\n']
Tokenized   (016): ['<s>', 'new', 'data', '=', 'np', '.', 'array', '(', 'parsed', '[', ':', ']', ')', '\\', 'n', '</s>']
Filtered   (014): ['new', 'data', '=', 'np', '.', 'array', '(', 'parsed', '[', ':', ']', ')', '\\', 'n']
Detokenized (012): ['newdata', '=', 'np', '.', 'array', '(', 'parsed', '[', ':', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "icc . DB_USER ) , shell = True ) \n"
Original    (010): ['icc', '.', 'DB_USER', ')', ',', 'shell', '=', 'True', ')', '\\n']
Tokenized   (015): ['<s>', 'icc', '.', 'DB', '_', 'USER', ')', ',', 'shell', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (013): ['icc', '.', 'DB', '_', 'USER', ')', ',', 'shell', '=', 'True', ')', '\\', 'n']
Detokenized (010): ['icc', '.', 'DB_USER', ')', ',', 'shell', '=', 'True', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "instance_db_name , shell = True ) \n"
Original    (007): ['instance_db_name', ',', 'shell', '=', 'True', ')', '\\n']
Tokenized   (014): ['<s>', 'instance', '_', 'db', '_', 'name', ',', 'shell', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (012): ['instance', '_', 'db', '_', 'name', ',', 'shell', '=', 'True', ')', '\\', 'n']
Detokenized (007): ['instance_db_name', ',', 'shell', '=', 'True', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "customslide = CustomSlide . objects . create ( title = , text = default_projector = Projector . objects . get ( pk = 1 ) \n"
Original    (026): ['customslide', '=', 'CustomSlide', '.', 'objects', '.', 'create', '(', 'title', '=', ',', 'text', '=', 'default_projector', '=', 'Projector', '.', 'objects', '.', 'get', '(', 'pk', '=', '1', ')', '\\n']
Tokenized   (038): ['<s>', 'custom', 'sl', 'ide', '=', 'Custom', 'Sl', 'ide', '.', 'objects', '.', 'create', '(', 'title', '=', ',', 'text', '=', 'default', '_', 'project', 'or', '=', 'Project', 'or', '.', 'objects', '.', 'get', '(', 'p', 'k', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (036): ['custom', 'sl', 'ide', '=', 'Custom', 'Sl', 'ide', '.', 'objects', '.', 'create', '(', 'title', '=', ',', 'text', '=', 'default', '_', 'project', 'or', '=', 'Project', 'or', '.', 'objects', '.', 'get', '(', 'p', 'k', '=', '1', ')', '\\', 'n']
Detokenized (026): ['customslide', '=', 'CustomSlide', '.', 'objects', '.', 'create', '(', 'title', '=', ',', 'text', '=', 'default_projector', '=', 'Projector', '.', 'objects', '.', 'get', '(', 'pk', '=', '1', ')', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "default_projector = Projector . objects . get ( pk = 1 ) \n"
Original    (013): ['default_projector', '=', 'Projector', '.', 'objects', '.', 'get', '(', 'pk', '=', '1', ')', '\\n']
Tokenized   (021): ['<s>', 'default', '_', 'project', 'or', '=', 'Project', 'or', '.', 'objects', '.', 'get', '(', 'p', 'k', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (019): ['default', '_', 'project', 'or', '=', 'Project', 'or', '.', 'objects', '.', 'get', '(', 'p', 'k', '=', '1', ')', '\\', 'n']
Detokenized (013): ['default_projector', '=', 'Projector', '.', 'objects', '.', 'get', '(', 'pk', '=', '1', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "reverse ( , args = [ ] ) ) \n"
Original    (010): ['reverse', '(', ',', 'args', '=', '[', ']', ')', ')', '\\n']
Tokenized   (013): ['<s>', 'reverse', '(', ',', 'args', '=', '[', ']', ')', ')', '\\', 'n', '</s>']
Filtered   (011): ['reverse', '(', ',', 'args', '=', '[', ']', ')', ')', '\\', 'n']
Detokenized (010): ['reverse', '(', ',', 'args', '=', '[', ']', ')', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "yield ConfigVariable ( \n"
Original    (004): ['yield', 'ConfigVariable', '(', '\\n']
Tokenized   (009): ['<s>', 'y', 'ield', 'Config', 'Variable', '(', '\\', 'n', '</s>']
Filtered   (007): ['y', 'ield', 'Config', 'Variable', '(', '\\', 'n']
Detokenized (004): ['yield', 'ConfigVariable', '(', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "{ : , : } , { : , : } ) \n"
Original    (013): ['{', ':', ',', ':', '}', ',', '{', ':', ',', ':', '}', ')', '\\n']
Tokenized   (016): ['<s>', '{', ':', ',', ':', '}', ',', '{', ':', ',', ':', '}', ')', '\\', 'n', '</s>']
Filtered   (014): ['{', ':', ',', ':', '}', ',', '{', ':', ',', ':', '}', ')', '\\', 'n']
Detokenized (013): ['{', ':', ',', ':', '}', ',', '{', ':', ',', ':', '}', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "validators = ( validator_for_testing , ) ) \n"
Original    (008): ['validators', '=', '(', 'validator_for_testing', ',', ')', ')', '\\n']
Tokenized   (017): ['<s>', 'valid', 'ators', '=', '(', 'valid', 'ator', '_', 'for', '_', 'testing', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (015): ['valid', 'ators', '=', '(', 'valid', 'ator', '_', 'for', '_', 'testing', ',', ')', ')', '\\', 'n']
Detokenized (008): ['validators', '=', '(', 'validator_for_testing', ',', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "generate_username . return_value = \n"
Original    (005): ['generate_username', '.', 'return_value', '=', '\\n']
Tokenized   (013): ['<s>', 'gener', 'ate', '_', 'username', '.', 'return', '_', 'value', '=', '\\', 'n', '</s>']
Filtered   (011): ['gener', 'ate', '_', 'username', '.', 'return', '_', 'value', '=', '\\', 'n']
Detokenized (005): ['generate_username', '.', 'return_value', '=', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "serializer = UserFullSerializer ( context = { : view } ) \n"
Original    (012): ['serializer', '=', 'UserFullSerializer', '(', 'context', '=', '{', ':', 'view', '}', ')', '\\n']
Tokenized   (019): ['<s>', 'serial', 'izer', '=', 'User', 'Full', 'Serial', 'izer', '(', 'context', '=', '{', ':', 'view', '}', ')', '\\', 'n', '</s>']
Filtered   (017): ['serial', 'izer', '=', 'User', 'Full', 'Serial', 'izer', '(', 'context', '=', '{', ':', 'view', '}', ')', '\\', 'n']
Detokenized (012): ['serializer', '=', 'UserFullSerializer', '(', 'context', '=', '{', ':', 'view', '}', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "#domain... #localhost... \n"
Original    (003): ['#domain...', '#localhost...', '\\n']
Tokenized   (010): ['<s>', '#', 'domain', '...', '#', 'localhost', '...', '\\', 'n', '</s>']
Filtered   (008): ['#', 'domain', '...', '#', 'localhost', '...', '\\', 'n']
Detokenized (003): ['#domain...', '#localhost...', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "USERNAME_REGEX = re . compile ( , re . I ) \n"
Original    (012): ['USERNAME_REGEX', '=', 're', '.', 'compile', '(', ',', 're', '.', 'I', ')', '\\n']
Tokenized   (019): ['<s>', 'USER', 'NAME', '_', 'REG', 'EX', '=', 're', '.', 'compile', '(', ',', 're', '.', 'I', ')', '\\', 'n', '</s>']
Filtered   (017): ['USER', 'NAME', '_', 'REG', 'EX', '=', 're', '.', 'compile', '(', ',', 're', '.', 'I', ')', '\\', 'n']
Detokenized (012): ['USERNAME_REGEX', '=', 're', '.', 'compile', '(', ',', 're', '.', 'I', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "RouteDistinguisher . TYPE_IP_LOC , None , \n"
Original    (007): ['RouteDistinguisher', '.', 'TYPE_IP_LOC', ',', 'None', ',', '\\n']
Tokenized   (017): ['<s>', 'Route', 'Dist', 'ingu', 'isher', '.', 'TYPE', '_', 'IP', '_', 'LOC', ',', 'None', ',', '\\', 'n', '</s>']
Filtered   (015): ['Route', 'Dist', 'ingu', 'isher', '.', 'TYPE', '_', 'IP', '_', 'LOC', ',', 'None', ',', '\\', 'n']
Detokenized (007): ['RouteDistinguisher', '.', 'TYPE_IP_LOC', ',', 'None', ',', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "10000 + label ) \n"
Original    (005): ['10000', '+', 'label', ')', '\\n']
Tokenized   (008): ['<s>', '10000', '+', 'label', ')', '\\', 'n', '</s>']
Filtered   (006): ['10000', '+', 'label', ')', '\\', 'n']
Detokenized (005): ['10000', '+', 'label', ')', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "nh = Inet ( 1 , socket . inet_pton ( socket . AF_INET , \n"
Original    (015): ['nh', '=', 'Inet', '(', '1', ',', 'socket', '.', 'inet_pton', '(', 'socket', '.', 'AF_INET', ',', '\\n']
Tokenized   (026): ['<s>', 'n', 'h', '=', 'In', 'et', '(', '1', ',', 'socket', '.', 'in', 'et', '_', 'pton', '(', 'socket', '.', 'AF', '_', 'IN', 'ET', ',', '\\', 'n', '</s>']
Filtered   (024): ['n', 'h', '=', 'In', 'et', '(', '1', ',', 'socket', '.', 'in', 'et', '_', 'pton', '(', 'socket', '.', 'AF', '_', 'IN', 'ET', ',', '\\', 'n']
Detokenized (015): ['nh', '=', 'Inet', '(', '1', ',', 'socket', '.', 'inet_pton', '(', 'socket', '.', 'AF_INET', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "route . attributes . add ( ECommunities ( self . readvertiseToRTs ) ) \n"
Original    (014): ['route', '.', 'attributes', '.', 'add', '(', 'ECommunities', '(', 'self', '.', 'readvertiseToRTs', ')', ')', '\\n']
Tokenized   (024): ['<s>', 'route', '.', 'attributes', '.', 'add', '(', 'E', 'Commun', 'ities', '(', 'self', '.', 'read', 'vert', 'ise', 'To', 'RT', 's', ')', ')', '\\', 'n', '</s>']
Filtered   (022): ['route', '.', 'attributes', '.', 'add', '(', 'E', 'Commun', 'ities', '(', 'self', '.', 'read', 'vert', 'ise', 'To', 'RT', 's', ')', ')', '\\', 'n']
Detokenized (014): ['route', '.', 'attributes', '.', 'add', '(', 'ECommunities', '(', 'self', '.', 'readvertiseToRTs', ')', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "nlri . prefix , label ) \n"
Original    (007): ['nlri', '.', 'prefix', ',', 'label', ')', '\\n']
Tokenized   (011): ['<s>', 'nl', 'ri', '.', 'prefix', ',', 'label', ')', '\\', 'n', '</s>']
Filtered   (009): ['nl', 'ri', '.', 'prefix', ',', 'label', ')', '\\', 'n']
Detokenized (007): ['nlri', '.', 'prefix', ',', 'label', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "set ( self . importRTs ) ) ) > 0 ) \n"
Original    (012): ['set', '(', 'self', '.', 'importRTs', ')', ')', ')', '>', '0', ')', '\\n']
Tokenized   (017): ['<s>', 'set', '(', 'self', '.', 'import', 'RT', 's', ')', ')', ')', '>', '0', ')', '\\', 'n', '</s>']
Filtered   (015): ['set', '(', 'self', '.', 'import', 'RT', 's', ')', ')', ')', '>', '0', ')', '\\', 'n']
Detokenized (012): ['set', '(', 'self', '.', 'importRTs', ')', ')', ')', '>', '0', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "newRoute . nlri . labelStack [ 0 ] . labelValue , newRoute . nlri , encaps ) \n"
Original    (018): ['newRoute', '.', 'nlri', '.', 'labelStack', '[', '0', ']', '.', 'labelValue', ',', 'newRoute', '.', 'nlri', ',', 'encaps', ')', '\\n']
Tokenized   (029): ['<s>', 'new', 'Route', '.', 'n', 'l', 'ri', '.', 'label', 'Stack', '[', '0', ']', '.', 'label', 'Value', ',', 'new', 'Route', '.', 'n', 'l', 'ri', ',', 'encaps', ')', '\\', 'n', '</s>']
Filtered   (027): ['new', 'Route', '.', 'n', 'l', 'ri', '.', 'label', 'Stack', '[', '0', ']', '.', 'label', 'Value', ',', 'new', 'Route', '.', 'n', 'l', 'ri', ',', 'encaps', ')', '\\', 'n']
Detokenized (018): ['newRoute', '.', 'nlri', '.', 'labelStack', '[', '0', ']', '.', 'labelValue', ',', 'newRoute', '.', 'nlri', ',', 'encaps', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "prefix , oldRoute . attributes . get ( NextHop . ID ) . next_hop , \n"
Original    (016): ['prefix', ',', 'oldRoute', '.', 'attributes', '.', 'get', '(', 'NextHop', '.', 'ID', ')', '.', 'next_hop', ',', '\\n']
Tokenized   (023): ['<s>', 'prefix', ',', 'old', 'Route', '.', 'attributes', '.', 'get', '(', 'Next', 'Hop', '.', 'ID', ')', '.', 'next', '_', 'hop', ',', '\\', 'n', '</s>']
Filtered   (021): ['prefix', ',', 'old', 'Route', '.', 'attributes', '.', 'get', '(', 'Next', 'Hop', '.', 'ID', ')', '.', 'next', '_', 'hop', ',', '\\', 'n']
Detokenized (016): ['prefix', ',', 'oldRoute', '.', 'attributes', '.', 'get', '(', 'NextHop', '.', 'ID', ')', '.', 'next_hop', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "oldRoute . nlri . labelStack [ 0 ] . labelValue , oldRoute . nlri ) \n"
Original    (016): ['oldRoute', '.', 'nlri', '.', 'labelStack', '[', '0', ']', '.', 'labelValue', ',', 'oldRoute', '.', 'nlri', ')', '\\n']
Tokenized   (027): ['<s>', 'old', 'Route', '.', 'n', 'l', 'ri', '.', 'label', 'Stack', '[', '0', ']', '.', 'label', 'Value', ',', 'old', 'Route', '.', 'n', 'l', 'ri', ')', '\\', 'n', '</s>']
Filtered   (025): ['old', 'Route', '.', 'n', 'l', 'ri', '.', 'label', 'Stack', '[', '0', ']', '.', 'label', 'Value', ',', 'old', 'Route', '.', 'n', 'l', 'ri', ')', '\\', 'n']
Detokenized (016): ['oldRoute', '.', 'nlri', '.', 'labelStack', '[', '0', ']', '.', 'labelValue', ',', 'oldRoute', '.', 'nlri', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : ""readvertised" : ( LGMap . VALUE , [ repr ( prefix ) for prefix in \n"
Original    (016): ['"readvertised"', ':', '(', 'LGMap', '.', 'VALUE', ',', '[', 'repr', '(', 'prefix', ')', 'for', 'prefix', 'in', '\\n']
Tokenized   (025): ['<s>', '"', 'read', 'vert', 'ised', '"', ':', '(', 'LG', 'Map', '.', 'VAL', 'UE', ',', '[', 'repr', '(', 'prefix', ')', 'for', 'prefix', 'in', '\\', 'n', '</s>']
Filtered   (023): ['"', 'read', 'vert', 'ised', '"', ':', '(', 'LG', 'Map', '.', 'VAL', 'UE', ',', '[', 'repr', '(', 'prefix', ')', 'for', 'prefix', 'in', '\\', 'n']
Detokenized (016): ['"readvertised"', ':', '(', 'LGMap', '.', 'VALUE', ',', '[', 'repr', '(', 'prefix', ')', 'for', 'prefix', 'in', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "REACTORNAME = DEFAULT_REACTORS . get ( platform . system ( ) , ) \n"
Original    (014): ['REACTORNAME', '=', 'DEFAULT_REACTORS', '.', 'get', '(', 'platform', '.', 'system', '(', ')', ',', ')', '\\n']
Tokenized   (025): ['<s>', 'RE', 'ACT', 'OR', 'NAME', '=', 'DE', 'FAULT', '_', 'RE', 'ACT', 'ORS', '.', 'get', '(', 'platform', '.', 'system', '(', ')', ',', ')', '\\', 'n', '</s>']
Filtered   (023): ['RE', 'ACT', 'OR', 'NAME', '=', 'DE', 'FAULT', '_', 'RE', 'ACT', 'ORS', '.', 'get', '(', 'platform', '.', 'system', '(', ')', ',', ')', '\\', 'n']
Detokenized (014): ['REACTORNAME', '=', 'DEFAULT_REACTORS', '.', 'get', '(', 'platform', '.', 'system', '(', ')', ',', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "set_reactor = lambda : reactor \n"
Original    (006): ['set_reactor', '=', 'lambda', ':', 'reactor', '\\n']
Tokenized   (012): ['<s>', 'set', '_', 're', 'actor', '=', 'lambda', ':', 'reactor', '\\', 'n', '</s>']
Filtered   (010): ['set', '_', 're', 'actor', '=', 'lambda', ':', 'reactor', '\\', 'n']
Detokenized (006): ['set_reactor', '=', 'lambda', ':', 'reactor', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "SIGNALS = dict ( ( k , v ) for v , k in signal . __dict__ . iteritems ( ) if v . startswith ( ) and not v . startswith ( ) ) \n"
Original    (036): ['SIGNALS', '=', 'dict', '(', '(', 'k', ',', 'v', ')', 'for', 'v', ',', 'k', 'in', 'signal', '.', '__dict__', '.', 'iteritems', '(', ')', 'if', 'v', '.', 'startswith', '(', ')', 'and', 'not', 'v', '.', 'startswith', '(', ')', ')', '\\n']
Tokenized   (047): ['<s>', 'SIGN', 'ALS', '=', 'dict', '(', '(', 'k', ',', 'v', ')', 'for', 'v', ',', 'k', 'in', 'signal', '.', '__', 'dict', '__', '.', 'iter', 'items', '(', ')', 'if', 'v', '.', 'start', 'sw', 'ith', '(', ')', 'and', 'not', 'v', '.', 'start', 'sw', 'ith', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (045): ['SIGN', 'ALS', '=', 'dict', '(', '(', 'k', ',', 'v', ')', 'for', 'v', ',', 'k', 'in', 'signal', '.', '__', 'dict', '__', '.', 'iter', 'items', '(', ')', 'if', 'v', '.', 'start', 'sw', 'ith', '(', ')', 'and', 'not', 'v', '.', 'start', 'sw', 'ith', '(', ')', ')', '\\', 'n']
Detokenized (036): ['SIGNALS', '=', 'dict', '(', '(', 'k', ',', 'v', ')', 'for', 'v', ',', 'k', 'in', 'signal', '.', '__dict__', '.', 'iteritems', '(', ')', 'if', 'v', '.', 'startswith', '(', ')', 'and', 'not', 'v', '.', 'startswith', '(', ')', ')', '\\n']
Counter: 45
===================================================================
Hidden states:  (13, 36, 768)
# Extracted words:  36
Sentence         : "pargs = ( self . name , self . label , self . reactor ) \n"
Original    (016): ['pargs', '=', '(', 'self', '.', 'name', ',', 'self', '.', 'label', ',', 'self', '.', 'reactor', ')', '\\n']
Tokenized   (020): ['<s>', 'p', 'args', '=', '(', 'self', '.', 'name', ',', 'self', '.', 'label', ',', 'self', '.', 'reactor', ')', '\\', 'n', '</s>']
Filtered   (018): ['p', 'args', '=', '(', 'self', '.', 'name', ',', 'self', '.', 'label', ',', 'self', '.', 'reactor', ')', '\\', 'n']
Detokenized (016): ['pargs', '=', '(', 'self', '.', 'name', ',', 'self', '.', 'label', ',', 'self', '.', 'reactor', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "* pargs , ** pkwargs \n"
Original    (006): ['*', 'pargs', ',', '**', 'pkwargs', '\\n']
Tokenized   (012): ['<s>', '*', 'p', 'args', ',', '**', 'p', 'kw', 'args', '\\', 'n', '</s>']
Filtered   (010): ['*', 'p', 'args', ',', '**', 'p', 'kw', 'args', '\\', 'n']
Detokenized (006): ['*', 'pargs', ',', '**', 'pkwargs', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "logdir = env . pop ( , os . path . join ( os . path . sep , ) ) \n"
Original    (022): ['logdir', '=', 'env', '.', 'pop', '(', ',', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'sep', ',', ')', ')', '\\n']
Tokenized   (026): ['<s>', 'log', 'dir', '=', 'env', '.', 'pop', '(', ',', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'sep', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['log', 'dir', '=', 'env', '.', 'pop', '(', ',', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'sep', ',', ')', ')', '\\', 'n']
Detokenized (022): ['logdir', '=', 'env', '.', 'pop', '(', ',', 'os', '.', 'path', '.', 'join', '(', 'os', '.', 'path', '.', 'sep', ',', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "masksignals = bool ( env . pop ( , True ) ) \n"
Original    (013): ['masksignals', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', 'True', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'mas', 'ks', 'ign', 'als', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', 'True', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['mas', 'ks', 'ign', 'als', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', 'True', ')', ')', '\\', 'n']
Detokenized (013): ['masksignals', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', 'True', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "usetty = bool ( env . pop ( , ) ) \n"
Original    (012): ['usetty', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', ')', ')', '\\n']
Tokenized   (017): ['<s>', 'us', 'et', 'ty', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (015): ['us', 'et', 'ty', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', ')', ')', '\\', 'n']
Detokenized (012): ['usetty', '=', 'bool', '(', 'env', '.', 'pop', '(', ',', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "maxfd = resource . getrlimit ( resource . RLIMIT_NOFILE ) [ 1 ] \n"
Original    (014): ['maxfd', '=', 'resource', '.', 'getrlimit', '(', 'resource', '.', 'RLIMIT_NOFILE', ')', '[', '1', ']', '\\n']
Tokenized   (025): ['<s>', 'max', 'fd', '=', 'resource', '.', 'get', 'r', 'limit', '(', 'resource', '.', 'RL', 'IM', 'IT', '_', 'NO', 'FILE', ')', '[', '1', ']', '\\', 'n', '</s>']
Filtered   (023): ['max', 'fd', '=', 'resource', '.', 'get', 'r', 'limit', '(', 'resource', '.', 'RL', 'IM', 'IT', '_', 'NO', 'FILE', ')', '[', '1', ']', '\\', 'n']
Detokenized (014): ['maxfd', '=', 'resource', '.', 'getrlimit', '(', 'resource', '.', 'RLIMIT_NOFILE', ')', '[', '1', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "hasattr ( os , "devnull" ) and os . devnull or "/dev/null" , \n"
Original    (014): ['hasattr', '(', 'os', ',', '"devnull"', ')', 'and', 'os', '.', 'devnull', 'or', '"/dev/null"', ',', '\\n']
Tokenized   (026): ['<s>', 'has', 'attr', '(', 'os', ',', '"', 'dev', 'null', '"', ')', 'and', 'os', '.', 'dev', 'null', 'or', '"/', 'dev', '/', 'null', '"', ',', '\\', 'n', '</s>']
Filtered   (024): ['has', 'attr', '(', 'os', ',', '"', 'dev', 'null', '"', ')', 'and', 'os', '.', 'dev', 'null', 'or', '"/', 'dev', '/', 'null', '"', ',', '\\', 'n']
Detokenized (014): ['hasattr', '(', 'os', ',', '"devnull"', ')', 'and', 'os', '.', 'devnull', 'or', '"/dev/null"', ',', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "droned . logging . logToDir ( \n"
Original    (007): ['droned', '.', 'logging', '.', 'logToDir', '(', '\\n']
Tokenized   (014): ['<s>', 'd', 'ron', 'ed', '.', 'logging', '.', 'log', 'To', 'Dir', '(', '\\', 'n', '</s>']
Filtered   (012): ['d', 'ron', 'ed', '.', 'logging', '.', 'log', 'To', 'Dir', '(', '\\', 'n']
Detokenized (007): ['droned', '.', 'logging', '.', 'logToDir', '(', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "conversation . say ( contextSummary , useHTML = False ) \n"
Original    (011): ['conversation', '.', 'say', '(', 'contextSummary', ',', 'useHTML', '=', 'False', ')', '\\n']
Tokenized   (018): ['<s>', 'con', 'vers', 'ation', '.', 'say', '(', 'context', 'Summary', ',', 'use', 'HTML', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (016): ['con', 'vers', 'ation', '.', 'say', '(', 'context', 'Summary', ',', 'use', 'HTML', '=', 'False', ')', '\\', 'n']
Detokenized (011): ['conversation', '.', 'say', '(', 'contextSummary', ',', 'useHTML', '=', 'False', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "moduleProvides ( IDroneDService ) #requirement \n"
Original    (006): ['moduleProvides', '(', 'IDroneDService', ')', '#requirement', '\\n']
Tokenized   (017): ['<s>', 'module', 'Prov', 'ides', '(', 'ID', 'rone', 'DS', 'erv', 'ice', ')', '#', 'requ', 'irement', '\\', 'n', '</s>']
Filtered   (015): ['module', 'Prov', 'ides', '(', 'ID', 'rone', 'DS', 'erv', 'ice', ')', '#', 'requ', 'irement', '\\', 'n']
Detokenized (006): ['moduleProvides', '(', 'IDroneDService', ')', '#requirement', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "hour = property ( lambda foo : 3600 ) \n"
Original    (010): ['hour', '=', 'property', '(', 'lambda', 'foo', ':', '3600', ')', '\\n']
Tokenized   (014): ['<s>', 'hour', '=', 'property', '(', 'lambda', 'foo', ':', '36', '00', ')', '\\', 'n', '</s>']
Filtered   (012): ['hour', '=', 'property', '(', 'lambda', 'foo', ':', '36', '00', ')', '\\', 'n']
Detokenized (010): ['hour', '=', 'property', '(', 'lambda', 'foo', ':', '3600', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "watchDict = property ( lambda s : SERVICECONFIG . wrapped . get ( , { } ) ) \n"
Original    (019): ['watchDict', '=', 'property', '(', 'lambda', 's', ':', 'SERVICECONFIG', '.', 'wrapped', '.', 'get', '(', ',', '{', '}', ')', ')', '\\n']
Tokenized   (028): ['<s>', 'watch', 'D', 'ict', '=', 'property', '(', 'lambda', 's', ':', 'SERV', 'IC', 'EC', 'ON', 'FIG', '.', 'wrapped', '.', 'get', '(', ',', '{', '}', ')', ')', '\\', 'n', '</s>']
Filtered   (026): ['watch', 'D', 'ict', '=', 'property', '(', 'lambda', 's', ':', 'SERV', 'IC', 'EC', 'ON', 'FIG', '.', 'wrapped', '.', 'get', '(', ',', '{', '}', ')', ')', '\\', 'n']
Detokenized (019): ['watchDict', '=', 'property', '(', 'lambda', 's', ':', 'SERVICECONFIG', '.', 'wrapped', '.', 'get', '(', ',', '{', '}', ')', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "model = gm . throat_surface_area . cylinder ) \n"
Original    (009): ['model', '=', 'gm', '.', 'throat_surface_area', '.', 'cylinder', ')', '\\n']
Tokenized   (017): ['<s>', 'model', '=', 'g', 'm', '.', 'throat', '_', 'surface', '_', 'area', '.', 'cylinder', ')', '\\', 'n', '</s>']
Filtered   (015): ['model', '=', 'g', 'm', '.', 'throat', '_', 'surface', '_', 'area', '.', 'cylinder', ')', '\\', 'n']
Detokenized (009): ['model', '=', 'gm', '.', 'throat_surface_area', '.', 'cylinder', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "pores = network . find_connected_pores ( throats , flatten = False ) \n"
Original    (013): ['pores', '=', 'network', '.', 'find_connected_pores', '(', 'throats', ',', 'flatten', '=', 'False', ')', '\\n']
Tokenized   (023): ['<s>', 'p', 'ores', '=', 'network', '.', 'find', '_', 'connected', '_', 'p', 'ores', '(', 'throats', ',', 'flatt', 'en', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (021): ['p', 'ores', '=', 'network', '.', 'find', '_', 'connected', '_', 'p', 'ores', '(', 'throats', ',', 'flatt', 'en', '=', 'False', ')', '\\', 'n']
Detokenized (013): ['pores', '=', 'network', '.', 'find_connected_pores', '(', 'throats', ',', 'flatten', '=', 'False', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "C0 = network [ ] [ pores , 0 ] \n"
Original    (011): ['C0', '=', 'network', '[', ']', '[', 'pores', ',', '0', ']', '\\n']
Tokenized   (015): ['<s>', 'C', '0', '=', 'network', '[', ']', '[', 'pores', ',', '0', ']', '\\', 'n', '</s>']
Filtered   (013): ['C', '0', '=', 'network', '[', ']', '[', 'pores', ',', '0', ']', '\\', 'n']
Detokenized (011): ['C0', '=', 'network', '[', ']', '[', 'pores', ',', '0', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "P = phase [ pore_P ] / 100000 \n"
Original    (009): ['P', '=', 'phase', '[', 'pore_P', ']', '/', '100000', '\\n']
Tokenized   (016): ['<s>', 'P', '=', 'phase', '[', 'p', 'ore', '_', 'P', ']', '/', '100', '000', '\\', 'n', '</s>']
Filtered   (014): ['P', '=', 'phase', '[', 'p', 'ore', '_', 'P', ']', '/', '100', '000', '\\', 'n']
Detokenized (009): ['P', '=', 'phase', '[', 'pore_P', ']', '/', '100000', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "a1 = - 1 / b \n"
Original    (007): ['a1', '=', '-', '1', '/', 'b', '\\n']
Tokenized   (011): ['<s>', 'a', '1', '=', '-', '1', '/', 'b', '\\', 'n', '</s>']
Filtered   (009): ['a', '1', '=', '-', '1', '/', 'b', '\\', 'n']
Detokenized (007): ['a1', '=', '-', '1', '/', 'b', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "a2 = ( R * T + b * P ) / ( a * b ) \n"
Original    (018): ['a2', '=', '(', 'R', '*', 'T', '+', 'b', '*', 'P', ')', '/', '(', 'a', '*', 'b', ')', '\\n']
Tokenized   (022): ['<s>', 'a', '2', '=', '(', 'R', '*', 'T', '+', 'b', '*', 'P', ')', '/', '(', 'a', '*', 'b', ')', '\\', 'n', '</s>']
Filtered   (020): ['a', '2', '=', '(', 'R', '*', 'T', '+', 'b', '*', 'P', ')', '/', '(', 'a', '*', 'b', ')', '\\', 'n']
Detokenized (018): ['a2', '=', '(', 'R', '*', 'T', '+', 'b', '*', 'P', ')', '/', '(', 'a', '*', 'b', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "a3 = - P / ( a * b ) \n"
Original    (011): ['a3', '=', '-', 'P', '/', '(', 'a', '*', 'b', ')', '\\n']
Tokenized   (015): ['<s>', 'a', '3', '=', '-', 'P', '/', '(', 'a', '*', 'b', ')', '\\', 'n', '</s>']
Filtered   (013): ['a', '3', '=', '-', 'P', '/', '(', 'a', '*', 'b', ')', '\\', 'n']
Detokenized (011): ['a3', '=', '-', 'P', '/', '(', 'a', '*', 'b', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "coeffs = sp . vstack ( ( a0 , a1 , a2 , a3 ) ) . T \n"
Original    (019): ['coeffs', '=', 'sp', '.', 'vstack', '(', '(', 'a0', ',', 'a1', ',', 'a2', ',', 'a3', ')', ')', '.', 'T', '\\n']
Tokenized   (029): ['<s>', 'co', 'eff', 's', '=', 'sp', '.', 'v', 'stack', '(', '(', 'a', '0', ',', 'a', '1', ',', 'a', '2', ',', 'a', '3', ')', ')', '.', 'T', '\\', 'n', '</s>']
Filtered   (027): ['co', 'eff', 's', '=', 'sp', '.', 'v', 'stack', '(', '(', 'a', '0', ',', 'a', '1', ',', 'a', '2', ',', 'a', '3', ')', ')', '.', 'T', '\\', 'n']
Detokenized (019): ['coeffs', '=', 'sp', '.', 'vstack', '(', '(', 'a0', ',', 'a1', ',', 'a2', ',', 'a3', ')', ')', '.', 'T', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "density = sp . array ( [ sp . roots ( C ) for C in coeffs ] ) \n"
Original    (020): ['density', '=', 'sp', '.', 'array', '(', '[', 'sp', '.', 'roots', '(', 'C', ')', 'for', 'C', 'in', 'coeffs', ']', ')', '\\n']
Tokenized   (025): ['<s>', 'density', '=', 'sp', '.', 'array', '(', '[', 'sp', '.', 'roots', '(', 'C', ')', 'for', 'C', 'in', 'co', 'eff', 's', ']', ')', '\\', 'n', '</s>']
Filtered   (023): ['density', '=', 'sp', '.', 'array', '(', '[', 'sp', '.', 'roots', '(', 'C', ')', 'for', 'C', 'in', 'co', 'eff', 's', ']', ')', '\\', 'n']
Detokenized (020): ['density', '=', 'sp', '.', 'array', '(', '[', 'sp', '.', 'roots', '(', 'C', ')', 'for', 'C', 'in', 'coeffs', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "comp2 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n"
Original    (015): ['comp2', '=', 'OpenPNM', '.', 'Phases', '.', 'GenericPhase', '(', 'network', '=', 'self', '.', 'net', ')', '\\n']
Tokenized   (023): ['<s>', 'comp', '2', '=', 'Open', 'PN', 'M', '.', 'Ph', 'ases', '.', 'Generic', 'Phase', '(', 'network', '=', 'self', '.', 'net', ')', '\\', 'n', '</s>']
Filtered   (021): ['comp', '2', '=', 'Open', 'PN', 'M', '.', 'Ph', 'ases', '.', 'Generic', 'Phase', '(', 'network', '=', 'self', '.', 'net', ')', '\\', 'n']
Detokenized (015): ['comp2', '=', 'OpenPNM', '.', 'Phases', '.', 'GenericPhase', '(', 'network', '=', 'self', '.', 'net', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "OpenPNM . Phases . GenericPhase ( network = self . net , \n"
Original    (013): ['OpenPNM', '.', 'Phases', '.', 'GenericPhase', '(', 'network', '=', 'self', '.', 'net', ',', '\\n']
Tokenized   (020): ['<s>', 'Open', 'PN', 'M', '.', 'Ph', 'ases', '.', 'Generic', 'Phase', '(', 'network', '=', 'self', '.', 'net', ',', '\\', 'n', '</s>']
Filtered   (018): ['Open', 'PN', 'M', '.', 'Ph', 'ases', '.', 'Generic', 'Phase', '(', 'network', '=', 'self', '.', 'net', ',', '\\', 'n']
Detokenized (013): ['OpenPNM', '.', 'Phases', '.', 'GenericPhase', '(', 'network', '=', 'self', '.', 'net', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "components = [ comp1 , comp2 ] ) \n"
Original    (009): ['components', '=', '[', 'comp1', ',', 'comp2', ']', ')', '\\n']
Tokenized   (015): ['<s>', 'comp', 'onents', '=', '[', 'comp', '1', ',', 'comp', '2', ']', ')', '\\', 'n', '</s>']
Filtered   (013): ['comp', 'onents', '=', '[', 'comp', '1', ',', 'comp', '2', ']', ')', '\\', 'n']
Detokenized (009): ['components', '=', '[', 'comp1', ',', 'comp2', ']', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "phase . set_component ( comp2 , mode = ) \n"
Original    (010): ['phase', '.', 'set_component', '(', 'comp2', ',', 'mode', '=', ')', '\\n']
Tokenized   (016): ['<s>', 'phase', '.', 'set', '_', 'component', '(', 'comp', '2', ',', 'mode', '=', ')', '\\', 'n', '</s>']
Filtered   (014): ['phase', '.', 'set', '_', 'component', '(', 'comp', '2', ',', 'mode', '=', ')', '\\', 'n']
Detokenized (010): ['phase', '.', 'set_component', '(', 'comp2', ',', 'mode', '=', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "best_seq = fd [ x ] . sequence \n"
Original    (009): ['best_seq', '=', 'fd', '[', 'x', ']', '.', 'sequence', '\\n']
Tokenized   (015): ['<s>', 'best', '_', 'seq', '=', 'f', 'd', '[', 'x', ']', '.', 'sequence', '\\', 'n', '</s>']
Filtered   (013): ['best', '_', 'seq', '=', 'f', 'd', '[', 'x', ']', '.', 'sequence', '\\', 'n']
Detokenized (009): ['best_seq', '=', 'fd', '[', 'x', ']', '.', 'sequence', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "best_id , best_seq , best_qual = rep_info [ pb_id ] \n"
Original    (011): ['best_id', ',', 'best_seq', ',', 'best_qual', '=', 'rep_info', '[', 'pb_id', ']', '\\n']
Tokenized   (025): ['<s>', 'best', '_', 'id', ',', 'best', '_', 'seq', ',', 'best', '_', 'qual', '=', 'rep', '_', 'info', '[', 'p', 'b', '_', 'id', ']', '\\', 'n', '</s>']
Filtered   (023): ['best', '_', 'id', ',', 'best', '_', 'seq', ',', 'best', '_', 'qual', '=', 'rep', '_', 'info', '[', 'p', 'b', '_', 'id', ']', '\\', 'n']
Detokenized (011): ['best_id', ',', 'best_seq', ',', 'best_qual', '=', 'rep_info', '[', 'pb_id', ']', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "_id_ = "{0}|{1}|{2}" . format ( pb_id , coords [ best_id ] , best_id ) \n"
Original    (016): ['_id_', '=', '"{0}|{1}|{2}"', '.', 'format', '(', 'pb_id', ',', 'coords', '[', 'best_id', ']', ',', 'best_id', ')', '\\n']
Tokenized   (039): ['<s>', '_', 'id', '_', '=', '"{', '0', '}', '|', '{', '1', '}', '|', '{', '2', '}"', '.', 'format', '(', 'p', 'b', '_', 'id', ',', 'co', 'ords', '[', 'best', '_', 'id', ']', ',', 'best', '_', 'id', ')', '\\', 'n', '</s>']
Filtered   (037): ['_', 'id', '_', '=', '"{', '0', '}', '|', '{', '1', '}', '|', '{', '2', '}"', '.', 'format', '(', 'p', 'b', '_', 'id', ',', 'co', 'ords', '[', 'best', '_', 'id', ']', ',', 'best', '_', 'id', ')', '\\', 'n']
Detokenized (016): ['_id_', '=', '"{0}|{1}|{2}"', '.', 'format', '(', 'pb_id', ',', 'coords', '[', 'best_id', ']', ',', 'best_id', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "iter = BioReaders . GMAPSAMReader ( gmap_sam_filename , True , query_len_dict = transfrag_len_dict ) \n"
Original    (015): ['iter', '=', 'BioReaders', '.', 'GMAPSAMReader', '(', 'gmap_sam_filename', ',', 'True', ',', 'query_len_dict', '=', 'transfrag_len_dict', ')', '\\n']
Tokenized   (038): ['<s>', 'iter', '=', 'Bio', 'Read', 'ers', '.', 'GM', 'APS', 'AM', 'Reader', '(', 'g', 'map', '_', 'sam', '_', 'filename', ',', 'True', ',', 'query', '_', 'len', '_', 'dict', '=', 'trans', 'fr', 'ag', '_', 'len', '_', 'dict', ')', '\\', 'n', '</s>']
Filtered   (036): ['iter', '=', 'Bio', 'Read', 'ers', '.', 'GM', 'APS', 'AM', 'Reader', '(', 'g', 'map', '_', 'sam', '_', 'filename', ',', 'True', ',', 'query', '_', 'len', '_', 'dict', '=', 'trans', 'fr', 'ag', '_', 'len', '_', 'dict', ')', '\\', 'n']
Detokenized (015): ['iter', '=', 'BioReaders', '.', 'GMAPSAMReader', '(', 'gmap_sam_filename', ',', 'True', ',', 'query_len_dict', '=', 'transfrag_len_dict', ')', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "TmpRec = namedtuple ( , [ , , , , , , ] ) \n"
Original    (015): ['TmpRec', '=', 'namedtuple', '(', ',', '[', ',', ',', ',', ',', ',', ',', ']', ')', '\\n']
Tokenized   (022): ['<s>', 'T', 'mp', 'Rec', '=', 'named', 't', 'uple', '(', ',', '[', ',', ',', ',', ',', ',', ',', ']', ')', '\\', 'n', '</s>']
Filtered   (020): ['T', 'mp', 'Rec', '=', 'named', 't', 'uple', '(', ',', '[', ',', ',', ',', ',', ',', ',', ']', ')', '\\', 'n']
Detokenized (015): ['TmpRec', '=', 'namedtuple', '(', ',', '[', ',', ',', ',', ',', ',', ',', ']', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "compressed_records_pointer_dict = defaultdict ( lambda : [ ] ) \n"
Original    (010): ['compressed_records_pointer_dict', '=', 'defaultdict', '(', 'lambda', ':', '[', ']', ')', '\\n']
Tokenized   (022): ['<s>', 'comp', 'ressed', '_', 'rec', 'ords', '_', 'pointer', '_', 'dict', '=', 'default', 'dict', '(', 'lambda', ':', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (020): ['comp', 'ressed', '_', 'rec', 'ords', '_', 'pointer', '_', 'dict', '=', 'default', 'dict', '(', 'lambda', ':', '[', ']', ')', '\\', 'n']
Detokenized (010): ['compressed_records_pointer_dict', '=', 'defaultdict', '(', 'lambda', ':', '[', ']', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "check_ids_unique ( fa_or_fq_filename , is_fq = is_fq ) \n"
Original    (009): ['check_ids_unique', '(', 'fa_or_fq_filename', ',', 'is_fq', '=', 'is_fq', ')', '\\n']
Tokenized   (029): ['<s>', 'check', '_', 'ids', '_', 'unique', '(', 'fa', '_', 'or', '_', 'f', 'q', '_', 'filename', ',', 'is', '_', 'f', 'q', '=', 'is', '_', 'f', 'q', ')', '\\', 'n', '</s>']
Filtered   (027): ['check', '_', 'ids', '_', 'unique', '(', 'fa', '_', 'or', '_', 'f', 'q', '_', 'filename', ',', 'is', '_', 'f', 'q', '=', 'is', '_', 'f', 'q', ')', '\\', 'n']
Detokenized (009): ['check_ids_unique', '(', 'fa_or_fq_filename', ',', 'is_fq', '=', 'is_fq', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "fusion_candidates = find_fusion_candidates ( sam_filename , bs . transfrag_len_dict , min_locus_coverage \n"
Original    (012): ['fusion_candidates', '=', 'find_fusion_candidates', '(', 'sam_filename', ',', 'bs', '.', 'transfrag_len_dict', ',', 'min_locus_coverage', '\\n']
Tokenized   (040): ['<s>', 'f', 'usion', '_', 'cand', 'idates', '=', 'find', '_', 'f', 'usion', '_', 'cand', 'idates', '(', 'sam', '_', 'filename', ',', 'b', 's', '.', 'trans', 'fr', 'ag', '_', 'len', '_', 'dict', ',', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', '\\', 'n', '</s>']
Filtered   (038): ['f', 'usion', '_', 'cand', 'idates', '=', 'find', '_', 'f', 'usion', '_', 'cand', 'idates', '(', 'sam', '_', 'filename', ',', 'b', 's', '.', 'trans', 'fr', 'ag', '_', 'len', '_', 'dict', ',', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', '\\', 'n']
Detokenized (012): ['fusion_candidates', '=', 'find_fusion_candidates', '(', 'sam_filename', ',', 'bs', '.', 'transfrag_len_dict', ',', 'min_locus_coverage', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "pbid1 , groups1 = line . strip ( ) . split ( ) \n"
Original    (014): ['pbid1', ',', 'groups1', '=', 'line', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Tokenized   (020): ['<s>', 'p', 'bid', '1', ',', 'groups', '1', '=', 'line', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n', '</s>']
Filtered   (018): ['p', 'bid', '1', ',', 'groups', '1', '=', 'line', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n']
Detokenized (014): ['pbid1', ',', 'groups1', '=', 'line', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "pbid2 , groups2 = f . readline ( ) . strip ( ) . split ( ) \n"
Original    (018): ['pbid2', ',', 'groups2', '=', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Tokenized   (025): ['<s>', 'p', 'bid', '2', ',', 'groups', '2', '=', 'f', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n', '</s>']
Filtered   (023): ['p', 'bid', '2', ',', 'groups', '2', '=', 'f', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n']
Detokenized (018): ['pbid2', ',', 'groups2', '=', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "f_group . write ( "{0}\\t{1}\\n" . format ( pbid1 [ : pbid1 . rfind ( ) ] , "," . join ( group ) ) ) \n"
Original    (027): ['f_group', '.', 'write', '(', '"{0}\\\\t{1}\\\\n"', '.', 'format', '(', 'pbid1', '[', ':', 'pbid1', '.', 'rfind', '(', ')', ']', ',', '","', '.', 'join', '(', 'group', ')', ')', ')', '\\n']
Tokenized   (048): ['<s>', 'f', '_', 'group', '.', 'write', '(', '"{', '0', '}', '\\\\', 't', '{', '1', '}', '\\\\', 'n', '"', '.', 'format', '(', 'p', 'bid', '1', '[', ':', 'p', 'bid', '1', '.', 'r', 'find', '(', ')', ']', ',', '"', ',"', '.', 'join', '(', 'group', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (046): ['f', '_', 'group', '.', 'write', '(', '"{', '0', '}', '\\\\', 't', '{', '1', '}', '\\\\', 'n', '"', '.', 'format', '(', 'p', 'bid', '1', '[', ':', 'p', 'bid', '1', '.', 'r', 'find', '(', ')', ']', ',', '"', ',"', '.', 'join', '(', 'group', ')', ')', ')', '\\', 'n']
Detokenized (027): ['f_group', '.', 'write', '(', '"{0}\\\\t{1}\\\\n"', '.', 'format', '(', 'pbid1', '[', ':', 'pbid1', '.', 'rfind', '(', ')', ']', ',', '","', '.', 'join', '(', 'group', ')', ')', ')', '\\n']
Counter: 46
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "group_info [ pbid1 [ : pbid1 . rfind ( ) ] ] = list ( group ) \n"
Original    (018): ['group_info', '[', 'pbid1', '[', ':', 'pbid1', '.', 'rfind', '(', ')', ']', ']', '=', 'list', '(', 'group', ')', '\\n']
Tokenized   (028): ['<s>', 'group', '_', 'info', '[', 'p', 'bid', '1', '[', ':', 'p', 'bid', '1', '.', 'r', 'find', '(', ')', ']', ']', '=', 'list', '(', 'group', ')', '\\', 'n', '</s>']
Filtered   (026): ['group', '_', 'info', '[', 'p', 'bid', '1', '[', ':', 'p', 'bid', '1', '.', 'r', 'find', '(', ')', ']', ']', '=', 'list', '(', 'group', ')', '\\', 'n']
Detokenized (018): ['group_info', '[', 'pbid1', '[', ':', 'pbid1', '.', 'rfind', '(', ')', ']', ']', '=', 'list', '(', 'group', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "d1 . update ( d [ ] ) \n"
Original    (009): ['d1', '.', 'update', '(', 'd', '[', ']', ')', '\\n']
Tokenized   (013): ['<s>', 'd', '1', '.', 'update', '(', 'd', '[', ']', ')', '\\', 'n', '</s>']
Filtered   (011): ['d', '1', '.', 'update', '(', 'd', '[', ']', ')', '\\', 'n']
Detokenized (009): ['d1', '.', 'update', '(', 'd', '[', ']', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "fusion_main ( args . input , args . sam , args . prefix , \n"
Original    (015): ['fusion_main', '(', 'args', '.', 'input', ',', 'args', '.', 'sam', ',', 'args', '.', 'prefix', ',', '\\n']
Tokenized   (021): ['<s>', 'f', 'usion', '_', 'main', '(', 'args', '.', 'input', ',', 'args', '.', 'sam', ',', 'args', '.', 'prefix', ',', '\\', 'n', '</s>']
Filtered   (019): ['f', 'usion', '_', 'main', '(', 'args', '.', 'input', ',', 'args', '.', 'sam', ',', 'args', '.', 'prefix', ',', '\\', 'n']
Detokenized (015): ['fusion_main', '(', 'args', '.', 'input', ',', 'args', '.', 'sam', ',', 'args', '.', 'prefix', ',', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "is_fq = args . fq , allow_extra_5_exons = args . allow_extra_5exon , \n"
Original    (013): ['is_fq', '=', 'args', '.', 'fq', ',', 'allow_extra_5_exons', '=', 'args', '.', 'allow_extra_5exon', ',', '\\n']
Tokenized   (033): ['<s>', 'is', '_', 'f', 'q', '=', 'args', '.', 'f', 'q', ',', 'allow', '_', 'extra', '_', '5', '_', 'ex', 'ons', '=', 'args', '.', 'allow', '_', 'extra', '_', '5', 'ex', 'on', ',', '\\', 'n', '</s>']
Filtered   (031): ['is', '_', 'f', 'q', '=', 'args', '.', 'f', 'q', ',', 'allow', '_', 'extra', '_', '5', '_', 'ex', 'ons', '=', 'args', '.', 'allow', '_', 'extra', '_', '5', 'ex', 'on', ',', '\\', 'n']
Detokenized (013): ['is_fq', '=', 'args', '.', 'fq', ',', 'allow_extra_5_exons', '=', 'args', '.', 'allow_extra_5exon', ',', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "skip_5_exon_alt = False , prefix_dict_pickle_filename = args . prefix_dict_pickle_filename , min_locus_coverage = args . min_locus_coverage , min_locus_coverage_bp = args . min_locus_coverage_bp min_total_coverage = args . min_total_coverage , \n"
Original    (028): ['skip_5_exon_alt', '=', 'False', ',', 'prefix_dict_pickle_filename', '=', 'args', '.', 'prefix_dict_pickle_filename', ',', 'min_locus_coverage', '=', 'args', '.', 'min_locus_coverage', ',', 'min_locus_coverage_bp', '=', 'args', '.', 'min_locus_coverage_bp', 'min_total_coverage', '=', 'args', '.', 'min_total_coverage', ',', '\\n']
Tokenized   (090): ['<s>', 'skip', '_', '5', '_', 'ex', 'on', '_', 'alt', '=', 'False', ',', 'prefix', '_', 'dict', '_', 'pick', 'le', '_', 'filename', '=', 'args', '.', 'prefix', '_', 'dict', '_', 'pick', 'le', '_', 'filename', ',', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', '=', 'args', '.', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', ',', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', '_', 'bp', '=', 'args', '.', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', '_', 'bp', 'min', '_', 'total', '_', 'co', 'verage', '=', 'args', '.', 'min', '_', 'total', '_', 'co', 'verage', ',', '\\', 'n', '</s>']
Filtered   (088): ['skip', '_', '5', '_', 'ex', 'on', '_', 'alt', '=', 'False', ',', 'prefix', '_', 'dict', '_', 'pick', 'le', '_', 'filename', '=', 'args', '.', 'prefix', '_', 'dict', '_', 'pick', 'le', '_', 'filename', ',', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', '=', 'args', '.', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', ',', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', '_', 'bp', '=', 'args', '.', 'min', '_', 'l', 'ocus', '_', 'co', 'verage', '_', 'bp', 'min', '_', 'total', '_', 'co', 'verage', '=', 'args', '.', 'min', '_', 'total', '_', 'co', 'verage', ',', '\\', 'n']
Detokenized (028): ['skip_5_exon_alt', '=', 'False', ',', 'prefix_dict_pickle_filename', '=', 'args', '.', 'prefix_dict_pickle_filename', ',', 'min_locus_coverage', '=', 'args', '.', 'min_locus_coverage', ',', 'min_locus_coverage_bp', '=', 'args', '.', 'min_locus_coverage_bp', 'min_total_coverage', '=', 'args', '.', 'min_total_coverage', ',', '\\n']
Counter: 88
===================================================================
Hidden states:  (13, 28, 768)
# Extracted words:  28
Sentence         : "raw = self . f . readline ( ) . strip ( ) . split ( ) \n"
Original    (018): ['raw', '=', 'self', '.', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Tokenized   (022): ['<s>', 'raw', '=', 'self', '.', 'f', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n', '</s>']
Filtered   (020): ['raw', '=', 'self', '.', 'f', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n']
Detokenized (018): ['raw', '=', 'self', '.', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "iden = float ( raw [ 3 ] ) \n"
Original    (010): ['iden', '=', 'float', '(', 'raw', '[', '3', ']', ')', '\\n']
Tokenized   (013): ['<s>', 'iden', '=', 'float', '(', 'raw', '[', '3', ']', ')', '\\', 'n', '</s>']
Filtered   (011): ['iden', '=', 'float', '(', 'raw', '[', '3', ']', ')', '\\', 'n']
Detokenized (010): ['iden', '=', 'float', '(', 'raw', '[', '3', ']', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "_qStart , qAln = self . f . readline ( ) . strip ( ) . split ( ) \n"
Original    (020): ['_qStart', ',', 'qAln', '=', 'self', '.', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Tokenized   (028): ['<s>', '_', 'q', 'Start', ',', 'q', 'Al', 'n', '=', 'self', '.', 'f', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n', '</s>']
Filtered   (026): ['_', 'q', 'Start', ',', 'q', 'Al', 'n', '=', 'self', '.', 'f', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\', 'n']
Detokenized (020): ['_qStart', ',', 'qAln', '=', 'self', '.', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "_sStart , sAln = self . f . readline ( ) . strip ( ) . split ( ) [ : 2 ] \n"
Original    (024): ['_sStart', ',', 'sAln', '=', 'self', '.', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '[', ':', '2', ']', '\\n']
Tokenized   (032): ['<s>', '_', 's', 'Start', ',', 's', 'Al', 'n', '=', 'self', '.', 'f', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '[', ':', '2', ']', '\\', 'n', '</s>']
Filtered   (030): ['_', 's', 'Start', ',', 's', 'Al', 'n', '=', 'self', '.', 'f', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '[', ':', '2', ']', '\\', 'n']
Detokenized (024): ['_sStart', ',', 'sAln', '=', 'self', '.', 'f', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '.', 'split', '(', ')', '[', ':', '2', ']', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "missed_q = missed_q * 1. / r . qLength , \n"
Original    (011): ['missed_q', '=', 'missed_q', '*', '1.', '/', 'r', '.', 'qLength', ',', '\\n']
Tokenized   (021): ['<s>', 'miss', 'ed', '_', 'q', '=', 'missed', '_', 'q', '*', '1', '.', '/', 'r', '.', 'q', 'Length', ',', '\\', 'n', '</s>']
Filtered   (019): ['miss', 'ed', '_', 'q', '=', 'missed', '_', 'q', '*', '1', '.', '/', 'r', '.', 'q', 'Length', ',', '\\', 'n']
Detokenized (011): ['missed_q', '=', 'missed_q', '*', '1.', '/', 'r', '.', 'qLength', ',', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "ece_penalty , ece_min_len ) : \n"
Original    (006): ['ece_penalty', ',', 'ece_min_len', ')', ':', '\\n']
Tokenized   (018): ['<s>', 'e', 'ce', '_', 'pen', 'alty', ',', 'e', 'ce', '_', 'min', '_', 'len', ')', ':', '\\', 'n', '</s>']
Filtered   (016): ['e', 'ce', '_', 'pen', 'alty', ',', 'e', 'ce', '_', 'min', '_', 'len', ')', ':', '\\', 'n']
Detokenized (006): ['ece_penalty', ',', 'ece_min_len', ')', ':', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "heading = % ( current_indent , , self . heading ) \n"
Original    (012): ['heading', '=', '%', '(', 'current_indent', ',', ',', 'self', '.', 'heading', ')', '\\n']
Tokenized   (018): ['<s>', 'heading', '=', '%', '(', 'current', '_', 'ind', 'ent', ',', ',', 'self', '.', 'heading', ')', '\\', 'n', '</s>']
Filtered   (016): ['heading', '=', '%', '(', 'current', '_', 'ind', 'ent', ',', ',', 'self', '.', 'heading', ')', '\\', 'n']
Detokenized (012): ['heading', '=', '%', '(', 'current_indent', ',', ',', 'self', '.', 'heading', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "section = self . _Section ( self , self . _current_section , heading ) \n"
Original    (015): ['section', '=', 'self', '.', '_Section', '(', 'self', ',', 'self', '.', '_current_section', ',', 'heading', ')', '\\n']
Tokenized   (022): ['<s>', 'section', '=', 'self', '.', '_', 'Section', '(', 'self', ',', 'self', '.', '_', 'current', '_', 'section', ',', 'heading', ')', '\\', 'n', '</s>']
Filtered   (020): ['section', '=', 'self', '.', '_', 'Section', '(', 'self', ',', 'self', '.', '_', 'current', '_', 'section', ',', 'heading', ')', '\\', 'n']
Detokenized (015): ['section', '=', 'self', '.', '_Section', '(', 'self', ',', 'self', '.', '_current_section', ',', 'heading', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "invocations = [ get_invocation ( action ) ] \n"
Original    (009): ['invocations', '=', '[', 'get_invocation', '(', 'action', ')', ']', '\\n']
Tokenized   (016): ['<s>', 'inv', 'ocations', '=', '[', 'get', '_', 'inv', 'ocation', '(', 'action', ')', ']', '\\', 'n', '</s>']
Filtered   (014): ['inv', 'ocations', '=', '[', 'get', '_', 'inv', 'ocation', '(', 'action', ')', ']', '\\', 'n']
Detokenized (009): ['invocations', '=', '[', 'get_invocation', '(', 'action', ')', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "action_usage = format ( optionals + positionals , groups ) \n"
Original    (011): ['action_usage', '=', 'format', '(', 'optionals', '+', 'positionals', ',', 'groups', ')', '\\n']
Tokenized   (018): ['<s>', 'action', '_', 'usage', '=', 'format', '(', 'option', 'als', '+', 'position', 'als', ',', 'groups', ')', '\\', 'n', '</s>']
Filtered   (016): ['action', '_', 'usage', '=', 'format', '(', 'option', 'als', '+', 'position', 'als', ',', 'groups', ')', '\\', 'n']
Detokenized (011): ['action_usage', '=', 'format', '(', 'optionals', '+', 'positionals', ',', 'groups', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "text_width = self . _width - self . _current_indent \n"
Original    (010): ['text_width', '=', 'self', '.', '_width', '-', 'self', '.', '_current_indent', '\\n']
Tokenized   (020): ['<s>', 'text', '_', 'width', '=', 'self', '.', '_', 'width', '-', 'self', '.', '_', 'current', '_', 'ind', 'ent', '\\', 'n', '</s>']
Filtered   (018): ['text', '_', 'width', '=', 'self', '.', '_', 'width', '-', 'self', '.', '_', 'current', '_', 'ind', 'ent', '\\', 'n']
Detokenized (010): ['text_width', '=', 'self', '.', '_width', '-', 'self', '.', '_current_indent', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "line_len += len ( part ) + 1 \n"
Original    (009): ['line_len', '+=', 'len', '(', 'part', ')', '+', '1', '\\n']
Tokenized   (014): ['<s>', 'line', '_', 'len', '+=', 'len', '(', 'part', ')', '+', '1', '\\', 'n', '</s>']
Filtered   (012): ['line', '_', 'len', '+=', 'len', '(', 'part', ')', '+', '1', '\\', 'n']
Detokenized (009): ['line_len', '+=', 'len', '(', 'part', ')', '+', '1', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "part = % ( option_string , args_string ) \n"
Original    (009): ['part', '=', '%', '(', 'option_string', ',', 'args_string', ')', '\\n']
Tokenized   (016): ['<s>', 'part', '=', '%', '(', 'option', '_', 'string', ',', 'args', '_', 'string', ')', '\\', 'n', '</s>']
Filtered   (014): ['part', '=', '%', '(', 'option', '_', 'string', ',', 'args', '_', 'string', ')', '\\', 'n']
Detokenized (009): ['part', '=', '%', '(', 'option_string', ',', 'args_string', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "indent = * self . _current_indent \n"
Original    (007): ['indent', '=', '*', 'self', '.', '_current_indent', '\\n']
Tokenized   (015): ['<s>', 'ind', 'ent', '=', '*', 'self', '.', '_', 'current', '_', 'ind', 'ent', '\\', 'n', '</s>']
Filtered   (013): ['ind', 'ent', '=', '*', 'self', '.', '_', 'current', '_', 'ind', 'ent', '\\', 'n']
Detokenized (007): ['indent', '=', '*', 'self', '.', '_current_indent', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "help_width = self . _width - help_position \n"
Original    (008): ['help_width', '=', 'self', '.', '_width', '-', 'help_position', '\\n']
Tokenized   (016): ['<s>', 'help', '_', 'width', '=', 'self', '.', '_', 'width', '-', 'help', '_', 'position', '\\', 'n', '</s>']
Filtered   (014): ['help', '_', 'width', '=', 'self', '.', '_', 'width', '-', 'help', '_', 'position', '\\', 'n']
Detokenized (008): ['help_width', '=', 'self', '.', '_width', '-', 'help_position', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "action_width = help_position - self . _current_indent - 2 \n"
Original    (010): ['action_width', '=', 'help_position', '-', 'self', '.', '_current_indent', '-', '2', '\\n']
Tokenized   (021): ['<s>', 'action', '_', 'width', '=', 'help', '_', 'position', '-', 'self', '.', '_', 'current', '_', 'ind', 'ent', '-', '2', '\\', 'n', '</s>']
Filtered   (019): ['action', '_', 'width', '=', 'help', '_', 'position', '-', 'self', '.', '_', 'current', '_', 'ind', 'ent', '-', '2', '\\', 'n']
Detokenized (010): ['action_width', '=', 'help_position', '-', 'self', '.', '_current_indent', '-', '2', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "sup . __init__ ( option_strings = [ ] , dest = name , help = help ) \n"
Original    (018): ['sup', '.', '__init__', '(', 'option_strings', '=', '[', ']', ',', 'dest', '=', 'name', ',', 'help', '=', 'help', ')', '\\n']
Tokenized   (025): ['<s>', 'sup', '.', '__', 'init', '__', '(', 'option', '_', 'strings', '=', '[', ']', ',', 'dest', '=', 'name', ',', 'help', '=', 'help', ')', '\\', 'n', '</s>']
Filtered   (023): ['sup', '.', '__', 'init', '__', '(', 'option', '_', 'strings', '=', '[', ']', ',', 'dest', '=', 'name', ',', 'help', '=', 'help', ')', '\\', 'n']
Detokenized (018): ['sup', '.', '__init__', '(', 'option_strings', '=', '[', ']', ',', 'dest', '=', 'name', ',', 'help', '=', 'help', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "arg_strings = values [ 1 : ] \n"
Original    (008): ['arg_strings', '=', 'values', '[', '1', ':', ']', '\\n']
Tokenized   (013): ['<s>', 'arg', '_', 'strings', '=', 'values', '[', '1', ':', ']', '\\', 'n', '</s>']
Filtered   (011): ['arg', '_', 'strings', '=', 'values', '[', '1', ':', ']', '\\', 'n']
Detokenized (008): ['arg_strings', '=', 'values', '[', '1', ':', ']', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "args_str = . join ( [ repr ( arg ) for arg in args if arg is not None ] ) \n"
Original    (022): ['args_str', '=', '.', 'join', '(', '[', 'repr', '(', 'arg', ')', 'for', 'arg', 'in', 'args', 'if', 'arg', 'is', 'not', 'None', ']', ')', '\\n']
Tokenized   (027): ['<s>', 'args', '_', 'str', '=', '.', 'join', '(', '[', 'repr', '(', 'arg', ')', 'for', 'arg', 'in', 'args', 'if', 'arg', 'is', 'not', 'None', ']', ')', '\\', 'n', '</s>']
Filtered   (025): ['args', '_', 'str', '=', '.', 'join', '(', '[', 'repr', '(', 'arg', ')', 'for', 'arg', 'in', 'args', 'if', 'arg', 'is', 'not', 'None', ']', ')', '\\', 'n']
Detokenized (022): ['args_str', '=', '.', 'join', '(', '[', 'repr', '(', 'arg', ')', 'for', 'arg', 'in', 'args', 'if', 'arg', 'is', 'not', 'None', ']', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "type_func = self . _registry_get ( , action . type , action . type ) \n"
Original    (016): ['type_func', '=', 'self', '.', '_registry_get', '(', ',', 'action', '.', 'type', ',', 'action', '.', 'type', ')', '\\n']
Tokenized   (025): ['<s>', 'type', '_', 'func', '=', 'self', '.', '_', 'reg', 'istry', '_', 'get', '(', ',', 'action', '.', 'type', ',', 'action', '.', 'type', ')', '\\', 'n', '</s>']
Filtered   (023): ['type', '_', 'func', '=', 'self', '.', '_', 'reg', 'istry', '_', 'get', '(', ',', 'action', '.', 'type', ',', 'action', '.', 'type', ')', '\\', 'n']
Detokenized (016): ['type_func', '=', 'self', '.', '_registry_get', '(', ',', 'action', '.', 'type', ',', 'action', '.', 'type', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "conflict_string = . join ( [ option_string \n"
Original    (008): ['conflict_string', '=', '.', 'join', '(', '[', 'option_string', '\\n']
Tokenized   (016): ['<s>', 'conf', 'lict', '_', 'string', '=', '.', 'join', '(', '[', 'option', '_', 'string', '\\', 'n', '</s>']
Filtered   (014): ['conf', 'lict', '_', 'string', '=', '.', 'join', '(', '[', 'option', '_', 'string', '\\', 'n']
Detokenized (008): ['conflict_string', '=', '.', 'join', '(', '[', 'option_string', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "in conflicting_actions ] ) \n"
Original    (005): ['in', 'conflicting_actions', ']', ')', '\\n']
Tokenized   (010): ['<s>', 'in', 'conflicting', '_', 'actions', ']', ')', '\\', 'n', '</s>']
Filtered   (008): ['in', 'conflicting', '_', 'actions', ']', ')', '\\', 'n']
Detokenized (005): ['in', 'conflicting_actions', ']', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "super_init ( description = description , ** kwargs ) \n"
Original    (010): ['super_init', '(', 'description', '=', 'description', ',', '**', 'kwargs', ')', '\\n']
Tokenized   (017): ['<s>', 'super', '_', 'init', '(', 'description', '=', 'description', ',', '**', 'k', 'w', 'args', ')', '\\', 'n', '</s>']
Filtered   (015): ['super', '_', 'init', '(', 'description', '=', 'description', ',', '**', 'k', 'w', 'args', ')', '\\', 'n']
Detokenized (010): ['super_init', '(', 'description', '=', 'description', ',', '**', 'kwargs', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : """"instead""" , DeprecationWarning ) \n"
Original    (005): ['"""instead"""', ',', 'DeprecationWarning', ')', '\\n']
Tokenized   (013): ['<s>', '"""', 'instead', '"""', ',', 'Dep', 'rec', 'ation', 'Warning', ')', '\\', 'n', '</s>']
Filtered   (011): ['"""', 'instead', '"""', ',', 'Dep', 'rec', 'ation', 'Warning', ')', '\\', 'n']
Detokenized (005): ['"""instead"""', ',', 'DeprecationWarning', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "superinit ( description = description , \n"
Original    (007): ['superinit', '(', 'description', '=', 'description', ',', '\\n']
Tokenized   (011): ['<s>', 'super', 'init', '(', 'description', '=', 'description', ',', '\\', 'n', '</s>']
Filtered   (009): ['super', 'init', '(', 'description', '=', 'description', ',', '\\', 'n']
Detokenized (007): ['superinit', '(', 'description', '=', 'description', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "default_prefix + , default_prefix * 2 + , \n"
Original    (009): ['default_prefix', '+', ',', 'default_prefix', '*', '2', '+', ',', '\\n']
Tokenized   (016): ['<s>', 'default', '_', 'prefix', '+', ',', 'default', '_', 'prefix', '*', '2', '+', ',', '\\', 'n', '</s>']
Filtered   (014): ['default', '_', 'prefix', '+', ',', 'default', '_', 'prefix', '*', '2', '+', ',', '\\', 'n']
Detokenized (009): ['default_prefix', '+', ',', 'default_prefix', '*', '2', '+', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "conflicts . extend ( group_actions [ i + 1 : ] ) \n"
Original    (013): ['conflicts', '.', 'extend', '(', 'group_actions', '[', 'i', '+', '1', ':', ']', ')', '\\n']
Tokenized   (019): ['<s>', 'conf', 'licts', '.', 'extend', '(', 'group', '_', 'actions', '[', 'i', '+', '1', ':', ']', ')', '\\', 'n', '</s>']
Filtered   (017): ['conf', 'licts', '.', 'extend', '(', 'group', '_', 'actions', '[', 'i', '+', '1', ':', ']', ')', '\\', 'n']
Detokenized (013): ['conflicts', '.', 'extend', '(', 'group_actions', '[', 'i', '+', '1', ':', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "action , option_string , explicit_arg = option_tuple \n"
Original    (008): ['action', ',', 'option_string', ',', 'explicit_arg', '=', 'option_tuple', '\\n']
Tokenized   (018): ['<s>', 'action', ',', 'option', '_', 'string', ',', 'explicit', '_', 'arg', '=', 'option', '_', 't', 'uple', '\\', 'n', '</s>']
Filtered   (016): ['action', ',', 'option', '_', 'string', ',', 'explicit', '_', 'arg', '=', 'option', '_', 't', 'uple', '\\', 'n']
Detokenized (008): ['action', ',', 'option_string', ',', 'explicit_arg', '=', 'option_tuple', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "option_string = char + explicit_arg [ 0 ] \n"
Original    (009): ['option_string', '=', 'char', '+', 'explicit_arg', '[', '0', ']', '\\n']
Tokenized   (016): ['<s>', 'option', '_', 'string', '=', 'char', '+', 'explicit', '_', 'arg', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (014): ['option', '_', 'string', '=', 'char', '+', 'explicit', '_', 'arg', '[', '0', ']', '\\', 'n']
Detokenized (009): ['option_string', '=', 'char', '+', 'explicit_arg', '[', '0', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "new_explicit_arg = explicit_arg [ 1 : ] or None \n"
Original    (010): ['new_explicit_arg', '=', 'explicit_arg', '[', '1', ':', ']', 'or', 'None', '\\n']
Tokenized   (020): ['<s>', 'new', '_', 'expl', 'icit', '_', 'arg', '=', 'explicit', '_', 'arg', '[', '1', ':', ']', 'or', 'None', '\\', 'n', '</s>']
Filtered   (018): ['new', '_', 'expl', 'icit', '_', 'arg', '=', 'explicit', '_', 'arg', '[', '1', ':', ']', 'or', 'None', '\\', 'n']
Detokenized (010): ['new_explicit_arg', '=', 'explicit_arg', '[', '1', ':', ']', 'or', 'None', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "action_tuples . append ( ( action , args , option_string ) ) \n"
Original    (013): ['action_tuples', '.', 'append', '(', '(', 'action', ',', 'args', ',', 'option_string', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'action', '_', 'tu', 'ples', '.', 'append', '(', '(', 'action', ',', 'args', ',', 'option', '_', 'string', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['action', '_', 'tu', 'ples', '.', 'append', '(', '(', 'action', ',', 'args', ',', 'option', '_', 'string', ')', ')', '\\', 'n']
Detokenized (013): ['action_tuples', '.', 'append', '(', '(', 'action', ',', 'args', ',', 'option_string', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "selected_patterns = arg_strings_pattern [ start : ] \n"
Original    (008): ['selected_patterns', '=', 'arg_strings_pattern', '[', 'start', ':', ']', '\\n']
Tokenized   (018): ['<s>', 'selected', '_', 'pattern', 's', '=', 'arg', '_', 'strings', '_', 'pattern', '[', 'start', ':', ']', '\\', 'n', '</s>']
Filtered   (016): ['selected', '_', 'pattern', 's', '=', 'arg', '_', 'strings', '_', 'pattern', '[', 'start', ':', ']', '\\', 'n']
Detokenized (008): ['selected_patterns', '=', 'arg_strings_pattern', '[', 'start', ':', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "extras . extend ( arg_strings [ stop_index : ] ) \n"
Original    (011): ['extras', '.', 'extend', '(', 'arg_strings', '[', 'stop_index', ':', ']', ')', '\\n']
Tokenized   (019): ['<s>', 'ext', 'ras', '.', 'extend', '(', 'arg', '_', 'strings', '[', 'stop', '_', 'index', ':', ']', ')', '\\', 'n', '</s>']
Filtered   (017): ['ext', 'ras', '.', 'extend', '(', 'arg', '_', 'strings', '[', 'stop', '_', 'index', ':', ']', ')', '\\', 'n']
Detokenized (011): ['extras', '.', 'extend', '(', 'arg_strings', '[', 'stop_index', ':', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "OPTIONAL : _ ( ) , \n"
Original    (007): ['OPTIONAL', ':', '_', '(', ')', ',', '\\n']
Tokenized   (012): ['<s>', 'OP', 'TION', 'AL', ':', '_', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (010): ['OP', 'TION', 'AL', ':', '_', '(', ')', ',', '\\', 'n']
Detokenized (007): ['OPTIONAL', ':', '_', '(', ')', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "pattern = . join ( [ self . _get_nargs_pattern ( action ) \n"
Original    (013): ['pattern', '=', '.', 'join', '(', '[', 'self', '.', '_get_nargs_pattern', '(', 'action', ')', '\\n']
Tokenized   (022): ['<s>', 'pattern', '=', '.', 'join', '(', '[', 'self', '.', '_', 'get', '_', 'n', 'args', '_', 'pattern', '(', 'action', ')', '\\', 'n', '</s>']
Filtered   (020): ['pattern', '=', '.', 'join', '(', '[', 'self', '.', '_', 'get', '_', 'n', 'args', '_', 'pattern', '(', 'action', ')', '\\', 'n']
Detokenized (013): ['pattern', '=', '.', 'join', '(', '[', 'self', '.', '_get_nargs_pattern', '(', 'action', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "short_option_prefix = option_string [ : 2 ] \n"
Original    (008): ['short_option_prefix', '=', 'option_string', '[', ':', '2', ']', '\\n']
Tokenized   (017): ['<s>', 'short', '_', 'option', '_', 'prefix', '=', 'option', '_', 'string', '[', ':', '2', ']', '\\', 'n', '</s>']
Filtered   (015): ['short', '_', 'option', '_', 'prefix', '=', 'option', '_', 'string', '[', ':', '2', ']', '\\', 'n']
Detokenized (008): ['short_option_prefix', '=', 'option_string', '[', ':', '2', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "tup = action , option_string , short_explicit_arg \n"
Original    (008): ['tup', '=', 'action', ',', 'option_string', ',', 'short_explicit_arg', '\\n']
Tokenized   (019): ['<s>', 't', 'up', '=', 'action', ',', 'option', '_', 'string', ',', 'short', '_', 'expl', 'icit', '_', 'arg', '\\', 'n', '</s>']
Filtered   (017): ['t', 'up', '=', 'action', ',', 'option', '_', 'string', ',', 'short', '_', 'expl', 'icit', '_', 'arg', '\\', 'n']
Detokenized (008): ['tup', '=', 'action', ',', 'option_string', ',', 'short_explicit_arg', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "not action . option_strings ) : \n"
Original    (007): ['not', 'action', '.', 'option_strings', ')', ':', '\\n']
Tokenized   (012): ['<s>', 'not', 'action', '.', 'option', '_', 'strings', ')', ':', '\\', 'n', '</s>']
Filtered   (010): ['not', 'action', '.', 'option', '_', 'strings', ')', ':', '\\', 'n']
Detokenized (007): ['not', 'action', '.', 'option_strings', ')', ':', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "vulnerability = obj [ ] [ ] [ ] [ ] [ ] \n"
Original    (014): ['vulnerability', '=', 'obj', '[', ']', '[', ']', '[', ']', '[', ']', '[', ']', '\\n']
Tokenized   (018): ['<s>', 'v', 'ulnerability', '=', 'obj', '[', ']', '[', ']', '[', ']', '[', ']', '[', ']', '\\', 'n', '</s>']
Filtered   (016): ['v', 'ulnerability', '=', 'obj', '[', ']', '[', ']', '[', ']', '[', ']', '[', ']', '\\', 'n']
Detokenized (014): ['vulnerability', '=', 'obj', '[', ']', '[', ']', '[', ']', '[', ']', '[', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "apikey = common . apikey ( sessionKey , args [ 0 ] , debug ) \n"
Original    (016): ['apikey', '=', 'common', '.', 'apikey', '(', 'sessionKey', ',', 'args', '[', '0', ']', ',', 'debug', ')', '\\n']
Tokenized   (024): ['<s>', 'ap', 'ike', 'y', '=', 'common', '.', 'ap', 'ike', 'y', '(', 'session', 'Key', ',', 'args', '[', '0', ']', ',', 'debug', ')', '\\', 'n', '</s>']
Filtered   (022): ['ap', 'ike', 'y', '=', 'common', '.', 'ap', 'ike', 'y', '(', 'session', 'Key', ',', 'args', '[', '0', ']', ',', 'debug', ')', '\\', 'n']
Detokenized (016): ['apikey', '=', 'common', '.', 'apikey', '(', 'sessionKey', ',', 'args', '[', '0', ']', ',', 'debug', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "device = pandevice . base . PanDevice ( args [ 0 ] , api_key = apikey ) \n"
Original    (018): ['device', '=', 'pandevice', '.', 'base', '.', 'PanDevice', '(', 'args', '[', '0', ']', ',', 'api_key', '=', 'apikey', ')', '\\n']
Tokenized   (028): ['<s>', 'device', '=', 'pand', 'ev', 'ice', '.', 'base', '.', 'Pan', 'Device', '(', 'args', '[', '0', ']', ',', 'api', '_', 'key', '=', 'ap', 'ike', 'y', ')', '\\', 'n', '</s>']
Filtered   (026): ['device', '=', 'pand', 'ev', 'ice', '.', 'base', '.', 'Pan', 'Device', '(', 'args', '[', '0', ']', ',', 'api', '_', 'key', '=', 'ap', 'ike', 'y', ')', '\\', 'n']
Detokenized (018): ['device', '=', 'pandevice', '.', 'base', '.', 'PanDevice', '(', 'args', '[', '0', ']', ',', 'api_key', '=', 'apikey', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "rebalance_backoff_ms = 2 * 1000 , \n"
Original    (007): ['rebalance_backoff_ms', '=', '2', '*', '1000', ',', '\\n']
Tokenized   (016): ['<s>', 're', 'balance', '_', 'back', 'off', '_', 'ms', '=', '2', '*', '1000', ',', '\\', 'n', '</s>']
Filtered   (014): ['re', 'balance', '_', 'back', 'off', '_', 'ms', '=', '2', '*', '1000', ',', '\\', 'n']
Detokenized (007): ['rebalance_backoff_ms', '=', '2', '*', '1000', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "uuid = uuid4 ( ) \n"
Original    (006): ['uuid', '=', 'uuid4', '(', ')', '\\n']
Tokenized   (012): ['<s>', 'uu', 'id', '=', 'u', 'uid', '4', '(', ')', '\\', 'n', '</s>']
Filtered   (010): ['uu', 'id', '=', 'u', 'uid', '4', '(', ')', '\\', 'n']
Detokenized (006): ['uuid', '=', 'uuid4', '(', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : """ . join ( traceback . format_tb ( tb ) ) ) \n"
Original    (013): ['""', '.', 'join', '(', 'traceback', '.', 'format_tb', '(', 'tb', ')', ')', ')', '\\n']
Tokenized   (021): ['<s>', '""', '.', 'join', '(', 'trace', 'back', '.', 'format', '_', 't', 'b', '(', 't', 'b', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['""', '.', 'join', '(', 'trace', 'back', '.', 'format', '_', 't', 'b', '(', 't', 'b', ')', ')', ')', '\\', 'n']
Detokenized (013): ['""', '.', 'join', '(', 'traceback', '.', 'format_tb', '(', 'tb', ')', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "kazoo_kwargs = { : timeout / 1000 } \n"
Original    (009): ['kazoo_kwargs', '=', '{', ':', 'timeout', '/', '1000', '}', '\\n']
Tokenized   (017): ['<s>', 'k', 'az', 'oo', '_', 'kw', 'args', '=', '{', ':', 'timeout', '/', '1000', '}', '\\', 'n', '</s>']
Filtered   (015): ['k', 'az', 'oo', '_', 'kw', 'args', '=', '{', ':', 'timeout', '/', '1000', '}', '\\', 'n']
Detokenized (009): ['kazoo_kwargs', '=', '{', ':', 'timeout', '/', '1000', '}', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "p_to_str = lambda p : . join ( [ str ( p . topic . name ) , str ( p . leader . id ) , str ( p . id ) ] ) \n"
Original    (036): ['p_to_str', '=', 'lambda', 'p', ':', '.', 'join', '(', '[', 'str', '(', 'p', '.', 'topic', '.', 'name', ')', ',', 'str', '(', 'p', '.', 'leader', '.', 'id', ')', ',', 'str', '(', 'p', '.', 'id', ')', ']', ')', '\\n']
Tokenized   (043): ['<s>', 'p', '_', 'to', '_', 'str', '=', 'lambda', 'p', ':', '.', 'join', '(', '[', 'str', '(', 'p', '.', 'topic', '.', 'name', ')', ',', 'str', '(', 'p', '.', 'leader', '.', 'id', ')', ',', 'str', '(', 'p', '.', 'id', ')', ']', ')', '\\', 'n', '</s>']
Filtered   (041): ['p', '_', 'to', '_', 'str', '=', 'lambda', 'p', ':', '.', 'join', '(', '[', 'str', '(', 'p', '.', 'topic', '.', 'name', ')', ',', 'str', '(', 'p', '.', 'leader', '.', 'id', ')', ',', 'str', '(', 'p', '.', 'id', ')', ']', ')', '\\', 'n']
Detokenized (036): ['p_to_str', '=', 'lambda', 'p', ':', '.', 'join', '(', '[', 'str', '(', 'p', '.', 'topic', '.', 'name', ')', ',', 'str', '(', 'p', '.', 'leader', '.', 'id', ')', ',', 'str', '(', 'p', '.', 'id', ')', ']', ')', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 36, 768)
# Extracted words:  36
Sentence         : "idx = participants . index ( consumer_id or self . _consumer_id ) \n"
Original    (013): ['idx', '=', 'participants', '.', 'index', '(', 'consumer_id', 'or', 'self', '.', '_consumer_id', ')', '\\n']
Tokenized   (022): ['<s>', 'id', 'x', '=', 'participants', '.', 'index', '(', 'consumer', '_', 'id', 'or', 'self', '.', '_', 'consumer', '_', 'id', ')', '\\', 'n', '</s>']
Filtered   (020): ['id', 'x', '=', 'participants', '.', 'index', '(', 'consumer', '_', 'id', 'or', 'self', '.', '_', 'consumer', '_', 'id', ')', '\\', 'n']
Detokenized (013): ['idx', '=', 'participants', '.', 'index', '(', 'consumer_id', 'or', 'self', '.', '_consumer_id', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "parts_per_consumer = len ( all_parts ) // len ( participants ) \n"
Original    (012): ['parts_per_consumer', '=', 'len', '(', 'all_parts', ')', '//', 'len', '(', 'participants', ')', '\\n']
Tokenized   (021): ['<s>', 'parts', '_', 'per', '_', 'consumer', '=', 'len', '(', 'all', '_', 'parts', ')', '//', 'len', '(', 'participants', ')', '\\', 'n', '</s>']
Filtered   (019): ['parts', '_', 'per', '_', 'consumer', '=', 'len', '(', 'all', '_', 'parts', ')', '//', 'len', '(', 'participants', ')', '\\', 'n']
Detokenized (012): ['parts_per_consumer', '=', 'len', '(', 'all_parts', ')', '//', 'len', '(', 'participants', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "remainder_ppc = len ( all_parts ) % len ( participants ) \n"
Original    (012): ['remainder_ppc', '=', 'len', '(', 'all_parts', ')', '%', 'len', '(', 'participants', ')', '\\n']
Tokenized   (022): ['<s>', 'rem', 'ain', 'der', '_', 'pp', 'c', '=', 'len', '(', 'all', '_', 'parts', ')', '%', 'len', '(', 'participants', ')', '\\', 'n', '</s>']
Filtered   (020): ['rem', 'ain', 'der', '_', 'pp', 'c', '=', 'len', '(', 'all', '_', 'parts', ')', '%', 'len', '(', 'participants', ')', '\\', 'n']
Detokenized (012): ['remainder_ppc', '=', 'len', '(', 'all_parts', ')', '%', 'len', '(', 'participants', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "num_parts = parts_per_consumer + ( 0 if ( idx + 1 > remainder_ppc ) else 1 ) \n"
Original    (018): ['num_parts', '=', 'parts_per_consumer', '+', '(', '0', 'if', '(', 'idx', '+', '1', '>', 'remainder_ppc', ')', 'else', '1', ')', '\\n']
Tokenized   (031): ['<s>', 'num', '_', 'parts', '=', 'parts', '_', 'per', '_', 'consumer', '+', '(', '0', 'if', '(', 'id', 'x', '+', '1', '>', 'remainder', '_', 'pp', 'c', ')', 'else', '1', ')', '\\', 'n', '</s>']
Filtered   (029): ['num', '_', 'parts', '=', 'parts', '_', 'per', '_', 'consumer', '+', '(', '0', 'if', '(', 'id', 'x', '+', '1', '>', 'remainder', '_', 'pp', 'c', ')', 'else', '1', ')', '\\', 'n']
Detokenized (018): ['num_parts', '=', 'parts_per_consumer', '+', '(', '0', 'if', '(', 'idx', '+', '1', '>', 'remainder_ppc', ')', 'else', '1', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "log . debug ( , [ p_to_str ( p ) for p in new_partitions ] ) \n"
Original    (017): ['log', '.', 'debug', '(', ',', '[', 'p_to_str', '(', 'p', ')', 'for', 'p', 'in', 'new_partitions', ']', ')', '\\n']
Tokenized   (027): ['<s>', 'log', '.', 'debug', '(', ',', '[', 'p', '_', 'to', '_', 'str', '(', 'p', ')', 'for', 'p', 'in', 'new', '_', 'part', 'itions', ']', ')', '\\', 'n', '</s>']
Filtered   (025): ['log', '.', 'debug', '(', ',', '[', 'p', '_', 'to', '_', 'str', '(', 'p', ')', 'for', 'p', 'in', 'new', '_', 'part', 'itions', ']', ')', '\\', 'n']
Detokenized (017): ['log', '.', 'debug', '(', ',', '[', 'p_to_str', '(', 'p', ')', 'for', 'p', 'in', 'new_partitions', ']', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "id_ = get_string ( self . _consumer_id ) \n"
Original    (009): ['id_', '=', 'get_string', '(', 'self', '.', '_consumer_id', ')', '\\n']
Tokenized   (018): ['<s>', 'id', '_', '=', 'get', '_', 'string', '(', 'self', '.', '_', 'consumer', '_', 'id', ')', '\\', 'n', '</s>']
Filtered   (016): ['id', '_', '=', 'get', '_', 'string', '(', 'self', '.', '_', 'consumer', '_', 'id', ')', '\\', 'n']
Detokenized (009): ['id_', '=', 'get_string', '(', 'self', '.', '_consumer_id', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "path = self . _topic_path , slug = partition_slug ) ) \n"
Original    (012): ['path', '=', 'self', '.', '_topic_path', ',', 'slug', '=', 'partition_slug', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'path', '=', 'self', '.', '_', 'topic', '_', 'path', ',', 'slug', '=', 'partition', '_', 'sl', 'ug', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['path', '=', 'self', '.', '_', 'topic', '_', 'path', ',', 'slug', '=', 'partition', '_', 'sl', 'ug', ')', ')', '\\', 'n']
Detokenized (012): ['path', '=', 'self', '.', '_topic_path', ',', 'slug', '=', 'partition_slug', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "HZCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 ) \n"
Original    (016): ['HZCharLenTable', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Tokenized   (023): ['<s>', 'H', 'Z', 'Char', 'Len', 'Table', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n', '</s>']
Filtered   (021): ['H', 'Z', 'Char', 'Len', 'Table', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n']
Detokenized (016): ['HZCharLenTable', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "ISO2022CNCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ) \n"
Original    (022): ['ISO2022CNCharLenTable', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Tokenized   (031): ['<s>', 'ISO', '20', '22', 'CN', 'Char', 'Len', 'Table', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n', '</s>']
Filtered   (029): ['ISO', '20', '22', 'CN', 'Char', 'Len', 'Table', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n']
Detokenized (022): ['ISO2022CNCharLenTable', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "ISO2022JPCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ) \n"
Original    (024): ['ISO2022JPCharLenTable', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Tokenized   (033): ['<s>', 'ISO', '20', '22', 'JP', 'Char', 'Len', 'Table', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n', '</s>']
Filtered   (031): ['ISO', '20', '22', 'JP', 'Char', 'Len', 'Table', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\', 'n']
Detokenized (024): ['ISO2022JPCharLenTable', '=', '(', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ',', '0', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "logging . basicConfig ( level = logging . INFO ) \n"
Original    (011): ['logging', '.', 'basicConfig', '(', 'level', '=', 'logging', '.', 'INFO', ')', '\\n']
Tokenized   (016): ['<s>', 'log', 'ging', '.', 'basic', 'Config', '(', 'level', '=', 'logging', '.', 'INFO', ')', '\\', 'n', '</s>']
Filtered   (014): ['log', 'ging', '.', 'basic', 'Config', '(', 'level', '=', 'logging', '.', 'INFO', ')', '\\', 'n']
Detokenized (011): ['logging', '.', 'basicConfig', '(', 'level', '=', 'logging', '.', 'INFO', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "tasa . __version__ , sys . version ) ) \n"
Original    (010): ['tasa', '.', '__version__', ',', 'sys', '.', 'version', ')', ')', '\\n']
Tokenized   (016): ['<s>', 't', 'asa', '.', '__', 'version', '__', ',', 'sys', '.', 'version', ')', ')', '\\', 'n', '</s>']
Filtered   (014): ['t', 'asa', '.', '__', 'version', '__', ',', 'sys', '.', 'version', ')', ')', '\\', 'n']
Detokenized (010): ['tasa', '.', '__version__', ',', 'sys', '.', 'version', ')', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "type = lambda w : w . partition ( ) [ : : 2 ] , \n"
Original    (017): ['type', '=', 'lambda', 'w', ':', 'w', '.', 'partition', '(', ')', '[', ':', ':', '2', ']', ',', '\\n']
Tokenized   (020): ['<s>', 'type', '=', 'lambda', 'w', ':', 'w', '.', 'partition', '(', ')', '[', ':', ':', '2', ']', ',', '\\', 'n', '</s>']
Filtered   (018): ['type', '=', 'lambda', 'w', ':', 'w', '.', 'partition', '(', ')', '[', ':', ':', '2', ']', ',', '\\', 'n']
Detokenized (017): ['type', '=', 'lambda', 'w', ':', 'w', '.', 'partition', '(', ')', '[', ':', ':', '2', ']', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "worker_class_name = args . worker [ 1 ] or \n"
Original    (010): ['worker_class_name', '=', 'args', '.', 'worker', '[', '1', ']', 'or', '\\n']
Tokenized   (017): ['<s>', 'worker', '_', 'class', '_', 'name', '=', 'args', '.', 'worker', '[', '1', ']', 'or', '\\', 'n', '</s>']
Filtered   (015): ['worker', '_', 'class', '_', 'name', '=', 'args', '.', 'worker', '[', '1', ']', 'or', '\\', 'n']
Detokenized (010): ['worker_class_name', '=', 'args', '.', 'worker', '[', '1', ']', 'or', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "str ( job ) [ : 50 ] ) \n"
Original    (010): ['str', '(', 'job', ')', '[', ':', '50', ']', ')', '\\n']
Tokenized   (013): ['<s>', 'str', '(', 'job', ')', '[', ':', '50', ']', ')', '\\', 'n', '</s>']
Filtered   (011): ['str', '(', 'job', ')', '[', ':', '50', ']', ')', '\\', 'n']
Detokenized (010): ['str', '(', 'job', ')', '[', ':', '50', ']', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "processes = [ Process ( target = run , args = ( ) ) for x in range ( count ) ] \n"
Original    (023): ['processes', '=', '[', 'Process', '(', 'target', '=', 'run', ',', 'args', '=', '(', ')', ')', 'for', 'x', 'in', 'range', '(', 'count', ')', ']', '\\n']
Tokenized   (027): ['<s>', 'process', 'es', '=', '[', 'Process', '(', 'target', '=', 'run', ',', 'args', '=', '(', ')', ')', 'for', 'x', 'in', 'range', '(', 'count', ')', ']', '\\', 'n', '</s>']
Filtered   (025): ['process', 'es', '=', '[', 'Process', '(', 'target', '=', 'run', ',', 'args', '=', '(', ')', ')', 'for', 'x', 'in', 'range', '(', 'count', ')', ']', '\\', 'n']
Detokenized (023): ['processes', '=', '[', 'Process', '(', 'target', '=', 'run', ',', 'args', '=', '(', ')', ')', 'for', 'x', 'in', 'range', '(', 'count', ')', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "color = models . CharField ( max_length = 6 , validators = [ color_regex ] , help_text = \n"
Original    (019): ['color', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '6', ',', 'validators', '=', '[', 'color_regex', ']', ',', 'help_text', '=', '\\n']
Tokenized   (031): ['<s>', 'color', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '6', ',', 'valid', 'ators', '=', '[', 'color', '_', 're', 'gex', ']', ',', 'help', '_', 'text', '=', '\\', 'n', '</s>']
Filtered   (029): ['color', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '6', ',', 'valid', 'ators', '=', '[', 'color', '_', 're', 'gex', ']', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (019): ['color', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '6', ',', 'validators', '=', '[', 'color_regex', ']', ',', 'help_text', '=', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "first_name = models . CharField ( max_length = 64 ) \n"
Original    (011): ['first_name', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '64', ')', '\\n']
Tokenized   (019): ['<s>', 'first', '_', 'name', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '64', ')', '\\', 'n', '</s>']
Filtered   (017): ['first', '_', 'name', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '64', ')', '\\', 'n']
Detokenized (011): ['first_name', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '64', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "role = models . CharField ( max_length = 17 , choices = ROLE_CHOICES ) \n"
Original    (015): ['role', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '17', ',', 'choices', '=', 'ROLE_CHOICES', ')', '\\n']
Tokenized   (025): ['<s>', 'role', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '17', ',', 'choices', '=', 'RO', 'LE', '_', 'CHO', 'ICES', ')', '\\', 'n', '</s>']
Filtered   (023): ['role', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '17', ',', 'choices', '=', 'RO', 'LE', '_', 'CHO', 'ICES', ')', '\\', 'n']
Detokenized (015): ['role', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '17', ',', 'choices', '=', 'ROLE_CHOICES', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "phone_work = models . CharField ( max_length = 15 , validators = [ phone_regex ] , blank = True ) \n"
Original    (021): ['phone_work', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '15', ',', 'validators', '=', '[', 'phone_regex', ']', ',', 'blank', '=', 'True', ')', '\\n']
Tokenized   (033): ['<s>', 'phone', '_', 'work', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '15', ',', 'valid', 'ators', '=', '[', 'phone', '_', 're', 'gex', ']', ',', 'blank', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (031): ['phone', '_', 'work', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '15', ',', 'valid', 'ators', '=', '[', 'phone', '_', 're', 'gex', ']', ',', 'blank', '=', 'True', ')', '\\', 'n']
Detokenized (021): ['phone_work', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '15', ',', 'validators', '=', '[', 'phone_regex', ']', ',', 'blank', '=', 'True', ')', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "job_title = models . CharField ( max_length = 128 , blank = True ) \n"
Original    (015): ['job_title', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '128', ',', 'blank', '=', 'True', ')', '\\n']
Tokenized   (023): ['<s>', 'job', '_', 'title', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '128', ',', 'blank', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (021): ['job', '_', 'title', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '128', ',', 'blank', '=', 'True', ')', '\\', 'n']
Detokenized (015): ['job_title', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '128', ',', 'blank', '=', 'True', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "category = models . CharField ( max_length = 21 , choices = CATEGORY_CHOICES , help_text = description = models . CharField ( max_length = 256 , blank = True , help_text = reference = models . URLField ( blank = True , help_text = ) \n"
Original    (046): ['category', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '21', ',', 'choices', '=', 'CATEGORY_CHOICES', ',', 'help_text', '=', 'description', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '256', ',', 'blank', '=', 'True', ',', 'help_text', '=', 'reference', '=', 'models', '.', 'URLField', '(', 'blank', '=', 'True', ',', 'help_text', '=', ')', '\\n']
Tokenized   (068): ['<s>', 'category', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '21', ',', 'choices', '=', 'C', 'ATE', 'G', 'ORY', '_', 'CHO', 'ICES', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '256', ',', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', 'reference', '=', 'models', '.', 'URL', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', ')', '\\', 'n', '</s>']
Filtered   (066): ['category', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '21', ',', 'choices', '=', 'C', 'ATE', 'G', 'ORY', '_', 'CHO', 'ICES', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '256', ',', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', 'reference', '=', 'models', '.', 'URL', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', ')', '\\', 'n']
Detokenized (046): ['category', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '21', ',', 'choices', '=', 'CATEGORY_CHOICES', ',', 'help_text', '=', 'description', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '256', ',', 'blank', '=', 'True', ',', 'help_text', '=', 'reference', '=', 'models', '.', 'URLField', '(', 'blank', '=', 'True', ',', 'help_text', '=', ')', '\\n']
Counter: 66
===================================================================
Hidden states:  (13, 46, 768)
# Extracted words:  46
Sentence         : "acronym = models . CharField ( max_length = 20 , unique = True , help_text = category = models . CharField ( max_length = 9 , choices = CATEGORY_CHOICES , help_text = jurisdiction = models . CharField ( max_length = 64 , help_text = description = models . TextField ( blank = True , help_text = reference = models . URLField ( blank = True , help_text = ) \n"
Original    (070): ['acronym', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '20', ',', 'unique', '=', 'True', ',', 'help_text', '=', 'category', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '9', ',', 'choices', '=', 'CATEGORY_CHOICES', ',', 'help_text', '=', 'jurisdiction', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '64', ',', 'help_text', '=', 'description', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', 'reference', '=', 'models', '.', 'URLField', '(', 'blank', '=', 'True', ',', 'help_text', '=', ')', '\\n']
Tokenized   (102): ['<s>', 'ac', 'ron', 'ym', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '20', ',', 'unique', '=', 'True', ',', 'help', '_', 'text', '=', 'category', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '9', ',', 'choices', '=', 'C', 'ATE', 'G', 'ORY', '_', 'CHO', 'ICES', ',', 'help', '_', 'text', '=', 'jurisdiction', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '64', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', 'reference', '=', 'models', '.', 'URL', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', ')', '\\', 'n', '</s>']
Filtered   (100): ['ac', 'ron', 'ym', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '20', ',', 'unique', '=', 'True', ',', 'help', '_', 'text', '=', 'category', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '9', ',', 'choices', '=', 'C', 'ATE', 'G', 'ORY', '_', 'CHO', 'ICES', ',', 'help', '_', 'text', '=', 'jurisdiction', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '64', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', 'reference', '=', 'models', '.', 'URL', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', ')', '\\', 'n']
Detokenized (070): ['acronym', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '20', ',', 'unique', '=', 'True', ',', 'help_text', '=', 'category', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '9', ',', 'choices', '=', 'CATEGORY_CHOICES', ',', 'help_text', '=', 'jurisdiction', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '64', ',', 'help_text', '=', 'description', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', 'reference', '=', 'models', '.', 'URLField', '(', 'blank', '=', 'True', ',', 'help_text', '=', ')', '\\n']
Counter: 100
===================================================================
Hidden states:  (13, 70, 768)
# Extracted words:  70
Sentence         : "description = models . CharField ( max_length = 256 , blank = True , help_text = \n"
Original    (017): ['description', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '256', ',', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (025): ['<s>', 'description', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '256', ',', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n', '</s>']
Filtered   (023): ['description', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '256', ',', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (017): ['description', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '256', ',', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "business_criticality = models . CharField ( max_length = 9 , choices = BUSINESS_CRITICALITY_CHOICES , blank platform = models . CharField ( max_length = 11 , choices = PLATFORM_CHOICES , blank = True , null = True ) \n"
Original    (038): ['business_criticality', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '9', ',', 'choices', '=', 'BUSINESS_CRITICALITY_CHOICES', ',', 'blank', 'platform', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '11', ',', 'choices', '=', 'PLATFORM_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\n']
Tokenized   (064): ['<s>', 'business', '_', 'critical', 'ity', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '9', ',', 'choices', '=', 'BUS', 'INESS', '_', 'CR', 'IT', 'ICAL', 'ITY', '_', 'CHO', 'ICES', ',', 'blank', 'platform', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '11', ',', 'choices', '=', 'PL', 'AT', 'FORM', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (062): ['business', '_', 'critical', 'ity', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '9', ',', 'choices', '=', 'BUS', 'INESS', '_', 'CR', 'IT', 'ICAL', 'ITY', '_', 'CHO', 'ICES', ',', 'blank', 'platform', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '11', ',', 'choices', '=', 'PL', 'AT', 'FORM', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\', 'n']
Detokenized (038): ['business_criticality', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '9', ',', 'choices', '=', 'BUSINESS_CRITICALITY_CHOICES', ',', 'blank', 'platform', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '11', ',', 'choices', '=', 'PLATFORM_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\n']
Counter: 62
===================================================================
Hidden states:  (13, 38, 768)
# Extracted words:  38
Sentence         : "lifecycle = models . CharField ( max_length = 8 , choices = LIFECYCLE_CHOICES , blank = True , null = True ) \n"
Original    (023): ['lifecycle', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '8', ',', 'choices', '=', 'LIFECYCLE_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\n']
Tokenized   (037): ['<s>', 'lif', 'ecycle', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '8', ',', 'choices', '=', 'L', 'IF', 'EC', 'Y', 'CLE', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (035): ['lif', 'ecycle', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '8', ',', 'choices', '=', 'L', 'IF', 'EC', 'Y', 'CLE', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\', 'n']
Detokenized (023): ['lifecycle', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '8', ',', 'choices', '=', 'LIFECYCLE_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "user_records = models . PositiveIntegerField ( blank = True , null = True , help_text = revenue = models . DecimalField ( max_digits = 15 , decimal_places = 2 , blank = True , null = True , help_text = external_audience = models . BooleanField ( default = False , help_text = internet_accessible = models . BooleanField ( default = False , help_text = requestable = models . NullBooleanField ( default = True , help_text = _ ( \n"
Original    (079): ['user_records', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'revenue', '=', 'models', '.', 'DecimalField', '(', 'max_digits', '=', '15', ',', 'decimal_places', '=', '2', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'external_audience', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ',', 'help_text', '=', 'internet_accessible', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ',', 'help_text', '=', 'requestable', '=', 'models', '.', 'NullBooleanField', '(', 'default', '=', 'True', ',', 'help_text', '=', '_', '(', '\\n']
Tokenized   (115): ['<s>', 'user', '_', 'rec', 'ords', '=', 'models', '.', 'Positive', 'Integer', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'revenue', '=', 'models', '.', 'Dec', 'imal', 'Field', '(', 'max', '_', 'dig', 'its', '=', '15', ',', 'decimal', '_', 'places', '=', '2', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'external', '_', 'aud', 'ience', '=', 'models', '.', 'Boolean', 'Field', '(', 'default', '=', 'False', ',', 'help', '_', 'text', '=', 'internet', '_', 'accessible', '=', 'models', '.', 'Boolean', 'Field', '(', 'default', '=', 'False', ',', 'help', '_', 'text', '=', 'request', 'able', '=', 'models', '.', 'Null', 'Boo', 'lean', 'Field', '(', 'default', '=', 'True', ',', 'help', '_', 'text', '=', '_', '(', '\\', 'n', '</s>']
Filtered   (113): ['user', '_', 'rec', 'ords', '=', 'models', '.', 'Positive', 'Integer', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'revenue', '=', 'models', '.', 'Dec', 'imal', 'Field', '(', 'max', '_', 'dig', 'its', '=', '15', ',', 'decimal', '_', 'places', '=', '2', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'external', '_', 'aud', 'ience', '=', 'models', '.', 'Boolean', 'Field', '(', 'default', '=', 'False', ',', 'help', '_', 'text', '=', 'internet', '_', 'accessible', '=', 'models', '.', 'Boolean', 'Field', '(', 'default', '=', 'False', ',', 'help', '_', 'text', '=', 'request', 'able', '=', 'models', '.', 'Null', 'Boo', 'lean', 'Field', '(', 'default', '=', 'True', ',', 'help', '_', 'text', '=', '_', '(', '\\', 'n']
Detokenized (079): ['user_records', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'revenue', '=', 'models', '.', 'DecimalField', '(', 'max_digits', '=', '15', ',', 'decimal_places', '=', '2', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'external_audience', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ',', 'help_text', '=', 'internet_accessible', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ',', 'help_text', '=', 'requestable', '=', 'models', '.', 'NullBooleanField', '(', 'default', '=', 'True', ',', 'help_text', '=', '_', '(', '\\n']
Counter: 113
===================================================================
Hidden states:  (13, 79, 768)
# Extracted words:  79
Sentence         : "override_dcl = models . IntegerField ( choices = DATA_CLASSIFICATION_CHOICES , blank = True , null = True , help_text override_reason = models . TextField ( blank = True , help_text = \n"
Original    (032): ['override_dcl', '=', 'models', '.', 'IntegerField', '(', 'choices', '=', 'DATA_CLASSIFICATION_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', 'override_reason', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (054): ['<s>', 'over', 'ride', '_', 'd', 'cl', '=', 'models', '.', 'Integer', 'Field', '(', 'choices', '=', 'DATA', '_', 'CLASS', 'IFIC', 'ATION', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', 'override', '_', 'reason', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n', '</s>']
Filtered   (052): ['over', 'ride', '_', 'd', 'cl', '=', 'models', '.', 'Integer', 'Field', '(', 'choices', '=', 'DATA', '_', 'CLASS', 'IFIC', 'ATION', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', 'override', '_', 'reason', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (032): ['override_dcl', '=', 'models', '.', 'IntegerField', '(', 'choices', '=', 'DATA_CLASSIFICATION_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', 'override_reason', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Counter: 52
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "threadfix = models . ForeignKey ( ThreadFix , blank = True , null = True , help_text = threadfix_team_id = models . PositiveIntegerField ( blank = True , null = True , help_text = threadfix_application_id = models . PositiveIntegerField ( blank = True , null = True , help_text = \n"
Original    (051): ['threadfix', '=', 'models', '.', 'ForeignKey', '(', 'ThreadFix', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'threadfix_team_id', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'threadfix_application_id', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (077): ['<s>', 'thread', 'fix', '=', 'models', '.', 'Foreign', 'Key', '(', 'Thread', 'Fix', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'thread', 'fix', '_', 'team', '_', 'id', '=', 'models', '.', 'Positive', 'Integer', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'thread', 'fix', '_', 'application', '_', 'id', '=', 'models', '.', 'Positive', 'Integer', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n', '</s>']
Filtered   (075): ['thread', 'fix', '=', 'models', '.', 'Foreign', 'Key', '(', 'Thread', 'Fix', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'thread', 'fix', '_', 'team', '_', 'id', '=', 'models', '.', 'Positive', 'Integer', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'thread', 'fix', '_', 'application', '_', 'id', '=', 'models', '.', 'Positive', 'Integer', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (051): ['threadfix', '=', 'models', '.', 'ForeignKey', '(', 'ThreadFix', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'threadfix_team_id', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'threadfix_application_id', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', '\\n']
Counter: 75
===================================================================
Hidden states:  (13, 51, 768)
# Extracted words:  51
Sentence         : "asvs_level = models . IntegerField ( choices = ASVS_CHOICES , blank = True , null = True , help_text = asvs_level_percent_achieved = models . PositiveIntegerField ( blank = True , null = True , help_text = asvs_doc_url = models . URLField ( blank = True , help_text = ) \n"
Original    (050): ['asvs_level', '=', 'models', '.', 'IntegerField', '(', 'choices', '=', 'ASVS_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'asvs_level_percent_achieved', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'asvs_doc_url', '=', 'models', '.', 'URLField', '(', 'blank', '=', 'True', ',', 'help_text', '=', ')', '\\n']
Tokenized   (083): ['<s>', 'as', 'vs', '_', 'level', '=', 'models', '.', 'Integer', 'Field', '(', 'choices', '=', 'AS', 'VS', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'as', 'vs', '_', 'level', '_', 'percent', '_', 'ach', 'ieved', '=', 'models', '.', 'Positive', 'Integer', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'as', 'vs', '_', 'doc', '_', 'url', '=', 'models', '.', 'URL', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', ')', '\\', 'n', '</s>']
Filtered   (081): ['as', 'vs', '_', 'level', '=', 'models', '.', 'Integer', 'Field', '(', 'choices', '=', 'AS', 'VS', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'as', 'vs', '_', 'level', '_', 'percent', '_', 'ach', 'ieved', '=', 'models', '.', 'Positive', 'Integer', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'as', 'vs', '_', 'doc', '_', 'url', '=', 'models', '.', 'URL', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', ')', '\\', 'n']
Detokenized (050): ['asvs_level', '=', 'models', '.', 'IntegerField', '(', 'choices', '=', 'ASVS_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'asvs_level_percent_achieved', '=', 'models', '.', 'PositiveIntegerField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'asvs_doc_url', '=', 'models', '.', 'URLField', '(', 'blank', '=', 'True', ',', 'help_text', '=', ')', '\\n']
Counter: 81
===================================================================
Hidden states:  (13, 50, 768)
# Extracted words:  50
Sentence         : "asvs_level_target = models . IntegerField ( choices = ASVS_CHOICES , blank = True , null = True , help_text = \n"
Original    (021): ['asvs_level_target', '=', 'models', '.', 'IntegerField', '(', 'choices', '=', 'ASVS_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (036): ['<s>', 'as', 'vs', '_', 'level', '_', 'target', '=', 'models', '.', 'Integer', 'Field', '(', 'choices', '=', 'AS', 'VS', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n', '</s>']
Filtered   (034): ['as', 'vs', '_', 'level', '_', 'target', '=', 'models', '.', 'Integer', 'Field', '(', 'choices', '=', 'AS', 'VS', '_', 'CHO', 'ICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (021): ['asvs_level_target', '=', 'models', '.', 'IntegerField', '(', 'choices', '=', 'ASVS_CHOICES', ',', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "organization = models . ForeignKey ( Organization , help_text = people = models . ManyToManyField ( Person , through = , blank = True ) \n"
Original    (026): ['organization', '=', 'models', '.', 'ForeignKey', '(', 'Organization', ',', 'help_text', '=', 'people', '=', 'models', '.', 'ManyToManyField', '(', 'Person', ',', 'through', '=', ',', 'blank', '=', 'True', ')', '\\n']
Tokenized   (036): ['<s>', 'organ', 'ization', '=', 'models', '.', 'Foreign', 'Key', '(', 'Organization', ',', 'help', '_', 'text', '=', 'people', '=', 'models', '.', 'Many', 'To', 'Many', 'Field', '(', 'Person', ',', 'through', '=', ',', 'blank', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (034): ['organ', 'ization', '=', 'models', '.', 'Foreign', 'Key', '(', 'Organization', ',', 'help', '_', 'text', '=', 'people', '=', 'models', '.', 'Many', 'To', 'Many', 'Field', '(', 'Person', ',', 'through', '=', ',', 'blank', '=', 'True', ')', '\\', 'n']
Detokenized (026): ['organization', '=', 'models', '.', 'ForeignKey', '(', 'Organization', ',', 'help_text', '=', 'people', '=', 'models', '.', 'ManyToManyField', '(', 'Person', ',', 'through', '=', ',', 'blank', '=', 'True', ')', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "delta = self . created_date - timezone . now ( ) \n"
Original    (012): ['delta', '=', 'self', '.', 'created_date', '-', 'timezone', '.', 'now', '(', ')', '\\n']
Tokenized   (019): ['<s>', 'd', 'elta', '=', 'self', '.', 'created', '_', 'date', '-', 'time', 'zone', '.', 'now', '(', ')', '\\', 'n', '</s>']
Filtered   (017): ['d', 'elta', '=', 'self', '.', 'created', '_', 'date', '-', 'time', 'zone', '.', 'now', '(', ')', '\\', 'n']
Detokenized (012): ['delta', '=', 'self', '.', 'created_date', '-', 'timezone', '.', 'now', '(', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "person = models . ForeignKey ( Person , help_text = ) \n"
Original    (012): ['person', '=', 'models', '.', 'ForeignKey', '(', 'Person', ',', 'help_text', '=', ')', '\\n']
Tokenized   (018): ['<s>', 'person', '=', 'models', '.', 'Foreign', 'Key', '(', 'Person', ',', 'help', '_', 'text', '=', ')', '\\', 'n', '</s>']
Filtered   (016): ['person', '=', 'models', '.', 'Foreign', 'Key', '(', 'Person', ',', 'help', '_', 'text', '=', ')', '\\', 'n']
Detokenized (012): ['person', '=', 'models', '.', 'ForeignKey', '(', 'Person', ',', 'help_text', '=', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "environment_type = models . CharField ( max_length = 4 , choices = ENVIRONMENT_CHOICES , help_text = description = models . TextField ( blank = True , help_text = testing_approved = models . BooleanField ( default = False , help_text = \n"
Original    (041): ['environment_type', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '4', ',', 'choices', '=', 'ENVIRONMENT_CHOICES', ',', 'help_text', '=', 'description', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', 'testing_approved', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ',', 'help_text', '=', '\\n']
Tokenized   (066): ['<s>', 'environment', '_', 'type', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '4', ',', 'choices', '=', 'EN', 'V', 'IR', 'ON', 'MENT', '_', 'CHO', 'ICES', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', 'testing', '_', 'approved', '=', 'models', '.', 'Boolean', 'Field', '(', 'default', '=', 'False', ',', 'help', '_', 'text', '=', '\\', 'n', '</s>']
Filtered   (064): ['environment', '_', 'type', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '4', ',', 'choices', '=', 'EN', 'V', 'IR', 'ON', 'MENT', '_', 'CHO', 'ICES', ',', 'help', '_', 'text', '=', 'description', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', 'testing', '_', 'approved', '=', 'models', '.', 'Boolean', 'Field', '(', 'default', '=', 'False', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (041): ['environment_type', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '4', ',', 'choices', '=', 'ENVIRONMENT_CHOICES', ',', 'help_text', '=', 'description', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', 'testing_approved', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ',', 'help_text', '=', '\\n']
Counter: 64
===================================================================
Hidden states:  (13, 41, 768)
# Extracted words:  41
Sentence         : "location = models . URLField ( help_text = notes = models . TextField ( blank = True , help_text = \n"
Original    (021): ['location', '=', 'models', '.', 'URLField', '(', 'help_text', '=', 'notes', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (030): ['<s>', 'location', '=', 'models', '.', 'URL', 'Field', '(', 'help', '_', 'text', '=', 'notes', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n', '</s>']
Filtered   (028): ['location', '=', 'models', '.', 'URL', 'Field', '(', 'help', '_', 'text', '=', 'notes', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (021): ['location', '=', 'models', '.', 'URLField', '(', 'help_text', '=', 'notes', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "role_description = models . CharField ( max_length = 128 , blank = True , help_text = notes = models . TextField ( blank = True , help_text = \n"
Original    (029): ['role_description', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '128', ',', 'blank', '=', 'True', ',', 'help_text', '=', 'notes', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Tokenized   (042): ['<s>', 'role', '_', 'description', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '128', ',', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', 'notes', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n', '</s>']
Filtered   (040): ['role', '_', 'description', '=', 'models', '.', 'Char', 'Field', '(', 'max', '_', 'length', '=', '128', ',', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', 'notes', '=', 'models', '.', 'Text', 'Field', '(', 'blank', '=', 'True', ',', 'help', '_', 'text', '=', '\\', 'n']
Detokenized (029): ['role_description', '=', 'models', '.', 'CharField', '(', 'max_length', '=', '128', ',', 'blank', '=', 'True', ',', 'help_text', '=', 'notes', '=', 'models', '.', 'TextField', '(', 'blank', '=', 'True', ',', 'help_text', '=', '\\n']
Counter: 40
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "start_date = models . DateField ( help_text = ) \n"
Original    (010): ['start_date', '=', 'models', '.', 'DateField', '(', 'help_text', '=', ')', '\\n']
Tokenized   (018): ['<s>', 'start', '_', 'date', '=', 'models', '.', 'Date', 'Field', '(', 'help', '_', 'text', '=', ')', '\\', 'n', '</s>']
Filtered   (016): ['start', '_', 'date', '=', 'models', '.', 'Date', 'Field', '(', 'help', '_', 'text', '=', ')', '\\', 'n']
Detokenized (010): ['start_date', '=', 'models', '.', 'DateField', '(', 'help_text', '=', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "open_date = models . DateTimeField ( blank = True , null = True , help_text = close_date = models . DateTimeField ( blank = True , null = True , help_text = duration = models . DurationField ( blank = True , null = True ) \n"
Original    (047): ['open_date', '=', 'models', '.', 'DateTimeField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'close_date', '=', 'models', '.', 'DateTimeField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'duration', '=', 'models', '.', 'DurationField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\n']
Tokenized   (063): ['<s>', 'open', '_', 'date', '=', 'models', '.', 'Date', 'Time', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'close', '_', 'date', '=', 'models', '.', 'Date', 'Time', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'duration', '=', 'models', '.', 'Duration', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (061): ['open', '_', 'date', '=', 'models', '.', 'Date', 'Time', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'close', '_', 'date', '=', 'models', '.', 'Date', 'Time', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help', '_', 'text', '=', 'duration', '=', 'models', '.', 'Duration', 'Field', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\', 'n']
Detokenized (047): ['open_date', '=', 'models', '.', 'DateTimeField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'close_date', '=', 'models', '.', 'DateTimeField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ',', 'help_text', '=', 'duration', '=', 'models', '.', 'DurationField', '(', 'blank', '=', 'True', ',', 'null', '=', 'True', ')', '\\n']
Counter: 61
===================================================================
Hidden states:  (13, 47, 768)
# Extracted words:  47
Sentence         : "metrics = managers . ActivityTypeMetrics . from_queryset ( managers . ActivityTypeQuerySet ) ( ) \n"
Original    (015): ['metrics', '=', 'managers', '.', 'ActivityTypeMetrics', '.', 'from_queryset', '(', 'managers', '.', 'ActivityTypeQuerySet', ')', '(', ')', '\\n']
Tokenized   (029): ['<s>', 'met', 'rics', '=', 'managers', '.', 'Activity', 'Type', 'Met', 'rics', '.', 'from', '_', 'quer', 'ys', 'et', '(', 'managers', '.', 'Activity', 'Type', 'Query', 'Set', ')', '(', ')', '\\', 'n', '</s>']
Filtered   (027): ['met', 'rics', '=', 'managers', '.', 'Activity', 'Type', 'Met', 'rics', '.', 'from', '_', 'quer', 'ys', 'et', '(', 'managers', '.', 'Activity', 'Type', 'Query', 'Set', ')', '(', ')', '\\', 'n']
Detokenized (015): ['metrics', '=', 'managers', '.', 'ActivityTypeMetrics', '.', 'from_queryset', '(', 'managers', '.', 'ActivityTypeQuerySet', ')', '(', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "token = models . UUIDField ( default = uuid . uuid4 , editable = False ) \n"
Original    (017): ['token', '=', 'models', '.', 'UUIDField', '(', 'default', '=', 'uuid', '.', 'uuid4', ',', 'editable', '=', 'False', ')', '\\n']
Tokenized   (026): ['<s>', 'token', '=', 'models', '.', 'U', 'UID', 'Field', '(', 'default', '=', 'u', 'uid', '.', 'u', 'uid', '4', ',', 'edit', 'able', '=', 'False', ')', '\\', 'n', '</s>']
Filtered   (024): ['token', '=', 'models', '.', 'U', 'UID', 'Field', '(', 'default', '=', 'u', 'uid', '.', 'u', 'uid', '4', ',', 'edit', 'able', '=', 'False', ')', '\\', 'n']
Detokenized (017): ['token', '=', 'models', '.', 'UUIDField', '(', 'default', '=', 'uuid', '.', 'uuid4', ',', 'editable', '=', 'False', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "activities = models . ManyToManyField ( ActivityType , limit_choices_to = { : True } ) \n"
Original    (016): ['activities', '=', 'models', '.', 'ManyToManyField', '(', 'ActivityType', ',', 'limit_choices_to', '=', '{', ':', 'True', '}', ')', '\\n']
Tokenized   (029): ['<s>', 'activ', 'ities', '=', 'models', '.', 'Many', 'To', 'Many', 'Field', '(', 'Activity', 'Type', ',', 'limit', '_', 'cho', 'ices', '_', 'to', '=', '{', ':', 'True', '}', ')', '\\', 'n', '</s>']
Filtered   (027): ['activ', 'ities', '=', 'models', '.', 'Many', 'To', 'Many', 'Field', '(', 'Activity', 'Type', ',', 'limit', '_', 'cho', 'ices', '_', 'to', '=', '{', ':', 'True', '}', ')', '\\', 'n']
Detokenized (016): ['activities', '=', 'models', '.', 'ManyToManyField', '(', 'ActivityType', ',', 'limit_choices_to', '=', '{', ':', 'True', '}', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "opener . addheaders = [ ( , ) ] \n"
Original    (010): ['opener', '.', 'addheaders', '=', '[', '(', ',', ')', ']', '\\n']
Tokenized   (015): ['<s>', 'op', 'ener', '.', 'add', 'headers', '=', '[', '(', ',', ')', ']', '\\', 'n', '</s>']
Filtered   (013): ['op', 'ener', '.', 'add', 'headers', '=', '[', '(', ',', ')', ']', '\\', 'n']
Detokenized (010): ['opener', '.', 'addheaders', '=', '[', '(', ',', ')', ']', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "False = 0 \n"
Original    (004): ['False', '=', '0', '\\n']
Tokenized   (007): ['<s>', 'False', '=', '0', '\\', 'n', '</s>']
Filtered   (005): ['False', '=', '0', '\\', 'n']
Detokenized (004): ['False', '=', '0', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "option_pattern = chr ( 0 ) * 8 \n"
Original    (009): ['option_pattern', '=', 'chr', '(', '0', ')', '*', '8', '\\n']
Tokenized   (015): ['<s>', 'option', '_', 'pattern', '=', 'ch', 'r', '(', '0', ')', '*', '8', '\\', 'n', '</s>']
Filtered   (013): ['option', '_', 'pattern', '=', 'ch', 'r', '(', '0', ')', '*', '8', '\\', 'n']
Detokenized (009): ['option_pattern', '=', 'chr', '(', '0', ')', '*', '8', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "begin = toint ( s [ 5 : 9 ] ) \n"
Original    (012): ['begin', '=', 'toint', '(', 's', '[', '5', ':', '9', ']', ')', '\\n']
Tokenized   (016): ['<s>', 'begin', '=', 'to', 'int', '(', 's', '[', '5', ':', '9', ']', ')', '\\', 'n', '</s>']
Filtered   (014): ['begin', '=', 'to', 'int', '(', 's', '[', '5', ':', '9', ']', ')', '\\', 'n']
Detokenized (012): ['begin', '=', 'toint', '(', 's', '[', '5', ':', '9', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "length = len ( s ) - 9 \n"
Original    (009): ['length', '=', 'len', '(', 's', ')', '-', '9', '\\n']
Tokenized   (012): ['<s>', 'length', '=', 'len', '(', 's', ')', '-', '9', '\\', 'n', '</s>']
Filtered   (010): ['length', '=', 'len', '(', 's', ')', '-', '9', '\\', 'n']
Detokenized (009): ['length', '=', 'len', '(', 's', ')', '-', '9', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "DivergeCommits = namedtuple ( "DivergeCommits" , [ "common_parent" , \n"
Original    (010): ['DivergeCommits', '=', 'namedtuple', '(', '"DivergeCommits"', ',', '[', '"common_parent"', ',', '\\n']
Tokenized   (029): ['<s>', 'D', 'iver', 'ge', 'Comm', 'its', '=', 'named', 't', 'uple', '(', '"', 'D', 'iver', 'ge', 'Comm', 'its', '"', ',', '[', '"', 'common', '_', 'parent', '"', ',', '\\', 'n', '</s>']
Filtered   (027): ['D', 'iver', 'ge', 'Comm', 'its', '=', 'named', 't', 'uple', '(', '"', 'D', 'iver', 'ge', 'Comm', 'its', '"', ',', '[', '"', 'common', '_', 'parent', '"', ',', '\\', 'n']
Detokenized (010): ['DivergeCommits', '=', 'namedtuple', '(', '"DivergeCommits"', ',', '[', '"common_parent"', ',', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : ""first_commits" , "second_commits" ] ) \n"
Original    (006): ['"first_commits"', ',', '"second_commits"', ']', ')', '\\n']
Tokenized   (019): ['<s>', '"', 'first', '_', 'comm', 'its', '"', ',', '"', 'second', '_', 'comm', 'its', '"', ']', ')', '\\', 'n', '</s>']
Filtered   (017): ['"', 'first', '_', 'comm', 'its', '"', ',', '"', 'second', '_', 'comm', 'its', '"', ']', ')', '\\', 'n']
Detokenized (006): ['"first_commits"', ',', '"second_commits"', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "behind = len ( diverge_commits . second_commits ) > 0 \n"
Original    (011): ['behind', '=', 'len', '(', 'diverge_commits', '.', 'second_commits', ')', '>', '0', '\\n']
Tokenized   (021): ['<s>', 'behind', '=', 'len', '(', 'diver', 'ge', '_', 'comm', 'its', '.', 'second', '_', 'comm', 'its', ')', '>', '0', '\\', 'n', '</s>']
Filtered   (019): ['behind', '=', 'len', '(', 'diver', 'ge', '_', 'comm', 'its', '.', 'second', '_', 'comm', 'its', ')', '>', '0', '\\', 'n']
Detokenized (011): ['behind', '=', 'len', '(', 'diverge_commits', '.', 'second_commits', ')', '>', '0', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "onerror = lambda function , fpath , excinfo : log . info ( \n"
Original    (014): ['onerror', '=', 'lambda', 'function', ',', 'fpath', ',', 'excinfo', ':', 'log', '.', 'info', '(', '\\n']
Tokenized   (020): ['<s>', 'oner', 'ror', '=', 'lambda', 'function', ',', 'f', 'path', ',', 'exc', 'info', ':', 'log', '.', 'info', '(', '\\', 'n', '</s>']
Filtered   (018): ['oner', 'ror', '=', 'lambda', 'function', ',', 'f', 'path', ',', 'exc', 'info', ':', 'log', '.', 'info', '(', '\\', 'n']
Detokenized (014): ['onerror', '=', 'lambda', 'function', ',', 'fpath', ',', 'excinfo', ':', 'log', '.', 'info', '(', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "commiter = Signature ( commiter [ 0 ] , commiter [ 1 ] ) \n"
Original    (015): ['commiter', '=', 'Signature', '(', 'commiter', '[', '0', ']', ',', 'commiter', '[', '1', ']', ')', '\\n']
Tokenized   (021): ['<s>', 'comm', 'iter', '=', 'Signature', '(', 'comm', 'iter', '[', '0', ']', ',', 'comm', 'iter', '[', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (019): ['comm', 'iter', '=', 'Signature', '(', 'comm', 'iter', '[', '0', ']', ',', 'comm', 'iter', '[', '1', ']', ')', '\\', 'n']
Detokenized (015): ['commiter', '=', 'Signature', '(', 'commiter', '[', '0', ']', ',', 'commiter', '[', '1', ']', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "len ( path_components ) == 1 and \n"
Original    (008): ['len', '(', 'path_components', ')', '==', '1', 'and', '\\n']
Tokenized   (014): ['<s>', 'len', '(', 'path', '_', 'comp', 'onents', ')', '==', '1', 'and', '\\', 'n', '</s>']
Filtered   (012): ['len', '(', 'path', '_', 'comp', 'onents', ')', '==', '1', 'and', '\\', 'n']
Detokenized (008): ['len', '(', 'path_components', ')', '==', '1', 'and', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "entry_name == path_components [ 0 ] ) \n"
Original    (008): ['entry_name', '==', 'path_components', '[', '0', ']', ')', '\\n']
Tokenized   (016): ['<s>', 'entry', '_', 'name', '==', 'path', '_', 'comp', 'onents', '[', '0', ']', ')', '\\', 'n', '</s>']
Filtered   (014): ['entry', '_', 'name', '==', 'path', '_', 'comp', 'onents', '[', '0', ']', ')', '\\', 'n']
Detokenized (008): ['entry_name', '==', 'path_components', '[', '0', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "lambda entry : self . _repo [ entry . id ] ) \n"
Original    (013): ['lambda', 'entry', ':', 'self', '.', '_repo', '[', 'entry', '.', 'id', ']', ')', '\\n']
Tokenized   (018): ['<s>', 'lambda', 'entry', ':', 'self', '.', '_', 're', 'po', '[', 'entry', '.', 'id', ']', ')', '\\', 'n', '</s>']
Filtered   (016): ['lambda', 'entry', ':', 'self', '.', '_', 're', 'po', '[', 'entry', '.', 'id', ']', ')', '\\', 'n']
Detokenized (013): ['lambda', 'entry', ':', 'self', '.', '_repo', '[', 'entry', '.', 'id', ']', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "GIT_FILEMODE_LINK : { \n"
Original    (004): ['GIT_FILEMODE_LINK', ':', '{', '\\n']
Tokenized   (014): ['<s>', 'G', 'IT', '_', 'FILE', 'MODE', '_', 'L', 'INK', ':', '{', '\\', 'n', '</s>']
Filtered   (012): ['G', 'IT', '_', 'FILE', 'MODE', '_', 'L', 'INK', ':', '{', '\\', 'n']
Detokenized (004): ['GIT_FILEMODE_LINK', ':', '{', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "iterators = [ self . _repo . walk ( branch . target , sort ) \n"
Original    (016): ['iterators', '=', '[', 'self', '.', '_repo', '.', 'walk', '(', 'branch', '.', 'target', ',', 'sort', ')', '\\n']
Tokenized   (022): ['<s>', 'iter', 'ators', '=', '[', 'self', '.', '_', 're', 'po', '.', 'walk', '(', 'branch', '.', 'target', ',', 'sort', ')', '\\', 'n', '</s>']
Filtered   (020): ['iter', 'ators', '=', '[', 'self', '.', '_', 're', 'po', '.', 'walk', '(', 'branch', '.', 'target', ',', 'sort', ')', '\\', 'n']
Detokenized (016): ['iterators', '=', '[', 'self', '.', '_repo', '.', 'walk', '(', 'branch', '.', 'target', ',', 'sort', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "stop_iteration = [ False for branch in branches ] \n"
Original    (010): ['stop_iteration', '=', '[', 'False', 'for', 'branch', 'in', 'branches', ']', '\\n']
Tokenized   (016): ['<s>', 'stop', '_', 'iter', 'ation', '=', '[', 'False', 'for', 'branch', 'in', 'branches', ']', '\\', 'n', '</s>']
Filtered   (014): ['stop', '_', 'iter', 'ation', '=', '[', 'False', 'for', 'branch', 'in', 'branches', ']', '\\', 'n']
Detokenized (010): ['stop_iteration', '=', '[', 'False', 'for', 'branch', 'in', 'branches', ']', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "second_commit in first_commits ) : \n"
Original    (006): ['second_commit', 'in', 'first_commits', ')', ':', '\\n']
Tokenized   (014): ['<s>', 'second', '_', 'commit', 'in', 'first', '_', 'comm', 'its', ')', ':', '\\', 'n', '</s>']
Filtered   (012): ['second', '_', 'commit', 'in', 'first', '_', 'comm', 'its', ')', ':', '\\', 'n']
Detokenized (006): ['second_commit', 'in', 'first_commits', ')', ':', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "new_commit = Commit ( 2 , 2 , "21111111111" ) \n"
Original    (011): ['new_commit', '=', 'Commit', '(', '2', ',', '2', ',', '"21111111111"', ')', '\\n']
Tokenized   (020): ['<s>', 'new', '_', 'commit', '=', 'Commit', '(', '2', ',', '2', ',', '"', '211', '1111', '1111', '"', ')', '\\', 'n', '</s>']
Filtered   (018): ['new', '_', 'commit', '=', 'Commit', '(', '2', ',', '2', ',', '"', '211', '1111', '1111', '"', ')', '\\', 'n']
Detokenized (011): ['new_commit', '=', 'Commit', '(', '2', ',', '2', ',', '"21111111111"', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "mocked_repo . walk . assert_called_once_with ( "head" , GIT_SORT_TIME ) \n"
Original    (011): ['mocked_repo', '.', 'walk', '.', 'assert_called_once_with', '(', '"head"', ',', 'GIT_SORT_TIME', ')', '\\n']
Tokenized   (032): ['<s>', 'm', 'ocked', '_', 're', 'po', '.', 'walk', '.', 'assert', '_', 'called', '_', 'once', '_', 'with', '(', '"', 'head', '"', ',', 'G', 'IT', '_', 'S', 'ORT', '_', 'TIME', ')', '\\', 'n', '</s>']
Filtered   (030): ['m', 'ocked', '_', 're', 'po', '.', 'walk', '.', 'assert', '_', 'called', '_', 'once', '_', 'with', '(', '"', 'head', '"', ',', 'G', 'IT', '_', 'S', 'ORT', '_', 'TIME', ')', '\\', 'n']
Detokenized (011): ['mocked_repo', '.', 'walk', '.', 'assert_called_once_with', '(', '"head"', ',', 'GIT_SORT_TIME', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "to_datetime = True ) == datetime \n"
Original    (007): ['to_datetime', '=', 'True', ')', '==', 'datetime', '\\n']
Tokenized   (014): ['<s>', 'to', '_', 'dat', 'etime', '=', 'True', ')', '==', 'dat', 'etime', '\\', 'n', '</s>']
Filtered   (012): ['to', '_', 'dat', 'etime', '=', 'True', ')', '==', 'dat', 'etime', '\\', 'n']
Detokenized (007): ['to_datetime', '=', 'True', ')', '==', 'datetime', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "date = dt . date ( 1970 , 1 , 1 ) \n"
Original    (013): ['date', '=', 'dt', '.', 'date', '(', '1970', ',', '1', ',', '1', ')', '\\n']
Tokenized   (017): ['<s>', 'date', '=', 'd', 't', '.', 'date', '(', '1970', ',', '1', ',', '1', ')', '\\', 'n', '</s>']
Filtered   (015): ['date', '=', 'd', 't', '.', 'date', '(', '1970', ',', '1', ',', '1', ')', '\\', 'n']
Detokenized (013): ['date', '=', 'dt', '.', 'date', '(', '1970', ',', '1', ',', '1', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "datetime = dt . datetime ( 1970 , 1 , 1 , 13 , 30 ) \n"
Original    (017): ['datetime', '=', 'dt', '.', 'datetime', '(', '1970', ',', '1', ',', '1', ',', '13', ',', '30', ')', '\\n']
Tokenized   (023): ['<s>', 'dat', 'etime', '=', 'd', 't', '.', 'dat', 'etime', '(', '1970', ',', '1', ',', '1', ',', '13', ',', '30', ')', '\\', 'n', '</s>']
Filtered   (021): ['dat', 'etime', '=', 'd', 't', '.', 'dat', 'etime', '(', '1970', ',', '1', ',', '1', ',', '13', ',', '30', ')', '\\', 'n']
Detokenized (017): ['datetime', '=', 'dt', '.', 'datetime', '(', '1970', ',', '1', ',', '1', ',', '13', ',', '30', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "internationalizeDocstring = lambda x : x \n"
Original    (007): ['internationalizeDocstring', '=', 'lambda', 'x', ':', 'x', '\\n']
Tokenized   (013): ['<s>', 'international', 'ize', 'Doc', 'string', '=', 'lambda', 'x', ':', 'x', '\\', 'n', '</s>']
Filtered   (011): ['international', 'ize', 'Doc', 'string', '=', 'lambda', 'x', ':', 'x', '\\', 'n']
Detokenized (007): ['internationalizeDocstring', '=', 'lambda', 'x', ':', 'x', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "conf . supybot . drivers . maxReconnectWait ( ) ) \n"
Original    (011): ['conf', '.', 'supybot', '.', 'drivers', '.', 'maxReconnectWait', '(', ')', ')', '\\n']
Tokenized   (020): ['<s>', 'conf', '.', 'sup', 'y', 'bot', '.', 'drivers', '.', 'max', 'Rec', 'on', 'nect', 'Wait', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (018): ['conf', '.', 'sup', 'y', 'bot', '.', 'drivers', '.', 'max', 'Rec', 'on', 'nect', 'Wait', '(', ')', ')', '\\', 'n']
Detokenized (011): ['conf', '.', 'supybot', '.', 'drivers', '.', 'maxReconnectWait', '(', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "inst . conn . _sock . __class__ is socket . _closedsocket ) : \n"
Original    (014): ['inst', '.', 'conn', '.', '_sock', '.', '__class__', 'is', 'socket', '.', '_closedsocket', ')', ':', '\\n']
Tokenized   (023): ['<s>', 'inst', '.', 'conn', '.', '_', 's', 'ock', '.', '__', 'class', '__', 'is', 'socket', '.', '_', 'closed', 'socket', ')', ':', '\\', 'n', '</s>']
Filtered   (021): ['inst', '.', 'conn', '.', '_', 's', 'ock', '.', '__', 'class', '__', 'is', 'socket', '.', '_', 'closed', 'socket', ')', ':', '\\', 'n']
Detokenized (014): ['inst', '.', 'conn', '.', '_sock', '.', '__class__', 'is', 'socket', '.', '_closedsocket', ')', ':', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "network_config = getattr ( conf . supybot . networks , self . irc . network ) \n"
Original    (017): ['network_config', '=', 'getattr', '(', 'conf', '.', 'supybot', '.', 'networks', ',', 'self', '.', 'irc', '.', 'network', ')', '\\n']
Tokenized   (026): ['<s>', 'network', '_', 'config', '=', 'get', 'attr', '(', 'conf', '.', 'sup', 'y', 'bot', '.', 'networks', ',', 'self', '.', '', 'irc', '.', 'network', ')', '\\', 'n', '</s>']
Filtered   (024): ['network', '_', 'config', '=', 'get', 'attr', '(', 'conf', '.', 'sup', 'y', 'bot', '.', 'networks', ',', 'self', '.', '', 'irc', '.', 'network', ')', '\\', 'n']
Detokenized (017): ['network_config', '=', 'getattr', '(', 'conf', '.', 'supybot', '.', 'networks', ',', 'self', '.', 'irc', '.', 'network', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "vhost = conf . supybot . protocols . irc . vhost ( ) , \n"
Original    (015): ['vhost', '=', 'conf', '.', 'supybot', '.', 'protocols', '.', 'irc', '.', 'vhost', '(', ')', ',', '\\n']
Tokenized   (023): ['<s>', 'v', 'host', '=', 'conf', '.', 'sup', 'y', 'bot', '.', 'protocols', '.', '', 'irc', '.', 'v', 'host', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (021): ['v', 'host', '=', 'conf', '.', 'sup', 'y', 'bot', '.', 'protocols', '.', '', 'irc', '.', 'v', 'host', '(', ')', ',', '\\', 'n']
Detokenized (015): ['vhost', '=', 'conf', '.', 'supybot', '.', 'protocols', '.', 'irc', '.', 'vhost', '(', ')', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "trusted_fingerprints = network_config . ssl . serverFingerprints ( ) , \n"
Original    (011): ['trusted_fingerprints', '=', 'network_config', '.', 'ssl', '.', 'serverFingerprints', '(', ')', ',', '\\n']
Tokenized   (024): ['<s>', 'tr', 'usted', '_', 'finger', 'prints', '=', 'network', '_', 'config', '.', 's', 'sl', '.', 'server', 'F', 'inger', 'prints', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (022): ['tr', 'usted', '_', 'finger', 'prints', '=', 'network', '_', 'config', '.', 's', 'sl', '.', 'server', 'F', 'inger', 'prints', '(', ')', ',', '\\', 'n']
Detokenized (011): ['trusted_fingerprints', '=', 'network_config', '.', 'ssl', '.', 'serverFingerprints', '(', ')', ',', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "while tb : \n"
Original    (004): ['while', 'tb', ':', '\\n']
Tokenized   (008): ['<s>', 'while', 't', 'b', ':', '\\', 'n', '</s>']
Filtered   (006): ['while', 't', 'b', ':', '\\', 'n']
Detokenized (004): ['while', 'tb', ':', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "frame . f_lineno ) ) \n"
Original    (006): ['frame', '.', 'f_lineno', ')', ')', '\\n']
Tokenized   (012): ['<s>', 'frame', '.', 'f', '_', 'lin', 'eno', ')', ')', '\\', 'n', '</s>']
Filtered   (010): ['frame', '.', 'f', '_', 'lin', 'eno', ')', ')', '\\', 'n']
Detokenized (006): ['frame', '.', 'f_lineno', ')', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "window = timedelta ( 0 , 0 , 0 , 0 , settings . SESSION_TIMEOUT ) \n"
Original    (017): ['window', '=', 'timedelta', '(', '0', ',', '0', ',', '0', ',', '0', ',', 'settings', '.', 'SESSION_TIMEOUT', ')', '\\n']
Tokenized   (025): ['<s>', 'window', '=', 'timed', 'elta', '(', '0', ',', '0', ',', '0', ',', '0', ',', 'settings', '.', 'S', 'ESSION', '_', 'TIME', 'OUT', ')', '\\', 'n', '</s>']
Filtered   (023): ['window', '=', 'timed', 'elta', '(', '0', ',', '0', ',', '0', ',', '0', ',', 'settings', '.', 'S', 'ESSION', '_', 'TIME', 'OUT', ')', '\\', 'n']
Detokenized (017): ['window', '=', 'timedelta', '(', '0', ',', '0', ',', '0', ',', '0', ',', 'settings', '.', 'SESSION_TIMEOUT', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "shared = request . POST . get ( "shared" , False ) \n"
Original    (013): ['shared', '=', 'request', '.', 'POST', '.', 'get', '(', '"shared"', ',', 'False', ')', '\\n']
Tokenized   (018): ['<s>', 'shared', '=', 'request', '.', 'POST', '.', 'get', '(', '"', 'shared', '"', ',', 'False', ')', '\\', 'n', '</s>']
Filtered   (016): ['shared', '=', 'request', '.', 'POST', '.', 'get', '(', '"', 'shared', '"', ',', 'False', ')', '\\', 'n']
Detokenized (013): ['shared', '=', 'request', '.', 'POST', '.', 'get', '(', '"shared"', ',', 'False', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "} , p . score_functions . all ( ) . filter ( selectable_bodies = plan . legislative_body } , report_displays [ 0 ] . scorepanel_set . all ( ) . order_by ( ) ) \n"
Original    (035): ['}', ',', 'p', '.', 'score_functions', '.', 'all', '(', ')', '.', 'filter', '(', 'selectable_bodies', '=', 'plan', '.', 'legislative_body', '}', ',', 'report_displays', '[', '0', ']', '.', 'scorepanel_set', '.', 'all', '(', ')', '.', 'order_by', '(', ')', ')', '\\n']
Tokenized   (055): ['<s>', '}', ',', 'p', '.', 'score', '_', 'fun', 'ctions', '.', 'all', '(', ')', '.', 'filter', '(', 'select', 'able', '_', 'b', 'odies', '=', 'plan', '.', 'legislative', '_', 'body', '}', ',', 'report', '_', 'dis', 'plays', '[', '0', ']', '.', 'score', 'panel', '_', 'set', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (053): ['}', ',', 'p', '.', 'score', '_', 'fun', 'ctions', '.', 'all', '(', ')', '.', 'filter', '(', 'select', 'able', '_', 'b', 'odies', '=', 'plan', '.', 'legislative', '_', 'body', '}', ',', 'report', '_', 'dis', 'plays', '[', '0', ']', '.', 'score', 'panel', '_', 'set', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ')', ')', '\\', 'n']
Detokenized (035): ['}', ',', 'p', '.', 'score_functions', '.', 'all', '(', ')', '.', 'filter', '(', 'selectable_bodies', '=', 'plan', '.', 'legislative_body', '}', ',', 'report_displays', '[', '0', ']', '.', 'scorepanel_set', '.', 'all', '(', ')', '.', 'order_by', '(', ')', ')', '\\n']
Counter: 53
===================================================================
Hidden states:  (13, 35, 768)
# Extracted words:  35
Sentence         : "body_member_long_label = _ ( ) + \n"
Original    (007): ['body_member_long_label', '=', '_', '(', ')', '+', '\\n']
Tokenized   (016): ['<s>', 'body', '_', 'member', '_', 'long', '_', 'label', '=', '_', '(', ')', '+', '\\', 'n', '</s>']
Filtered   (014): ['body', '_', 'member', '_', 'long', '_', 'label', '=', '_', '(', ')', '+', '\\', 'n']
Detokenized (007): ['body_member_long_label', '=', '_', '(', ')', '+', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "body_members = _n ( , , 2 ) \n"
Original    (009): ['body_members', '=', '_n', '(', ',', ',', '2', ')', '\\n']
Tokenized   (015): ['<s>', 'body', '_', 'members', '=', '_', 'n', '(', ',', ',', '2', ')', '\\', 'n', '</s>']
Filtered   (013): ['body', '_', 'members', '=', '_', 'n', '(', ',', ',', '2', ')', '\\', 'n']
Detokenized (009): ['body_members', '=', '_n', '(', ',', ',', '2', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "has_regions = Region . objects . all ( ) . count ( ) > 1 \n"
Original    (016): ['has_regions', '=', 'Region', '.', 'objects', '.', 'all', '(', ')', '.', 'count', '(', ')', '>', '1', '\\n']
Tokenized   (022): ['<s>', 'has', '_', 'reg', 'ions', '=', 'Region', '.', 'objects', '.', 'all', '(', ')', '.', 'count', '(', ')', '>', '1', '\\', 'n', '</s>']
Filtered   (020): ['has', '_', 'reg', 'ions', '=', 'Region', '.', 'objects', '.', 'all', '(', ')', '.', 'count', '(', ')', '>', '1', '\\', 'n']
Detokenized (016): ['has_regions', '=', 'Region', '.', 'objects', '.', 'all', '(', ')', '.', 'count', '(', ')', '>', '1', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "bodies = LegislativeBody . objects . all ( ) . order_by ( , ) \n"
Original    (015): ['bodies', '=', 'LegislativeBody', '.', 'objects', '.', 'all', '(', ')', '.', 'order_by', '(', ',', ')', '\\n']
Tokenized   (022): ['<s>', 'b', 'odies', '=', 'Legislative', 'Body', '.', 'objects', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (020): ['b', 'odies', '=', 'Legislative', 'Body', '.', 'objects', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ',', ')', '\\', 'n']
Detokenized (015): ['bodies', '=', 'LegislativeBody', '.', 'objects', '.', 'all', '(', ')', '.', 'order_by', '(', ',', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "l_bodies = [ b for b in bodies if b in [ sd . legislative_body for sd in ScoreDisplay . objects . filter \n"
Original    (024): ['l_bodies', '=', '[', 'b', 'for', 'b', 'in', 'bodies', 'if', 'b', 'in', '[', 'sd', '.', 'legislative_body', 'for', 'sd', 'in', 'ScoreDisplay', '.', 'objects', '.', 'filter', '\\n']
Tokenized   (033): ['<s>', 'l', '_', 'b', 'odies', '=', '[', 'b', 'for', 'b', 'in', 'bodies', 'if', 'b', 'in', '[', 'sd', '.', 'legislative', '_', 'body', 'for', 'sd', 'in', 'Score', 'Display', '.', 'objects', '.', 'filter', '\\', 'n', '</s>']
Filtered   (031): ['l', '_', 'b', 'odies', '=', '[', 'b', 'for', 'b', 'in', 'bodies', 'if', 'b', 'in', '[', 'sd', '.', 'legislative', '_', 'body', 'for', 'sd', 'in', 'Score', 'Display', '.', 'objects', '.', 'filter', '\\', 'n']
Detokenized (024): ['l_bodies', '=', '[', 'b', 'for', 'b', 'in', 'bodies', 'if', 'b', 'in', '[', 'sd', '.', 'legislative_body', 'for', 'sd', 'in', 'ScoreDisplay', '.', 'objects', '.', 'filter', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "cfg [ ] = datetime . now ( ) \n"
Original    (010): ['cfg', '[', ']', '=', 'datetime', '.', 'now', '(', ')', '\\n']
Tokenized   (014): ['<s>', 'cfg', '[', ']', '=', 'dat', 'etime', '.', 'now', '(', ')', '\\', 'n', '</s>']
Filtered   (012): ['cfg', '[', ']', '=', 'dat', 'etime', '.', 'now', '(', ')', '\\', 'n']
Detokenized (010): ['cfg', '[', ']', '=', 'datetime', '.', 'now', '(', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "ll = ModestMaps . Geo . Location ( pt1 . y , pt1 . x ) \n"
Original    (017): ['ll', '=', 'ModestMaps', '.', 'Geo', '.', 'Location', '(', 'pt1', '.', 'y', ',', 'pt1', '.', 'x', ')', '\\n']
Tokenized   (023): ['<s>', 'll', '=', 'Modest', 'Maps', '.', 'Geo', '.', 'Location', '(', 'pt', '1', '.', 'y', ',', 'pt', '1', '.', 'x', ')', '\\', 'n', '</s>']
Filtered   (021): ['ll', '=', 'Modest', 'Maps', '.', 'Geo', '.', 'Location', '(', 'pt', '1', '.', 'y', ',', 'pt', '1', '.', 'x', ')', '\\', 'n']
Detokenized (017): ['ll', '=', 'ModestMaps', '.', 'Geo', '.', 'Location', '(', 'pt1', '.', 'y', ',', 'pt1', '.', 'x', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "provider = ModestMaps . WMS . Provider ( cfg [ ] , { \n"
Original    (014): ['provider', '=', 'ModestMaps', '.', 'WMS', '.', 'Provider', '(', 'cfg', '[', ']', ',', '{', '\\n']
Tokenized   (021): ['<s>', 'prov', 'ider', '=', 'Modest', 'Maps', '.', 'W', 'MS', '.', 'Provider', '(', 'cf', 'g', '[', ']', ',', '{', '\\', 'n', '</s>']
Filtered   (019): ['prov', 'ider', '=', 'Modest', 'Maps', '.', 'W', 'MS', '.', 'Provider', '(', 'cf', 'g', '[', ']', ',', '{', '\\', 'n']
Detokenized (014): ['provider', '=', 'ModestMaps', '.', 'WMS', '.', 'Provider', '(', 'cfg', '[', ']', ',', '{', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "overlayImg = Image . blend ( overlayImg , ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) , \n"
Original    (026): ['overlayImg', '=', 'Image', '.', 'blend', '(', 'overlayImg', ',', 'ModestMaps', '.', 'mapByExtent', '(', 'provider', ',', 'll', ',', 'ur', ',', 'dims', ')', '.', 'draw', '(', ')', ',', '\\n']
Tokenized   (039): ['<s>', 'over', 'lay', 'Im', 'g', '=', 'Image', '.', 'blend', '(', 'overlay', 'Im', 'g', ',', 'Modest', 'Maps', '.', 'map', 'By', 'Ext', 'ent', '(', 'provider', ',', 'll', ',', 'ur', ',', 'dim', 's', ')', '.', 'draw', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (037): ['over', 'lay', 'Im', 'g', '=', 'Image', '.', 'blend', '(', 'overlay', 'Im', 'g', ',', 'Modest', 'Maps', '.', 'map', 'By', 'Ext', 'ent', '(', 'provider', ',', 'll', ',', 'ur', ',', 'dim', 's', ')', '.', 'draw', '(', ')', ',', '\\', 'n']
Detokenized (026): ['overlayImg', '=', 'Image', '.', 'blend', '(', 'overlayImg', ',', 'ModestMaps', '.', 'mapByExtent', '(', 'provider', ',', 'll', ',', 'ur', ',', 'dims', ')', '.', 'draw', '(', ')', ',', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "fullImg . save ( settings . WEB_TEMP + ( % sha . hexdigest ( ) ) , , quality = 100 ) \n"
Original    (023): ['fullImg', '.', 'save', '(', 'settings', '.', 'WEB_TEMP', '+', '(', '%', 'sha', '.', 'hexdigest', '(', ')', ')', ',', ',', 'quality', '=', '100', ')', '\\n']
Tokenized   (035): ['<s>', 'full', 'Im', 'g', '.', 'save', '(', 'settings', '.', 'WE', 'B', '_', 'T', 'EMP', '+', '(', '%', 'sh', 'a', '.', 'hex', 'dig', 'est', '(', ')', ')', ',', ',', 'quality', '=', '100', ')', '\\', 'n', '</s>']
Filtered   (033): ['full', 'Im', 'g', '.', 'save', '(', 'settings', '.', 'WE', 'B', '_', 'T', 'EMP', '+', '(', '%', 'sh', 'a', '.', 'hex', 'dig', 'est', '(', ')', ')', ',', ',', 'quality', '=', '100', ')', '\\', 'n']
Detokenized (023): ['fullImg', '.', 'save', '(', 'settings', '.', 'WEB_TEMP', '+', '(', '%', 'sha', '.', 'hexdigest', '(', ')', ')', ',', ',', 'quality', '=', '100', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "CreatePDF ( page , result , show_error_as_pdf = True ) \n"
Original    (011): ['CreatePDF', '(', 'page', ',', 'result', ',', 'show_error_as_pdf', '=', 'True', ')', '\\n']
Tokenized   (021): ['<s>', 'Create', 'PDF', '(', 'page', ',', 'result', ',', 'show', '_', 'error', '_', 'as', '_', 'pdf', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (019): ['Create', 'PDF', '(', 'page', ',', 'result', ',', 'show', '_', 'error', '_', 'as', '_', 'pdf', '=', 'True', ')', '\\', 'n']
Detokenized (011): ['CreatePDF', '(', 'page', ',', 'result', ',', 'show_error_as_pdf', '=', 'True', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "body = LegislativeBody . objects . get ( id = int ( request . POST [ ] ) ) \n"
Original    (020): ['body', '=', 'LegislativeBody', '.', 'objects', '.', 'get', '(', 'id', '=', 'int', '(', 'request', '.', 'POST', '[', ']', ')', ')', '\\n']
Tokenized   (024): ['<s>', 'body', '=', 'Legislative', 'Body', '.', 'objects', '.', 'get', '(', 'id', '=', 'int', '(', 'request', '.', 'POST', '[', ']', ')', ')', '\\', 'n', '</s>']
Filtered   (022): ['body', '=', 'Legislative', 'Body', '.', 'objects', '.', 'get', '(', 'id', '=', 'int', '(', 'request', '.', 'POST', '[', ']', ')', ')', '\\', 'n']
Detokenized (020): ['body', '=', 'LegislativeBody', '.', 'objects', '.', 'get', '(', 'id', '=', 'int', '(', 'request', '.', 'POST', '[', ']', ')', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "PlanReport . createreport . delay ( planid , stamp , req , language = translation . get_language ( ) ) \n"
Original    (021): ['PlanReport', '.', 'createreport', '.', 'delay', '(', 'planid', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get_language', '(', ')', ')', '\\n']
Tokenized   (029): ['<s>', 'Plan', 'Report', '.', 'create', 'report', '.', 'delay', '(', 'plan', 'id', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get', '_', 'language', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (027): ['Plan', 'Report', '.', 'create', 'report', '.', 'delay', '(', 'plan', 'id', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get', '_', 'language', '(', ')', ')', '\\', 'n']
Detokenized (021): ['PlanReport', '.', 'createreport', '.', 'delay', '(', 'planid', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get_language', '(', ')', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "stamp = request . POST . get ( , sha . hexdigest ( ) ) \n"
Original    (016): ['stamp', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'sha', '.', 'hexdigest', '(', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'st', 'amp', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'sh', 'a', '.', 'hex', 'dig', 'est', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['st', 'amp', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'sh', 'a', '.', 'hex', 'dig', 'est', '(', ')', ')', '\\', 'n']
Detokenized (016): ['stamp', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'sha', '.', 'hexdigest', '(', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "CalculatorReport . createcalculatorreport . delay ( planid , stamp , req , language = translation . get_language ~~ else : \n"
Original    (021): ['CalculatorReport', '.', 'createcalculatorreport', '.', 'delay', '(', 'planid', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get_language', '~~', 'else', ':', '\\n']
Tokenized   (035): ['<s>', 'Cal', 'cul', 'ator', 'Report', '.', 'create', 'cal', 'cul', 'ator', 'report', '.', 'delay', '(', 'plan', 'id', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get', '_', 'language', '', '~~', 'else', ':', '\\', 'n', '</s>']
Filtered   (033): ['Cal', 'cul', 'ator', 'Report', '.', 'create', 'cal', 'cul', 'ator', 'report', '.', 'delay', '(', 'plan', 'id', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get', '_', 'language', '', '~~', 'else', ':', '\\', 'n']
Detokenized (021): ['CalculatorReport', '.', 'createcalculatorreport', '.', 'delay', '(', 'planid', ',', 'stamp', ',', 'req', ',', 'language', '=', 'translation', '.', 'get_language', '~~', 'else', ':', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "site_id = Site . objects . get_current ( ) . id , \n"
Original    (013): ['site_id', '=', 'Site', '.', 'objects', '.', 'get_current', '(', ')', '.', 'id', ',', '\\n']
Tokenized   (020): ['<s>', 'site', '_', 'id', '=', 'Site', '.', 'objects', '.', 'get', '_', 'current', '(', ')', '.', 'id', ',', '\\', 'n', '</s>']
Filtered   (018): ['site', '_', 'id', '=', 'Site', '.', 'objects', '.', 'get', '_', 'current', '(', ')', '.', 'id', ',', '\\', 'n']
Detokenized (013): ['site_id', '=', 'Site', '.', 'objects', '.', 'get_current', '(', ')', '.', 'id', ',', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "from_id = int ( request . POST . get ( , - 1 ) ) \n"
Original    (016): ['from_id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '-', '1', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'from', '_', 'id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '-', '1', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['from', '_', 'id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '-', '1', ')', ')', '\\', 'n']
Detokenized (016): ['from_id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '-', '1', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "to_id = int ( request . POST . get ( , None ) ) \n"
Original    (015): ['to_id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', 'None', ')', ')', '\\n']
Tokenized   (020): ['<s>', 'to', '_', 'id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', 'None', ')', ')', '\\', 'n', '</s>']
Filtered   (018): ['to', '_', 'id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', 'None', ')', ')', '\\', 'n']
Detokenized (015): ['to_id', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', 'None', ')', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "from_districts = filter ( lambda d : True if d . district_id == from_id else False , all_districts to_district = filter ( lambda d : True if d . district_id == to_id else False , all_districts ) [ 0 ] \n"
Original    (041): ['from_districts', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district_id', '==', 'from_id', 'else', 'False', ',', 'all_districts', 'to_district', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district_id', '==', 'to_id', 'else', 'False', ',', 'all_districts', ')', '[', '0', ']', '\\n']
Tokenized   (067): ['<s>', 'from', '_', 'dist', 'rict', 's', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district', '_', 'id', '==', 'from', '_', 'id', 'else', 'False', ',', 'all', '_', 'dist', 'rict', 's', 'to', '_', 'dist', 'rict', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district', '_', 'id', '==', 'to', '_', 'id', 'else', 'False', ',', 'all', '_', 'dist', 'rict', 's', ')', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (065): ['from', '_', 'dist', 'rict', 's', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district', '_', 'id', '==', 'from', '_', 'id', 'else', 'False', ',', 'all', '_', 'dist', 'rict', 's', 'to', '_', 'dist', 'rict', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district', '_', 'id', '==', 'to', '_', 'id', 'else', 'False', ',', 'all', '_', 'dist', 'rict', 's', ')', '[', '0', ']', '\\', 'n']
Detokenized (041): ['from_districts', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district_id', '==', 'from_id', 'else', 'False', ',', 'all_districts', 'to_district', '=', 'filter', '(', 'lambda', 'd', ':', 'True', 'if', 'd', '.', 'district_id', '==', 'to_id', 'else', 'False', ',', 'all_districts', ')', '[', '0', ']', '\\n']
Counter: 65
===================================================================
Hidden states:  (13, 41, 768)
# Extracted words:  41
Sentence         : "inverse = request . REQUEST [ ] == if in request . REQUEST else False \n"
Original    (016): ['inverse', '=', 'request', '.', 'REQUEST', '[', ']', '==', 'if', 'in', 'request', '.', 'REQUEST', 'else', 'False', '\\n']
Tokenized   (022): ['<s>', 'in', 'verse', '=', 'request', '.', 'RE', 'QUEST', '[', ']', '==', 'if', 'in', 'request', '.', 'RE', 'QUEST', 'else', 'False', '\\', 'n', '</s>']
Filtered   (020): ['in', 'verse', '=', 'request', '.', 'RE', 'QUEST', '[', ']', '==', 'if', 'in', 'request', '.', 'RE', 'QUEST', 'else', 'False', '\\', 'n']
Detokenized (016): ['inverse', '=', 'request', '.', 'REQUEST', '[', ']', '==', 'if', 'in', 'request', '.', 'REQUEST', 'else', 'False', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "my_context . update ( plan . compute_splits ( layer , version = version , inverse = inverse , extended last_item = layer is layers [ - 1 ] \n"
Original    (029): ['my_context', '.', 'update', '(', 'plan', '.', 'compute_splits', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', ',', 'extended', 'last_item', '=', 'layer', 'is', 'layers', '[', '-', '1', ']', '\\n']
Tokenized   (039): ['<s>', 'my', '_', 'context', '.', 'update', '(', 'plan', '.', 'compute', '_', 'spl', 'its', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', ',', 'extended', 'last', '_', 'item', '=', 'layer', 'is', 'layers', '[', '-', '1', ']', '\\', 'n', '</s>']
Filtered   (037): ['my', '_', 'context', '.', 'update', '(', 'plan', '.', 'compute', '_', 'spl', 'its', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', ',', 'extended', 'last', '_', 'item', '=', 'layer', 'is', 'layers', '[', '-', '1', ']', '\\', 'n']
Detokenized (029): ['my_context', '.', 'update', '(', 'plan', '.', 'compute_splits', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', ',', 'extended', 'last_item', '=', 'layer', 'is', 'layers', '[', '-', '1', ']', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "community_info = plan . get_community_type_info ( layer , version = version , inverse = inverse if community_info is not None : \n"
Original    (022): ['community_info', '=', 'plan', '.', 'get_community_type_info', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', 'if', 'community_info', 'is', 'not', 'None', ':', '\\n']
Tokenized   (035): ['<s>', 'community', '_', 'info', '=', 'plan', '.', 'get', '_', 'community', '_', 'type', '_', 'info', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', 'if', 'community', '_', 'info', 'is', 'not', 'None', ':', '\\', 'n', '</s>']
Filtered   (033): ['community', '_', 'info', '=', 'plan', '.', 'get', '_', 'community', '_', 'type', '_', 'info', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', 'if', 'community', '_', 'info', 'is', 'not', 'None', ':', '\\', 'n']
Detokenized (022): ['community_info', '=', 'plan', '.', 'get_community_type_info', '(', 'layer', ',', 'version', '=', 'version', ',', 'inverse', '=', 'inverse', 'if', 'community_info', 'is', 'not', 'None', ':', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "html += report . render ( calc_context ) \n"
Original    (009): ['html', '+=', 'report', '.', 'render', '(', 'calc_context', ')', '\\n']
Tokenized   (014): ['<s>', 'html', '+=', 'report', '.', 'render', '(', 'calc', '_', 'context', ')', '\\', 'n', '</s>']
Filtered   (012): ['html', '+=', 'report', '.', 'render', '(', 'calc', '_', 'context', ')', '\\', 'n']
Detokenized (009): ['html', '+=', 'report', '.', 'render', '(', 'calc_context', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "geounit_ids = string . split ( request . REQUEST [ "geounits" ] , "|" ) \n"
Original    (016): ['geounit_ids', '=', 'string', '.', 'split', '(', 'request', '.', 'REQUEST', '[', '"geounits"', ']', ',', '"|"', ')', '\\n']
Tokenized   (030): ['<s>', 'ge', 'oun', 'it', '_', 'ids', '=', 'string', '.', 'split', '(', 'request', '.', 'RE', 'QUEST', '[', '"', 'ge', 'oun', 'its', '"', ']', ',', '"', '|', '"', ')', '\\', 'n', '</s>']
Filtered   (028): ['ge', 'oun', 'it', '_', 'ids', '=', 'string', '.', 'split', '(', 'request', '.', 'RE', 'QUEST', '[', '"', 'ge', 'oun', 'its', '"', ']', ',', '"', '|', '"', ')', '\\', 'n']
Detokenized (016): ['geounit_ids', '=', 'string', '.', 'split', '(', 'request', '.', 'REQUEST', '[', '"geounits"', ']', ',', '"|"', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "max_version = max ( [ d . version for d in districts ] ) \n"
Original    (015): ['max_version', '=', 'max', '(', '[', 'd', '.', 'version', 'for', 'd', 'in', 'districts', ']', ')', '\\n']
Tokenized   (020): ['<s>', 'max', '_', 'version', '=', 'max', '(', '[', 'd', '.', 'version', 'for', 'd', 'in', 'districts', ']', ')', '\\', 'n', '</s>']
Filtered   (018): ['max', '_', 'version', '=', 'max', '(', '[', 'd', '.', 'version', 'for', 'd', 'in', 'districts', ']', ')', '\\', 'n']
Detokenized (015): ['max_version', '=', 'max', '(', '[', 'd', '.', 'version', 'for', 'd', 'in', 'districts', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "can_undo = max_version > plan . min_version \n"
Original    (008): ['can_undo', '=', 'max_version', '>', 'plan', '.', 'min_version', '\\n']
Tokenized   (017): ['<s>', 'can', '_', 'undo', '=', 'max', '_', 'version', '>', 'plan', '.', 'min', '_', 'version', '\\', 'n', '</s>']
Filtered   (015): ['can', '_', 'undo', '=', 'max', '_', 'version', '>', 'plan', '.', 'min', '_', 'version', '\\', 'n']
Detokenized (008): ['can_undo', '=', 'max_version', '>', 'plan', '.', 'min_version', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "bbox = tuple ( map ( lambda x : float ( x ) , bbox . split ( ) ) ) \n"
Original    (022): ['bbox', '=', 'tuple', '(', 'map', '(', 'lambda', 'x', ':', 'float', '(', 'x', ')', ',', 'bbox', '.', 'split', '(', ')', ')', ')', '\\n']
Tokenized   (027): ['<s>', 'b', 'box', '=', 'tuple', '(', 'map', '(', 'lambda', 'x', ':', 'float', '(', 'x', ')', ',', 'b', 'box', '.', 'split', '(', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (025): ['b', 'box', '=', 'tuple', '(', 'map', '(', 'lambda', 'x', ':', 'float', '(', 'x', ')', ',', 'b', 'box', '.', 'split', '(', ')', ')', ')', '\\', 'n']
Detokenized (022): ['bbox', '=', 'tuple', '(', 'map', '(', 'lambda', 'x', ':', 'float', '(', 'x', ')', ',', 'bbox', '.', 'split', '(', ')', ')', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "wkt = wkt . replace ( , ) . replace ( , ) \n"
Original    (014): ['wkt', '=', 'wkt', '.', 'replace', '(', ',', ')', '.', 'replace', '(', ',', ')', '\\n']
Tokenized   (019): ['<s>', 'w', 'kt', '=', 'w', 'kt', '.', 'replace', '(', ',', ')', '.', 'replace', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (017): ['w', 'kt', '=', 'w', 'kt', '.', 'replace', '(', ',', ')', '.', 'replace', '(', ',', ')', '\\', 'n']
Detokenized (014): ['wkt', '=', 'wkt', '.', 'replace', '(', ',', ')', '.', 'replace', '(', ',', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "districts = [ d . id for d in plan . get_districts_at_version ( version , include_geom = True ) if locked = District . objects . filter ( id__in = districts ) . collect ( ) \n"
Original    (037): ['districts', '=', '[', 'd', '.', 'id', 'for', 'd', 'in', 'plan', '.', 'get_districts_at_version', '(', 'version', ',', 'include_geom', '=', 'True', ')', 'if', 'locked', '=', 'District', '.', 'objects', '.', 'filter', '(', 'id__in', '=', 'districts', ')', '.', 'collect', '(', ')', '\\n']
Tokenized   (055): ['<s>', 'dist', 'rict', 's', '=', '[', 'd', '.', 'id', 'for', 'd', 'in', 'plan', '.', 'get', '_', 'dist', 'rict', 's', '_', 'at', '_', 'version', '(', 'version', ',', 'include', '_', 'ge', 'om', '=', 'True', ')', 'if', 'locked', '=', 'District', '.', 'objects', '.', 'filter', '(', 'id', '__', 'in', '=', 'districts', ')', '.', 'collect', '(', ')', '\\', 'n', '</s>']
Filtered   (053): ['dist', 'rict', 's', '=', '[', 'd', '.', 'id', 'for', 'd', 'in', 'plan', '.', 'get', '_', 'dist', 'rict', 's', '_', 'at', '_', 'version', '(', 'version', ',', 'include', '_', 'ge', 'om', '=', 'True', ')', 'if', 'locked', '=', 'District', '.', 'objects', '.', 'filter', '(', 'id', '__', 'in', '=', 'districts', ')', '.', 'collect', '(', ')', '\\', 'n']
Detokenized (037): ['districts', '=', '[', 'd', '.', 'id', 'for', 'd', 'in', 'plan', '.', 'get_districts_at_version', '(', 'version', ',', 'include_geom', '=', 'True', ')', 'if', 'locked', '=', 'District', '.', 'objects', '.', 'filter', '(', 'id__in', '=', 'districts', ')', '.', 'collect', '(', ')', '\\n']
Counter: 53
===================================================================
Hidden states:  (13, 37, 768)
# Extracted words:  37
Sentence         : "locked_buffered = locked . simplify ( 100 , True ) . buffer ( 100 ) if locked else None \n"
Original    (020): ['locked_buffered', '=', 'locked', '.', 'simplify', '(', '100', ',', 'True', ')', '.', 'buffer', '(', '100', ')', 'if', 'locked', 'else', 'None', '\\n']
Tokenized   (026): ['<s>', 'locked', '_', 'buff', 'ered', '=', 'locked', '.', 'simplify', '(', '100', ',', 'True', ')', '.', 'buffer', '(', '100', ')', 'if', 'locked', 'else', 'None', '\\', 'n', '</s>']
Filtered   (024): ['locked', '_', 'buff', 'ered', '=', 'locked', '.', 'simplify', '(', '100', ',', 'True', ')', '.', 'buffer', '(', '100', ')', 'if', 'locked', 'else', 'None', '\\', 'n']
Detokenized (020): ['locked_buffered', '=', 'locked', '.', 'simplify', '(', '100', ',', 'True', ')', '.', 'buffer', '(', '100', ')', 'if', 'locked', 'else', 'None', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "filtered = Geolevel . objects . get ( id = geolevel ) . geounit_set . filter ( selection ) \n"
Original    (020): ['filtered', '=', 'Geolevel', '.', 'objects', '.', 'get', '(', 'id', '=', 'geolevel', ')', '.', 'geounit_set', '.', 'filter', '(', 'selection', ')', '\\n']
Tokenized   (032): ['<s>', 'fil', 'tered', '=', 'Ge', 'ole', 'vel', '.', 'objects', '.', 'get', '(', 'id', '=', 'ge', 'ole', 'vel', ')', '.', 'ge', 'oun', 'it', '_', 'set', '.', 'filter', '(', 'selection', ')', '\\', 'n', '</s>']
Filtered   (030): ['fil', 'tered', '=', 'Ge', 'ole', 'vel', '.', 'objects', '.', 'get', '(', 'id', '=', 'ge', 'ole', 'vel', ')', '.', 'ge', 'oun', 'it', '_', 'set', '.', 'filter', '(', 'selection', ')', '\\', 'n']
Detokenized (020): ['filtered', '=', 'Geolevel', '.', 'objects', '.', 'get', '(', 'id', '=', 'geolevel', ')', '.', 'geounit_set', '.', 'filter', '(', 'selection', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "pfilter = Q ( legislative_body = leg_body ) & Q ( is_valid = True ) \n"
Original    (016): ['pfilter', '=', 'Q', '(', 'legislative_body', '=', 'leg_body', ')', '&', 'Q', '(', 'is_valid', '=', 'True', ')', '\\n']
Tokenized   (026): ['<s>', 'p', 'filter', '=', 'Q', '(', 'legislative', '_', 'body', '=', 'leg', '_', 'body', ')', '&', 'Q', '(', 'is', '_', 'valid', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (024): ['p', 'filter', '=', 'Q', '(', 'legislative', '_', 'body', '=', 'leg', '_', 'body', ')', '&', 'Q', '(', 'is', '_', 'valid', '=', 'True', ')', '\\', 'n']
Detokenized (016): ['pfilter', '=', 'Q', '(', 'legislative_body', '=', 'leg_body', ')', '&', 'Q', '(', 'is_valid', '=', 'True', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "panels = display . scorepanel_set . all ( ) . order_by ( ) \n"
Original    (014): ['panels', '=', 'display', '.', 'scorepanel_set', '.', 'all', '(', ')', '.', 'order_by', '(', ')', '\\n']
Tokenized   (023): ['<s>', 'pan', 'els', '=', 'display', '.', 'score', 'panel', '_', 'set', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ')', '\\', 'n', '</s>']
Filtered   (021): ['pan', 'els', '=', 'display', '.', 'score', 'panel', '_', 'set', '.', 'all', '(', ')', '.', 'order', '_', 'by', '(', ')', '\\', 'n']
Detokenized (014): ['panels', '=', 'display', '.', 'scorepanel_set', '.', 'all', '(', ')', '.', 'order_by', '(', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "writer . writerow ( [ , , ] + [ p . __unicode__ ( ) for p in panels ] ) \n"
Original    (022): ['writer', '.', 'writerow', '(', '[', ',', ',', ']', '+', '[', 'p', '.', '__unicode__', '(', ')', 'for', 'p', 'in', 'panels', ']', ')', '\\n']
Tokenized   (029): ['<s>', 'writer', '.', 'writer', 'ow', '(', '[', ',', ',', ']', '+', '[', 'p', '.', '__', 'unic', 'ode', '__', '(', ')', 'for', 'p', 'in', 'panels', ']', ')', '\\', 'n', '</s>']
Filtered   (027): ['writer', '.', 'writer', 'ow', '(', '[', ',', ',', ']', '+', '[', 'p', '.', '__', 'unic', 'ode', '__', '(', ')', 'for', 'p', 'in', 'panels', ']', ')', '\\', 'n']
Detokenized (022): ['writer', '.', 'writerow', '(', '[', ',', ',', ']', '+', '[', 'p', '.', '__unicode__', '(', ')', 'for', 'p', 'in', 'panels', ']', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "rows = int ( request . POST . get ( , 10 ) ) \n"
Original    (015): ['rows', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '10', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'rows', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '10', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['rows', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '10', ')', ')', '\\', 'n']
Detokenized (015): ['rows', '=', 'int', '(', 'request', '.', 'POST', '.', 'get', '(', ',', '10', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "sidx = request . POST . get ( , ) \n"
Original    (011): ['sidx', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', '\\n']
Tokenized   (015): ['<s>', 'sid', 'x', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', '\\', 'n', '</s>']
Filtered   (013): ['sid', 'x', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', '\\', 'n']
Detokenized (011): ['sidx', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "owner_filter = request . POST . get ( ) ; \n"
Original    (011): ['owner_filter', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ';', '\\n']
Tokenized   (016): ['<s>', 'owner', '_', 'filter', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ';', '\\', 'n', '</s>']
Filtered   (014): ['owner', '_', 'filter', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ';', '\\', 'n']
Detokenized (011): ['owner_filter', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ';', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "body_pk = int ( body_pk ) if body_pk else body_pk ; \n"
Original    (012): ['body_pk', '=', 'int', '(', 'body_pk', ')', 'if', 'body_pk', 'else', 'body_pk', ';', '\\n']
Tokenized   (027): ['<s>', 'body', '_', 'p', 'k', '=', 'int', '(', 'body', '_', 'p', 'k', ')', 'if', 'body', '_', 'p', 'k', 'else', 'body', '_', 'p', 'k', ';', '\\', 'n', '</s>']
Filtered   (025): ['body', '_', 'p', 'k', '=', 'int', '(', 'body', '_', 'p', 'k', ')', 'if', 'body', '_', 'p', 'k', 'else', 'body', '_', 'p', 'k', ';', '\\', 'n']
Detokenized (012): ['body_pk', '=', 'int', '(', 'body_pk', ')', 'if', 'body_pk', 'else', 'body_pk', ';', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "search = request . POST . get ( , False ) ; \n"
Original    (013): ['search', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', ';', '\\n']
Tokenized   (016): ['<s>', 'search', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', ';', '\\', 'n', '</s>']
Filtered   (014): ['search', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', ';', '\\', 'n']
Detokenized (013): ['search', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', ';', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "search_string = request . POST . get ( , ) ; \n"
Original    (012): ['search_string', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', ';', '\\n']
Tokenized   (017): ['<s>', 'search', '_', 'string', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', ';', '\\', 'n', '</s>']
Filtered   (015): ['search', '_', 'string', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', ';', '\\', 'n']
Detokenized (012): ['search_string', '=', 'request', '.', 'POST', '.', 'get', '(', ',', ')', ';', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "is_community = request . POST . get ( , False ) == ; \n"
Original    (014): ['is_community', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', '==', ';', '\\n']
Tokenized   (019): ['<s>', 'is', '_', 'community', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', '==', ';', '\\', 'n', '</s>']
Filtered   (017): ['is', '_', 'community', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', '==', ';', '\\', 'n']
Detokenized (014): ['is_community', '=', 'request', '.', 'POST', '.', 'get', '(', ',', 'False', ')', '==', ';', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "all_plans = Plan . objects . filter ( available , not_creating , search_filter , community_filter ) . order_by \n"
Original    (019): ['all_plans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'available', ',', 'not_creating', ',', 'search_filter', ',', 'community_filter', ')', '.', 'order_by', '\\n']
Tokenized   (034): ['<s>', 'all', '_', 'pl', 'ans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'available', ',', 'not', '_', 'creat', 'ing', ',', 'search', '_', 'filter', ',', 'community', '_', 'filter', ')', '.', 'order', '_', 'by', '\\', 'n', '</s>']
Filtered   (032): ['all', '_', 'pl', 'ans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'available', ',', 'not', '_', 'creat', 'ing', ',', 'search', '_', 'filter', ',', 'community', '_', 'filter', ')', '.', 'order', '_', 'by', '\\', 'n']
Detokenized (019): ['all_plans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'available', ',', 'not_creating', ',', 'search_filter', ',', 'community_filter', ')', '.', 'order_by', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "all_districts = ( ) \n"
Original    (005): ['all_districts', '=', '(', ')', '\\n']
Tokenized   (012): ['<s>', 'all', '_', 'dist', 'rict', 's', '=', '(', ')', '\\', 'n', '</s>']
Filtered   (010): ['all', '_', 'dist', 'rict', 's', '=', '(', ')', '\\', 'n']
Detokenized (005): ['all_districts', '=', '(', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "_ ( ) ) \n"
Original    (005): ['_', '(', ')', ')', '\\n']
Tokenized   (008): ['<s>', '_', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (006): ['_', '(', ')', ')', '\\', 'n']
Detokenized (005): ['_', '(', ')', ')', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "user_functions = ScoreFunction . objects . filter ( selectable_bodies = plan . legislative_body ) . order_by for f in user_functions : \n"
Original    (022): ['user_functions', '=', 'ScoreFunction', '.', 'objects', '.', 'filter', '(', 'selectable_bodies', '=', 'plan', '.', 'legislative_body', ')', '.', 'order_by', 'for', 'f', 'in', 'user_functions', ':', '\\n']
Tokenized   (040): ['<s>', 'user', '_', 'fun', 'ctions', '=', 'Score', 'Function', '.', 'objects', '.', 'filter', '(', 'select', 'able', '_', 'b', 'odies', '=', 'plan', '.', 'legislative', '_', 'body', ')', '.', 'order', '_', 'by', 'for', 'f', 'in', 'user', '_', 'fun', 'ctions', ':', '\\', 'n', '</s>']
Filtered   (038): ['user', '_', 'fun', 'ctions', '=', 'Score', 'Function', '.', 'objects', '.', 'filter', '(', 'select', 'able', '_', 'b', 'odies', '=', 'plan', '.', 'legislative', '_', 'body', ')', '.', 'order', '_', 'by', 'for', 'f', 'in', 'user', '_', 'fun', 'ctions', ':', '\\', 'n']
Detokenized (022): ['user_functions', '=', 'ScoreFunction', '.', 'objects', '.', 'filter', '(', 'selectable_bodies', '=', 'plan', '.', 'legislative_body', ')', '.', 'order_by', 'for', 'f', 'in', 'user_functions', ':', '\\n']
Counter: 38
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : ""%s_sidebar_demo" % plan . legislative_body . name , \n"
Original    (009): ['"%s_sidebar_demo"', '%', 'plan', '.', 'legislative_body', '.', 'name', ',', '\\n']
Tokenized   (023): ['<s>', '"', '%', 's', '_', 'side', 'bar', '_', 'dem', 'o', '"', '%', 'plan', '.', 'legislative', '_', 'body', '.', 'name', ',', '\\', 'n', '</s>']
Filtered   (021): ['"', '%', 's', '_', 'side', 'bar', '_', 'dem', 'o', '"', '%', 'plan', '.', 'legislative', '_', 'body', '.', 'name', ',', '\\', 'n']
Detokenized (009): ['"%s_sidebar_demo"', '%', 'plan', '.', 'legislative_body', '.', 'name', ',', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "plan . legislative_body . name ) \n"
Original    (007): ['plan', '.', 'legislative_body', '.', 'name', ')', '\\n']
Tokenized   (012): ['<s>', 'plan', '.', 'legislative', '_', 'body', '.', 'name', ')', '\\', 'n', '</s>']
Filtered   (010): ['plan', '.', 'legislative', '_', 'body', '.', 'name', ')', '\\', 'n']
Detokenized (007): ['plan', '.', 'legislative_body', '.', 'name', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "functions = map ( lambda x : int ( x ) , functions ) \n"
Original    (015): ['functions', '=', 'map', '(', 'lambda', 'x', ':', 'int', '(', 'x', ')', ',', 'functions', ')', '\\n']
Tokenized   (019): ['<s>', 'fun', 'ctions', '=', 'map', '(', 'lambda', 'x', ':', 'int', '(', 'x', ')', ',', 'functions', ')', '\\', 'n', '</s>']
Filtered   (017): ['fun', 'ctions', '=', 'map', '(', 'lambda', 'x', ':', 'int', '(', 'x', ')', ',', 'functions', ')', '\\', 'n']
Detokenized (015): ['functions', '=', 'map', '(', 'lambda', 'x', ':', 'int', '(', 'x', ')', ',', 'functions', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "display = display . copy_from ( display = demo , title = request . POST . get ( ) , owner = result [ ] = True \n"
Original    (028): ['display', '=', 'display', '.', 'copy_from', '(', 'display', '=', 'demo', ',', 'title', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ',', 'owner', '=', 'result', '[', ']', '=', 'True', '\\n']
Tokenized   (033): ['<s>', 'display', '=', 'display', '.', 'copy', '_', 'from', '(', 'display', '=', 'demo', ',', 'title', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ',', 'owner', '=', 'result', '[', ']', '=', 'True', '\\', 'n', '</s>']
Filtered   (031): ['display', '=', 'display', '.', 'copy', '_', 'from', '(', 'display', '=', 'demo', ',', 'title', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ',', 'owner', '=', 'result', '[', ']', '=', 'True', '\\', 'n']
Detokenized (028): ['display', '=', 'display', '.', 'copy_from', '(', 'display', '=', 'demo', ',', 'title', '=', 'request', '.', 'POST', '.', 'get', '(', ')', ',', 'owner', '=', 'result', '[', ']', '=', 'True', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 28, 768)
# Extracted words:  28
Sentence         : "version = min ( plan . version , int ( version ) ) \n"
Original    (014): ['version', '=', 'min', '(', 'plan', '.', 'version', ',', 'int', '(', 'version', ')', ')', '\\n']
Tokenized   (017): ['<s>', 'version', '=', 'min', '(', 'plan', '.', 'version', ',', 'int', '(', 'version', ')', ')', '\\', 'n', '</s>']
Filtered   (015): ['version', '=', 'min', '(', 'plan', '.', 'version', ',', 'int', '(', 'version', ')', ')', '\\', 'n']
Detokenized (014): ['version', '=', 'min', '(', 'plan', '.', 'version', ',', 'int', '(', 'version', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "Comment . objects . filter ( object_pk = district . id , content_type = ct ) . delete ( ) \n"
Original    (021): ['Comment', '.', 'objects', '.', 'filter', '(', 'object_pk', '=', 'district', '.', 'id', ',', 'content_type', '=', 'ct', ')', '.', 'delete', '(', ')', '\\n']
Tokenized   (030): ['<s>', 'Comment', '.', 'objects', '.', 'filter', '(', 'object', '_', 'p', 'k', '=', 'district', '.', 'id', ',', 'content', '_', 'type', '=', 'c', 't', ')', '.', 'delete', '(', ')', '\\', 'n', '</s>']
Filtered   (028): ['Comment', '.', 'objects', '.', 'filter', '(', 'object', '_', 'p', 'k', '=', 'district', '.', 'id', ',', 'content', '_', 'type', '=', 'c', 't', ')', '.', 'delete', '(', ')', '\\', 'n']
Detokenized (021): ['Comment', '.', 'objects', '.', 'filter', '(', 'object_pk', '=', 'district', '.', 'id', ',', 'content_type', '=', 'ct', ')', '.', 'delete', '(', ')', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "TaggedItem . objects . filter ( tag__in = tset , object_id = district . id ) . delete ( ) \n"
Original    (021): ['TaggedItem', '.', 'objects', '.', 'filter', '(', 'tag__in', '=', 'tset', ',', 'object_id', '=', 'district', '.', 'id', ')', '.', 'delete', '(', ')', '\\n']
Tokenized   (031): ['<s>', 'T', 'agged', 'Item', '.', 'objects', '.', 'filter', '(', 'tag', '__', 'in', '=', 't', 'set', ',', 'object', '_', 'id', '=', 'district', '.', 'id', ')', '.', 'delete', '(', ')', '\\', 'n', '</s>']
Filtered   (029): ['T', 'agged', 'Item', '.', 'objects', '.', 'filter', '(', 'tag', '__', 'in', '=', 't', 'set', ',', 'object', '_', 'id', '=', 'district', '.', 'id', ')', '.', 'delete', '(', ')', '\\', 'n']
Detokenized (021): ['TaggedItem', '.', 'objects', '.', 'filter', '(', 'tag__in', '=', 'tset', ',', 'object_id', '=', 'district', '.', 'id', ')', '.', 'delete', '(', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "geolevel = plans [ 0 ] . legislative_body . get_geolevels ( ) [ 0 ] \n"
Original    (016): ['geolevel', '=', 'plans', '[', '0', ']', '.', 'legislative_body', '.', 'get_geolevels', '(', ')', '[', '0', ']', '\\n']
Tokenized   (028): ['<s>', 'ge', 'ole', 'vel', '=', 'plans', '[', '0', ']', '.', 'legislative', '_', 'body', '.', 'get', '_', 'ge', 'ole', 'vel', 's', '(', ')', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (026): ['ge', 'ole', 'vel', '=', 'plans', '[', '0', ']', '.', 'legislative', '_', 'body', '.', 'get', '_', 'ge', 'ole', 'vel', 's', '(', ')', '[', '0', ']', '\\', 'n']
Detokenized (016): ['geolevel', '=', 'plans', '[', '0', ']', '.', 'legislative_body', '.', 'get_geolevels', '(', ')', '[', '0', ']', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "plans = Plan . objects . filter ( is_shared = True ) . order_by ( ) [ 0 : 10 ] \n"
Original    (022): ['plans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'is_shared', '=', 'True', ')', '.', 'order_by', '(', ')', '[', '0', ':', '10', ']', '\\n']
Tokenized   (030): ['<s>', 'pl', 'ans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'is', '_', 'shared', '=', 'True', ')', '.', 'order', '_', 'by', '(', ')', '[', '0', ':', '10', ']', '\\', 'n', '</s>']
Filtered   (028): ['pl', 'ans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'is', '_', 'shared', '=', 'True', ')', '.', 'order', '_', 'by', '(', ')', '[', '0', ':', '10', ']', '\\', 'n']
Detokenized (022): ['plans', '=', 'Plan', '.', 'objects', '.', 'filter', '(', 'is_shared', '=', 'True', ')', '.', 'order_by', '(', ')', '[', '0', ':', '10', ']', '\\n']
Counter: 28
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "write_page = 0 if write_page == 1 else write_page + 1 \n"
Original    (012): ['write_page', '=', '0', 'if', 'write_page', '==', '1', 'else', 'write_page', '+', '1', '\\n']
Tokenized   (021): ['<s>', 'write', '_', 'page', '=', '0', 'if', 'write', '_', 'page', '==', '1', 'else', 'write', '_', 'page', '+', '1', '\\', 'n', '</s>']
Filtered   (019): ['write', '_', 'page', '=', '0', 'if', 'write', '_', 'page', '==', '1', 'else', 'write', '_', 'page', '+', '1', '\\', 'n']
Detokenized (012): ['write_page', '=', '0', 'if', 'write_page', '==', '1', 'else', 'write_page', '+', '1', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "init_sum = 1 if i == 0 else 0 \n"
Original    (010): ['init_sum', '=', '1', 'if', 'i', '==', '0', 'else', '0', '\\n']
Tokenized   (015): ['<s>', 'init', '_', 'sum', '=', '1', 'if', 'i', '==', '0', 'else', '0', '\\', 'n', '</s>']
Filtered   (013): ['init', '_', 'sum', '=', '1', 'if', 'i', '==', '0', 'else', '0', '\\', 'n']
Detokenized (010): ['init_sum', '=', '1', 'if', 'i', '==', '0', 'else', '0', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "mem_d0 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n"
Original    (013): ['mem_d0', '.', 'read_nonblocking', '(', '1', ',', 'write_addr', ',', 'mesh_size', '-', '2', ')', '\\n']
Tokenized   (026): ['<s>', 'mem', '_', 'd', '0', '.', 'read', '_', 'non', 'blocking', '(', '1', ',', 'write', '_', 'addr', ',', 'mesh', '_', 'size', '-', '2', ')', '\\', 'n', '</s>']
Filtered   (024): ['mem', '_', 'd', '0', '.', 'read', '_', 'non', 'blocking', '(', '1', ',', 'write', '_', 'addr', ',', 'mesh', '_', 'size', '-', '2', ')', '\\', 'n']
Detokenized (013): ['mem_d0', '.', 'read_nonblocking', '(', '1', ',', 'write_addr', ',', 'mesh_size', '-', '2', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "write_addr += mesh_size * DSIZE \n"
Original    (006): ['write_addr', '+=', 'mesh_size', '*', 'DSIZE', '\\n']
Tokenized   (014): ['<s>', 'write', '_', 'addr', '+=', 'mesh', '_', 'size', '*', 'D', 'SIZE', '\\', 'n', '</s>']
Filtered   (012): ['write', '_', 'addr', '+=', 'mesh', '_', 'size', '*', 'D', 'SIZE', '\\', 'n']
Detokenized (006): ['write_addr', '+=', 'mesh_size', '*', 'DSIZE', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "sub_id_base = ( 10 if sub_id_m . group ( 1 ) . count ( "\'d" ) > 0 else \n"
Original    (020): ['sub_id_base', '=', '(', '10', 'if', 'sub_id_m', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\\'d"', ')', '>', '0', 'else', '\\n']
Tokenized   (034): ['<s>', 'sub', '_', 'id', '_', 'base', '=', '(', '10', 'if', 'sub', '_', 'id', '_', 'm', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\', "'", 'd', '"', ')', '>', '0', 'else', '\\', 'n', '</s>']
Filtered   (032): ['sub', '_', 'id', '_', 'base', '=', '(', '10', 'if', 'sub', '_', 'id', '_', 'm', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\', "'", 'd', '"', ')', '>', '0', 'else', '\\', 'n']
Detokenized (020): ['sub_id_base', '=', '(', '10', 'if', 'sub_id_m', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\\'d"', ')', '>', '0', 'else', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "16 if sub_id_m . group ( 1 ) . count ( "\'h" ) > 0 else \n"
Original    (017): ['16', 'if', 'sub_id_m', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\\'h"', ')', '>', '0', 'else', '\\n']
Tokenized   (027): ['<s>', '16', 'if', 'sub', '_', 'id', '_', 'm', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\', "'", 'h', '"', ')', '>', '0', 'else', '\\', 'n', '</s>']
Filtered   (025): ['16', 'if', 'sub', '_', 'id', '_', 'm', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\', "'", 'h', '"', ')', '>', '0', 'else', '\\', 'n']
Detokenized (017): ['16', 'if', 'sub_id_m', '.', 'group', '(', '1', ')', '.', 'count', '(', '"\\\'h"', ')', '>', '0', 'else', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "10 ) \n"
Original    (003): ['10', ')', '\\n']
Tokenized   (006): ['<s>', '10', ')', '\\', 'n', '</s>']
Filtered   (004): ['10', ')', '\\', 'n']
Detokenized (003): ['10', ')', '\\n']
Counter: 4
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "optparser . add_option ( "--noreorder" , action = "store_true" , dest = "noreorder" , \n"
Original    (015): ['optparser', '.', 'add_option', '(', '"--noreorder"', ',', 'action', '=', '"store_true"', ',', 'dest', '=', '"noreorder"', ',', '\\n']
Tokenized   (034): ['<s>', 'opt', 'parser', '.', 'add', '_', 'option', '(', '"', '--', 'n', 'ore', 'order', '"', ',', 'action', '=', '"', 'store', '_', 'true', '"', ',', 'dest', '=', '"', 'n', 'ore', 'order', '"', ',', '\\', 'n', '</s>']
Filtered   (032): ['opt', 'parser', '.', 'add', '_', 'option', '(', '"', '--', 'n', 'ore', 'order', '"', ',', 'action', '=', '"', 'store', '_', 'true', '"', ',', 'dest', '=', '"', 'n', 'ore', 'order', '"', ',', '\\', 'n']
Detokenized (015): ['optparser', '.', 'add_option', '(', '"--noreorder"', ',', 'action', '=', '"store_true"', ',', 'dest', '=', '"noreorder"', ',', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "truenode = replaceUndefined ( tree . truenode , termname ) \n"
Original    (011): ['truenode', '=', 'replaceUndefined', '(', 'tree', '.', 'truenode', ',', 'termname', ')', '\\n']
Tokenized   (022): ['<s>', 't', 'ru', 'en', 'ode', '=', 'replace', 'Und', 'efined', '(', 'tree', '.', 'tru', 'en', 'ode', ',', 'term', 'name', ')', '\\', 'n', '</s>']
Filtered   (020): ['t', 'ru', 'en', 'ode', '=', 'replace', 'Und', 'efined', '(', 'tree', '.', 'tru', 'en', 'ode', ',', 'term', 'name', ')', '\\', 'n']
Detokenized (011): ['truenode', '=', 'replaceUndefined', '(', 'tree', '.', 'truenode', ',', 'termname', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "codedir = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) + \n"
Original    (033): ['codedir', '=', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'abspath', '(', '__file__', ')', ')', ')', ')', '+', '\\n']
Tokenized   (043): ['<s>', 'coded', 'ir', '=', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'abs', 'path', '(', '__', 'file', '__', ')', ')', ')', ')', '+', '\\', 'n', '</s>']
Filtered   (041): ['coded', 'ir', '=', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'dir', 'name', '(', 'os', '.', 'path', '.', 'abs', 'path', '(', '__', 'file', '__', ')', ')', ')', ')', '+', '\\', 'n']
Detokenized (033): ['codedir', '=', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'dirname', '(', 'os', '.', 'path', '.', 'abspath', '(', '__file__', ')', ')', ')', ')', '+', '\\n']
Counter: 41
===================================================================
Hidden states:  (13, 33, 768)
# Extracted words:  33
Sentence         : "analyzer = VerilogDataflowAnalyzer ( filelist , topmodule , \n"
Original    (009): ['analyzer', '=', 'VerilogDataflowAnalyzer', '(', 'filelist', ',', 'topmodule', ',', '\\n']
Tokenized   (021): ['<s>', 'analy', 'zer', '=', 'Ver', 'il', 'og', 'Data', 'flow', 'Analy', 'zer', '(', 'file', 'list', ',', 'top', 'module', ',', '\\', 'n', '</s>']
Filtered   (019): ['analy', 'zer', '=', 'Ver', 'il', 'og', 'Data', 'flow', 'Analy', 'zer', '(', 'file', 'list', ',', 'top', 'module', ',', '\\', 'n']
Detokenized (009): ['analyzer', '=', 'VerilogDataflowAnalyzer', '(', 'filelist', ',', 'topmodule', ',', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "constlist = optimizer . getConstlist ( ) \n"
Original    (008): ['constlist', '=', 'optimizer', '.', 'getConstlist', '(', ')', '\\n']
Tokenized   (015): ['<s>', 'const', 'list', '=', 'optim', 'izer', '.', 'get', 'Const', 'list', '(', ')', '\\', 'n', '</s>']
Filtered   (013): ['const', 'list', '=', 'optim', 'izer', '.', 'get', 'Const', 'list', '(', ')', '\\', 'n']
Detokenized (008): ['constlist', '=', 'optimizer', '.', 'getConstlist', '(', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "interval = m . Parameter ( , 16 ) \n"
Original    (010): ['interval', '=', 'm', '.', 'Parameter', '(', ',', '16', ')', '\\n']
Tokenized   (015): ['<s>', 'inter', 'val', '=', 'm', '.', 'Param', 'eter', '(', ',', '16', ')', '\\', 'n', '</s>']
Filtered   (013): ['inter', 'val', '=', 'm', '.', 'Param', 'eter', '(', ',', '16', ')', '\\', 'n']
Detokenized (010): ['interval', '=', 'm', '.', 'Parameter', '(', ',', '16', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "led ( led + 1 ) , \n"
Original    (008): ['led', '(', 'led', '+', '1', ')', ',', '\\n']
Tokenized   (011): ['<s>', 'led', '(', 'led', '+', '1', ')', ',', '\\', 'n', '</s>']
Filtered   (009): ['led', '(', 'led', '+', '1', ')', ',', '\\', 'n']
Detokenized (008): ['led', '(', 'led', '+', '1', ')', ',', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "SingleStatement ( SystemTask ( , , led ) ) \n"
Original    (010): ['SingleStatement', '(', 'SystemTask', '(', ',', ',', 'led', ')', ')', '\\n']
Tokenized   (015): ['<s>', 'Single', 'Statement', '(', 'System', 'Task', '(', ',', ',', 'led', ')', ')', '\\', 'n', '</s>']
Filtered   (013): ['Single', 'Statement', '(', 'System', 'Task', '(', ',', ',', 'led', ')', ')', '\\', 'n']
Detokenized (010): ['SingleStatement', '(', 'SystemTask', '(', ',', ',', 'led', ')', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "y = dataflow . Variable ( , valid = , ready = , point = 4 ) \n"
Original    (018): ['y', '=', 'dataflow', '.', 'Variable', '(', ',', 'valid', '=', ',', 'ready', '=', ',', 'point', '=', '4', ')', '\\n']
Tokenized   (022): ['<s>', 'y', '=', 'data', 'flow', '.', 'Variable', '(', ',', 'valid', '=', ',', 'ready', '=', ',', 'point', '=', '4', ')', '\\', 'n', '</s>']
Filtered   (020): ['y', '=', 'data', 'flow', '.', 'Variable', '(', ',', 'valid', '=', ',', 'ready', '=', ',', 'point', '=', '4', ')', '\\', 'n']
Detokenized (018): ['y', '=', 'dataflow', '.', 'Variable', '(', ',', 'valid', '=', ',', 'ready', '=', ',', 'point', '=', '4', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "z . output ( , valid = , ready = ) \n"
Original    (012): ['z', '.', 'output', '(', ',', 'valid', '=', ',', 'ready', '=', ')', '\\n']
Tokenized   (015): ['<s>', 'z', '.', 'output', '(', ',', 'valid', '=', ',', 'ready', '=', ')', '\\', 'n', '</s>']
Filtered   (013): ['z', '.', 'output', '(', ',', 'valid', '=', ',', 'ready', '=', ')', '\\', 'n']
Detokenized (012): ['z', '.', 'output', '(', ',', 'valid', '=', ',', 'ready', '=', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "xdata_orig = m . RegLike ( ports [ ] , name = , initval = 0 ) \n"
Original    (018): ['xdata_orig', '=', 'm', '.', 'RegLike', '(', 'ports', '[', ']', ',', 'name', '=', ',', 'initval', '=', '0', ')', '\\n']
Tokenized   (026): ['<s>', 'x', 'data', '_', 'orig', '=', 'm', '.', 'Reg', 'Like', '(', 'ports', '[', ']', ',', 'name', '=', ',', 'init', 'val', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (024): ['x', 'data', '_', 'orig', '=', 'm', '.', 'Reg', 'Like', '(', 'ports', '[', ']', ',', 'name', '=', ',', 'init', 'val', '=', '0', ')', '\\', 'n']
Detokenized (018): ['xdata_orig', '=', 'm', '.', 'RegLike', '(', 'ports', '[', ']', ',', 'name', '=', ',', 'initval', '=', '0', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "zdata_orig = m . WireLike ( ports [ ] , name = ) \n"
Original    (014): ['zdata_orig', '=', 'm', '.', 'WireLike', '(', 'ports', '[', ']', ',', 'name', '=', ')', '\\n']
Tokenized   (021): ['<s>', 'z', 'data', '_', 'orig', '=', 'm', '.', 'Wire', 'Like', '(', 'ports', '[', ']', ',', 'name', '=', ')', '\\', 'n', '</s>']
Filtered   (019): ['z', 'data', '_', 'orig', '=', 'm', '.', 'Wire', 'Like', '(', 'ports', '[', ']', ',', 'name', '=', ')', '\\', 'n']
Detokenized (014): ['zdata_orig', '=', 'm', '.', 'WireLike', '(', 'ports', '[', ']', ',', 'name', '=', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "params = m . connect_params ( main ) , \n"
Original    (010): ['params', '=', 'm', '.', 'connect_params', '(', 'main', ')', ',', '\\n']
Tokenized   (015): ['<s>', 'params', '=', 'm', '.', 'connect', '_', 'params', '(', 'main', ')', ',', '\\', 'n', '</s>']
Filtered   (013): ['params', '=', 'm', '.', 'connect', '_', 'params', '(', 'main', ')', ',', '\\', 'n']
Detokenized (010): ['params', '=', 'm', '.', 'connect_params', '(', 'main', ')', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "reset_stmt . append ( ydata_orig ( 0 ) ) \n"
Original    (010): ['reset_stmt', '.', 'append', '(', 'ydata_orig', '(', '0', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'reset', '_', 'st', 'mt', '.', 'append', '(', 'y', 'data', '_', 'orig', '(', '0', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['reset', '_', 'st', 'mt', '.', 'append', '(', 'y', 'data', '_', 'orig', '(', '0', ')', ')', '\\', 'n']
Detokenized (010): ['reset_stmt', '.', 'append', '(', 'ydata_orig', '(', '0', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "nclk ( clk ) , \n"
Original    (006): ['nclk', '(', 'clk', ')', ',', '\\n']
Tokenized   (012): ['<s>', 'n', 'cl', 'k', '(', 'cl', 'k', ')', ',', '\\', 'n', '</s>']
Filtered   (010): ['n', 'cl', 'k', '(', 'cl', 'k', ')', ',', '\\', 'n']
Detokenized (006): ['nclk', '(', 'clk', ')', ',', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "send ( , ydata_orig , yvalid , yready , step = 1 , waitnum = 20 ) \n"
Original    (018): ['send', '(', ',', 'ydata_orig', ',', 'yvalid', ',', 'yready', ',', 'step', '=', '1', ',', 'waitnum', '=', '20', ')', '\\n']
Tokenized   (027): ['<s>', 'send', '(', ',', 'y', 'data', '_', 'orig', ',', 'y', 'valid', ',', 'y', 'ready', ',', 'step', '=', '1', ',', 'wait', 'num', '=', '20', ')', '\\', 'n', '</s>']
Filtered   (025): ['send', '(', ',', 'y', 'data', '_', 'orig', ',', 'y', 'valid', ',', 'y', 'ready', ',', 'step', '=', '1', ',', 'wait', 'num', '=', '20', ')', '\\', 'n']
Detokenized (018): ['send', '(', ',', 'ydata_orig', ',', 'yvalid', ',', 'yready', ',', 'step', '=', '1', ',', 'waitnum', '=', '20', ')', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "receive ( , zdata , zvalid , zready , waitnum = 50 ) \n"
Original    (014): ['receive', '(', ',', 'zdata', ',', 'zvalid', ',', 'zready', ',', 'waitnum', '=', '50', ')', '\\n']
Tokenized   (022): ['<s>', 're', 'ceive', '(', ',', 'z', 'data', ',', 'z', 'valid', ',', 'z', 'ready', ',', 'wait', 'num', '=', '50', ')', '\\', 'n', '</s>']
Filtered   (020): ['re', 'ceive', '(', ',', 'z', 'data', ',', 'z', 'valid', ',', 'z', 'ready', ',', 'wait', 'num', '=', '50', ')', '\\', 'n']
Detokenized (014): ['receive', '(', ',', 'zdata', ',', 'zvalid', ',', 'zready', ',', 'waitnum', '=', '50', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "If ( AndList ( zvalid , zready ) ) ( \n"
Original    (011): ['If', '(', 'AndList', '(', 'zvalid', ',', 'zready', ')', ')', '(', '\\n']
Tokenized   (017): ['<s>', 'If', '(', 'And', 'List', '(', 'z', 'valid', ',', 'z', 'ready', ')', ')', '(', '\\', 'n', '</s>']
Filtered   (015): ['If', '(', 'And', 'List', '(', 'z', 'valid', ',', 'z', 'ready', ')', ')', '(', '\\', 'n']
Detokenized (011): ['If', '(', 'AndList', '(', 'zvalid', ',', 'zready', ')', ')', '(', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "Systask ( , , zdata_orig ) \n"
Original    (007): ['Systask', '(', ',', ',', 'zdata_orig', ')', '\\n']
Tokenized   (015): ['<s>', 'Sy', 'st', 'ask', '(', ',', ',', 'z', 'data', '_', 'orig', ')', '\\', 'n', '</s>']
Filtered   (013): ['Sy', 'st', 'ask', '(', ',', ',', 'z', 'data', '_', 'orig', ')', '\\', 'n']
Detokenized (007): ['Systask', '(', ',', ',', 'zdata_orig', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "count = m . Reg ( , width = 32 , initval = 0 ) \n"
Original    (016): ['count', '=', 'm', '.', 'Reg', '(', ',', 'width', '=', '32', ',', 'initval', '=', '0', ')', '\\n']
Tokenized   (020): ['<s>', 'count', '=', 'm', '.', 'Reg', '(', ',', 'width', '=', '32', ',', 'init', 'val', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (018): ['count', '=', 'm', '.', 'Reg', '(', ',', 'width', '=', '32', ',', 'init', 'val', '=', '0', ')', '\\', 'n']
Detokenized (016): ['count', '=', 'm', '.', 'Reg', '(', ',', 'width', '=', '32', ',', 'initval', '=', '0', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "fsm . add ( valid ( down ) , cond = c , delay = 4 , eager_val = True , lazy_cond = True ) \n"
Original    (026): ['fsm', '.', 'add', '(', 'valid', '(', 'down', ')', ',', 'cond', '=', 'c', ',', 'delay', '=', '4', ',', 'eager_val', '=', 'True', ',', 'lazy_cond', '=', 'True', ')', '\\n']
Tokenized   (034): ['<s>', 'f', 'sm', '.', 'add', '(', 'valid', '(', 'down', ')', ',', 'cond', '=', 'c', ',', 'delay', '=', '4', ',', 'eager', '_', 'val', '=', 'True', ',', 'lazy', '_', 'cond', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (032): ['f', 'sm', '.', 'add', '(', 'valid', '(', 'down', ')', ',', 'cond', '=', 'c', ',', 'delay', '=', '4', ',', 'eager', '_', 'val', '=', 'True', ',', 'lazy', '_', 'cond', '=', 'True', ')', '\\', 'n']
Detokenized (026): ['fsm', '.', 'add', '(', 'valid', '(', 'down', ')', ',', 'cond', '=', 'c', ',', 'delay', '=', '4', ',', 'eager_val', '=', 'True', ',', 'lazy_cond', '=', 'True', ')', '\\n']
Counter: 32
===================================================================
Hidden states:  (13, 26, 768)
# Extracted words:  26
Sentence         : "uut = m . Instance ( mkLed ( ) , , \n"
Original    (012): ['uut', '=', 'm', '.', 'Instance', '(', 'mkLed', '(', ')', ',', ',', '\\n']
Tokenized   (018): ['<s>', 'u', 'ut', '=', 'm', '.', 'Inst', 'ance', '(', 'mk', 'Led', '(', ')', ',', ',', '\\', 'n', '</s>']
Filtered   (016): ['u', 'ut', '=', 'm', '.', 'Inst', 'ance', '(', 'mk', 'Led', '(', ')', ',', ',', '\\', 'n']
Detokenized (012): ['uut', '=', 'm', '.', 'Instance', '(', 'mkLed', '(', ')', ',', ',', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "rslt = m . Wire ( , retwidth , signed = True ) \n"
Original    (014): ['rslt', '=', 'm', '.', 'Wire', '(', ',', 'retwidth', ',', 'signed', '=', 'True', ')', '\\n']
Tokenized   (019): ['<s>', 'rs', 'lt', '=', 'm', '.', 'Wire', '(', ',', 'ret', 'width', ',', 'signed', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (017): ['rs', 'lt', '=', 'm', '.', 'Wire', '(', ',', 'ret', 'width', ',', 'signed', '=', 'True', ')', '\\', 'n']
Detokenized (014): ['rslt', '=', 'm', '.', 'Wire', '(', ',', 'retwidth', ',', 'signed', '=', 'True', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "tmpval [ 0 ] ( rslt ) , \n"
Original    (009): ['tmpval', '[', '0', ']', '(', 'rslt', ')', ',', '\\n']
Tokenized   (014): ['<s>', 'tmp', 'val', '[', '0', ']', '(', 'rs', 'lt', ')', ',', '\\', 'n', '</s>']
Filtered   (012): ['tmp', 'val', '[', '0', ']', '(', 'rs', 'lt', ')', ',', '\\', 'n']
Detokenized (009): ['tmpval', '[', '0', ']', '(', 'rslt', ')', ',', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "vtypes . If ( rst ) ( \n"
Original    (008): ['vtypes', '.', 'If', '(', 'rst', ')', '(', '\\n']
Tokenized   (013): ['<s>', 'v', 'types', '.', 'If', '(', 'r', 'st', ')', '(', '\\', 'n', '</s>']
Filtered   (011): ['v', 'types', '.', 'If', '(', 'r', 'st', ')', '(', '\\', 'n']
Detokenized (008): ['vtypes', '.', 'If', '(', 'rst', ')', '(', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "ports = [ ( , clk ) , ( , update ) , ( , a ) , ( , b ) , ( , c ) ] \n"
Original    (029): ['ports', '=', '[', '(', ',', 'clk', ')', ',', '(', ',', 'update', ')', ',', '(', ',', 'a', ')', ',', '(', ',', 'b', ')', ',', '(', ',', 'c', ')', ']', '\\n']
Tokenized   (033): ['<s>', 'ports', '=', '[', '(', ',', 'cl', 'k', ')', ',', '(', ',', 'update', ')', ',', '(', ',', 'a', ')', ',', '(', ',', 'b', ')', ',', '(', ',', 'c', ')', ']', '\\', 'n', '</s>']
Filtered   (031): ['ports', '=', '[', '(', ',', 'cl', 'k', ')', ',', '(', ',', 'update', ')', ',', '(', ',', 'a', ')', ',', '(', ',', 'b', ')', ',', '(', ',', 'c', ')', ']', '\\', 'n']
Detokenized (029): ['ports', '=', '[', '(', ',', 'clk', ')', ',', '(', ',', 'update', ')', ',', '(', ',', 'a', ')', ',', '(', ',', 'b', ')', ',', '(', ',', 'c', ')', ']', '\\n']
Counter: 31
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "m . Instance ( mult , , ports = ports ) \n"
Original    (012): ['m', '.', 'Instance', '(', 'mult', ',', ',', 'ports', '=', 'ports', ')', '\\n']
Tokenized   (016): ['<s>', 'm', '.', 'Inst', 'ance', '(', 'mult', ',', ',', 'ports', '=', 'ports', ')', '\\', 'n', '</s>']
Filtered   (014): ['m', '.', 'Inst', 'ance', '(', 'mult', ',', ',', 'ports', '=', 'ports', ')', '\\', 'n']
Detokenized (012): ['m', '.', 'Instance', '(', 'mult', ',', ',', 'ports', '=', 'ports', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "stdout = subprocess . PIPE ) . stdout \n"
Original    (009): ['stdout', '=', 'subprocess', '.', 'PIPE', ')', '.', 'stdout', '\\n']
Tokenized   (017): ['<s>', 'std', 'out', '=', 'sub', 'process', '.', 'P', 'IP', 'E', ')', '.', 'std', 'out', '\\', 'n', '</s>']
Filtered   (015): ['std', 'out', '=', 'sub', 'process', '.', 'P', 'IP', 'E', ')', '.', 'std', 'out', '\\', 'n']
Detokenized (009): ['stdout', '=', 'subprocess', '.', 'PIPE', ')', '.', 'stdout', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "line = [ l for l in sout ] [ 0 ] \n"
Original    (013): ['line', '=', '[', 'l', 'for', 'l', 'in', 'sout', ']', '[', '0', ']', '\\n']
Tokenized   (017): ['<s>', 'line', '=', '[', 'l', 'for', 'l', 'in', 's', 'out', ']', '[', '0', ']', '\\', 'n', '</s>']
Filtered   (015): ['line', '=', '[', 'l', 'for', 'l', 'in', 's', 'out', ']', '[', '0', ']', '\\', 'n']
Detokenized (013): ['line', '=', '[', 'l', 'for', 'l', 'in', 'sout', ']', '[', '0', ']', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "round ( wmean , prec ) , "+-" , round ( wstd , prec ) ) \n"
Original    (017): ['round', '(', 'wmean', ',', 'prec', ')', ',', '"+-"', ',', 'round', '(', 'wstd', ',', 'prec', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'round', '(', 'w', 'mean', ',', 'prec', ')', ',', '"+', '-"', ',', 'round', '(', 'w', 'std', ',', 'prec', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['round', '(', 'w', 'mean', ',', 'prec', ')', ',', '"+', '-"', ',', 'round', '(', 'w', 'std', ',', 'prec', ')', ')', '\\', 'n']
Detokenized (017): ['round', '(', 'wmean', ',', 'prec', ')', ',', '"+-"', ',', 'round', '(', 'wstd', ',', 'prec', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "size = stop - start ) \n"
Original    (007): ['size', '=', 'stop', '-', 'start', ')', '\\n']
Tokenized   (010): ['<s>', 'size', '=', 'stop', '-', 'start', ')', '\\', 'n', '</s>']
Filtered   (008): ['size', '=', 'stop', '-', 'start', ')', '\\', 'n']
Detokenized (007): ['size', '=', 'stop', '-', 'start', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "rndbase = numpy . random . randint ( self . nrows , size = niter ) \n"
Original    (017): ['rndbase', '=', 'numpy', '.', 'random', '.', 'randint', '(', 'self', '.', 'nrows', ',', 'size', '=', 'niter', ')', '\\n']
Tokenized   (026): ['<s>', 'r', 'nd', 'base', '=', 'n', 'umpy', '.', 'random', '.', 'rand', 'int', '(', 'self', '.', 'n', 'rows', ',', 'size', '=', 'n', 'iter', ')', '\\', 'n', '</s>']
Filtered   (024): ['r', 'nd', 'base', '=', 'n', 'umpy', '.', 'random', '.', 'rand', 'int', '(', 'self', '.', 'n', 'rows', ',', 'size', '=', 'n', 'iter', ')', '\\', 'n']
Detokenized (017): ['rndbase', '=', 'numpy', '.', 'random', '.', 'randint', '(', 'self', '.', 'nrows', ',', 'size', '=', 'niter', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "rng = [ - 1000 , - 1000 ] \n"
Original    (010): ['rng', '=', '[', '-', '1000', ',', '-', '1000', ']', '\\n']
Tokenized   (014): ['<s>', 'r', 'ng', '=', '[', '-', '1000', ',', '-', '1000', ']', '\\', 'n', '</s>']
Filtered   (012): ['r', 'ng', '=', '[', '-', '1000', ',', '-', '1000', ']', '\\', 'n']
Detokenized (010): ['rng', '=', '[', '-', '1000', ',', '-', '1000', ']', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "benchtime , stones = prof . run ( \n"
Original    (009): ['benchtime', ',', 'stones', '=', 'prof', '.', 'run', '(', '\\n']
Tokenized   (013): ['<s>', 'bench', 'time', ',', 'stones', '=', 'prof', '.', 'run', '(', '\\', 'n', '</s>']
Filtered   (011): ['bench', 'time', ',', 'stones', '=', 'prof', '.', 'run', '(', '\\', 'n']
Detokenized (009): ['benchtime', ',', 'stones', '=', 'prof', '.', 'run', '(', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "db . rng = [ - rng / 2 , rng / 2 ] \n"
Original    (015): ['db', '.', 'rng', '=', '[', '-', 'rng', '/', '2', ',', 'rng', '/', '2', ']', '\\n']
Tokenized   (021): ['<s>', 'db', '.', 'r', 'ng', '=', '[', '-', 'r', 'ng', '/', '2', ',', 'r', 'ng', '/', '2', ']', '\\', 'n', '</s>']
Filtered   (019): ['db', '.', 'r', 'ng', '=', '[', '-', 'r', 'ng', '/', '2', ',', 'r', 'ng', '/', '2', ']', '\\', 'n']
Detokenized (015): ['db', '.', 'rng', '=', '[', '-', 'rng', '/', '2', ',', 'rng', '/', '2', ']', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "MB_ = 1024 * KB_ \n"
Original    (006): ['MB_', '=', '1024', '*', 'KB_', '\\n']
Tokenized   (011): ['<s>', 'MB', '_', '=', '1024', '*', 'KB', '_', '\\', 'n', '</s>']
Filtered   (009): ['MB', '_', '=', '1024', '*', 'KB', '_', '\\', 'n']
Detokenized (006): ['MB_', '=', '1024', '*', 'KB_', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "markers = [ , , , , , , , , , ] \n"
Original    (014): ['markers', '=', '[', ',', ',', ',', ',', ',', ',', ',', ',', ',', ']', '\\n']
Tokenized   (018): ['<s>', 'mark', 'ers', '=', '[', ',', ',', ',', ',', ',', ',', ',', ',', ',', ']', '\\', 'n', '</s>']
Filtered   (016): ['mark', 'ers', '=', '[', ',', ',', ',', ',', ',', ',', ',', ',', ',', ']', '\\', 'n']
Detokenized (014): ['markers', '=', '[', ',', ',', ',', ',', ',', ',', ',', ',', ',', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "memcpyw = float ( tmp . split ( ) [ 1 ] ) \n"
Original    (014): ['memcpyw', '=', 'float', '(', 'tmp', '.', 'split', '(', ')', '[', '1', ']', ')', '\\n']
Tokenized   (020): ['<s>', 'mem', 'c', 'py', 'w', '=', 'float', '(', 'tmp', '.', 'split', '(', ')', '[', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (018): ['mem', 'c', 'py', 'w', '=', 'float', '(', 'tmp', '.', 'split', '(', ')', '[', '1', ']', ')', '\\', 'n']
Detokenized (014): ['memcpyw', '=', 'float', '(', 'tmp', '.', 'split', '(', ')', '[', '1', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "values [ "memcpyr" ] . append ( memcpyr ) \n"
Original    (010): ['values', '[', '"memcpyr"', ']', '.', 'append', '(', 'memcpyr', ')', '\\n']
Tokenized   (019): ['<s>', 'values', '[', '"', 'mem', 'cp', 'yr', '"', ']', '.', 'append', '(', 'mem', 'cp', 'yr', ')', '\\', 'n', '</s>']
Filtered   (017): ['values', '[', '"', 'mem', 'cp', 'yr', '"', ']', '.', 'append', '(', 'mem', 'cp', 'yr', ')', '\\', 'n']
Detokenized (010): ['values', '[', '"memcpyr"', ']', '.', 'append', '(', 'memcpyr', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "ratio = float ( line . split ( ) [ - 1 ] ) \n"
Original    (015): ['ratio', '=', 'float', '(', 'line', '.', 'split', '(', ')', '[', '-', '1', ']', ')', '\\n']
Tokenized   (019): ['<s>', 'rat', 'io', '=', 'float', '(', 'line', '.', 'split', '(', ')', '[', '-', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (017): ['rat', 'io', '=', 'float', '(', 'line', '.', 'split', '(', ')', '[', '-', '1', ']', ')', '\\', 'n']
Detokenized (015): ['ratio', '=', 'float', '(', 'line', '.', 'split', '(', ')', '[', '-', '1', ']', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "xlim ( 0 , xmax ) \n"
Original    (007): ['xlim', '(', '0', ',', 'xmax', ')', '\\n']
Tokenized   (012): ['<s>', 'x', 'lim', '(', '0', ',', 'x', 'max', ')', '\\', 'n', '</s>']
Filtered   (010): ['x', 'lim', '(', '0', ',', 'x', 'max', ')', '\\', 'n']
Detokenized (007): ['xlim', '(', '0', ',', 'xmax', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "ylim ( 0 , None ) \n"
Original    (007): ['ylim', '(', '0', ',', 'None', ')', '\\n']
Tokenized   (011): ['<s>', 'y', 'lim', '(', '0', ',', 'None', ')', '\\', 'n', '</s>']
Filtered   (009): ['y', 'lim', '(', '0', ',', 'None', ')', '\\', 'n']
Detokenized (007): ['ylim', '(', '0', ',', 'None', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "grid ( True ) \n"
Original    (005): ['grid', '(', 'True', ')', '\\n']
Tokenized   (008): ['<s>', 'grid', '(', 'True', ')', '\\', 'n', '</s>']
Filtered   (006): ['grid', '(', 'True', ')', '\\', 'n']
Detokenized (005): ['grid', '(', 'True', ')', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "legend ( [ p [ 0 ] for p in plots \n"
Original    (012): ['legend', '(', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'plots', '\\n']
Tokenized   (016): ['<s>', 'leg', 'end', '(', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'plots', '\\', 'n', '</s>']
Filtered   (014): ['leg', 'end', '(', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'plots', '\\', 'n']
Detokenized (012): ['legend', '(', '[', 'p', '[', '0', ']', 'for', 'p', 'in', 'plots', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "savefig ( outfile , dpi = 64 ) \n"
Original    (009): ['savefig', '(', 'outfile', ',', 'dpi', '=', '64', ')', '\\n']
Tokenized   (015): ['<s>', 'save', 'fig', '(', 'out', 'file', ',', 'd', 'pi', '=', '64', ')', '\\', 'n', '</s>']
Filtered   (013): ['save', 'fig', '(', 'out', 'file', ',', 'd', 'pi', '=', '64', ')', '\\', 'n']
Detokenized (009): ['savefig', '(', 'outfile', ',', 'dpi', '=', '64', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "help = , ) \n"
Original    (005): ['help', '=', ',', ')', '\\n']
Tokenized   (008): ['<s>', 'help', '=', ',', ')', '\\', 'n', '</s>']
Filtered   (006): ['help', '=', ',', ')', '\\', 'n']
Detokenized (005): ['help', '=', ',', ')', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "parser . add_option ( , , action = , \n"
Original    (010): ['parser', '.', 'add_option', '(', ',', ',', 'action', '=', ',', '\\n']
Tokenized   (015): ['<s>', 'parser', '.', 'add', '_', 'option', '(', ',', ',', 'action', '=', ',', '\\', 'n', '</s>']
Filtered   (013): ['parser', '.', 'add', '_', 'option', '(', ',', ',', 'action', '=', ',', '\\', 'n']
Detokenized (010): ['parser', '.', 'add_option', '(', ',', ',', 'action', '=', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "show_plot ( plots , yaxis , legends , gtitle , xmax = int ( options . xmax ) if \n"
Original    (020): ['show_plot', '(', 'plots', ',', 'yaxis', ',', 'legends', ',', 'gtitle', ',', 'xmax', '=', 'int', '(', 'options', '.', 'xmax', ')', 'if', '\\n']
Tokenized   (029): ['<s>', 'show', '_', 'plot', '(', 'plots', ',', 'y', 'axis', ',', 'legends', ',', 'g', 'title', ',', 'x', 'max', '=', 'int', '(', 'options', '.', 'x', 'max', ')', 'if', '\\', 'n', '</s>']
Filtered   (027): ['show', '_', 'plot', '(', 'plots', ',', 'y', 'axis', ',', 'legends', ',', 'g', 'title', ',', 'x', 'max', '=', 'int', '(', 'options', '.', 'x', 'max', ')', 'if', '\\', 'n']
Detokenized (020): ['show_plot', '(', 'plots', ',', 'yaxis', ',', 'legends', ',', 'gtitle', ',', 'xmax', '=', 'int', '(', 'options', '.', 'xmax', ')', 'if', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "options . xmax else None ) \n"
Original    (007): ['options', '.', 'xmax', 'else', 'None', ')', '\\n']
Tokenized   (011): ['<s>', 'options', '.', 'x', 'max', 'else', 'None', ')', '\\', 'n', '</s>']
Filtered   (009): ['options', '.', 'x', 'max', 'else', 'None', ')', '\\', 'n']
Detokenized (007): ['options', '.', 'xmax', 'else', 'None', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "hdfarray . attrs . object = { "a" : 32.1 , "b" : 1 , "c" : [ 1 , 2 ] } \n"
Original    (024): ['hdfarray', '.', 'attrs', '.', 'object', '=', '{', '"a"', ':', '32.1', ',', '"b"', ':', '1', ',', '"c"', ':', '[', '1', ',', '2', ']', '}', '\\n']
Tokenized   (038): ['<s>', 'h', 'df', 'array', '.', 'att', 'rs', '.', 'object', '=', '{', '"', 'a', '"', ':', '32', '.', '1', ',', '"', 'b', '"', ':', '1', ',', '"', 'c', '"', ':', '[', '1', ',', '2', ']', '}', '\\', 'n', '</s>']
Filtered   (036): ['h', 'df', 'array', '.', 'att', 'rs', '.', 'object', '=', '{', '"', 'a', '"', ':', '32', '.', '1', ',', '"', 'b', '"', ':', '1', ',', '"', 'c', '"', ':', '[', '1', ',', '2', ']', '}', '\\', 'n']
Detokenized (024): ['hdfarray', '.', 'attrs', '.', 'object', '=', '{', '"a"', ':', '32.1', ',', '"b"', ':', '1', ',', '"c"', ':', '[', '1', ',', '2', ']', '}', '\\n']
Counter: 36
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "addr = hex ( id ( self ) ) \n"
Original    (010): ['addr', '=', 'hex', '(', 'id', '(', 'self', ')', ')', '\\n']
Tokenized   (013): ['<s>', 'addr', '=', 'hex', '(', 'id', '(', 'self', ')', ')', '\\', 'n', '</s>']
Filtered   (011): ['addr', '=', 'hex', '(', 'id', '(', 'self', ')', ')', '\\', 'n']
Detokenized (010): ['addr', '=', 'hex', '(', 'id', '(', 'self', ')', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "node_manager . registry . pop ( pathname , None ) \n"
Original    (011): ['node_manager', '.', 'registry', '.', 'pop', '(', 'pathname', ',', 'None', ')', '\\n']
Tokenized   (017): ['<s>', 'node', '_', 'manager', '.', 'registry', '.', 'pop', '(', 'path', 'name', ',', 'None', ')', '\\', 'n', '</s>']
Filtered   (015): ['node', '_', 'manager', '.', 'registry', '.', 'pop', '(', 'path', 'name', ',', 'None', ')', '\\', 'n']
Detokenized (011): ['node_manager', '.', 'registry', '.', 'pop', '(', 'pathname', ',', 'None', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "oldpathname , self . _v_pathname ) \n"
Original    (007): ['oldpathname', ',', 'self', '.', '_v_pathname', ')', '\\n']
Tokenized   (016): ['<s>', 'old', 'path', 'name', ',', 'self', '.', '_', 'v', '_', 'path', 'name', ')', '\\', 'n', '</s>']
Filtered   (014): ['old', 'path', 'name', ',', 'self', '.', '_', 'v', '_', 'path', 'name', ')', '\\', 'n']
Detokenized (007): ['oldpathname', ',', 'self', '.', '_v_pathname', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "recursive = False , _log = False , ** kwargs ) \n"
Original    (012): ['recursive', '=', 'False', ',', '_log', '=', 'False', ',', '**', 'kwargs', ')', '\\n']
Tokenized   (019): ['<s>', 'rec', 'ursive', '=', 'False', ',', '_', 'log', '=', 'False', ',', '**', 'k', 'w', 'args', ')', '\\', 'n', '</s>']
Filtered   (017): ['rec', 'ursive', '=', 'False', ',', '_', 'log', '=', 'False', ',', '**', 'k', 'w', 'args', ')', '\\', 'n']
Detokenized (012): ['recursive', '=', 'False', ',', '_log', '=', 'False', ',', '**', 'kwargs', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "% node . _v_pathname ) \n"
Original    (006): ['%', 'node', '.', '_v_pathname', ')', '\\n']
Tokenized   (013): ['<s>', '%', 'node', '.', '_', 'v', '_', 'path', 'name', ')', '\\', 'n', '</s>']
Filtered   (011): ['%', 'node', '.', '_', 'v', '_', 'path', 'name', ')', '\\', 'n']
Detokenized (006): ['%', 'node', '.', '_v_pathname', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "or pathname . startswith ( mypathname + ) ) : \n"
Original    (011): ['or', 'pathname', '.', 'startswith', '(', 'mypathname', '+', ')', ')', ':', '\\n']
Tokenized   (019): ['<s>', 'or', 'path', 'name', '.', 'start', 'sw', 'ith', '(', 'my', 'path', 'name', '+', ')', ')', ':', '\\', 'n', '</s>']
Filtered   (017): ['or', 'path', 'name', '.', 'start', 'sw', 'ith', '(', 'my', 'path', 'name', '+', ')', ')', ':', '\\', 'n']
Detokenized (011): ['or', 'pathname', '.', 'startswith', '(', 'mypathname', '+', ')', ')', ':', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "newarr = self . h5file . create_array ( , , [ 1 ] ) \n"
Original    (015): ['newarr', '=', 'self', '.', 'h5file', '.', 'create_array', '(', ',', ',', '[', '1', ']', ')', '\\n']
Tokenized   (023): ['<s>', 'new', 'arr', '=', 'self', '.', 'h', '5', 'file', '.', 'create', '_', 'array', '(', ',', ',', '[', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (021): ['new', 'arr', '=', 'self', '.', 'h', '5', 'file', '.', 'create', '_', 'array', '(', ',', ',', '[', '1', ']', ')', '\\', 'n']
Detokenized (015): ['newarr', '=', 'self', '.', 'h5file', '.', 'create_array', '(', ',', ',', '[', '1', ']', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "with self . assertRaises ( tables . UndoRedoError ) : \n"
Original    (011): ['with', 'self', '.', 'assertRaises', '(', 'tables', '.', 'UndoRedoError', ')', ':', '\\n']
Tokenized   (020): ['<s>', 'with', 'self', '.', 'assert', 'Ra', 'ises', '(', 'tables', '.', 'Und', 'o', 'Red', 'o', 'Error', ')', ':', '\\', 'n', '</s>']
Filtered   (018): ['with', 'self', '.', 'assert', 'Ra', 'ises', '(', 'tables', '.', 'Und', 'o', 'Red', 'o', 'Error', ')', ':', '\\', 'n']
Detokenized (011): ['with', 'self', '.', 'assertRaises', '(', 'tables', '.', 'UndoRedoError', ')', ':', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : ""/othergroup1/othergroup2/othergroup3" not in self . h5file ) \n"
Original    (008): ['"/othergroup1/othergroup2/othergroup3"', 'not', 'in', 'self', '.', 'h5file', ')', '\\n']
Tokenized   (026): ['<s>', '"', '/', 'other', 'group', '1', '/', 'other', 'group', '2', '/', 'other', 'group', '3', '"', 'not', 'in', 'self', '.', 'h', '5', 'file', ')', '\\', 'n', '</s>']
Filtered   (024): ['"', '/', 'other', 'group', '1', '/', 'other', 'group', '2', '/', 'other', 'group', '3', '"', 'not', 'in', 'self', '.', 'h', '5', 'file', ')', '\\', 'n']
Detokenized (008): ['"/othergroup1/othergroup2/othergroup3"', 'not', 'in', 'self', '.', 'h5file', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "var2 = BoolCol ( dflt = 0 , pos = 2 ) \n"
Original    (013): ['var2', '=', 'BoolCol', '(', 'dflt', '=', '0', ',', 'pos', '=', '2', ')', '\\n']
Tokenized   (020): ['<s>', 'var', '2', '=', 'B', 'ool', 'Col', '(', 'df', 'lt', '=', '0', ',', 'pos', '=', '2', ')', '\\', 'n', '</s>']
Filtered   (018): ['var', '2', '=', 'B', 'ool', 'Col', '(', 'df', 'lt', '=', '0', ',', 'pos', '=', '2', ')', '\\', 'n']
Detokenized (013): ['var2', '=', 'BoolCol', '(', 'dflt', '=', '0', ',', 'pos', '=', '2', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "None , nrows ) \n"
Original    (005): ['None', ',', 'nrows', ')', '\\n']
Tokenized   (009): ['<s>', 'None', ',', 'n', 'rows', ')', '\\', 'n', '</s>']
Filtered   (007): ['None', ',', 'n', 'rows', ')', '\\', 'n']
Detokenized (005): ['None', ',', 'nrows', ')', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "populateTable ( self . h5file . root , ) \n"
Original    (010): ['populateTable', '(', 'self', '.', 'h5file', '.', 'root', ',', ')', '\\n']
Tokenized   (017): ['<s>', 'pop', 'ulate', 'Table', '(', 'self', '.', 'h', '5', 'file', '.', 'root', ',', ')', '\\', 'n', '</s>']
Filtered   (015): ['pop', 'ulate', 'Table', '(', 'self', '.', 'h', '5', 'file', '.', 'root', ',', ')', '\\', 'n']
Detokenized (010): ['populateTable', '(', 'self', '.', 'h5file', '.', 'root', ',', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "new_node = self . h5file . copy_children ( \n"
Original    (009): ['new_node', '=', 'self', '.', 'h5file', '.', 'copy_children', '(', '\\n']
Tokenized   (018): ['<s>', 'new', '_', 'node', '=', 'self', '.', 'h', '5', 'file', '.', 'copy', '_', 'children', '(', '\\', 'n', '</s>']
Filtered   (016): ['new', '_', 'node', '=', 'self', '.', 'h', '5', 'file', '.', 'copy', '_', 'children', '(', '\\', 'n']
Detokenized (009): ['new_node', '=', 'self', '.', 'h5file', '.', 'copy_children', '(', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : ", , recursive = 1 ) \n"
Original    (007): [',', ',', 'recursive', '=', '1', ')', '\\n']
Tokenized   (010): ['<s>', ',', ',', 'recursive', '=', '1', ')', '\\', 'n', '</s>']
Filtered   (008): [',', ',', 'recursive', '=', '1', ')', '\\', 'n']
Detokenized (007): [',', ',', 'recursive', '=', '1', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "setattr ( attrs , , 11 ) \n"
Original    (008): ['setattr', '(', 'attrs', ',', ',', '11', ')', '\\n']
Tokenized   (013): ['<s>', 'set', 'attr', '(', 'att', 'rs', ',', ',', '11', ')', '\\', 'n', '</s>']
Filtered   (011): ['set', 'attr', '(', 'att', 'rs', ',', ',', '11', ')', '\\', 'n']
Detokenized (008): ['setattr', '(', 'attrs', ',', ',', '11', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "delattr ( attrs , ) \n"
Original    (006): ['delattr', '(', 'attrs', ',', ')', '\\n']
Tokenized   (011): ['<s>', 'del', 'attr', '(', 'att', 'rs', ',', ')', '\\', 'n', '</s>']
Filtered   (009): ['del', 'attr', '(', 'att', 'rs', ',', ')', '\\', 'n']
Detokenized (006): ['delattr', '(', 'attrs', ',', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "arr . _v_attrs . foo = \n"
Original    (007): ['arr', '.', '_v_attrs', '.', 'foo', '=', '\\n']
Tokenized   (014): ['<s>', 'arr', '.', '_', 'v', '_', 'att', 'rs', '.', 'foo', '=', '\\', 'n', '</s>']
Filtered   (012): ['arr', '.', '_', 'v', '_', 'att', 'rs', '.', 'foo', '=', '\\', 'n']
Detokenized (007): ['arr', '.', '_v_attrs', '.', 'foo', '=', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "assert response . header ( ) == \n"
Original    (008): ['assert', 'response', '.', 'header', '(', ')', '==', '\\n']
Tokenized   (011): ['<s>', 'assert', 'response', '.', 'header', '(', ')', '==', '\\', 'n', '</s>']
Filtered   (009): ['assert', 'response', '.', 'header', '(', ')', '==', '\\', 'n']
Detokenized (008): ['assert', 'response', '.', 'header', '(', ')', '==', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "CHANGES = open ( os . path . join ( here , ) , encoding = "utf-8" ) . read ( ) \n"
Original    (023): ['CHANGES', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ',', 'encoding', '=', '"utf-8"', ')', '.', 'read', '(', ')', '\\n']
Tokenized   (032): ['<s>', 'CH', 'ANG', 'ES', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ',', 'encoding', '=', '"', 'utf', '-', '8', '"', ')', '.', 'read', '(', ')', '\\', 'n', '</s>']
Filtered   (030): ['CH', 'ANG', 'ES', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ',', 'encoding', '=', '"', 'utf', '-', '8', '"', ')', '.', 'read', '(', ')', '\\', 'n']
Detokenized (023): ['CHANGES', '=', 'open', '(', 'os', '.', 'path', '.', 'join', '(', 'here', ',', ')', ',', 'encoding', '=', '"utf-8"', ')', '.', 'read', '(', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 23, 768)
# Extracted words:  23
Sentence         : "tests_require = requires + [ ] , \n"
Original    (008): ['tests_require', '=', 'requires', '+', '[', ']', ',', '\\n']
Tokenized   (013): ['<s>', 'tests', '_', 'require', '=', 'requires', '+', '[', ']', ',', '\\', 'n', '</s>']
Filtered   (011): ['tests', '_', 'require', '=', 'requires', '+', '[', ']', ',', '\\', 'n']
Detokenized (008): ['tests_require', '=', 'requires', '+', '[', ']', ',', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "user_id = Column ( Integer , primary_key = True ) \n"
Original    (011): ['user_id', '=', 'Column', '(', 'Integer', ',', 'primary_key', '=', 'True', ')', '\\n']
Tokenized   (018): ['<s>', 'user', '_', 'id', '=', 'Column', '(', 'Integer', ',', 'primary', '_', 'key', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (016): ['user', '_', 'id', '=', 'Column', '(', 'Integer', ',', 'primary', '_', 'key', '=', 'True', ')', '\\', 'n']
Detokenized (011): ['user_id', '=', 'Column', '(', 'Integer', ',', 'primary_key', '=', 'True', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "username = Column ( Unicode ( 20 ) , unique = True ) \n"
Original    (014): ['username', '=', 'Column', '(', 'Unicode', '(', '20', ')', ',', 'unique', '=', 'True', ')', '\\n']
Tokenized   (017): ['<s>', 'username', '=', 'Column', '(', 'Unicode', '(', '20', ')', ',', 'unique', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (015): ['username', '=', 'Column', '(', 'Unicode', '(', '20', ')', ',', 'unique', '=', 'True', ')', '\\', 'n']
Detokenized (014): ['username', '=', 'Column', '(', 'Unicode', '(', '20', ')', ',', 'unique', '=', 'True', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "hits = Column ( Integer , default = 0 ) \n"
Original    (011): ['hits', '=', 'Column', '(', 'Integer', ',', 'default', '=', '0', ')', '\\n']
Tokenized   (015): ['<s>', 'h', 'its', '=', 'Column', '(', 'Integer', ',', 'default', '=', '0', ')', '\\', 'n', '</s>']
Filtered   (013): ['h', 'its', '=', 'Column', '(', 'Integer', ',', 'default', '=', '0', ')', '\\', 'n']
Detokenized (011): ['hits', '=', 'Column', '(', 'Integer', ',', 'default', '=', '0', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "_password = Column ( , Unicode ( 60 ) ) \n"
Original    (011): ['_password', '=', 'Column', '(', ',', 'Unicode', '(', '60', ')', ')', '\\n']
Tokenized   (015): ['<s>', '_', 'password', '=', 'Column', '(', ',', 'Unicode', '(', '60', ')', ')', '\\', 'n', '</s>']
Filtered   (013): ['_', 'password', '=', 'Column', '(', ',', 'Unicode', '(', '60', ')', ')', '\\', 'n']
Detokenized (011): ['_password', '=', 'Column', '(', ',', 'Unicode', '(', '60', ')', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "Column ( , Integer , ForeignKey ( ) ) \n"
Original    (010): ['Column', '(', ',', 'Integer', ',', 'ForeignKey', '(', ')', ')', '\\n']
Tokenized   (014): ['<s>', 'Column', '(', ',', 'Integer', ',', 'Foreign', 'Key', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (012): ['Column', '(', ',', 'Integer', ',', 'Foreign', 'Key', '(', ')', ')', '\\', 'n']
Detokenized (010): ['Column', '(', ',', 'Integer', ',', 'ForeignKey', '(', ')', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "target_id = Column ( Integer , ForeignKey ( ) ) \n"
Original    (011): ['target_id', '=', 'Column', '(', 'Integer', ',', 'ForeignKey', '(', ')', ')', '\\n']
Tokenized   (017): ['<s>', 'target', '_', 'id', '=', 'Column', '(', 'Integer', ',', 'Foreign', 'Key', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (015): ['target', '_', 'id', '=', 'Column', '(', 'Integer', ',', 'Foreign', 'Key', '(', ')', ')', '\\', 'n']
Detokenized (011): ['target_id', '=', 'Column', '(', 'Integer', ',', 'ForeignKey', '(', ')', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "comments = relation ( , cascade = "delete" , \n"
Original    (010): ['comments', '=', 'relation', '(', ',', 'cascade', '=', '"delete"', ',', '\\n']
Tokenized   (015): ['<s>', 'comments', '=', 'relation', '(', ',', 'cascade', '=', '"', 'delete', '"', ',', '\\', 'n', '</s>']
Filtered   (013): ['comments', '=', 'relation', '(', ',', 'cascade', '=', '"', 'delete', '"', ',', '\\', 'n']
Detokenized (010): ['comments', '=', 'relation', '(', ',', 'cascade', '=', '"delete"', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "author = relation ( User , cascade = "delete" , backref = ) \n"
Original    (014): ['author', '=', 'relation', '(', 'User', ',', 'cascade', '=', '"delete"', ',', 'backref', '=', ')', '\\n']
Tokenized   (020): ['<s>', 'author', '=', 'relation', '(', 'User', ',', 'cascade', '=', '"', 'delete', '"', ',', 'back', 'ref', '=', ')', '\\', 'n', '</s>']
Filtered   (018): ['author', '=', 'relation', '(', 'User', ',', 'cascade', '=', '"', 'delete', '"', ',', 'back', 'ref', '=', ')', '\\', 'n']
Detokenized (014): ['author', '=', 'relation', '(', 'User', ',', 'cascade', '=', '"delete"', ',', 'backref', '=', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "tags = relation ( Tag , secondary = ideas_tags , backref = ) \n"
Original    (014): ['tags', '=', 'relation', '(', 'Tag', ',', 'secondary', '=', 'ideas_tags', ',', 'backref', '=', ')', '\\n']
Tokenized   (020): ['<s>', 'tags', '=', 'relation', '(', 'Tag', ',', 'secondary', '=', 'ideas', '_', 'tags', ',', 'back', 'ref', '=', ')', '\\', 'n', '</s>']
Filtered   (018): ['tags', '=', 'relation', '(', 'Tag', ',', 'secondary', '=', 'ideas', '_', 'tags', ',', 'back', 'ref', '=', ')', '\\', 'n']
Detokenized (014): ['tags', '=', 'relation', '(', 'Tag', ',', 'secondary', '=', 'ideas_tags', ',', 'backref', '=', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "voted_users = relation ( User , secondary = voted_users , lazy = , \n"
Original    (014): ['voted_users', '=', 'relation', '(', 'User', ',', 'secondary', '=', 'voted_users', ',', 'lazy', '=', ',', '\\n']
Tokenized   (022): ['<s>', 'v', 'oted', '_', 'users', '=', 'relation', '(', 'User', ',', 'secondary', '=', 'voted', '_', 'users', ',', 'lazy', '=', ',', '\\', 'n', '</s>']
Filtered   (020): ['v', 'oted', '_', 'users', '=', 'relation', '(', 'User', ',', 'secondary', '=', 'voted', '_', 'users', ',', 'lazy', '=', ',', '\\', 'n']
Detokenized (014): ['voted_users', '=', 'relation', '(', 'User', ',', 'secondary', '=', 'voted_users', ',', 'lazy', '=', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "total_votes = column_property ( ( hits + misses ) . label ( ) ) \n"
Original    (015): ['total_votes', '=', 'column_property', '(', '(', 'hits', '+', 'misses', ')', '.', 'label', '(', ')', ')', '\\n']
Tokenized   (022): ['<s>', 'total', '_', 'votes', '=', 'column', '_', 'property', '(', '(', 'hits', '+', 'misses', ')', '.', 'label', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (020): ['total', '_', 'votes', '=', 'column', '_', 'property', '(', '(', 'hits', '+', 'misses', ')', '.', 'label', '(', ')', ')', '\\', 'n']
Detokenized (015): ['total_votes', '=', 'column_property', '(', '(', 'hits', '+', 'misses', ')', '.', 'label', '(', ')', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "query = query . filter ( cls . target == None ) . order_by ( order_by ) \n"
Original    (018): ['query', '=', 'query', '.', 'filter', '(', 'cls', '.', 'target', '==', 'None', ')', '.', 'order_by', '(', 'order_by', ')', '\\n']
Tokenized   (026): ['<s>', 'query', '=', 'query', '.', 'filter', '(', 'cl', 's', '.', 'target', '==', 'None', ')', '.', 'order', '_', 'by', '(', 'order', '_', 'by', ')', '\\', 'n', '</s>']
Filtered   (024): ['query', '=', 'query', '.', 'filter', '(', 'cl', 's', '.', 'target', '==', 'None', ')', '.', 'order', '_', 'by', '(', 'order', '_', 'by', ')', '\\', 'n']
Detokenized (018): ['query', '=', 'query', '.', 'filter', '(', 'cls', '.', 'target', '==', 'None', ')', '.', 'order_by', '(', 'order_by', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "mock_get_auditlog . side_effect = lambda c : auditlog \n"
Original    (009): ['mock_get_auditlog', '.', 'side_effect', '=', 'lambda', 'c', ':', 'auditlog', '\\n']
Tokenized   (022): ['<s>', 'm', 'ock', '_', 'get', '_', 'aud', 'it', 'log', '.', 'side', '_', 'effect', '=', 'lambda', 'c', ':', 'audit', 'log', '\\', 'n', '</s>']
Filtered   (020): ['m', 'ock', '_', 'get', '_', 'aud', 'it', 'log', '.', 'side', '_', 'effect', '=', 'lambda', 'c', ':', 'audit', 'log', '\\', 'n']
Detokenized (009): ['mock_get_auditlog', '.', 'side_effect', '=', 'lambda', 'c', ':', 'auditlog', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "json . loads ( entry [ 2 ] . payload ) , \n"
Original    (013): ['json', '.', 'loads', '(', 'entry', '[', '2', ']', '.', 'payload', ')', ',', '\\n']
Tokenized   (016): ['<s>', 'json', '.', 'loads', '(', 'entry', '[', '2', ']', '.', 'payload', ')', ',', '\\', 'n', '</s>']
Filtered   (014): ['json', '.', 'loads', '(', 'entry', '[', '2', ']', '.', 'payload', ')', ',', '\\', 'n']
Detokenized (013): ['json', '.', 'loads', '(', 'entry', '[', '2', ']', '.', 'payload', ')', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : ": 5 \n"
Original    (003): [':', '5', '\\n']
Tokenized   (006): ['<s>', ':', '5', '\\', 'n', '</s>']
Filtered   (004): [':', '5', '\\', 'n']
Detokenized (003): [':', '5', '\\n']
Counter: 4
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "registry . content = DummyContentRegistry ( ) \n"
Original    (008): ['registry', '.', 'content', '=', 'DummyContentRegistry', '(', ')', '\\n']
Tokenized   (016): ['<s>', 'reg', 'istry', '.', 'content', '=', 'D', 'ummy', 'Content', 'Reg', 'istry', '(', ')', '\\', 'n', '</s>']
Filtered   (014): ['reg', 'istry', '.', 'content', '=', 'D', 'ummy', 'Content', 'Reg', 'istry', '(', ')', '\\', 'n']
Detokenized (008): ['registry', '.', 'content', '=', 'DummyContentRegistry', '(', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "ep = DummyFunction ( True ) \n"
Original    (007): ['ep', '=', 'DummyFunction', '(', 'True', ')', '\\n']
Tokenized   (012): ['<s>', 'ep', '=', 'D', 'ummy', 'Function', '(', 'True', ')', '\\', 'n', '</s>']
Filtered   (010): ['ep', '=', 'D', 'ummy', 'Function', '(', 'True', ')', '\\', 'n']
Detokenized (007): ['ep', '=', 'DummyFunction', '(', 'True', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "name_node . validator ( node [ ] , filename ) \n"
Original    (011): ['name_node', '.', 'validator', '(', 'node', '[', ']', ',', 'filename', ')', '\\n']
Tokenized   (017): ['<s>', 'name', '_', 'node', '.', 'valid', 'ator', '(', 'node', '[', ']', ',', 'filename', ')', '\\', 'n', '</s>']
Filtered   (015): ['name', '_', 'node', '.', 'valid', 'ator', '(', 'node', '[', ']', ',', 'filename', ')', '\\', 'n']
Detokenized (011): ['name_node', '.', 'validator', '(', 'node', '[', ']', ',', 'filename', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "schema [ ] . missing = colander . null \n"
Original    (010): ['schema', '[', ']', '.', 'missing', '=', 'colander', '.', 'null', '\\n']
Tokenized   (015): ['<s>', 'sche', 'ma', '[', ']', '.', 'missing', '=', 'col', 'ander', '.', 'null', '\\', 'n', '</s>']
Filtered   (013): ['sche', 'ma', '[', ']', '.', 'missing', '=', 'col', 'ander', '.', 'null', '\\', 'n']
Detokenized (010): ['schema', '[', ']', '.', 'missing', '=', 'colander', '.', 'null', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "title = appstruct [ ] or None \n"
Original    (008): ['title', '=', 'appstruct', '[', ']', 'or', 'None', '\\n']
Tokenized   (012): ['<s>', 'title', '=', 'app', 'struct', '[', ']', 'or', 'None', '\\', 'n', '</s>']
Filtered   (010): ['title', '=', 'app', 'struct', '[', ']', 'or', 'None', '\\', 'n']
Detokenized (008): ['title', '=', 'appstruct', '[', ']', 'or', 'None', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "mimetype = appstruct [ ] or USE_MAGIC \n"
Original    (008): ['mimetype', '=', 'appstruct', '[', ']', 'or', 'USE_MAGIC', '\\n']
Tokenized   (018): ['<s>', 'm', 'im', 'ety', 'pe', '=', 'app', 'struct', '[', ']', 'or', 'USE', '_', 'MAG', 'IC', '\\', 'n', '</s>']
Filtered   (016): ['m', 'im', 'ety', 'pe', '=', 'app', 'struct', '[', ']', 'or', 'USE', '_', 'MAG', 'IC', '\\', 'n']
Detokenized (008): ['mimetype', '=', 'appstruct', '[', ']', 'or', 'USE_MAGIC', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "filedata = tempstore . get ( uid , { } ) \n"
Original    (012): ['filedata', '=', 'tempstore', '.', 'get', '(', 'uid', ',', '{', '}', ')', '\\n']
Tokenized   (019): ['<s>', 'f', 'iled', 'ata', '=', 'temp', 'store', '.', 'get', '(', 'u', 'id', ',', '{', '}', ')', '\\', 'n', '</s>']
Filtered   (017): ['f', 'iled', 'ata', '=', 'temp', 'store', '.', 'get', '(', 'u', 'id', ',', '{', '}', ')', '\\', 'n']
Detokenized (012): ['filedata', '=', 'tempstore', '.', 'get', '(', 'uid', ',', '{', '}', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "resource1 . __acl__ = [ ( None , , None ) , ( None , 1 , None ) ] \n"
Original    (021): ['resource1', '.', '__acl__', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', '\\n']
Tokenized   (027): ['<s>', 'resource', '1', '.', '__', 'acl', '__', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', '\\', 'n', '</s>']
Filtered   (025): ['resource', '1', '.', '__', 'acl', '__', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', '\\', 'n']
Detokenized (021): ['resource1', '.', '__acl__', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "new_acl = [ ( None , , None ) , ( None , 1 , None ) ] , \n"
Original    (020): ['new_acl', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', ',', '\\n']
Tokenized   (025): ['<s>', 'new', '_', 'acl', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', ',', '\\', 'n', '</s>']
Filtered   (023): ['new', '_', 'acl', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', ',', '\\', 'n']
Detokenized (020): ['new_acl', '=', '[', '(', 'None', ',', ',', 'None', ')', ',', '(', 'None', ',', '1', ',', 'None', ')', ']', ',', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "request . registry . notify ( LoggedIn ( login , user , context , request ) ) \n"
Original    (018): ['request', '.', 'registry', '.', 'notify', '(', 'LoggedIn', '(', 'login', ',', 'user', ',', 'context', ',', 'request', ')', ')', '\\n']
Tokenized   (022): ['<s>', 'request', '.', 'registry', '.', 'notify', '(', 'Logged', 'In', '(', 'login', ',', 'user', ',', 'context', ',', 'request', ')', ')', '\\', 'n', '</s>']
Filtered   (020): ['request', '.', 'registry', '.', 'notify', '(', 'Logged', 'In', '(', 'login', ',', 'user', ',', 'context', ',', 'request', ')', ')', '\\', 'n']
Detokenized (018): ['request', '.', 'registry', '.', 'notify', '(', 'LoggedIn', '(', 'login', ',', 'user', ',', 'context', ',', 'request', ')', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "dirname , filename = os . path . split ( context . path ) \n"
Original    (015): ['dirname', ',', 'filename', '=', 'os', '.', 'path', '.', 'split', '(', 'context', '.', 'path', ')', '\\n']
Tokenized   (019): ['<s>', 'dir', 'name', ',', 'filename', '=', 'os', '.', 'path', '.', 'split', '(', 'context', '.', 'path', ')', '\\', 'n', '</s>']
Filtered   (017): ['dir', 'name', ',', 'filename', '=', 'os', '.', 'path', '.', 'split', '(', 'context', '.', 'path', ')', '\\', 'n']
Detokenized (015): ['dirname', ',', 'filename', '=', 'os', '.', 'path', '.', 'split', '(', 'context', '.', 'path', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "response . content_type = mt or \n"
Original    (007): ['response', '.', 'content_type', '=', 'mt', 'or', '\\n']
Tokenized   (012): ['<s>', 'response', '.', 'content', '_', 'type', '=', 'mt', 'or', '\\', 'n', '</s>']
Filtered   (010): ['response', '.', 'content', '_', 'type', '=', 'mt', 'or', '\\', 'n']
Detokenized (007): ['response', '.', 'content_type', '=', 'mt', 'or', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "getattr ( SkipCase ( ) , ) ) \n"
Original    (009): ['getattr', '(', 'SkipCase', '(', ')', ',', ')', ')', '\\n']
Tokenized   (014): ['<s>', 'get', 'attr', '(', 'Skip', 'Case', '(', ')', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (012): ['get', 'attr', '(', 'Skip', 'Case', '(', ')', ',', ')', ')', '\\', 'n']
Detokenized (009): ['getattr', '(', 'SkipCase', '(', ')', ',', ')', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "iterator . __class__ . __name__ ) ) \n"
Original    (008): ['iterator', '.', '__class__', '.', '__name__', ')', ')', '\\n']
Tokenized   (015): ['<s>', 'iterator', '.', '__', 'class', '__', '.', '__', 'name', '__', ')', ')', '\\', 'n', '</s>']
Filtered   (013): ['iterator', '.', '__', 'class', '__', '.', '__', 'name', '__', ')', ')', '\\', 'n']
Detokenized (008): ['iterator', '.', '__class__', '.', '__name__', ')', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "parts = super ( newbytes , self ) . splitlines ( keepends ) \n"
Original    (014): ['parts', '=', 'super', '(', 'newbytes', ',', 'self', ')', '.', 'splitlines', '(', 'keepends', ')', '\\n']
Tokenized   (020): ['<s>', 'parts', '=', 'super', '(', 'new', 'bytes', ',', 'self', ')', '.', 'split', 'lines', '(', 'keep', 'ends', ')', '\\', 'n', '</s>']
Filtered   (018): ['parts', '=', 'super', '(', 'new', 'bytes', ',', 'self', ')', '.', 'split', 'lines', '(', 'keep', 'ends', ')', '\\', 'n']
Detokenized (014): ['parts', '=', 'super', '(', 'newbytes', ',', 'self', ')', '.', 'splitlines', '(', 'keepends', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "pos = self . rfind ( sub , * args ) \n"
Original    (012): ['pos', '=', 'self', '.', 'rfind', '(', 'sub', ',', '*', 'args', ')', '\\n']
Tokenized   (016): ['<s>', 'pos', '=', 'self', '.', 'r', 'find', '(', 'sub', ',', '*', 'args', ')', '\\', 'n', '</s>']
Filtered   (014): ['pos', '=', 'self', '.', 'r', 'find', '(', 'sub', ',', '*', 'args', ')', '\\', 'n']
Detokenized (012): ['pos', '=', 'self', '.', 'rfind', '(', 'sub', ',', '*', 'args', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "replaced_builtins = . split ( ) \n"
Original    (007): ['replaced_builtins', '=', '.', 'split', '(', ')', '\\n']
Tokenized   (014): ['<s>', 're', 'placed', '_', 'built', 'ins', '=', '.', 'split', '(', ')', '\\', 'n', '</s>']
Filtered   (012): ['re', 'placed', '_', 'built', 'ins', '=', '.', 'split', '(', ')', '\\', 'n']
Detokenized (007): ['replaced_builtins', '=', '.', 'split', '(', ')', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "expression = . join ( [ "name=\'{0}\'" . format ( name ) for name in replaced_builtins ] ) \n"
Original    (019): ['expression', '=', '.', 'join', '(', '[', '"name=\\\'{0}\\\'"', '.', 'format', '(', 'name', ')', 'for', 'name', 'in', 'replaced_builtins', ']', ')', '\\n']
Tokenized   (032): ['<s>', 'expression', '=', '.', 'join', '(', '[', '"', 'name', '=', "\\'", '{', '0', '}\\', '\'"', '.', 'format', '(', 'name', ')', 'for', 'name', 'in', 'replaced', '_', 'built', 'ins', ']', ')', '\\', 'n', '</s>']
Filtered   (030): ['expression', '=', '.', 'join', '(', '[', '"', 'name', '=', "\\'", '{', '0', '}\\', '\'"', '.', 'format', '(', 'name', ')', 'for', 'name', 'in', 'replaced', '_', 'built', 'ins', ']', ')', '\\', 'n']
Detokenized (019): ['expression', '=', '.', 'join', '(', '[', '"name=\\\'{0}\\\'"', '.', 'format', '(', 'name', ')', 'for', 'name', 'in', 'replaced_builtins', ']', ')', '\\n']
Counter: 30
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "touch_import_top ( , name . value , node ) \n"
Original    (010): ['touch_import_top', '(', ',', 'name', '.', 'value', ',', 'node', ')', '\\n']
Tokenized   (017): ['<s>', 'touch', '_', 'import', '_', 'top', '(', ',', 'name', '.', 'value', ',', 'node', ')', '\\', 'n', '</s>']
Filtered   (015): ['touch', '_', 'import', '_', 'top', '(', ',', 'name', '.', 'value', ',', 'node', ')', '\\', 'n']
Detokenized (010): ['touch_import_top', '(', ',', 'name', '.', 'value', ',', 'node', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "retcode = main ( [ self . textfilename ] ) \n"
Original    (011): ['retcode', '=', 'main', '(', '[', 'self', '.', 'textfilename', ']', ')', '\\n']
Tokenized   (016): ['<s>', 'ret', 'code', '=', 'main', '(', '[', 'self', '.', 'text', 'filename', ']', ')', '\\', 'n', '</s>']
Filtered   (014): ['ret', 'code', '=', 'main', '(', '[', 'self', '.', 'text', 'filename', ']', ')', '\\', 'n']
Detokenized (011): ['retcode', '=', 'main', '(', '[', 'self', '.', 'textfilename', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "v = self . visit ( node . values [ i ] ) \n"
Original    (014): ['v', '=', 'self', '.', 'visit', '(', 'node', '.', 'values', '[', 'i', ']', ')', '\\n']
Tokenized   (017): ['<s>', 'v', '=', 'self', '.', 'visit', '(', 'node', '.', 'values', '[', 'i', ']', ')', '\\', 'n', '</s>']
Filtered   (015): ['v', '=', 'self', '.', 'visit', '(', 'node', '.', 'values', '[', 'i', ']', ')', '\\', 'n']
Detokenized (014): ['v', '=', 'self', '.', 'visit', '(', 'node', '.', 'values', '[', 'i', ']', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "props . update ( self . _class_props [ p ] ) \n"
Original    (012): ['props', '.', 'update', '(', 'self', '.', '_class_props', '[', 'p', ']', ')', '\\n']
Tokenized   (020): ['<s>', 'pro', 'ps', '.', 'update', '(', 'self', '.', '_', 'class', '_', 'pro', 'ps', '[', 'p', ']', ')', '\\', 'n', '</s>']
Filtered   (018): ['pro', 'ps', '.', 'update', '(', 'self', '.', '_', 'class', '_', 'pro', 'ps', '[', 'p', ']', ')', '\\', 'n']
Detokenized (012): ['props', '.', 'update', '(', 'self', '.', '_class_props', '[', 'p', ']', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "kwargs_init = [ % ( x . split ( ) [ 0 ] , x . split ( ) [ 0 ] ) for x in kwargs ] \n"
Original    (029): ['kwargs_init', '=', '[', '%', '(', 'x', '.', 'split', '(', ')', '[', '0', ']', ',', 'x', '.', 'split', '(', ')', '[', '0', ']', ')', 'for', 'x', 'in', 'kwargs', ']', '\\n']
Tokenized   (037): ['<s>', 'kw', 'args', '_', 'init', '=', '[', '%', '(', 'x', '.', 'split', '(', ')', '[', '0', ']', ',', 'x', '.', 'split', '(', ')', '[', '0', ']', ')', 'for', 'x', 'in', 'k', 'w', 'args', ']', '\\', 'n', '</s>']
Filtered   (035): ['kw', 'args', '_', 'init', '=', '[', '%', '(', 'x', '.', 'split', '(', ')', '[', '0', ']', ',', 'x', '.', 'split', '(', ')', '[', '0', ']', ')', 'for', 'x', 'in', 'k', 'w', 'args', ']', '\\', 'n']
Detokenized (029): ['kwargs_init', '=', '[', '%', '(', 'x', '.', 'split', '(', ')', '[', '0', ']', ',', 'x', '.', 'split', '(', ')', '[', '0', ']', ')', 'for', 'x', 'in', 'kwargs', ']', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 29, 768)
# Extracted words:  29
Sentence         : "else : nargs = node . args . args \n"
Original    (010): ['else', ':', 'nargs', '=', 'node', '.', 'args', '.', 'args', '\\n']
Tokenized   (014): ['<s>', 'else', ':', 'n', 'args', '=', 'node', '.', 'args', '.', 'args', '\\', 'n', '</s>']
Filtered   (012): ['else', ':', 'n', 'args', '=', 'node', '.', 'args', '.', 'args', '\\', 'n']
Detokenized (010): ['else', ':', 'nargs', '=', 'node', '.', 'args', '.', 'args', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "kwargs . append ( % ( a , default_value ) ) \n"
Original    (012): ['kwargs', '.', 'append', '(', '%', '(', 'a', ',', 'default_value', ')', ')', '\\n']
Tokenized   (018): ['<s>', 'kw', 'args', '.', 'append', '(', '%', '(', 'a', ',', 'default', '_', 'value', ')', ')', '\\', 'n', '</s>']
Filtered   (016): ['kw', 'args', '.', 'append', '(', '%', '(', 'a', ',', 'default', '_', 'value', ')', ')', '\\', 'n']
Detokenized (012): ['kwargs', '.', 'append', '(', '%', '(', 'a', ',', 'default_value', ')', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "offset = len ( node . args . args ) - len ( node . args . defaults ) \n"
Original    (020): ['offset', '=', 'len', '(', 'node', '.', 'args', '.', 'args', ')', '-', 'len', '(', 'node', '.', 'args', '.', 'defaults', ')', '\\n']
Tokenized   (023): ['<s>', 'offset', '=', 'len', '(', 'node', '.', 'args', '.', 'args', ')', '-', 'len', '(', 'node', '.', 'args', '.', 'defaults', ')', '\\', 'n', '</s>']
Filtered   (021): ['offset', '=', 'len', '(', 'node', '.', 'args', '.', 'args', ')', '-', 'len', '(', 'node', '.', 'args', '.', 'defaults', ')', '\\', 'n']
Detokenized (020): ['offset', '=', 'len', '(', 'node', '.', 'args', '.', 'args', ')', '-', 'len', '(', 'node', '.', 'args', '.', 'defaults', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "varargs = [ % n for n in range ( 16 ) ] \n"
Original    (014): ['varargs', '=', '[', '%', 'n', 'for', 'n', 'in', 'range', '(', '16', ')', ']', '\\n']
Tokenized   (018): ['<s>', 'var', 'args', '=', '[', '%', 'n', 'for', 'n', 'in', 'range', '(', '16', ')', ']', '\\', 'n', '</s>']
Filtered   (016): ['var', 'args', '=', '[', '%', 'n', 'for', 'n', 'in', 'range', '(', '16', ')', ']', '\\', 'n']
Detokenized (014): ['varargs', '=', '[', '%', 'n', 'for', 'n', 'in', 'range', '(', '16', ')', ']', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "buffer += % self . indent ( ) \n"
Original    (009): ['buffer', '+=', '%', 'self', '.', 'indent', '(', ')', '\\n']
Tokenized   (012): ['<s>', 'buffer', '+=', '%', 'self', '.', 'indent', '(', ')', '\\', 'n', '</s>']
Filtered   (010): ['buffer', '+=', '%', 'self', '.', 'indent', '(', ')', '\\', 'n']
Detokenized (009): ['buffer', '+=', '%', 'self', '.', 'indent', '(', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "arg_name = args = None \n"
Original    (006): ['arg_name', '=', 'args', '=', 'None', '\\n']
Tokenized   (011): ['<s>', 'arg', '_', 'name', '=', 'args', '=', 'None', '\\', 'n', '</s>']
Filtered   (009): ['arg', '_', 'name', '=', 'args', '=', 'None', '\\', 'n']
Detokenized (006): ['arg_name', '=', 'args', '=', 'None', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "comp . append ( self . visit ( node . comparators [ i ] ) ) \n"
Original    (017): ['comp', '.', 'append', '(', 'self', '.', 'visit', '(', 'node', '.', 'comparators', '[', 'i', ']', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'comp', '.', 'append', '(', 'self', '.', 'visit', '(', 'node', '.', 'compar', 'ators', '[', 'i', ']', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['comp', '.', 'append', '(', 'self', '.', 'visit', '(', 'node', '.', 'compar', 'ators', '[', 'i', ']', ')', ')', '\\', 'n']
Detokenized (017): ['comp', '.', 'append', '(', 'self', '.', 'visit', '(', 'node', '.', 'comparators', '[', 'i', ']', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "testtime = time ( ) - starttime \n"
Original    (008): ['testtime', '=', 'time', '(', ')', '-', 'starttime', '\\n']
Tokenized   (013): ['<s>', 'test', 'time', '=', 'time', '(', ')', '-', 'start', 'time', '\\', 'n', '</s>']
Filtered   (011): ['test', 'time', '=', 'time', '(', ')', '-', 'start', 'time', '\\', 'n']
Detokenized (008): ['testtime', '=', 'time', '(', ')', '-', 'starttime', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "primes_per_sec = len ( seq ) * ( 1.0 / testtime ) \n"
Original    (013): ['primes_per_sec', '=', 'len', '(', 'seq', ')', '*', '(', '1.0', '/', 'testtime', ')', '\\n']
Tokenized   (024): ['<s>', 'pr', 'imes', '_', 'per', '_', 'sec', '=', 'len', '(', 'seq', ')', '*', '(', '1', '.', '0', '/', 'test', 'time', ')', '\\', 'n', '</s>']
Filtered   (022): ['pr', 'imes', '_', 'per', '_', 'sec', '=', 'len', '(', 'seq', ')', '*', '(', '1', '.', '0', '/', 'test', 'time', ')', '\\', 'n']
Detokenized (013): ['primes_per_sec', '=', 'len', '(', 'seq', ')', '*', '(', '1.0', '/', 'testtime', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "b = range ( 1 , 10 ) \n"
Original    (009): ['b', '=', 'range', '(', '1', ',', '10', ')', '\\n']
Tokenized   (012): ['<s>', 'b', '=', 'range', '(', '1', ',', '10', ')', '\\', 'n', '</s>']
Filtered   (010): ['b', '=', 'range', '(', '1', ',', '10', ')', '\\', 'n']
Detokenized (009): ['b', '=', 'range', '(', '1', ',', '10', ')', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "w1 = threading . start_webworker ( worker , ( seq , , ) ) \n"
Original    (015): ['w1', '=', 'threading', '.', 'start_webworker', '(', 'worker', ',', '(', 'seq', ',', ',', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'w', '1', '=', 'thread', 'ing', '.', 'start', '_', 'web', 'worker', '(', 'worker', ',', '(', 'seq', ',', ',', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['w', '1', '=', 'thread', 'ing', '.', 'start', '_', 'web', 'worker', '(', 'worker', ',', '(', 'seq', ',', ',', ')', ')', '\\', 'n']
Detokenized (015): ['w1', '=', 'threading', '.', 'start_webworker', '(', 'worker', ',', '(', 'seq', ',', ',', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "TestError ( in seq ) \n"
Original    (006): ['TestError', '(', 'in', 'seq', ')', '\\n']
Tokenized   (010): ['<s>', 'Test', 'Error', '(', 'in', 'seq', ')', '\\', 'n', '</s>']
Filtered   (008): ['Test', 'Error', '(', 'in', 'seq', ')', '\\', 'n']
Detokenized (006): ['TestError', '(', 'in', 'seq', ')', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "del self . face_groups [ : ] \n"
Original    (008): ['del', 'self', '.', 'face_groups', '[', ':', ']', '\\n']
Tokenized   (013): ['<s>', 'del', 'self', '.', 'face', '_', 'groups', '[', ':', ']', '\\', 'n', '</s>']
Filtered   (011): ['del', 'self', '.', 'face', '_', 'groups', '[', ':', ']', '\\', 'n']
Detokenized (008): ['del', 'self', '.', 'face_groups', '[', ':', ']', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "mtllib_path = os . path . join ( model_path , data [ 0 ] ) \n"
Original    (016): ['mtllib_path', '=', 'os', '.', 'path', '.', 'join', '(', 'model_path', ',', 'data', '[', '0', ']', ')', '\\n']
Tokenized   (025): ['<s>', 'mt', 'll', 'ib', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'model', '_', 'path', ',', 'data', '[', '0', ']', ')', '\\', 'n', '</s>']
Filtered   (023): ['mt', 'll', 'ib', '_', 'path', '=', 'os', '.', 'path', '.', 'join', '(', 'model', '_', 'path', ',', 'data', '[', '0', ']', ')', '\\', 'n']
Detokenized (016): ['mtllib_path', '=', 'os', '.', 'path', '.', 'join', '(', 'model_path', ',', 'data', '[', '0', ']', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "vertex = ( float ( x ) , float ( y ) , float ( z ) ) \n"
Original    (019): ['vertex', '=', '(', 'float', '(', 'x', ')', ',', 'float', '(', 'y', ')', ',', 'float', '(', 'z', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'ver', 'tex', '=', '(', 'float', '(', 'x', ')', ',', 'float', '(', 'y', ')', ',', 'float', '(', 'z', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['ver', 'tex', '=', '(', 'float', '(', 'x', ')', ',', 'float', '(', 'y', ')', ',', 'float', '(', 'z', ')', ')', '\\', 'n']
Detokenized (019): ['vertex', '=', '(', 'float', '(', 'x', ')', ',', 'float', '(', 'y', ')', ',', 'float', '(', 'z', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "tex_coord = ( float ( s ) , float ( t ) ) \n"
Original    (014): ['tex_coord', '=', '(', 'float', '(', 's', ')', ',', 'float', '(', 't', ')', ')', '\\n']
Tokenized   (019): ['<s>', 'tex', '_', 'coord', '=', '(', 'float', '(', 's', ')', ',', 'float', '(', 't', ')', ')', '\\', 'n', '</s>']
Filtered   (017): ['tex', '_', 'coord', '=', '(', 'float', '(', 's', ')', ',', 'float', '(', 't', ')', ')', '\\', 'n']
Detokenized (014): ['tex_coord', '=', '(', 'float', '(', 's', ')', ',', 'float', '(', 't', ')', ')', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "indices = ( int ( vi ) - 1 , int ( ti ) - 1 , int ( ni ) - 1 ) \n"
Original    (025): ['indices', '=', '(', 'int', '(', 'vi', ')', '-', '1', ',', 'int', '(', 'ti', ')', '-', '1', ',', 'int', '(', 'ni', ')', '-', '1', ')', '\\n']
Tokenized   (029): ['<s>', 'ind', 'ices', '=', '(', 'int', '(', 'vi', ')', '-', '1', ',', 'int', '(', 'ti', ')', '-', '1', ',', 'int', '(', 'ni', ')', '-', '1', ')', '\\', 'n', '</s>']
Filtered   (027): ['ind', 'ices', '=', '(', 'int', '(', 'vi', ')', '-', '1', ',', 'int', '(', 'ti', ')', '-', '1', ',', 'int', '(', 'ni', ')', '-', '1', ')', '\\', 'n']
Detokenized (025): ['indices', '=', '(', 'int', '(', 'vi', ')', '-', '1', ',', 'int', '(', 'ti', ')', '-', '1', ',', 'int', '(', 'ni', ')', '-', '1', ')', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "glBindTexture ( GL_TEXTURE_2D , material . texture_id ) \n"
Original    (009): ['glBindTexture', '(', 'GL_TEXTURE_2D', ',', 'material', '.', 'texture_id', ')', '\\n']
Tokenized   (021): ['<s>', 'gl', 'Bind', 'Texture', '(', 'GL', '_', 'TEXTURE', '_', '2', 'D', ',', 'material', '.', 'texture', '_', 'id', ')', '\\', 'n', '</s>']
Filtered   (019): ['gl', 'Bind', 'Texture', '(', 'GL', '_', 'TEXTURE', '_', '2', 'D', ',', 'material', '.', 'texture', '_', 'id', ')', '\\', 'n']
Detokenized (009): ['glBindTexture', '(', 'GL_TEXTURE_2D', ',', 'material', '.', 'texture_id', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "glPixelStorei ( GL_UNPACK_ALIGNMENT , 1 ) \n"
Original    (007): ['glPixelStorei', '(', 'GL_UNPACK_ALIGNMENT', ',', '1', ')', '\\n']
Tokenized   (021): ['<s>', 'gl', 'Pixel', 'Store', 'i', '(', 'GL', '_', 'UN', 'P', 'ACK', '_', 'AL', 'IGN', 'MENT', ',', '1', ')', '\\', 'n', '</s>']
Filtered   (019): ['gl', 'Pixel', 'Store', 'i', '(', 'GL', '_', 'UN', 'P', 'ACK', '_', 'AL', 'IGN', 'MENT', ',', '1', ')', '\\', 'n']
Detokenized (007): ['glPixelStorei', '(', 'GL_UNPACK_ALIGNMENT', ',', '1', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "glNormal3fv ( normals [ ni ] ) \n"
Original    (008): ['glNormal3fv', '(', 'normals', '[', 'ni', ']', ')', '\\n']
Tokenized   (016): ['<s>', 'gl', 'Normal', '3', 'f', 'v', '(', 'norm', 'als', '[', 'ni', ']', ')', '\\', 'n', '</s>']
Filtered   (014): ['gl', 'Normal', '3', 'f', 'v', '(', 'norm', 'als', '[', 'ni', ']', ')', '\\', 'n']
Detokenized (008): ['glNormal3fv', '(', 'normals', '[', 'ni', ']', ')', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "picture = pygame . image . load ( picture_file ) . convert ( ) \n"
Original    (015): ['picture', '=', 'pygame', '.', 'image', '.', 'load', '(', 'picture_file', ')', '.', 'convert', '(', ')', '\\n']
Tokenized   (021): ['<s>', 'picture', '=', 'py', 'game', '.', 'image', '.', 'load', '(', 'picture', '_', 'file', ')', '.', 'convert', '(', ')', '\\', 'n', '</s>']
Filtered   (019): ['picture', '=', 'py', 'game', '.', 'image', '.', 'load', '(', 'picture', '_', 'file', ')', '.', 'convert', '(', ')', '\\', 'n']
Detokenized (015): ['picture', '=', 'pygame', '.', 'image', '.', 'load', '(', 'picture_file', ')', '.', 'convert', '(', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "screen . blit ( picture , ( - picture_pos . x , picture_pos . y ) ) \n"
Original    (018): ['screen', '.', 'blit', '(', 'picture', ',', '(', '-', 'picture_pos', '.', 'x', ',', 'picture_pos', '.', 'y', ')', ')', '\\n']
Tokenized   (026): ['<s>', 'screen', '.', 'bl', 'it', '(', 'picture', ',', '(', '-', 'picture', '_', 'pos', '.', 'x', ',', 'picture', '_', 'pos', '.', 'y', ')', ')', '\\', 'n', '</s>']
Filtered   (024): ['screen', '.', 'bl', 'it', '(', 'picture', ',', '(', '-', 'picture', '_', 'pos', '.', 'x', ',', 'picture', '_', 'pos', '.', 'y', ')', ')', '\\', 'n']
Detokenized (018): ['screen', '.', 'blit', '(', 'picture', ',', '(', '-', 'picture_pos', '.', 'x', ',', 'picture_pos', '.', 'y', ')', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "time_passed_seconds = time_passed / 1000.0 \n"
Original    (006): ['time_passed_seconds', '=', 'time_passed', '/', '1000.0', '\\n']
Tokenized   (019): ['<s>', 'time', '_', 'pass', 'ed', '_', 'seconds', '=', 'time', '_', 'pass', 'ed', '/', '1000', '.', '0', '\\', 'n', '</s>']
Filtered   (017): ['time', '_', 'pass', 'ed', '_', 'seconds', '=', 'time', '_', 'pass', 'ed', '/', '1000', '.', '0', '\\', 'n']
Detokenized (006): ['time_passed_seconds', '=', 'time_passed', '/', '1000.0', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "picture_pos += scroll_direction * scroll_speed * time_passed_seconds \n"
Original    (008): ['picture_pos', '+=', 'scroll_direction', '*', 'scroll_speed', '*', 'time_passed_seconds', '\\n']
Tokenized   (022): ['<s>', 'picture', '_', 'pos', '+=', 'scroll', '_', 'direction', '*', 'scroll', '_', 'speed', '*', 'time', '_', 'pass', 'ed', '_', 'seconds', '\\', 'n', '</s>']
Filtered   (020): ['picture', '_', 'pos', '+=', 'scroll', '_', 'direction', '*', 'scroll', '_', 'speed', '*', 'time', '_', 'pass', 'ed', '_', 'seconds', '\\', 'n']
Detokenized (008): ['picture_pos', '+=', 'scroll_direction', '*', 'scroll_speed', '*', 'time_passed_seconds', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "VERSION = open ( "version.txt" ) . readline ( ) . strip ( ) \n"
Original    (015): ['VERSION', '=', 'open', '(', '"version.txt"', ')', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '\\n']
Tokenized   (023): ['<s>', 'VERSION', '=', 'open', '(', '"', 'version', '.', 'txt', '"', ')', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '\\', 'n', '</s>']
Filtered   (021): ['VERSION', '=', 'open', '(', '"', 'version', '.', 'txt', '"', ')', '.', 'read', 'line', '(', ')', '.', 'strip', '(', ')', '\\', 'n']
Detokenized (015): ['VERSION', '=', 'open', '(', '"version.txt"', ')', '.', 'readline', '(', ')', '.', 'strip', '(', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "DOWNLOAD_URL = DOWNLOAD_BASEURL + "dubbo-client-%s-py2.7.egg" % VERSION \n"
Original    (008): ['DOWNLOAD_URL', '=', 'DOWNLOAD_BASEURL', '+', '"dubbo-client-%s-py2.7.egg"', '%', 'VERSION', '\\n']
Tokenized   (036): ['<s>', 'DOWN', 'LOAD', '_', 'URL', '=', 'DOWN', 'LOAD', '_', 'B', 'ASE', 'URL', '+', '"', 'd', 'ub', 'bo', '-', 'client', '-', '%', 's', '-', 'py', '2', '.', '7', '.', 'egg', '"', '%', 'VERS', 'ION', '\\', 'n', '</s>']
Filtered   (034): ['DOWN', 'LOAD', '_', 'URL', '=', 'DOWN', 'LOAD', '_', 'B', 'ASE', 'URL', '+', '"', 'd', 'ub', 'bo', '-', 'client', '-', '%', 's', '-', 'py', '2', '.', '7', '.', 'egg', '"', '%', 'VERS', 'ION', '\\', 'n']
Detokenized (008): ['DOWNLOAD_URL', '=', 'DOWNLOAD_BASEURL', '+', '"dubbo-client-%s-py2.7.egg"', '%', 'VERSION', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "long_description = open ( "README.md" ) . read ( ) , \n"
Original    (012): ['long_description', '=', 'open', '(', '"README.md"', ')', '.', 'read', '(', ')', ',', '\\n']
Tokenized   (022): ['<s>', 'long', '_', 'description', '=', 'open', '(', '"', 'READ', 'ME', '.', 'md', '"', ')', '.', 'read', '(', ')', ',', '\\', 'n', '</s>']
Filtered   (020): ['long', '_', 'description', '=', 'open', '(', '"', 'READ', 'ME', '.', 'md', '"', ')', '.', 'read', '(', ')', ',', '\\', 'n']
Detokenized (012): ['long_description', '=', 'open', '(', '"README.md"', ')', '.', 'read', '(', ')', ',', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "install_requires = [ "kazoo>=2.0" , "python-jsonrpc>=0.7.3" ] , \n"
Original    (009): ['install_requires', '=', '[', '"kazoo>=2.0"', ',', '"python-jsonrpc>=0.7.3"', ']', ',', '\\n']
Tokenized   (036): ['<s>', 'install', '_', 'requires', '=', '[', '"', 'k', 'az', 'oo', '>', '=', '2', '.', '0', '"', ',', '"', 'python', '-', 'json', 'r', 'pc', '>', '=', '0', '.', '7', '.', '3', '"', ']', ',', '\\', 'n', '</s>']
Filtered   (034): ['install', '_', 'requires', '=', '[', '"', 'k', 'az', 'oo', '>', '=', '2', '.', '0', '"', ',', '"', 'python', '-', 'json', 'r', 'pc', '>', '=', '0', '.', '7', '.', '3', '"', ']', ',', '\\', 'n']
Detokenized (009): ['install_requires', '=', '[', '"kazoo>=2.0"', ',', '"python-jsonrpc>=0.7.3"', ']', ',', '\\n']
Counter: 34
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "field = models . BooleanField ( default = False ) , \n"
Original    (012): ['field', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ')', ',', '\\n']
Tokenized   (016): ['<s>', 'field', '=', 'models', '.', 'Boolean', 'Field', '(', 'default', '=', 'False', ')', ',', '\\', 'n', '</s>']
Filtered   (014): ['field', '=', 'models', '.', 'Boolean', 'Field', '(', 'default', '=', 'False', ')', ',', '\\', 'n']
Detokenized (012): ['field', '=', 'models', '.', 'BooleanField', '(', 'default', '=', 'False', ')', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "args = [ in_path , user_out_path ] , env = [ "PATH=" + os . environ . get ( "PATH" , "" ) use_sandbox = True , use_nobody = True ) \n"
Original    (032): ['args', '=', '[', 'in_path', ',', 'user_out_path', ']', ',', 'env', '=', '[', '"PATH="', '+', 'os', '.', 'environ', '.', 'get', '(', '"PATH"', ',', '""', ')', 'use_sandbox', '=', 'True', ',', 'use_nobody', '=', 'True', ')', '\\n']
Tokenized   (052): ['<s>', 'args', '=', '[', 'in', '_', 'path', ',', 'user', '_', 'out', '_', 'path', ']', ',', 'env', '=', '[', '"', 'PATH', '="', '+', 'os', '.', 'en', 'viron', '.', 'get', '(', '"', 'PATH', '"', ',', '""', ')', 'use', '_', 'sand', 'box', '=', 'True', ',', 'use', '_', 'nob', 'ody', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (050): ['args', '=', '[', 'in', '_', 'path', ',', 'user', '_', 'out', '_', 'path', ']', ',', 'env', '=', '[', '"', 'PATH', '="', '+', 'os', '.', 'en', 'viron', '.', 'get', '(', '"', 'PATH', '"', ',', '""', ')', 'use', '_', 'sand', 'box', '=', 'True', ',', 'use', '_', 'nob', 'ody', '=', 'True', ')', '\\', 'n']
Detokenized (032): ['args', '=', '[', 'in_path', ',', 'user_out_path', ']', ',', 'env', '=', '[', '"PATH="', '+', 'os', '.', 'environ', '.', 'get', '(', '"PATH"', ',', '""', ')', 'use_sandbox', '=', 'True', ',', 'use_nobody', '=', 'True', ')', '\\n']
Counter: 50
===================================================================
Hidden states:  (13, 32, 768)
# Extracted words:  32
Sentence         : "print_skip = 5 , * args , ** kwargs ) : \n"
Original    (012): ['print_skip', '=', '5', ',', '*', 'args', ',', '**', 'kwargs', ')', ':', '\\n']
Tokenized   (019): ['<s>', 'print', '_', 'skip', '=', '5', ',', '*', 'args', ',', '**', 'k', 'w', 'args', ')', ':', '\\', 'n', '</s>']
Filtered   (017): ['print', '_', 'skip', '=', '5', ',', '*', 'args', ',', '**', 'k', 'w', 'args', ')', ':', '\\', 'n']
Detokenized (012): ['print_skip', '=', '5', ',', '*', 'args', ',', '**', 'kwargs', ')', ':', '\\n']
Counter: 17
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "error = np . max ( np . abs ( new_v - v ) ) \n"
Original    (016): ['error', '=', 'np', '.', 'max', '(', 'np', '.', 'abs', '(', 'new_v', '-', 'v', ')', ')', '\\n']
Tokenized   (021): ['<s>', 'error', '=', 'np', '.', 'max', '(', 'np', '.', 'abs', '(', 'new', '_', 'v', '-', 'v', ')', ')', '\\', 'n', '</s>']
Filtered   (019): ['error', '=', 'np', '.', 'max', '(', 'np', '.', 'abs', '(', 'new', '_', 'v', '-', 'v', ')', ')', '\\', 'n']
Detokenized (016): ['error', '=', 'np', '.', 'max', '(', 'np', '.', 'abs', '(', 'new_v', '-', 'v', ')', ')', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "ac = ( a_0 - c ) / 2.0 \n"
Original    (010): ['ac', '=', '(', 'a_0', '-', 'c', ')', '/', '2.0', '\\n']
Tokenized   (017): ['<s>', 'ac', '=', '(', 'a', '_', '0', '-', 'c', ')', '/', '2', '.', '0', '\\', 'n', '</s>']
Filtered   (015): ['ac', '=', '(', 'a', '_', '0', '-', 'c', ')', '/', '2', '.', '0', '\\', 'n']
Detokenized (010): ['ac', '=', '(', 'a_0', '-', 'c', ')', '/', '2.0', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "R = - R \n"
Original    (005): ['R', '=', '-', 'R', '\\n']
Tokenized   (008): ['<s>', 'R', '=', '-', 'R', '\\', 'n', '</s>']
Filtered   (006): ['R', '=', '-', 'R', '\\', 'n']
Detokenized (005): ['R', '=', '-', 'R', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "B = np . array ( [ [ 0. ] , \n"
Original    (012): ['B', '=', 'np', '.', 'array', '(', '[', '[', '0.', ']', ',', '\\n']
Tokenized   (016): ['<s>', 'B', '=', 'np', '.', 'array', '(', '[', '[', '0', '.', ']', ',', '\\', 'n', '</s>']
Filtered   (014): ['B', '=', 'np', '.', 'array', '(', '[', '[', '0', '.', ']', ',', '\\', 'n']
Detokenized (012): ['B', '=', 'np', '.', 'array', '(', '[', '[', '0.', ']', ',', '\\n']
Counter: 14
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "Fr , Kr , Pr = self . Fr , self . Kr , self . Pr \n"
Original    (018): ['Fr', ',', 'Kr', ',', 'Pr', '=', 'self', '.', 'Fr', ',', 'self', '.', 'Kr', ',', 'self', '.', 'Pr', '\\n']
Tokenized   (021): ['<s>', 'Fr', ',', 'Kr', ',', 'Pr', '=', 'self', '.', 'Fr', ',', 'self', '.', 'Kr', ',', 'self', '.', 'Pr', '\\', 'n', '</s>']
Filtered   (019): ['Fr', ',', 'Kr', ',', 'Pr', '=', 'self', '.', 'Fr', ',', 'self', '.', 'Kr', ',', 'self', '.', 'Pr', '\\', 'n']
Detokenized (018): ['Fr', ',', 'Kr', ',', 'Pr', '=', 'self', '.', 'Fr', ',', 'self', '.', 'Kr', ',', 'self', '.', 'Pr', '\\n']
Counter: 19
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "Fs , Ks , Ps = rblq . robust_rule_simple ( P_init = Pr , tol = 1e-12 ) \n"
Original    (019): ['Fs', ',', 'Ks', ',', 'Ps', '=', 'rblq', '.', 'robust_rule_simple', '(', 'P_init', '=', 'Pr', ',', 'tol', '=', '1e-12', ')', '\\n']
Tokenized   (035): ['<s>', 'Fs', ',', 'K', 's', ',', 'Ps', '=', 'r', 'bl', 'q', '.', 'robust', '_', 'rule', '_', 'simple', '(', 'P', '_', 'init', '=', 'Pr', ',', 'to', 'l', '=', '1', 'e', '-', '12', ')', '\\', 'n', '</s>']
Filtered   (033): ['Fs', ',', 'K', 's', ',', 'Ps', '=', 'r', 'bl', 'q', '.', 'robust', '_', 'rule', '_', 'simple', '(', 'P', '_', 'init', '=', 'Pr', ',', 'to', 'l', '=', '1', 'e', '-', '12', ')', '\\', 'n']
Detokenized (019): ['Fs', ',', 'Ks', ',', 'Ps', '=', 'rblq', '.', 'robust_rule_simple', '(', 'P_init', '=', 'Pr', ',', 'tol', '=', '1e-12', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 19, 768)
# Extracted words:  19
Sentence         : "Kf , Pf , df , Of , of = rblq . evaluate_F ( Fr ) \n"
Original    (017): ['Kf', ',', 'Pf', ',', 'df', ',', 'Of', ',', 'of', '=', 'rblq', '.', 'evaluate_F', '(', 'Fr', ')', '\\n']
Tokenized   (025): ['<s>', 'K', 'f', ',', 'Pf', ',', 'df', ',', 'Of', ',', 'of', '=', 'r', 'bl', 'q', '.', 'evaluate', '_', 'F', '(', 'Fr', ')', '\\', 'n', '</s>']
Filtered   (023): ['K', 'f', ',', 'Pf', ',', 'df', ',', 'Of', ',', 'of', '=', 'r', 'bl', 'q', '.', 'evaluate', '_', 'F', '(', 'Fr', ')', '\\', 'n']
Detokenized (017): ['Kf', ',', 'Pf', ',', 'df', ',', 'Of', ',', 'of', '=', 'rblq', '.', 'evaluate_F', '(', 'Fr', ')', '\\n']
Counter: 23
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "group = h5f . createGroup ( "/" , ) \n"
Original    (010): ['group', '=', 'h5f', '.', 'createGroup', '(', '"/"', ',', ')', '\\n']
Tokenized   (017): ['<s>', 'group', '=', 'h', '5', 'f', '.', 'create', 'Group', '(', '"/', '"', ',', ')', '\\', 'n', '</s>']
Filtered   (015): ['group', '=', 'h', '5', 'f', '.', 'create', 'Group', '(', '"/', '"', ',', ')', '\\', 'n']
Detokenized (010): ['group', '=', 'h5f', '.', 'createGroup', '(', '"/"', ',', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "global ctr \n"
Original    (003): ['global', 'ctr', '\\n']
Tokenized   (007): ['<s>', 'global', 'c', 'tr', '\\', 'n', '</s>']
Filtered   (005): ['global', 'c', 'tr', '\\', 'n']
Detokenized (003): ['global', 'ctr', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 3, 768)
# Extracted words:  3
Sentence         : "listOfInputPaths . append ( rootdir + "/Raw/Yahoo/US/NYSE/" ) \n"
Original    (009): ['listOfInputPaths', '.', 'append', '(', 'rootdir', '+', '"/Raw/Yahoo/US/NYSE/"', ')', '\\n']
Tokenized   (026): ['<s>', 'list', 'Of', 'Input', 'Path', 's', '.', 'append', '(', 'root', 'dir', '+', '"/', 'Raw', '/', 'Y', 'ahoo', '/', 'US', '/', 'NYSE', '/"', ')', '\\', 'n', '</s>']
Filtered   (024): ['list', 'Of', 'Input', 'Path', 's', '.', 'append', '(', 'root', 'dir', '+', '"/', 'Raw', '/', 'Y', 'ahoo', '/', 'US', '/', 'NYSE', '/"', ')', '\\', 'n']
Detokenized (009): ['listOfInputPaths', '.', 'append', '(', 'rootdir', '+', '"/Raw/Yahoo/US/NYSE/"', ')', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "filtered_names = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , filtered_names ) \n"
Original    (025): ['filtered_names', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'fileExtensionToRemove', ')', ')', '[', '0', ']', ')', ',', 'filtered_names', ')', '\\n']
Tokenized   (037): ['<s>', 'fil', 'tered', '_', 'names', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'file', 'Ext', 'ension', 'To', 'Remove', ')', ')', '[', '0', ']', ')', ',', 'filtered', '_', 'names', ')', '\\', 'n', '</s>']
Filtered   (035): ['fil', 'tered', '_', 'names', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'file', 'Ext', 'ension', 'To', 'Remove', ')', ')', '[', '0', ']', ')', ',', 'filtered', '_', 'names', ')', '\\', 'n']
Detokenized (025): ['filtered_names', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'fileExtensionToRemove', ')', ')', '[', '0', ']', ')', ',', 'filtered_names', ')', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 25, 768)
# Extracted words:  25
Sentence         : "stock_data = np . loadtxt ( path + stock + ".csv" , np . float , None , "," , None , 1 , use_cols ) \n"
Original    (027): ['stock_data', '=', 'np', '.', 'loadtxt', '(', 'path', '+', 'stock', '+', '".csv"', ',', 'np', '.', 'float', ',', 'None', ',', '","', ',', 'None', ',', '1', ',', 'use_cols', ')', '\\n']
Tokenized   (039): ['<s>', 'stock', '_', 'data', '=', 'np', '.', 'load', 'txt', '(', 'path', '+', 'stock', '+', '".', 'csv', '"', ',', 'np', '.', 'float', ',', 'None', ',', '"', ',"', ',', 'None', ',', '1', ',', 'use', '_', 'col', 's', ')', '\\', 'n', '</s>']
Filtered   (037): ['stock', '_', 'data', '=', 'np', '.', 'load', 'txt', '(', 'path', '+', 'stock', '+', '".', 'csv', '"', ',', 'np', '.', 'float', ',', 'None', ',', '"', ',"', ',', 'None', ',', '1', ',', 'use', '_', 'col', 's', ')', '\\', 'n']
Detokenized (027): ['stock_data', '=', 'np', '.', 'loadtxt', '(', 'path', '+', 'stock', '+', '".csv"', ',', 'np', '.', 'float', ',', 'None', ',', '","', ',', 'None', ',', '1', ',', 'use_cols', ')', '\\n']
Counter: 37
===================================================================
Hidden states:  (13, 27, 768)
# Extracted words:  27
Sentence         : "pkl . dump ( stock_data , f , - 1 ) \n"
Original    (012): ['pkl', '.', 'dump', '(', 'stock_data', ',', 'f', ',', '-', '1', ')', '\\n']
Tokenized   (018): ['<s>', 'p', 'kl', '.', 'dump', '(', 'stock', '_', 'data', ',', 'f', ',', '-', '1', ')', '\\', 'n', '</s>']
Filtered   (016): ['p', 'kl', '.', 'dump', '(', 'stock', '_', 'data', ',', 'f', ',', '-', '1', ')', '\\', 'n']
Detokenized (012): ['pkl', '.', 'dump', '(', 'stock_data', ',', 'f', ',', '-', '1', ')', '\\n']
Counter: 16
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "startday = dt . datetime ( t [ 2 ] , t [ 0 ] , t [ 1 ] ) \n"
Original    (022): ['startday', '=', 'dt', '.', 'datetime', '(', 't', '[', '2', ']', ',', 't', '[', '0', ']', ',', 't', '[', '1', ']', ')', '\\n']
Tokenized   (028): ['<s>', 'start', 'day', '=', 'd', 't', '.', 'dat', 'etime', '(', 't', '[', '2', ']', ',', 't', '[', '0', ']', ',', 't', '[', '1', ']', ')', '\\', 'n', '</s>']
Filtered   (026): ['start', 'day', '=', 'd', 't', '.', 'dat', 'etime', '(', 't', '[', '2', ']', ',', 't', '[', '0', ']', ',', 't', '[', '1', ']', ')', '\\', 'n']
Detokenized (022): ['startday', '=', 'dt', '.', 'datetime', '(', 't', '[', '2', ']', ',', 't', '[', '0', ']', ',', 't', '[', '1', ']', ')', '\\n']
Counter: 26
===================================================================
Hidden states:  (13, 22, 768)
# Extracted words:  22
Sentence         : "t = map ( int , sys . argv [ 2 ] . split ( ) ) \n"
Original    (018): ['t', '=', 'map', '(', 'int', ',', 'sys', '.', 'argv', '[', '2', ']', '.', 'split', '(', ')', ')', '\\n']
Tokenized   (022): ['<s>', 't', '=', 'map', '(', 'int', ',', 'sys', '.', 'arg', 'v', '[', '2', ']', '.', 'split', '(', ')', ')', '\\', 'n', '</s>']
Filtered   (020): ['t', '=', 'map', '(', 'int', ',', 'sys', '.', 'arg', 'v', '[', '2', ']', '.', 'split', '(', ')', ')', '\\', 'n']
Detokenized (018): ['t', '=', 'map', '(', 'int', ',', 'sys', '.', 'argv', '[', '2', ']', '.', 'split', '(', ')', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "historic = dataobj . get_data ( timestamps , symbols , "close" ) \n"
Original    (013): ['historic', '=', 'dataobj', '.', 'get_data', '(', 'timestamps', ',', 'symbols', ',', '"close"', ')', '\\n']
Tokenized   (023): ['<s>', 'historic', '=', 'data', 'obj', '.', 'get', '_', 'data', '(', 'tim', 'est', 'amps', ',', 'symbols', ',', '"', 'close', '"', ')', '\\', 'n', '</s>']
Filtered   (021): ['historic', '=', 'data', 'obj', '.', 'get', '_', 'data', '(', 'tim', 'est', 'amps', ',', 'symbols', ',', '"', 'close', '"', ')', '\\', 'n']
Detokenized (013): ['historic', '=', 'dataobj', '.', 'get_data', '(', 'timestamps', ',', 'symbols', ',', '"close"', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 13, 768)
# Extracted words:  13
Sentence         : "alloc = alloc . append ( DataMatrix ( index = [ historic . index [ date ] ] , data = [ alloc_val ] , columns = [ symbols ~~ alloc [ ] = 1 - alloc [ symbols [ 0 ] ] \n"
Original    (044): ['alloc', '=', 'alloc', '.', 'append', '(', 'DataMatrix', '(', 'index', '=', '[', 'historic', '.', 'index', '[', 'date', ']', ']', ',', 'data', '=', '[', 'alloc_val', ']', ',', 'columns', '=', '[', 'symbols', '~~', 'alloc', '[', ']', '=', '1', '-', 'alloc', '[', 'symbols', '[', '0', ']', ']', '\\n']
Tokenized   (051): ['<s>', 'alloc', '=', 'alloc', '.', 'append', '(', 'Data', 'Matrix', '(', 'index', '=', '[', 'historic', '.', 'index', '[', 'date', ']', ']', ',', 'data', '=', '[', 'alloc', '_', 'val', ']', ',', 'columns', '=', '[', 'symbols', '', '~~', 'alloc', '[', ']', '=', '1', '-', 'alloc', '[', 'symbols', '[', '0', ']', ']', '\\', 'n', '</s>']
Filtered   (049): ['alloc', '=', 'alloc', '.', 'append', '(', 'Data', 'Matrix', '(', 'index', '=', '[', 'historic', '.', 'index', '[', 'date', ']', ']', ',', 'data', '=', '[', 'alloc', '_', 'val', ']', ',', 'columns', '=', '[', 'symbols', '', '~~', 'alloc', '[', ']', '=', '1', '-', 'alloc', '[', 'symbols', '[', '0', ']', ']', '\\', 'n']
Detokenized (044): ['alloc', '=', 'alloc', '.', 'append', '(', 'DataMatrix', '(', 'index', '=', '[', 'historic', '.', 'index', '[', 'date', ']', ']', ',', 'data', '=', '[', 'alloc_val', ']', ',', 'columns', '=', '[', 'symbols', '~~', 'alloc', '[', ']', '=', '1', '-', 'alloc', '[', 'symbols', '[', '0', ']', ']', '\\n']
Counter: 49
===================================================================
Hidden states:  (13, 44, 768)
# Extracted words:  44
Sentence         : "output = open ( sys . argv [ 3 ] , "wb" ) \n"
Original    (014): ['output', '=', 'open', '(', 'sys', '.', 'argv', '[', '3', ']', ',', '"wb"', ')', '\\n']
Tokenized   (020): ['<s>', 'output', '=', 'open', '(', 'sys', '.', 'arg', 'v', '[', '3', ']', ',', '"', 'wb', '"', ')', '\\', 'n', '</s>']
Filtered   (018): ['output', '=', 'open', '(', 'sys', '.', 'arg', 'v', '[', '3', ']', ',', '"', 'wb', '"', ')', '\\', 'n']
Detokenized (014): ['output', '=', 'open', '(', 'sys', '.', 'argv', '[', '3', ']', ',', '"wb"', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "stocksAtThisPath = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , stocksAtThisPath \n"
Original    (024): ['stocksAtThisPath', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'fileExtensionToRemove', ')', ')', '[', '0', ']', ')', ',', 'stocksAtThisPath', '\\n']
Tokenized   (037): ['<s>', 'stocks', 'At', 'This', 'Path', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'file', 'Ext', 'ension', 'To', 'Remove', ')', ')', '[', '0', ']', ')', ',', 'stocks', 'At', 'This', 'Path', '\\', 'n', '</s>']
Filtered   (035): ['stocks', 'At', 'This', 'Path', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'file', 'Ext', 'ension', 'To', 'Remove', ')', ')', '[', '0', ']', ')', ',', 'stocks', 'At', 'This', 'Path', '\\', 'n']
Detokenized (024): ['stocksAtThisPath', '=', 'map', '(', 'lambda', 'x', ':', '(', 'x', '.', 'partition', '(', 'str', '(', 'fileExtensionToRemove', ')', ')', '[', '0', ']', ')', ',', 'stocksAtThisPath', '\\n']
Counter: 35
===================================================================
Hidden states:  (13, 24, 768)
# Extracted words:  24
Sentence         : "@ memoize_default ( None , evaluator_is_first_arg = True ) \n"
Original    (010): ['@', 'memoize_default', '(', 'None', ',', 'evaluator_is_first_arg', '=', 'True', ')', '\\n']
Tokenized   (024): ['<s>', '@', 'memo', 'ize', '_', 'default', '(', 'None', ',', 'eval', 'u', 'ator', '_', 'is', '_', 'first', '_', 'arg', '=', 'True', ')', '\\', 'n', '</s>']
Filtered   (022): ['@', 'memo', 'ize', '_', 'default', '(', 'None', ',', 'eval', 'u', 'ator', '_', 'is', '_', 'first', '_', 'arg', '=', 'True', ')', '\\', 'n']
Detokenized (010): ['@', 'memoize_default', '(', 'None', ',', 'evaluator_is_first_arg', '=', 'True', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "param_str = _search_param_in_docstr ( func . raw_doc , str ( param . get_name ( ) ) ) \n"
Original    (018): ['param_str', '=', '_search_param_in_docstr', '(', 'func', '.', 'raw_doc', ',', 'str', '(', 'param', '.', 'get_name', '(', ')', ')', ')', '\\n']
Tokenized   (035): ['<s>', 'param', '_', 'str', '=', '_', 'search', '_', 'param', '_', 'in', '_', 'doc', 'str', '(', 'func', '.', 'raw', '_', 'doc', ',', 'str', '(', 'param', '.', 'get', '_', 'name', '(', ')', ')', ')', '\\', 'n', '</s>']
Filtered   (033): ['param', '_', 'str', '=', '_', 'search', '_', 'param', '_', 'in', '_', 'doc', 'str', '(', 'func', '.', 'raw', '_', 'doc', ',', 'str', '(', 'param', '.', 'get', '_', 'name', '(', ')', ')', ')', '\\', 'n']
Detokenized (018): ['param_str', '=', '_search_param_in_docstr', '(', 'func', '.', 'raw_doc', ',', 'str', '(', 'param', '.', 'get_name', '(', ')', ')', ')', '\\n']
Counter: 33
===================================================================
Hidden states:  (13, 18, 768)
# Extracted words:  18
Sentence         : "patterns = [ re . compile ( p % re . escape ( param_str ) ) \n"
Original    (017): ['patterns', '=', '[', 're', '.', 'compile', '(', 'p', '%', 're', '.', 'escape', '(', 'param_str', ')', ')', '\\n']
Tokenized   (023): ['<s>', 'pattern', 's', '=', '[', 're', '.', 'compile', '(', 'p', '%', 're', '.', 'escape', '(', 'param', '_', 'str', ')', ')', '\\', 'n', '</s>']
Filtered   (021): ['pattern', 's', '=', '[', 're', '.', 'compile', '(', 'p', '%', 're', '.', 'escape', '(', 'param', '_', 'str', ')', ')', '\\', 'n']
Detokenized (017): ['patterns', '=', '[', 're', '.', 'compile', '(', 'p', '%', 're', '.', 'escape', '(', 'param_str', ')', ')', '\\n']
Counter: 21
===================================================================
Hidden states:  (13, 17, 768)
# Extracted words:  17
Sentence         : "it = ( evaluator . execute ( d ) for d in definitions ) \n"
Original    (015): ['it', '=', '(', 'evaluator', '.', 'execute', '(', 'd', ')', 'for', 'd', 'in', 'definitions', ')', '\\n']
Tokenized   (020): ['<s>', 'it', '=', '(', 'eval', 'u', 'ator', '.', 'execute', '(', 'd', ')', 'for', 'd', 'in', 'definitions', ')', '\\', 'n', '</s>']
Filtered   (018): ['it', '=', '(', 'eval', 'u', 'ator', '.', 'execute', '(', 'd', ')', 'for', 'd', 'in', 'definitions', ')', '\\', 'n']
Detokenized (015): ['it', '=', '(', 'evaluator', '.', 'execute', '(', 'd', ')', 'for', 'd', 'in', 'definitions', ')', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 15, 768)
# Extracted words:  15
Sentence         : "tok = parsed . module . subscopes [ 0 ] . statements [ 0 ] . _token_list [ 2 ] \n"
Original    (021): ['tok', '=', 'parsed', '.', 'module', '.', 'subscopes', '[', '0', ']', '.', 'statements', '[', '0', ']', '.', '_token_list', '[', '2', ']', '\\n']
Tokenized   (029): ['<s>', 't', 'ok', '=', 'parsed', '.', 'module', '.', 'subsc', 'opes', '[', '0', ']', '.', 'statements', '[', '0', ']', '.', '_', 'token', '_', 'list', '[', '2', ']', '\\', 'n', '</s>']
Filtered   (027): ['t', 'ok', '=', 'parsed', '.', 'module', '.', 'subsc', 'opes', '[', '0', ']', '.', 'statements', '[', '0', ']', '.', '_', 'token', '_', 'list', '[', '2', ']', '\\', 'n']
Detokenized (021): ['tok', '=', 'parsed', '.', 'module', '.', 'subscopes', '[', '0', ']', '.', 'statements', '[', '0', ']', '.', '_token_list', '[', '2', ']', '\\n']
Counter: 27
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "__slots__ = ( "graphVariable" , \n"
Original    (006): ['__slots__', '=', '(', '"graphVariable"', ',', '\\n']
Tokenized   (015): ['<s>', '__', 'sl', 'ots', '__', '=', '(', '"', 'graph', 'Variable', '"', ',', '\\', 'n', '</s>']
Filtered   (013): ['__', 'sl', 'ots', '__', '=', '(', '"', 'graph', 'Variable', '"', ',', '\\', 'n']
Detokenized (006): ['__slots__', '=', '(', '"graphVariable"', ',', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Sentence         : "( None , \n"
Original    (004): ['(', 'None', ',', '\\n']
Tokenized   (007): ['<s>', '(', 'None', ',', '\\', 'n', '</s>']
Filtered   (005): ['(', 'None', ',', '\\', 'n']
Detokenized (004): ['(', 'None', ',', '\\n']
Counter: 5
===================================================================
Hidden states:  (13, 4, 768)
# Extracted words:  4
Sentence         : "def __init__ ( self , patterns = [ ] , prolog = None ) : \n"
Original    (016): ['def', '__init__', '(', 'self', ',', 'patterns', '=', '[', ']', ',', 'prolog', '=', 'None', ')', ':', '\\n']
Tokenized   (022): ['<s>', 'def', '__', 'init', '__', '(', 'self', ',', 'patterns', '=', '[', ']', ',', 'pro', 'log', '=', 'None', ')', ':', '\\', 'n', '</s>']
Filtered   (020): ['def', '__', 'init', '__', '(', 'self', ',', 'patterns', '=', '[', ']', ',', 'pro', 'log', '=', 'None', ')', ':', '\\', 'n']
Detokenized (016): ['def', '__init__', '(', 'self', ',', 'patterns', '=', '[', ']', ',', 'prolog', '=', 'None', ')', ':', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 16, 768)
# Extracted words:  16
Sentence         : "return term . n3 ( ) \n"
Original    (007): ['return', 'term', '.', 'n3', '(', ')', '\\n']
Tokenized   (011): ['<s>', 'return', 'term', '.', 'n', '3', '(', ')', '\\', 'n', '</s>']
Filtered   (009): ['return', 'term', '.', 'n', '3', '(', ')', '\\', 'n']
Detokenized (007): ['return', 'term', '.', 'n3', '(', ')', '\\n']
Counter: 9
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : ". join ( [ + . join ( [ \n"
Original    (010): ['.', 'join', '(', '[', '+', '.', 'join', '(', '[', '\\n']
Tokenized   (013): ['<s>', '.', 'join', '(', '[', '+', '.', 'join', '(', '[', '\\', 'n', '</s>']
Filtered   (011): ['.', 'join', '(', '[', '+', '.', 'join', '(', '[', '\\', 'n']
Detokenized (010): ['.', 'join', '(', '[', '+', '.', 'join', '(', '[', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "[ ( "a" , "?b" , 24 ) , ( "?r" , "?c" , 12345 ) , ( v1 , "?c" , 3333 ) , ( u1 , "?c" , 9999 ) ] ) \n"
Original    (035): ['[', '(', '"a"', ',', '"?b"', ',', '24', ')', ',', '(', '"?r"', ',', '"?c"', ',', '12345', ')', ',', '(', 'v1', ',', '"?c"', ',', '3333', ')', ',', '(', 'u1', ',', '"?c"', ',', '9999', ')', ']', ')', '\\n']
Tokenized   (060): ['<s>', '[', '(', '"', 'a', '"', ',', '"', '?', 'b', '"', ',', '24', ')', ',', '(', '"', '?', 'r', '"', ',', '"', '?', 'c', '"', ',', '123', '45', ')', ',', '(', 'v', '1', ',', '"', '?', 'c', '"', ',', '3', '333', ')', ',', '(', 'u', '1', ',', '"', '?', 'c', '"', ',', '9', '999', ')', ']', ')', '\\', 'n', '</s>']
Filtered   (058): ['[', '(', '"', 'a', '"', ',', '"', '?', 'b', '"', ',', '24', ')', ',', '(', '"', '?', 'r', '"', ',', '"', '?', 'c', '"', ',', '123', '45', ')', ',', '(', 'v', '1', ',', '"', '?', 'c', '"', ',', '3', '333', ')', ',', '(', 'u', '1', ',', '"', '?', 'c', '"', ',', '9', '999', ')', ']', ')', '\\', 'n']
Detokenized (035): ['[', '(', '"a"', ',', '"?b"', ',', '24', ')', ',', '(', '"?r"', ',', '"?c"', ',', '12345', ')', ',', '(', 'v1', ',', '"?c"', ',', '3333', ')', ',', '(', 'u1', ',', '"?c"', ',', '9999', ')', ']', ')', '\\n']
Counter: 58
===================================================================
Hidden states:  (13, 35, 768)
# Extracted words:  35
Sentence         : "unittest . TextTestRunner ( verbosity = 3 ) . run ( suite ) \n"
Original    (014): ['unittest', '.', 'TextTestRunner', '(', 'verbosity', '=', '3', ')', '.', 'run', '(', 'suite', ')', '\\n']
Tokenized   (022): ['<s>', 'un', 'itt', 'est', '.', 'Text', 'Test', 'Runner', '(', 'verb', 'osity', '=', '3', ')', '.', 'run', '(', 'suite', ')', '\\', 'n', '</s>']
Filtered   (020): ['un', 'itt', 'est', '.', 'Text', 'Test', 'Runner', '(', 'verb', 'osity', '=', '3', ')', '.', 'run', '(', 'suite', ')', '\\', 'n']
Detokenized (014): ['unittest', '.', 'TextTestRunner', '(', 'verbosity', '=', '3', ')', '.', 'run', '(', 'suite', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : ") . parse ( "http://www.w3.org/People/Berners-Lee/card.rdf" ) \n"
Original    (007): [')', '.', 'parse', '(', '"http://www.w3.org/People/Berners-Lee/card.rdf"', ')', '\\n']
Tokenized   (031): ['<s>', ')', '.', 'parse', '(', '"', 'http', '://', 'www', '.', 'w', '3', '.', 'org', '/', 'People', '/', 'Bern', 'ers', '-', 'Lee', '/', 'card', '.', 'rd', 'f', '"', ')', '\\', 'n', '</s>']
Filtered   (029): [')', '.', 'parse', '(', '"', 'http', '://', 'www', '.', 'w', '3', '.', 'org', '/', 'People', '/', 'Bern', 'ers', '-', 'Lee', '/', 'card', '.', 'rd', 'f', '"', ')', '\\', 'n']
Detokenized (007): [')', '.', 'parse', '(', '"http://www.w3.org/People/Berners-Lee/card.rdf"', ')', '\\n']
Counter: 29
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "graph . get_context ( URIRef ( ) \n"
Original    (008): ['graph', '.', 'get_context', '(', 'URIRef', '(', ')', '\\n']
Tokenized   (015): ['<s>', 'graph', '.', 'get', '_', 'context', '(', 'UR', 'IR', 'ef', '(', ')', '\\', 'n', '</s>']
Filtered   (013): ['graph', '.', 'get', '_', 'context', '(', 'UR', 'IR', 'ef', '(', ')', '\\', 'n']
Detokenized (008): ['graph', '.', 'get_context', '(', 'URIRef', '(', ')', '\\n']
Counter: 13
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "bob . set ( FOAF . name , Literal ( "Bob" ) ) \n"
Original    (014): ['bob', '.', 'set', '(', 'FOAF', '.', 'name', ',', 'Literal', '(', '"Bob"', ')', ')', '\\n']
Tokenized   (022): ['<s>', 'b', 'ob', '.', 'set', '(', 'FO', 'AF', '.', 'name', ',', 'Lit', 'eral', '(', '"', 'Bob', '"', ')', ')', '\\', 'n', '</s>']
Filtered   (020): ['b', 'ob', '.', 'set', '(', 'FO', 'AF', '.', 'name', ',', 'Lit', 'eral', '(', '"', 'Bob', '"', ')', ')', '\\', 'n']
Detokenized (014): ['bob', '.', 'set', '(', 'FOAF', '.', 'name', ',', 'Literal', '(', '"Bob"', ')', ')', '\\n']
Counter: 20
===================================================================
Hidden states:  (13, 14, 768)
# Extracted words:  14
Sentence         : "print g . serialize ( format = ) \n"
Original    (009): ['print', 'g', '.', 'serialize', '(', 'format', '=', ')', '\\n']
Tokenized   (013): ['<s>', 'print', 'g', '.', 'serial', 'ize', '(', 'format', '=', ')', '\\', 'n', '</s>']
Filtered   (011): ['print', 'g', '.', 'serial', 'ize', '(', 'format', '=', ')', '\\', 'n']
Detokenized (009): ['print', 'g', '.', 'serialize', '(', 'format', '=', ')', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "context = self . uriref ( ) or self . nodeid ( ) or self . sink . identifier \n"
Original    (020): ['context', '=', 'self', '.', 'uriref', '(', ')', 'or', 'self', '.', 'nodeid', '(', ')', 'or', 'self', '.', 'sink', '.', 'identifier', '\\n']
Tokenized   (026): ['<s>', 'context', '=', 'self', '.', 'ur', 'ire', 'f', '(', ')', 'or', 'self', '.', 'node', 'id', '(', ')', 'or', 'self', '.', 'sink', '.', 'identifier', '\\', 'n', '</s>']
Filtered   (024): ['context', '=', 'self', '.', 'ur', 'ire', 'f', '(', ')', 'or', 'self', '.', 'node', 'id', '(', ')', 'or', 'self', '.', 'sink', '.', 'identifier', '\\', 'n']
Detokenized (020): ['context', '=', 'self', '.', 'uriref', '(', ')', 'or', 'self', '.', 'nodeid', '(', ')', 'or', 'self', '.', 'sink', '.', 'identifier', '\\n']
Counter: 24
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "from rdflib . query import Result , ResultSerializer , ResultParser \n"
Original    (011): ['from', 'rdflib', '.', 'query', 'import', 'Result', ',', 'ResultSerializer', ',', 'ResultParser', '\\n']
Tokenized   (020): ['<s>', 'from', 'r', 'd', 'fl', 'ib', '.', 'query', 'import', 'Result', ',', 'Result', 'Serial', 'izer', ',', 'Result', 'Parser', '\\', 'n', '</s>']
Filtered   (018): ['from', 'r', 'd', 'fl', 'ib', '.', 'query', 'import', 'Result', ',', 'Result', 'Serial', 'izer', ',', 'Result', 'Parser', '\\', 'n']
Detokenized (011): ['from', 'rdflib', '.', 'query', 'import', 'Result', ',', 'ResultSerializer', ',', 'ResultParser', '\\n']
Counter: 18
===================================================================
Hidden states:  (13, 11, 768)
# Extracted words:  11
Sentence         : "class CSVResultParser ( ResultParser ) : \n"
Original    (007): ['class', 'CSVResultParser', '(', 'ResultParser', ')', ':', '\\n']
Tokenized   (013): ['<s>', 'class', 'CSV', 'Result', 'Parser', '(', 'Result', 'Parser', ')', ':', '\\', 'n', '</s>']
Filtered   (011): ['class', 'CSV', 'Result', 'Parser', '(', 'Result', 'Parser', ')', ':', '\\', 'n']
Detokenized (007): ['class', 'CSVResultParser', '(', 'ResultParser', ')', ':', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "r . bindings = [ ] \n"
Original    (007): ['r', '.', 'bindings', '=', '[', ']', '\\n']
Tokenized   (010): ['<s>', 'r', '.', 'bindings', '=', '[', ']', '\\', 'n', '</s>']
Filtered   (008): ['r', '.', 'bindings', '=', '[', ']', '\\', 'n']
Detokenized (007): ['r', '.', 'bindings', '=', '[', ']', '\\n']
Counter: 8
===================================================================
Hidden states:  (13, 7, 768)
# Extracted words:  7
Sentence         : "if result . type != "SELECT" : \n"
Original    (008): ['if', 'result', '.', 'type', '!=', '"SELECT"', ':', '\\n']
Tokenized   (013): ['<s>', 'if', 'result', '.', 'type', '!=', '"', 'SELECT', '"', ':', '\\', 'n', '</s>']
Filtered   (011): ['if', 'result', '.', 'type', '!=', '"', 'SELECT', '"', ':', '\\', 'n']
Detokenized (008): ['if', 'result', '.', 'type', '!=', '"SELECT"', ':', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 8, 768)
# Extracted words:  8
Sentence         : "stream = codecs . getwriter ( encoding ) ( stream ) \n"
Original    (012): ['stream', '=', 'codecs', '.', 'getwriter', '(', 'encoding', ')', '(', 'stream', ')', '\\n']
Tokenized   (017): ['<s>', 'stream', '=', 'codec', 's', '.', 'get', 'writer', '(', 'encoding', ')', '(', 'stream', ')', '\\', 'n', '</s>']
Filtered   (015): ['stream', '=', 'codec', 's', '.', 'get', 'writer', '(', 'encoding', ')', '(', 'stream', ')', '\\', 'n']
Detokenized (012): ['stream', '=', 'codecs', '.', 'getwriter', '(', 'encoding', ')', '(', 'stream', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 12, 768)
# Extracted words:  12
Sentence         : "vs = [ self . serializeTerm ( v , encoding ) for v in self . result . vars ] \n"
Original    (021): ['vs', '=', '[', 'self', '.', 'serializeTerm', '(', 'v', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'vars', ']', '\\n']
Tokenized   (027): ['<s>', 'vs', '=', '[', 'self', '.', 'serial', 'ize', 'Term', '(', 'v', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'v', 'ars', ']', '\\', 'n', '</s>']
Filtered   (025): ['vs', '=', '[', 'self', '.', 'serial', 'ize', 'Term', '(', 'v', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'v', 'ars', ']', '\\', 'n']
Detokenized (021): ['vs', '=', '[', 'self', '.', 'serializeTerm', '(', 'v', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'vars', ']', '\\n']
Counter: 25
===================================================================
Hidden states:  (13, 21, 768)
# Extracted words:  21
Sentence         : "for row in self . result . bindings : \n"
Original    (010): ['for', 'row', 'in', 'self', '.', 'result', '.', 'bindings', ':', '\\n']
Tokenized   (013): ['<s>', 'for', 'row', 'in', 'self', '.', 'result', '.', 'bindings', ':', '\\', 'n', '</s>']
Filtered   (011): ['for', 'row', 'in', 'self', '.', 'result', '.', 'bindings', ':', '\\', 'n']
Detokenized (010): ['for', 'row', 'in', 'self', '.', 'result', '.', 'bindings', ':', '\\n']
Counter: 11
===================================================================
Hidden states:  (13, 10, 768)
# Extracted words:  10
Sentence         : "row . get ( v ) , encoding ) for v in self . result . vars ] ) \n"
Original    (020): ['row', '.', 'get', '(', 'v', ')', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'vars', ']', ')', '\\n']
Tokenized   (024): ['<s>', 'row', '.', 'get', '(', 'v', ')', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'v', 'ars', ']', ')', '\\', 'n', '</s>']
Filtered   (022): ['row', '.', 'get', '(', 'v', ')', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'v', 'ars', ']', ')', '\\', 'n']
Detokenized (020): ['row', '.', 'get', '(', 'v', ')', ',', 'encoding', ')', 'for', 'v', 'in', 'self', '.', 'result', '.', 'vars', ']', ')', '\\n']
Counter: 22
===================================================================
Hidden states:  (13, 20, 768)
# Extracted words:  20
Sentence         : "try : import nose \n"
Original    (005): ['try', ':', 'import', 'nose', '\\n']
Tokenized   (008): ['<s>', 'try', ':', 'import', 'nose', '\\', 'n', '</s>']
Filtered   (006): ['try', ':', 'import', 'nose', '\\', 'n']
Detokenized (005): ['try', ':', 'import', 'nose', '\\n']
Counter: 6
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "~~~ argv += DEFAULT_DIRS \n"
Original    (005): ['~~~', 'argv', '+=', 'DEFAULT_DIRS', '\\n']
Tokenized   (014): ['<s>', '~~', '~', 'arg', 'v', '+=', 'DE', 'FAULT', '_', 'DIR', 'S', '\\', 'n', '</s>']
Filtered   (012): ['~~', '~', 'arg', 'v', '+=', 'DE', 'FAULT', '_', 'DIR', 'S', '\\', 'n']
Detokenized (005): ['~~~', 'argv', '+=', 'DEFAULT_DIRS', '\\n']
Counter: 12
===================================================================
Hidden states:  (13, 5, 768)
# Extracted words:  5
Sentence         : "nose . run_exit ( argv = finalArgs ) \n"
Original    (009): ['nose', '.', 'run_exit', '(', 'argv', '=', 'finalArgs', ')', '\\n']
Tokenized   (017): ['<s>', 'n', 'ose', '.', 'run', '_', 'exit', '(', 'arg', 'v', '=', 'final', 'Args', ')', '\\', 'n', '</s>']
Filtered   (015): ['n', 'ose', '.', 'run', '_', 'exit', '(', 'arg', 'v', '=', 'final', 'Args', ')', '\\', 'n']
Detokenized (009): ['nose', '.', 'run_exit', '(', 'argv', '=', 'finalArgs', ')', '\\n']
Counter: 15
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Loading json activations from bert_temp_activations.json...
1547 13.0
Loading json activations from codebert_temp_activations.json...
1547 13.0
Loading json activations from graphcodebert_temp_activations.json...
1547 13.0
Loading json activations from bert_temp_activations.json...
1547 13.0
Loading json activations from codebert_temp_activations.json...
1547 13.0
Loading json activations from graphcodebert_temp_activations.json...
1547 13.0
Number of tokens:  20509
length of source dictionary:  3730
length of target dictionary:  41
20509
Total instances: 20509
['matcher', 'u_', '"core_tasks"', 'line_len', 'Point2PointService', 'yield', 'fields_by_name', '"{0}|{1}|{2}"', 'v', 'richtext', 'meq', '"c"', '"raw"', '"can"', 'IDroneDService', '"grid.png"', 'replaceUndefined', 'Quantity', 'nout', 'entry_pts']
Number of samples:  20509
Stats: Labels with their frequencies in the final set
NAME 6901
DOT 1726
COMMA 1673
LPAR 1645
EQUAL 1600
RPAR 1593
NEWLINE 1195
NUMBER 715
LSQB 659
RSQB 623
KEYWORD 551
NL 352
STRING 349
COLON 270
PLUS 150
STAR 101
MINUS 94
LBRACE 65
RBRACE 61
SLASH 39
EQEQUAL 34
PERCENT 26
DOUBLESTAR 21
PLUSEQUAL 16
GREATER 9
COMMENT 7
SEMI 6
GREATEREQUAL 4
DEDENT 4
VBAR 3
NOTEQUAL 2
MINEQUAL 2
AMPER 2
TILDE 2
ERRORTOKEN 2
INDENT 2
LESS 1
SLASHEQUAL 1
LEFTSHIFT 1
DOUBLESLASH 1
AT 1
Number of tokens:  20509
length of source dictionary:  3730
length of target dictionary:  41
20509
Total instances: 20509
['matcher', 'u_', '"core_tasks"', 'line_len', 'Point2PointService', 'yield', 'fields_by_name', '"{0}|{1}|{2}"', 'v', 'richtext', 'meq', '"c"', '"raw"', '"can"', 'IDroneDService', '"grid.png"', 'replaceUndefined', 'Quantity', 'nout', 'entry_pts']
Number of samples:  20509
Stats: Labels with their frequencies in the final set
NAME 6901
DOT 1726
COMMA 1673
LPAR 1645
EQUAL 1600
RPAR 1593
NEWLINE 1195
NUMBER 715
LSQB 659
RSQB 623
KEYWORD 551
NL 352
STRING 349
COLON 270
PLUS 150
STAR 101
MINUS 94
LBRACE 65
RBRACE 61
SLASH 39
EQEQUAL 34
PERCENT 26
DOUBLESTAR 21
PLUSEQUAL 16
GREATER 9
COMMENT 7
SEMI 6
GREATEREQUAL 4
DEDENT 4
VBAR 3
NOTEQUAL 2
MINEQUAL 2
AMPER 2
TILDE 2
ERRORTOKEN 2
INDENT 2
LESS 1
SLASHEQUAL 1
LEFTSHIFT 1
DOUBLESLASH 1
AT 1
Number of tokens:  20509
length of source dictionary:  3730
length of target dictionary:  41
20509
Total instances: 20509
['matcher', 'u_', '"core_tasks"', 'line_len', 'Point2PointService', 'yield', 'fields_by_name', '"{0}|{1}|{2}"', 'v', 'richtext', 'meq', '"c"', '"raw"', '"can"', 'IDroneDService', '"grid.png"', 'replaceUndefined', 'Quantity', 'nout', 'entry_pts']
Number of samples:  20509
Stats: Labels with their frequencies in the final set
NAME 6901
DOT 1726
COMMA 1673
LPAR 1645
EQUAL 1600
RPAR 1593
NEWLINE 1195
NUMBER 715
LSQB 659
RSQB 623
KEYWORD 551
NL 352
STRING 349
COLON 270
PLUS 150
STAR 101
MINUS 94
LBRACE 65
RBRACE 61
SLASH 39
EQEQUAL 34
PERCENT 26
DOUBLESTAR 21
PLUSEQUAL 16
GREATER 9
COMMENT 7
SEMI 6
GREATEREQUAL 4
DEDENT 4
VBAR 3
NOTEQUAL 2
MINEQUAL 2
AMPER 2
TILDE 2
ERRORTOKEN 2
INDENT 2
LESS 1
SLASHEQUAL 1
LEFTSHIFT 1
DOUBLESLASH 1
AT 1
distribution:
{0: 6901, 1: 1726, 2: 1673, 3: 1645, 4: 1600, 5: 1593, 6: 1195, 7: 715, 8: 659, 9: 623, 10: 551, 11: 352, 12: 349, 13: 270, 14: 150, 15: 101, 16: 94, 17: 65, 18: 61, 19: 39, 20: 34, 21: 26, 22: 21, 23: 16, 24: 9, 25: 7, 26: 6, 27: 4, 28: 4, 29: 3, 30: 2, 31: 2, 32: 2, 33: 2, 34: 2, 35: 2, 36: 1, 37: 1, 38: 1, 39: 1, 40: 1}
Training classification probe
Creating model...
Number of training instances: 16082
Number of classes: 16
Epoch: [1/10], Loss: 0.0148
Epoch: [2/10], Loss: 0.0146
Epoch: [3/10], Loss: 0.0133
Epoch: [4/10], Loss: 0.0146
Epoch: [5/10], Loss: 0.0137
Epoch: [6/10], Loss: 0.0157
Epoch: [7/10], Loss: 0.0138
Epoch: [8/10], Loss: 0.0117
Epoch: [9/10], Loss: 0.0164
Epoch: [10/10], Loss: 0.0144
Score (accuracy) of the probe: 0.98
Score (accuracy) of the probe: 0.98
Score on the test set: {'__OVERALL__': 0.9781148967918428, 'NAME': 0.9985601151907847, 'DOT': 1.0, 'COMMA': 1.0, 'LPAR': 1.0, 'EQUAL': 1.0, 'RPAR': 0.9941176470588236, 'NEWLINE': 1.0, 'NUMBER': 0.9847328244274809, 'LSQB': 1.0, 'RSQB': 0.9634146341463414, 'KEYWORD': 0.95, 'NL': 0.0, 'STRING': 0.7346938775510204, 'COLON': 1.0, 'PLUS': 1.0, 'STAR': 0.875, 'MINUS': nan, 'LBRACE': nan, 'RBRACE': nan, 'SLASH': nan, 'EQEQUAL': nan, 'PERCENT': nan, 'DOUBLESTAR': nan, 'PLUSEQUAL': nan, 'GREATER': nan, 'COMMENT': nan, 'SEMI': nan, 'GREATEREQUAL': nan, 'DEDENT': nan, 'VBAR': nan, 'NOTEQUAL': nan, 'MINEQUAL': nan, 'AMPER': nan, 'TILDE': nan, 'ERRORTOKEN': nan, 'INDENT': nan, 'LESS': nan, 'SLASHEQUAL': nan, 'LEFTSHIFT': nan, 'DOUBLESLASH': nan, 'AT': nan}
Score (accuracy) of the probe: 0.98
Score on the training set: {'__OVERALL__': 0.9750031090660366, 'NAME': 0.9996371552975326, 'DOT': 0.9992647058823529, 'COMMA': 1.0, 'LPAR': 1.0, 'EQUAL': 1.0, 'RPAR': 0.9976057462090981, 'NEWLINE': 1.0, 'NUMBER': 1.0, 'LSQB': 1.0, 'RSQB': 0.988909426987061, 'KEYWORD': 0.9919137466307277, 'NL': 0.006756756756756757, 'STRING': 0.71, 'COLON': 1.0, 'PLUS': 1.0, 'STAR': 0.9354838709677419, 'MINUS': nan, 'LBRACE': nan, 'RBRACE': nan, 'SLASH': nan, 'EQEQUAL': nan, 'PERCENT': nan, 'DOUBLESTAR': nan, 'PLUSEQUAL': nan, 'GREATER': nan, 'COMMENT': nan, 'SEMI': nan, 'GREATEREQUAL': nan, 'DEDENT': nan, 'VBAR': nan, 'NOTEQUAL': nan, 'MINEQUAL': nan, 'AMPER': nan, 'TILDE': nan, 'ERRORTOKEN': nan, 'INDENT': nan, 'LESS': nan, 'SLASHEQUAL': nan, 'LEFTSHIFT': nan, 'DOUBLESLASH': nan, 'AT': nan}
