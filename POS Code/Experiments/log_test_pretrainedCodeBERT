Getting activations from json files. If you need to extract them, run with --extract=True 

Anayzing pretrained_CodeBERT
Loading json activations from codebert_activations_train.json...
7656 13.0
Number of tokens:  72951
length of source dictionary:  5099
length of target dictionary:  42
72951
Total instances: 72951
['date', 'importRTs', 'RegisterHandler', 'remove_cover', 'test_no_acls', 'yready', 'logdepth_str', 'deepcopy', '21', '"m_MaxParticleSize"', 'close', '_createHeaders', '_send_request', 'struct', 'setdistrictlock', 'DBG_NODEBUILDER', '"*.h"', 'Stretch', 'include_package_data', 'RegisterHandlerOnce']
Number of samples:  72951
Stats: Labels with their frequencies in the final set
NAME 23204
NEWLINE 6586
DOT 5889
LPAR 5444
RPAR 5263
KEYWORD 5018
COMMA 4364
EQUAL 3897
COLON 2622
DEDENT 1878
INDENT 1593
NUMBER 1356
LSQB 1291
RSQB 1267
NL 1069
STRING 550
LBRACE 293
EQEQUAL 226
RBRACE 211
PLUS 200
PERCENT 109
STAR 94
MINUS 83
AT 61
DOUBLESTAR 60
GREATER 59
PLUSEQUAL 52
NOTEQUAL 44
LEFTSHIFT 31
LESS 29
LESSEQUAL 18
COMMENT 15
GREATEREQUAL 15
SEMI 13
VBAR 12
SLASH 11
TILDE 7
ELLIPSIS 6
AMPER 5
MINEQUAL 3
ERRORTOKEN 2
RIGHTSHIFT 1
pretrained_CodeBERT distribution after trauncating:
{0: 0.7701805629314923, 3: 0.16655602761550717, 2: 0.045007966011683484, 1: 0.01825544344131705}
{0: 23204, 3: 5018, 2: 1356, 1: 550}
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}
Loading json activations from codebert_activations_test.json...
5985 13.0
Number of tokens:  64699
length of source dictionary:  3727
length of target dictionary:  42
64699
Total instances: 64699
['DllWithError', '_g_check_not_contains', 'date', 'b1_h', 'AlphaDataModelClass', 'import_neuroshare_segment', 'TestElemwiseMergeLayerMaximum', 'memoize_default', 'feature_shape', 'callable', 'insert', 'title', '0.8', 'dangling', 'eye', 'train', 'locked', 'container_children', 'pnames', 'Bd']
Number of samples:  64699
Stats: Labels with their frequencies in the final set
NAME 20893
NEWLINE 5444
DOT 5083
COMMA 4481
RPAR 4279
LPAR 4277
KEYWORD 3639
EQUAL 3459
NUMBER 2218
COLON 1972
LSQB 1939
RSQB 1927
DEDENT 1387
INDENT 1203
NL 540
STRING 324
MINUS 262
PLUS 248
STAR 236
EQEQUAL 141
LBRACE 128
RBRACE 126
SLASH 122
PERCENT 69
DOUBLESTAR 55
PLUSEQUAL 50
GREATER 47
NOTEQUAL 32
COMMENT 22
LESS 20
STAREQUAL 16
GREATEREQUAL 16
AT 10
MINEQUAL 7
SEMI 7
VBAR 6
LESSEQUAL 5
AMPER 3
SLASHEQUAL 2
DOUBLESLASH 2
TILDE 1
LEFTSHIFT 1
pretrained_CodeBERT distribution after trauncating:
{0: 0.7716997857723277, 3: 0.134409396468937, 2: 0.08192361675408141, 1: 0.011967201004653911}
{0: 20893, 3: 3639, 2: 2218, 1: 324}
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}

The distribution of classes in training after removing repeated tokens between training and tesing:
Counter({0: 13506, 3: 1019, 1: 491, 2: 136})
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}
The distribution of classes in testing:
Counter({2: 2039, 0: 1882, 3: 68, 1: 37})
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}

The shape of the training set: (13636, 9984)
The shape of the validation set: (1516, 9984)
The shape of the testing set: (4026, 9984)
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0065
Epoch: [2/10], Loss: 0.0010
Epoch: [3/10], Loss: 0.0002
Epoch: [4/10], Loss: 0.0001
Epoch: [5/10], Loss: 0.0001
Epoch: [6/10], Loss: 0.0001
Epoch: [7/10], Loss: 0.0001
Epoch: [8/10], Loss: 0.0001
Epoch: [9/10], Loss: 0.0001
Epoch: [10/10], Loss: 0.0001
Score (accuracy) of the probe: 0.99
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0066
Epoch: [2/10], Loss: 0.0012
Epoch: [3/10], Loss: 0.0003
Epoch: [4/10], Loss: 0.0002
Epoch: [5/10], Loss: 0.0001
Epoch: [6/10], Loss: 0.0001
Epoch: [7/10], Loss: 0.0001
Epoch: [8/10], Loss: 0.0001
Epoch: [9/10], Loss: 0.0001
Epoch: [10/10], Loss: 0.0001
Score (accuracy) of the probe: 0.99
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0067
Epoch: [2/10], Loss: 0.0014
Epoch: [3/10], Loss: 0.0004
Epoch: [4/10], Loss: 0.0004
Epoch: [5/10], Loss: 0.0003
Epoch: [6/10], Loss: 0.0003
Epoch: [7/10], Loss: 0.0003
Epoch: [8/10], Loss: 0.0003
Epoch: [9/10], Loss: 0.0003
Epoch: [10/10], Loss: 0.0002
Score (accuracy) of the probe: 0.99
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0081
Epoch: [2/10], Loss: 0.0029
Epoch: [3/10], Loss: 0.0019
Epoch: [4/10], Loss: 0.0016
Epoch: [5/10], Loss: 0.0014
Epoch: [6/10], Loss: 0.0014
Epoch: [7/10], Loss: 0.0014
Epoch: [8/10], Loss: 0.0014
Epoch: [9/10], Loss: 0.0014
Epoch: [10/10], Loss: 0.0014
Score (accuracy) of the probe: 0.99

The best l1=0, the best l2=0 for pretrained_CodeBERT
Accuracy on the test set of probing pretrained_CodeBERT of all layers:
Score (accuracy) of the probe: 0.98
{'__OVERALL__': 0.9754098360655737, 'NAME': 0.9665249734325186, 'STRING': 1.0, 'NUMBER': 0.982834722903384, 'KEYWORD': 0.9852941176470589}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1819,KW_NAME:0
NAME_KW:32,KW_KW:67
NAME_STRING:14,KW_other:1
NAME_NUMBER:17
NAME_STRING_list:['T', 'T', 'T0', 'T', 'i', 'T', 'i', 'i', 'T', 'T', 'T', 'T', 'h5f', 'h5f']
NAME_NUMBER_list:['T', 'N', 'T', 'T0', 'T0', 'T0', 'T0', 'T', 'T', 'p0', 'q0', 'T', 'T', 'T', 'T', 'T', 'a0']
Accuracy on the test set of pretrained_CodeBERT model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_CodeBERT Layer 0
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0094
Epoch: [2/10], Loss: 0.0057
Epoch: [3/10], Loss: 0.0040
Epoch: [4/10], Loss: 0.0031
Epoch: [5/10], Loss: 0.0025
Epoch: [6/10], Loss: 0.0021
Epoch: [7/10], Loss: 0.0018
Epoch: [8/10], Loss: 0.0016
Epoch: [9/10], Loss: 0.0014
Epoch: [10/10], Loss: 0.0012
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0094
Epoch: [2/10], Loss: 0.0057
Epoch: [3/10], Loss: 0.0040
Epoch: [4/10], Loss: 0.0031
Epoch: [5/10], Loss: 0.0025
Epoch: [6/10], Loss: 0.0021
Epoch: [7/10], Loss: 0.0018
Epoch: [8/10], Loss: 0.0016
Epoch: [9/10], Loss: 0.0014
Epoch: [10/10], Loss: 0.0013
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0094
Epoch: [2/10], Loss: 0.0057
Epoch: [3/10], Loss: 0.0041
Epoch: [4/10], Loss: 0.0033
Epoch: [5/10], Loss: 0.0027
Epoch: [6/10], Loss: 0.0023
Epoch: [7/10], Loss: 0.0020
Epoch: [8/10], Loss: 0.0018
Epoch: [9/10], Loss: 0.0016
Epoch: [10/10], Loss: 0.0015
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0103
Epoch: [2/10], Loss: 0.0069
Epoch: [3/10], Loss: 0.0054
Epoch: [4/10], Loss: 0.0047
Epoch: [5/10], Loss: 0.0042
Epoch: [6/10], Loss: 0.0039
Epoch: [7/10], Loss: 0.0036
Epoch: [8/10], Loss: 0.0034
Epoch: [9/10], Loss: 0.0032
Epoch: [10/10], Loss: 0.0030
Score (accuracy) of the probe: 0.97

The best l1=0, the best l2=0.1 for pretrained_CodeBERT_layer_0
Accuracy on the test set of probing pretrained_CodeBERT_layer_0 of all layers:
Score (accuracy) of the probe: 0.93
{'__OVERALL__': 0.9334326875310481, 'NAME': 0.871413390010627, 'STRING': 1.0, 'NUMBER': 0.9911721432074546, 'KEYWORD': 0.8823529411764706}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1640,KW_NAME:8
NAME_KW:92,KW_KW:60
NAME_STRING:0,KW_other:0
NAME_NUMBER:150
NAME_STRING_list:[]
NAME_NUMBER_list:['X', 'X', 'X', 'X', 'X', 'X', 'h0', 'h0', 'h0', 'X', 'X', 't0', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'f2', 'X', 'X', 'f2', 'X', 'N', 'X', 'X', 'N', 'X', 'T0', 'T0', 'T0', 't0', 't0', 't0', 'T0', 'T0', 'T0', 't0', 't0', 't0', 'T0', 'T0', 'T0', 't0', 't0', 't0', 'T0', 't0', 't0', 'T0', 't0', 't0', 'X', 'X', 'p0', 'p0', 'p0', 'p0', 'X', 'p0', 'p0', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'f0', 'X', 'X', 'X', 'X', 'X', 'X', 'f0', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'X', 'a0', 'a0', 'a2', 'a3', 'h5f', 'h5f', 'h5f', 'h5f', 'h5f', 'h5f']
Accuracy on the test set of pretrained_CodeBERT_layer_0 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_CodeBERT Layer 1
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0093
Epoch: [2/10], Loss: 0.0052
Epoch: [3/10], Loss: 0.0035
Epoch: [4/10], Loss: 0.0027
Epoch: [5/10], Loss: 0.0021
Epoch: [6/10], Loss: 0.0018
Epoch: [7/10], Loss: 0.0015
Epoch: [8/10], Loss: 0.0013
Epoch: [9/10], Loss: 0.0011
Epoch: [10/10], Loss: 0.0010
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0093
Epoch: [2/10], Loss: 0.0053
Epoch: [3/10], Loss: 0.0036
Epoch: [4/10], Loss: 0.0027
Epoch: [5/10], Loss: 0.0022
Epoch: [6/10], Loss: 0.0018
Epoch: [7/10], Loss: 0.0015
Epoch: [8/10], Loss: 0.0013
Epoch: [9/10], Loss: 0.0012
Epoch: [10/10], Loss: 0.0010
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0091
Epoch: [2/10], Loss: 0.0052
Epoch: [3/10], Loss: 0.0036
Epoch: [4/10], Loss: 0.0028
Epoch: [5/10], Loss: 0.0023
Epoch: [6/10], Loss: 0.0019
Epoch: [7/10], Loss: 0.0017
Epoch: [8/10], Loss: 0.0015
Epoch: [9/10], Loss: 0.0014
Epoch: [10/10], Loss: 0.0012
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0103
Epoch: [2/10], Loss: 0.0065
Epoch: [3/10], Loss: 0.0050
Epoch: [4/10], Loss: 0.0043
Epoch: [5/10], Loss: 0.0039
Epoch: [6/10], Loss: 0.0036
Epoch: [7/10], Loss: 0.0033
Epoch: [8/10], Loss: 0.0031
Epoch: [9/10], Loss: 0.0030
Epoch: [10/10], Loss: 0.0028
Score (accuracy) of the probe: 0.99

The best l1=0, the best l2=0.1 for pretrained_CodeBERT_layer_1
Accuracy on the test set of probing pretrained_CodeBERT_layer_1 of all layers:
Score (accuracy) of the probe: 0.93
{'__OVERALL__': 0.9339294585196225, 'NAME': 0.8777895855472901, 'STRING': 1.0, 'NUMBER': 0.982834722903384, 'KEYWORD': 0.9852941176470589}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1652,KW_NAME:0
NAME_KW:160,KW_KW:67
NAME_STRING:26,KW_other:1
NAME_NUMBER:44
NAME_STRING_list:['a', 'T', 'k', 'a', 'k', 'f', 'k', 'k', 'k', 'k', 'k', 'k', 'Y1', 'u', 'u', 'u', 'u', 'u', 'u', 'u', 'p', 'u', 'u', 'k', 'k', 'k']
NAME_NUMBER_list:['h0', 'h0', 'h0', 't0', 't0', 'f2', 'f2', 'T0', 'T0', 'T0', 't0', 't0', 't0', 'T0', 'T0', 'T0', 't0', 't0', 't0', 'T0', 'T0', 'T0', 't0', 't0', 't0', 'T0', 't0', 't0', 'T0', 't0', 't0', 'p0', 'p0', 'p0', 'p0', 'p0', 'q0', 'p0', 'q0', 'f0', 'f0', 'a3', 'a0', 'a0']
Accuracy on the test set of pretrained_CodeBERT_layer_1 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_CodeBERT Layer 2
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0084
Epoch: [2/10], Loss: 0.0044
Epoch: [3/10], Loss: 0.0029
Epoch: [4/10], Loss: 0.0021
Epoch: [5/10], Loss: 0.0017
Epoch: [6/10], Loss: 0.0014
Epoch: [7/10], Loss: 0.0012
Epoch: [8/10], Loss: 0.0010
Epoch: [9/10], Loss: 0.0009
Epoch: [10/10], Loss: 0.0008
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0085
Epoch: [2/10], Loss: 0.0044
Epoch: [3/10], Loss: 0.0029
Epoch: [4/10], Loss: 0.0021
Epoch: [5/10], Loss: 0.0017
Epoch: [6/10], Loss: 0.0014
Epoch: [7/10], Loss: 0.0012
Epoch: [8/10], Loss: 0.0010
Epoch: [9/10], Loss: 0.0009
Epoch: [10/10], Loss: 0.0008
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0087
Epoch: [2/10], Loss: 0.0046
Epoch: [3/10], Loss: 0.0031
Epoch: [4/10], Loss: 0.0023
Epoch: [5/10], Loss: 0.0019
Epoch: [6/10], Loss: 0.0016
Epoch: [7/10], Loss: 0.0014
Epoch: [8/10], Loss: 0.0013
Epoch: [9/10], Loss: 0.0011
Epoch: [10/10], Loss: 0.0011
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0094
Epoch: [2/10], Loss: 0.0057
Epoch: [3/10], Loss: 0.0044
Epoch: [4/10], Loss: 0.0038
Epoch: [5/10], Loss: 0.0034
Epoch: [6/10], Loss: 0.0032
Epoch: [7/10], Loss: 0.0030
Epoch: [8/10], Loss: 0.0029
Epoch: [9/10], Loss: 0.0027
Epoch: [10/10], Loss: 0.0026
Score (accuracy) of the probe: 0.99

The best l1=0, the best l2=0.1 for pretrained_CodeBERT_layer_2
Accuracy on the test set of probing pretrained_CodeBERT_layer_2 of all layers:
Score (accuracy) of the probe: 0.88
{'__OVERALL__': 0.881023348236463, 'NAME': 0.7879914984059511, 'STRING': 1.0, 'NUMBER': 0.9607650809220206, 'KEYWORD': 1.0}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1483,KW_NAME:0
NAME_KW:310,KW_KW:68
NAME_STRING:11,KW_other:0
NAME_NUMBER:78
NAME_STRING_list:['W1', 'b1', 'a', 's', 's', 't0', 'q1', 'u', 's', 'f', 'a0']
NAME_NUMBER_list:['h0', 'h0', 'X', 'i', 't0', 'i', 'i', 'Y1', 'i', 'f1', 'X', 'T0', 'T0', 'T0', 't0', 't0', 'T0', 'T0', 'T0', 't0', 't0', 't0', 'T0', 'T0', 'T0', 't0', 't0', 't0', 'T0', 't0', 't0', 'T0', 't0', 'p0', 'q0', 'p0', 'X', 'X', 'X', 'X', 'X', 'X', 's', 'i', 'X', 'X', 'f0', 'X', 'i', 'i', 'f0', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'i', 'j1', 'j1', 'j1', 'j1', 'j1', 'a0', 'a2', 'i', 'i', 'A', 'A']
Accuracy on the test set of pretrained_CodeBERT_layer_2 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_CodeBERT Layer 3
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0089
Epoch: [2/10], Loss: 0.0047
Epoch: [3/10], Loss: 0.0031
Epoch: [4/10], Loss: 0.0023
Epoch: [5/10], Loss: 0.0018
Epoch: [6/10], Loss: 0.0015
Epoch: [7/10], Loss: 0.0012
Epoch: [8/10], Loss: 0.0011
Epoch: [9/10], Loss: 0.0009
Epoch: [10/10], Loss: 0.0008
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0089
Epoch: [2/10], Loss: 0.0048
Epoch: [3/10], Loss: 0.0032
Epoch: [4/10], Loss: 0.0023
Epoch: [5/10], Loss: 0.0018
Epoch: [6/10], Loss: 0.0015
Epoch: [7/10], Loss: 0.0013
Epoch: [8/10], Loss: 0.0011
Epoch: [9/10], Loss: 0.0010
Epoch: [10/10], Loss: 0.0008
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0088
Epoch: [2/10], Loss: 0.0048
Epoch: [3/10], Loss: 0.0032
Epoch: [4/10], Loss: 0.0025
Epoch: [5/10], Loss: 0.0020
Epoch: [6/10], Loss: 0.0017
Epoch: [7/10], Loss: 0.0014
Epoch: [8/10], Loss: 0.0013
Epoch: [9/10], Loss: 0.0012
Epoch: [10/10], Loss: 0.0011
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0095
Epoch: [2/10], Loss: 0.0058
Epoch: [3/10], Loss: 0.0045
Epoch: [4/10], Loss: 0.0039
Epoch: [5/10], Loss: 0.0035
Epoch: [6/10], Loss: 0.0032
Epoch: [7/10], Loss: 0.0030
Epoch: [8/10], Loss: 0.0029
Epoch: [9/10], Loss: 0.0027
Epoch: [10/10], Loss: 0.0026
Score (accuracy) of the probe: 0.98

The best l1=0, the best l2=0.1 for pretrained_CodeBERT_layer_3
Accuracy on the test set of probing pretrained_CodeBERT_layer_3 of all layers:
Score (accuracy) of the probe: 0.94
{'__OVERALL__': 0.944113263785395, 'NAME': 0.9404888416578109, 'STRING': 1.0, 'NUMBER': 0.9455615497793036, 'KEYWORD': 0.9705882352941176}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1770,KW_NAME:1
NAME_KW:63,KW_KW:66
NAME_STRING:20,KW_other:1
NAME_NUMBER:29
NAME_STRING_list:['T', 'x', 't0', 'k', 'f', 'x', 'T0', 'T0', 'T0', 'T', 'p0', 'p0', 'p0', 'p0', 'q0', 'U', 'x', 'f0', 'a0', '_']
NAME_NUMBER_list:['t0', 'x', 'T0', 'T0', 't0', 't0', 'T0', 'T0', 't0', 't0', 't0', 'T0', 't0', 't0', 't0', 'T0', 't0', 't0', 'T0', 't0', 't0', 'x', 'p0', 'T', 'i', 'x', 'g', 'h5f', 'h5f']
Accuracy on the test set of pretrained_CodeBERT_layer_3 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_CodeBERT Layer 4
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0086
Epoch: [2/10], Loss: 0.0045
Epoch: [3/10], Loss: 0.0029
Epoch: [4/10], Loss: 0.0021
Epoch: [5/10], Loss: 0.0016
Epoch: [6/10], Loss: 0.0013
Epoch: [7/10], Loss: 0.0011
Epoch: [8/10], Loss: 0.0009
Epoch: [9/10], Loss: 0.0008
Epoch: [10/10], Loss: 0.0007
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0085
Epoch: [2/10], Loss: 0.0044
Epoch: [3/10], Loss: 0.0029
Epoch: [4/10], Loss: 0.0021
Epoch: [5/10], Loss: 0.0016
Epoch: [6/10], Loss: 0.0013
Epoch: [7/10], Loss: 0.0011
Epoch: [8/10], Loss: 0.0009
Epoch: [9/10], Loss: 0.0008
Epoch: [10/10], Loss: 0.0007
Score (accuracy) of the probe: 0.99
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0088
Epoch: [2/10], Loss: 0.0046
Epoch: [3/10], Loss: 0.0031
Epoch: [4/10], Loss: 0.0023
Epoch: [5/10], Loss: 0.0018
Epoch: [6/10], Loss: 0.0015
Epoch: [7/10], Loss: 0.0013
Epoch: [8/10], Loss: 0.0012
Epoch: [9/10], Loss: 0.0011
Epoch: [10/10], Loss: 0.0010
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0094
Epoch: [2/10], Loss: 0.0056
Epoch: [3/10], Loss: 0.0043
Epoch: [4/10], Loss: 0.0037
Epoch: [5/10], Loss: 0.0033
Epoch: [6/10], Loss: 0.0031
Epoch: [7/10], Loss: 0.0029
Epoch: [8/10], Loss: 0.0028
Epoch: [9/10], Loss: 0.0027
Epoch: [10/10], Loss: 0.0026
Score (accuracy) of the probe: 0.99

The best l1=0, the best l2=0.1 for pretrained_CodeBERT_layer_4
Accuracy on the test set of probing pretrained_CodeBERT_layer_4 of all layers:
Score (accuracy) of the probe: 0.93
{'__OVERALL__': 0.9316939890710383, 'NAME': 0.9282678002125399, 'STRING': 0.972972972972973, 'NUMBER': 0.9318293281020108, 'KEYWORD': 1.0}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1747,KW_NAME:0
NAME_KW:95,KW_KW:68
NAME_STRING:35,KW_other:0
NAME_NUMBER:5
NAME_STRING_list:['q', 'T', 'T', 'T', 'T0', 't0', 'T', 'T0', 'T', 'T0', 'T', 't0', 'T', 'T', 'p0', 'q0', 'D', 'D', 'D', 'T', 'T', 'T', 'T', 'T', 'i', 'T', 'T', 'T', 'T', 'i', 'V', 'T', 'T', '_', 'Q']
NAME_NUMBER_list:['T0', 'T0', 'T0', 'R', 'i']
Accuracy on the test set of pretrained_CodeBERT_layer_4 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_CodeBERT Layer 5
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0088
Epoch: [2/10], Loss: 0.0049
Epoch: [3/10], Loss: 0.0033
Epoch: [4/10], Loss: 0.0024
Epoch: [5/10], Loss: 0.0019
Epoch: [6/10], Loss: 0.0015
Epoch: [7/10], Loss: 0.0013
Epoch: [8/10], Loss: 0.0011
Epoch: [9/10], Loss: 0.0009
Epoch: [10/10], Loss: 0.0008
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0089
Epoch: [2/10], Loss: 0.0049
Epoch: [3/10], Loss: 0.0033
Epoch: [4/10], Loss: 0.0025
Epoch: [5/10], Loss: 0.0019
Epoch: [6/10], Loss: 0.0016
Epoch: [7/10], Loss: 0.0013
Epoch: [8/10], Loss: 0.0011
Epoch: [9/10], Loss: 0.0010
Epoch: [10/10], Loss: 0.0008
Score (accuracy) of the probe: 0.99
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0089
Epoch: [2/10], Loss: 0.0051
Epoch: [3/10], Loss: 0.0035
Epoch: [4/10], Loss: 0.0026
Epoch: [5/10], Loss: 0.0021
Epoch: [6/10], Loss: 0.0018
Epoch: [7/10], Loss: 0.0015
Epoch: [8/10], Loss: 0.0013
Epoch: [9/10], Loss: 0.0012
Epoch: [10/10], Loss: 0.0011
Score (accuracy) of the probe: 0.99
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0093
Epoch: [2/10], Loss: 0.0059
Epoch: [3/10], Loss: 0.0047
Epoch: [4/10], Loss: 0.0040
Epoch: [5/10], Loss: 0.0036
Epoch: [6/10], Loss: 0.0033
Epoch: [7/10], Loss: 0.0031
Epoch: [8/10], Loss: 0.0029
Epoch: [9/10], Loss: 0.0028
Epoch: [10/10], Loss: 0.0027
Score (accuracy) of the probe: 0.99

The best l1=0, the best l2=0.1 for pretrained_CodeBERT_layer_5
Accuracy on the test set of probing pretrained_CodeBERT_layer_5 of all layers:
Score (accuracy) of the probe: 0.93
{'__OVERALL__': 0.9321907600596125, 'NAME': 0.9213602550478215, 'STRING': 0.8918918918918919, 'NUMBER': 0.9406571848945562, 'KEYWORD': 1.0}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1734,KW_NAME:0
NAME_KW:123,KW_KW:68
NAME_STRING:12,KW_other:0
NAME_NUMBER:13
NAME_STRING_list:['x', 'f', 'x', 'x', 'x', 'x', 'D', 'D', 'D', 'x', 'V', '_']
NAME_NUMBER_list:['m', 'i', 'i', '_', '_', 'D', 'R', '_', 's', 'R', 'R', 'R', 'R']
Accuracy on the test set of pretrained_CodeBERT_layer_5 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_CodeBERT Layer 6
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0087
Epoch: [2/10], Loss: 0.0048
Epoch: [3/10], Loss: 0.0033
Epoch: [4/10], Loss: 0.0024
Epoch: [5/10], Loss: 0.0019
Epoch: [6/10], Loss: 0.0016
Epoch: [7/10], Loss: 0.0013
Epoch: [8/10], Loss: 0.0011
Epoch: [9/10], Loss: 0.0010
Epoch: [10/10], Loss: 0.0009
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0086
Epoch: [2/10], Loss: 0.0047
Epoch: [3/10], Loss: 0.0032
Epoch: [4/10], Loss: 0.0024
Epoch: [5/10], Loss: 0.0019
Epoch: [6/10], Loss: 0.0015
Epoch: [7/10], Loss: 0.0013
Epoch: [8/10], Loss: 0.0011
Epoch: [9/10], Loss: 0.0010
Epoch: [10/10], Loss: 0.0009
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0091
Epoch: [2/10], Loss: 0.0051
Epoch: [3/10], Loss: 0.0035
Epoch: [4/10], Loss: 0.0027
Epoch: [5/10], Loss: 0.0021
Epoch: [6/10], Loss: 0.0018
Epoch: [7/10], Loss: 0.0016
Epoch: [8/10], Loss: 0.0014
Epoch: [9/10], Loss: 0.0012
Epoch: [10/10], Loss: 0.0011
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0097
Epoch: [2/10], Loss: 0.0061
Epoch: [3/10], Loss: 0.0047
Epoch: [4/10], Loss: 0.0040
Epoch: [5/10], Loss: 0.0036
Epoch: [6/10], Loss: 0.0034
Epoch: [7/10], Loss: 0.0031
Epoch: [8/10], Loss: 0.0030
Epoch: [9/10], Loss: 0.0028
Epoch: [10/10], Loss: 0.0027
Score (accuracy) of the probe: 0.99

The best l1=0, the best l2=0.1 for pretrained_CodeBERT_layer_6
Accuracy on the test set of probing pretrained_CodeBERT_layer_6 of all layers:
Score (accuracy) of the probe: 0.89
{'__OVERALL__': 0.8874813710879285, 'NAME': 0.910201912858661, 'STRING': 1.0, 'NUMBER': 0.8607160372731731, 'KEYWORD': 1.0}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1713,KW_NAME:0
NAME_KW:98,KW_KW:68
NAME_STRING:52,KW_other:0
NAME_NUMBER:19
NAME_STRING_list:['x', 'h', 'b', 'h', 'b', 'h', 'b', 'h', 'b', 'h', 'b', 'h', 'b', 'x', 'i', 'i', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'T', 'T', 'T', 'T', 'T', 'T', 'p0', 'q0', 'T', 'f', 'T', 'T', 'T', 'p', 'y', 'x', 'a2', 'a3', 'i', 'd']
NAME_NUMBER_list:['T', 'T', 'T', 'D', 'T', 'T', 'T', 'i', 'u', 'u', 'i', 'T', 'T', 'T', 'T', 'T', 'T', 'a3', 'i']
Accuracy on the test set of pretrained_CodeBERT_layer_6 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_CodeBERT Layer 7
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0091
Epoch: [2/10], Loss: 0.0053
Epoch: [3/10], Loss: 0.0037
Epoch: [4/10], Loss: 0.0028
Epoch: [5/10], Loss: 0.0022
Epoch: [6/10], Loss: 0.0018
Epoch: [7/10], Loss: 0.0015
Epoch: [8/10], Loss: 0.0013
Epoch: [9/10], Loss: 0.0011
Epoch: [10/10], Loss: 0.0010
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0090
Epoch: [2/10], Loss: 0.0054
Epoch: [3/10], Loss: 0.0037
Epoch: [4/10], Loss: 0.0028
Epoch: [5/10], Loss: 0.0022
Epoch: [6/10], Loss: 0.0018
Epoch: [7/10], Loss: 0.0015
Epoch: [8/10], Loss: 0.0013
Epoch: [9/10], Loss: 0.0011
Epoch: [10/10], Loss: 0.0010
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0091
Epoch: [2/10], Loss: 0.0054
Epoch: [3/10], Loss: 0.0039
Epoch: [4/10], Loss: 0.0030
Epoch: [5/10], Loss: 0.0024
Epoch: [6/10], Loss: 0.0020
Epoch: [7/10], Loss: 0.0017
Epoch: [8/10], Loss: 0.0015
Epoch: [9/10], Loss: 0.0014
Epoch: [10/10], Loss: 0.0012
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0100
Epoch: [2/10], Loss: 0.0065
Epoch: [3/10], Loss: 0.0051
Epoch: [4/10], Loss: 0.0044
Epoch: [5/10], Loss: 0.0039
Epoch: [6/10], Loss: 0.0036
Epoch: [7/10], Loss: 0.0034
Epoch: [8/10], Loss: 0.0032
Epoch: [9/10], Loss: 0.0030
Epoch: [10/10], Loss: 0.0029
Score (accuracy) of the probe: 0.99

The best l1=0, the best l2=0.1 for pretrained_CodeBERT_layer_7
Accuracy on the test set of probing pretrained_CodeBERT_layer_7 of all layers:
Score (accuracy) of the probe: 0.92
{'__OVERALL__': 0.920019870839543, 'NAME': 0.8916046758767269, 'STRING': 0.972972972972973, 'NUMBER': 0.9445806768023541, 'KEYWORD': 0.9411764705882353}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1678,KW_NAME:0
NAME_KW:130,KW_KW:64
NAME_STRING:48,KW_other:4
NAME_NUMBER:26
NAME_STRING_list:['_', 'b', 'h', 'h', 'p', 'T', 'T', 'i', 'i', 'x', 'd', 'k2e', 'i', 'f1', 'T', '_', '_', 'T', 'T', 'T0', 'T0', 'T', 'p0', 'q0', 'B', 'c', 'T', 'T', 'N', 'K', 'v', '_', 'x', 'T', 'T', 'T', 'i', 'V', 'T', 'b', 'a3', 'a0', 'a3', 'i', 'i', '_', 'd', 'f']
NAME_NUMBER_list:['m', 'm', 'm', 'K', 'K', 'D', 'D', 'K', 'D', 'N', 'c', 'n', 'N', 'n', 'D', 'C', 'D', 'K', 'D', 'K', 'D', 'K', 'N', 's', 'u', 'P']
Accuracy on the test set of pretrained_CodeBERT_layer_7 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_CodeBERT Layer 8
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0086
Epoch: [2/10], Loss: 0.0046
Epoch: [3/10], Loss: 0.0031
Epoch: [4/10], Loss: 0.0023
Epoch: [5/10], Loss: 0.0018
Epoch: [6/10], Loss: 0.0014
Epoch: [7/10], Loss: 0.0012
Epoch: [8/10], Loss: 0.0010
Epoch: [9/10], Loss: 0.0009
Epoch: [10/10], Loss: 0.0008
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0085
Epoch: [2/10], Loss: 0.0046
Epoch: [3/10], Loss: 0.0031
Epoch: [4/10], Loss: 0.0023
Epoch: [5/10], Loss: 0.0018
Epoch: [6/10], Loss: 0.0014
Epoch: [7/10], Loss: 0.0012
Epoch: [8/10], Loss: 0.0010
Epoch: [9/10], Loss: 0.0009
Epoch: [10/10], Loss: 0.0008
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0086
Epoch: [2/10], Loss: 0.0047
Epoch: [3/10], Loss: 0.0032
Epoch: [4/10], Loss: 0.0024
Epoch: [5/10], Loss: 0.0020
Epoch: [6/10], Loss: 0.0016
Epoch: [7/10], Loss: 0.0014
Epoch: [8/10], Loss: 0.0013
Epoch: [9/10], Loss: 0.0011
Epoch: [10/10], Loss: 0.0010
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0095
Epoch: [2/10], Loss: 0.0059
Epoch: [3/10], Loss: 0.0046
Epoch: [4/10], Loss: 0.0039
Epoch: [5/10], Loss: 0.0035
Epoch: [6/10], Loss: 0.0033
Epoch: [7/10], Loss: 0.0031
Epoch: [8/10], Loss: 0.0029
Epoch: [9/10], Loss: 0.0028
Epoch: [10/10], Loss: 0.0027
Score (accuracy) of the probe: 0.99

The best l1=0, the best l2=0.1 for pretrained_CodeBERT_layer_8
Accuracy on the test set of probing pretrained_CodeBERT_layer_8 of all layers:
Score (accuracy) of the probe: 0.93
{'__OVERALL__': 0.9262295081967213, 'NAME': 0.953241232731137, 'STRING': 1.0, 'NUMBER': 0.8984796468857283, 'KEYWORD': 0.9705882352941176}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1794,KW_NAME:0
NAME_KW:47,KW_KW:66
NAME_STRING:12,KW_other:2
NAME_NUMBER:29
NAME_STRING_list:['b', 's', 'm', 'f', 'U', 'x', 'i', 'i', 'R', 'a3', '_', 'h5f']
NAME_NUMBER_list:['K', 'N', 'K', 'T0', 'T0', 'D', 'D', 'K', 'N', 'n', 'n', 'K', 'N', 'P', 'n', 'D', 'D', 'D', 'K', 'D', 'K', 'D', 'D', 'K', 'n', 's', 's', 'u', 'T']
Accuracy on the test set of pretrained_CodeBERT_layer_8 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_CodeBERT Layer 9
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0081
Epoch: [2/10], Loss: 0.0043
Epoch: [3/10], Loss: 0.0029
Epoch: [4/10], Loss: 0.0021
Epoch: [5/10], Loss: 0.0017
Epoch: [6/10], Loss: 0.0014
Epoch: [7/10], Loss: 0.0011
Epoch: [8/10], Loss: 0.0010
Epoch: [9/10], Loss: 0.0008
Epoch: [10/10], Loss: 0.0007
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0082
Epoch: [2/10], Loss: 0.0044
Epoch: [3/10], Loss: 0.0029
Epoch: [4/10], Loss: 0.0022
Epoch: [5/10], Loss: 0.0017
Epoch: [6/10], Loss: 0.0014
Epoch: [7/10], Loss: 0.0012
Epoch: [8/10], Loss: 0.0010
Epoch: [9/10], Loss: 0.0009
Epoch: [10/10], Loss: 0.0008
Score (accuracy) of the probe: 0.99
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0085
Epoch: [2/10], Loss: 0.0046
Epoch: [3/10], Loss: 0.0032
Epoch: [4/10], Loss: 0.0024
Epoch: [5/10], Loss: 0.0019
Epoch: [6/10], Loss: 0.0016
Epoch: [7/10], Loss: 0.0014
Epoch: [8/10], Loss: 0.0012
Epoch: [9/10], Loss: 0.0011
Epoch: [10/10], Loss: 0.0010
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0093
Epoch: [2/10], Loss: 0.0057
Epoch: [3/10], Loss: 0.0045
Epoch: [4/10], Loss: 0.0038
Epoch: [5/10], Loss: 0.0035
Epoch: [6/10], Loss: 0.0032
Epoch: [7/10], Loss: 0.0030
Epoch: [8/10], Loss: 0.0029
Epoch: [9/10], Loss: 0.0028
Epoch: [10/10], Loss: 0.0026
Score (accuracy) of the probe: 0.99

The best l1=0, the best l2=0.1 for pretrained_CodeBERT_layer_9
Accuracy on the test set of probing pretrained_CodeBERT_layer_9 of all layers:
Score (accuracy) of the probe: 0.90
{'__OVERALL__': 0.8969200198708396, 'NAME': 0.9362380446333688, 'STRING': 1.0, 'NUMBER': 0.8553212358999509, 'KEYWORD': 1.0}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1762,KW_NAME:0
NAME_KW:72,KW_KW:68
NAME_STRING:15,KW_other:0
NAME_NUMBER:33
NAME_STRING_list:['f', 'x', 'x', 'x', '_', 'T', 'T', 'f', 'c', 'p', 'i', 'i', 'f', '_', 'i']
NAME_NUMBER_list:['m', 'm', 't0', '_', 'K', 'K', 'T0', 'T0', 't0', 'T0', 'T0', 't0', 't0', 'T0', 'T0', 't0', 't0', 'T0', 't0', 't0', 'T0', 't0', 'D', 'p0', 'n', 'n', 'T', 'N', 'K', 'i', 'a3', 'a0', 'h5f']
Accuracy on the test set of pretrained_CodeBERT_layer_9 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_CodeBERT Layer 10
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0080
Epoch: [2/10], Loss: 0.0039
Epoch: [3/10], Loss: 0.0025
Epoch: [4/10], Loss: 0.0018
Epoch: [5/10], Loss: 0.0013
Epoch: [6/10], Loss: 0.0011
Epoch: [7/10], Loss: 0.0009
Epoch: [8/10], Loss: 0.0008
Epoch: [9/10], Loss: 0.0007
Epoch: [10/10], Loss: 0.0006
Score (accuracy) of the probe: 0.99
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0078
Epoch: [2/10], Loss: 0.0038
Epoch: [3/10], Loss: 0.0025
Epoch: [4/10], Loss: 0.0018
Epoch: [5/10], Loss: 0.0014
Epoch: [6/10], Loss: 0.0011
Epoch: [7/10], Loss: 0.0009
Epoch: [8/10], Loss: 0.0008
Epoch: [9/10], Loss: 0.0007
Epoch: [10/10], Loss: 0.0006
Score (accuracy) of the probe: 0.99
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0081
Epoch: [2/10], Loss: 0.0040
Epoch: [3/10], Loss: 0.0026
Epoch: [4/10], Loss: 0.0020
Epoch: [5/10], Loss: 0.0016
Epoch: [6/10], Loss: 0.0013
Epoch: [7/10], Loss: 0.0011
Epoch: [8/10], Loss: 0.0010
Epoch: [9/10], Loss: 0.0009
Epoch: [10/10], Loss: 0.0008
Score (accuracy) of the probe: 0.99
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0089
Epoch: [2/10], Loss: 0.0052
Epoch: [3/10], Loss: 0.0040
Epoch: [4/10], Loss: 0.0035
Epoch: [5/10], Loss: 0.0032
Epoch: [6/10], Loss: 0.0029
Epoch: [7/10], Loss: 0.0028
Epoch: [8/10], Loss: 0.0027
Epoch: [9/10], Loss: 0.0026
Epoch: [10/10], Loss: 0.0025
Score (accuracy) of the probe: 0.99

The best l1=0, the best l2=0.1 for pretrained_CodeBERT_layer_10
Accuracy on the test set of probing pretrained_CodeBERT_layer_10 of all layers:
Score (accuracy) of the probe: 0.92
{'__OVERALL__': 0.9187779433681073, 'NAME': 0.9723698193411264, 'STRING': 1.0, 'NUMBER': 0.8651299656694458, 'KEYWORD': 1.0}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1830,KW_NAME:0
NAME_KW:15,KW_KW:68
NAME_STRING:12,KW_other:0
NAME_NUMBER:25
NAME_STRING_list:['b', 'i', 'i', 'i', 'i', 'q0', 'i', 'u', 'i', 'i', 'i', 'R']
NAME_NUMBER_list:['f1', 'C', 'K', 'N', 'T0', 'T0', 'T0', 'D', 'D', 'D', 'k', 'N', 'D', 'D', 'N', 'P', 'p0', 'D', 'D', 'D', 'D', 'D', 'K', 'D', 'a0']
Accuracy on the test set of pretrained_CodeBERT_layer_10 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_CodeBERT Layer 11
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0079
Epoch: [2/10], Loss: 0.0040
Epoch: [3/10], Loss: 0.0026
Epoch: [4/10], Loss: 0.0019
Epoch: [5/10], Loss: 0.0014
Epoch: [6/10], Loss: 0.0011
Epoch: [7/10], Loss: 0.0010
Epoch: [8/10], Loss: 0.0008
Epoch: [9/10], Loss: 0.0007
Epoch: [10/10], Loss: 0.0006
Score (accuracy) of the probe: 0.99
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0079
Epoch: [2/10], Loss: 0.0040
Epoch: [3/10], Loss: 0.0026
Epoch: [4/10], Loss: 0.0019
Epoch: [5/10], Loss: 0.0014
Epoch: [6/10], Loss: 0.0012
Epoch: [7/10], Loss: 0.0010
Epoch: [8/10], Loss: 0.0008
Epoch: [9/10], Loss: 0.0007
Epoch: [10/10], Loss: 0.0006
Score (accuracy) of the probe: 0.99
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0081
Epoch: [2/10], Loss: 0.0041
Epoch: [3/10], Loss: 0.0027
Epoch: [4/10], Loss: 0.0021
Epoch: [5/10], Loss: 0.0016
Epoch: [6/10], Loss: 0.0014
Epoch: [7/10], Loss: 0.0012
Epoch: [8/10], Loss: 0.0011
Epoch: [9/10], Loss: 0.0010
Epoch: [10/10], Loss: 0.0009
Score (accuracy) of the probe: 0.99
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0089
Epoch: [2/10], Loss: 0.0052
Epoch: [3/10], Loss: 0.0041
Epoch: [4/10], Loss: 0.0035
Epoch: [5/10], Loss: 0.0032
Epoch: [6/10], Loss: 0.0030
Epoch: [7/10], Loss: 0.0028
Epoch: [8/10], Loss: 0.0027
Epoch: [9/10], Loss: 0.0026
Epoch: [10/10], Loss: 0.0025
Score (accuracy) of the probe: 0.99

The best l1=0, the best l2=0.1 for pretrained_CodeBERT_layer_11
Accuracy on the test set of probing pretrained_CodeBERT_layer_11 of all layers:
Score (accuracy) of the probe: 0.93
{'__OVERALL__': 0.9344262295081968, 'NAME': 0.9516471838469713, 'STRING': 1.0, 'NUMBER': 0.9151544874938695, 'KEYWORD': 1.0}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1791,KW_NAME:0
NAME_KW:42,KW_KW:68
NAME_STRING:17,KW_other:0
NAME_NUMBER:32
NAME_STRING_list:['c', 'x', 'j', 'j', 'j', 'j', 'j', 'j', 'j', 'j', 'j', 'x', 'J', 'a2', 'a3', 'C', 'l']
NAME_NUMBER_list:['m', 'm', 'x', 'x', 'x', 't', 'x', 'x', 'x', 'C', 'C', 'K', 'N', 'C', 'T0', 'T0', 'T0', 'D', 'D', 'C', 'C', 'D', 'D', 'T', 'C', 'N', 'D', 'D', 'D', 'D', 'D', 'C']
Accuracy on the test set of pretrained_CodeBERT_layer_11 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_CodeBERT Layer 12
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0088
Epoch: [2/10], Loss: 0.0052
Epoch: [3/10], Loss: 0.0037
Epoch: [4/10], Loss: 0.0029
Epoch: [5/10], Loss: 0.0023
Epoch: [6/10], Loss: 0.0019
Epoch: [7/10], Loss: 0.0016
Epoch: [8/10], Loss: 0.0014
Epoch: [9/10], Loss: 0.0012
Epoch: [10/10], Loss: 0.0011
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0086
Epoch: [2/10], Loss: 0.0051
Epoch: [3/10], Loss: 0.0037
Epoch: [4/10], Loss: 0.0028
Epoch: [5/10], Loss: 0.0023
Epoch: [6/10], Loss: 0.0019
Epoch: [7/10], Loss: 0.0016
Epoch: [8/10], Loss: 0.0014
Epoch: [9/10], Loss: 0.0012
Epoch: [10/10], Loss: 0.0011
Score (accuracy) of the probe: 0.99
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0085
Epoch: [2/10], Loss: 0.0051
Epoch: [3/10], Loss: 0.0037
Epoch: [4/10], Loss: 0.0029
Epoch: [5/10], Loss: 0.0024
Epoch: [6/10], Loss: 0.0020
Epoch: [7/10], Loss: 0.0018
Epoch: [8/10], Loss: 0.0016
Epoch: [9/10], Loss: 0.0014
Epoch: [10/10], Loss: 0.0013
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0094
Epoch: [2/10], Loss: 0.0062
Epoch: [3/10], Loss: 0.0050
Epoch: [4/10], Loss: 0.0043
Epoch: [5/10], Loss: 0.0039
Epoch: [6/10], Loss: 0.0036
Epoch: [7/10], Loss: 0.0033
Epoch: [8/10], Loss: 0.0031
Epoch: [9/10], Loss: 0.0029
Epoch: [10/10], Loss: 0.0028
Score (accuracy) of the probe: 0.99

The best l1=0, the best l2=0.1 for pretrained_CodeBERT_layer_12
Accuracy on the test set of probing pretrained_CodeBERT_layer_12 of all layers:
Score (accuracy) of the probe: 0.93
{'__OVERALL__': 0.9346746150024838, 'NAME': 0.9319872476089267, 'STRING': 1.0, 'NUMBER': 0.9337910740559098, 'KEYWORD': 1.0}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1754,KW_NAME:0
NAME_KW:83,KW_KW:68
NAME_STRING:24,KW_other:0
NAME_NUMBER:21
NAME_STRING_list:['b1', 'x', 'j', 't', 'r', 'z', 'x', 'f', 'x', 'j', 'j', 'i', 'v', 'v', 'v', 'v', 's', 'F', 'F', 'R', 'a3', 'a2', 'C', 'C']
NAME_NUMBER_list:['d', 'e', 'f', 'c', 'c', 't', 'f2', 'Q', 'I', 't3', 'D', 'D', 'u', 'c', 'c', 'c', 'F', 'f0', 'a0', 't', 't']
Accuracy on the test set of pretrained_CodeBERT_layer_12 model using the intercept:
Score (accuracy) of the probe: 0.47

pretrained_CodeBERT top neurons
array([8193,   20,   31, 2084, 8231, 8238, 6192, 4151, 4152, 6205, 4160,
         64, 2118, 6216, 2124, 6222, 2137, 4187, 6245, 8308,  116, 8316,
       2175, 6273, 8326,  145, 8337,  154, 8346, 6304, 4263, 2223, 2241,
       8386, 6337, 8393, 4298, 2255, 6357, 4309, 2267, 8411, 8416, 4336,
       6384, 4348, 6404, 8454,  264, 2312, 6418, 8469, 4376, 2334, 8480,
       6432, 8492, 8498, 6455, 8509, 2365, 8516,  337,  351, 6499, 2408,
       2413, 4470, 8568, 8577, 2443, 8587, 8604, 4508, 2477, 6592, 2497,
       8640, 8641,  453, 6603,  466,  471, 4571,  479, 6624, 6638, 2542,
       6640, 8688, 4599, 2556, 2573, 8722, 2583, 2584, 6679, 8727,  551,
       2602, 8749, 6702, 8753,  567, 2617, 4669, 2627, 2629,  588,  593,
       8786, 8794, 2655, 8803, 6775, 8830,  642, 8842, 4749,  663,  668,
       4775, 8877, 2743, 4801, 4808, 8915,  730, 6875, 4828,  731, 2786,
       2787, 2789, 2791, 8941, 2800, 8961, 2818, 8963, 6916, 4865, 6918,
       8969, 4882,  788, 2843, 8992, 8999, 6968, 9019, 6973,  829, 4928,
       6984,  840, 6990,  848, 4953, 2923, 9077, 2934, 9084,  907, 2961,
       9113, 7072, 7074, 2983, 2991,  945, 2995, 7102,  959, 7105, 5061,
       9158, 3015, 5065, 7125, 3035, 9179, 5104, 1011, 7162, 7165, 3074,
       7186, 3102, 5159, 9260, 3121, 9274, 3133, 9277, 7230, 9292, 7246,
       3152, 1113, 1114, 7267, 5220, 5226, 5240, 3200, 7302, 1170, 7324,
       1199, 7357, 3265, 1230, 7379, 5339, 9437, 1247, 7395, 3306, 1262,
       7408, 7410, 7413, 3328, 1281, 1285, 3335, 1291, 3352, 1307, 3366,
       1323, 5424, 1328, 5429, 3384, 3385, 7482, 5437, 3400, 1356, 9548,
       3409, 1369, 3419, 3422, 5477, 5486, 3439, 7538, 1399, 3452, 7577,
       3482, 3498, 3509, 7615, 3519, 1473, 7618, 5569, 7625, 3530, 5583,
       1498, 7643, 3550, 1504, 3555, 3559, 3568, 5639, 5650, 1556, 7710,
       7712, 1581, 7730, 3634, 1597, 7741, 9798, 1608, 5717, 7768, 7769,
       3684, 7780, 9849, 3707, 9852, 3709, 3712, 1666, 9861, 7814, 1670,
       7817, 7820, 7823, 3729, 1687, 3740, 7836, 7844, 3749, 1725, 7870,
       7872, 3780, 7877, 1749, 7893, 5845, 3803, 9947, 7901, 3806, 7913,
       7920, 5872, 5874, 1784, 3839, 1800, 1805, 7954, 3871, 7969, 1834,
       3884, 7985, 5942, 1848, 5948, 3901, 5964, 8012, 8014, 1871, 8016,
       1877, 1881, 8026, 3929, 8035, 8036, 1916, 8064, 6016, 6030, 1936,
       1938, 1943, 8089, 3998, 1980, 8134, 4040, 8136, 8147, 6107, 2021,
       4074, 6122, 2043])
pretrained_CodeBERT top neurons per class
{'NAME': array([6638, 6875, 6107, 1916, 8794, 6192, 7643, 8147, 3133, 8480,  479,
       4571, 7710, 6205, 8411, 5339, 3803, 5437, 3559, 9179, 2365, 3509,
        642, 6973, 9947, 3712, 4669,  264, 7954, 5104, 7618, 7872, 3102,
       8036, 2602, 1608, 4336, 2223,  337, 2791, 6304, 4928, 4074,  829,
        551, 8722, 8492,  668, 3740,  453, 3998, 7125, 8064, 7246, 8877,
       6016, 3709, 8830, 2334, 1323, 3335, 8089, 5424, 5061, 6640, 4828,
        848, 1597, 8509, 6432,  466, 3780, 1262, 3328, 7072, 2267, 3035,
       1805, 1871, 7730, 2043, 7893, 2556, 1399,  730, 7769, 7625, 8326,
       7105, 6245, 7357, 2241, 1881, 5226, 5159, 8308, 1848, 8963, 2800,
       9260, 6384, 8231, 3901, 3634, 3568, 6916, 7920, 7408, 4882, 2408,
       3265, 3200, 7538, 6499, 8915, 6592, 4801, 5964, 8193, 8035, 8803,
       8568, 8498, 8026,  731, 8961,  567, 4040, 7379, 7413, 3929]), 'STRING': array([2021, 6638, 2961, 4669, 1399,  829, 3133, 3712, 7730, 8411, 6107,
       8877, 6973, 2791, 8498, 3709, 9179, 9277, 7618,  642, 3901, 9113,
       1597,  264, 6875, 3335, 2365, 7710, 2602, 8026, 8147, 1356, 1916,
       7814, 5437, 4336,  730, 7413, 1328, 2124, 2334, 8509, 9849, 8640,
       7102, 4160, 8722, 2786, 2043, 6192, 7954, 8794,   31, 9861, 6384,
       3707, 3366, 8492, 8064, 1938, 1247, 7643,   20, 3871, 8961, 2267,
       7741, 8036, 6205, 4074, 8577, 5061, 1498, 1307, 3568, 2443, 8786,
       6624, 7769, 4187, 3803, 3559, 2255, 1323, 2137, 6418, 5339, 1369,
       7820, 8999, 4263, 4808, 7615, 8749, 3929, 1199, 2175, 1473, 1834,
       8587, 9019,  351, 3306, 3519, 7712, 4775, 2787, 1936, 2995,  453,
       8469, 2743, 8238, 7395, 6916, 4298, 8641, 2584, 3102, 3152, 1114,
       1170, 2084, 4470, 3385,  154, 7482, 8688, 2583, 7267, 2655,  959]), 'NUMBER': array([8411, 6638, 3133,  479, 7618, 4669, 3871, 1247, 8794, 6107, 7741,
       6973, 3712, 6679, 8386, 3709, 7710, 1262, 3498, 2602, 7102, 8231,
       7074, 7267, 7643, 6337,  471, 8036, 6245, 2043,  642, 3035, 5240,
       9260, 1916, 8026, 1597, 3439, 6640, 6205,  466, 7357, 5845, 6304,
       8454, 3901, 9277, 3684, 8346, 6875, 8509, 8492, 2443, 5872, 7817,
       6192, 8480, 3400, 8830, 6357, 1281, 3555, 9798, 8877, 2629, 4309,
       1323, 5437, 4151, 9113, 9437, 1749, 2497, 8915,  337, 8842, 3550,
       1291, 6404, 7730, 8999, 8136,  453, 1670, 9077, 7870, 9179, 7410,
       3509, 8134, 7125,  663, 8604, 3568, 3559, 4953, 4571, 5220, 3452,
       8238, 5065, 3015, 3352, 3121, 9274, 7836,  264, 1285, 1800, 7538,
       6418, 9852, 7969, 4040, 1113, 3366,  945, 6702, 7165, 1011, 1687,
       2573, 8641, 2843, 8969, 3409, 7901, 3074, 8014, 2995, 1943, 4865,
       5874, 3749, 2542, 4599]), 'KEYWORD': array([1916,  264, 6638, 3707, 5583, 1556, 3806, 3740, 4376, 1834, 6455,
       2818, 1805, 2542, 8794, 3419, 8941, 8516, 7538, 2573, 8480, 7618,
       3530, 7768, 5220, 3385, 4040,  730, 3306, 8640, 2617, 7102,  788,
       8016, 6304, 6222, 7877, 6030, 6918, 8727, 9548,  588,  642, 2312,
        907, 8393, 6192, 3709, 8641, 8386, 3384, 6216, 7985, 3102, 6273,
       7186, 7844, 5477, 7230, 9084, 3482, 5429, 2477, 7302, 2791, 5942,
       3729, 1504,   20, 4749, 5948, 5226, 6603, 3422,   64, 8416, 7324,
       8012, 7162, 2413, 8992, 7913, 2043, 6775, 7780, 2934, 4152, 6968,
       3884, 6984, 5486,  479, 1670, 8193, 8316, 9019, 7577, 2991, 1784,
       8147, 2124, 5650,  145, 3555, 4508, 8803, 2786, 1666, 9292, 6990,
       8753, 1581, 2627, 6122, 1725, 4348, 1980, 3839,  116, 2602, 2923,
        840, 5569, 8915, 6404, 2789, 1877, 2118, 1230, 8337,  593,  848,
       5639, 9158, 7823, 6245, 1170, 8961, 5717, 8035, 2983])}
The shape of selected features (13636, 366)
The shape of the training set: (13636, 366)
The shape of the validation set: (1516, 366)
The shape of the testing set: (4026, 366)
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0064
Epoch: [2/10], Loss: 0.0030
Epoch: [3/10], Loss: 0.0020
Epoch: [4/10], Loss: 0.0015
Epoch: [5/10], Loss: 0.0012
Epoch: [6/10], Loss: 0.0010
Epoch: [7/10], Loss: 0.0009
Epoch: [8/10], Loss: 0.0007
Epoch: [9/10], Loss: 0.0007
Epoch: [10/10], Loss: 0.0006
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0069
Epoch: [2/10], Loss: 0.0033
Epoch: [3/10], Loss: 0.0021
Epoch: [4/10], Loss: 0.0016
Epoch: [5/10], Loss: 0.0013
Epoch: [6/10], Loss: 0.0011
Epoch: [7/10], Loss: 0.0009
Epoch: [8/10], Loss: 0.0008
Epoch: [9/10], Loss: 0.0007
Epoch: [10/10], Loss: 0.0006
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0067
Epoch: [2/10], Loss: 0.0033
Epoch: [3/10], Loss: 0.0022
Epoch: [4/10], Loss: 0.0017
Epoch: [5/10], Loss: 0.0014
Epoch: [6/10], Loss: 0.0012
Epoch: [7/10], Loss: 0.0011
Epoch: [8/10], Loss: 0.0010
Epoch: [9/10], Loss: 0.0009
Epoch: [10/10], Loss: 0.0009
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0079
Epoch: [2/10], Loss: 0.0045
Epoch: [3/10], Loss: 0.0036
Epoch: [4/10], Loss: 0.0032
Epoch: [5/10], Loss: 0.0029
Epoch: [6/10], Loss: 0.0027
Epoch: [7/10], Loss: 0.0026
Epoch: [8/10], Loss: 0.0025
Epoch: [9/10], Loss: 0.0024
Epoch: [10/10], Loss: 0.0023
Score (accuracy) of the probe: 0.99

The best l1=0, the best l2=0.1 for pretrained_CodeBERT_top5%_neurons
Accuracy on the test set of probing pretrained_CodeBERT_top5%_neurons of all layers:
Score (accuracy) of the probe: 0.84
{'__OVERALL__': 0.8392945851962246, 'NAME': 0.936769394261424, 'STRING': 1.0, 'NUMBER': 0.7410495340853359, 'KEYWORD': 1.0}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1763,KW_NAME:0
NAME_KW:84,KW_KW:68
NAME_STRING:22,KW_other:0
NAME_NUMBER:13
NAME_STRING_list:['s', 'y', 'y', 'y', 'y', 'e', 'x', 'x', 't0', 'y', 'x', 's', 'x', 's', 'p', 'p', 'p', 'i', 'y', 'j1', 'a0', 'T']
NAME_NUMBER_list:['y', 'yy', 'T0', 'T0', 'T0', 'T0', 'T0', 'y', 'j', 'y', 'y', 'j1', 'T']
Accuracy on the test set of pretrained_CodeBERT_top5%_neurons model using the intercept:
Score (accuracy) of the probe: 0.47
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0081
Epoch: [2/10], Loss: 0.0048
Epoch: [3/10], Loss: 0.0035
Epoch: [4/10], Loss: 0.0028
Epoch: [5/10], Loss: 0.0023
Epoch: [6/10], Loss: 0.0020
Epoch: [7/10], Loss: 0.0018
Epoch: [8/10], Loss: 0.0016
Epoch: [9/10], Loss: 0.0014
Epoch: [10/10], Loss: 0.0013
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0073
Epoch: [2/10], Loss: 0.0045
Epoch: [3/10], Loss: 0.0033
Epoch: [4/10], Loss: 0.0026
Epoch: [5/10], Loss: 0.0022
Epoch: [6/10], Loss: 0.0019
Epoch: [7/10], Loss: 0.0017
Epoch: [8/10], Loss: 0.0015
Epoch: [9/10], Loss: 0.0014
Epoch: [10/10], Loss: 0.0012
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0085
Epoch: [2/10], Loss: 0.0051
Epoch: [3/10], Loss: 0.0037
Epoch: [4/10], Loss: 0.0030
Epoch: [5/10], Loss: 0.0026
Epoch: [6/10], Loss: 0.0023
Epoch: [7/10], Loss: 0.0020
Epoch: [8/10], Loss: 0.0018
Epoch: [9/10], Loss: 0.0017
Epoch: [10/10], Loss: 0.0015
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0085
Epoch: [2/10], Loss: 0.0057
Epoch: [3/10], Loss: 0.0046
Epoch: [4/10], Loss: 0.0041
Epoch: [5/10], Loss: 0.0037
Epoch: [6/10], Loss: 0.0035
Epoch: [7/10], Loss: 0.0032
Epoch: [8/10], Loss: 0.0031
Epoch: [9/10], Loss: 0.0029
Epoch: [10/10], Loss: 0.0028
Score (accuracy) of the probe: 0.99

The best l1=0, the best l2=0.1 for pretrained_CodeBERT_top200_neurons
Accuracy on the test set of probing pretrained_CodeBERT_top200_neurons of all layers:
Score (accuracy) of the probe: 0.75
{'__OVERALL__': 0.7533532041728763, 'NAME': 0.9171094580233794, 'STRING': 0.8648648648648649, 'NUMBER': 0.592447278077489, 'KEYWORD': 0.9852941176470589}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1726,KW_NAME:1
NAME_KW:99,KW_KW:67
NAME_STRING:41,KW_other:0
NAME_NUMBER:16
NAME_STRING_list:['x', 'x', 'y', 'x', 'y', 'x', 'x', 'x', 'Y', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'F', 'x', 's', 's', 's', 's', 's', 's', 's', 's', 't', 's', 's', 'p', 'p', 'm', 'f', 'i', 'x', 'T', 'p', 't', 't', 't']
NAME_NUMBER_list:['x', 'o', 'y', 'y', 'i', 'q', 'q', 'q', 'T0', 'T0', 'T0', 'q', 'q', 'y', 'y', 'i']
Accuracy on the test set of pretrained_CodeBERT_top200_neurons model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_CodeBERT top words
Top words for pretrained_CodeBERT neuron indx 8193 [('or', 1.0), ('62', 0.8981688677367969), ('"append"', 0.8237473044362817), ('multiple', 0.7881350370156479), ('KeyboardInterrupt', 0.7386896479834358)]
Top words for pretrained_CodeBERT neuron indx 20 [('127', 1.0), ('Subject', 0.916970506683875), ('demo', 0.8936491635041679), ('Frame', 0.7956545143439595), ('broker', 0.7749665387542065)]
Top words for pretrained_CodeBERT neuron indx 31 [('Card', 1.0), ('video', 0.9687682397299179), ('commits', 0.8891225453650108), ('inverse', 0.8491047992131386), ('Video', 0.8452792180306101)]
Top words for pretrained_CodeBERT neuron indx 2084 [('within', 1.0), ('HTTPRequest', 0.752808639322344), ('750', 0.7288366508118035), ('height', 0.7059419063904224), ('no', 0.680922985985059)]
Top words for pretrained_CodeBERT neuron indx 8231 [('return', 1.0), ('for', 0.9697695894383666), ('try', 0.9601254567278009), ('from', 0.9250753909983582), ('def', 0.8978228091789426)]
Top words for pretrained_CodeBERT neuron indx 8238 [('class', 1.0), ('750', 0.7092617551261093), ('providers', 0.7020675145150448), ('Unauthorized', 0.6866584187521602), ('within', 0.6837775421070524)]
Top words for pretrained_CodeBERT neuron indx 6192 [('for', 1.0), ('not', 0.7867762175380687), ('secs', 0.7607689338309737), ('Unregister', 0.7596314937975516), ('OSError', 0.6916648951438856)]
Top words for pretrained_CodeBERT neuron indx 4151 [('fail', 1.0), ('shortcuts', 0.9993272584199351), ('locals', 0.9067743072493152), ('fabric', 0.873862111640727), ('excinfo', 0.8566679412041517)]
Top words for pretrained_CodeBERT neuron indx 4152 [('acquire', 1.0), ('deepcopy', 0.9929107774160947), ('initial', 0.9337976506100638), ('SECONDS', 0.8656095235332746), ('OK', 0.8427556046895143)]
Top words for pretrained_CodeBERT neuron indx 6205 [('is', 1.0), ('as', 0.9238362667959058), ('kls', 0.8990127443924699), ('dr', 0.8944169554020851), ('fabric', 0.8898674072346265)]
Top words for pretrained_CodeBERT neuron indx 4160 [('acquire', 1.0), ('inc', 0.7870347951042371), ('chmod', 0.7477525235567187), ('mktime', 0.7426222302010257), ('contenttypes', 0.7354599282282249)]
Top words for pretrained_CodeBERT neuron indx 64 [('eg', 1.0), ('comment', 0.9859974261924147), ('engine', 0.9083156800810841), ('Allow', 0.8825273696562133), ('decimal', 0.8633599115409322)]
Top words for pretrained_CodeBERT neuron indx 2118 [('brightness', 1.0), ('texture', 0.8225950641043347), ('communities', 0.8223788241073008), ('flat', 0.7800089233213966), ('onerror', 0.7243269001312121)]
Top words for pretrained_CodeBERT neuron indx 6216 [('rjust', 1.0), ('try', 0.9606684204909129), ('MSG', 0.9409404885316691), ('"card-gallery"', 0.9403883287620106), ('providers', 0.9361539358214546)]
Top words for pretrained_CodeBERT neuron indx 2124 [('tornado', 1.0), ('MINIMUM', 0.9248707200025021), ('ALPHABET', 0.8892779873283101), ('splitext', 0.8430163575434979), ('SUCCESS', 0.829200158084917)]
Top words for pretrained_CodeBERT neuron indx 6222 [('resolve', 1.0), ('StopIteration', 0.9009682612268611), ('try', 0.8771364336610449), ('vpn_instance', 0.8079548173826864), ('HTTP_PORT', 0.7912640860900316)]
Top words for pretrained_CodeBERT neuron indx 2137 [('missing', 1.0), ('omit', 0.9213961836598729), ('regexp', 0.8987618077569411), ('302', 0.893639942178093), ('hidden', 0.878667125042154)]
Top words for pretrained_CodeBERT neuron indx 4187 [('auto_index', 1.0), ('204', 0.8898028086347589), ('add_help', 0.84573107446747), ('Failure', 0.7598637108417653), ('success', 0.7234705767133767)]
Top words for pretrained_CodeBERT neuron indx 6245 [('35', 1.0), ('42', 0.8939844114777427), ('closing', 0.88509228271514), ('sites', 0.8689794668738634), ('21', 0.819644843478075)]
Top words for pretrained_CodeBERT neuron indx 8308 [('inc', 1.0), ('300', 0.8931251681787379), ('SOCK_STREAM', 0.8335236921656382), ('closing', 0.8172956251479297), ('HTTP_BAD_REQUEST', 0.8107499849257853)]
Top words for pretrained_CodeBERT neuron indx 116 [('Region', 1.0), ('Card', 0.8779487884452238), ('repository', 0.8179926139603051), ('emit', 0.8178965779184468), ('sublime', 0.7829618497485501)]
Top words for pretrained_CodeBERT neuron indx 8316 [('multiple', 1.0), ('sendall', 0.959870527737978), ('If', 0.94238519767), ('Always', 0.9415736783258462), ('fetchall', 0.8897957811852243)]
Top words for pretrained_CodeBERT neuron indx 2175 [('Off', 1.0), ('urlsplit', 0.8388854079683143), ('31', 0.8163935587817351), ('rosters', 0.7669614410548656), ('opcode', 0.7225894407029445)]
Top words for pretrained_CodeBERT neuron indx 6273 [('with', 1.0), ('for', 0.7716738504494862), ('"1111111111"', 0.7100829573681668), ('"21111111111"', 0.6826071796055061), ('33', 0.6247905381507872)]
Top words for pretrained_CodeBERT neuron indx 8326 [('getJob', 1.0), ('parameter', 0.9421474386215151), ('302', 0.8741339062678913), ('300', 0.8514544372385545), ('Label', 0.8341352711093877)]
Top words for pretrained_CodeBERT neuron indx 145 [('twisted', 1.0), ('responses', 0.957976818174734), ('downgrade', 0.9573972641311168), ('Response', 0.9110450232874643), ('span', 0.8644848825530893)]
Top words for pretrained_CodeBERT neuron indx 8337 [('READY', 1.0), ('"://"', 0.9733748332423695), ('"warning"', 0.9227843626328053), ('fstat', 0.8848638839360122), ('"http://"', 0.8714065617714403)]
Top words for pretrained_CodeBERT neuron indx 154 [('width', 1.0), ('dictionary', 0.9660425332621199), ('detect', 0.9580161545346143), ('restart', 0.9490087726511169), ('reservation', 0.9055912443668268)]
Top words for pretrained_CodeBERT neuron indx 8346 [('closing', 1.0), ('rjust', 0.9437299721454064), ('18', 0.9268223907417731), ('"Attach"', 0.8732609097705369), ('HTTPRequest', 0.8730080308705606)]
Top words for pretrained_CodeBERT neuron indx 6304 [('Geo', 1.0), ('127', 0.8571970572841476), ('lat', 0.8116984449845042), ('95', 0.783512587522296), ('internet', 0.780804912092756)]
Top words for pretrained_CodeBERT neuron indx 4263 [('fromhex', 1.0), ('GenericBackend', 0.9549003152793355), ('tokens', 0.9339582909942364), ('passwd', 0.8800271370292538), ('capitalize', 0.8702561949712548)]
Top words for pretrained_CodeBERT neuron indx 2223 [('def', 1.0), ('rd', 0.9511363123238987), ('purpose', 0.9186369125797408), ('hot_points', 0.8984228266795006), ('crt', 0.8448590047819666)]
Top words for pretrained_CodeBERT neuron indx 2241 [('seek', 1.0), ('blank', 0.939704332003824), ('as', 0.9392918156159532), ('reflection', 0.9318338828694221), ('enum', 0.9270717457834613)]
Top words for pretrained_CodeBERT neuron indx 8386 [('Int', 1.0), ('visit', 0.9593358551535985), ('MULTILINE', 0.9139772177897268), ('renderers', 0.8963486919619955), ('sites', 0.8535355390803083)]
Top words for pretrained_CodeBERT neuron indx 6337 [('ll', 1.0), ('TAG', 0.9959080683041038), ('StreamClosed', 0.9226461226436264), ('SUCCESS', 0.9101591255735324), ('decorator', 0.9097428177889135)]
Top words for pretrained_CodeBERT neuron indx 8393 [('tastypie', 1.0), ('urlopen', 0.9827306690042437), ('lon__lt', 0.9816260118721367), ('0xFF', 0.9595345752641794), ('rest', 0.9570110405226544)]
Top words for pretrained_CodeBERT neuron indx 4298 [('visit', 1.0), ('compute', 0.9205767064863364), ('implementation', 0.8923824681566634), ('SIGINT', 0.8713785649334691), ('View', 0.8290465868157768)]
Top words for pretrained_CodeBERT neuron indx 2255 [('outgoing', 1.0), ('Else', 0.965356721386199), ('reverse', 0.8705063748218094), ('current', 0.8689053417485239), ('sel', 0.8628327845075712)]
Top words for pretrained_CodeBERT neuron indx 6357 [('try', 1.0), ('MINIMUM', 0.9594335835845167), ('"_cls"', 0.9584752033984363), ('archive', 0.9066725587308846), ('getdistrictfilestatus', 0.8939965787398008)]
Top words for pretrained_CodeBERT neuron indx 4309 [('Criteria', 1.0), ('def', 0.9645178855347925), ('blend', 0.9476441588753558), ('Cond', 0.8786506802594409), ('texture', 0.8317668833795206)]
Top words for pretrained_CodeBERT neuron indx 2267 [('subscriptions', 1.0), ('rosters', 0.9656701946865799), ('recordings', 0.9231467844308944), ('protocols', 0.9094844811524035), ('not', 0.8898131647708563)]
Top words for pretrained_CodeBERT neuron indx 8411 [('for', 1.0), ('def', 0.8887885094965025), ('from', 0.872239330613088), ('return', 0.8323368005010215), ('Truefrom', 0.8299525735854456)]
Top words for pretrained_CodeBERT neuron indx 8416 [('0x1', 1.0), ('__import__', 0.859310234589632), ('62', 0.8373554055840461), ('0xFF', 0.7986518220376807), ('print_help', 0.7913980013815358)]
Top words for pretrained_CodeBERT neuron indx 4336 [('is', 1.0), ('rosters', 0.9307118712899866), ('not', 0.9273918726604259), ('else', 0.8885225200957012), ('from', 0.8535415194327162)]
Top words for pretrained_CodeBERT neuron indx 6384 [('Byte', 1.0), ('hyper', 0.8059965505674161), ('sender', 0.7495912013347725), ('long', 0.7364239489878558), ('tell', 0.7354345990208327)]
Top words for pretrained_CodeBERT neuron indx 4348 [('excepts', 1.0), ('STRLEN', 0.9152146917979644), ('Truefrom', 0.8899611505408368), ('fetchall', 0.8844904703949245), ('continue', 0.8725349087406499)]
Top words for pretrained_CodeBERT neuron indx 6404 [('def', 1.0), ('millis', 0.8351022703781408), ('createProviderHeader', 0.7663004972862707), ('PROVISION', 0.7463123180558888), ('DISCONNECTED', 0.7023528065620332)]
Top words for pretrained_CodeBERT neuron indx 8454 [('SOCK_STREAM', 1.0), ('SOL_SOCKET', 0.9915822274466699), ('IGNORECASE', 0.967876415506759), ('62', 0.9649075226372881), ('cdef', 0.9373537903030208)]
Top words for pretrained_CodeBERT neuron indx 264 [('deferred', 1.0), ('Cond', 0.9462149935469171), ('ProviderService', 0.9186724192541398), ('Provider', 0.9108341897386172), ('sa', 0.8893725818549657)]
Top words for pretrained_CodeBERT neuron indx 2312 [('splitLine', 1.0), ('512', 0.9033328664954886), ('Action', 0.8304836392852577), ('Plugin', 0.7671953959400344), ('204', 0.7614511813374158)]
Top words for pretrained_CodeBERT neuron indx 6418 [('Action', 1.0), ('FunctionalTest', 0.9255362597961846), ('calculator', 0.7877849868654352), ('Register', 0.7537932348010314), ('Widget', 0.7412347658173002)]
Top words for pretrained_CodeBERT neuron indx 8469 [('from', 1.0), ('75', 0.8146213272483173), ('"{}-{}-{}"', 0.7853501563314946), ('"x"', 0.7784828080373936), ('mins', 0.7094157111099754)]
Top words for pretrained_CodeBERT neuron indx 4376 [('CheckReadPerm', 1.0), ('Unauthorized', 0.9843098794393602), ('is_authenticated', 0.9188549513172819), ('NoPerm', 0.9120546292983126), ('HEADERS', 0.8940720565148804)]
Top words for pretrained_CodeBERT neuron indx 2334 [('def', 1.0), ('"%s/%s"', 0.7428466898392417), ('RELEASE', 0.6295531625460404), ('"%s@%s"', 0.6230212332757977), ('"%0x"', 0.6074896113158612)]
Top words for pretrained_CodeBERT neuron indx 8480 [('def', 1.0), ('microseconds', 0.9804109846372704), ('bk', 0.9792355438899671), ('b64encode', 0.9554244706652575), ('milliseconds', 0.8606062166158077)]
Top words for pretrained_CodeBERT neuron indx 6432 [('chunks', 1.0), ('or', 0.8721943585730108), ('readvertise', 0.8458212692438483), ('69', 0.8283691596736444), ('tries', 0.7982240257812244)]
Top words for pretrained_CodeBERT neuron indx 8492 [('Failure', 1.0), ('0x1', 0.9639421872314055), ('SCPreferencesCreate', 0.9398898329786229), ('as', 0.9183652946557366), ('closing', 0.9058379065763807)]
Top words for pretrained_CodeBERT neuron indx 8498 [('Byte', 1.0), ('sel', 0.9431479201371166), ('119', 0.8865538210589524), ('ax', 0.871905299427984), ('entry', 0.8572489415730605)]
Top words for pretrained_CodeBERT neuron indx 6455 [('convert', 1.0), ('token_type', 0.8572357986811023), ('cmp', 0.8086297579858909), ('443', 0.7392880197099306), ('loads', 0.7250250180420859)]
Top words for pretrained_CodeBERT neuron indx 8509 [('is', 1.0), ('else', 0.9062418388305601), ('fabric', 0.8799243276034668), ('with', 0.876139809089539), ('run_until_complete', 0.8381445285338333)]
Top words for pretrained_CodeBERT neuron indx 2365 [('as', 1.0), ('is', 0.8774888847355635), ('dr', 0.8562507744785587), ('else', 0.8194376613426125), ('tv', 0.7516238350873686)]
Top words for pretrained_CodeBERT neuron indx 8516 [('fileno', 1.0), ('href', 0.6904714568683041), ('deepcopy', 0.6466198868221256), ('d_up', 0.6427668975430819), ('1800', 0.63137676093399)]
Top words for pretrained_CodeBERT neuron indx 337 [('8000', 1.0), ('tell', 0.9493210768845132), ('from', 0.9275961409376025), ('10000', 0.7283082826143171), ('465', 0.7116174371036312)]
Top words for pretrained_CodeBERT neuron indx 351 [('trial', 1.0), ('calculator', 0.9707412766787171), ('confirm', 0.957450092424094), ('API', 0.8921913061242819), ('cert', 0.8241961337542967)]
Top words for pretrained_CodeBERT neuron indx 6499 [('omit', 1.0), ('flat', 0.9348994949438139), ('seen', 0.8034816467193845), ('cond', 0.8023386914635823), ('API', 0.742917449372523)]
Top words for pretrained_CodeBERT neuron indx 2408 [('sender', 1.0), ('tokens', 0.9883075204660139), ('runm', 0.8318744252763932), ('NotImplemented', 0.8245485476862471), ('modelName', 0.8123834136356738)]
Top words for pretrained_CodeBERT neuron indx 2413 [('license', 1.0), ('API', 0.8997314262709547), ('def', 0.878600975158018), ('Subject', 0.8685923025903471), ('comment', 0.8424842527985268)]
Top words for pretrained_CodeBERT neuron indx 4470 [('reflection', 1.0), ('microseconds', 0.9212743089670812), ('Module', 0.8747826123425329), ('refresh', 0.8586519213776063), ('dr', 0.8585720718196129)]
Top words for pretrained_CodeBERT neuron indx 8568 [('23', 1.0), ('0x1', 0.9635926819048966), ('try', 0.9380070637858622), ('class', 0.8796664945717249), ('0o444', 0.8435540863092003)]
Top words for pretrained_CodeBERT neuron indx 8577 [('as', 1.0), ('meta_info', 0.9723948733657755), ('get_all_field_names', 0.9651359713885551), ('exc_info', 0.9423407495106143), ('Int', 0.9133665261663845)]
Top words for pretrained_CodeBERT neuron indx 2443 [('tagging', 1.0), ('django', 0.8845743751175451), ('trello', 0.8661713656610426), ('vCard', 0.8274581672546878), ('redistricting', 0.7960555342150841)]
Top words for pretrained_CodeBERT neuron indx 8587 [('31', 1.0), ('Else', 0.904366742624283), ('try', 0.895800600989785), ('MANIFEST', 0.8798263589986632), ('class', 0.8184227800437411)]
Top words for pretrained_CodeBERT neuron indx 8604 [('passwd', 1.0), ('POST', 0.9483990621116474), ('parameter', 0.9284969860074916), ('Integer', 0.8471374732333783), ('GET', 0.823965771559955)]
Top words for pretrained_CodeBERT neuron indx 4508 [('StreamError', 1.0), ('displays', 0.984357292854686), ('TEMPLATES', 0.9761487958386889), ('Unauthorized', 0.9564869627135417), ('62', 0.9528625608718478)]
Top words for pretrained_CodeBERT neuron indx 2477 [('deleteAll', 1.0), ('Delay', 0.7739315511764784), ('internet', 0.7475302287593034), ('SUCCESS', 0.71892079826241), ('inc', 0.6842169101886413)]
Top words for pretrained_CodeBERT neuron indx 6592 [('accept', 1.0), ('def', 0.6962177172832165), ('ImageMath', 0.6680787222008355), ('Flag', 0.665815292571725), ('functional', 0.658422866861849)]
Top words for pretrained_CodeBERT neuron indx 2497 [('ll', 1.0), ('Tag', 0.9037461816961309), ('year', 0.8804554264495371), ('score', 0.8230934529294468), ('decorator', 0.8099485331080712)]
Top words for pretrained_CodeBERT neuron indx 8640 [('NoPerm', 1.0), ('SUCCESS', 0.9314409232588895), ('CheckReadPerm', 0.892633983460775), ('False', 0.8709433200696755), ('__copyright__', 0.8471984247641897)]
Top words for pretrained_CodeBERT neuron indx 8641 [('ll', 1.0), ('SUCCESS', 0.939731593469325), ('TAG', 0.9286121343751736), ('decorator', 0.8929622000298605), ('File', 0.8592878839391697)]
Top words for pretrained_CodeBERT neuron indx 453 [('"1.2.3"', 1.0), ('is', 0.9974653229765382), ('.3', 0.9211724529465901), ('in', 0.9113177132563053), ('2.1', 0.8966721050173596)]
Top words for pretrained_CodeBERT neuron indx 6603 [('Tab', 1.0), ('Action', 0.9516625964661342), ('synchronized', 0.8935446943191163), ('JobBase', 0.8666041008056561), ('CREATING', 0.865416802762375)]
Top words for pretrained_CodeBERT neuron indx 466 [('purge', 1.0), ('variable', 0.9016673945758831), ('Reg', 0.8964621996611042), ('119', 0.8837868812511318), ('Switch', 0.8212909977740824)]
Top words for pretrained_CodeBERT neuron indx 471 [('Always', 1.0), ('ListOperation', 0.828302154082807), ('117', 0.8066557114667627), ('119', 0.769473374525645), ('Plugin', 0.7494920103995227)]
Top words for pretrained_CodeBERT neuron indx 4571 [('for', 1.0), ('rosters', 0.8947170272740887), ('return', 0.8558004340557397), ('not', 0.8524528313939899), ('subscriptions', 0.8414347648271981)]
Top words for pretrained_CodeBERT neuron indx 479 [('Cat', 1.0), ('signed', 0.6152127388661003), ('CardOperation', 0.6105748500637185), ('BoardOperation', 0.6042545227612557), ('el', 0.601683973207677)]
Top words for pretrained_CodeBERT neuron indx 6624 [('multiple', 1.0), ('descriptor', 0.9963106949011338), ('Action', 0.9769267529259087), ('ServiceParameters', 0.922978157319905), ('chr', 0.9051412833068921)]
Top words for pretrained_CodeBERT neuron indx 6638 [('for', 1.0), ('else', 0.8821944740098403), ('except', 0.7865639726358226), ('while', 0.7608804667886525), ('with', 0.7143995338083216)]
Top words for pretrained_CodeBERT neuron indx 2542 [('joinall', 1.0), ('ero', 0.8824014074716634), ('def', 0.8540942511447368), ('unquote', 0.8414494031081814), ('Auditor', 0.8163135495353363)]
Top words for pretrained_CodeBERT neuron indx 6640 [('rosters', 1.0), ('in', 0.9490064589722764), ('is', 0.941811818731892), ('Truefrom', 0.8937503802465195), ('deploy', 0.8821178673232464)]
Top words for pretrained_CodeBERT neuron indx 8688 [('hyper', 1.0), ('Byte', 0.9639881691135226), ('long', 0.9493407735427907), ('null', 0.9190348979721306), ('tell', 0.9122394367066189)]
Top words for pretrained_CodeBERT neuron indx 4599 [('joinall', 1.0), ('future', 0.9023360311405278), ('plural', 0.8950328695208958), ('_encode', 0.8085999189825989), ('isdigit', 0.8012470037551843)]
Top words for pretrained_CodeBERT neuron indx 2556 [('None_', 1.0), ('comments', 0.8810570737206689), ('WARNING', 0.8730848309905967), ('NoneType', 0.8367455598706569), ('passed', 0.8315973537852007)]
Top words for pretrained_CodeBERT neuron indx 2573 [('sendall', 1.0), ('Clock', 0.883281883561211), ('joinall', 0.8744829192574621), ('makeTree', 0.8067452065644652), ('deleteAll', 0.7572632051572955)]
Top words for pretrained_CodeBERT neuron indx 8722 [('95', 1.0), ('and', 0.81071022162744), ('not', 0.8014802276302054), ('fetchall', 0.7563115754917431), ('31', 0.7456422750146119)]
Top words for pretrained_CodeBERT neuron indx 2583 [('SIGINT', 1.0), ('fetch', 0.9997450820492714), ('comments', 0.9439925474692784), ('vparser', 0.9279813121000776), ('GENERATOR', 0.9172509948526283)]
Top words for pretrained_CodeBERT neuron indx 2584 [('spawn', 1.0), ('ping', 0.8232812019726341), ('else', 0.786581108049657), ('Parse', 0.7711543269716908), ('span', 0.7645575709616516)]
Top words for pretrained_CodeBERT neuron indx 6679 [('Failure', 1.0), ('NetworkServiceAgent', 0.9982408599060376), ('STP', 0.993605956055078), ('Dataflow', 0.9858211402938825), ('RemoteRunner', 0.9765609381247501)]
Top words for pretrained_CodeBERT neuron indx 8727 [('"str"', 1.0), ('Parameter', 0.9184990774223882), ('splitext', 0.8924526009038587), ('"{0}.{1}.{2}"', 0.8821065979108551), ('basestring', 0.8820142386001327)]
Top words for pretrained_CodeBERT neuron indx 551 [('465', 1.0), ('in', 0.8773581876439028), ('and', 0.8768493580726054), ('to', 0.787654277432566), ('or', 0.7846585010787901)]
Top words for pretrained_CodeBERT neuron indx 2602 [('def', 1.0), ('raise', 0.8376885232246895), ('continue', 0.7280876978969049), ('collect', 0.6750822444572978), ('mpls', 0.6422402419773825)]
Top words for pretrained_CodeBERT neuron indx 8749 [('Parameter', 1.0), ('201', 0.9444231208389692), ('UnpackingError', 0.9392915174571945), ('Invalid', 0.8960693394296957), ('SyntaxError', 0.8817153045629866)]
Top words for pretrained_CodeBERT neuron indx 6702 [('plugin', 1.0), ('millis', 0.9795764366942409), ('PROVISION', 0.9339507050276583), ('print_help', 0.9263038082463034), ('25', 0.9252679500144084)]
Top words for pretrained_CodeBERT neuron indx 8753 [('2014', 1.0), ('excepts', 0.9222529228714705), ('serialized_start', 0.8546992399250098), ('exc_type', 0.8445032576775457), ('microseconds', 0.8361819638253492)]
Top words for pretrained_CodeBERT neuron indx 567 [('Distance', 1.0), ('SingleStatement', 0.8133441537641538), ('REF', 0.743648145865738), ('Mesh', 0.7414317027783098), ('terminate', 0.7356471976959484)]
Top words for pretrained_CodeBERT neuron indx 2617 [('flat', 1.0), ('omit', 0.9657209644636171), ('internet', 0.9187405188477045), ('email', 0.8967068111751565), ('has', 0.810165460663882)]
Top words for pretrained_CodeBERT neuron indx 4669 [('as', 1.0), ('is', 0.9709977265190052), ('else', 0.9188571285563644), ('with', 0.8319611180029537), ('dr', 0.7973793272424938)]
Top words for pretrained_CodeBERT neuron indx 2627 [('POST', 1.0), ('_grant_access_token', 0.9995187080044611), ('Point', 0.9635435657311778), ('pass', 0.9203558932568626), ('for', 0.9151012320706744)]
Top words for pretrained_CodeBERT neuron indx 2629 [('within', 1.0), ('pattern', 0.7971025765923342), ('redistricting', 0.7929042217725062), ('mult', 0.7423074380426234), ('UTF8', 0.7165257274968477)]
Top words for pretrained_CodeBERT neuron indx 588 [('is', 1.0), ('in', 0.9342818543775719), ('is_', 0.9069741594601085), ('and', 0.8880700813801959), ('to', 0.8697321336851894)]
Top words for pretrained_CodeBERT neuron indx 593 [('spawn', 1.0), ('Message', 0.7913449301651171), ('erase', 0.7047165564927399), ('Default', 0.660019463532549), ('lat', 0.6598745212798386)]
Top words for pretrained_CodeBERT neuron indx 8786 [('"-I"', 1.0), ('for', 0.9990219782068466), ('"-v"', 0.9721516166056406), ('Off', 0.9543570438364803), ('"FAILED"', 0.857994573721757)]
Top words for pretrained_CodeBERT neuron indx 8794 [('MULTILINE', 1.0), ('composite', 0.9702683773692685), ('subpath', 0.9541428350878021), ('None_', 0.9161060704527659), ('within', 0.9146583433499597)]
Top words for pretrained_CodeBERT neuron indx 2655 [('FieldDescriptor', 1.0), ('Descriptor', 0.8652502630512345), ('FileDescriptor', 0.7689210606704961), ('erase', 0.767740953932591), ('AuditLog', 0.7537073452689711)]
Top words for pretrained_CodeBERT neuron indx 8803 [('omit', 1.0), ('httpRequest', 0.7429201794253018), ('NodeBuilder', 0.7106543135846102), ('div', 0.7097600116497981), ('enabled', 0.6922184245070909)]
Top words for pretrained_CodeBERT neuron indx 6775 [('accept', 1.0), ('wait', 0.7182313548069212), ('detect', 0.7161085086370987), ('edited', 0.7118644096617429), ('unquote', 0.6911523995253764)]
Top words for pretrained_CodeBERT neuron indx 8830 [('and', 1.0), ('Failure', 0.8762856177330691), ('class', 0.7948464883639421), ('def', 0.7409633833804323), ('Provider', 0.7276339902497123)]
Top words for pretrained_CodeBERT neuron indx 642 [('attribute', 1.0), ('asset', 0.904358147841877), ('presets', 0.8176589073246991), ('119', 0.7913909847392414), ('reservation', 0.7655532395170167)]
Top words for pretrained_CodeBERT neuron indx 8842 [('receive_shadows', 1.0), ('"y"', 0.917733686105451), ('as', 0.9030179875657248), ('use_light_probes', 0.8593004884953527), ('SECURITY_POST_RESET_VIEW', 0.8490176408509901)]
Top words for pretrained_CodeBERT neuron indx 4749 [('Bytes', 1.0), ('Parameter', 0.858607489763037), ('RemoteExecutionError', 0.7631732146477361), ('humanize', 0.7536916071384112), ('chr', 0.7498747213701652)]
Top words for pretrained_CodeBERT neuron indx 663 [('chunks', 1.0), ('services', 0.9528211737136236), ('pa', 0.9313458749269765), ('implementation', 0.9102010040474995), ('pytz', 0.9073138378270671)]
Top words for pretrained_CodeBERT neuron indx 668 [('accept', 1.0), ('clone', 0.9153832233568392), ('verifier', 0.8560543217210265), ('decimal', 0.828992420943077), ('ON', 0.8147255361979697)]
Top words for pretrained_CodeBERT neuron indx 4775 [('hyper', 1.0), ('fabric', 0.8696430632583593), ('503', 0.8630157507739882), ('201', 0.8475293123072614), ('204', 0.8465487081320082)]
Top words for pretrained_CodeBERT neuron indx 8877 [('Truefrom', 1.0), ('cdef', 0.9742517037633742), ('for', 0.944207856462459), ('scheme', 0.887638497828597), ('subpath', 0.8769899216556641)]
Top words for pretrained_CodeBERT neuron indx 2743 [('".."', 1.0), ('inc', 0.9961117574105786), ('"#"', 0.9956321236953588), ('"~/"', 0.9726988962983558), ('Verifier', 0.9677501769527459)]
Top words for pretrained_CodeBERT neuron indx 4801 [('ll', 1.0), ('decorator', 0.9401495752006541), ('TAG', 0.9049268048680722), ('TERMINATE', 0.8695659765717542), ('SUCCESS', 0.8683642588494689)]
Top words for pretrained_CodeBERT neuron indx 4808 [('omit', 1.0), ('venue', 0.965588152071079), ('"application/json"', 0.8537335728708705), ('symbol_database', 0.8047635649373545), ('transfer', 0.7895405253898558)]
Top words for pretrained_CodeBERT neuron indx 8915 [('5001', 1.0), ('zone', 0.9917826157826773), ('List', 0.8955117854096919), ('Int', 0.8868880272373661), ('created', 0.8304982735110004)]
Top words for pretrained_CodeBERT neuron indx 730 [('Command', 1.0), ('blocking', 0.8060152790471768), ('StringField', 0.7911787202536165), ('MapField', 0.7759067077573182), ('User', 0.7715427242192824)]
Top words for pretrained_CodeBERT neuron indx 6875 [('for', 1.0), ('rosters', 0.9670177670845747), ('def', 0.9038020994524704), ('from', 0.8873520293096211), ('return', 0.8438252535025992)]
Top words for pretrained_CodeBERT neuron indx 4828 [('keyword', 1.0), ('lon', 0.9830787442099044), ('ord', 0.9669930319629604), ('"geolevel"', 0.9246760182442278), ('min_version', 0.9194559282254365)]
Top words for pretrained_CodeBERT neuron indx 731 [('in', 1.0), ('security', 0.9897083344498807), ('"100"', 0.9030511047971925), ('callback', 0.8854320938680522), ('from', 0.876568926800361)]
Top words for pretrained_CodeBERT neuron indx 2786 [('clone', 1.0), ('execute_command', 0.7618962091775631), ('pytz', 0.7417726639209067), ('slurp', 0.7410706150540669), ('django', 0.7405128667540408)]
Top words for pretrained_CodeBERT neuron indx 2787 [('alembic', 1.0), ('proc', 0.9943493585415927), ('call_count', 0.97883215135797), ('bus', 0.9638939741148607), ('sock', 0.9136120549276399)]
Top words for pretrained_CodeBERT neuron indx 2789 [('multiple', 1.0), ('rollback', 0.7619203459415803), ('select', 0.7485842889072104), ('seen', 0.7442426218198951), ('CREATING', 0.7246907179633867)]
Top words for pretrained_CodeBERT neuron indx 2791 [('def', 1.0), ('750', 0.7904804205204592), ('flash', 0.7699609613054331), ('"x"', 0.7346778253710808), ('signed', 0.6905350450918181)]
Top words for pretrained_CodeBERT neuron indx 8941 [('from', 1.0), ('exception', 0.9381368461777616), ('issubclass', 0.8851996270423881), ('_page_from_response', 0.8842776947340707), ('material', 0.874331855710783)]
Top words for pretrained_CodeBERT neuron indx 2800 [('rosters', 1.0), ('omit', 0.9796392341123598), ('not', 0.9639180420414145), ('for', 0.9374452503937243), ('is', 0.8915275534090247)]
Top words for pretrained_CodeBERT neuron indx 8961 [('62', 1.0), ('31', 0.8018498379818775), ('multiple', 0.7481032167250657), ('"r"', 0.7249277911883568), ('"lists"', 0.7107524277210414)]
Top words for pretrained_CodeBERT neuron indx 2818 [('Reg', 1.0), ('select', 0.8325895377629812), ('env', 0.748179975212131), ('VERSION', 0.744203838547586), ('loading', 0.7126693559466277)]
Top words for pretrained_CodeBERT neuron indx 8963 [('jtype', 1.0), ('Command', 0.9618402768307135), ('class', 0.8989213004813196), ('fetchall', 0.896044548655698), ('resources', 0.884088051982054)]
Top words for pretrained_CodeBERT neuron indx 6916 [('try', 1.0), ('compat', 0.9851372983719666), ('with', 0.864448396142949), ('except', 0.8159282004957141), ('decorators', 0.8091725545917781)]
Top words for pretrained_CodeBERT neuron indx 4865 [('created', 1.0), ('omit', 0.9971408429251071), ('unpackers', 0.9937293398026004), ('secs', 0.9240769706415253), ('sha1', 0.8790925917759239)]
Top words for pretrained_CodeBERT neuron indx 6918 [('multiple', 1.0), ('Manager', 0.8997842743926668), ('Clock', 0.8664892019936203), ('huffman', 0.8637638123349068), ('urls', 0.8239346752076082)]
Top words for pretrained_CodeBERT neuron indx 8969 [('False', 1.0), ('urandom', 0.9068113972878614), ('longname', 0.8180884716707824), ('attributes', 0.796382128439281), ('convert', 0.790273239917915)]
Top words for pretrained_CodeBERT neuron indx 4882 [('strict', 1.0), ('blank', 0.8013847909588357), ('signed', 0.7786213436831027), ('hidden', 0.7179036537428604), ('required', 0.7073782848975723)]
Top words for pretrained_CodeBERT neuron indx 788 [('Subject', 1.0), ('sans', 0.7136317916886631), ('127', 0.7045996844978629), ('demo', 0.6955520677569255), ('broadcast', 0.671876306969376)]
Top words for pretrained_CodeBERT neuron indx 2843 [('srid', 1.0), ('decode', 0.9577282476953727), ('instances', 0.9401149547942531), ('inst', 0.8821688175738144), ('tagging', 0.8387954351636793)]
Top words for pretrained_CodeBERT neuron indx 8992 [('closing', 1.0), ('in_bulk', 0.931995000364398), ('lon__gt', 0.8684112438451275), ('WAYNODES', 0.844912396819401), ('as', 0.8343701607114192)]
Top words for pretrained_CodeBERT neuron indx 8999 [('for', 1.0), ('return', 0.9828239870516552), ('try', 0.9624993878020137), ('from', 0.947584141740445), ('117', 0.9199567510455763)]
Top words for pretrained_CodeBERT neuron indx 6968 [('listen', 1.0), ('Provider', 0.9624944857129627), ('milliseconds', 0.9383233471666567), ('millis', 0.9136716802352343), ('receive_shadows', 0.8489784516813345)]
Top words for pretrained_CodeBERT neuron indx 9019 [('urlopen', 1.0), ('issubclass', 0.991172950348678), ('HTTPRequest', 0.9114118082989439), ('StreamHandler', 0.8761095830029516), ('print_help', 0.8141470421151599)]
Top words for pretrained_CodeBERT neuron indx 6973 [('is', 1.0), ('fabric', 0.8954501999362524), ('else', 0.8852016824402125), ('as', 0.8779696605753923), ('get_or_create', 0.8471908923937943)]
Top words for pretrained_CodeBERT neuron indx 829 [('as', 1.0), ('Wire', 0.8737353195378202), ('google', 0.8211263643877991), ('is', 0.7980398975485838), ('dr', 0.7536690941707994)]
Top words for pretrained_CodeBERT neuron indx 4928 [('multiple', 1.0), ('manager', 0.9354388840839033), ('def', 0.8665961865309113), ('rest_api', 0.8649054154089603), ('strict', 0.8113331268472488)]
Top words for pretrained_CodeBERT neuron indx 6984 [('visit', 1.0), ('Plugin', 0.9955186101617033), ('Byte', 0.9821809275418832), ('Parameter', 0.9475184940609828), ('future', 0.8582180572381451)]
Top words for pretrained_CodeBERT neuron indx 840 [('Simple', 1.0), ('site', 0.9777277020091262), ('sites', 0.9051699457654726), ('AstNode', 0.818008006248365), ('collection', 0.816302956752242)]
Top words for pretrained_CodeBERT neuron indx 6990 [('StopIteration', 1.0), ('downgrade', 0.8355424819844475), ('http11', 0.8197817770035939), ('gdal', 0.7601091778634782), ('resolve', 0.7504291038976326)]
Top words for pretrained_CodeBERT neuron indx 848 [('select', 1.0), ('one', 0.96411937411624), ('calculator', 0.9115033674177079), ('else', 0.8554569663429644), ('Integer', 0.7854077817142102)]
Top words for pretrained_CodeBERT neuron indx 4953 [('lat__lt', 1.0), ('multiple', 0.9687948284128916), ('lat__gt', 0.9653956519564921), ('sha1', 0.8447202608982989), ('version__lte', 0.8286250221843957)]
Top words for pretrained_CodeBERT neuron indx 2923 [('cond', 1.0), ('Cond', 0.9061641592040048), ('Presence', 0.7968600975782242), ('Always', 0.7895045921325875), ('Filter', 0.7284408025992153)]
Top words for pretrained_CodeBERT neuron indx 9077 [('closing', 1.0), ('SSLContext', 0.7545333773289079), ('"*"', 0.7162183000023723), ('"multiple"', 0.7145585993016781), ('"%0x"', 0.7085948805541717)]
Top words for pretrained_CodeBERT neuron indx 2934 [('validator', 1.0), ('31', 0.9697812026111424), ('changed', 0.91119006226776), ('try', 0.9095053002866073), ('ord', 0.9053000008331041)]
Top words for pretrained_CodeBERT neuron indx 9084 [('fetchall', 1.0), ('Default', 0.8740775588053432), ('lstrip', 0.8519597287284264), ('Always', 0.8491773056980432), ('engine', 0.8450497324287549)]
Top words for pretrained_CodeBERT neuron indx 907 [('partial', 1.0), ('4095', 0.9715988142200734), ('sender', 0.9001136720642197), ('define', 0.856387413929157), ('direct', 0.8399390919072668)]
Top words for pretrained_CodeBERT neuron indx 2961 [('Input', 1.0), ('"queue"', 0.9713504030157871), ('functional', 0.935347322951366), ('success', 0.8734251315332126), ('"time"', 0.87176689098753)]
Top words for pretrained_CodeBERT neuron indx 9113 [('"0000385c02001b006162636465666768696a6b6c6d6e6f70717273747576776162"', 1.0), ('"0800305c02001b006162636465666768696a6b6c6d6e6f70717273747576776162"', 0.9772504508403831), ('"Text"', 0.8880883006526448), ('None_', 0.8846380037620726), ('20000', 0.8673679081321538)]
Top words for pretrained_CodeBERT neuron indx 7072 [('for', 1.0), ('lat', 0.9888347171681755), ('mins', 0.8843249574752173), ('simple', 0.7837723355271077), ('with', 0.7596137184317265)]
Top words for pretrained_CodeBERT neuron indx 7074 [('implementation', 1.0), ('next', 0.9649999328811278), ('NotFound', 0.9289500703764216), ('35', 0.918137466056651), ('28', 0.9165615430509231)]
Top words for pretrained_CodeBERT neuron indx 2983 [('CANCEL', 1.0), ('within', 0.9739366698292784), ('deferred', 0.94443325689365), ('"KILLED"', 0.9309640595673518), ('__parent__', 0.875314092963676)]
Top words for pretrained_CodeBERT neuron indx 2991 [('TERMINATE', 1.0), ('framing', 0.9147626171252959), ('ip', 0.8825017971337444), ('LOG', 0.8613665085261227), ('simple', 0.8575989968024161)]
Top words for pretrained_CodeBERT neuron indx 945 [('backwards', 1.0), ('as', 0.9349695418511215), ('forwards', 0.883263587942785), ('DEFAULT', 0.7894281188184334), ('tornado', 0.7846948036457482)]
Top words for pretrained_CodeBERT neuron indx 2995 [('by', 1.0), ('OK', 0.9982538920873695), ('do', 0.9088267214877724), ('epoch_mins', 0.8946943872254752), ('OpenStreetMap', 0.8639885523461728)]
Top words for pretrained_CodeBERT neuron indx 7102 [('try', 1.0), ('plural', 0.9937076583529154), ('Failure', 0.9831650880830057), ('success', 0.93482802238812), ('b64decode', 0.9035568654127798)]
Top words for pretrained_CodeBERT neuron indx 959 [('resolve', 1.0), ('answer', 0.7455016999553994), ('Geo', 0.7247212699531058), ('emit', 0.6495391823353464), ('identical', 0.6302805880117233)]
Top words for pretrained_CodeBERT neuron indx 7105 [('TAG', 1.0), ('ll', 0.9785733627063986), ('SUCCESS', 0.9477081570456147), ('decorator', 0.8894212534957805), ('StreamClosed', 0.8812362466775818)]
Top words for pretrained_CodeBERT neuron indx 5061 [('try', 1.0), ('def', 0.958954623375675), ('tornado', 0.9422345201647568), ('break', 0.9187750793049404), ('Else', 0.8583779188274453)]
Top words for pretrained_CodeBERT neuron indx 9158 [('createreport', 1.0), ('Card', 0.9880301744088883), ('ctx', 0.9088585422583508), ('render_for', 0.8717445886826639), ('Connection', 0.849835352638733)]
Top words for pretrained_CodeBERT neuron indx 3015 [('remotes', 1.0), ('MULTILINE', 0.7658157030650028), ('purpose', 0.7393348167675791), ('missing', 0.6807979713900345), ('except', 0.6773966968875491)]
Top words for pretrained_CodeBERT neuron indx 5065 [('VALUE', 1.0), ('IsDir', 0.9640115262287966), ('framing', 0.896610861604559), ('add_help', 0.8888199963269667), ('import', 0.8816970829503166)]
Top words for pretrained_CodeBERT neuron indx 7125 [('getNumExecutors', 1.0), ('getdistrictfilestatus', 0.9573077130611827), ('MINIMUM', 0.9386720693838206), ('"_cls"', 0.9294335947720836), ('Simple', 0.9262173674351727)]
Top words for pretrained_CodeBERT neuron indx 3035 [('rosters', 1.0), ('subscriptions', 0.9597278371663411), ('for', 0.9511038101716618), ('protocols', 0.9324291372985539), ('return', 0.8835777985746814)]
Top words for pretrained_CodeBERT neuron indx 9179 [('for', 1.0), ('def', 0.8521444840516195), ('return', 0.7977199349612263), ('Truefrom', 0.7599956388480641), ('from', 0.7580392457418684)]
Top words for pretrained_CodeBERT neuron indx 5104 [('is', 1.0), ('in', 0.9367249346829244), ('rosters', 0.923115959189484), ('not', 0.9099723761566173), ('for', 0.8922410759717361)]
Top words for pretrained_CodeBERT neuron indx 1011 [('spawn', 1.0), ('decimal', 0.8980717801615533), ('define', 0.8367057531521237), ('inc', 0.8245015932069051), ('brightness', 0.8161189492608572)]
Top words for pretrained_CodeBERT neuron indx 7162 [('quality', 1.0), ('broker', 0.888274442554605), ('hidden', 0.8408235024296733), ('by', 0.823322250999815), ('503', 0.7970425805086542)]
Top words for pretrained_CodeBERT neuron indx 7165 [('READY', 1.0), ('acquire', 0.8661819355402673), ('Dispatcher', 0.8295266138827314), ('qdict', 0.8114643157530123), ('fetchall', 0.7553186559554124)]
Top words for pretrained_CodeBERT neuron indx 3074 [('OSError', 1.0), ('import', 0.976015142679592), ('UCache', 0.9324309138465798), ('created', 0.9160458189627791), ('writedata', 0.8948699236563294)]
Top words for pretrained_CodeBERT neuron indx 7186 [('Manager', 1.0), ('DriverBase', 0.9853317440733542), ('not', 0.9744635745538677), ('Action', 0.9479205886288984), ('joinall', 0.9418623466452034)]
Top words for pretrained_CodeBERT neuron indx 3102 [('def', 1.0), ('SUCCESS', 0.7250236441027241), ('loads', 0.7204395135805175), ('reservations', 0.6870073664006598), ('gf', 0.6670212365461556)]
Top words for pretrained_CodeBERT neuron indx 5159 [('SendAndCallForResponse', 1.0), ('117', 0.8879025211688728), ('yield', 0.875928464643186), ('def', 0.843669645897661), ('SendAndWaitForResponse', 0.8380984389900674)]
Top words for pretrained_CodeBERT neuron indx 9260 [('name__startswith', 1.0), ('material', 0.9616434871483549), ('import', 0.8279620900077072), ('ObjectDoesNotExist', 0.8017163049045476), ('assert_called_with', 0.7568782727443172)]
Top words for pretrained_CodeBERT neuron indx 3121 [('OSError', 1.0), ('remotes', 0.8890319675935442), ('gaierror', 0.8310459367804042), ('URLError', 0.8146951583452388), ('LOGGING', 0.7428594135274769)]
Top words for pretrained_CodeBERT neuron indx 9274 [('closing', 1.0), ('null', 0.7510562943860795), ('Invalid', 0.6891000429829236), ('total_pages', 0.6489791540432109), ('statvfs', 0.6208351730012438)]
Top words for pretrained_CodeBERT neuron indx 3133 [('as', 1.0), ('is', 0.8410814586707146), ('dr', 0.8189287950216099), ('else', 0.7798938160686383), ('or', 0.6937681886996342)]
Top words for pretrained_CodeBERT neuron indx 9277 [('is', 1.0), ('google', 0.8962505027428346), ('assert_called_with', 0.8527920574568906), ('if', 0.8523143534307381), ('kls', 0.8506830310805839)]
Top words for pretrained_CodeBERT neuron indx 7230 [('rest', 1.0), ('print_help', 0.8958007722327441), ('getmembers', 0.8137627358652283), ('sendall', 0.8060397512232422), ('CREATING', 0.7763806664256201)]
Top words for pretrained_CodeBERT neuron indx 9292 [('check_password', 1.0), ('hexlify', 0.9955249904935031), ('set_next_path', 0.8830836028496967), ('base_add', 0.8727537837182526), ('RecordLogin', 0.8722222148614038)]
Top words for pretrained_CodeBERT neuron indx 7246 [('class', 1.0), ('import', 0.9013028037329316), ('def', 0.8934088712766527), ('within', 0.8561523387303869), ('created', 0.8507550115149614)]
Top words for pretrained_CodeBERT neuron indx 3152 [('multiple', 1.0), ('scoreplan', 0.9155060885914769), ('Point', 0.8580269087767703), ('0xFF', 0.8039678332151129), ('Update', 0.7989668405133761)]
Top words for pretrained_CodeBERT neuron indx 1113 [('keyword', 1.0), ('constants', 0.9727730587203403), ('simple', 0.9515046759127288), ('fetch', 0.9469952916867701), ('python', 0.9258430425715723)]
Top words for pretrained_CodeBERT neuron indx 1114 [('interfaces', 1.0), ('bindings', 0.8946507311034696), ('adapters', 0.8651107623612918), ('execute', 0.8510557830776267), ('Region', 0.845611587744103)]
Top words for pretrained_CodeBERT neuron indx 7267 [('omit', 1.0), ('NodeBuilder', 0.7902609351403473), ('seen', 0.7417592411459846), ('cond', 0.7246837960658715), ('flat', 0.6889811618814765)]
Top words for pretrained_CodeBERT neuron indx 5220 [('find_all', 1.0), ('Recording', 0.9795220942390133), ('humanize', 0.9699889827890852), ('"100"', 0.9590027159433063), ('quota', 0.9522966311288196)]
Top words for pretrained_CodeBERT neuron indx 5226 [('normpath', 1.0), ('splitext', 0.8684536928891406), ('Cond', 0.8488158578503219), ('Bytes', 0.8423534130445023), ('isalnum', 0.8267323978111722)]
Top words for pretrained_CodeBERT neuron indx 5240 [('_cache', 1.0), ('SERVER_NAME', 0.9443702722116039), ('accept', 0.9288388283287466), ('dictionary', 0.8667091557034827), ('HTTP_PORT', 0.8139525020683556)]
Top words for pretrained_CodeBERT neuron indx 3200 [('"#"', 1.0), ('strict', 0.8439709832316903), ('800', 0.8348852929306899), ('On', 0.8287780119135013), ('"|"', 0.8046359212897302)]
Top words for pretrained_CodeBERT neuron indx 7302 [('acquire', 1.0), ('parameter', 0.8456004869119779), ('notify', 0.8373317006868018), ('django', 0.826657009476018), ('Distance', 0.7819164945243392)]
Top words for pretrained_CodeBERT neuron indx 1170 [('Label', 1.0), ('delete', 0.933304119273161), ('except', 0.8133788505994609), ('displays', 0.8066628568893197), ('Parent', 0.7743124243282544)]
Top words for pretrained_CodeBERT neuron indx 7324 [('with', 1.0), ('closing', 0.9647563404419144), ('success', 0.8936264187893899), ('utc_dt', 0.8795527831228197), ('decode_hex', 0.8642457398841865)]
Top words for pretrained_CodeBERT neuron indx 1199 [('begin', 1.0), ('bindings', 0.924479716798638), ('license', 0.9205041730663717), ('terms', 0.8910680656673913), ('If', 0.8840196763741088)]
Top words for pretrained_CodeBERT neuron indx 7357 [('def', 1.0), ('NotFound', 0.9384232903499996), ('visit', 0.9271383341615789), ('Renderer', 0.8858694658654099), ('exists', 0.8396197373121048)]
Top words for pretrained_CodeBERT neuron indx 3265 [('ll', 1.0), ('Tag', 0.8911869206318396), ('web', 0.8800944526375523), ('year', 0.8697003266300475), ('decorator', 0.8547171823774082)]
Top words for pretrained_CodeBERT neuron indx 1230 [('Input', 1.0), ('14', 0.7020250847902643), ('scheme', 0.6786032799285553), ('18', 0.6367278407293158), ('Output', 0.6353701695636791)]
Top words for pretrained_CodeBERT neuron indx 7379 [('OK', 1.0), ('List', 0.9791776637401158), ('ppn', 0.8775865108621952), ('continue', 0.8645540946415607), ('deleteAll', 0.7988344617112568)]
Top words for pretrained_CodeBERT neuron indx 5339 [('for', 1.0), ('rosters', 0.876733621776576), ('return', 0.801707972231627), ('not', 0.7874836245251858), ('and', 0.7847138550210416)]
Top words for pretrained_CodeBERT neuron indx 9437 [('Failure', 1.0), ('excepts', 0.9870149739190789), ('Else', 0.9817385617190342), ('unlink', 0.9302449394882478), ('boto', 0.9013794011815307)]
Top words for pretrained_CodeBERT neuron indx 1247 [('Cat', 1.0), ('WAY', 0.8419436506764716), ('tell', 0.8003021944834864), ('strict', 0.7647948075169076), ('subscriptions', 0.7640499933304703)]
Top words for pretrained_CodeBERT neuron indx 7395 [('8000', 1.0), ('next', 0.80880745292029), ('within', 0.7846309505818877), ('five', 0.7821082996334106), ('id__in', 0.7760042112661202)]
Top words for pretrained_CodeBERT neuron indx 3306 [('builtins', 1.0), ('for', 0.9935188608196647), ('Struct', 0.8338776358343237), ('decorators', 0.8338315581711501), ('calculators', 0.8217645053089956)]
Top words for pretrained_CodeBERT neuron indx 1262 [('break', 1.0), ('reservations', 0.976934813103749), ('Invalid', 0.924293371094474), ('descriptor', 0.9038726797359188), ('bindings', 0.8992524068709942)]
Top words for pretrained_CodeBERT neuron indx 7408 [('Truefrom', 1.0), ('rosters', 0.9248062025236022), ('else', 0.8875915895588051), ('rjust', 0.8774485006395011), ('Subject', 0.8751841516581147)]
Top words for pretrained_CodeBERT neuron indx 7410 [('advance', 1.0), ('digest', 0.9928133432488826), ('Int', 0.9870657494855773), ('reserve', 0.9828788842885852), ('confirm', 0.9825291235588115)]
Top words for pretrained_CodeBERT neuron indx 7413 [('UBInt16', 1.0), ('Else', 0.9308304863893359), ('cancel', 0.9244944481462256), ('lat', 0.9234301512960735), ('hexdigest', 0.9052735742002692)]
Top words for pretrained_CodeBERT neuron indx 3328 [('unlink', 1.0), ('SUCCESS', 0.9652648131513397), ('current', 0.9627247730602682), ('PointField', 0.8789337622153546), ('8000', 0.8781153551251215)]
Top words for pretrained_CodeBERT neuron indx 1281 [('confirm', 1.0), ('302', 0.9862684979788423), ('except', 0.9584179933736257), ('dir', 0.903234009506203), ('framing', 0.8853074072929608)]
Top words for pretrained_CodeBERT neuron indx 1285 [('WAY', 1.0), ('minute', 0.9792817263154655), ('Site', 0.9774835637587668), ('tornado', 0.904658985855009), ('walk', 0.8885686293059626)]
Top words for pretrained_CodeBERT neuron indx 3335 [('"_cls"', 1.0), ('"application/json"', 0.9675521543171426), ('license', 0.905938793320126), ('future', 0.8790575885312869), ('89.999999999999992', 0.8780133109585748)]
Top words for pretrained_CodeBERT neuron indx 1291 [('tenant', 1.0), ('Bytes', 0.9289866227851197), ('ur', 0.8985492350883761), ('urls', 0.83883610811914), ('extent', 0.7734574843860791)]
Top words for pretrained_CodeBERT neuron indx 3352 [('ping', 1.0), ('span', 0.8587822260353034), ('diverge', 0.8410267364235868), ('spawn', 0.8328958329964654), ('localize', 0.79260271913935)]
Top words for pretrained_CodeBERT neuron indx 1307 [('1970', 1.0), ('Task', 0.9035228302447265), ('entries', 0.792940003928569), ('eventlet', 0.7683526520322805), ('span', 0.7509827394143894)]
Top words for pretrained_CodeBERT neuron indx 3366 [('locals', 1.0), ('Core', 0.9980781635547287), ('find_all', 0.9463601911804588), ('ON', 0.9092963554594875), ('next', 0.8727550457736953)]
Top words for pretrained_CodeBERT neuron indx 1323 [('inc', 1.0), ('187', 0.9581665509693678), ('acquire', 0.8520544792493973), ('required', 0.841501395177689), ('WAYS', 0.8122841839745584)]
Top words for pretrained_CodeBERT neuron indx 5424 [('zeroservices', 1.0), ('Default', 0.9947174571261425), ('62', 0.9465153806008234), ('ImageMath', 0.9431285652863891), ('send_mail', 0.8806654167177378)]
Top words for pretrained_CodeBERT neuron indx 1328 [('within', 1.0), ('LOG', 0.7635388821895209), ('avail', 0.7491225107518757), ('LoadUser', 0.715208018605348), ('None', 0.686936376800488)]
Top words for pretrained_CodeBERT neuron indx 5429 [('se', 1.0), ('kls', 0.8680042241099859), ('tries', 0.8187672124973767), ('sender', 0.8095513984684694), ('117', 0.7986655766704422)]
Top words for pretrained_CodeBERT neuron indx 3384 [('implementation', 1.0), ('class', 0.7861644368406857), ('verbatim', 0.6662020457154347), ('and', 0.6247053417810363), ('presence', 0.6241401550924196)]
Top words for pretrained_CodeBERT neuron indx 3385 [('builtins', 1.0), ('Plan', 0.8133596770110655), ('Subject', 0.794670323455575), ('subscribers', 0.7912791339201056), ('reflection', 0.730389081751459)]
Top words for pretrained_CodeBERT neuron indx 7482 [('probe', 1.0), ('defer', 0.9975818334887583), ('push', 0.9177178631121793), ('presence', 0.8161285608367913), ('callback', 0.7940469550496636)]
Top words for pretrained_CodeBERT neuron indx 5437 [('is', 1.0), ('as', 0.9885604950333822), ('else', 0.8987429466571512), ('fabric', 0.8768961576696376), ('with', 0.8602383068268548)]
Top words for pretrained_CodeBERT neuron indx 3400 [('from', 1.0), ('__members__', 0.9294367536349984), ('Cat', 0.8928246646596274), ('__provides__', 0.848437347302439), ('def', 0.8271841767309597)]
Top words for pretrained_CodeBERT neuron indx 1356 [('and', 1.0), ('is', 0.991790690507562), ('MINIMUM', 0.9672599068640443), ('tornado', 0.966137604359123), ('465', 0.8780415917374458)]
Top words for pretrained_CodeBERT neuron indx 9548 [('"location"', 1.0), ('"datetime"', 0.9250565053994608), ('"debug"', 0.9240679679863417), ('"y"', 0.8828087589699684), ('"critical"', 0.8624097774970373)]
Top words for pretrained_CodeBERT neuron indx 3409 [('do', 1.0), ('geos', 0.780282545042509), ('try', 0.7188816984643572), ('authenticate', 0.7069793182217368), ('ping', 0.6643877704577082)]
Top words for pretrained_CodeBERT neuron indx 1369 [('Failure', 1.0), ('JSON', 0.8494327815039839), ('302', 0.8150541289824639), ('_port', 0.8146012703320397), ('disconnect', 0.8071742328930549)]
Top words for pretrained_CodeBERT neuron indx 3419 [('Input', 1.0), ('Geo', 0.8437540825936398), ('synchronized', 0.816621618073886), ('Clock', 0.7938698714736226), ('itemgetter', 0.7765587396074743)]
Top words for pretrained_CodeBERT neuron indx 3422 [('CREATING', 1.0), ('NotFound', 0.9427493756638783), ('renavigate', 0.8951288391239182), ('TRANSITIVE', 0.8722776570037324), ('Unauthorized', 0.8710957515834539)]
Top words for pretrained_CodeBERT neuron indx 5477 [('42', 1.0), ('normpath', 0.8744151709926284), ('splitext', 0.8300381453163438), ('ip', 0.8046280350101152), ('num', 0.7760199188459652)]
Top words for pretrained_CodeBERT neuron indx 5486 [('Action', 1.0), ('inc', 0.8174313706076481), ('rollback', 0.8050451074912107), ('strict', 0.7946062516266462), ('pxe', 0.7929178656590967)]
Top words for pretrained_CodeBERT neuron indx 3439 [('Invalid', 1.0), ('DEFAULT', 0.8366765062391197), ('accept', 0.7835690868936196), ('venue', 0.7669878002301028), ('quot', 0.7259904357049877)]
Top words for pretrained_CodeBERT neuron indx 7538 [('lat', 1.0), ('shortcuts', 0.8396002743293534), ('FunctionalTest', 0.8357022882460521), ('127', 0.8307074789502279), ('reflection', 0.817677344808148)]
Top words for pretrained_CodeBERT neuron indx 1399 [('28', 1.0), ('ND', 0.8324974540921415), ('Subject', 0.713263273244073), ('deferred', 0.67209095775675), ('File', 0.6576819945135843)]
Top words for pretrained_CodeBERT neuron indx 3452 [('raise', 1.0), ('DESCRIPTOR', 0.8504647433356866), ('UTF8', 0.8274360886875144), ('ND', 0.820756880774833), ('rjust', 0.8015349600892933)]
Top words for pretrained_CodeBERT neuron indx 7577 [('flat', 1.0), ('pywechat', 0.8964292004843317), ('trello', 0.8637729283681053), ('Bytes', 0.8203802081930004), ('36', 0.789627483179755)]
Top words for pretrained_CodeBERT neuron indx 3482 [('NotImplemented', 1.0), ('LOG', 0.9777327123242187), ('setAttr', 0.955300901345465), ('Redirect', 0.8837450707784469), ('microseconds', 0.8766646350832442)]
Top words for pretrained_CodeBERT neuron indx 3498 [('OutputReg', 1.0), ('tostr', 0.9788712625396493), ('0.04', 0.8957685838614492), ('rjust', 0.8956114823931769), ('"\\\'h"', 0.8786970337629296)]
Top words for pretrained_CodeBERT neuron indx 3509 [('primary_key', 1.0), ('inc', 0.9583910570199075), ('Default', 0.901825642316103), ('PRIORITY', 0.8529809559177304), ('VOLUMES_TYPE', 0.8326915762522383)]
Top words for pretrained_CodeBERT neuron indx 7615 [('lat', 1.0), ('exc_val', 0.6877465681622118), ('ctx', 0.6484198956139813), ('WMS', 0.6323962564999496), ('register_conn', 0.6215952163188061)]
Top words for pretrained_CodeBERT neuron indx 3519 [('comments', 1.0), ('signed', 0.9692901202962414), ('from', 0.7878835629822982), ('do', 0.7569733255787907), ('renderers', 0.755631847976231)]
Top words for pretrained_CodeBERT neuron indx 1473 [('blank', 1.0), ('Distance', 0.8131614256042744), ('auditors', 0.8063632770138935), ('push', 0.7862312659051944), ('and', 0.7719575564914215)]
Top words for pretrained_CodeBERT neuron indx 7618 [('visit', 1.0), ('sites', 0.8209055078226902), ('Unregister', 0.7820624796933385), ('unquote', 0.687246025668145), ('__import__', 0.6812142583075569)]
Top words for pretrained_CodeBERT neuron indx 5569 [('ll', 1.0), ('TAG', 0.9883811023133714), ('Wire', 0.9242888344824438), ('Encoder', 0.9012049867938197), ('decorator', 0.8967763080636229)]
Top words for pretrained_CodeBERT neuron indx 7625 [('AddField', 1.0), ('Crawler', 0.9697511739319985), ('lon', 0.8356814979160735), ('rest', 0.8279696690446471), ('Action', 0.814811746013799)]
Top words for pretrained_CodeBERT neuron indx 3530 [('collect', 1.0), ('search', 0.9748473936183546), ('resolve', 0.9198664091262285), ('Parse', 0.9106071062627747), ('convert', 0.9010305516586065)]
Top words for pretrained_CodeBERT neuron indx 5583 [('LOG', 1.0), ('sim', 0.9364096742669766), ('69', 0.933704079870058), ('end_date', 0.9256106778438177), ('187', 0.8579077722808183)]
Top words for pretrained_CodeBERT neuron indx 1498 [('Command', 1.0), ('blocking', 0.7624868295643192), ('tokens', 0.7201807018207912), ('upgrade', 0.6902235745088834), ('Parent', 0.6648416907066701)]
Top words for pretrained_CodeBERT neuron indx 7643 [('for', 1.0), ('from', 0.9453663086779415), ('rosters', 0.8776613825877729), ('def', 0.8687784993632758), ('Truefrom', 0.8655085223743547)]
Top words for pretrained_CodeBERT neuron indx 3550 [('201', 1.0), ('cancel', 0.9282092488945609), ('with', 0.925941229985872), ('rollback', 0.9059531771335386), ('iskeyword', 0.8879374231247245)]
Top words for pretrained_CodeBERT neuron indx 1504 [('package', 1.0), ('installed', 0.8981256446093349), ('negotiate', 0.8861155249261091), ('lb', 0.7985565932829429), ('manager', 0.7875781617290655)]
Top words for pretrained_CodeBERT neuron indx 3555 [('Input', 1.0), ('next', 0.9983970098963308), ('seek', 0.8731153604387395), ('fpath', 0.8368191383724002), ('current', 0.8219703383374679)]
Top words for pretrained_CodeBERT neuron indx 3559 [('"append"', 1.0), ('for', 0.9528317734323648), ('SUCCESS', 0.8702786467858196), ('destroy', 0.8692681627627518), ('NotImplemented', 0.8413412145629715)]
Top words for pretrained_CodeBERT neuron indx 3568 [('rosters', 1.0), ('is', 0.9589087130478433), ('not', 0.9404441327666372), ('else', 0.8605947112858141), ('for', 0.840816290645407)]
Top words for pretrained_CodeBERT neuron indx 5639 [('venue', 1.0), ('URL_REGEX', 0.9130713679284216), ('Invalid', 0.9037143974460403), ('lon__gt', 0.8897775529577996), ('quality', 0.8570670865586699)]
Top words for pretrained_CodeBERT neuron indx 5650 [('Action', 1.0), ('deleteAll', 0.7415551553457248), ('uploadfile', 0.7274730045365213), ('Tab', 0.6954547529010997), ('_meta', 0.6739237716644527)]
Top words for pretrained_CodeBERT neuron indx 1556 [('except', 1.0), ('python', 0.8012081915056187), ('raise', 0.7969569942409342), ('demo', 0.7587668376816673), ('Subject', 0.7484116950166722)]
Top words for pretrained_CodeBERT neuron indx 7710 [('if', 1.0), ('locals', 0.9814586562149303), ('manager', 0.9685057765654216), ('Invalid', 0.9175834232176715), ('MAP_SERVER_PROTOCOL', 0.8737038586767834)]
Top words for pretrained_CodeBERT neuron indx 7712 [('bk', 1.0), ('with', 0.9615372363994726), ('long', 0.884926213499722), ('Geolevel', 0.829864782992982), ('StreamClosed', 0.8218835212770425)]
Top words for pretrained_CodeBERT neuron indx 1581 [('Register', 1.0), ('SUCCESS', 0.9189085110258579), ('resolve', 0.8030731093779858), ('loads', 0.7933113421256723), ('presence', 0.7817616564098381)]
Top words for pretrained_CodeBERT neuron indx 7730 [('119', 1.0), ('sel', 0.9659336439395285), ('127', 0.922224840207518), ('187', 0.9099901134876892), ('sa', 0.9035812591719931)]
Top words for pretrained_CodeBERT neuron indx 3634 [('except', 1.0), ('accept', 0.8782662328522037), ('targets', 0.8584228331417575), ('month', 0.795648981372692), ('plugin', 0.7884124255769812)]
Top words for pretrained_CodeBERT neuron indx 1597 [('as', 1.0), ('is', 0.8557661368150038), ('tv', 0.8133612161971498), ('else', 0.7822770782700476), ('dr', 0.7733979730687119)]
Top words for pretrained_CodeBERT neuron indx 7741 [('is', 1.0), ('else', 0.9844026371428689), ('fabric', 0.9051873457577613), ('as', 0.8574276422269833), ('with', 0.8498314590383179)]
Top words for pretrained_CodeBERT neuron indx 9798 [('wait', 1.0), ('progressBar', 0.9918990164155049), ('BASE_URL', 0.9670658056978834), ('REPORTS_ENABLED', 0.9590932860199184), ('acquire', 0.8950659004751246)]
Top words for pretrained_CodeBERT neuron indx 1608 [('Off', 1.0), ('initial', 0.9096213349328929), ('else', 0.9014563819533693), ('Point', 0.8887661265455669), ('moving', 0.8515703596468064)]
Top words for pretrained_CodeBERT neuron indx 5717 [('117', 1.0), ('127', 0.951493896770451), ('single', 0.8857530441247354), ('187', 0.8494268289946801), ('Output', 0.8420240607169005)]
Top words for pretrained_CodeBERT neuron indx 7768 [('synchronized', 1.0), ('humanize', 0.9808929731048843), ('subpath', 0.9291250842491977), ('qdict', 0.9269765334005324), ('_init', 0.9057066478578576)]
Top words for pretrained_CodeBERT neuron indx 7769 [('success', 1.0), ('FunctionalTest', 0.7861334896968541), ('as', 0.7409540398480302), ('Filter', 0.7347339338876577), ('for', 0.7006535930434915)]
Top words for pretrained_CodeBERT neuron indx 3684 [('strict', 1.0), ('shortcuts', 0.9468044006092101), ('hidden', 0.8063494160805288), ('from', 0.7685221214182258), ('preserve_default', 0.7500688282104945)]
Top words for pretrained_CodeBERT neuron indx 7780 [('single', 1.0), ('seek', 0.8588428301228626), ('makedirs', 0.798073500933609), ('750', 0.7271944413315121), ('completed', 0.7206908760766009)]
Top words for pretrained_CodeBERT neuron indx 9849 [('boto', 1.0), ('closing', 0.9419965577177378), ('SECURITY_RECOVERABLE', 0.7510369467246516), ('next', 0.7025615576599282), ('length_scale', 0.6937229130540348)]
Top words for pretrained_CodeBERT neuron indx 3707 [('rollback', 1.0), ('next', 0.9261901928503881), ('ADVERTISE', 0.8879661006971765), ('Int', 0.8313133919513683), ('REAGGREGATING', 0.8008763964509683)]
Top words for pretrained_CodeBERT neuron indx 9852 [('18', 1.0), ('MIDDLEWARE_CLASSES', 0.739733130150364), ('for', 0.7241246396849139), ('"multiple"', 0.6963057616869719), ('Libvirt', 0.6926756820623546)]
Top words for pretrained_CodeBERT neuron indx 3709 [('Stretch', 1.0), ('ROLE', 0.9328590487577756), ('UTF8', 0.91667054940406), ('Mesh', 0.8750854475658737), ('Padding', 0.8712411000549708)]
Top words for pretrained_CodeBERT neuron indx 3712 [('def', 1.0), ('boto', 0.9578564626388723), ('STP', 0.8170773680766329), ('huffman', 0.7673358041631196), ('authorization', 0.7504548102593842)]
Top words for pretrained_CodeBERT neuron indx 1666 [('NamedTemporaryFile', 1.0), ('Simulator', 0.9407166776010555), ('Job', 0.9399385335298474), ('UnpackingError', 0.9286027746864479), ('RotatingFileHandler', 0.9202454876146079)]
Top words for pretrained_CodeBERT neuron indx 9861 [('for', 1.0), ('symlink', 0.9848305576606953), ('sim', 0.9104392396283474), ('lat', 0.8521547912227022), ('"r"', 0.8395795720222116)]
Top words for pretrained_CodeBERT neuron indx 7814 [('0x1', 1.0), ('def', 0.8612864392087934), ('chr', 0.8544946343195101), ('0x00000001', 0.7743149049206446), ('acquire', 0.7685626584421669)]
Top words for pretrained_CodeBERT neuron indx 1670 [('continue', 1.0), ('realpath', 0.8454596886010506), ('CommentForm', 0.8210264986398478), ('unquote', 0.7516232493909533), ('EGP', 0.7441206023317191)]
Top words for pretrained_CodeBERT neuron indx 7817 [('Int', 1.0), ('tell', 0.8310895547593281), ('auto_index', 0.8180592876428731), ('get_int', 0.8154648781562385), ('milliseconds', 0.7954898251174973)]
Top words for pretrained_CodeBERT neuron indx 7820 [('chunks', 1.0), ('TAGS', 0.981883540126648), ('SOL_SOCKET', 0.9805158065807115), ('headers_in', 0.9695319285411639), ('wait', 0.9539514815509145)]
Top words for pretrained_CodeBERT neuron indx 7823 [('created', 1.0), ('venue', 0.9920363550941503), ('epoch_microseconds', 0.9351204515175631), ('number', 0.9130614220024044), ('assertFalse', 0.8992499420127437)]
Top words for pretrained_CodeBERT neuron indx 3729 [('ppn', 1.0), ('restart', 0.9287005244006203), ('destroy', 0.9043961285606676), ('commit_manually', 0.8636301919583859), ('callback', 0.8550166974683799)]
Top words for pretrained_CodeBERT neuron indx 1687 [('Update', 1.0), ('Error', 0.9803949960664706), ('Process', 0.9438722659909451), ('implementation', 0.9214085269708212), ('auditor', 0.9196045871970483)]
Top words for pretrained_CodeBERT neuron indx 3740 [('SIGINT', 1.0), ('Unauthorized', 0.9981296955296745), ('visit', 0.9689463472040936), ('RemoteExecutionError', 0.9107269599512031), ('Parse', 0.9061572329119053)]
Top words for pretrained_CodeBERT neuron indx 7836 [('Integer', 1.0), ('passwd', 0.9578916388052678), ('POST', 0.9376707742343932), ('86400', 0.8673929589397681), ('GET', 0.8411268576708051)]
Top words for pretrained_CodeBERT neuron indx 7844 [('HTTPRequest', 1.0), ('success', 0.9439560133131071), ('180.0', 0.9046466108761801), ('90.0', 0.8665700582425998), ('UBInt16', 0.8367750458329267)]
Top words for pretrained_CodeBERT neuron indx 3749 [('Signature', 1.0), ('Else', 0.9990815676133474), ('Presence', 0.9428647442317064), ('presence', 0.8766046865589194), ('Input', 0.8582217531184423)]
Top words for pretrained_CodeBERT neuron indx 1725 [('duplicating', 1.0), ('sanetime', 0.9605445935287877), ('builtins', 0.9400678861973532), ('future', 0.8966037322700225), ('watcher', 0.8958729918152588)]
Top words for pretrained_CodeBERT neuron indx 7870 [('rjust', 1.0), ('expanduser', 0.8050483273213417), ('plural', 0.7813222538823712), ('urandom', 0.7575055582951132), ('Failure', 0.7389586548603639)]
Top words for pretrained_CodeBERT neuron indx 7872 [('"HTTP_BEARER_TOKEN"', 1.0), ('BGUI_DEFAULT', 0.9270884275895456), ('sort_mode', 0.9035813194804259), ('SUCCESS', 0.8970809224467843), ('passwd', 0.8845299054508508)]
Top words for pretrained_CodeBERT neuron indx 3780 [('closing', 1.0), ('synchronized', 0.8360928494957803), ('SUCCESS', 0.7864432494249743), ('token_type', 0.7675478457712755), ('with', 0.7318179965194356)]
Top words for pretrained_CodeBERT neuron indx 7877 [('wait', 1.0), ('keyword', 0.9292560414091733), ('b64encode', 0.9069352466096674), ('TextField', 0.8189479305656069), ('listen', 0.7858549429187889)]
Top words for pretrained_CodeBERT neuron indx 1749 [('number', 1.0), ('archive', 0.9861717170404127), ('moderate', 0.9829090350859274), ('outgoing', 0.9785345446059434), ('no', 0.9571543333334022)]
Top words for pretrained_CodeBERT neuron indx 7893 [('127', 1.0), ('MAXACTIVE', 0.9767002548119543), ('MINIMUM', 0.9700560418049412), ('3128', 0.8934102652318603), ('getNumExecutors', 0.8780150137152882)]
Top words for pretrained_CodeBERT neuron indx 5845 [('values_list', 1.0), ('current', 0.9388021415738196), ('tries', 0.8907827371258662), ('maxlength', 0.8719620886798881), ('variable', 0.8560341709206456)]
Top words for pretrained_CodeBERT neuron indx 3803 [('for', 1.0), ('rosters', 0.9257279986238417), ('return', 0.8727659414356154), ('protocols', 0.8532319450791337), ('not', 0.849593074638526)]
Top words for pretrained_CodeBERT neuron indx 9947 [('for', 1.0), ('not', 0.9392840981432138), ('return', 0.9143071476217182), ('Truefrom', 0.8999133897680254), ('protocols', 0.8889212236915991)]
Top words for pretrained_CodeBERT neuron indx 7901 [('sourceSTP', 1.0), ('destSTP', 0.8958527732842428), ('Parameter', 0.8842740476561438), ('DefinitionError', 0.8741608105753425), ('excepts', 0.8474140564907413)]
Top words for pretrained_CodeBERT neuron indx 3806 [('REFERENCES', 1.0), ('Venue', 0.9606168900082549), ('raise', 0.9066066631363082), ('CANCEL', 0.881427557470055), ('reddit', 0.8751973961058709)]
Top words for pretrained_CodeBERT neuron indx 7913 [('within', 1.0), ('construct', 0.9262413415230334), ('in', 0.8956660679623317), ('transfer', 0.8847243582664571), ('Auditor', 0.8613679101551317)]
Top words for pretrained_CodeBERT neuron indx 7920 [('Byte', 1.0), ('hyper', 0.9971386399022244), ('long', 0.8441870011710831), ('mem', 0.779981424085055), ('null', 0.773816621144697)]
Top words for pretrained_CodeBERT neuron indx 5872 [('rosters', 1.0), ('is', 0.9000643474364705), ('in', 0.8931977463342542), ('else', 0.8685713366757325), ('deploy', 0.822071304002501)]
Top words for pretrained_CodeBERT neuron indx 5874 [('try', 1.0), ('listen', 0.9691637655264067), ('reserve', 0.930686718704294), ('ping', 0.9281969506814108), ('pk', 0.9149304613567697)]
Top words for pretrained_CodeBERT neuron indx 1784 [('True', 1.0), ('False', 0.9861257413720057), ('inc', 0.9713338040582321), ('feed', 0.9440341734993997), ('installed', 0.9435963890139326)]
Top words for pretrained_CodeBERT neuron indx 3839 [('get_roster', 1.0), ('forwards', 0.9591236523463329), ('allocate_range_ids', 0.9548076564996588), ('get_health', 0.9391006571603085), ('get_encoder', 0.8969840150908474)]
Top words for pretrained_CodeBERT neuron indx 1800 [('ipaddr', 1.0), ('service', 0.9915503775493045), ('demos', 0.8657511939450339), ('demo', 0.8417857664660449), ('Gallery', 0.8137500959921176)]
Top words for pretrained_CodeBERT neuron indx 1805 [('DOTALL', 1.0), ('remotes', 0.8737512768971977), ('composite', 0.8636781317044772), ('Provider', 0.8367052516364027), ('severity', 0.8281855150462495)]
Top words for pretrained_CodeBERT neuron indx 7954 [('95', 1.0), ('five', 0.873987765571201), ('not', 0.8646548400415537), ('25', 0.8417332136080362), ('75', 0.8033009264510317)]
Top words for pretrained_CodeBERT neuron indx 3871 [('PRIORITY', 1.0), ('Dispatcher', 0.9789702678350317), ('15000', 0.9649974888130588), ('max_size', 0.8793970844011831), ('directionality', 0.85919422166118)]
Top words for pretrained_CodeBERT neuron indx 7969 [('backwards', 1.0), ('accept', 0.8580362458228877), ('BRIGHTNESS', 0.8477796826847018), ('year', 0.8472832755593104), ('detect', 0.8464089141082666)]
Top words for pretrained_CodeBERT neuron indx 1834 [('no', 1.0), ('required', 0.8591697980884935), ('single', 0.8555403751380011), ('raise', 0.8428118804696837), ('continue', 0.8376474279122821)]
Top words for pretrained_CodeBERT neuron indx 3884 [('google', 1.0), ('lstat', 0.8883993310261673), ('NODE', 0.8159170109058238), ('multiple', 0.7572532381483299), ('basestring', 0.7186116011826321)]
Top words for pretrained_CodeBERT neuron indx 7985 [('receiver', 1.0), ('emit', 0.93176791904278), ('epoch_minutes', 0.9289537589574619), ('FileData', 0.9162733356176415), ('epoch_seconds', 0.9119253310501423)]
Top words for pretrained_CodeBERT neuron indx 5942 [('multiple', 1.0), ('ts', 0.8728164009222358), ('period', 0.8628743817024074), ('FILES', 0.7788421981097571), ('omit', 0.7444429615832875)]
Top words for pretrained_CodeBERT neuron indx 1848 [('Int', 1.0), ('microseconds', 0.8267467795077613), ('FILES', 0.78457090598498), ('preload', 0.7295391017654533), ('Integer', 0.6851926531272406)]
Top words for pretrained_CodeBERT neuron indx 5948 [('accept', 1.0), ('handle', 0.8240579932898826), ('"--version"', 0.8207842666975962), ('"--top"', 0.7921947195567313), ('"--include"', 0.7537288420374174)]
Top words for pretrained_CodeBERT neuron indx 3901 [('as', 1.0), ('is', 0.9302335477407233), ('else', 0.8659444171830837), ('with', 0.8050725006328638), ('dr', 0.7906099425683882)]
Top words for pretrained_CodeBERT neuron indx 5964 [('splitext', 1.0), ('locals', 0.7228899133512037), ('deepcopy', 0.6424563259877794), ('MULTILINE', 0.6317946053044352), ('exc_info', 0.5989100163801923)]
Top words for pretrained_CodeBERT neuron indx 8012 [('millis', 1.0), ('mins', 0.8632085853694784), ('unpackers', 0.8467529034806228), ('milliseconds', 0.7767025716369581), ('lgdal', 0.7029091042361457)]
Top words for pretrained_CodeBERT neuron indx 8014 [('WEB_TEMP', 1.0), ('"\\\'"', 0.9225837963105245), ('def', 0.8685100667600506), ('clusterCriterias', 0.866052589446994), ('search', 0.8520475469802642)]
Top words for pretrained_CodeBERT neuron indx 1871 [('fifteen', 1.0), ('On', 0.9900943836033699), ('mins', 0.9698832616226057), ('moving', 0.9469955690631751), ('identical', 0.9416955980167137)]
Top words for pretrained_CodeBERT neuron indx 8016 [('rest', 1.0), ('long', 0.8527558036138325), ('multiple', 0.7933562578039737), ('5001', 0.7901506085139489), ('15000', 0.7634172937059961)]
Top words for pretrained_CodeBERT neuron indx 1877 [('milliseconds', 1.0), ('multiple', 0.8409705131666445), ('REQUEST', 0.8144474497953416), ('timetuple', 0.7968462758715237), ('fifteen', 0.7927286004322934)]
Top words for pretrained_CodeBERT neuron indx 1881 [('store', 1.0), ('300', 0.8907380994554109), ('constants', 0.8432119800204825), ('interfaces', 0.8268278402581505), ('five', 0.8251562736336638)]
Top words for pretrained_CodeBERT neuron indx 8026 [('api_key', 1.0), ('convert', 0.9347024902389973), ('future', 0.9183943728874259), ('hexlify', 0.894103450240499), ('Output', 0.8839270911555603)]
Top words for pretrained_CodeBERT neuron indx 3929 [('comments', 1.0), ('protobuf', 0.9920901507073786), ('sites', 0.9713974187036735), ('audit', 0.8992654619785971), ('rjust', 0.8770398993300667)]
Top words for pretrained_CodeBERT neuron indx 8035 [('omit', 1.0), ('HTTPRequest', 0.760226031797193), ('seen', 0.7276696679990398), ('NodeBuilder', 0.7068250369889877), ('createProviderHeader', 0.6896178170703198)]
Top words for pretrained_CodeBERT neuron indx 8036 [('else', 1.0), ('requires', 0.9366317584827759), ('WMS', 0.9366247575729048), ('AREA', 0.8834796152860007), ('try', 0.8811894598034797)]
Top words for pretrained_CodeBERT neuron indx 1916 [('in', 1.0), ('future', 0.8038156538045977), ('Plan', 0.6634213171428774), ('Padding', 0.6513432286971647), ('512', 0.6462346322491028)]
Top words for pretrained_CodeBERT neuron indx 8064 [('"view"', 1.0), ('Int', 0.9694099012633987), ('"list"', 0.9027840386756979), ('"debug"', 0.8929839643574567), ('"critical"', 0.8863104289183331)]
Top words for pretrained_CodeBERT neuron indx 6016 [('identical', 1.0), ('63', 0.990852232145171), ('synchronized', 0.9680705329884778), ('o2', 0.9459079705525119), ('iskeyword', 0.9393520377751632)]
Top words for pretrained_CodeBERT neuron indx 6030 [('62', 1.0), ('pass', 0.926033211520418), ('collect', 0.8776719070538221), ('VALUE', 0.8275488555489585), ('API', 0.7548216777143179)]
Top words for pretrained_CodeBERT neuron indx 1936 [('exception', 1.0), ('kw', 0.9363906545730889), ('warn', 0.8835364569971712), ('proc', 0.7559047239352109), ('oslo', 0.7526739850489491)]
Top words for pretrained_CodeBERT neuron indx 1938 [('Label', 1.0), ('break', 0.9571123086502504), ('form', 0.9513954001842707), ('is', 0.8373999919204148), ('except', 0.8216436567585972)]
Top words for pretrained_CodeBERT neuron indx 1943 [('Padding', 1.0), ('Document', 0.7888486853105188), ('play', 0.764271577706085), ('MEMBERS', 0.749173274572109), ('each', 0.7393885527542345)]
Top words for pretrained_CodeBERT neuron indx 8089 [('while', 1.0), ('backwards', 0.9846919794127883), ('95', 0.9634064875359452), ('fetch', 0.9615578291847536), ('acquire', 0.9605736945253402)]
Top words for pretrained_CodeBERT neuron indx 3998 [('watchers', 1.0), ('import', 0.7758372977382363), ('Plan', 0.7720357778076969), ('ND', 0.7302987365452343), ('auditor', 0.7145487347737048)]
Top words for pretrained_CodeBERT neuron indx 1980 [('300', 1.0), ('95', 0.986777550198828), ('locals', 0.9813136554973942), ('tornado', 0.9486119647336287), ('17', 0.9440439176533122)]
Top words for pretrained_CodeBERT neuron indx 8134 [('intersects', 1.0), ('oslo', 0.9683790246165567), ('pxe', 0.9672477948229413), ('gis', 0.9524035901906674), ('Region', 0.9380795167437596)]
Top words for pretrained_CodeBERT neuron indx 4040 [('spawn', 1.0), ('else', 0.9473800332651877), ('ping', 0.9282637169910156), ('443', 0.9203373173896158), ('Video', 0.8840874554422341)]
Top words for pretrained_CodeBERT neuron indx 8136 [('trello', 1.0), ('backwards', 0.9982813121292916), ('Invalid', 0.9759843883171798), ('VALUE', 0.9735709817860893), ('truncate', 0.9655723373795163)]
Top words for pretrained_CodeBERT neuron indx 8147 [('if', 1.0), ('zone', 0.9909031007736058), ('5001', 0.9306346066564753), ('List', 0.9222403727157782), ('in', 0.8593360325201417)]
Top words for pretrained_CodeBERT neuron indx 6107 [('for', 1.0), ('rosters', 0.9205548446529055), ('protocols', 0.8256790307392649), ('def', 0.823153556831702), ('from', 0.8040452652491851)]
Top words for pretrained_CodeBERT neuron indx 2021 [('Bytes', 1.0), ('seen', 0.9868113071305704), ('shortcuts', 0.9484036796181797), ('adapters', 0.885612389390078), ('button', 0.884165848683785)]
Top words for pretrained_CodeBERT neuron indx 4074 [('TYPE', 1.0), ('decorators', 0.914237144125162), ('ELEMENT', 0.8755253313110745), ('functional', 0.8571412799228818), ('TAG', 0.8428319352674749)]
Top words for pretrained_CodeBERT neuron indx 6122 [('internet', 1.0), ('Action', 0.9275338261748859), ('mod_python', 0.9061404635608589), ('getError', 0.8544328623647204), ('reddit', 0.8400812275805585)]
Top words for pretrained_CodeBERT neuron indx 2043 [('except', 1.0), ('def', 0.9818734152778076), ('has', 0.9465554111498181), ('DBHOST', 0.8210192590023775), ('Account', 0.7953143813172535)]
Creating control dataset for pretrained_CodeBERT POS tagging task
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0163
Epoch: [2/10], Loss: 0.0110
Epoch: [3/10], Loss: 0.0081
Epoch: [4/10], Loss: 0.0067
Epoch: [5/10], Loss: 0.0060
Epoch: [6/10], Loss: 0.0056
Epoch: [7/10], Loss: 0.0052
Epoch: [8/10], Loss: 0.0049
Epoch: [9/10], Loss: 0.0046
Epoch: [10/10], Loss: 0.0043
Score (accuracy) of the probe: 0.36
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0160
Epoch: [2/10], Loss: 0.0112
Epoch: [3/10], Loss: 0.0081
Epoch: [4/10], Loss: 0.0067
Epoch: [5/10], Loss: 0.0061
Epoch: [6/10], Loss: 0.0056
Epoch: [7/10], Loss: 0.0052
Epoch: [8/10], Loss: 0.0049
Epoch: [9/10], Loss: 0.0046
Epoch: [10/10], Loss: 0.0044
Score (accuracy) of the probe: 0.37
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0162
Epoch: [2/10], Loss: 0.0113
Epoch: [3/10], Loss: 0.0083
Epoch: [4/10], Loss: 0.0069
Epoch: [5/10], Loss: 0.0063
Epoch: [6/10], Loss: 0.0058
Epoch: [7/10], Loss: 0.0055
Epoch: [8/10], Loss: 0.0052
Epoch: [9/10], Loss: 0.0050
Epoch: [10/10], Loss: 0.0047
Score (accuracy) of the probe: 0.37
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0172
Epoch: [2/10], Loss: 0.0123
Epoch: [3/10], Loss: 0.0097
Epoch: [4/10], Loss: 0.0087
Epoch: [5/10], Loss: 0.0084
Epoch: [6/10], Loss: 0.0081
Epoch: [7/10], Loss: 0.0080
Epoch: [8/10], Loss: 0.0079
Epoch: [9/10], Loss: 0.0078
Epoch: [10/10], Loss: 0.0077
Score (accuracy) of the probe: 0.36

The best l1=0, the best l2=0.01 for pretrained_CodeBERT_control_task
Accuracy on the test set of probing pretrained_CodeBERT_control_task of all layers:
Score (accuracy) of the probe: 0.24
Accuracy on the test set of pretrained_CodeBERT_control_task model using the intercept:
Score (accuracy) of the probe: 0.13

pretrained_CodeBERT_control_task Selectivity (Diff. between true task and probing task performance):  0.7394436164927968
----------------------------------------------------------------
