Getting activations from json files. If you need to extract them, run with --extract=True 

Anayzing pretrained_BERT
Loading json activations from bert_activations_train.json...
27457 13.0
Number of tokens:  262021
length of source dictionary:  16215
length of target dictionary:  48
262021
Total instances: 262021
['CMD_TARGET_VIEW', 'Test_user_will_be_removed', 'consume_in_thread', 'SESSION_TIMEOUT', 'CoramChannel', 'subset_indices', 'furious', 'get_server_profiles', 'unittest', '_defaultHandler', 'iteritems', 'inventory_slots', 'ReadValue', '_randomState', 'sd', 'test_retrieve', 'foreign_keys', 'LoadBalancer', 'fixed_to_int', 'test_target_replace_table']
Number of samples:  262021
Stats: Labels with their frequencies in the final set
NAME 84096
NEWLINE 23758
DOT 21385
LPAR 19831
RPAR 18962
KEYWORD 18058
COMMA 15856
EQUAL 12895
COLON 9468
DEDENT 7309
INDENT 5853
NUMBER 4913
LSQB 4180
RSQB 4015
NL 3698
STRING 2312
LBRACE 920
EQEQUAL 739
RBRACE 665
PLUS 583
STAR 395
MINUS 393
PERCENT 364
DOUBLESTAR 245
AT 216
PLUSEQUAL 159
NOTEQUAL 149
GREATER 136
LESS 94
SLASH 80
LESSEQUAL 44
SEMI 44
GREATEREQUAL 41
COMMENT 40
LEFTSHIFT 36
VBAR 21
MINEQUAL 17
ELLIPSIS 14
TILDE 10
AMPER 9
VBAREQUAL 5
STAREQUAL 4
AMPEREQUAL 3
ERRORTOKEN 2
RIGHTSHIFT 1
SLASHEQUAL 1
DOUBLESLASH 1
CIRCUMFLEX 1
pretrained_BERT distribution after trauncating:
{0: 0.768849596357619, 3: 0.16509567650097368, 2: 0.04491721445615703, 1: 0.021137512685250368}
{0: 84096, 3: 18058, 2: 4913, 1: 2312}
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}
Loading json activations from bert_activations_test.json...
30516 13.0
Number of tokens:  312495
length of source dictionary:  15745
length of target dictionary:  47
312495
Total instances: 312495
['DefaultIterSeq', 'mutual_info', 'jmax', 'is_qnan', 'testFormulaStores', '"in3_file"', 'unittest', 'ocmd', 'iteritems', 'maxx', 'region1', 'ex_get_size', 'layer2', 'bb_y', 'universe', 'input_transfered', 'sio', 'sd', 'guess_format', 'spikes2']
Number of samples:  312495
Stats: Labels with their frequencies in the final set
NAME 99754
NEWLINE 26616
DOT 23527
LPAR 22950
RPAR 22271
COMMA 22145
KEYWORD 19432
EQUAL 15019
COLON 9793
NUMBER 9492
DEDENT 7967
LSQB 6634
RSQB 6535
INDENT 6482
NL 3900
STRING 1675
PLUS 1207
MINUS 1099
STAR 1009
EQEQUAL 956
LBRACE 745
RBRACE 640
PERCENT 482
DOUBLESTAR 360
SLASH 339
PLUSEQUAL 217
GREATER 204
NOTEQUAL 184
AT 149
LESS 142
GREATEREQUAL 117
COMMENT 90
LESSEQUAL 77
SEMI 41
DOUBLESLASH 39
MINEQUAL 36
STAREQUAL 33
RIGHTSHIFT 24
AMPER 23
LEFTSHIFT 23
VBAR 21
VBAREQUAL 13
CIRCUMFLEX 11
SLASHEQUAL 7
TILDE 7
ELLIPSIS 7
AMPEREQUAL 1
pretrained_BERT distribution after trauncating:
{0: 0.7652604849907558, 3: 0.14907213489524598, 2: 0.07281765667073255, 1: 0.012849723443265595}
{0: 99754, 3: 19432, 2: 9492, 1: 1675}
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}

The distribution of classes in training after removing repeated tokens between training and tesing:
Counter({3: 3201, 0: 3201, 2: 3201, 1: 1723})
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}
The distribution of classes in testing:
Counter({1: 495, 0: 401, 2: 401})
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}
