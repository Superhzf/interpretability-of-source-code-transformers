Getting activations from json files. If you need to extract them, run with --extract=True 

Anayzing pretrained_BERT
Loading json activations from bert_activations_train.json...
7656 13.0
Number of tokens:  72951
length of source dictionary:  5099
length of target dictionary:  42
72951
Total instances: 72951
['d_down', '"include"', '2150', 'splitLine', 'port_map', '"m_LightmapIndex"', 'signal_handler', 'strong_store', '_tuple', 'firstitem', 'exc_val', 'part', '"SUCCEEDED"', 'route_array', 'deferred', 'oneofs', 'get_image_url', 'overlayImg', 'REQUIRES', 'blank']
Number of samples:  72951
Stats: Labels with their frequencies in the final set
NAME 23204
NEWLINE 6586
DOT 5889
LPAR 5444
RPAR 5263
KEYWORD 5018
COMMA 4364
EQUAL 3897
COLON 2622
DEDENT 1878
INDENT 1593
NUMBER 1356
LSQB 1291
RSQB 1267
NL 1069
STRING 550
LBRACE 293
EQEQUAL 226
RBRACE 211
PLUS 200
PERCENT 109
STAR 94
MINUS 83
AT 61
DOUBLESTAR 60
GREATER 59
PLUSEQUAL 52
NOTEQUAL 44
LEFTSHIFT 31
LESS 29
LESSEQUAL 18
COMMENT 15
GREATEREQUAL 15
SEMI 13
VBAR 12
SLASH 11
TILDE 7
ELLIPSIS 6
AMPER 5
MINEQUAL 3
ERRORTOKEN 2
RIGHTSHIFT 1
pretrained_BERT distribution after trauncating:
{0: 0.7701805629314923, 3: 0.16655602761550717, 2: 0.045007966011683484, 1: 0.01825544344131705}
{0: 23204, 3: 5018, 2: 1356, 1: 550}
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}
Loading json activations from bert_activations_test.json...
5985 13.0
Number of tokens:  64699
length of source dictionary:  3727
length of target dictionary:  42
64699
Total instances: 64699
['backend_default', 'rectify', 'feed_data', 'record_iteration', 'baseio', 'plain_bfs', 'set_nosrc', 'draw', 'show', '_multi_parent_containers', '*', 'check_creation', '"defaultAlphaFileName.h5"', 'group_name', 'x1', 'retries', '"/othergroup1/othergroup2"', 'PIPE', '_p_length', '_get_local_idxs']
Number of samples:  64699
Stats: Labels with their frequencies in the final set
NAME 20893
NEWLINE 5444
DOT 5083
COMMA 4481
RPAR 4279
LPAR 4277
KEYWORD 3639
EQUAL 3459
NUMBER 2218
COLON 1972
LSQB 1939
RSQB 1927
DEDENT 1387
INDENT 1203
NL 540
STRING 324
MINUS 262
PLUS 248
STAR 236
EQEQUAL 141
LBRACE 128
RBRACE 126
SLASH 122
PERCENT 69
DOUBLESTAR 55
PLUSEQUAL 50
GREATER 47
NOTEQUAL 32
COMMENT 22
LESS 20
STAREQUAL 16
GREATEREQUAL 16
AT 10
MINEQUAL 7
SEMI 7
VBAR 6
LESSEQUAL 5
AMPER 3
SLASHEQUAL 2
DOUBLESLASH 2
TILDE 1
LEFTSHIFT 1
pretrained_BERT distribution after trauncating:
{0: 0.7716997857723277, 3: 0.134409396468937, 2: 0.08192361675408141, 1: 0.011967201004653911}
{0: 20893, 3: 3639, 2: 2218, 1: 324}
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}

The distribution of classes in training after removing repeated tokens between training and tesing:
Counter({0: 13506, 3: 1019, 1: 491, 2: 136})
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}
The distribution of classes in testing:
Counter({2: 2039, 0: 1882, 3: 68, 1: 37})
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}

The shape of the training set: (13636, 9984)
The shape of the validation set: (1516, 9984)
The shape of the testing set: (4026, 9984)
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0100
Epoch: [2/10], Loss: 0.0040
Epoch: [3/10], Loss: 0.0011
Epoch: [4/10], Loss: 0.0007
Epoch: [5/10], Loss: 0.0003
Epoch: [6/10], Loss: 0.0002
Epoch: [7/10], Loss: 0.0002
Epoch: [8/10], Loss: 0.0001
Epoch: [9/10], Loss: 0.0001
Epoch: [10/10], Loss: 0.0001
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0101
Epoch: [2/10], Loss: 0.0040
Epoch: [3/10], Loss: 0.0011
Epoch: [4/10], Loss: 0.0006
Epoch: [5/10], Loss: 0.0003
Epoch: [6/10], Loss: 0.0002
Epoch: [7/10], Loss: 0.0002
Epoch: [8/10], Loss: 0.0001
Epoch: [9/10], Loss: 0.0001
Epoch: [10/10], Loss: 0.0001
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0099
Epoch: [2/10], Loss: 0.0042
Epoch: [3/10], Loss: 0.0013
Epoch: [4/10], Loss: 0.0009
Epoch: [5/10], Loss: 0.0006
Epoch: [6/10], Loss: 0.0004
Epoch: [7/10], Loss: 0.0004
Epoch: [8/10], Loss: 0.0004
Epoch: [9/10], Loss: 0.0003
Epoch: [10/10], Loss: 0.0003
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0110
Epoch: [2/10], Loss: 0.0059
Epoch: [3/10], Loss: 0.0030
Epoch: [4/10], Loss: 0.0027
Epoch: [5/10], Loss: 0.0022
Epoch: [6/10], Loss: 0.0020
Epoch: [7/10], Loss: 0.0018
Epoch: [8/10], Loss: 0.0019
Epoch: [9/10], Loss: 0.0020
Epoch: [10/10], Loss: 0.0023
Score (accuracy) of the probe: 0.92

The best l1=0, the best l2=0 for pretrained_BERT
Accuracy on the test set of probing pretrained_BERT of all layers:
Score (accuracy) of the probe: 0.82
{'__OVERALL__': 0.8191753601589667, 'NAME': 0.6179596174282678, 'STRING': 1.0, 'NUMBER': 0.9955860716037274, 'KEYWORD': 1.0}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1163,KW_NAME:0
NAME_KW:161,KW_KW:68
NAME_STRING:381,KW_other:0
NAME_NUMBER:177
NAME_STRING_list:[5, 9, 17, 21, 25, 44, 46, 118, 119, 163, 167, 169, 170, 194, 197, 205, 221, 234, 237, 245, 259, 265, 268, 269, 270, 272, 276, 277, 280, 282, 284, 285, 608, 611, 623, 624, 625, 627, 628, 668, 715, 716, 718, 719, 726, 728, 729, 735, 736, 742, 747, 751, 755, 758, 761, 762, 765, 767, 770, 774, 778, 779, 840, 846, 852, 858, 864, 870, 875, 903, 906, 907, 919, 925, 927, 993, 995, 997, 1006, 1020, 1022, 1023, 1074, 1077, 1194, 1199, 1204, 1211, 1215, 1219, 1224, 1226, 1234, 1235, 1245, 1248, 1258, 1266, 1310, 1323, 1355, 1376, 1395, 1404, 1427, 1432, 1438, 1439, 1453, 1457, 1469, 1484, 1487, 1500, 1507, 1509, 1510, 1667, 1669, 1673, 1675, 1677, 1679, 1681, 1683, 1686, 1688, 1692, 1694, 1713, 1715, 1721, 1724, 1726, 1738, 1740, 1742, 1744, 1748, 1767, 1772, 1774, 1785, 1788, 1790, 1792, 1794, 1796, 1798, 1800, 1803, 1805, 1825, 1833, 1835, 1838, 1840, 1858, 1871, 1876, 1878, 1881, 1883, 1892, 1894, 1911, 1912, 1920, 1923, 1927, 1934, 1963, 1986, 1989, 1991, 1992, 1995, 2002, 2006, 2023, 2039, 2046, 2049, 2072, 2075, 2080, 2083, 2086, 2129, 2140, 2146, 2151, 2154, 2156, 2159, 2163, 2173, 2175, 2182, 2184, 2185, 2352, 2353, 2601, 2606, 2629, 2633, 2637, 2647, 2649, 2654, 2657, 2675, 2679, 2681, 2684, 2686, 2693, 2700, 2703, 2705, 2707, 2708, 2720, 2743, 2746, 2748, 2750, 2753, 2756, 2757, 2773, 2774, 2778, 2779, 2783, 2784, 2785, 2787, 2789, 2794, 2796, 2797, 2800, 2802, 2825, 2826, 2848, 2849, 2857, 2858, 2871, 2884, 2889, 2900, 2905, 2907, 2940, 2952, 2969, 2976, 2977, 2985, 2987, 2989, 2993, 2995, 2997, 3027, 3028, 3030, 3032, 3045, 3046, 3048, 3061, 3063, 3090, 3093, 3134, 3156, 3160, 3193, 3243, 3247, 3254, 3265, 3284, 3288, 3293, 3295, 3296, 3304, 3314, 3318, 3322, 3334, 3336, 3338, 3342, 3344, 3345, 3346, 3348, 3351, 3353, 3361, 3367, 3369, 3373, 3376, 3377, 3378, 3387, 3399, 3400, 3401, 3403, 3406, 3410, 3412, 3413, 3415, 3417, 3418, 3424, 3426, 3428, 3431, 3432, 3433, 3435, 3438, 3441, 3443, 3446, 3447, 3451, 3457, 3459, 3469, 3476, 3514, 3567, 3574, 3577, 3600, 3604, 3606, 3608, 3623, 3625, 3626, 3636, 3666, 3667, 3668, 3698, 3699, 3717, 3724, 3732, 3733, 3743, 3746, 3845, 3893, 3895, 3896, 3904, 3947, 3948, 3954, 3995, 3997, 3999, 4001, 4003, 4005, 4007, 4009]
NAME_NUMBER_list:[166, 193, 233, 262, 275, 292, 293, 660, 669, 676, 744, 748, 794, 910, 939, 947, 958, 964, 971, 1002, 1005, 1033, 1106, 1220, 1227, 1237, 1262, 1341, 1347, 1372, 1396, 1403, 1420, 1436, 1446, 1455, 1460, 1467, 1490, 1498, 1661, 1668, 1672, 1701, 1704, 1728, 1732, 1733, 1737, 1752, 1757, 1778, 1779, 1786, 1789, 1809, 1818, 1828, 1830, 1844, 1847, 1864, 1873, 1887, 1914, 1917, 1921, 1939, 1943, 1944, 1946, 1947, 1949, 1968, 1975, 1976, 1981, 1983, 1999, 2000, 2004, 2009, 2010, 2014, 2017, 2027, 2028, 2029, 2031, 2032, 2033, 2059, 2062, 2063, 2065, 2068, 2078, 2133, 2134, 2138, 2139, 2142, 2143, 2149, 2150, 2153, 2161, 2495, 2498, 2582, 2612, 2659, 2702, 2751, 2760, 2761, 2763, 2764, 2766, 2769, 2770, 2771, 2772, 2812, 2817, 2822, 2855, 2856, 2868, 2879, 2913, 2915, 2916, 2945, 2959, 2970, 3003, 3103, 3118, 3132, 3222, 3224, 3239, 3246, 3249, 3252, 3255, 3263, 3308, 3311, 3317, 3319, 3323, 3329, 3337, 3349, 3358, 3380, 3430, 3436, 3440, 3445, 3543, 3562, 3564, 3573, 3628, 3632, 3633, 3634, 3745, 3846, 3847, 3849, 3879, 3883, 3886]
Accuracy on the test set of pretrained_BERT model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_BERT Layer 0
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0090
Epoch: [2/10], Loss: 0.0053
Epoch: [3/10], Loss: 0.0037
Epoch: [4/10], Loss: 0.0029
Epoch: [5/10], Loss: 0.0024
Epoch: [6/10], Loss: 0.0020
Epoch: [7/10], Loss: 0.0017
Epoch: [8/10], Loss: 0.0015
Epoch: [9/10], Loss: 0.0013
Epoch: [10/10], Loss: 0.0011
Score (accuracy) of the probe: 0.93
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0090
Epoch: [2/10], Loss: 0.0052
Epoch: [3/10], Loss: 0.0037
Epoch: [4/10], Loss: 0.0029
Epoch: [5/10], Loss: 0.0023
Epoch: [6/10], Loss: 0.0020
Epoch: [7/10], Loss: 0.0017
Epoch: [8/10], Loss: 0.0015
Epoch: [9/10], Loss: 0.0013
Epoch: [10/10], Loss: 0.0011
Score (accuracy) of the probe: 0.94
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0093
Epoch: [2/10], Loss: 0.0055
Epoch: [3/10], Loss: 0.0039
Epoch: [4/10], Loss: 0.0031
Epoch: [5/10], Loss: 0.0026
Epoch: [6/10], Loss: 0.0022
Epoch: [7/10], Loss: 0.0019
Epoch: [8/10], Loss: 0.0017
Epoch: [9/10], Loss: 0.0015
Epoch: [10/10], Loss: 0.0014
Score (accuracy) of the probe: 0.93
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0101
Epoch: [2/10], Loss: 0.0066
Epoch: [3/10], Loss: 0.0052
Epoch: [4/10], Loss: 0.0045
Epoch: [5/10], Loss: 0.0041
Epoch: [6/10], Loss: 0.0037
Epoch: [7/10], Loss: 0.0035
Epoch: [8/10], Loss: 0.0033
Epoch: [9/10], Loss: 0.0031
Epoch: [10/10], Loss: 0.0029
Score (accuracy) of the probe: 0.95

The best l1=0, the best l2=0.1 for pretrained_BERT_layer_0
Accuracy on the test set of probing pretrained_BERT_layer_0 of all layers:
Score (accuracy) of the probe: 0.86
{'__OVERALL__': 0.8628912071535022, 'NAME': 0.7518597236981934, 'STRING': 1.0, 'NUMBER': 0.9916625796959294, 'KEYWORD': 0.0}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1415,KW_NAME:67
NAME_KW:162,KW_KW:0
NAME_STRING:254,KW_other:1
NAME_NUMBER:51
NAME_STRING_list:[44, 45, 46, 48, 200, 250, 274, 598, 608, 611, 625, 628, 660, 667, 668, 669, 671, 676, 715, 716, 718, 719, 910, 913, 919, 925, 927, 1017, 1020, 1022, 1023, 1211, 1215, 1217, 1219, 1220, 1224, 1226, 1227, 1234, 1235, 1237, 1320, 1323, 1376, 1395, 1404, 1420, 1453, 1460, 1467, 1484, 1660, 1667, 1669, 1673, 1675, 1677, 1679, 1681, 1683, 1686, 1688, 1692, 1694, 1697, 1713, 1715, 1718, 1724, 1732, 1738, 1740, 1742, 1744, 1748, 1753, 1758, 1762, 1767, 1772, 1785, 1790, 1792, 1794, 1796, 1798, 1800, 1803, 1805, 1810, 1823, 1833, 1835, 1838, 1840, 1858, 1861, 1871, 1876, 1878, 1881, 1883, 1889, 1923, 1968, 2023, 2143, 2150, 2163, 2173, 2175, 2182, 2184, 2495, 2498, 2579, 2582, 2708, 2773, 2775, 2778, 2783, 2785, 2787, 2789, 2794, 2796, 2825, 2826, 2848, 2849, 2889, 2976, 2977, 2985, 2987, 2989, 2993, 2995, 2997, 2999, 3001, 3003, 3005, 3007, 3009, 3156, 3160, 3161, 3193, 3243, 3245, 3246, 3247, 3249, 3251, 3252, 3254, 3255, 3304, 3308, 3311, 3317, 3318, 3319, 3323, 3329, 3332, 3333, 3334, 3346, 3348, 3351, 3353, 3355, 3357, 3361, 3362, 3365, 3367, 3369, 3373, 3376, 3377, 3378, 3380, 3399, 3400, 3401, 3403, 3404, 3406, 3407, 3410, 3412, 3413, 3415, 3417, 3418, 3424, 3425, 3426, 3427, 3428, 3430, 3431, 3432, 3433, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3449, 3450, 3514, 3543, 3545, 3564, 3567, 3574, 3577, 3610, 3626, 3630, 3666, 3667, 3668, 3698, 3699, 3717, 3724, 3732, 3733, 3758, 3845, 3846, 3847, 3849, 3850, 3888, 3893, 3930, 3948, 3954]
NAME_NUMBER_list:[744, 748, 1030, 1033, 1148, 1149, 1151, 1153, 1154, 1155, 1156, 1157, 1163, 1661, 1668, 1670, 1672, 1701, 1704, 1728, 1733, 1735, 1737, 1752, 1757, 1779, 1786, 1787, 1789, 1809, 1818, 1830, 1844, 1847, 1864, 1873, 1887, 1914, 1917, 1921, 1922, 2000, 2004, 2017, 2018, 3263, 3325, 3628, 3632, 3633, 3635]
Accuracy on the test set of pretrained_BERT_layer_0 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_BERT Layer 1
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0094
Epoch: [2/10], Loss: 0.0056
Epoch: [3/10], Loss: 0.0040
Epoch: [4/10], Loss: 0.0032
Epoch: [5/10], Loss: 0.0026
Epoch: [6/10], Loss: 0.0022
Epoch: [7/10], Loss: 0.0019
Epoch: [8/10], Loss: 0.0016
Epoch: [9/10], Loss: 0.0014
Epoch: [10/10], Loss: 0.0013
Score (accuracy) of the probe: 0.94
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0097
Epoch: [2/10], Loss: 0.0058
Epoch: [3/10], Loss: 0.0042
Epoch: [4/10], Loss: 0.0033
Epoch: [5/10], Loss: 0.0027
Epoch: [6/10], Loss: 0.0023
Epoch: [7/10], Loss: 0.0020
Epoch: [8/10], Loss: 0.0017
Epoch: [9/10], Loss: 0.0015
Epoch: [10/10], Loss: 0.0014
Score (accuracy) of the probe: 0.94
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0098
Epoch: [2/10], Loss: 0.0059
Epoch: [3/10], Loss: 0.0043
Epoch: [4/10], Loss: 0.0034
Epoch: [5/10], Loss: 0.0029
Epoch: [6/10], Loss: 0.0025
Epoch: [7/10], Loss: 0.0021
Epoch: [8/10], Loss: 0.0019
Epoch: [9/10], Loss: 0.0017
Epoch: [10/10], Loss: 0.0016
Score (accuracy) of the probe: 0.94
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0104
Epoch: [2/10], Loss: 0.0069
Epoch: [3/10], Loss: 0.0055
Epoch: [4/10], Loss: 0.0047
Epoch: [5/10], Loss: 0.0042
Epoch: [6/10], Loss: 0.0039
Epoch: [7/10], Loss: 0.0036
Epoch: [8/10], Loss: 0.0034
Epoch: [9/10], Loss: 0.0032
Epoch: [10/10], Loss: 0.0030
Score (accuracy) of the probe: 0.96

The best l1=0, the best l2=0.1 for pretrained_BERT_layer_1
Accuracy on the test set of probing pretrained_BERT_layer_1 of all layers:
Score (accuracy) of the probe: 0.84
{'__OVERALL__': 0.8430203676105316, 'NAME': 0.6992561105207227, 'STRING': 1.0, 'NUMBER': 0.9970573810691515, 'KEYWORD': 0.11764705882352941}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1316,KW_NAME:60
NAME_KW:56,KW_KW:8
NAME_STRING:442,KW_other:0
NAME_NUMBER:68
NAME_STRING_list:[44, 45, 46, 48, 152, 159, 169, 172, 179, 184, 187, 196, 197, 200, 204, 205, 209, 215, 216, 221, 236, 237, 240, 244, 245, 250, 265, 268, 269, 272, 274, 276, 280, 281, 282, 285, 290, 292, 294, 598, 608, 609, 611, 624, 625, 627, 628, 660, 667, 668, 669, 671, 705, 709, 712, 715, 716, 718, 719, 729, 736, 747, 778, 802, 808, 809, 811, 812, 816, 817, 819, 910, 913, 915, 919, 925, 927, 948, 949, 1017, 1020, 1022, 1023, 1077, 1079, 1192, 1204, 1211, 1215, 1217, 1219, 1220, 1224, 1226, 1227, 1230, 1234, 1235, 1237, 1270, 1289, 1303, 1320, 1323, 1376, 1395, 1404, 1420, 1453, 1459, 1460, 1467, 1484, 1510, 1660, 1667, 1669, 1671, 1673, 1675, 1677, 1679, 1681, 1683, 1686, 1688, 1692, 1694, 1697, 1713, 1715, 1718, 1721, 1724, 1726, 1732, 1736, 1738, 1740, 1742, 1744, 1748, 1753, 1758, 1762, 1767, 1772, 1774, 1776, 1778, 1785, 1788, 1790, 1792, 1794, 1796, 1798, 1800, 1803, 1805, 1810, 1823, 1825, 1828, 1833, 1835, 1838, 1840, 1858, 1861, 1871, 1876, 1878, 1881, 1883, 1889, 1892, 1923, 1928, 1960, 1966, 1968, 1984, 1986, 1988, 1989, 1990, 1991, 1992, 1994, 1995, 2008, 2023, 2069, 2071, 2072, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2082, 2083, 2084, 2085, 2086, 2143, 2150, 2163, 2173, 2175, 2178, 2180, 2182, 2184, 2495, 2498, 2579, 2582, 2629, 2633, 2637, 2646, 2675, 2679, 2681, 2700, 2703, 2708, 2746, 2747, 2748, 2749, 2750, 2752, 2753, 2757, 2762, 2773, 2775, 2778, 2780, 2781, 2782, 2783, 2785, 2787, 2789, 2794, 2796, 2824, 2825, 2826, 2848, 2849, 2860, 2861, 2866, 2881, 2884, 2889, 2897, 2905, 2907, 2925, 2935, 2936, 2937, 2958, 2976, 2977, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3039, 3041, 3043, 3090, 3107, 3113, 3119, 3124, 3156, 3160, 3161, 3167, 3193, 3225, 3228, 3243, 3245, 3246, 3247, 3249, 3251, 3252, 3254, 3255, 3284, 3296, 3304, 3308, 3311, 3317, 3318, 3319, 3322, 3323, 3326, 3329, 3332, 3333, 3334, 3337, 3338, 3346, 3348, 3351, 3353, 3355, 3357, 3358, 3361, 3362, 3365, 3367, 3369, 3373, 3376, 3377, 3378, 3380, 3396, 3399, 3400, 3401, 3403, 3404, 3406, 3407, 3410, 3412, 3413, 3415, 3417, 3418, 3424, 3425, 3426, 3427, 3428, 3430, 3431, 3432, 3433, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3449, 3450, 3514, 3543, 3545, 3564, 3567, 3574, 3577, 3606, 3610, 3623, 3626, 3627, 3630, 3631, 3636, 3666, 3667, 3668, 3698, 3699, 3717, 3732, 3733, 3758, 3845, 3846, 3847, 3849, 3850, 3888, 3893, 3894, 3895, 3896, 3904, 3930, 3948, 3954, 3995, 3997, 3999, 4001, 4003, 4005, 4007, 4009]
NAME_NUMBER_list:[263, 275, 623, 676, 744, 748, 1148, 1149, 1151, 1153, 1154, 1156, 1157, 1163, 1352, 1661, 1668, 1670, 1672, 1701, 1704, 1710, 1728, 1733, 1735, 1737, 1752, 1757, 1779, 1786, 1787, 1789, 1809, 1814, 1818, 1821, 1830, 1844, 1847, 1850, 1854, 1864, 1873, 1887, 1914, 1917, 1921, 1922, 2000, 2004, 2017, 2018, 2294, 2756, 2965, 3263, 3325, 3539, 3547, 3552, 3555, 3562, 3573, 3628, 3632, 3633, 3635, 3879]
Accuracy on the test set of pretrained_BERT_layer_1 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_BERT Layer 2
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0093
Epoch: [2/10], Loss: 0.0059
Epoch: [3/10], Loss: 0.0043
Epoch: [4/10], Loss: 0.0034
Epoch: [5/10], Loss: 0.0028
Epoch: [6/10], Loss: 0.0024
Epoch: [7/10], Loss: 0.0020
Epoch: [8/10], Loss: 0.0018
Epoch: [9/10], Loss: 0.0016
Epoch: [10/10], Loss: 0.0014
Score (accuracy) of the probe: 0.94
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0094
Epoch: [2/10], Loss: 0.0059
Epoch: [3/10], Loss: 0.0044
Epoch: [4/10], Loss: 0.0035
Epoch: [5/10], Loss: 0.0029
Epoch: [6/10], Loss: 0.0024
Epoch: [7/10], Loss: 0.0021
Epoch: [8/10], Loss: 0.0018
Epoch: [9/10], Loss: 0.0016
Epoch: [10/10], Loss: 0.0014
Score (accuracy) of the probe: 0.94
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0098
Epoch: [2/10], Loss: 0.0063
Epoch: [3/10], Loss: 0.0047
Epoch: [4/10], Loss: 0.0038
Epoch: [5/10], Loss: 0.0031
Epoch: [6/10], Loss: 0.0027
Epoch: [7/10], Loss: 0.0023
Epoch: [8/10], Loss: 0.0021
Epoch: [9/10], Loss: 0.0019
Epoch: [10/10], Loss: 0.0017
Score (accuracy) of the probe: 0.94
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0102
Epoch: [2/10], Loss: 0.0069
Epoch: [3/10], Loss: 0.0056
Epoch: [4/10], Loss: 0.0048
Epoch: [5/10], Loss: 0.0043
Epoch: [6/10], Loss: 0.0039
Epoch: [7/10], Loss: 0.0036
Epoch: [8/10], Loss: 0.0034
Epoch: [9/10], Loss: 0.0032
Epoch: [10/10], Loss: 0.0030
Score (accuracy) of the probe: 0.95

The best l1=0, the best l2=0.1 for pretrained_BERT_layer_2
Accuracy on the test set of probing pretrained_BERT_layer_2 of all layers:
Score (accuracy) of the probe: 0.67
{'__OVERALL__': 0.6676602086438153, 'NAME': 0.3289054197662062, 'STRING': 1.0, 'NUMBER': 0.9941147621383031, 'KEYWORD': 0.07352941176470588}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:619,KW_NAME:62
NAME_KW:111,KW_KW:5
NAME_STRING:398,KW_other:1
NAME_NUMBER:754
NAME_STRING_list:[5, 9, 17, 21, 25, 44, 45, 46, 48, 166, 171, 184, 193, 199, 207, 209, 218, 221, 233, 239, 240, 247, 277, 279, 280, 282, 284, 285, 290, 292, 295, 598, 608, 611, 625, 628, 667, 668, 669, 671, 676, 715, 716, 717, 718, 719, 728, 729, 735, 736, 755, 758, 765, 767, 770, 778, 779, 786, 789, 809, 811, 812, 816, 819, 822, 831, 842, 848, 854, 860, 866, 901, 910, 913, 915, 919, 920, 925, 927, 948, 993, 1017, 1020, 1022, 1023, 1048, 1074, 1077, 1079, 1124, 1139, 1158, 1179, 1182, 1211, 1215, 1217, 1219, 1220, 1224, 1226, 1227, 1234, 1235, 1237, 1270, 1289, 1303, 1307, 1320, 1323, 1361, 1369, 1371, 1376, 1395, 1404, 1420, 1439, 1453, 1460, 1467, 1484, 1506, 1660, 1667, 1669, 1671, 1673, 1675, 1677, 1679, 1681, 1683, 1686, 1688, 1692, 1694, 1697, 1713, 1715, 1718, 1724, 1732, 1738, 1740, 1742, 1744, 1748, 1753, 1758, 1762, 1767, 1772, 1785, 1788, 1790, 1792, 1794, 1796, 1798, 1800, 1803, 1805, 1810, 1823, 1833, 1835, 1838, 1840, 1858, 1871, 1876, 1878, 1881, 1883, 1889, 1923, 1931, 1957, 1958, 1960, 1968, 1984, 1991, 2023, 2069, 2075, 2076, 2083, 2143, 2150, 2163, 2173, 2175, 2182, 2184, 2488, 2495, 2498, 2579, 2582, 2604, 2605, 2668, 2708, 2747, 2748, 2749, 2757, 2773, 2775, 2778, 2781, 2783, 2785, 2787, 2789, 2794, 2796, 2798, 2824, 2825, 2826, 2833, 2843, 2845, 2848, 2849, 2852, 2860, 2861, 2866, 2867, 2868, 2871, 2889, 2890, 2893, 2894, 2897, 2898, 2902, 2905, 2916, 2923, 2925, 2935, 2976, 2977, 2985, 2987, 2989, 2993, 2995, 2997, 2999, 3001, 3003, 3005, 3007, 3009, 3013, 3027, 3037, 3039, 3040, 3041, 3043, 3044, 3046, 3047, 3048, 3081, 3086, 3090, 3107, 3119, 3124, 3125, 3143, 3156, 3159, 3160, 3161, 3167, 3169, 3182, 3193, 3243, 3245, 3246, 3247, 3249, 3251, 3252, 3254, 3255, 3304, 3308, 3311, 3317, 3318, 3319, 3322, 3323, 3329, 3332, 3333, 3334, 3346, 3351, 3353, 3355, 3357, 3361, 3362, 3365, 3367, 3369, 3373, 3376, 3377, 3378, 3380, 3399, 3400, 3401, 3403, 3404, 3406, 3407, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3417, 3418, 3424, 3425, 3426, 3427, 3428, 3430, 3431, 3432, 3433, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3449, 3450, 3514, 3543, 3545, 3564, 3567, 3574, 3577, 3591, 3610, 3626, 3627, 3630, 3666, 3667, 3668, 3698, 3699, 3717, 3732, 3733, 3743, 3758, 3845, 3846, 3847, 3849, 3850, 3888, 3893, 3894, 3930, 3948, 3954]
NAME_NUMBER_list:[118, 119, 152, 156, 159, 163, 169, 170, 172, 175, 176, 177, 179, 187, 190, 191, 196, 197, 198, 200, 203, 204, 205, 206, 213, 215, 216, 217, 231, 236, 237, 238, 243, 244, 245, 246, 250, 258, 259, 260, 262, 263, 267, 270, 271, 272, 273, 274, 275, 281, 294, 405, 406, 413, 612, 623, 624, 626, 627, 629, 672, 673, 674, 675, 680, 682, 683, 684, 688, 690, 700, 704, 705, 708, 709, 712, 714, 726, 742, 744, 747, 748, 761, 762, 781, 792, 793, 794, 795, 796, 797, 799, 800, 801, 802, 806, 807, 808, 813, 817, 823, 827, 834, 836, 837, 839, 840, 845, 846, 852, 857, 858, 863, 864, 869, 870, 872, 875, 878, 881, 884, 885, 886, 887, 888, 893, 895, 896, 897, 903, 904, 906, 907, 949, 995, 997, 1002, 1004, 1006, 1016, 1019, 1021, 1025, 1027, 1028, 1030, 1032, 1033, 1035, 1036, 1040, 1041, 1046, 1050, 1054, 1055, 1062, 1065, 1067, 1072, 1075, 1076, 1084, 1085, 1089, 1090, 1093, 1094, 1113, 1116, 1125, 1128, 1141, 1145, 1147, 1148, 1150, 1151, 1153, 1154, 1155, 1156, 1157, 1160, 1163, 1167, 1170, 1180, 1181, 1184, 1185, 1188, 1189, 1192, 1193, 1199, 1201, 1203, 1204, 1230, 1231, 1232, 1233, 1239, 1241, 1245, 1246, 1248, 1249, 1251, 1252, 1253, 1254, 1255, 1257, 1258, 1260, 1261, 1262, 1265, 1266, 1267, 1268, 1310, 1311, 1312, 1313, 1315, 1325, 1332, 1333, 1335, 1341, 1343, 1344, 1346, 1347, 1352, 1354, 1357, 1370, 1372, 1382, 1386, 1403, 1409, 1417, 1418, 1425, 1427, 1433, 1438, 1441, 1443, 1450, 1452, 1455, 1459, 1464, 1465, 1469, 1472, 1480, 1490, 1495, 1500, 1507, 1508, 1510, 1512, 1661, 1668, 1670, 1672, 1685, 1701, 1704, 1707, 1710, 1721, 1726, 1728, 1733, 1735, 1736, 1737, 1752, 1757, 1774, 1778, 1779, 1786, 1787, 1789, 1802, 1809, 1818, 1825, 1828, 1830, 1837, 1844, 1847, 1850, 1854, 1861, 1864, 1873, 1887, 1892, 1894, 1895, 1897, 1899, 1901, 1908, 1910, 1911, 1912, 1914, 1917, 1919, 1921, 1922, 1928, 1934, 1936, 1937, 1938, 1963, 1964, 1966, 1979, 1980, 1985, 1986, 1987, 1989, 1992, 2000, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2012, 2013, 2017, 2018, 2031, 2045, 2046, 2048, 2049, 2050, 2051, 2052, 2070, 2072, 2078, 2080, 2084, 2086, 2104, 2115, 2121, 2126, 2129, 2130, 2132, 2133, 2134, 2136, 2140, 2141, 2142, 2146, 2147, 2148, 2149, 2151, 2153, 2154, 2155, 2156, 2157, 2159, 2178, 2180, 2294, 2352, 2353, 2354, 2429, 2435, 2450, 2460, 2463, 2468, 2475, 2478, 2483, 2500, 2502, 2504, 2506, 2508, 2512, 2514, 2516, 2522, 2525, 2528, 2538, 2541, 2546, 2553, 2556, 2561, 2564, 2570, 2606, 2611, 2629, 2630, 2631, 2632, 2633, 2634, 2636, 2637, 2638, 2639, 2646, 2647, 2649, 2654, 2657, 2660, 2661, 2663, 2669, 2670, 2671, 2672, 2673, 2675, 2676, 2679, 2681, 2684, 2685, 2686, 2693, 2697, 2700, 2702, 2703, 2704, 2705, 2706, 2707, 2716, 2718, 2719, 2720, 2721, 2731, 2739, 2742, 2746, 2751, 2756, 2760, 2762, 2766, 2770, 2772, 2782, 2790, 2791, 2792, 2797, 2799, 2802, 2806, 2807, 2808, 2813, 2818, 2823, 2838, 2855, 2857, 2872, 2873, 2874, 2875, 2876, 2881, 2884, 2910, 2911, 2912, 2914, 2918, 2919, 2930, 2931, 2932, 2933, 2936, 2937, 2945, 2946, 2947, 2948, 2952, 2953, 2958, 2959, 2960, 2961, 2963, 2965, 2966, 2970, 2971, 2972, 2973, 2982, 2983, 2984, 2986, 2988, 2990, 2991, 2992, 2994, 2996, 2998, 3000, 3002, 3004, 3006, 3008, 3010, 3014, 3045, 3061, 3062, 3063, 3064, 3065, 3067, 3068, 3072, 3075, 3076, 3077, 3078, 3093, 3094, 3095, 3096, 3097, 3098, 3101, 3110, 3113, 3114, 3115, 3116, 3117, 3120, 3122, 3128, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3145, 3147, 3154, 3168, 3172, 3173, 3175, 3178, 3185, 3186, 3191, 3194, 3197, 3199, 3202, 3206, 3213, 3214, 3215, 3223, 3224, 3225, 3228, 3232, 3235, 3237, 3240, 3241, 3257, 3258, 3259, 3260, 3262, 3263, 3264, 3265, 3266, 3267, 3270, 3277, 3278, 3279, 3284, 3286, 3288, 3293, 3295, 3296, 3310, 3313, 3314, 3315, 3316, 3325, 3326, 3327, 3335, 3336, 3337, 3338, 3342, 3344, 3348, 3358, 3366, 3387, 3388, 3389, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3402, 3421, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3506, 3507, 3508, 3510, 3512, 3513, 3532, 3533, 3534, 3535, 3537, 3539, 3540, 3546, 3547, 3552, 3555, 3556, 3561, 3562, 3572, 3573, 3593, 3600, 3604, 3606, 3621, 3623, 3624, 3625, 3628, 3631, 3632, 3633, 3634, 3636, 3639, 3718, 3722, 3729, 3730, 3740, 3745, 3746, 3889, 3895, 3896, 3904, 3908, 3909, 3926, 3946, 3952, 3984, 3985, 3997, 3999, 4001, 4005, 4007, 4009, 4021, 4022]
Accuracy on the test set of pretrained_BERT_layer_2 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_BERT Layer 3
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0089
Epoch: [2/10], Loss: 0.0055
Epoch: [3/10], Loss: 0.0040
Epoch: [4/10], Loss: 0.0032
Epoch: [5/10], Loss: 0.0026
Epoch: [6/10], Loss: 0.0022
Epoch: [7/10], Loss: 0.0019
Epoch: [8/10], Loss: 0.0017
Epoch: [9/10], Loss: 0.0015
Epoch: [10/10], Loss: 0.0013
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0091
Epoch: [2/10], Loss: 0.0056
Epoch: [3/10], Loss: 0.0041
Epoch: [4/10], Loss: 0.0033
Epoch: [5/10], Loss: 0.0027
Epoch: [6/10], Loss: 0.0023
Epoch: [7/10], Loss: 0.0020
Epoch: [8/10], Loss: 0.0017
Epoch: [9/10], Loss: 0.0015
Epoch: [10/10], Loss: 0.0013
Score (accuracy) of the probe: 0.95
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0092
Epoch: [2/10], Loss: 0.0058
Epoch: [3/10], Loss: 0.0043
Epoch: [4/10], Loss: 0.0034
Epoch: [5/10], Loss: 0.0029
Epoch: [6/10], Loss: 0.0025
Epoch: [7/10], Loss: 0.0022
Epoch: [8/10], Loss: 0.0019
Epoch: [9/10], Loss: 0.0017
Epoch: [10/10], Loss: 0.0016
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0101
Epoch: [2/10], Loss: 0.0069
Epoch: [3/10], Loss: 0.0055
Epoch: [4/10], Loss: 0.0048
Epoch: [5/10], Loss: 0.0043
Epoch: [6/10], Loss: 0.0040
Epoch: [7/10], Loss: 0.0037
Epoch: [8/10], Loss: 0.0034
Epoch: [9/10], Loss: 0.0032
Epoch: [10/10], Loss: 0.0030
Score (accuracy) of the probe: 0.97

The best l1=0, the best l2=0.1 for pretrained_BERT_layer_3
Accuracy on the test set of probing pretrained_BERT_layer_3 of all layers:
Score (accuracy) of the probe: 0.67
{'__OVERALL__': 0.6669150521609538, 'NAME': 0.29596174282678, 'STRING': 1.0, 'NUMBER': 0.9941147621383031, 'KEYWORD': 0.9411764705882353}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:557,KW_NAME:4
NAME_KW:89,KW_KW:64
NAME_STRING:461,KW_other:0
NAME_NUMBER:775
NAME_STRING_list:[9, 17, 21, 25, 44, 45, 46, 48, 118, 119, 184, 190, 193, 199, 207, 209, 218, 221, 230, 239, 240, 247, 277, 279, 280, 282, 284, 285, 295, 598, 608, 625, 628, 660, 667, 668, 669, 675, 676, 714, 715, 716, 718, 719, 728, 729, 735, 736, 755, 758, 760, 766, 767, 770, 778, 779, 823, 842, 848, 854, 860, 866, 870, 872, 881, 903, 904, 906, 907, 910, 915, 919, 920, 925, 927, 946, 1007, 1015, 1016, 1017, 1019, 1020, 1021, 1022, 1023, 1040, 1041, 1074, 1079, 1150, 1158, 1160, 1184, 1185, 1211, 1215, 1217, 1219, 1220, 1224, 1226, 1227, 1232, 1234, 1235, 1237, 1245, 1246, 1248, 1249, 1258, 1260, 1263, 1266, 1288, 1289, 1320, 1323, 1361, 1369, 1371, 1376, 1381, 1391, 1393, 1395, 1404, 1407, 1420, 1421, 1433, 1439, 1453, 1460, 1467, 1484, 1487, 1490, 1495, 1510, 1659, 1660, 1667, 1669, 1671, 1673, 1675, 1677, 1679, 1681, 1683, 1686, 1688, 1692, 1694, 1697, 1713, 1715, 1718, 1720, 1722, 1724, 1732, 1736, 1738, 1740, 1742, 1744, 1748, 1753, 1758, 1762, 1767, 1772, 1785, 1788, 1790, 1792, 1794, 1796, 1798, 1800, 1803, 1805, 1810, 1823, 1833, 1835, 1838, 1840, 1858, 1871, 1876, 1878, 1881, 1883, 1889, 1923, 1940, 1945, 1948, 1951, 1968, 1974, 1977, 1984, 1991, 1993, 1995, 2023, 2030, 2037, 2058, 2069, 2073, 2075, 2076, 2078, 2079, 2081, 2083, 2084, 2086, 2088, 2102, 2111, 2135, 2143, 2150, 2163, 2173, 2175, 2177, 2179, 2182, 2184, 2352, 2353, 2468, 2483, 2488, 2495, 2498, 2570, 2579, 2582, 2605, 2668, 2678, 2680, 2682, 2693, 2708, 2748, 2773, 2775, 2778, 2780, 2781, 2783, 2785, 2787, 2789, 2794, 2796, 2824, 2825, 2826, 2833, 2841, 2843, 2845, 2848, 2849, 2857, 2858, 2860, 2861, 2862, 2866, 2867, 2868, 2871, 2875, 2876, 2877, 2889, 2890, 2891, 2893, 2897, 2898, 2905, 2907, 2925, 2926, 2935, 2936, 2937, 2976, 2977, 2985, 2987, 2989, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3013, 3014, 3038, 3039, 3040, 3041, 3043, 3044, 3046, 3047, 3048, 3090, 3111, 3156, 3160, 3161, 3193, 3212, 3214, 3230, 3237, 3243, 3245, 3246, 3247, 3249, 3251, 3252, 3254, 3255, 3267, 3276, 3296, 3304, 3308, 3311, 3317, 3318, 3319, 3323, 3326, 3329, 3332, 3333, 3334, 3337, 3346, 3348, 3351, 3353, 3355, 3357, 3361, 3362, 3365, 3367, 3369, 3373, 3376, 3377, 3378, 3380, 3399, 3400, 3401, 3403, 3404, 3406, 3407, 3410, 3412, 3413, 3415, 3417, 3418, 3424, 3425, 3426, 3427, 3428, 3430, 3431, 3432, 3433, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3449, 3450, 3461, 3471, 3514, 3543, 3545, 3564, 3567, 3574, 3577, 3610, 3626, 3630, 3637, 3653, 3654, 3666, 3667, 3668, 3698, 3699, 3717, 3724, 3732, 3733, 3758, 3845, 3846, 3847, 3849, 3888, 3893, 3930, 3948, 3950, 3954, 3956, 3984, 4001, 4009, 4021]
NAME_NUMBER_list:[5, 152, 156, 159, 160, 163, 164, 166, 168, 169, 170, 171, 175, 187, 189, 191, 192, 196, 197, 198, 200, 204, 205, 206, 213, 215, 216, 217, 231, 232, 233, 236, 237, 238, 244, 245, 246, 250, 255, 258, 259, 260, 262, 263, 269, 270, 271, 272, 273, 275, 281, 283, 286, 289, 294, 405, 406, 413, 603, 612, 623, 624, 626, 627, 629, 672, 673, 674, 680, 682, 683, 684, 686, 688, 690, 700, 702, 704, 705, 708, 709, 712, 726, 742, 744, 748, 751, 761, 762, 764, 765, 792, 793, 794, 795, 797, 799, 800, 801, 802, 806, 811, 817, 819, 822, 827, 831, 834, 836, 837, 839, 840, 845, 846, 852, 857, 858, 863, 864, 869, 875, 877, 878, 884, 885, 887, 888, 892, 893, 894, 895, 896, 897, 898, 901, 913, 939, 956, 970, 971, 992, 993, 994, 995, 997, 1002, 1003, 1004, 1006, 1025, 1027, 1028, 1030, 1033, 1038, 1046, 1050, 1054, 1055, 1062, 1065, 1068, 1072, 1075, 1077, 1085, 1090, 1093, 1094, 1113, 1114, 1116, 1124, 1125, 1139, 1141, 1142, 1145, 1147, 1148, 1153, 1154, 1156, 1157, 1163, 1166, 1167, 1170, 1175, 1179, 1180, 1181, 1182, 1188, 1189, 1199, 1204, 1230, 1233, 1251, 1252, 1254, 1255, 1257, 1261, 1262, 1265, 1267, 1270, 1303, 1307, 1310, 1311, 1312, 1313, 1315, 1316, 1325, 1332, 1333, 1341, 1344, 1346, 1347, 1352, 1354, 1356, 1357, 1362, 1364, 1370, 1372, 1392, 1400, 1403, 1417, 1418, 1419, 1425, 1427, 1428, 1429, 1430, 1436, 1438, 1441, 1443, 1446, 1450, 1452, 1455, 1459, 1464, 1465, 1466, 1469, 1470, 1471, 1472, 1474, 1480, 1481, 1498, 1500, 1502, 1507, 1508, 1661, 1668, 1670, 1672, 1701, 1704, 1707, 1710, 1721, 1726, 1728, 1733, 1735, 1737, 1752, 1757, 1774, 1778, 1779, 1786, 1787, 1789, 1809, 1814, 1818, 1821, 1825, 1828, 1830, 1844, 1847, 1850, 1854, 1861, 1864, 1873, 1887, 1892, 1894, 1895, 1896, 1897, 1899, 1900, 1901, 1908, 1910, 1911, 1912, 1913, 1914, 1916, 1917, 1918, 1921, 1922, 1927, 1928, 1929, 1931, 1934, 1935, 1936, 1937, 1938, 1939, 1941, 1943, 1944, 1946, 1949, 1957, 1958, 1959, 1960, 1962, 1963, 1964, 1965, 1966, 1973, 1975, 1976, 1979, 1981, 1983, 1985, 1987, 1999, 2000, 2001, 2002, 2003, 2004, 2006, 2008, 2009, 2012, 2013, 2014, 2017, 2018, 2020, 2027, 2028, 2029, 2031, 2033, 2034, 2035, 2038, 2040, 2041, 2043, 2045, 2046, 2047, 2048, 2049, 2051, 2052, 2059, 2062, 2063, 2065, 2068, 2070, 2109, 2116, 2119, 2121, 2124, 2126, 2129, 2130, 2131, 2132, 2133, 2134, 2136, 2138, 2140, 2141, 2142, 2144, 2146, 2147, 2148, 2149, 2151, 2153, 2154, 2155, 2156, 2157, 2159, 2160, 2161, 2294, 2354, 2396, 2399, 2412, 2429, 2435, 2450, 2460, 2463, 2475, 2478, 2500, 2502, 2504, 2506, 2508, 2512, 2514, 2516, 2522, 2525, 2528, 2538, 2541, 2546, 2553, 2556, 2561, 2564, 2606, 2611, 2629, 2630, 2631, 2632, 2633, 2634, 2636, 2637, 2638, 2639, 2646, 2654, 2657, 2658, 2660, 2666, 2667, 2671, 2672, 2673, 2675, 2676, 2679, 2681, 2684, 2685, 2686, 2697, 2700, 2703, 2704, 2705, 2706, 2707, 2718, 2719, 2720, 2721, 2743, 2746, 2750, 2751, 2753, 2756, 2760, 2761, 2762, 2763, 2764, 2766, 2767, 2769, 2770, 2771, 2772, 2774, 2779, 2782, 2784, 2788, 2790, 2791, 2792, 2795, 2797, 2798, 2800, 2801, 2802, 2807, 2813, 2818, 2823, 2838, 2855, 2856, 2874, 2879, 2880, 2881, 2884, 2910, 2911, 2912, 2913, 2914, 2915, 2918, 2919, 2930, 2931, 2932, 2933, 2941, 2945, 2948, 2951, 2952, 2953, 2958, 2960, 2962, 2963, 2965, 2973, 2982, 2983, 2984, 2986, 2988, 2990, 2991, 3010, 3028, 3045, 3061, 3062, 3063, 3064, 3065, 3068, 3070, 3072, 3073, 3075, 3076, 3077, 3081, 3093, 3094, 3095, 3096, 3097, 3098, 3101, 3103, 3105, 3107, 3108, 3110, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3122, 3124, 3128, 3129, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3143, 3167, 3168, 3169, 3172, 3173, 3178, 3185, 3186, 3191, 3192, 3195, 3196, 3197, 3198, 3199, 3201, 3202, 3203, 3204, 3206, 3207, 3209, 3215, 3223, 3224, 3225, 3239, 3240, 3241, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3269, 3277, 3278, 3284, 3287, 3288, 3309, 3313, 3314, 3315, 3316, 3325, 3327, 3338, 3342, 3358, 3387, 3388, 3389, 3391, 3393, 3394, 3395, 3397, 3398, 3402, 3420, 3421, 3422, 3423, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3464, 3465, 3466, 3467, 3469, 3470, 3472, 3473, 3475, 3476, 3477, 3506, 3507, 3508, 3510, 3512, 3513, 3532, 3533, 3534, 3535, 3537, 3539, 3540, 3546, 3547, 3552, 3553, 3555, 3556, 3561, 3562, 3572, 3573, 3593, 3600, 3603, 3606, 3608, 3616, 3621, 3622, 3623, 3624, 3625, 3628, 3632, 3633, 3634, 3636, 3639, 3718, 3722, 3743, 3745, 3746, 3886, 3889, 3894, 3895, 3896, 3897, 3904, 3908, 3910, 3926, 3927, 3939, 3946, 3949, 3952, 3955, 3985, 3994, 3995, 4003, 4022]
Accuracy on the test set of pretrained_BERT_layer_3 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_BERT Layer 4
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0088
Epoch: [2/10], Loss: 0.0056
Epoch: [3/10], Loss: 0.0041
Epoch: [4/10], Loss: 0.0032
Epoch: [5/10], Loss: 0.0026
Epoch: [6/10], Loss: 0.0022
Epoch: [7/10], Loss: 0.0019
Epoch: [8/10], Loss: 0.0016
Epoch: [9/10], Loss: 0.0014
Epoch: [10/10], Loss: 0.0013
Score (accuracy) of the probe: 0.95
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0088
Epoch: [2/10], Loss: 0.0055
Epoch: [3/10], Loss: 0.0040
Epoch: [4/10], Loss: 0.0032
Epoch: [5/10], Loss: 0.0026
Epoch: [6/10], Loss: 0.0022
Epoch: [7/10], Loss: 0.0019
Epoch: [8/10], Loss: 0.0016
Epoch: [9/10], Loss: 0.0015
Epoch: [10/10], Loss: 0.0013
Score (accuracy) of the probe: 0.95
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0091
Epoch: [2/10], Loss: 0.0058
Epoch: [3/10], Loss: 0.0043
Epoch: [4/10], Loss: 0.0034
Epoch: [5/10], Loss: 0.0028
Epoch: [6/10], Loss: 0.0024
Epoch: [7/10], Loss: 0.0021
Epoch: [8/10], Loss: 0.0019
Epoch: [9/10], Loss: 0.0017
Epoch: [10/10], Loss: 0.0015
Score (accuracy) of the probe: 0.95
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0099
Epoch: [2/10], Loss: 0.0067
Epoch: [3/10], Loss: 0.0054
Epoch: [4/10], Loss: 0.0047
Epoch: [5/10], Loss: 0.0042
Epoch: [6/10], Loss: 0.0039
Epoch: [7/10], Loss: 0.0036
Epoch: [8/10], Loss: 0.0033
Epoch: [9/10], Loss: 0.0031
Epoch: [10/10], Loss: 0.0030
Score (accuracy) of the probe: 0.96

The best l1=0, the best l2=0.1 for pretrained_BERT_layer_4
Accuracy on the test set of probing pretrained_BERT_layer_4 of all layers:
Score (accuracy) of the probe: 0.77
{'__OVERALL__': 0.765772478887233, 'NAME': 0.5196599362380446, 'STRING': 1.0, 'NUMBER': 0.980872976949485, 'KEYWORD': 1.0}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:978,KW_NAME:0
NAME_KW:30,KW_KW:68
NAME_STRING:388,KW_other:0
NAME_NUMBER:486
NAME_STRING_list:[5, 9, 17, 21, 25, 44, 46, 48, 118, 119, 221, 259, 270, 277, 279, 280, 282, 284, 285, 295, 598, 608, 625, 628, 660, 667, 668, 669, 676, 714, 715, 716, 718, 719, 729, 735, 736, 751, 755, 761, 767, 778, 779, 903, 904, 906, 910, 919, 925, 927, 1007, 1015, 1017, 1020, 1022, 1023, 1077, 1079, 1211, 1215, 1217, 1219, 1220, 1224, 1226, 1227, 1234, 1235, 1237, 1245, 1248, 1255, 1258, 1320, 1323, 1371, 1376, 1393, 1395, 1404, 1405, 1421, 1433, 1438, 1439, 1453, 1457, 1460, 1461, 1473, 1484, 1485, 1487, 1500, 1510, 1659, 1660, 1667, 1669, 1671, 1673, 1675, 1677, 1679, 1681, 1683, 1686, 1688, 1692, 1694, 1697, 1713, 1715, 1718, 1721, 1724, 1736, 1738, 1740, 1742, 1744, 1748, 1753, 1758, 1762, 1767, 1772, 1774, 1785, 1788, 1790, 1792, 1794, 1796, 1798, 1800, 1803, 1805, 1810, 1823, 1833, 1835, 1838, 1840, 1858, 1871, 1874, 1876, 1878, 1881, 1883, 1889, 1912, 1915, 1919, 1920, 1923, 1927, 1928, 1934, 1947, 1951, 1968, 1996, 2002, 2023, 2030, 2046, 2083, 2102, 2104, 2111, 2129, 2135, 2143, 2150, 2156, 2163, 2175, 2182, 2183, 2184, 2352, 2353, 2478, 2495, 2498, 2508, 2512, 2601, 2605, 2647, 2649, 2658, 2659, 2693, 2708, 2737, 2742, 2743, 2773, 2774, 2775, 2779, 2783, 2784, 2787, 2789, 2794, 2796, 2800, 2824, 2825, 2826, 2834, 2843, 2845, 2848, 2849, 2857, 2858, 2861, 2866, 2868, 2889, 2893, 2905, 2907, 2918, 2919, 2925, 2935, 2936, 2937, 2940, 2947, 2952, 2961, 2976, 2985, 2987, 2989, 2993, 2994, 2995, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3013, 3014, 3045, 3046, 3047, 3048, 3075, 3090, 3156, 3160, 3161, 3168, 3193, 3206, 3212, 3230, 3243, 3245, 3246, 3247, 3249, 3251, 3252, 3254, 3255, 3267, 3276, 3304, 3317, 3319, 3322, 3323, 3329, 3334, 3346, 3348, 3351, 3353, 3355, 3357, 3358, 3359, 3361, 3362, 3365, 3367, 3369, 3373, 3376, 3377, 3378, 3399, 3400, 3401, 3403, 3404, 3406, 3407, 3410, 3412, 3413, 3415, 3417, 3418, 3424, 3426, 3427, 3431, 3432, 3433, 3435, 3436, 3438, 3439, 3440, 3441, 3443, 3444, 3445, 3446, 3449, 3450, 3461, 3514, 3543, 3545, 3564, 3567, 3574, 3577, 3606, 3610, 3625, 3626, 3630, 3638, 3653, 3654, 3666, 3667, 3668, 3698, 3699, 3717, 3722, 3724, 3732, 3733, 3743, 3745, 3746, 3758, 3845, 3846, 3847, 3849, 3904, 3917, 3930, 3948, 3950, 3954, 3956, 3997, 3999, 4001, 4005, 4007, 4009, 4021]
NAME_NUMBER_list:[152, 156, 159, 160, 163, 168, 169, 175, 184, 188, 189, 191, 192, 193, 195, 196, 199, 200, 203, 204, 205, 207, 209, 213, 215, 216, 218, 226, 230, 231, 232, 233, 236, 239, 240, 243, 244, 245, 247, 250, 255, 258, 260, 262, 263, 265, 268, 273, 275, 281, 294, 405, 406, 603, 609, 612, 623, 624, 626, 627, 629, 672, 673, 675, 680, 684, 688, 690, 704, 708, 709, 712, 726, 744, 748, 749, 762, 793, 794, 797, 799, 806, 827, 834, 840, 846, 852, 858, 864, 870, 875, 884, 885, 886, 888, 893, 895, 897, 898, 907, 939, 946, 970, 971, 993, 995, 997, 1002, 1004, 1006, 1016, 1019, 1021, 1032, 1033, 1054, 1055, 1071, 1072, 1074, 1075, 1093, 1094, 1130, 1150, 1154, 1157, 1160, 1163, 1166, 1199, 1232, 1233, 1246, 1249, 1253, 1254, 1257, 1260, 1262, 1265, 1267, 1310, 1311, 1313, 1315, 1333, 1339, 1341, 1343, 1346, 1347, 1352, 1354, 1362, 1364, 1370, 1372, 1416, 1418, 1420, 1427, 1428, 1436, 1441, 1446, 1447, 1452, 1455, 1463, 1465, 1467, 1469, 1470, 1472, 1507, 1508, 1661, 1668, 1670, 1672, 1701, 1704, 1707, 1710, 1728, 1732, 1733, 1735, 1737, 1752, 1757, 1779, 1786, 1787, 1789, 1809, 1814, 1818, 1821, 1830, 1844, 1847, 1850, 1854, 1864, 1873, 1887, 1894, 1895, 1899, 1900, 1908, 1910, 1911, 1913, 1914, 1916, 1917, 1918, 1921, 1922, 1935, 1936, 1937, 1946, 1949, 1959, 1963, 1965, 1966, 1973, 1975, 1976, 1980, 1981, 1983, 1986, 1999, 2000, 2003, 2004, 2006, 2008, 2009, 2013, 2014, 2017, 2018, 2020, 2029, 2033, 2048, 2049, 2051, 2052, 2059, 2062, 2063, 2065, 2068, 2087, 2126, 2130, 2131, 2132, 2133, 2134, 2140, 2141, 2142, 2146, 2147, 2149, 2151, 2154, 2157, 2159, 2160, 2161, 2354, 2463, 2606, 2611, 2612, 2629, 2630, 2631, 2632, 2634, 2636, 2637, 2639, 2646, 2652, 2654, 2657, 2666, 2671, 2673, 2675, 2676, 2679, 2681, 2684, 2686, 2700, 2705, 2707, 2716, 2718, 2719, 2720, 2721, 2746, 2751, 2756, 2760, 2766, 2767, 2769, 2770, 2772, 2791, 2792, 2795, 2797, 2798, 2802, 2813, 2818, 2823, 2838, 2855, 2856, 2871, 2876, 2879, 2884, 2891, 2910, 2911, 2912, 2913, 2914, 2915, 2930, 2931, 2932, 2941, 2948, 2951, 2958, 2959, 2960, 2962, 2963, 2965, 2966, 2973, 2982, 2983, 2984, 2986, 2988, 2990, 2991, 2992, 2996, 3010, 3061, 3062, 3063, 3064, 3065, 3068, 3072, 3077, 3093, 3095, 3097, 3098, 3101, 3103, 3108, 3116, 3117, 3118, 3120, 3128, 3132, 3133, 3134, 3135, 3137, 3139, 3154, 3173, 3178, 3185, 3191, 3197, 3199, 3214, 3215, 3228, 3237, 3239, 3240, 3241, 3260, 3262, 3263, 3264, 3265, 3278, 3287, 3288, 3313, 3314, 3316, 3325, 3327, 3387, 3389, 3393, 3397, 3402, 3414, 3421, 3451, 3453, 3455, 3456, 3457, 3459, 3469, 3470, 3476, 3477, 3506, 3510, 3532, 3533, 3534, 3535, 3537, 3539, 3540, 3546, 3547, 3552, 3553, 3555, 3556, 3562, 3572, 3573, 3592, 3593, 3600, 3616, 3620, 3621, 3624, 3627, 3628, 3631, 3632, 3633, 3634, 3635, 3636, 3718, 3879, 3886, 3908, 3910, 3939, 3966, 3974, 3976, 3980, 3984, 3985, 4022]
Accuracy on the test set of pretrained_BERT_layer_4 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_BERT Layer 5
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0088
Epoch: [2/10], Loss: 0.0056
Epoch: [3/10], Loss: 0.0041
Epoch: [4/10], Loss: 0.0032
Epoch: [5/10], Loss: 0.0026
Epoch: [6/10], Loss: 0.0022
Epoch: [7/10], Loss: 0.0019
Epoch: [8/10], Loss: 0.0017
Epoch: [9/10], Loss: 0.0015
Epoch: [10/10], Loss: 0.0013
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0086
Epoch: [2/10], Loss: 0.0055
Epoch: [3/10], Loss: 0.0040
Epoch: [4/10], Loss: 0.0032
Epoch: [5/10], Loss: 0.0026
Epoch: [6/10], Loss: 0.0022
Epoch: [7/10], Loss: 0.0019
Epoch: [8/10], Loss: 0.0016
Epoch: [9/10], Loss: 0.0015
Epoch: [10/10], Loss: 0.0013
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0087
Epoch: [2/10], Loss: 0.0056
Epoch: [3/10], Loss: 0.0041
Epoch: [4/10], Loss: 0.0033
Epoch: [5/10], Loss: 0.0027
Epoch: [6/10], Loss: 0.0024
Epoch: [7/10], Loss: 0.0021
Epoch: [8/10], Loss: 0.0018
Epoch: [9/10], Loss: 0.0016
Epoch: [10/10], Loss: 0.0015
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0095
Epoch: [2/10], Loss: 0.0066
Epoch: [3/10], Loss: 0.0053
Epoch: [4/10], Loss: 0.0046
Epoch: [5/10], Loss: 0.0041
Epoch: [6/10], Loss: 0.0038
Epoch: [7/10], Loss: 0.0035
Epoch: [8/10], Loss: 0.0033
Epoch: [9/10], Loss: 0.0031
Epoch: [10/10], Loss: 0.0029
Score (accuracy) of the probe: 0.97

The best l1=0, the best l2=0.1 for pretrained_BERT_layer_5
Accuracy on the test set of probing pretrained_BERT_layer_5 of all layers:
Score (accuracy) of the probe: 0.72
{'__OVERALL__': 0.7210630899155489, 'NAME': 0.4341126461211477, 'STRING': 1.0, 'NUMBER': 0.971554683668465, 'KEYWORD': 1.0}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:817,KW_NAME:0
NAME_KW:35,KW_KW:68
NAME_STRING:615,KW_other:0
NAME_NUMBER:415
NAME_STRING_list:[5, 9, 17, 21, 25, 44, 46, 48, 118, 119, 164, 184, 189, 221, 259, 265, 270, 273, 277, 279, 280, 282, 284, 285, 293, 294, 295, 405, 406, 598, 608, 611, 624, 625, 628, 660, 667, 668, 669, 676, 680, 682, 683, 684, 715, 716, 718, 719, 726, 749, 751, 760, 762, 766, 793, 797, 806, 837, 840, 870, 878, 881, 887, 893, 895, 898, 903, 904, 906, 907, 910, 919, 925, 927, 946, 953, 955, 956, 958, 962, 964, 974, 995, 997, 1004, 1005, 1016, 1017, 1019, 1020, 1021, 1022, 1023, 1072, 1074, 1093, 1158, 1160, 1196, 1199, 1211, 1219, 1220, 1224, 1226, 1227, 1234, 1235, 1245, 1246, 1248, 1249, 1255, 1258, 1260, 1266, 1314, 1320, 1323, 1331, 1336, 1355, 1358, 1361, 1369, 1371, 1376, 1395, 1404, 1420, 1423, 1427, 1432, 1433, 1438, 1439, 1441, 1452, 1453, 1455, 1457, 1459, 1460, 1467, 1469, 1475, 1482, 1484, 1487, 1494, 1495, 1500, 1507, 1660, 1667, 1669, 1673, 1675, 1677, 1679, 1681, 1686, 1692, 1694, 1697, 1713, 1715, 1718, 1724, 1732, 1738, 1740, 1742, 1744, 1748, 1753, 1762, 1767, 1772, 1774, 1785, 1790, 1792, 1794, 1796, 1798, 1803, 1810, 1823, 1833, 1835, 1838, 1840, 1858, 1871, 1876, 1881, 1889, 1892, 1911, 1912, 1919, 1920, 1923, 1927, 1928, 1934, 1946, 1947, 1948, 1950, 1952, 1962, 1963, 1964, 1976, 1981, 1983, 1984, 1988, 1990, 1994, 1998, 1999, 2002, 2006, 2009, 2010, 2013, 2014, 2020, 2023, 2033, 2034, 2038, 2043, 2046, 2059, 2062, 2064, 2065, 2066, 2068, 2069, 2071, 2074, 2077, 2079, 2082, 2085, 2102, 2111, 2125, 2126, 2129, 2131, 2134, 2135, 2137, 2138, 2140, 2142, 2143, 2146, 2151, 2153, 2154, 2156, 2159, 2161, 2163, 2173, 2175, 2182, 2183, 2184, 2316, 2320, 2329, 2352, 2353, 2463, 2468, 2478, 2495, 2498, 2579, 2582, 2601, 2604, 2605, 2606, 2611, 2630, 2631, 2646, 2647, 2649, 2654, 2657, 2666, 2671, 2693, 2697, 2705, 2707, 2708, 2730, 2731, 2732, 2737, 2742, 2743, 2745, 2747, 2751, 2755, 2761, 2763, 2764, 2767, 2769, 2771, 2773, 2775, 2778, 2783, 2785, 2787, 2789, 2794, 2796, 2800, 2801, 2811, 2816, 2821, 2824, 2826, 2827, 2833, 2834, 2835, 2840, 2841, 2843, 2845, 2848, 2849, 2851, 2852, 2853, 2856, 2857, 2858, 2860, 2862, 2865, 2866, 2867, 2868, 2870, 2871, 2884, 2888, 2889, 2893, 2894, 2895, 2897, 2898, 2899, 2900, 2901, 2902, 2905, 2907, 2913, 2915, 2916, 2918, 2919, 2923, 2925, 2926, 2931, 2932, 2935, 2936, 2937, 2940, 2944, 2947, 2951, 2952, 2958, 2959, 2960, 2963, 2965, 2966, 2969, 2970, 2971, 2976, 2977, 2982, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3028, 3029, 3031, 3032, 3034, 3037, 3038, 3039, 3040, 3042, 3043, 3045, 3046, 3047, 3048, 3075, 3097, 3103, 3108, 3109, 3111, 3121, 3156, 3160, 3161, 3168, 3173, 3174, 3178, 3191, 3193, 3194, 3212, 3214, 3228, 3232, 3237, 3240, 3241, 3243, 3246, 3247, 3249, 3252, 3254, 3255, 3259, 3265, 3276, 3284, 3287, 3288, 3293, 3296, 3304, 3308, 3311, 3314, 3317, 3318, 3319, 3321, 3323, 3329, 3332, 3333, 3334, 3336, 3337, 3342, 3344, 3346, 3348, 3351, 3353, 3355, 3357, 3358, 3361, 3362, 3365, 3366, 3367, 3369, 3373, 3376, 3377, 3378, 3380, 3393, 3397, 3399, 3400, 3401, 3403, 3406, 3410, 3412, 3413, 3415, 3417, 3418, 3424, 3425, 3426, 3427, 3428, 3430, 3431, 3432, 3433, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3449, 3450, 3461, 3469, 3471, 3506, 3507, 3508, 3512, 3513, 3514, 3543, 3545, 3564, 3598, 3602, 3603, 3606, 3625, 3626, 3637, 3638, 3653, 3654, 3666, 3667, 3668, 3698, 3699, 3717, 3722, 3724, 3732, 3733, 3743, 3745, 3746, 3845, 3846, 3847, 3849, 3896, 3904, 3930, 3948, 3950, 3954, 3956, 3984, 3997, 3999, 4001, 4005, 4007, 4009, 4021, 4022]
NAME_NUMBER_list:[152, 159, 163, 170, 177, 191, 192, 198, 206, 231, 232, 238, 258, 262, 263, 275, 281, 286, 287, 289, 292, 603, 623, 673, 687, 688, 704, 708, 712, 727, 734, 744, 748, 756, 768, 777, 792, 794, 795, 799, 801, 815, 827, 834, 846, 852, 858, 864, 875, 885, 888, 892, 897, 913, 920, 939, 945, 947, 949, 959, 965, 968, 971, 993, 1002, 1006, 1027, 1033, 1035, 1036, 1044, 1054, 1075, 1076, 1077, 1079, 1106, 1107, 1108, 1144, 1150, 1157, 1163, 1230, 1231, 1233, 1239, 1252, 1254, 1257, 1262, 1265, 1267, 1307, 1310, 1315, 1325, 1332, 1333, 1335, 1339, 1341, 1346, 1347, 1354, 1356, 1357, 1362, 1364, 1370, 1372, 1382, 1396, 1417, 1418, 1419, 1424, 1425, 1426, 1428, 1429, 1430, 1431, 1437, 1446, 1458, 1464, 1465, 1468, 1470, 1471, 1472, 1476, 1477, 1480, 1481, 1488, 1508, 1509, 1512, 1661, 1668, 1670, 1672, 1685, 1690, 1701, 1704, 1707, 1710, 1711, 1728, 1733, 1735, 1737, 1746, 1750, 1752, 1755, 1757, 1760, 1765, 1770, 1776, 1779, 1786, 1787, 1789, 1807, 1809, 1818, 1830, 1844, 1847, 1854, 1864, 1873, 1874, 1885, 1887, 1894, 1895, 1901, 1914, 1916, 1917, 1918, 1921, 1922, 1935, 1936, 1937, 1938, 1939, 1941, 1943, 1949, 1956, 1960, 1966, 1968, 1973, 1975, 1980, 1986, 2000, 2003, 2004, 2005, 2008, 2011, 2015, 2017, 2018, 2027, 2028, 2029, 2031, 2032, 2035, 2039, 2040, 2041, 2044, 2048, 2049, 2051, 2052, 2053, 2063, 2067, 2087, 2130, 2132, 2133, 2136, 2139, 2141, 2147, 2148, 2149, 2150, 2152, 2155, 2157, 2158, 2160, 2162, 2180, 2187, 2290, 2294, 2296, 2354, 2396, 2399, 2612, 2629, 2632, 2634, 2636, 2639, 2648, 2652, 2659, 2660, 2676, 2700, 2717, 2718, 2720, 2721, 2746, 2756, 2760, 2766, 2770, 2772, 2774, 2779, 2784, 2790, 2791, 2797, 2802, 2807, 2812, 2817, 2822, 2838, 2855, 2873, 2879, 2910, 2914, 2930, 2941, 2948, 2962, 2967, 2973, 2983, 3061, 3062, 3063, 3065, 3072, 3076, 3081, 3093, 3095, 3096, 3105, 3113, 3116, 3118, 3120, 3124, 3128, 3132, 3134, 3137, 3175, 3179, 3185, 3186, 3187, 3197, 3199, 3203, 3213, 3215, 3222, 3238, 3239, 3257, 3261, 3262, 3263, 3264, 3269, 3277, 3278, 3286, 3292, 3309, 3312, 3313, 3325, 3327, 3341, 3345, 3349, 3359, 3387, 3389, 3402, 3421, 3451, 3453, 3455, 3457, 3458, 3459, 3464, 3470, 3476, 3510, 3532, 3533, 3534, 3535, 3537, 3539, 3540, 3546, 3547, 3552, 3553, 3555, 3556, 3561, 3562, 3572, 3573, 3592, 3593, 3621, 3628, 3632, 3633, 3634, 3635, 3639, 3640, 3718, 3730, 3740, 3878, 3879, 3883, 3886, 3910, 3927, 3946, 3947, 3949, 3952, 3953, 3966, 3972, 3974, 3976, 3980, 3992]
Accuracy on the test set of pretrained_BERT_layer_5 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_BERT Layer 6
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0084
Epoch: [2/10], Loss: 0.0051
Epoch: [3/10], Loss: 0.0036
Epoch: [4/10], Loss: 0.0029
Epoch: [5/10], Loss: 0.0023
Epoch: [6/10], Loss: 0.0019
Epoch: [7/10], Loss: 0.0017
Epoch: [8/10], Loss: 0.0014
Epoch: [9/10], Loss: 0.0013
Epoch: [10/10], Loss: 0.0011
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0083
Epoch: [2/10], Loss: 0.0050
Epoch: [3/10], Loss: 0.0036
Epoch: [4/10], Loss: 0.0028
Epoch: [5/10], Loss: 0.0023
Epoch: [6/10], Loss: 0.0019
Epoch: [7/10], Loss: 0.0017
Epoch: [8/10], Loss: 0.0015
Epoch: [9/10], Loss: 0.0013
Epoch: [10/10], Loss: 0.0011
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0087
Epoch: [2/10], Loss: 0.0054
Epoch: [3/10], Loss: 0.0039
Epoch: [4/10], Loss: 0.0031
Epoch: [5/10], Loss: 0.0026
Epoch: [6/10], Loss: 0.0022
Epoch: [7/10], Loss: 0.0019
Epoch: [8/10], Loss: 0.0017
Epoch: [9/10], Loss: 0.0015
Epoch: [10/10], Loss: 0.0014
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0094
Epoch: [2/10], Loss: 0.0063
Epoch: [3/10], Loss: 0.0051
Epoch: [4/10], Loss: 0.0044
Epoch: [5/10], Loss: 0.0040
Epoch: [6/10], Loss: 0.0037
Epoch: [7/10], Loss: 0.0034
Epoch: [8/10], Loss: 0.0032
Epoch: [9/10], Loss: 0.0030
Epoch: [10/10], Loss: 0.0028
Score (accuracy) of the probe: 0.97

The best l1=0, the best l2=0.1 for pretrained_BERT_layer_6
Accuracy on the test set of probing pretrained_BERT_layer_6 of all layers:
Score (accuracy) of the probe: 0.75
{'__OVERALL__': 0.7468951813214109, 'NAME': 0.48034006376195537, 'STRING': 1.0, 'NUMBER': 0.9798921039725356, 'KEYWORD': 1.0}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:904,KW_NAME:0
NAME_KW:9,KW_KW:68
NAME_STRING:694,KW_other:0
NAME_NUMBER:275
NAME_STRING_list:[9, 17, 21, 25, 44, 45, 46, 48, 118, 119, 152, 163, 164, 165, 170, 190, 191, 192, 198, 206, 217, 221, 222, 231, 232, 238, 246, 251, 258, 259, 270, 273, 277, 279, 280, 282, 284, 285, 286, 287, 293, 295, 405, 406, 598, 608, 611, 623, 625, 628, 660, 667, 668, 669, 671, 675, 676, 680, 700, 704, 708, 715, 716, 718, 719, 728, 729, 735, 736, 749, 760, 761, 766, 778, 779, 792, 793, 795, 797, 799, 801, 806, 823, 827, 834, 837, 840, 842, 846, 848, 852, 854, 858, 860, 864, 866, 870, 872, 875, 878, 881, 885, 887, 888, 893, 895, 898, 903, 904, 906, 907, 910, 915, 919, 920, 925, 927, 946, 970, 1002, 1005, 1006, 1015, 1016, 1017, 1019, 1020, 1021, 1022, 1023, 1040, 1041, 1046, 1054, 1072, 1074, 1075, 1084, 1085, 1089, 1090, 1093, 1106, 1150, 1158, 1160, 1184, 1185, 1196, 1211, 1215, 1217, 1219, 1220, 1224, 1226, 1227, 1232, 1233, 1234, 1235, 1237, 1245, 1246, 1248, 1249, 1253, 1254, 1255, 1257, 1258, 1260, 1263, 1265, 1266, 1267, 1270, 1310, 1312, 1315, 1323, 1331, 1332, 1336, 1343, 1344, 1354, 1355, 1357, 1358, 1361, 1369, 1371, 1376, 1379, 1381, 1393, 1395, 1396, 1404, 1418, 1420, 1421, 1423, 1427, 1432, 1433, 1437, 1438, 1439, 1441, 1452, 1453, 1457, 1460, 1461, 1465, 1467, 1469, 1472, 1473, 1475, 1478, 1480, 1482, 1484, 1487, 1494, 1495, 1499, 1500, 1506, 1507, 1509, 1510, 1660, 1667, 1669, 1673, 1675, 1677, 1681, 1686, 1692, 1694, 1697, 1713, 1718, 1724, 1732, 1738, 1740, 1742, 1744, 1748, 1753, 1762, 1767, 1772, 1785, 1790, 1792, 1794, 1796, 1798, 1803, 1805, 1810, 1833, 1835, 1838, 1840, 1858, 1871, 1874, 1876, 1878, 1881, 1883, 1889, 1894, 1895, 1908, 1911, 1912, 1923, 1934, 1940, 1941, 1946, 1947, 1948, 1951, 1954, 1962, 1963, 1964, 1968, 1980, 1982, 1984, 1988, 1990, 1991, 1992, 1994, 1995, 1996, 1998, 1999, 2002, 2006, 2013, 2015, 2023, 2024, 2030, 2034, 2037, 2038, 2039, 2042, 2043, 2046, 2049, 2053, 2060, 2061, 2069, 2071, 2074, 2077, 2079, 2082, 2085, 2095, 2102, 2104, 2111, 2126, 2129, 2131, 2133, 2135, 2137, 2140, 2143, 2145, 2146, 2149, 2150, 2151, 2154, 2156, 2157, 2159, 2163, 2169, 2173, 2175, 2182, 2183, 2184, 2185, 2316, 2320, 2329, 2338, 2352, 2353, 2463, 2478, 2495, 2498, 2579, 2582, 2601, 2603, 2604, 2605, 2606, 2629, 2630, 2631, 2632, 2634, 2636, 2638, 2639, 2647, 2649, 2657, 2659, 2660, 2668, 2670, 2671, 2673, 2676, 2680, 2683, 2693, 2697, 2705, 2707, 2708, 2716, 2718, 2720, 2730, 2731, 2732, 2737, 2742, 2743, 2771, 2773, 2775, 2778, 2783, 2785, 2787, 2788, 2789, 2794, 2796, 2800, 2824, 2826, 2827, 2834, 2835, 2841, 2843, 2848, 2849, 2851, 2852, 2857, 2858, 2860, 2868, 2888, 2889, 2893, 2894, 2895, 2897, 2898, 2899, 2902, 2916, 2918, 2919, 2925, 2931, 2935, 2936, 2937, 2940, 2944, 2945, 2951, 2952, 2958, 2959, 2960, 2963, 2966, 2969, 2976, 2977, 2982, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3002, 3004, 3005, 3006, 3008, 3010, 3014, 3029, 3038, 3039, 3040, 3046, 3048, 3061, 3063, 3065, 3067, 3072, 3075, 3078, 3093, 3097, 3108, 3120, 3132, 3134, 3156, 3160, 3161, 3168, 3169, 3173, 3178, 3191, 3193, 3212, 3214, 3228, 3230, 3232, 3236, 3237, 3240, 3241, 3243, 3245, 3246, 3247, 3249, 3251, 3252, 3254, 3255, 3257, 3258, 3259, 3262, 3264, 3265, 3267, 3276, 3278, 3287, 3304, 3308, 3311, 3313, 3314, 3316, 3317, 3318, 3319, 3321, 3323, 3326, 3327, 3329, 3332, 3333, 3334, 3337, 3341, 3346, 3348, 3349, 3351, 3353, 3355, 3357, 3358, 3359, 3361, 3362, 3365, 3367, 3369, 3373, 3376, 3377, 3378, 3380, 3387, 3389, 3391, 3392, 3393, 3397, 3399, 3400, 3401, 3402, 3403, 3404, 3406, 3407, 3410, 3412, 3413, 3415, 3417, 3418, 3421, 3424, 3425, 3426, 3427, 3428, 3430, 3431, 3432, 3433, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3449, 3450, 3451, 3453, 3455, 3457, 3459, 3461, 3465, 3469, 3470, 3471, 3476, 3506, 3514, 3543, 3545, 3564, 3567, 3574, 3577, 3598, 3606, 3608, 3610, 3625, 3626, 3637, 3638, 3653, 3654, 3666, 3667, 3668, 3698, 3699, 3717, 3718, 3724, 3732, 3733, 3743, 3745, 3746, 3758, 3845, 3846, 3847, 3849, 3888, 3915, 3917, 3930, 3948, 3950, 3954, 3956, 3984, 3985, 4021, 4022]
NAME_NUMBER_list:[5, 159, 168, 262, 263, 275, 281, 289, 292, 603, 690, 712, 726, 744, 748, 751, 756, 762, 768, 794, 836, 839, 845, 857, 884, 886, 892, 897, 913, 939, 945, 947, 956, 962, 971, 995, 997, 1004, 1044, 1076, 1077, 1079, 1094, 1107, 1108, 1252, 1261, 1262, 1268, 1271, 1333, 1335, 1339, 1341, 1346, 1347, 1348, 1362, 1364, 1370, 1372, 1382, 1419, 1425, 1426, 1429, 1436, 1446, 1450, 1455, 1456, 1466, 1468, 1471, 1477, 1488, 1498, 1508, 1661, 1668, 1672, 1704, 1728, 1733, 1737, 1757, 1776, 1779, 1786, 1789, 1830, 1847, 1864, 1896, 1901, 1914, 1916, 1917, 1918, 1920, 1921, 1922, 1928, 1929, 1932, 1936, 1938, 1939, 1949, 1950, 1960, 1965, 1966, 1973, 1975, 1976, 1981, 1983, 1985, 1986, 1987, 2000, 2004, 2009, 2010, 2011, 2014, 2017, 2020, 2027, 2028, 2029, 2031, 2032, 2033, 2035, 2036, 2040, 2041, 2051, 2059, 2062, 2063, 2064, 2065, 2067, 2068, 2070, 2087, 2109, 2130, 2134, 2138, 2139, 2141, 2142, 2147, 2152, 2153, 2155, 2160, 2161, 2231, 2290, 2354, 2399, 2412, 2612, 2648, 2652, 2672, 2717, 2760, 2761, 2763, 2764, 2766, 2767, 2769, 2770, 2772, 2784, 2790, 2797, 2801, 2807, 2812, 2817, 2822, 2855, 2856, 2872, 2873, 2879, 2913, 2914, 2915, 2930, 2932, 2941, 2967, 2970, 2983, 3045, 3047, 3062, 3076, 3096, 3098, 3103, 3111, 3113, 3116, 3118, 3124, 3125, 3128, 3131, 3136, 3137, 3167, 3196, 3197, 3207, 3222, 3239, 3260, 3263, 3277, 3312, 3325, 3345, 3464, 3468, 3533, 3534, 3537, 3539, 3540, 3546, 3547, 3553, 3555, 3556, 3561, 3562, 3572, 3573, 3592, 3593, 3616, 3620, 3621, 3624, 3627, 3628, 3632, 3633, 3634, 3635, 3639, 3640, 3730, 3879, 3883, 3886, 3910, 3911, 3926, 3939, 3946, 3949, 3955, 3966, 3980]
Accuracy on the test set of pretrained_BERT_layer_6 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_BERT Layer 7
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0078
Epoch: [2/10], Loss: 0.0047
Epoch: [3/10], Loss: 0.0033
Epoch: [4/10], Loss: 0.0026
Epoch: [5/10], Loss: 0.0021
Epoch: [6/10], Loss: 0.0017
Epoch: [7/10], Loss: 0.0015
Epoch: [8/10], Loss: 0.0013
Epoch: [9/10], Loss: 0.0011
Epoch: [10/10], Loss: 0.0010
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0080
Epoch: [2/10], Loss: 0.0048
Epoch: [3/10], Loss: 0.0034
Epoch: [4/10], Loss: 0.0027
Epoch: [5/10], Loss: 0.0021
Epoch: [6/10], Loss: 0.0018
Epoch: [7/10], Loss: 0.0015
Epoch: [8/10], Loss: 0.0013
Epoch: [9/10], Loss: 0.0012
Epoch: [10/10], Loss: 0.0010
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0081
Epoch: [2/10], Loss: 0.0049
Epoch: [3/10], Loss: 0.0035
Epoch: [4/10], Loss: 0.0028
Epoch: [5/10], Loss: 0.0023
Epoch: [6/10], Loss: 0.0020
Epoch: [7/10], Loss: 0.0017
Epoch: [8/10], Loss: 0.0015
Epoch: [9/10], Loss: 0.0014
Epoch: [10/10], Loss: 0.0013
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0089
Epoch: [2/10], Loss: 0.0060
Epoch: [3/10], Loss: 0.0048
Epoch: [4/10], Loss: 0.0042
Epoch: [5/10], Loss: 0.0038
Epoch: [6/10], Loss: 0.0035
Epoch: [7/10], Loss: 0.0032
Epoch: [8/10], Loss: 0.0030
Epoch: [9/10], Loss: 0.0029
Epoch: [10/10], Loss: 0.0027
Score (accuracy) of the probe: 0.97

The best l1=0, the best l2=0.1 for pretrained_BERT_layer_7
Accuracy on the test set of probing pretrained_BERT_layer_7 of all layers:
Score (accuracy) of the probe: 0.75
{'__OVERALL__': 0.7488822652757079, 'NAME': 0.573326248671626, 'STRING': 1.0, 'NUMBER': 0.8979892103972535, 'KEYWORD': 1.0}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1079,KW_NAME:0
NAME_KW:14,KW_KW:68
NAME_STRING:609,KW_other:0
NAME_NUMBER:180
NAME_STRING_list:[5, 9, 17, 44, 46, 48, 118, 119, 165, 190, 192, 194, 232, 234, 259, 270, 286, 293, 295, 411, 413, 608, 611, 625, 628, 660, 667, 668, 669, 674, 676, 680, 683, 684, 704, 705, 709, 715, 716, 718, 719, 726, 728, 729, 734, 735, 736, 737, 749, 751, 761, 762, 778, 779, 840, 846, 852, 858, 864, 870, 875, 878, 881, 887, 893, 896, 897, 898, 903, 904, 906, 907, 910, 919, 920, 925, 927, 946, 953, 956, 962, 970, 1002, 1004, 1006, 1007, 1017, 1019, 1020, 1022, 1023, 1040, 1072, 1077, 1079, 1084, 1089, 1093, 1106, 1107, 1184, 1194, 1196, 1199, 1211, 1215, 1217, 1219, 1220, 1224, 1226, 1227, 1233, 1234, 1235, 1237, 1245, 1246, 1248, 1249, 1252, 1255, 1258, 1260, 1266, 1270, 1322, 1323, 1332, 1355, 1358, 1376, 1379, 1393, 1395, 1396, 1404, 1409, 1418, 1420, 1421, 1423, 1427, 1428, 1433, 1438, 1439, 1441, 1453, 1455, 1457, 1460, 1461, 1465, 1467, 1469, 1472, 1473, 1475, 1478, 1482, 1484, 1487, 1495, 1500, 1506, 1507, 1509, 1660, 1667, 1673, 1675, 1677, 1679, 1681, 1683, 1686, 1688, 1692, 1694, 1697, 1713, 1715, 1718, 1727, 1732, 1738, 1740, 1742, 1744, 1748, 1772, 1784, 1785, 1790, 1792, 1794, 1796, 1798, 1800, 1803, 1805, 1829, 1833, 1835, 1838, 1840, 1858, 1871, 1876, 1878, 1881, 1883, 1894, 1911, 1912, 1923, 1927, 1928, 1934, 1940, 1941, 1942, 1945, 1946, 1947, 1948, 1962, 1966, 1968, 1973, 1981, 1982, 1983, 1984, 1986, 1996, 1998, 1999, 2006, 2013, 2014, 2015, 2020, 2023, 2024, 2030, 2033, 2034, 2037, 2038, 2041, 2042, 2043, 2053, 2059, 2060, 2061, 2062, 2065, 2066, 2068, 2069, 2087, 2102, 2111, 2129, 2131, 2134, 2135, 2137, 2140, 2142, 2143, 2145, 2146, 2149, 2150, 2151, 2154, 2156, 2157, 2159, 2161, 2163, 2166, 2169, 2173, 2175, 2177, 2179, 2182, 2183, 2184, 2185, 2188, 2352, 2353, 2354, 2429, 2435, 2463, 2478, 2495, 2498, 2579, 2582, 2601, 2603, 2604, 2605, 2606, 2611, 2629, 2634, 2638, 2639, 2647, 2648, 2657, 2658, 2659, 2693, 2705, 2707, 2708, 2718, 2720, 2730, 2731, 2732, 2739, 2742, 2743, 2761, 2763, 2764, 2769, 2771, 2773, 2778, 2783, 2785, 2786, 2787, 2788, 2789, 2794, 2795, 2796, 2797, 2800, 2801, 2824, 2825, 2826, 2848, 2849, 2851, 2852, 2856, 2857, 2858, 2860, 2869, 2872, 2884, 2889, 2894, 2895, 2898, 2899, 2900, 2902, 2913, 2916, 2918, 2919, 2930, 2931, 2932, 2933, 2935, 2936, 2937, 2940, 2944, 2945, 2948, 2951, 2952, 2953, 2958, 2959, 2960, 2963, 2969, 2970, 2972, 2973, 2976, 2977, 2982, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 3000, 3004, 3006, 3008, 3010, 3028, 3029, 3030, 3032, 3038, 3045, 3046, 3048, 3061, 3063, 3065, 3070, 3072, 3073, 3078, 3093, 3101, 3128, 3132, 3156, 3160, 3161, 3168, 3169, 3191, 3193, 3194, 3206, 3212, 3213, 3214, 3228, 3230, 3232, 3243, 3245, 3246, 3247, 3249, 3251, 3252, 3254, 3255, 3258, 3265, 3276, 3277, 3278, 3286, 3287, 3304, 3308, 3311, 3314, 3317, 3318, 3319, 3320, 3321, 3323, 3327, 3329, 3332, 3333, 3334, 3341, 3343, 3345, 3346, 3349, 3351, 3353, 3355, 3357, 3359, 3361, 3362, 3365, 3367, 3369, 3373, 3376, 3377, 3378, 3380, 3387, 3399, 3400, 3401, 3403, 3406, 3407, 3410, 3412, 3413, 3415, 3417, 3418, 3424, 3425, 3426, 3427, 3428, 3430, 3431, 3432, 3433, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3449, 3450, 3451, 3459, 3461, 3469, 3470, 3471, 3476, 3506, 3508, 3512, 3513, 3514, 3532, 3543, 3545, 3564, 3567, 3574, 3577, 3592, 3598, 3602, 3603, 3604, 3606, 3625, 3626, 3629, 3635, 3637, 3638, 3640, 3653, 3666, 3667, 3668, 3698, 3699, 3717, 3718, 3724, 3730, 3732, 3733, 3743, 3745, 3758, 3845, 3846, 3847, 3849, 3883, 3888, 3915, 3917, 3930, 3946, 3948, 3950, 3954, 3956, 3984, 3990, 3994, 4021]
NAME_NUMBER_list:[159, 262, 263, 275, 281, 287, 289, 292, 603, 708, 744, 748, 756, 768, 771, 792, 794, 888, 939, 945, 947, 964, 971, 1005, 1044, 1076, 1155, 1230, 1231, 1261, 1262, 1268, 1310, 1325, 1335, 1341, 1346, 1356, 1364, 1370, 1372, 1382, 1419, 1425, 1426, 1429, 1430, 1436, 1437, 1446, 1447, 1450, 1452, 1466, 1468, 1471, 1477, 1498, 1499, 1508, 1661, 1668, 1672, 1704, 1728, 1733, 1779, 1786, 1830, 1847, 1864, 1901, 1914, 1917, 1918, 1920, 1921, 1929, 1931, 1936, 1949, 1950, 1975, 1976, 1985, 1987, 2000, 2009, 2010, 2017, 2029, 2031, 2039, 2063, 2064, 2070, 2124, 2133, 2138, 2139, 2141, 2144, 2147, 2152, 2153, 2155, 2160, 2290, 2670, 2760, 2770, 2772, 2791, 2807, 2812, 2817, 2822, 2855, 2873, 2879, 2915, 3002, 3047, 3076, 3103, 3113, 3118, 3124, 3134, 3136, 3137, 3178, 3185, 3197, 3203, 3204, 3207, 3222, 3239, 3241, 3261, 3263, 3269, 3309, 3312, 3468, 3533, 3534, 3537, 3539, 3540, 3546, 3547, 3552, 3553, 3555, 3556, 3561, 3562, 3572, 3573, 3593, 3616, 3620, 3621, 3628, 3632, 3633, 3634, 3639, 3740, 3878, 3879, 3886, 3889, 3928, 3939, 3966, 3980, 3992]
Accuracy on the test set of pretrained_BERT_layer_7 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_BERT Layer 8
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0080
Epoch: [2/10], Loss: 0.0047
Epoch: [3/10], Loss: 0.0033
Epoch: [4/10], Loss: 0.0025
Epoch: [5/10], Loss: 0.0020
Epoch: [6/10], Loss: 0.0017
Epoch: [7/10], Loss: 0.0014
Epoch: [8/10], Loss: 0.0012
Epoch: [9/10], Loss: 0.0011
Epoch: [10/10], Loss: 0.0010
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0079
Epoch: [2/10], Loss: 0.0046
Epoch: [3/10], Loss: 0.0032
Epoch: [4/10], Loss: 0.0025
Epoch: [5/10], Loss: 0.0020
Epoch: [6/10], Loss: 0.0017
Epoch: [7/10], Loss: 0.0014
Epoch: [8/10], Loss: 0.0012
Epoch: [9/10], Loss: 0.0011
Epoch: [10/10], Loss: 0.0010
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0081
Epoch: [2/10], Loss: 0.0048
Epoch: [3/10], Loss: 0.0034
Epoch: [4/10], Loss: 0.0027
Epoch: [5/10], Loss: 0.0022
Epoch: [6/10], Loss: 0.0019
Epoch: [7/10], Loss: 0.0017
Epoch: [8/10], Loss: 0.0015
Epoch: [9/10], Loss: 0.0013
Epoch: [10/10], Loss: 0.0012
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0088
Epoch: [2/10], Loss: 0.0058
Epoch: [3/10], Loss: 0.0046
Epoch: [4/10], Loss: 0.0040
Epoch: [5/10], Loss: 0.0036
Epoch: [6/10], Loss: 0.0033
Epoch: [7/10], Loss: 0.0031
Epoch: [8/10], Loss: 0.0029
Epoch: [9/10], Loss: 0.0028
Epoch: [10/10], Loss: 0.0026
Score (accuracy) of the probe: 0.97

The best l1=0, the best l2=0.1 for pretrained_BERT_layer_8
Accuracy on the test set of probing pretrained_BERT_layer_8 of all layers:
Score (accuracy) of the probe: 0.88
{'__OVERALL__': 0.8845007451564829, 'NAME': 0.798087141339001, 'STRING': 1.0, 'NUMBER': 0.9612555174104953, 'KEYWORD': 0.9117647058823529}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1502,KW_NAME:6
NAME_KW:19,KW_KW:62
NAME_STRING:237,KW_other:0
NAME_NUMBER:124
NAME_STRING_list:[44, 46, 163, 164, 167, 192, 194, 232, 234, 293, 608, 611, 625, 628, 668, 676, 680, 683, 715, 718, 728, 729, 735, 736, 737, 755, 758, 766, 770, 771, 778, 779, 896, 897, 920, 925, 927, 1002, 1194, 1215, 1217, 1224, 1226, 1227, 1252, 1266, 1305, 1322, 1409, 1423, 1427, 1438, 1439, 1441, 1450, 1453, 1457, 1460, 1469, 1475, 1487, 1500, 1506, 1507, 1509, 1667, 1748, 1911, 1912, 1934, 1946, 1948, 1968, 1981, 1983, 2015, 2023, 2024, 2033, 2046, 2053, 2059, 2062, 2065, 2068, 2087, 2111, 2134, 2135, 2140, 2142, 2150, 2151, 2154, 2156, 2159, 2166, 2182, 2183, 2184, 2185, 2296, 2463, 2478, 2606, 2611, 2657, 2658, 2659, 2705, 2720, 2731, 2742, 2743, 2761, 2763, 2764, 2769, 2771, 2783, 2785, 2786, 2787, 2794, 2808, 2849, 2889, 2900, 2931, 2933, 2935, 2936, 2937, 2953, 2958, 2960, 2963, 2966, 2972, 2977, 2984, 2986, 2987, 2988, 2989, 2992, 2994, 2996, 2997, 2998, 3000, 3002, 3004, 3006, 3008, 3030, 3032, 3045, 3046, 3047, 3048, 3061, 3073, 3093, 3168, 3172, 3178, 3186, 3193, 3194, 3204, 3206, 3213, 3228, 3232, 3247, 3249, 3254, 3255, 3261, 3286, 3317, 3318, 3319, 3320, 3329, 3333, 3345, 3346, 3351, 3353, 3355, 3357, 3359, 3361, 3365, 3366, 3367, 3369, 3400, 3401, 3403, 3406, 3410, 3411, 3412, 3413, 3415, 3417, 3418, 3425, 3426, 3427, 3428, 3430, 3431, 3432, 3438, 3439, 3443, 3444, 3447, 3449, 3514, 3593, 3604, 3625, 3626, 3637, 3668, 3743, 3946, 3947, 3948, 3954, 3990, 3992]
NAME_NUMBER_list:[9, 159, 262, 275, 292, 708, 726, 744, 751, 762, 840, 846, 939, 971, 1076, 1106, 1155, 1163, 1230, 1231, 1268, 1310, 1333, 1335, 1341, 1346, 1347, 1348, 1352, 1364, 1370, 1372, 1396, 1425, 1437, 1446, 1447, 1455, 1488, 1499, 1668, 1894, 1914, 1917, 1921, 1936, 1949, 1950, 1963, 1975, 1976, 2000, 2004, 2006, 2009, 2010, 2017, 2029, 2041, 2063, 2064, 2133, 2139, 2141, 2153, 2157, 2290, 2353, 2634, 2718, 2760, 2767, 2770, 2772, 2822, 2839, 2856, 2872, 2879, 2915, 3063, 3078, 3103, 3111, 3118, 3128, 3132, 3134, 3208, 3222, 3263, 3265, 3277, 3314, 3387, 3451, 3455, 3459, 3471, 3476, 3533, 3535, 3539, 3546, 3547, 3552, 3553, 3555, 3561, 3562, 3572, 3573, 3616, 3628, 3632, 3633, 3635, 3639, 3740, 3879, 3883, 3886, 3952, 3980]
Accuracy on the test set of pretrained_BERT_layer_8 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_BERT Layer 9
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0079
Epoch: [2/10], Loss: 0.0046
Epoch: [3/10], Loss: 0.0031
Epoch: [4/10], Loss: 0.0024
Epoch: [5/10], Loss: 0.0019
Epoch: [6/10], Loss: 0.0016
Epoch: [7/10], Loss: 0.0014
Epoch: [8/10], Loss: 0.0012
Epoch: [9/10], Loss: 0.0010
Epoch: [10/10], Loss: 0.0009
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0077
Epoch: [2/10], Loss: 0.0044
Epoch: [3/10], Loss: 0.0031
Epoch: [4/10], Loss: 0.0024
Epoch: [5/10], Loss: 0.0019
Epoch: [6/10], Loss: 0.0016
Epoch: [7/10], Loss: 0.0014
Epoch: [8/10], Loss: 0.0012
Epoch: [9/10], Loss: 0.0010
Epoch: [10/10], Loss: 0.0009
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0076
Epoch: [2/10], Loss: 0.0045
Epoch: [3/10], Loss: 0.0032
Epoch: [4/10], Loss: 0.0025
Epoch: [5/10], Loss: 0.0020
Epoch: [6/10], Loss: 0.0017
Epoch: [7/10], Loss: 0.0015
Epoch: [8/10], Loss: 0.0014
Epoch: [9/10], Loss: 0.0012
Epoch: [10/10], Loss: 0.0011
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0085
Epoch: [2/10], Loss: 0.0056
Epoch: [3/10], Loss: 0.0044
Epoch: [4/10], Loss: 0.0039
Epoch: [5/10], Loss: 0.0035
Epoch: [6/10], Loss: 0.0033
Epoch: [7/10], Loss: 0.0031
Epoch: [8/10], Loss: 0.0029
Epoch: [9/10], Loss: 0.0027
Epoch: [10/10], Loss: 0.0026
Score (accuracy) of the probe: 0.98

The best l1=0, the best l2=0.1 for pretrained_BERT_layer_9
Accuracy on the test set of probing pretrained_BERT_layer_9 of all layers:
Score (accuracy) of the probe: 0.85
{'__OVERALL__': 0.8537009438648783, 'NAME': 0.6976620616365569, 'STRING': 0.972972972972973, 'NUMBER': 0.9950956351152526, 'KEYWORD': 0.8676470588235294}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1313,KW_NAME:8
NAME_KW:67,KW_KW:59
NAME_STRING:200,KW_other:1
NAME_NUMBER:302
NAME_STRING_list:[44, 46, 48, 163, 164, 192, 232, 265, 269, 270, 280, 285, 293, 295, 608, 611, 625, 628, 668, 715, 718, 735, 749, 761, 766, 770, 903, 906, 925, 927, 1217, 1219, 1226, 1266, 1272, 1331, 1355, 1376, 1393, 1395, 1409, 1422, 1423, 1427, 1441, 1450, 1457, 1460, 1467, 1469, 1474, 1475, 1484, 1485, 1487, 1500, 1506, 1507, 1667, 1923, 1927, 1946, 1948, 1968, 1981, 1983, 1998, 2015, 2023, 2024, 2037, 2042, 2053, 2059, 2062, 2065, 2068, 2071, 2079, 2134, 2137, 2142, 2150, 2151, 2159, 2161, 2173, 2175, 2179, 2182, 2183, 2184, 2185, 2296, 2606, 2657, 2705, 2708, 2742, 2747, 2771, 2785, 2787, 2794, 2800, 2825, 2849, 2857, 2871, 2884, 2900, 2966, 2977, 2985, 2987, 2989, 2995, 2997, 3000, 3004, 3006, 3030, 3046, 3048, 3061, 3093, 3156, 3160, 3168, 3186, 3193, 3212, 3230, 3247, 3254, 3276, 3304, 3311, 3317, 3318, 3332, 3333, 3345, 3346, 3351, 3353, 3355, 3357, 3359, 3361, 3365, 3367, 3369, 3376, 3378, 3403, 3410, 3412, 3413, 3415, 3417, 3418, 3424, 3425, 3426, 3427, 3430, 3431, 3432, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3449, 3450, 3457, 3514, 3625, 3626, 3637, 3638, 3640, 3653, 3668, 3724, 3732, 3733, 3947, 3948, 3953, 3954, 3992]
NAME_NUMBER_list:[119, 159, 167, 170, 189, 191, 193, 231, 233, 234, 262, 275, 281, 292, 687, 726, 744, 751, 760, 762, 794, 840, 846, 852, 858, 864, 870, 875, 893, 897, 904, 907, 939, 946, 947, 953, 971, 1002, 1005, 1006, 1044, 1074, 1076, 1077, 1079, 1106, 1108, 1155, 1163, 1196, 1199, 1211, 1230, 1231, 1233, 1246, 1249, 1252, 1254, 1257, 1260, 1262, 1267, 1268, 1310, 1335, 1341, 1346, 1347, 1348, 1354, 1364, 1370, 1372, 1382, 1396, 1416, 1418, 1420, 1421, 1425, 1426, 1428, 1436, 1437, 1438, 1439, 1446, 1447, 1452, 1455, 1458, 1463, 1465, 1468, 1470, 1472, 1473, 1498, 1499, 1508, 1668, 1732, 1733, 1785, 1786, 1847, 1894, 1901, 1911, 1912, 1914, 1917, 1921, 1928, 1934, 1936, 1938, 1939, 1940, 1941, 1943, 1944, 1945, 1947, 1949, 1950, 1960, 1962, 1963, 1975, 1976, 1985, 1986, 1987, 2000, 2002, 2004, 2006, 2007, 2009, 2010, 2011, 2027, 2028, 2029, 2031, 2032, 2033, 2034, 2035, 2036, 2038, 2040, 2041, 2043, 2046, 2047, 2049, 2050, 2063, 2064, 2070, 2111, 2131, 2133, 2135, 2136, 2138, 2139, 2140, 2141, 2146, 2147, 2149, 2152, 2153, 2154, 2155, 2156, 2157, 2160, 2162, 2290, 2353, 2601, 2612, 2647, 2648, 2659, 2666, 2686, 2700, 2718, 2720, 2737, 2743, 2760, 2763, 2766, 2767, 2769, 2770, 2772, 2789, 2796, 2801, 2807, 2810, 2811, 2812, 2815, 2816, 2817, 2820, 2821, 2822, 2826, 2848, 2855, 2856, 2868, 2872, 2873, 2879, 2889, 2911, 2914, 2915, 2944, 2945, 2952, 2959, 2969, 2970, 3045, 3063, 3076, 3078, 3103, 3111, 3118, 3128, 3132, 3134, 3197, 3213, 3214, 3222, 3228, 3243, 3249, 3263, 3265, 3277, 3278, 3314, 3327, 3329, 3387, 3393, 3399, 3451, 3455, 3459, 3469, 3470, 3471, 3533, 3535, 3539, 3543, 3555, 3561, 3562, 3572, 3573, 3593, 3598, 3604, 3616, 3628, 3632, 3633, 3634, 3635, 3639, 3666, 3667, 3717, 3730, 3745, 3845, 3879, 3883, 3886, 3917, 3946, 3952, 3966, 3980, 3990]
Accuracy on the test set of pretrained_BERT_layer_9 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_BERT Layer 10
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0080
Epoch: [2/10], Loss: 0.0049
Epoch: [3/10], Loss: 0.0034
Epoch: [4/10], Loss: 0.0026
Epoch: [5/10], Loss: 0.0021
Epoch: [6/10], Loss: 0.0017
Epoch: [7/10], Loss: 0.0015
Epoch: [8/10], Loss: 0.0013
Epoch: [9/10], Loss: 0.0011
Epoch: [10/10], Loss: 0.0010
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0082
Epoch: [2/10], Loss: 0.0049
Epoch: [3/10], Loss: 0.0035
Epoch: [4/10], Loss: 0.0027
Epoch: [5/10], Loss: 0.0021
Epoch: [6/10], Loss: 0.0018
Epoch: [7/10], Loss: 0.0015
Epoch: [8/10], Loss: 0.0013
Epoch: [9/10], Loss: 0.0012
Epoch: [10/10], Loss: 0.0010
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0082
Epoch: [2/10], Loss: 0.0051
Epoch: [3/10], Loss: 0.0036
Epoch: [4/10], Loss: 0.0028
Epoch: [5/10], Loss: 0.0023
Epoch: [6/10], Loss: 0.0020
Epoch: [7/10], Loss: 0.0017
Epoch: [8/10], Loss: 0.0015
Epoch: [9/10], Loss: 0.0014
Epoch: [10/10], Loss: 0.0012
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0090
Epoch: [2/10], Loss: 0.0061
Epoch: [3/10], Loss: 0.0048
Epoch: [4/10], Loss: 0.0041
Epoch: [5/10], Loss: 0.0037
Epoch: [6/10], Loss: 0.0034
Epoch: [7/10], Loss: 0.0032
Epoch: [8/10], Loss: 0.0030
Epoch: [9/10], Loss: 0.0028
Epoch: [10/10], Loss: 0.0027
Score (accuracy) of the probe: 0.97

The best l1=0, the best l2=0.1 for pretrained_BERT_layer_10
Accuracy on the test set of probing pretrained_BERT_layer_10 of all layers:
Score (accuracy) of the probe: 0.86
{'__OVERALL__': 0.856929955290611, 'NAME': 0.7157279489904357, 'STRING': 1.0, 'NUMBER': 0.986758214811182, 'KEYWORD': 0.7941176470588235}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1347,KW_NAME:13
NAME_KW:66,KW_KW:54
NAME_STRING:235,KW_other:1
NAME_NUMBER:234
NAME_STRING_list:[9, 44, 46, 48, 163, 164, 191, 192, 231, 232, 265, 270, 405, 625, 628, 668, 708, 709, 715, 718, 728, 729, 734, 735, 736, 755, 758, 766, 777, 778, 779, 797, 823, 837, 840, 846, 852, 858, 864, 870, 875, 885, 887, 903, 904, 906, 907, 925, 948, 949, 964, 1074, 1075, 1158, 1219, 1226, 1232, 1233, 1235, 1245, 1249, 1253, 1266, 1270, 1354, 1376, 1381, 1393, 1395, 1401, 1427, 1441, 1457, 1473, 1484, 1485, 1487, 1500, 1669, 1894, 1911, 1923, 1940, 1945, 1948, 1963, 1974, 1982, 2006, 2030, 2037, 2042, 2049, 2060, 2061, 2088, 2104, 2135, 2142, 2156, 2157, 2159, 2173, 2175, 2296, 2352, 2353, 2646, 2657, 2673, 2693, 2705, 2707, 2720, 2742, 2763, 2769, 2771, 2785, 2787, 2788, 2794, 2800, 2825, 2838, 2845, 2849, 2862, 2868, 2875, 2876, 2884, 2890, 2892, 2966, 2977, 2985, 2987, 2989, 2993, 2995, 2997, 3000, 3004, 3006, 3008, 3013, 3014, 3048, 3061, 3075, 3093, 3097, 3128, 3160, 3168, 3169, 3193, 3210, 3233, 3247, 3254, 3255, 3259, 3265, 3276, 3307, 3311, 3314, 3318, 3321, 3329, 3332, 3333, 3346, 3351, 3353, 3355, 3357, 3361, 3365, 3367, 3369, 3373, 3376, 3378, 3392, 3400, 3401, 3403, 3412, 3413, 3415, 3417, 3418, 3426, 3427, 3430, 3431, 3432, 3435, 3436, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3446, 3447, 3449, 3450, 3451, 3465, 3469, 3470, 3471, 3514, 3593, 3626, 3637, 3638, 3654, 3668, 3724, 3732, 3733, 3917, 3947, 3948, 3953, 3954, 3990, 3992]
NAME_NUMBER_list:[119, 159, 260, 262, 292, 406, 673, 687, 712, 726, 751, 760, 762, 794, 893, 897, 939, 947, 971, 995, 997, 1005, 1014, 1044, 1076, 1077, 1079, 1106, 1108, 1150, 1163, 1196, 1199, 1231, 1246, 1254, 1257, 1260, 1267, 1268, 1333, 1335, 1341, 1343, 1346, 1347, 1364, 1370, 1372, 1416, 1420, 1421, 1425, 1426, 1428, 1431, 1436, 1437, 1438, 1439, 1446, 1447, 1452, 1453, 1455, 1458, 1467, 1468, 1469, 1470, 1480, 1493, 1498, 1499, 1506, 1507, 1508, 1661, 1667, 1668, 1728, 1732, 1733, 1779, 1785, 1786, 1830, 1864, 1901, 1908, 1912, 1914, 1917, 1921, 1928, 1934, 1936, 1938, 1939, 1943, 1944, 1947, 1949, 1950, 1960, 1962, 1975, 1976, 1985, 1986, 1987, 2000, 2002, 2004, 2007, 2009, 2010, 2011, 2027, 2028, 2029, 2031, 2032, 2033, 2035, 2036, 2039, 2040, 2041, 2046, 2050, 2063, 2064, 2070, 2111, 2131, 2133, 2134, 2136, 2138, 2140, 2141, 2146, 2147, 2149, 2151, 2152, 2153, 2154, 2155, 2160, 2162, 2290, 2354, 2612, 2648, 2654, 2659, 2684, 2686, 2700, 2708, 2737, 2743, 2751, 2760, 2770, 2772, 2796, 2807, 2811, 2812, 2816, 2817, 2821, 2822, 2826, 2855, 2856, 2857, 2871, 2872, 2873, 2879, 2889, 2914, 2915, 2952, 2959, 3045, 3046, 3078, 3079, 3096, 3103, 3113, 3118, 3132, 3137, 3197, 3208, 3214, 3229, 3236, 3237, 3239, 3243, 3263, 3278, 3304, 3313, 3327, 3387, 3399, 3424, 3455, 3459, 3464, 3468, 3562, 3573, 3606, 3632, 3633, 3666, 3667, 3717, 3745, 3845, 3883, 3886, 3952, 3966, 3980]
Accuracy on the test set of pretrained_BERT_layer_10 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_BERT Layer 11
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0081
Epoch: [2/10], Loss: 0.0050
Epoch: [3/10], Loss: 0.0036
Epoch: [4/10], Loss: 0.0028
Epoch: [5/10], Loss: 0.0023
Epoch: [6/10], Loss: 0.0019
Epoch: [7/10], Loss: 0.0016
Epoch: [8/10], Loss: 0.0014
Epoch: [9/10], Loss: 0.0012
Epoch: [10/10], Loss: 0.0011
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0084
Epoch: [2/10], Loss: 0.0052
Epoch: [3/10], Loss: 0.0037
Epoch: [4/10], Loss: 0.0029
Epoch: [5/10], Loss: 0.0024
Epoch: [6/10], Loss: 0.0020
Epoch: [7/10], Loss: 0.0017
Epoch: [8/10], Loss: 0.0015
Epoch: [9/10], Loss: 0.0013
Epoch: [10/10], Loss: 0.0011
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0085
Epoch: [2/10], Loss: 0.0054
Epoch: [3/10], Loss: 0.0039
Epoch: [4/10], Loss: 0.0031
Epoch: [5/10], Loss: 0.0026
Epoch: [6/10], Loss: 0.0022
Epoch: [7/10], Loss: 0.0019
Epoch: [8/10], Loss: 0.0017
Epoch: [9/10], Loss: 0.0015
Epoch: [10/10], Loss: 0.0014
Score (accuracy) of the probe: 0.97
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0090
Epoch: [2/10], Loss: 0.0062
Epoch: [3/10], Loss: 0.0050
Epoch: [4/10], Loss: 0.0043
Epoch: [5/10], Loss: 0.0039
Epoch: [6/10], Loss: 0.0035
Epoch: [7/10], Loss: 0.0033
Epoch: [8/10], Loss: 0.0031
Epoch: [9/10], Loss: 0.0029
Epoch: [10/10], Loss: 0.0027
Score (accuracy) of the probe: 0.97

The best l1=0, the best l2=0.1 for pretrained_BERT_layer_11
Accuracy on the test set of probing pretrained_BERT_layer_11 of all layers:
Score (accuracy) of the probe: 0.81
{'__OVERALL__': 0.8137108792846498, 'NAME': 0.6429330499468651, 'STRING': 1.0, 'NUMBER': 0.967631191760667, 'KEYWORD': 0.8235294117647058}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1210,KW_NAME:8
NAME_KW:115,KW_KW:56
NAME_STRING:240,KW_other:4
NAME_NUMBER:317
NAME_STRING_list:[9, 21, 25, 44, 46, 48, 118, 163, 164, 167, 170, 171, 190, 191, 198, 199, 206, 217, 231, 238, 246, 286, 287, 611, 625, 628, 660, 708, 715, 718, 728, 729, 734, 735, 736, 755, 756, 758, 761, 777, 778, 779, 795, 797, 823, 842, 860, 866, 872, 903, 906, 925, 927, 948, 949, 958, 964, 995, 1019, 1040, 1074, 1075, 1150, 1158, 1160, 1184, 1194, 1217, 1226, 1232, 1237, 1245, 1248, 1249, 1253, 1266, 1381, 1385, 1392, 1393, 1395, 1401, 1405, 1416, 1421, 1433, 1473, 1485, 1487, 1669, 1675, 1681, 1683, 1686, 1688, 1740, 1772, 1792, 1794, 1796, 1800, 1803, 1805, 1835, 1840, 1858, 1876, 1881, 1894, 1935, 1937, 1940, 1945, 1948, 1963, 1974, 1978, 1982, 2006, 2030, 2037, 2042, 2049, 2061, 2088, 2131, 2135, 2142, 2306, 2332, 2335, 2338, 2352, 2353, 2636, 2657, 2671, 2673, 2693, 2705, 2707, 2763, 2785, 2794, 2800, 2808, 2825, 2838, 2845, 2849, 2875, 2876, 2890, 2892, 2924, 2940, 2985, 2989, 2995, 2997, 3013, 3014, 3027, 3048, 3061, 3063, 3065, 3067, 3072, 3097, 3168, 3173, 3178, 3193, 3206, 3210, 3212, 3214, 3254, 3255, 3259, 3265, 3272, 3273, 3276, 3286, 3306, 3307, 3314, 3316, 3318, 3319, 3323, 3351, 3353, 3354, 3355, 3356, 3357, 3365, 3367, 3369, 3378, 3397, 3400, 3401, 3403, 3406, 3407, 3415, 3417, 3418, 3431, 3432, 3436, 3440, 3444, 3445, 3469, 3514, 3626, 3630, 3637, 3638, 3654, 3698, 3724, 3732, 3893, 3915, 3917, 3943, 3947, 3948, 3950, 3954, 3956, 3985, 3990, 4022]
NAME_NUMBER_list:[119, 159, 166, 175, 192, 193, 232, 233, 260, 262, 265, 289, 292, 405, 406, 612, 626, 629, 673, 676, 688, 712, 719, 726, 727, 751, 760, 762, 766, 768, 793, 837, 840, 846, 852, 858, 864, 870, 875, 885, 887, 888, 893, 895, 897, 904, 907, 910, 939, 946, 970, 971, 993, 997, 1002, 1014, 1020, 1021, 1044, 1054, 1076, 1077, 1079, 1084, 1089, 1196, 1199, 1230, 1231, 1233, 1239, 1241, 1246, 1252, 1254, 1257, 1260, 1265, 1267, 1268, 1312, 1325, 1333, 1335, 1338, 1339, 1341, 1343, 1346, 1347, 1354, 1364, 1367, 1368, 1369, 1370, 1371, 1372, 1379, 1382, 1386, 1389, 1420, 1426, 1431, 1436, 1437, 1438, 1446, 1450, 1455, 1468, 1480, 1493, 1495, 1498, 1499, 1506, 1666, 1731, 1774, 1784, 1825, 1899, 1901, 1908, 1910, 1914, 1916, 1917, 1918, 1920, 1921, 1927, 1928, 1932, 1934, 1936, 1946, 1947, 1949, 1950, 1962, 1975, 1976, 1985, 1986, 2000, 2004, 2009, 2011, 2020, 2024, 2039, 2040, 2041, 2053, 2063, 2064, 2104, 2111, 2133, 2139, 2141, 2146, 2147, 2149, 2153, 2154, 2155, 2156, 2157, 2158, 2160, 2290, 2296, 2325, 2354, 2601, 2606, 2612, 2630, 2634, 2647, 2654, 2660, 2666, 2670, 2684, 2686, 2708, 2718, 2737, 2742, 2743, 2751, 2760, 2767, 2769, 2770, 2771, 2772, 2773, 2789, 2791, 2796, 2807, 2826, 2848, 2855, 2856, 2857, 2861, 2862, 2868, 2871, 2872, 2873, 2879, 2884, 2889, 2914, 2915, 2945, 2952, 2959, 2969, 2970, 3001, 3003, 3028, 3031, 3068, 3073, 3075, 3096, 3098, 3103, 3107, 3113, 3116, 3118, 3125, 3131, 3132, 3137, 3167, 3169, 3197, 3208, 3216, 3219, 3233, 3236, 3237, 3239, 3241, 3243, 3260, 3263, 3264, 3270, 3278, 3304, 3310, 3313, 3327, 3350, 3389, 3392, 3393, 3399, 3424, 3451, 3455, 3459, 3464, 3465, 3468, 3470, 3471, 3533, 3535, 3543, 3552, 3553, 3562, 3564, 3573, 3593, 3606, 3620, 3627, 3628, 3631, 3632, 3633, 3636, 3639, 3666, 3667, 3717, 3718, 3733, 3740, 3745, 3845, 3847, 3849, 3850, 3886, 3927, 3946, 3952, 3953, 3980, 3992]
Accuracy on the test set of pretrained_BERT_layer_11 model using the intercept:
Score (accuracy) of the probe: 0.47
pretrained_BERT Layer 12
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0087
Epoch: [2/10], Loss: 0.0055
Epoch: [3/10], Loss: 0.0041
Epoch: [4/10], Loss: 0.0033
Epoch: [5/10], Loss: 0.0027
Epoch: [6/10], Loss: 0.0023
Epoch: [7/10], Loss: 0.0019
Epoch: [8/10], Loss: 0.0017
Epoch: [9/10], Loss: 0.0015
Epoch: [10/10], Loss: 0.0013
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0087
Epoch: [2/10], Loss: 0.0056
Epoch: [3/10], Loss: 0.0041
Epoch: [4/10], Loss: 0.0033
Epoch: [5/10], Loss: 0.0027
Epoch: [6/10], Loss: 0.0023
Epoch: [7/10], Loss: 0.0020
Epoch: [8/10], Loss: 0.0017
Epoch: [9/10], Loss: 0.0015
Epoch: [10/10], Loss: 0.0014
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0086
Epoch: [2/10], Loss: 0.0056
Epoch: [3/10], Loss: 0.0042
Epoch: [4/10], Loss: 0.0034
Epoch: [5/10], Loss: 0.0028
Epoch: [6/10], Loss: 0.0024
Epoch: [7/10], Loss: 0.0021
Epoch: [8/10], Loss: 0.0019
Epoch: [9/10], Loss: 0.0017
Epoch: [10/10], Loss: 0.0015
Score (accuracy) of the probe: 0.96
Training classification probe
Creating model...
Number of training instances: 13636
Number of classes: 4
Epoch: [1/10], Loss: 0.0096
Epoch: [2/10], Loss: 0.0066
Epoch: [3/10], Loss: 0.0054
Epoch: [4/10], Loss: 0.0047
Epoch: [5/10], Loss: 0.0042
Epoch: [6/10], Loss: 0.0038
Epoch: [7/10], Loss: 0.0035
Epoch: [8/10], Loss: 0.0033
Epoch: [9/10], Loss: 0.0031
Epoch: [10/10], Loss: 0.0029
Score (accuracy) of the probe: 0.97

The best l1=0, the best l2=0.1 for pretrained_BERT_layer_12
Accuracy on the test set of probing pretrained_BERT_layer_12 of all layers:
Score (accuracy) of the probe: 0.81
{'__OVERALL__': 0.8062593144560357, 'NAME': 0.6227417640807651, 'STRING': 1.0, 'NUMBER': 0.9705738106915155, 'KEYWORD': 0.8529411764705882}
Confusion matrix between NAME and KEYWORD:
NAME_NAME:1172,KW_NAME:3
NAME_KW:112,KW_KW:58
NAME_STRING:288,KW_other:7
NAME_NUMBER:310
NAME_STRING_list:[9, 44, 46, 48, 163, 164, 165, 167, 170, 191, 198, 231, 238, 598, 611, 625, 628, 668, 672, 673, 674, 682, 683, 708, 715, 716, 718, 728, 729, 735, 736, 755, 757, 758, 765, 767, 778, 779, 797, 799, 800, 801, 823, 840, 842, 848, 854, 860, 864, 866, 872, 878, 885, 903, 906, 925, 948, 949, 950, 955, 993, 994, 1004, 1020, 1040, 1074, 1075, 1150, 1158, 1160, 1184, 1215, 1217, 1219, 1224, 1226, 1232, 1237, 1246, 1249, 1257, 1262, 1265, 1310, 1312, 1320, 1354, 1381, 1385, 1393, 1395, 1404, 1409, 1433, 1484, 1487, 1495, 1673, 1675, 1686, 1692, 1694, 1713, 1715, 1724, 1738, 1740, 1742, 1744, 1748, 1772, 1790, 1792, 1794, 1796, 1798, 1800, 1803, 1805, 1833, 1835, 1838, 1840, 1858, 1876, 1878, 1881, 1883, 1894, 1896, 1945, 1948, 1954, 1963, 1965, 1974, 1978, 1980, 1981, 1982, 2006, 2013, 2041, 2049, 2051, 2061, 2088, 2090, 2095, 2102, 2104, 2131, 2149, 2352, 2353, 2611, 2670, 2671, 2693, 2697, 2705, 2707, 2761, 2775, 2783, 2785, 2787, 2794, 2825, 2836, 2838, 2849, 2875, 2876, 2884, 2890, 2892, 2924, 2931, 2933, 2935, 2937, 2940, 2948, 2953, 2958, 2965, 2966, 2973, 2985, 2987, 2989, 2993, 2995, 2997, 2998, 2999, 3001, 3004, 3013, 3014, 3047, 3048, 3061, 3063, 3075, 3093, 3134, 3160, 3161, 3193, 3212, 3214, 3249, 3254, 3255, 3265, 3273, 3276, 3306, 3307, 3308, 3314, 3318, 3319, 3332, 3346, 3351, 3353, 3355, 3357, 3362, 3365, 3367, 3369, 3376, 3378, 3380, 3387, 3392, 3397, 3400, 3401, 3406, 3407, 3413, 3415, 3417, 3418, 3426, 3428, 3430, 3431, 3432, 3433, 3435, 3436, 3440, 3443, 3445, 3451, 3455, 3459, 3465, 3469, 3470, 3507, 3508, 3512, 3513, 3593, 3626, 3630, 3668, 3698, 3699, 3722, 3724, 3732, 3733, 3893, 3909, 3943, 3947, 3948, 3954, 3985, 4022]
NAME_NUMBER_list:[119, 159, 166, 171, 173, 175, 189, 190, 192, 194, 232, 233, 234, 260, 263, 275, 281, 289, 292, 603, 626, 629, 660, 680, 687, 688, 707, 712, 726, 751, 760, 762, 766, 888, 893, 904, 907, 919, 939, 946, 947, 958, 964, 970, 971, 1002, 1005, 1014, 1015, 1044, 1050, 1054, 1055, 1076, 1084, 1089, 1108, 1155, 1199, 1233, 1239, 1241, 1260, 1268, 1305, 1311, 1325, 1333, 1338, 1341, 1343, 1347, 1348, 1362, 1364, 1365, 1367, 1368, 1369, 1370, 1371, 1372, 1418, 1431, 1436, 1437, 1438, 1441, 1446, 1452, 1455, 1465, 1468, 1472, 1477, 1492, 1493, 1498, 1499, 1506, 1666, 1668, 1721, 1731, 1733, 1774, 1784, 1786, 1825, 1887, 1900, 1901, 1908, 1914, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1932, 1935, 1936, 1938, 1941, 1943, 1944, 1946, 1947, 1949, 1950, 1958, 1960, 1962, 1975, 1976, 1984, 1985, 1986, 1987, 1998, 1999, 2000, 2003, 2004, 2008, 2009, 2010, 2011, 2016, 2017, 2020, 2024, 2031, 2033, 2034, 2039, 2040, 2043, 2046, 2047, 2048, 2050, 2053, 2063, 2064, 2067, 2111, 2132, 2133, 2134, 2136, 2138, 2139, 2142, 2146, 2148, 2153, 2154, 2156, 2157, 2158, 2161, 2162, 2177, 2179, 2183, 2185, 2290, 2332, 2338, 2354, 2498, 2601, 2606, 2634, 2647, 2654, 2663, 2666, 2684, 2686, 2720, 2721, 2742, 2743, 2751, 2760, 2767, 2769, 2770, 2772, 2773, 2807, 2810, 2812, 2817, 2822, 2826, 2845, 2848, 2855, 2856, 2862, 2872, 2873, 2879, 2891, 2914, 2915, 2930, 2951, 2954, 2959, 2970, 3003, 3005, 3007, 3009, 3010, 3068, 3078, 3096, 3103, 3107, 3109, 3111, 3113, 3118, 3119, 3121, 3125, 3131, 3132, 3135, 3137, 3167, 3204, 3206, 3208, 3219, 3236, 3237, 3239, 3246, 3252, 3263, 3278, 3323, 3327, 3399, 3464, 3468, 3476, 3477, 3510, 3532, 3533, 3535, 3537, 3543, 3552, 3553, 3561, 3562, 3572, 3573, 3620, 3627, 3631, 3632, 3633, 3636, 3638, 3639, 3666, 3718, 3740, 3745, 3847, 3849, 3886, 3915, 3917, 3927, 3946, 3952, 3953, 3980]
Accuracy on the test set of pretrained_BERT_layer_12 model using the intercept:
Score (accuracy) of the probe: 0.47
