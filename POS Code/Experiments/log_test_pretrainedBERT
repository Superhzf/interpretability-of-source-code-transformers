Getting activations from json files. If you need to extract them, run with --extract=True 

Anayzing pretrained_BERT
Loading json activations from bert_activations_train.json...
1289 13.0
Number of tokens:  11684
length of source dictionary:  1042
length of target dictionary:  35
11684
Total instances: 11684
['names_path', 'find', '14', 'server_hardware_type', 'recv_presence', 'save', '17', 'allow_login', 'stream_closed', 'verbatim', 'ParticleSystemSortMode', '"Port"', '}', 'get_storage_vol_template_policy', 'timegm', 'render_mode', 'dest', 'timetuple', 'formatter_class', '3600']
Number of samples:  11684
Stats: Labels with their frequencies in the final set
NAME 3656
NEWLINE 1172
KEYWORD 905
LPAR 870
RPAR 866
DOT 858
EQUAL 597
COMMA 576
COLON 447
DEDENT 369
INDENT 293
LSQB 204
RSQB 204
NUMBER 190
NL 117
STRING 82
EQEQUAL 48
PLUS 33
LBRACE 25
STAR 24
RBRACE 24
MINUS 22
PERCENT 22
AT 14
PLUSEQUAL 12
GREATER 11
NOTEQUAL 10
DOUBLESTAR 8
LESS 7
SLASH 5
COMMENT 4
GREATEREQUAL 3
MINEQUAL 3
LESSEQUAL 2
SEMI 1
pretrained_BERT distribution after trauncating:
{0: 0.7564659631698738, 3: 0.1872542933995448, 2: 0.03931305607283261, 1: 0.01696668735774881}
{0: 3656, 3: 905, 2: 190, 1: 82}
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}
Loading json activations from bert_activations_test.json...
494 13.0
Number of tokens:  6400
length of source dictionary:  482
length of target dictionary:  34
6400
Total instances: 6400
['Exponential', '0.5', 'on_rtd', 'describable', 'f_on', '0.001', 'tuple', 'environ', 'results', '}', 'outs', 'param', 'init_scale', 'filename', 'model', 'ModelLMBase', 'conv', 'targets_name', 'get_html_theme_path', 'divide_mv']
Number of samples:  6400
Stats: Labels with their frequencies in the final set
NAME 1721
COMMA 798
NUMBER 482
RPAR 439
LPAR 437
NEWLINE 433
DOT 348
KEYWORD 336
COLON 293
EQUAL 241
LSQB 233
RSQB 228
DEDENT 97
INDENT 82
STAR 62
NL 60
EQEQUAL 29
PLUS 26
MINUS 15
DOUBLESTAR 7
SLASH 6
AT 6
STRING 4
LBRACE 3
RBRACE 3
STAREQUAL 2
PERCENT 2
GREATER 1
LESS 1
COMMENT 1
GREATEREQUAL 1
PLUSEQUAL 1
LESSEQUAL 1
AMPER 1
pretrained_BERT distribution after trauncating:
{0: 0.6767597325992922, 2: 0.18953991348800628, 3: 0.1321274085725521, 1: 0.0015729453401494297}
{0: 1721, 2: 482, 3: 336, 1: 4}
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}

The distribution of classes in training after removing repeated tokens between training and tesing:
Counter({0: 2915, 1: 82, 3: 58, 2: 42})
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}
