Getting activations from json files. If you need to extract them, run with --extract=True 

Anayzing pretrained_BERT
Loading json activations from bert_activations.json...
32002 13.0
Number of tokens:  325100
length of source dictionary:  17255
length of target dictionary:  47
325100
Total instances: 325100
['file_name', 'local_branch', '"Indexed"', 'fullpath', '"type"', 'wrap', 'CachedMetaClass', '0x1000', 'CopyImageMetadata', 'LGMap', 'INSTANCE_DB_NAME', 'tnum', 'g_T', 'NODES_PER_SLAB', 'ColorNoise', 'test_wo_principals_service', 'testHandlesAreHumanReadable', 'lblChangegt', 'break_on', 'prec']
Number of samples:  325100
Stats: Labels with their frequencies in the final set
NAME 104243
NEWLINE 28415
DOT 27737
LPAR 24899
RPAR 23914
KEYWORD 21043
COMMA 18332
EQUAL 15063
COLON 10649
DEDENT 8365
INDENT 7255
LSQB 7221
RSQB 7056
NUMBER 6667
STRING 3985
NL 3586
PLUS 928
LBRACE 870
EQEQUAL 827
MINUS 701
RBRACE 673
STAR 606
PERCENT 459
DOUBLESTAR 242
PLUSEQUAL 204
GREATER 187
SLASH 177
NOTEQUAL 176
AT 165
LESS 90
COMMENT 72
GREATEREQUAL 55
LESSEQUAL 40
VBAR 39
LEFTSHIFT 32
SEMI 22
MINEQUAL 21
AMPER 20
DOUBLESLASH 19
STAREQUAL 16
TILDE 10
ELLIPSIS 6
VBAREQUAL 5
RIGHTSHIFT 3
SLASHEQUAL 2
ERRORTOKEN 2
AMPEREQUAL 1
pretrained_BERT distribution:
{0: 104243, 1: 28415, 2: 27737, 3: 24899, 4: 23914, 5: 21043, 6: 18332, 7: 15063, 8: 10649, 9: 8365, 10: 7255, 11: 7221, 12: 7056, 13: 6667, 14: 3985, 15: 3586, 16: 928, 17: 870, 18: 827, 19: 701, 20: 673, 21: 606, 22: 459, 23: 242, 24: 204, 25: 187, 26: 177, 27: 176, 28: 165, 29: 90, 30: 72, 31: 55, 32: 40, 33: 39, 34: 32, 35: 22, 36: 21, 37: 20, 38: 19, 39: 16, 40: 10, 41: 6, 42: 5, 43: 3, 44: 2, 45: 2, 46: 1}
pretrained_BERT distribution after trauncating:
{0: 0.3206677720322012, 1: 0.0874089842223938, 2: 0.08532335018041658, 3: 0.07659321830559153, 4: 0.07356320424755676, 5: 0.06473155921139655, 6: 0.05639209919989172, 7: 0.04633614391490121, 8: 0.03275798954722054, 9: 0.025732048320264794, 10: 0.022317514711718004, 11: 0.022212925393978733, 12: 0.021705359587302856, 13: 0.020508734746109432, 14: 0.012258483270323396, 15: 0.011031096865089008, 16: 0.002854673143001283, 17: 0.0026762560715637026, 18: 0.0025439813461875654, 19: 0.0021563856392714433, 20: 0.002070253259956749, 21: 0.0018641507808823032, 22: 0.0014119557894801604, 23: 0.0007444298497912828, 24: 0.0006275359064356268, 25: 0.0005752412475659912, 26: 0.0005444796835250292, 27: 0.0005414035271209329, 28: 0.0005075658066758746, 29: 0.0002768540763686589, 30: 0.0002214832610949271, 31: 0.00016918860222529155, 32: 0.0001230462561638484, 33: 0.00011997009975975218, 34: 9.843700493107872e-05, 35: 6.767544089011662e-05, 36: 6.45992844860204e-05, 37: 6.15231280819242e-05, 38: 5.8446971677827984e-05, 39: 4.921850246553936e-05, 40: 3.07615640409621e-05}
Training classification probe
Creating model...
Number of training instances: 260064
Number of classes: 41
Epoch: [1/10], Loss: 0.0298
Epoch: [2/10], Loss: 0.0287
Epoch: [3/10], Loss: 0.0291
Epoch: [4/10], Loss: 0.0289
Epoch: [5/10], Loss: 0.0287
Epoch: [6/10], Loss: 0.0284
Epoch: [7/10], Loss: 0.0288
Epoch: [8/10], Loss: 0.0287
Epoch: [9/10], Loss: 0.0283
Epoch: [10/10], Loss: 0.0285
Accuracy on the test set of probing pretrained_BERT of all layers:
Score (accuracy) of the probe: 0.98
{'__OVERALL__': 0.9793438639125152, 'NAME': 0.9990423290557364, 'NEWLINE': 0.9946780202235231, 'DOT': 0.9998181156784285, 'LPAR': 1.0, 'RPAR': 0.9997910572503134, 'KEYWORD': 0.9015719467956469, 'COMMA': 1.0, 'EQUAL': 0.9996665555185061, 'COLON': 1.0, 'DEDENT': 0.8902147971360382, 'INDENT': 0.9943820224719101, 'LSQB': 0.9993069993069993, 'RSQB': 0.9451566951566952, 'NUMBER': 0.9977957384276267, 'STRING': 0.9195544554455446, 'NL': 0.8188010899182562, 'PLUS': 1.0, 'LBRACE': 0.978494623655914, 'EQEQUAL': 0.9084967320261438, 'MINUS': 0.9929577464788732, 'RBRACE': 1.0, 'STAR': 0.4198473282442748, 'PERCENT': 0.47572815533980584, 'DOUBLESTAR': 1.0, 'PLUSEQUAL': 0.0, 'GREATER': 0.0, 'SLASH': 0.0, 'NOTEQUAL': 0.0, 'AT': 0.7575757575757576, 'LESS': 0.0, 'COMMENT': 0.0, 'GREATEREQUAL': 0.0, 'LESSEQUAL': 0.0, 'VBAR': 0.0, 'LEFTSHIFT': 0.0, 'SEMI': 0.0, 'MINEQUAL': 0.0, 'AMPER': 0.0, 'DOUBLESLASH': 0.0, 'STAREQUAL': 0.0, 'TILDE': 0.0}
Accuracy on the test set of pretrained_BERT model using the intercept:
Score (accuracy) of the probe: 0.32
pretrained_BERT top neurons
array([4100, 2054,   35, 8228,   41, 4137, 4139, 6187, 4149, 4156, 2115,
       4163, 6216,   74, 8268,   78,   80, 2129,   86, 8281,   92,   98,
         99, 8291, 6249, 8304, 2161, 8306,  114,  126, 6281,  144,  148,
        155,  160, 8364,  176, 6320,  180, 2231, 2234, 6331,  187, 6333,
        198, 6345, 6351, 2260, 4309,  225,  228, 2281,  239, 2290, 6386,
       6392, 4350, 6402,  262, 6407, 8461,  270,  275, 2332, 2335, 2343,
       2350, 8496, 4408, 2375, 4431, 8527, 2390, 4442, 2395,  350, 6503,
        362, 4459, 8557, 8561, 4471,  394, 2456, 2464, 2465,  416, 6565,
       4525,  430, 4527, 6577, 8628,  437, 4534,  439, 6588, 8648, 6607,
       4566,  472, 8673,  484,  490, 4586, 8693, 2551,  505, 4602, 6662,
       4614, 6664, 8714, 6668,  525, 8721, 2582, 6681, 2585,  556, 4656,
       6706, 6714,  572, 2622, 8771,  580, 6726, 4680,  586,  590, 6735,
        591,  593,  597, 6746,  605,  616, 4715, 4717,  627, 6771, 2682,
       4731,  639, 8833,  644, 4742, 8840, 6792, 6800,  659, 2710, 8858,
       4764, 8866,  682, 6826, 8884, 6842, 2747,  699,  703, 4799,  707,
       6852, 2757, 8899, 2758, 6857, 4820, 8917, 4822, 8921,  731, 2786,
       6890,  754,  756, 8952,  760,  763, 2813,  770, 8972, 4885, 2841,
       8996,  806, 4903, 9000, 2857,  811, 6958, 2866,  821, 9014, 4919,
        822,  825, 6970, 4922, 4924,  832,  838, 9031, 6984, 6986, 9036,
        846, 2899, 2900,  852, 9049, 7009, 2914,  866, 9062, 2922, 9066,
       7020, 2940,  898, 2954, 9108, 5012,  919, 7064,  921, 9116,  928,
       5025, 7074, 7082,  938, 7096, 5049, 3002, 3010, 5059, 7110, 3014,
        967, 5063, 7115, 7121, 7133, 5087, 9185, 5089, 7139, 3043, 5094,
       3050, 3051, 5100, 1023, 3080, 5136, 3096, 1061, 3114, 9265, 9269,
       7222, 5173, 3126, 3125, 7230, 5187, 3144, 5199, 7248, 5203, 5216,
       3178, 9323, 9324, 3180, 7282, 3191, 1153, 1157, 1166, 1169, 3220,
       7340, 1198, 9396, 3253, 9405, 1213, 5318, 1223, 1227, 7371, 9421,
       3280, 7385, 7395, 5351, 9453, 1270, 1271, 7414, 5370, 9466, 3324,
       5372, 9470, 7424, 1282, 9474, 9482, 9489, 1301, 9495, 3355, 5416,
       9514, 1324, 1334, 7479, 1338, 3387, 1354, 1359, 3412, 9572, 3432,
       5492, 5500, 7559, 5511, 1420, 7570, 1433, 1447, 9640, 7596, 9647,
       1458, 7603, 9667, 3523, 7623, 5587, 1493, 3554, 9701, 7654, 3582,
       3587, 5641, 1552, 5651, 1582, 1584, 5687, 7738, 7752, 5717, 1630,
       1632, 9830, 1642, 9840, 7804, 3713, 5768, 7817, 5770, 1689, 3745,
       3753, 1706, 5803, 1711, 3759, 5809, 9903, 5814, 1724, 1727, 7878,
       9929, 9935, 7894, 7905, 1764, 5878, 1791, 5887, 5891, 1796, 3851,
       3862, 5913, 7964, 3874, 7973, 3882, 5941, 7990, 5955, 5956, 3912,
       3914, 8016, 1874, 5971, 5972, 1878, 5982, 1891, 8036, 1909, 1916,
       3972, 6028, 6031, 3993, 4003, 8103, 6056, 1968, 6067, 6068, 4021,
       4029, 6080, 1988, 6088, 6097, 2007, 8152, 8153, 6109, 8170, 8171,
       4085, 8182, 6138, 4091, 4092])
pretrained_BERT top neurons per class
{'NAME': array([1169,  262, 6664]), 'NEWLINE': array([8561]), 'DOT': array([5100]), 'LPAR': array([1270]), 'RPAR': array([572]), 'KEYWORD': array([505, 919]), 'COMMA': array([9269, 4003, 9830, 9572, 1711]), 'EQUAL': array([7074]), 'COLON': array([1153]), 'DEDENT': array([225]), 'INDENT': array([2332]), 'LSQB': array([763]), 'RSQB': array([41]), 'NUMBER': array([2922]), 'STRING': array([586]), 'NL': array([7230]), 'PLUS': array([126]), 'LBRACE': array([605]), 'EQEQUAL': array([5891]), 'MINUS': array([3759, 2375, 4527]), 'RBRACE': array([639]), 'STAR': array([5351, 4100, 1796]), 'PERCENT': array([484]), 'DOUBLESTAR': array([5370, 8228, 6984, 6138, 5199, 5500, 7248, 6216, 7804, 7752, 5814,
       9647, 8996, 4602, 8016, 4431, 4717, 6735, 4680, 3144, 9935, 3912,
       6681, 3972, 7990, 2747, 7222, 2682, 5913, 6088, 5641]), 'PLUSEQUAL': array([4350, 7110, 9116, 5941, 7878, 5173, 8153, 6109, 8721, 7385, 9514,
       8921, 6028, 3355]), 'GREATER': array([2395,  144]), 'SLASH': array([ 228,  430, 3862,  832, 6577, 7009, 5809, 1552, 6067, 4715, 1282,
       3126, 7096, 7973, 2866, 6958,  846, 5049, 1166, 1271, 3753, 8182,
       1433, 4822,  490, 2710, 1447, 8171,  591, 3851, 1420, 6392, 7964,
       5878, 9421,   41, 4149]), 'NOTEQUAL': array([ 754, 5768, 8840, 2940, 3324, 9324, 4100, 5955, 8364, 7596, 6970,
        556, 4092,  616, 6331, 2129, 6407, 3191, 7738, 8527, 8833]), 'AT': array([  74,  590, 2813]), 'LESS': array([9396,  148,  756,   92,   78, 8306, 3882, 4525, 9031, 7894, 5982,
       4731, 9489, 1354, 6706, 3080, 3220, 6333, 5492, 6890, 7395, 6565,
       3178, 4922, 4309, 2260, 2115, 1584, 7282, 3002, 3745, 6668, 6714,
       9185]), 'COMMENT': array([7817,  275,  770, 4459, 6097, 6068, 1791,  350, 8771,  580, 2390,
         99,  525,  825, 5087, 7414, 1727, 3125, 3096, 9405,  187,  699,
       7064, 8673,  472, 7020, 7570, 2161, 1632,  707, 5803, 8866, 2465,
       2900,   86, 4885, 3412, 4091, 9474, 6852]), 'GREATEREQUAL': array([7905, 1061, 1227, 2914, 6031, 1968, 3713,   98, 6842, 3387, 2231,
       4924, 1988, 9066,  703,  176, 8952, 1909, 2464, 4156, 2622,  180,
       3432, 7133, 7570,  362, 4799, 3014, 4085, 8291, 2350, 8170, 2551,
       5587]), 'LESSEQUAL': array([8628,  597, 7340, 6800, 7479,  967, 1689, 7623, 5025, 3280, 9265,
       1023, 6857, 3587, 7559, 7082, 1582, 2582,  811,   35, 7654,  394,
       9108, 8557, 2343, 6249, 8496, 1874, 1270, 7395, 1338, 3914,  760,
        921, 4149]), 'VBAR': array([4903, 8648, 1706]), 'LEFTSHIFT': array([2585, 9470, 5687, 6320, 5971, 6056, 1642, 8714, 1724,  439, 1359,
       4656, 8152, 3253, 9062, 4139, 2281, 4919, 7603, 5318, 2234,  852,
        644, 6402, 5089, 4021, 8461, 5651, 6726,  239, 2954, 6351,  866,
        898, 5094]), 'SEMI': array([5770, 9014, 3993, 1198,  437, 5372, 5136, 2335, 2054, 2290,  114,
       4137, 1334, 8771,  682,  593, 8972, 9000, 2007, 4715, 9482, 7371,
       5187,  659, 5887, 2899, 2757, 4586, 6503, 4742, 2857, 9929, 7424,
       7115, 5956, 1493, 6792, 6986, 6746,  627]), 'MINEQUAL': array([4566, 9701, 8304, 7121, 6662, 8858, 4029,  270,  806,  838, 9640,
       9466, 3582, 4764, 8917, 8036, 6080, 5416, 9840, 6607, 8884, 7139,
       4820, 9495, 4614, 3010, 1223, 5063,  822, 6386, 1891, 6187,   80,
       2456, 9323, 3874]), 'AMPER': array([9667, 3523, 5059, 9453, 8899, 6345]), 'DOUBLESLASH': array([1916, 6826, 1301, 3051, 5203, 5216, 5717, 6281, 4408,  756,  416,
       5511, 5972, 1324, 8281, 4471, 6588,  160, 2758, 1157, 6771, 9049,
       1764, 3114,  155, 1458,  928, 4163,  731, 8103, 1630,  198, 2841,
       3050,  938, 1213, 9903, 5012, 1878, 4534]), 'STAREQUAL': array([2786, 8693,  821, 9036, 3554, 8268, 4442]), 'TILDE': array([3180, 3043])}
The shape of selected features (260064, 445)
Training classification probe
Creating model...
Number of training instances: 260064
Number of classes: 41
Epoch: [1/10], Loss: 0.0075
Epoch: [2/10], Loss: 0.0063
Epoch: [3/10], Loss: 0.0062
Epoch: [4/10], Loss: 0.0062
Epoch: [5/10], Loss: 0.0062
Epoch: [6/10], Loss: 0.0062
Epoch: [7/10], Loss: 0.0062
Epoch: [8/10], Loss: 0.0062
Epoch: [9/10], Loss: 0.0062
Epoch: [10/10], Loss: 0.0062
Accuracy on the test set of pretrained_BERT model on top 2% neurons:
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 260064
Number of classes: 41
Epoch: [1/10], Loss: 0.0085
Epoch: [2/10], Loss: 0.0066
Epoch: [3/10], Loss: 0.0065
Epoch: [4/10], Loss: 0.0065
Epoch: [5/10], Loss: 0.0065
Epoch: [6/10], Loss: 0.0065
Epoch: [7/10], Loss: 0.0065
Epoch: [8/10], Loss: 0.0065
Epoch: [9/10], Loss: 0.0065
Epoch: [10/10], Loss: 0.0065
Accuracy on the test set of pretrained_BERT model on top 200 neurons:
Creating control dataset for pretrained_BERT POS tagging task
['\\n/21']
Number of tokens:  325100
length of source dictionary:  17255
length of target dictionary:  47
325100
Total instances: 325100
['file_name', 'local_branch', '"Indexed"', 'fullpath', '"type"', 'wrap', 'CachedMetaClass', '0x1000', 'CopyImageMetadata', 'LGMap', 'INSTANCE_DB_NAME', 'tnum', 'g_T', 'NODES_PER_SLAB', 'ColorNoise', 'test_wo_principals_service', 'testHandlesAreHumanReadable', 'lblChangegt', 'break_on', 'prec']
Number of samples:  325100
Stats: Labels with their frequencies in the final set
21 33876
7 31701
31 27526
44 26994
24 22605
0 17620
6 16339
32 12605
27 11358
22 9857
25 9620
3 9065
2 6794
30 6018
38 4998
18 4234
5 4167
37 4031
26 3516
11 3423
14 3369
10 3044
12 2900
23 2864
8 2459
33 2400
35 2395
19 2374
16 2361
40 2340
34 2232
29 2170
46 2141
4 2069
1 2066
20 2054
15 1993
42 1969
43 1935
39 1911
41 1909
36 1786
45 1712
9 1626
28 1573
13 1551
17 1550
pretrained_BERT_control_task distribution:
{0: 33876, 1: 31701, 2: 27526, 3: 26994, 4: 22605, 5: 17620, 6: 16339, 7: 12605, 8: 11358, 9: 9857, 10: 9620, 11: 9065, 12: 6794, 13: 6018, 14: 4998, 15: 4234, 16: 4167, 17: 4031, 18: 3516, 19: 3423, 20: 3369, 21: 3044, 22: 2900, 23: 2864, 24: 2459, 25: 2400, 26: 2395, 27: 2374, 28: 2361, 29: 2340, 30: 2232, 31: 2170, 32: 2141, 33: 2069, 34: 2066, 35: 2054, 36: 1993, 37: 1969, 38: 1935, 39: 1911, 40: 1909, 41: 1786, 42: 1712, 43: 1626, 44: 1573, 45: 1551, 46: 1550}
pretrained_BERT_control_task distribution after trauncating:
{0: 0.1074398513171499, 1: 0.10054170287533856, 2: 0.08730042942956277, 3: 0.08561315817850823, 4: 0.07169317035730823, 5: 0.05588293128492683, 6: 0.0518201597198876, 7: 0.039977545337485966, 8: 0.03602260689751413, 9: 0.031262091582038805, 10: 0.030510431269069018, 11: 0.028750214080468885, 12: 0.021547595638467247, 13: 0.01908646313692904, 14: 0.01585146938490717, 15: 0.013428395633392747, 16: 0.013215900945759938, 17: 0.012784568445490356, 18: 0.011151213756969508, 19: 0.010856258444285161, 20: 0.010684994069178121, 21: 0.009654236256033897, 22: 0.009197531255748457, 23: 0.009083355005677097, 24: 0.007798872192374295, 25: 0.007611750004757343, 26: 0.007595892192247432, 27: 0.007529289379705806, 28: 0.007488059067180037, 29: 0.00742145625463841, 30: 0.00707892750442433, 31: 0.006882290629301431, 32: 0.006790315316743947, 33: 0.006561962816601227, 34: 0.00655244812909528, 35: 0.006514389379071494, 36: 0.006320924066450577, 37: 0.006244806566403004, 38: 0.006136973441335608, 39: 0.006060855941288035, 40: 0.0060545128162840705}
Training classification probe
Creating model...
Number of training instances: 252241
Number of classes: 41
Epoch: [1/10], Loss: 0.1020
Epoch: [2/10], Loss: 0.1016
Epoch: [3/10], Loss: 0.1016
Epoch: [4/10], Loss: 0.1016
Epoch: [5/10], Loss: 0.1017
Epoch: [6/10], Loss: 0.1016
Epoch: [7/10], Loss: 0.1015
Epoch: [8/10], Loss: 0.1014
Epoch: [9/10], Loss: 0.1015
Epoch: [10/10], Loss: 0.1015
Accuracy on the test set of pretrained_BERT control model:
Score (accuracy) of the probe: 0.79
pretrained_BERT Selectivity (Diff. between true task and probing task performance):  0.18438342877828007
Accuracy on the test set of pretrained_BERT control model using the intercept:
Score (accuracy) of the probe: 0.10
Training classification probe
Creating model...
Number of training instances: 252241
Number of classes: 41
Epoch: [1/10], Loss: 0.0354
Epoch: [2/10], Loss: 0.0341
Epoch: [3/10], Loss: 0.0341
Epoch: [4/10], Loss: 0.0341
Epoch: [5/10], Loss: 0.0341
Epoch: [6/10], Loss: 0.0341
Epoch: [7/10], Loss: 0.0341
Epoch: [8/10], Loss: 0.0341
Epoch: [9/10], Loss: 0.0341
Epoch: [10/10], Loss: 0.0341
Accuracy on the test set of pretrained_BERT control model on top neurons:
Score (accuracy) of the probe: 0.81
Training classification probe
Creating model...
Number of training instances: 252241
Number of classes: 41
Epoch: [1/10], Loss: 0.0357
Epoch: [2/10], Loss: 0.0344
Epoch: [3/10], Loss: 0.0343
Epoch: [4/10], Loss: 0.0343
Epoch: [5/10], Loss: 0.0343
Epoch: [6/10], Loss: 0.0343
Epoch: [7/10], Loss: 0.0343
Epoch: [8/10], Loss: 0.0343
Epoch: [9/10], Loss: 0.0343
Epoch: [10/10], Loss: 0.0343
Accuracy on the test set of pretrained_BERT control model on top 200 neurons:
Score (accuracy) of the probe: 0.81
----------------------------------------------------------------
