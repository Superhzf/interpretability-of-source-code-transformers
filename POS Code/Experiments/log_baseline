Getting activations from json files. If you need to extract them, run with --extract=True 

Anayzing pretrained_BERT
Loading json activations from bert_activations.json...
32002 13.0
Number of tokens:  325100
length of source dictionary:  17255
length of target dictionary:  47
325100
Total instances: 325100
['do_compile_media', '_fetch', 'ExprCaseFilter', 'res13', 'HTTPWorkerConnection', 'analogsignalarrays', 'update_add', '_getformat', 'SqliteRecorder', 'format_exception', 'CurrentTaskFilter', 'Warning', '__instancecheck__', 'DBSession', 'Describable', 'BLUE', 'defer', 'parseXMLTimestamp', 'Parameterised', 'header']
Number of samples:  325100
Stats: Labels with their frequencies in the final set
NAME 104243
NEWLINE 28415
DOT 27737
LPAR 24899
RPAR 23914
KEYWORD 21043
COMMA 18332
EQUAL 15063
COLON 10649
DEDENT 8365
INDENT 7255
LSQB 7221
RSQB 7056
NUMBER 6667
STRING 3985
NL 3586
PLUS 928
LBRACE 870
EQEQUAL 827
MINUS 701
RBRACE 673
STAR 606
PERCENT 459
DOUBLESTAR 242
PLUSEQUAL 204
GREATER 187
SLASH 177
NOTEQUAL 176
AT 165
LESS 90
COMMENT 72
GREATEREQUAL 55
LESSEQUAL 40
VBAR 39
LEFTSHIFT 32
SEMI 22
MINEQUAL 21
AMPER 20
DOUBLESLASH 19
STAREQUAL 16
TILDE 10
ELLIPSIS 6
VBAREQUAL 5
RIGHTSHIFT 3
SLASHEQUAL 2
ERRORTOKEN 2
AMPEREQUAL 1
pretrained_BERT distribution:
{0: 104243, 1: 28415, 2: 27737, 3: 24899, 4: 23914, 5: 21043, 6: 18332, 7: 15063, 8: 10649, 9: 8365, 10: 7255, 11: 7221, 12: 7056, 13: 6667, 14: 3985, 15: 3586, 16: 928, 17: 870, 18: 827, 19: 701, 20: 673, 21: 606, 22: 459, 23: 242, 24: 204, 25: 187, 26: 177, 27: 176, 28: 165, 29: 90, 30: 72, 31: 55, 32: 40, 33: 39, 34: 32, 35: 22, 36: 21, 37: 20, 38: 19, 39: 16, 40: 10, 41: 6, 42: 5, 43: 3, 44: 2, 45: 2, 46: 1}
pretrained_BERT distribution after trauncating:
{0: 0.3206677720322012, 1: 0.0874089842223938, 2: 0.08532335018041658, 3: 0.07659321830559153, 4: 0.07356320424755676, 5: 0.06473155921139655, 6: 0.05639209919989172, 7: 0.04633614391490121, 8: 0.03275798954722054, 9: 0.025732048320264794, 10: 0.022317514711718004, 11: 0.022212925393978733, 12: 0.021705359587302856, 13: 0.020508734746109432, 14: 0.012258483270323396, 15: 0.011031096865089008, 16: 0.002854673143001283, 17: 0.0026762560715637026, 18: 0.0025439813461875654, 19: 0.0021563856392714433, 20: 0.002070253259956749, 21: 0.0018641507808823032, 22: 0.0014119557894801604, 23: 0.0007444298497912828, 24: 0.0006275359064356268, 25: 0.0005752412475659912, 26: 0.0005444796835250292, 27: 0.0005414035271209329, 28: 0.0005075658066758746, 29: 0.0002768540763686589, 30: 0.0002214832610949271, 31: 0.00016918860222529155, 32: 0.0001230462561638484, 33: 0.00011997009975975218, 34: 9.843700493107872e-05, 35: 6.767544089011662e-05, 36: 6.45992844860204e-05, 37: 6.15231280819242e-05, 38: 5.8446971677827984e-05, 39: 4.921850246553936e-05, 40: 3.07615640409621e-05}
Training classification probe
Creating model...
Number of training instances: 260064
Number of classes: 41
Epoch: [1/10], Loss: 0.0299
Epoch: [2/10], Loss: 0.0293
Epoch: [3/10], Loss: 0.0291
Epoch: [4/10], Loss: 0.0289
Epoch: [5/10], Loss: 0.0289
Epoch: [6/10], Loss: 0.0288
Epoch: [7/10], Loss: 0.0286
Epoch: [8/10], Loss: 0.0282
Epoch: [9/10], Loss: 0.0286
Epoch: [10/10], Loss: 0.0288
Accuracy on the test set of probing pretrained_BERT of all layers:
Score (accuracy) of the probe: 0.98
{'__OVERALL__': 0.9838196164080164, 'NAME': 0.9917640298793334, 'NEWLINE': 0.9748092957246762, 'DOT': 0.9981811567842852, 'LPAR': 0.9984167821096378, 'RPAR': 0.9977016297534476, 'KEYWORD': 0.9908101571946796, 'COMMA': 1.0, 'EQUAL': 0.9973324441480493, 'COLON': 1.0, 'DEDENT': 0.9558472553699284, 'INDENT': 0.9810393258426966, 'LSQB': 1.0, 'RSQB': 0.9978632478632479, 'NUMBER': 0.9933872152828802, 'STRING': 0.9170792079207921, 'NL': 0.888283378746594, 'PLUS': 0.9944444444444445, 'LBRACE': 1.0, 'EQEQUAL': 0.8888888888888888, 'MINUS': 1.0, 'RBRACE': 1.0, 'STAR': 0.13740458015267176, 'PERCENT': 0.8543689320388349, 'DOUBLESTAR': 1.0, 'PLUSEQUAL': 0.0, 'GREATER': 0.0, 'SLASH': 0.0, 'NOTEQUAL': 0.0, 'AT': 1.0, 'LESS': 0.0, 'COMMENT': 0.0, 'GREATEREQUAL': 0.0, 'LESSEQUAL': 0.0, 'VBAR': 0.0, 'LEFTSHIFT': 0.0, 'SEMI': 0.0, 'MINEQUAL': 0.0, 'AMPER': 0.0, 'DOUBLESLASH': 0.0, 'STAREQUAL': 0.0, 'TILDE': 0.0}
Accuracy on the test set of pretrained_BERT model using the intercept:
Score (accuracy) of the probe: 0.32
pretrained_BERT top neurons
array([4100, 2066, 6164, 2070,   27, 4129, 8228,   41, 4139, 8244, 6216,
       4169,   74,   79, 4181, 6230, 6237, 6239,  107, 8303, 8310, 8311,
        126, 8321, 4230,  136, 4238,  144, 8337, 2195, 4249,  157, 2207,
       8356,  165,  170,  176,  180, 4278, 6332, 6334, 4291, 6345, 8398,
        209,  222,  225, 6370,  239, 2293, 8438, 6391, 4350,  259,  262,
       4368,  272, 8464, 6427, 2332,  297, 2350, 2353, 4405, 4406, 8503,
       8508, 6464,  322,  325, 2375,  338,  339,  342,  344, 2395, 6493,
       6494, 8546, 8555, 8556, 8561,  375, 6536, 2444, 4494, 6547, 4504,
       4509, 4512,  421,  430, 4527, 4526,  435,  437, 6582, 6583, 6597,
       4551,  463, 4566, 4567, 2525,  478, 6625, 6627,  484,  505, 4602,
        515, 6661, 4614, 6664, 2576, 4629, 4634, 2589, 2598,  550, 6694,
        554,  556, 2613, 2615,  568, 2622,  582, 4680,  586,  590, 6735,
        597, 4694, 4697,  605, 8803,  621, 4722, 2680,  639,  645, 6791,
       2697,  651, 2700, 8855, 6810, 6813, 4766,  675,  679,  684, 2747,
        700,  704, 8899, 2755, 2762, 6862,  720, 6868, 2778, 4832, 2788,
        741,  744,  754, 6900, 6906,  763,  777,  779, 8989,  799,  803,
       8996, 4903, 2859, 9009, 4914, 2875, 4925, 4933, 6984, 7002, 7007,
       2922, 4974, 4985, 5000,  919,  920,  921, 5020, 5024, 7074, 7081,
       2986,  939, 7085, 7098, 5059, 3012,  968,  973, 7121, 5075, 5077,
       1000, 5100, 1004, 9196, 1017, 1020, 9220, 1029, 9223, 1037, 1038,
       1041, 1042, 7188, 3093, 7190, 7193, 7199, 5155, 5170, 9266, 5173,
       9269, 7222, 5175, 9270, 7230, 9279, 3143, 3144, 7239, 9290, 1099,
       5199, 1107, 1116, 1122, 9315, 7269, 3186, 3187, 9331, 7285, 1142,
       3188, 1153, 1155, 7304, 5260, 1169, 3241, 1200, 9394, 1205, 7350,
       7354, 3260, 7362, 7364, 3272, 3278, 1231, 3293, 9441, 1250, 3299,
       5351, 5359, 5361, 5365, 1270, 7414, 1272, 7416, 5370, 1275, 1277,
       3326, 9477, 9483, 9500, 1309, 1314, 7462, 7463, 9514, 3370, 9517,
       3378, 1348, 5448, 3401, 5453, 5456, 1384, 5483, 5485, 1394, 7542,
       3452, 9598, 1409, 9605, 3484, 9633, 3492, 7594, 1465, 5566, 1470,
       7615, 9666, 3523, 9668, 9667, 7623, 7624, 9676, 7630, 3545, 3547,
       7645, 9694, 1524, 9717, 1530, 1544, 7691, 3595, 9749, 3612, 5665,
       5669, 3633, 7730, 9779, 7738, 5694, 7752, 5710, 1619, 3669, 3671,
       7767, 7771, 1628, 7777, 9830, 1639, 1642, 7790, 9842, 9846, 5754,
       5756, 5766, 9876, 3734, 7839, 1696, 5802, 1706, 9900, 3759, 5812,
       3773, 7869, 9919, 5829, 5836, 9934, 9935, 7887, 9940, 3798, 7905,
       3813, 5864, 9964, 5869, 7919, 1777, 7922, 9973, 1786, 1788, 5891,
       1810, 3859, 1818, 3874, 5941, 5943, 7994, 5948, 3902, 5951, 1854,
       3912, 3918, 5980, 8034, 1909, 1910, 1914, 1916, 1921, 1925, 3974,
       1929, 6028, 3980, 1941, 1955, 6057, 8106, 6062, 4016, 1971, 8122,
       8123, 4028, 6080, 8132, 6088, 8139, 4043, 6096, 8153, 8157, 6109,
       6115, 6125, 6132, 2039, 6138, 4093])
pretrained_BERT top neurons per class
{'NAME': array([1169,  262, 6664]), 'NEWLINE': array([8561]), 'DOT': array([5100]), 'LPAR': array([1270, 2778, 3773]), 'RPAR': array([107]), 'KEYWORD': array([505, 919]), 'COMMA': array([9830, 9269]), 'EQUAL': array([7074]), 'COLON': array([1153, 1921]), 'DEDENT': array([225]), 'INDENT': array([2332]), 'LSQB': array([763]), 'RSQB': array([41]), 'NUMBER': array([2922, 3452]), 'STRING': array([ 586, 9315]), 'NL': array([7230]), 'PLUS': array([126]), 'LBRACE': array([605, 209, 645]), 'EQEQUAL': array([5891, 8244]), 'MINUS': array([3759, 2375, 4527, 3143,  136,  779, 4139]), 'RBRACE': array([639]), 'STAR': array([5351, 4100]), 'PERCENT': array([484]), 'DOUBLESTAR': array([6984, 6138, 5370, 6216, 6080, 3144, 3912, 7350, 4602, 5199, 9935,
       6906, 8228, 2615, 7752, 4680, 6427, 2747, 5448, 1971, 6735, 6230,
       5864, 1955, 6582, 7188, 7222, 3918, 1941, 8996, 8503]), 'PLUSEQUAL': array([4350, 7007, 5173, 4405, 6028, 7869, 8438, 8157, 5260, 6239, 6109,
       9514, 7645, 5941, 8153, 3671, 7730, 7777, 9876]), 'GREATER': array([2395,  144, 1000]), 'SLASH': array([605]), 'NOTEQUAL': array([ 754, 3902,  556, 1020, 9919, 6536,  421, 7304, 1788, 7738, 5359,
       2700, 1818, 6547, 1910, 5000, 3378, 8556, 1272, 1142]), 'AT': array([ 74, 590, 720]), 'LESS': array([6132, 9668, 7285, 6900, 7364, 8803, 4494, 2622, 7623, 1275, 3186,
       1037, 2697, 1544, 2859, 8132]), 'COMMENT': array([ 222,  344, 3798, 6115, 3187, 1628, 7414, 4933,  679, 8321, 4566,
       6370, 4985, 3012, 3547, 6088,  375, 8123, 5566, 2762, 1205, 5694,
       9441,  435,  684, 1200, 4509, 6334, 7121, 1155, 6661, 1004, 1384,
       8989,  176, 4974, 5756, 7542, 6627]), 'GREATEREQUAL': array([ 777, 5754,  322, 7098, 9517, 5077, 8337, 7691,  621,  515, 1231,
       9220, 3874, 1277, 8356, 1642, 5802, 8139, 7624, 5829, 7085, 2039,
       2350, 3401,  597, 4278, 4169, 3299, 1041,   79, 1696, 3272,  921,
        568]), 'LESSEQUAL': array([5710, 1116, 1309, 3974,  157, 2353, 1250, 3326, 1955,  339,  180,
        920,   27,  973,  259, 4028, 4093,  437, 1914,  968, 9605, 7354,
       4129, 1524, 9009, 9483, 1777,  597, 1786, 5766,  342, 9749, 2066,
       1041, 6813, 7416, 4832]), 'VBAR': array([4903, 1706,  297,  170]), 'LEFTSHIFT': array([6164, 5170,  799, 6332, 1530,  463, 1909, 3669, 1099, 1017, 1038,
       1929,  239, 7594, 1314, 5365, 9223, 4368,  621, 7269, 6493, 2525,
       5669, 3612, 9694, 3293, 4914,  700, 5175, 9842, 4697, 4551, 4016,
       2444, 3773]), 'SEMI': array([2576, 9900, 8555, 4504, 5155, 7463, 4181, 5951, 5836, 4697, 4249,
       8310, 9940,  338, 7081,  763, 8106, 5483,  165, 6583, 6810, 5361,
        272, 5456, 7190, 8034, 5485, 4567, 6868, 4634, 9290, 7304, 8508,
        325, 1122, 9331, 2875, 8311]), 'MINEQUAL': array([6391, 1107, 3278,  803, 8122, 5980, 6791, 5020, 6125, 5869, 7994,
       5665, 1029,  704, 3370, 2589,  651, 7905, 4230, 5943, 9934, 1348,
       9666, 3260, 9717, 1810, 1854,  744, 2195, 4566, 3188, 6096, 9598,
       4722, 4614,  675, 8855]), 'AMPER': array([3523, 5059, 4291, 7230, 9279, 6345, 9973, 8899, 2755, 9667]), 'DOUBLESLASH': array([9266, 6597,  430, 1925, 2613, 2598, 3813, 1639,  582,  939, 7839,
       4512, 9779, 2986, 1619, 6464, 9270, 1042, 7790, 1916, 7542, 2788,
       1465, 5948,  554, 7002, 4169, 5075, 1470, 7239, 3545,  478, 4766,
       3859, 5024, 3492,  921,  550, 6057, 2697, 6625, 3093]), 'STAREQUAL': array([9964, 8398, 9196, 4526, 6062, 7630, 7615, 9676, 9477, 7771, 4406,
       6862, 5453, 4925, 8303]), 'TILDE': array([4694, 8464, 2207, 2070, 3633, 6237, 9633, 3241, 7919, 4043, 9846,
       5812, 9500, 7193,  741, 4629, 7462, 1409, 3595, 7199, 9394, 4238,
       1394, 3980, 3734, 7922, 2680, 7767, 6494, 3484, 2293, 7362, 7887,
       8546, 6694])}
The shape of selected features (260064, 446)
Training classification probe
Creating model...
Number of training instances: 260064
Number of classes: 41
Epoch: [1/10], Loss: 0.0074
Epoch: [2/10], Loss: 0.0062
Epoch: [3/10], Loss: 0.0061
Epoch: [4/10], Loss: 0.0061
Epoch: [5/10], Loss: 0.0061
Epoch: [6/10], Loss: 0.0061
Epoch: [7/10], Loss: 0.0061
Epoch: [8/10], Loss: 0.0061
Epoch: [9/10], Loss: 0.0061
Epoch: [10/10], Loss: 0.0061
Accuracy on the test set of pretrained_BERT model on top 2% neurons:
Score (accuracy) of the probe: 0.98
Training classification probe
Creating model...
Number of training instances: 260064
Number of classes: 41
Epoch: [1/10], Loss: 0.0082
Epoch: [2/10], Loss: 0.0064
Epoch: [3/10], Loss: 0.0063
Epoch: [4/10], Loss: 0.0063
Epoch: [5/10], Loss: 0.0063
Epoch: [6/10], Loss: 0.0063
Epoch: [7/10], Loss: 0.0063
Epoch: [8/10], Loss: 0.0063
Epoch: [9/10], Loss: 0.0063
Epoch: [10/10], Loss: 0.0063
Accuracy on the test set of pretrained_BERT model on top 200 neurons:
Creating control dataset for pretrained_BERT POS tagging task
['\\n/41']
Number of tokens:  325100
length of source dictionary:  17255
length of target dictionary:  47
325100
Total instances: 325100
['do_compile_media', '_fetch', 'ExprCaseFilter', 'res13', 'HTTPWorkerConnection', 'analogsignalarrays', 'update_add', '_getformat', 'SqliteRecorder', 'format_exception', 'CurrentTaskFilter', 'Warning', '__instancecheck__', 'DBSession', 'Describable', 'BLUE', 'defer', 'parseXMLTimestamp', 'Parameterised', 'header']
Number of samples:  325100
Stats: Labels with their frequencies in the final set
37 51747
41 37187
29 27202
20 26587
25 24137
9 17502
34 12388
12 11987
44 10409
35 9107
26 5122
39 4556
22 4444
27 4343
38 4114
7 3948
28 3489
2 3259
8 2950
11 2852
36 2847
30 2784
3 2512
13 2502
1 2474
10 2473
23 2466
5 2391
14 2346
46 2328
24 2320
43 2227
16 2203
6 2125
21 2101
33 2085
0 2053
15 2006
18 1934
19 1914
45 1903
31 1790
42 1682
17 1646
40 1604
32 1545
4 1509
Training classification probe
Creating model...
Number of training instances: 260080
Number of classes: 47
Epoch: [1/10], Loss: 0.1135
Epoch: [2/10], Loss: 0.1133
Epoch: [3/10], Loss: 0.1132
Epoch: [4/10], Loss: 0.1132
Epoch: [5/10], Loss: 0.1132
Epoch: [6/10], Loss: 0.1131
Epoch: [7/10], Loss: 0.1131
Epoch: [8/10], Loss: 0.1131
Epoch: [9/10], Loss: 0.1133
Epoch: [10/10], Loss: 0.1131
Accuracy on the test set of pretrained_BERT control model:
Score (accuracy) of the probe: 0.75
pretrained_BERT Selectivity (Diff. between true task and probing task performance):  0.23835668192631843
Accuracy on the test set of pretrained_BERT control model using the intercept:
Score (accuracy) of the probe: 0.16
Training classification probe
Creating model...
Number of training instances: 260080
Number of classes: 47
Epoch: [1/10], Loss: 0.0390
Epoch: [2/10], Loss: 0.0378
Epoch: [3/10], Loss: 0.0378
Epoch: [4/10], Loss: 0.0378
Epoch: [5/10], Loss: 0.0378
Epoch: [6/10], Loss: 0.0378
Epoch: [7/10], Loss: 0.0378
Epoch: [8/10], Loss: 0.0378
Epoch: [9/10], Loss: 0.0378
Epoch: [10/10], Loss: 0.0378
Accuracy on the test set of pretrained_BERT control model on top neurons:
Score (accuracy) of the probe: 0.79
Training classification probe
Creating model...
Number of training instances: 260080
Number of classes: 47
Epoch: [1/10], Loss: 0.0397
Epoch: [2/10], Loss: 0.0383
Epoch: [3/10], Loss: 0.0383
Epoch: [4/10], Loss: 0.0383
Epoch: [5/10], Loss: 0.0383
Epoch: [6/10], Loss: 0.0383
Epoch: [7/10], Loss: 0.0383
Epoch: [8/10], Loss: 0.0383
Epoch: [9/10], Loss: 0.0383
Epoch: [10/10], Loss: 0.0383
Accuracy on the test set of pretrained_BERT control model on top 200 neurons:
Score (accuracy) of the probe: 0.78
----------------------------------------------------------------
