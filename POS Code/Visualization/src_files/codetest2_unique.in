\n 
# \n 
from horizon import tabs \n 
class NetworkProfileTab ( tabs . Tab ) : \n 
slug = "network_profile" \n 
template_name = \n 
def get_context_data ( self , request ) : \n 
~~~ return None \n 
~~ ~~ class PolicyProfileTab ( tabs . Tab ) : \n 
slug = "policy_profile" \n 
preload = False \n 
~~ class IndexTabs ( tabs . TabGroup ) : \n 
~~~ slug = "indextabs" \n 
tabs = ( NetworkProfileTab , PolicyProfileTab ) \n 
import weakref \n 
from eventlet import corolocal \n 
class WeakLocal ( corolocal . local ) : \n 
~~~ def __getattribute__ ( self , attr ) : \n 
~~~ rval = corolocal . local . __getattribute__ ( self , attr ) \n 
if rval : \n 
~~~ rval = rval ( ) \n 
~~ return rval \n 
~~ def __setattr__ ( self , attr , value ) : \n 
~~~ value = weakref . ref ( value ) \n 
return corolocal . local . __setattr__ ( self , attr , value ) \n 
~~ ~~ store = WeakLocal ( ) \n 
weak_store = WeakLocal ( ) \n 
strong_store = corolocal . local \n 
import eventlet \n 
eventlet . monkey_patch ( ) \n 
import contextlib \n 
import sys \n 
from oslo . config import cfg \n 
from openstack_dashboard . openstack . common import log as logging \n 
from openstack_dashboard . openstack . common import rpc \n 
from openstack_dashboard . openstack . common . rpc import impl_zmq \n 
CONF = cfg . CONF \n 
CONF . register_opts ( rpc . rpc_opts ) \n 
CONF . register_opts ( impl_zmq . zmq_opts ) \n 
def main ( ) : \n 
~~~ CONF ( sys . argv [ 1 : ] , project = ) \n 
logging . setup ( "oslo" ) \n 
with contextlib . closing ( impl_zmq . ZmqProxy ( CONF ) ) as reactor : \n 
~~~ reactor . consume_in_thread ( ) \n 
reactor . wait ( ) \n 
~~ ~~ from openstack_dashboard import api \n 
from openstack_dashboard . test import helpers as test \n 
from neutronclient . v2_0 import client \n 
neutronclient = client . Client \n 
class VPNaasApiTests ( test . APITestCase ) : \n 
~~~ @ test . create_stubs ( { neutronclient : ( , ) } ) \n 
def test_vpnservice_create ( self ) : \n 
~~~ vpnservice1 = self . api_vpnservices . first ( ) \n 
form_data = { \n 
: vpnservice1 [ ] , \n 
: vpnservice1 [ ] \n 
} \n 
vpnservice = { : self . api_vpnservices . first ( ) } \n 
neutronclient . create_vpnservice ( \n 
{ : form_data } ) . AndReturn ( vpnservice ) \n 
self . mox . ReplayAll ( ) \n 
ret_val = api . vpn . vpnservice_create ( self . request , ** form_data ) \n 
self . assertIsInstance ( ret_val , api . vpn . VPNService ) \n 
~~ @ test . create_stubs ( { neutronclient : ( , ) } ) \n 
def test_vpnservices_get ( self ) : \n 
~~~ vpnservices = { : self . vpnservices . list ( ) } \n 
vpnservices_dict = { : self . api_vpnservices . list ( ) } \n 
neutronclient . list_vpnservices ( ) . AndReturn ( vpnservices_dict ) \n 
ret_val = api . vpn . vpnservices_get ( self . request ) \n 
for ( v , d ) in zip ( ret_val , vpnservices [ ] ) : \n 
~~~ self . assertIsInstance ( v , api . vpn . VPNService ) \n 
self . assertTrue ( v . name , d . name ) \n 
self . assertTrue ( v . id ) \n 
~~ ~~ @ test . create_stubs ( { neutronclient : ( , ) } ) \n 
def test_vpnservice_get ( self ) : \n 
vpnservice = { : vpnservice1 } \n 
neutronclient . show_vpnservice ( \n 
vpnservice [ ] [ ] ) . AndReturn ( vpnservice ) \n 
ret_val = api . vpn . vpnservice_get ( self . request , \n 
vpnservice [ ] [ ] ) \n 
def test_ikepolicy_create ( self ) : \n 
~~~ ikepolicy1 = self . api_ikepolicies . first ( ) \n 
: ikepolicy1 [ ] , \n 
: ikepolicy1 [ ] \n 
ikepolicy = { : self . api_ikepolicies . first ( ) } \n 
neutronclient . create_ikepolicy ( \n 
{ : form_data } ) . AndReturn ( ikepolicy ) \n 
ret_val = api . vpn . ikepolicy_create ( self . request , ** form_data ) \n 
self . assertIsInstance ( ret_val , api . vpn . IKEPolicy ) \n 
def test_ikepolicies_get ( self ) : \n 
~~~ ikepolicies = { : self . ikepolicies . list ( ) } \n 
ikepolicies_dict = { : self . api_ikepolicies . list ( ) } \n 
neutronclient . list_ikepolicies ( ) . AndReturn ( ikepolicies_dict ) \n 
ret_val = api . vpn . ikepolicies_get ( self . request ) \n 
for ( v , d ) in zip ( ret_val , ikepolicies [ ] ) : \n 
~~~ self . assertIsInstance ( v , api . vpn . IKEPolicy ) \n 
def test_ikepolicy_get ( self ) : \n 
ikepolicy = { : ikepolicy1 } \n 
neutronclient . show_ikepolicy ( \n 
ikepolicy [ ] [ ] ) . AndReturn ( ikepolicy ) \n 
ret_val = api . vpn . ikepolicy_get ( self . request , \n 
ikepolicy [ ] [ ] ) \n 
def test_ipsecpolicy_create ( self ) : \n 
~~~ ipsecpolicy1 = self . api_ipsecpolicies . first ( ) \n 
: ipsecpolicy1 [ ] , \n 
: ipsecpolicy1 [ ] \n 
ipsecpolicy = { : self . api_ipsecpolicies . first ( ) } \n 
neutronclient . create_ipsecpolicy ( \n 
{ : form_data } ) . AndReturn ( ipsecpolicy ) \n 
ret_val = api . vpn . ipsecpolicy_create ( self . request , ** form_data ) \n 
self . assertIsInstance ( ret_val , api . vpn . IPSecPolicy ) \n 
def test_ipsecpolicies_get ( self ) : \n 
~~~ ipsecpolicies = { : self . ipsecpolicies . list ( ) } \n 
ipsecpolicies_dict = { : self . api_ipsecpolicies . list ( ) } \n 
neutronclient . list_ipsecpolicies ( ) . AndReturn ( ipsecpolicies_dict ) \n 
ret_val = api . vpn . ipsecpolicies_get ( self . request ) \n 
for ( v , d ) in zip ( ret_val , ipsecpolicies [ ] ) : \n 
~~~ self . assertIsInstance ( v , api . vpn . IPSecPolicy ) \n 
def test_ipsecpolicy_get ( self ) : \n 
ipsecpolicy = { : ipsecpolicy1 } \n 
neutronclient . show_ipsecpolicy ( \n 
ipsecpolicy [ ] [ ] ) . AndReturn ( ipsecpolicy ) \n 
ret_val = api . vpn . ipsecpolicy_get ( self . request , \n 
ipsecpolicy [ ] [ ] ) \n 
def test_ipsecsiteconnection_create ( self ) : \n 
~~~ ipsecsiteconnection1 = self . api_ipsecsiteconnections . first ( ) \n 
: ipsecsiteconnection1 [ ] , \n 
: ipsecsiteconnection1 [ ] \n 
ipsecsiteconnection = { : \n 
self . api_ipsecsiteconnections . first ( ) } \n 
neutronclient . create_ipsec_site_connection ( \n 
{ : \n 
form_data } ) . AndReturn ( ipsecsiteconnection ) \n 
ret_val = api . vpn . ipsecsiteconnection_create ( \n 
self . request , ** form_data ) \n 
self . assertIsInstance ( ret_val , api . vpn . IPSecSiteConnection ) \n 
def test_ipsecsiteconnections_get ( self ) : \n 
~~~ ipsecsiteconnections = { \n 
: self . ipsecsiteconnections . list ( ) } \n 
ipsecsiteconnections_dict = { \n 
: self . api_ipsecsiteconnections . list ( ) } \n 
neutronclient . list_ipsec_site_connections ( ) . AndReturn ( \n 
ipsecsiteconnections_dict ) \n 
ret_val = api . vpn . ipsecsiteconnections_get ( self . request ) \n 
for ( v , d ) in zip ( ret_val , \n 
ipsecsiteconnections [ ] ) : \n 
~~~ self . assertIsInstance ( v , api . vpn . IPSecSiteConnection ) \n 
def test_ipsecsiteconnection_get ( self ) : \n 
ipsecsiteconnection = { : ipsecsiteconnection1 } \n 
neutronclient . show_ipsec_site_connection ( \n 
ipsecsiteconnection [ ] [ ] ) . AndReturn ( \n 
ipsecsiteconnection ) \n 
ret_val = api . vpn . ipsecsiteconnection_get ( self . request , \n 
ipsecsiteconnection [ ] [ ] ) \n 
~~ ~~ from horizon import tables \n 
from openstack_dashboard . usage import base \n 
class UsageView ( tables . DataTableView ) : \n 
~~~ usage_class = None \n 
show_terminated = True \n 
def __init__ ( self , * args , ** kwargs ) : \n 
~~~ super ( UsageView , self ) . __init__ ( * args , ** kwargs ) \n 
if not issubclass ( self . usage_class , base . BaseUsage ) : \n 
~~ ~~ def get_template_names ( self ) : \n 
~~~ if self . request . GET . get ( , ) == : \n 
~~~ return "." . join ( ( self . template_name . rsplit ( , 1 ) [ 0 ] , ) ) \n 
~~ return self . template_name \n 
~~ def get_content_type ( self ) : \n 
~~~ return "text/csv" \n 
~~ return "text/html" \n 
~~ def get_data ( self ) : \n 
~~~ project_id = self . kwargs . get ( , self . request . user . tenant_id ) \n 
self . usage = self . usage_class ( self . request , project_id ) \n 
self . usage . summarize ( * self . usage . get_date_range ( ) ) \n 
self . usage . get_limits ( ) \n 
self . kwargs [ ] = self . usage \n 
return self . usage . usage_list \n 
~~ def get_context_data ( self , ** kwargs ) : \n 
~~~ context = super ( UsageView , self ) . get_context_data ( ** kwargs ) \n 
context [ ] . kwargs [ ] = self . usage \n 
context [ ] = self . usage . form \n 
context [ ] = self . usage \n 
return context \n 
~~ def render_to_response ( self , context , ** response_kwargs ) : \n 
~~~ render_class = self . csv_response_class \n 
response_kwargs . setdefault ( "filename" , "usage.csv" ) \n 
~~ else : \n 
~~~ render_class = self . response_class \n 
~~ resp = render_class ( request = self . request , \n 
template = self . get_template_names ( ) , \n 
context = context , \n 
content_type = self . get_content_type ( ) , \n 
** response_kwargs ) \n 
return resp \n 
~~ ~~ from enum import IntEnum \n 
from . component import Component \n 
from . object import field \n 
class ReflectionProbeUsage ( IntEnum ) : \n 
~~~ Off = 0 \n 
BlendProbes = 1 \n 
BlendProbesAndSkybox = 2 \n 
Simple = 3 \n 
~~ class ShadowCastingMode ( IntEnum ) : \n 
On = 1 \n 
TwoSided = 2 \n 
ShadowsOnly = 3 \n 
~~ class Renderer ( Component ) : \n 
~~~ enabled = field ( "m_Enabled" , bool ) \n 
lightmap_index = field ( "m_LightmapIndex" ) \n 
materials = field ( "m_Materials" ) \n 
probe_anchor = field ( "m_ProbeAnchor" ) \n 
receive_shadows = field ( "m_ReceiveShadows" , bool ) \n 
reflection_probe_usage = field ( "m_ReflectionProbeUsage" , ReflectionProbeUsage ) \n 
shadow_casting_mode = field ( "m_CastShadows" , ShadowCastingMode ) \n 
sorting_layer_id = field ( "m_SortingLayerID" ) \n 
sorting_order = field ( "m_SortingOrder" ) \n 
use_light_probes = field ( "m_UseLightProbes" , bool ) \n 
lightmap_index_dynamic = field ( "m_LightmapIndexDynamic" ) \n 
lightmap_tiling_offset = field ( "m_LightmapTilingOffset" ) \n 
lightmap_tiling_offset_dynamic = field ( "m_LightmapTilingOffsetDynamic" ) \n 
static_batch_root = field ( "m_StaticBatchRoot" ) \n 
subset_indices = field ( "m_SubsetIndices" ) \n 
@ property \n 
def material ( self ) : \n 
~~~ return self . materials [ 0 ] \n 
~~ ~~ class ParticleSystemRenderMode ( IntEnum ) : \n 
~~~ Billboard = 0 \n 
Stretch = 1 \n 
HorizontalBillboard = 2 \n 
VerticalBillboard = 3 \n 
Mesh = 4 \n 
~~ class ParticleSystemSortMode ( IntEnum ) : \n 
~~~ None_ = 0 \n 
Distance = 1 \n 
OldestInFront = 2 \n 
YoungestInFront = 3 \n 
~~ class MeshRenderer ( Component ) : \n 
~~~ pass \n 
~~ class ParticleRenderer ( Renderer ) : \n 
~~~ camera_velocity_scale = field ( "m_CameraVelocityScale" ) \n 
length_scale = field ( "m_LengthScale" ) \n 
max_particle_size = field ( "m_MaxParticleSize" ) \n 
velocity_scale = field ( "m_VelocityScale" ) \n 
stretch_particles = field ( "m_StretchParticles" ) \n 
~~ class ParticleSystemRenderer ( Renderer ) : \n 
mesh = field ( "m_Mesh" ) \n 
mesh1 = field ( "m_Mesh1" ) \n 
mesh2 = field ( "m_Mesh2" ) \n 
mesh3 = field ( "m_Mesh3" ) \n 
normal_direction = field ( "m_NormalDirection" ) \n 
render_mode = field ( "m_RenderMode" , ParticleSystemRenderMode ) \n 
sort_mode = field ( "m_SortMode" , ParticleSystemSortMode ) \n 
sorting_fudge = field ( "m_SortingFudge" ) \n 
~~ from ConfigParser import * \n 
from StringIO import * \n 
from Log import Log \n 
import datetime \n 
class Config : \n 
~~~ @ staticmethod \n 
def LoadConfig ( ) : \n 
~~~ Config . parser = ConfigParser ( ) \n 
try : \n 
~~~ sconff = open ( CONFIG_FILE , "r" ) \n 
~~ except : \n 
return \n 
~~ sconf = StringIO ( ) \n 
sconf . write ( "[sysconf]\\n" ) \n 
sconf . write ( sconff . read ( ) ) \n 
sconf . seek ( 0 ) \n 
Config . parser . readfp ( sconf ) \n 
sconff . close ( ) \n 
sconf . close ( ) \n 
~~ @ staticmethod \n 
def GetBoardsFile ( ) : \n 
~~~ return BOARDS_FILE \n 
def GetInt ( name , defval ) : \n 
~~~ if ( Config . parser . has_option ( , name ) ) : \n 
~~~ return Config . parser . getint ( , name ) \n 
~~~ return defval \n 
~~ ~~ @ staticmethod \n 
def GetString ( name , defval ) : \n 
~~~ val = Config . parser . get ( , name ) \n 
if ( val [ 0 ] == \'"\' and val . endswith ( \'"\' ) ) : \n 
~~~ val = val [ 1 : - 1 ] \n 
~~ return val . decode ( ) \n 
~~ ~~ ~~ BBS_ROOT = \n 
BBS_XMPP_CERT_FILE = BBS_ROOT + "xmpp.crt" \n 
BBS_XMPP_KEY_FILE = BBS_ROOT + "xmpp.key" \n 
BOARDS_FILE = BBS_ROOT + \n 
STRLEN = 80 \n 
ARTICLE_TITLE_LEN = 60 \n 
BM_LEN = 60 \n 
MAXBOARD = 400 \n 
CONFIG_FILE = BBS_ROOT + \n 
FILENAME_LEN = 20 \n 
OWNER_LEN = 30 \n 
SESSIONID_LEN = 32 \n 
REFRESH_TOKEN_LEN = 128 \n 
NAMELEN = 40 \n 
IDLEN = 12 \n 
MD5PASSLEN = 16 \n 
OLDPASSLEN = 14 \n 
MOBILE_NUMBER_LEN = 17 \n 
MAXCLUB = 128 \n 
MAXUSERS = 20000 \n 
MAX_MSG_SIZE = 1024 \n 
MAXFRIENDS = 400 \n 
MAXMESSAGE = 5 \n 
MAXSIGLINES = 6 \n 
IPLEN = 16 \n 
DEFAULTBOARD = "sysop" \n 
BLESS_BOARD = "happy_birthday" \n 
QUOTED_LINES = 10 \n 
MAXACTIVE = 8000 \n 
USHM_SIZE = MAXACTIVE + 10 \n 
UTMP_HASHSIZE = USHM_SIZE * 4 \n 
UCACHE_SEMLOCK = 0 \n 
LEN_FRIEND_EXP = 15 \n 
SESSION_TIMEOUT = datetime . timedelta ( 30 ) \n 
SESSION_TIMEOUT_SECONDS = 86400 * 30 \n 
XMPP_IDLE_TIME = 300 \n 
XMPP_LONG_IDLE_TIME = 1800 \n 
XMPP_UPDATE_TIME_INTERVAL = 10 \n 
XMPP_PING_TIME_INTERVAL = 60 \n 
PUBLIC_SHMKEY = 3700 \n 
MAX_ATTACHSIZE = 20 * 1024 * 1024 \n 
BMDEL_DECREASE = True \n 
SYSMAIL_BOARD = "sysmail" \n 
ADD_EDITMARK = True \n 
SEARCH_COUNT_LIMIT = 20 \n 
MAIL_SIZE_LIMIT = - 1 \n 
SEC_DELETED_OLDHOME = 3600 * 24 * 3 \n 
SELF_INTRO_MAX_LEN = 800 \n 
import re \n 
import os \n 
import stat \n 
import json \n 
import struct \n 
import time \n 
import Config \n 
import Board \n 
import Post \n 
import BoardManager \n 
from Util import Util \n 
from errors import * \n 
DEFAULT_DIGEST_LIST_COUNT = 20 \n 
class DigestItem : \n 
~~~ def __init__ ( self , basepath ) : \n 
~~~ self . basepath = basepath \n 
self . title = \n 
self . host = \n 
self . port = 0 \n 
self . attachpos = 0 \n 
self . fname = \n 
self . mtitle = \n 
self . items = [ ] \n 
self . update_time = 0 \n 
self . id = 0 \n 
self . sysop_only = 0 \n 
self . bms_only = 0 \n 
self . zixia_only = 0 \n 
~~ def IsDir ( self ) : \n 
~~~ try : \n 
~~~ st = os . stat ( self . realpath ( ) ) \n 
return stat . S_ISDIR ( st . st_mode ) \n 
~~~ return False \n 
~~ ~~ def IsFile ( self ) : \n 
return stat . S_ISREG ( st . st_mode ) \n 
~~ ~~ def GetModTime ( self ) : \n 
mtime = st . st_mtime \n 
~~~ mtime = time . time ( ) \n 
~~ return mtime \n 
~~ def names_path ( self ) : \n 
~~~ return "%s/.Names" % self . realpath ( ) \n 
~~ def realpath ( self ) : \n 
~~~ return "%s/%s" % ( Config . BBS_ROOT , self . path ( ) ) \n 
~~ def path ( self ) : \n 
~~~ if ( self . fname ) : \n 
~~~ return "%s/%s" % ( self . basepath , self . fname ) \n 
~~~ return self . basepath \n 
~~ ~~ def CheckUpdate ( self ) : \n 
~~~ stat = os . stat ( self . names_path ( ) ) \n 
if ( stat . st_mtime > self . update_time ) : \n 
~~~ self . LoadNames ( ) \n 
~~ ~~ except : \n 
~~ return True \n 
~~ def LoadNames ( self ) : \n 
~~~ f = open ( self . names_path ( ) , "r" ) \n 
~~ except IOError : \n 
~~~ return 0 \n 
~~ stat = os . fstat ( f . fileno ( ) ) \n 
self . update_time = stat . st_mtime \n 
item = DigestItem ( self . path ( ) ) \n 
hostname = \n 
_id = 0 \n 
bms_only = 0 \n 
sysop_only = 0 \n 
zixia_only = 0 \n 
while ( True ) : \n 
~~~ line = f . readline ( ) \n 
if ( line == "" ) : break \n 
npos = line . find ( "\\n" ) \n 
if ( npos != - 1 ) : line = line [ : npos ] \n 
if ( line [ : 1 ] == ) : \n 
~~~ if ( not self . mtitle ) : \n 
~~~ self . mtitle = line [ 8 : ] \n 
~~ ~~ ~~ result = re . match ( , line ) \n 
if ( result ) : \n 
~~~ key = result . group ( 1 ) \n 
value = result . group ( 2 ) \n 
if ( key == "Name" ) : \n 
~~~ item . title = value \n 
item . attachpos = 0 \n 
~~ elif ( key == "Path" ) : \n 
~~~ if ( value [ : 2 ] == "~/" ) : \n 
~~~ item . fname = value [ 2 : ] \n 
~~~ item . fname = value \n 
~~ if ( item . fname . find ( ".." ) != - 1 ) : \n 
~~~ continue \n 
~~~ bms_only += 1 \n 
~~~ sysop_only += 1 \n 
~~~ zixia_only += 1 \n 
~~ if ( item . fname . find ( "!@#$%" ) != - 1 ) : \n 
~~~ parts = re . split ( , item . fname ) \n 
newparts = [ ] \n 
for part in parts : \n 
~~~ if ( part ) : \n 
~~~ newparts += [ part ] \n 
~~ ~~ hostname = newparts [ 0 ] \n 
item . fname = newparts [ 1 ] \n 
~~~ item . port = int ( newparts [ 2 ] ) \n 
~~~ item . port = 0 \n 
~~ ~~ item . id = _id \n 
_id += 1 \n 
item . bms_only = bms_only \n 
item . sysop_only = sysop_only \n 
item . zixia_only = zixia_only \n 
item . host = hostname \n 
self . items += [ item ] \n 
~~ elif ( key == "Host" ) : \n 
~~~ hostname = value \n 
~~ elif ( key == "Port" ) : \n 
~~~ item . port = int ( value ) \n 
~~ ~~ elif ( key == "Attach" ) : \n 
~~~ item . attachpos = int ( value ) \n 
~~~ item . attachpos = 0 \n 
~~ ~~ ~~ ~~ f . close ( ) \n 
return 1 \n 
~~ def GetItem ( self , user , route , has_perm = False , need_perm = False ) : \n 
~~~ self . CheckUpdate ( ) \n 
if ( self . mtitle . find ( "(BM:" ) != - 1 ) : \n 
~~~ if ( Board . Board . IsBM ( user , self . mtitle [ 4 : ] , ) or user . IsSysop ( ) ) : \n 
~~~ has_perm = True \n 
~~ elif ( need_perm and not has_perm ) : \n 
~~ if ( len ( route ) == 0 ) : \n 
~~~ return self \n 
~~ target = route [ 0 ] - 1 \n 
_id = target \n 
if ( _id >= len ( self . items ) ) : \n 
~~ while ( self . items [ _id ] . EffectiveId ( user ) < target ) : \n 
~~~ _id += 1 \n 
~~ ~~ item = self . items [ _id ] \n 
item . mtitle = item . title \n 
if ( len ( route ) == 1 ) : \n 
~~~ return item \n 
~~~ if ( item . IsDir ( ) ) : \n 
~~~ if ( not item . CheckUpdate ( ) ) : \n 
~~ return item . GetItem ( user , route [ 1 : ] , has_perm , need_perm ) \n 
~~ ~~ ~~ def GetRange ( self , user , route , start , end , has_perm = False , need_perm = False ) : \n 
firstitem = self . GetItem ( user , route + [ start ] , has_perm , need_perm ) \n 
if ( not firstitem ) : \n 
~~~ return [ ] \n 
~~ parent = self . GetItem ( user , route , has_perm , need_perm ) \n 
if ( not parent ) : \n 
~~ if ( not parent . IsDir ( ) ) : \n 
~~ result = [ ] \n 
_id = start - 1 \n 
for i in range ( start , end + 1 ) : \n 
~~~ target = i - 1 \n 
if ( _id >= len ( parent . items ) ) : \n 
~~ while ( parent . items [ _id ] . EffectiveId ( user ) < target ) : \n 
~~~ return result \n 
~~ ~~ item = parent . items [ _id ] \n 
result += [ item ] \n 
~~ return result \n 
~~ def EffectiveId ( self , user ) : \n 
~~~ _id = self . id \n 
if ( user . IsSysop ( ) ) : \n 
~~~ return _id \n 
~~ if ( not user . IsSysop ( ) ) : \n 
~~~ _id -= self . sysop_only \n 
~~ if ( not user . IsBM ( ) ) : \n 
~~~ _id -= self . bms_only \n 
~~ if ( not user . IsSECANC ( ) ) : \n 
~~~ _id -= self . zixia_only \n 
~~ return _id \n 
~~ def GetInfo ( self ) : \n 
~~~ info = { } \n 
info [ ] = Util . gbkDec ( self . mtitle ) \n 
info [ ] = Util . gbkDec ( self . title ) \n 
info [ ] = self . attachpos \n 
if ( self . host != ) : \n 
~~~ info [ ] = self . host \n 
info [ ] = self . port \n 
info [ ] = \n 
~~ elif ( self . IsDir ( ) ) : \n 
~~~ info [ ] = \n 
~~ elif ( self . IsFile ( ) ) : \n 
~~ info [ ] = int ( self . GetModTime ( ) ) \n 
return info \n 
~~ def GetInfoForUser ( self , user ) : \n 
~~~ info = self . GetInfo ( ) \n 
info [ ] = self . EffectiveId ( user ) + 1 \n 
~~ def GetAttachLink ( self , session ) : \n 
filename = \n 
for i in range ( 2 ) : \n 
~~~ filename += "%0x" % struct . unpack ( , _hash [ i * 4 : ( i + 1 ) * 4 ] ) \n 
~~ link = "http://%s/bbscon.php?b=xattach&f=%s" % ( session . GetMirror ( Config . Config . GetInt ( , 80 ) ) , filename ) \n 
linkfile = "%s/boards/xattach/%s" % ( Config . BBS_ROOT , filename ) \n 
target = "../../%s" % self . path ( ) \n 
~~~ os . symlink ( target , linkfile ) \n 
~~ return link \n 
~~ ~~ class Digest : \n 
~~~ root = DigestItem ( "0Announce" ) \n 
def __init__ ( self , board , path ) : \n 
~~~ self . board = board \n 
self . path = path \n 
self . root = DigestItem ( self . path ) \n 
def GET ( svc , session , params , action ) : \n 
~~~ if ( session is None ) : raise Unauthorized ( ) \n 
user = session . GetUser ( ) \n 
boardname = svc . get_str ( params , , ) \n 
if ( boardname ) : \n 
~~~ board = BoardManager . BoardManager . GetBoard ( boardname ) \n 
if ( board is None ) : raise NotFound ( % boardname ) \n 
if ( not board . CheckReadPerm ( user ) ) : \n 
~~~ raise NoPerm ( ) \n 
~~ basenode = board . digest . root \n 
has_perm = user . IsDigestMgr ( ) or user . IsSysop ( ) or user . IsSuperBM ( ) \n 
~~~ basenode = Digest . root \n 
has_perm = user . IsDigestMgr ( ) \n 
~~ if ( action == "list" ) : \n 
~~~ route = svc . get_str ( params , ) \n 
start = svc . get_int ( params , , 1 ) \n 
end = svc . get_int ( params , , start + DEFAULT_DIGEST_LIST_COUNT - 1 ) \n 
Digest . List ( svc , basenode , route , start , end , session , has_perm ) \n 
~~ elif ( action == "view" ) : \n 
start = svc . get_int ( params , , 0 ) \n 
count = svc . get_int ( params , , 0 ) \n 
Digest . View ( svc , basenode , route , session , has_perm , start , count ) \n 
~~~ raise WrongArgs ( % action ) \n 
def ParseRoute ( route ) : \n 
~~~ ret = [ ] \n 
items = re . split ( , route ) \n 
items = items [ 1 : ] \n 
for item in items : \n 
~~~ ret += [ int ( item ) ] \n 
~~~ raise WrongArgs ( % item ) \n 
~~ ~~ return ret \n 
def List ( svc , basenode , route , start , end , session , has_perm ) : \n 
~~~ route_array = Digest . ParseRoute ( route ) \n 
parent = basenode . GetItem ( session . GetUser ( ) , route_array , has_perm ) \n 
~~~ raise WrongArgs ( % route ) \n 
~~ items = basenode . GetRange ( session . GetUser ( ) , route_array , start , end , has_perm ) \n 
result = { } \n 
result [ ] = parent . GetInfoForUser ( session . GetUser ( ) ) \n 
result [ ] = len ( items ) \n 
result_list = [ ] \n 
~~~ result_list += [ item . GetInfoForUser ( session . GetUser ( ) ) ] \n 
~~ result [ ] = result_list \n 
svc . writedata ( json . dumps ( result ) ) \n 
def View ( svc , basenode , route , session , has_perm , start , count ) : \n 
item = basenode . GetItem ( session . GetUser ( ) , route_array , has_perm ) \n 
if ( not item ) : \n 
~~ if ( not item . IsFile ( ) ) : \n 
~~ result = { } \n 
result [ ] = item . GetInfoForUser ( session . GetUser ( ) ) \n 
postinfo = Post . Post ( item . realpath ( ) , None ) \n 
( result [ ] , result [ ] ) = postinfo . GetContent ( start , count ) \n 
attachlist = postinfo . GetAttachListByType ( ) \n 
result [ ] = attachlist [ 0 ] \n 
result [ ] = attachlist [ 1 ] \n 
if ( attachlist [ 0 ] or attachlist [ 1 ] ) : \n 
~~~ result [ ] = item . GetAttachLink ( session ) \n 
~~ svc . writedata ( json . dumps ( result ) ) \n 
~~ ~~ import time \n 
import UserManager \n 
import UserInfo \n 
from Session import Session \n 
import UCache \n 
import MsgBox \n 
import xmpp \n 
import modes \n 
import Util \n 
import traceback \n 
from xmpp . features import NoRoute \n 
__disco_info_ns__ = \n 
__disco_items_ns__ = \n 
__vcard_ns__ = \n 
STEAL_AFTER_SEEN = 3 \n 
def elem_to_str ( elem ) : \n 
~~ class XMPPServer ( xmpp . Plugin ) : \n 
def __init__ ( self , rosters , host ) : \n 
~~~ self . probed = False \n 
self . _closed = False \n 
self . rosters = rosters \n 
self . _session = None \n 
self . rosters . set_resources ( self . get_resources ( ) ) \n 
self . _fixedjid = UCache . UCache . formalize_jid ( unicode ( self . authJID ) ) \n 
self . _userid = self . _fixedjid . partition ( ) [ 0 ] . encode ( "gbk" ) \n 
if ( not self . rosters . allow_login ( self . authJID . bare ) ) : \n 
self . stream_error ( , ) \n 
if self . authJID . resource [ : - 8 ] != "Resource" and len ( self . authJID . resource ) > 8 : \n 
~~~ routes = self . routes ( self . authJID . bare ) \n 
for route in routes : \n 
~~~ jid = route [ 0 ] \n 
if jid . resource [ : - 8 ] == self . authJID . resource [ : - 8 ] : \n 
~~~ if jid . resource != self . authJID . resource : \n 
route [ 1 ] . stream_error ( , ) \n 
~~ ~~ else : \n 
~~ ~~ ~~ except NoRoute : \n 
~~ self . _user = UserManager . UserManager . LoadUser ( self . _userid ) \n 
if ( self . _user == None ) : \n 
~~ self . _peer_addr = self . getpeername ( ) \n 
self . _session = Session ( self . _user , self . _peer_addr [ 0 ] ) \n 
self . _session . RecordLogin ( ) \n 
self . _userinfo = self . _session . Register ( ) \n 
self . _loginid = self . _session . utmpent \n 
self . _hostname = host \n 
self . bind ( xmpp . ReceivedCloseStream , self . recv_close ) \n 
self . bind ( xmpp . StreamClosed , self . stream_closed ) \n 
self . bind ( xmpp . SentCloseStream , self . sent_close ) \n 
self . rosters . register_conn ( self ) \n 
msgbox = MsgBox . MsgBox ( self . _userid ) \n 
if self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) is None : \n 
~~~ self . rosters . set_xmpp_read ( self . _user . GetUID ( ) , msgbox . GetMsgCount ( all = False ) - msgbox . GetUnreadCount ( ) ) \n 
~~ self . check_msg ( ) \n 
~~ def get_loginid ( self ) : \n 
~~~ return self . _loginid \n 
~~ def recv_close ( self ) : \n 
return self . close ( ) \n 
~~ def stream_closed ( self ) : \n 
~~ def sent_close ( self ) : \n 
~~ def close ( self ) : \n 
~~~ if ( self . _closed ) : \n 
~~ self . _closed = True \n 
if ( self . _session ) : \n 
~~~ self . _session . Unregister ( ) \n 
~~ self . unbind_res ( ) \n 
self . rosters . unregister_conn ( self ) \n 
~~ @ xmpp . iq ( ) \n 
def ping ( self , iq ) : \n 
self . refresh ( ) \n 
return self . iq ( , iq ) \n 
~~ @ xmpp . stanza ( ) \n 
def message ( self , elem ) : \n 
to_jid = elem . get ( ) \n 
from_jid = elem . get ( ) \n 
if ( from_jid == None ) : \n 
~~~ return \n 
~~ text_body = None \n 
for child in elem : \n 
~~~ if ( child . tag . endswith ( ) ) : \n 
~~~ text_body = child . text \n 
~~ ~~ if ( text_body == None ) : \n 
~~ ret = self . rosters . send_msg ( from_jid , to_jid , text_body ) \n 
if ( ret <= 0 ) : \n 
errors = { \n 
if ( ret in errors ) : \n 
~~~ elem = self . E . message ( { : to_jid , \n 
: from_jid , \n 
: } , \n 
self . E . body ( errors [ ret ] ) ) \n 
self . recv ( from_jid , elem ) \n 
~~ ~~ ~~ def make_jid ( self , userid ) : \n 
~~~ return "%s@%s" % ( userid , self . _hostname ) \n 
~~ def refresh ( self ) : \n 
~~~ self . _userinfo . freshtime = int ( time . time ( ) ) \n 
self . _userinfo . save ( ) \n 
~~ def ping_result ( self , iq ) : \n 
~~~ self . refresh ( ) \n 
~~ def ping_client ( self ) : \n 
~~~ pingelem = self . E . ping ( xmlns = ) \n 
return self . iq ( , self . ping_result , pingelem ) \n 
~~ except Exception as e : \n 
Log . debug ( traceback . format_exc ( ) ) \n 
return False \n 
~~ ~~ def get_uid ( self ) : \n 
~~~ return self . _user . GetUID ( ) \n 
~~ def recv_msg ( self , from_ , msgtext ) : \n 
~~~ elem = self . E . message ( { : from_ , : unicode ( self . authJID ) } , \n 
self . E . body ( msgtext ) ) \n 
self . recv ( unicode ( self . authJID ) , elem ) \n 
~~ def check_msg ( self ) : \n 
msg_count = msgbox . GetMsgCount ( all = False ) \n 
my_pid = os . getpid ( ) \n 
xmpp_read = self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) \n 
if xmpp_read > msg_count : \n 
~~~ xmpp_read = 0 \n 
self . rosters . set_xmpp_read ( self . _user . GetUID ( ) , msg_count ) \n 
if xmpp_read < msg_count : \n 
~~~ return xmpp_read \n 
~~~ return - 1 \n 
~~ ~~ def deliver_msg ( self , start ) : \n 
for i in range ( start , msg_count ) : \n 
~~~ msghead = msgbox . LoadMsgHead ( i , all = False ) \n 
if msghead . topid == my_pid : \n 
~~~ msgtext = msgbox . LoadMsgText ( msghead ) \n 
self . recv_msg ( self . make_jid ( msghead . id ) , msgtext ) \n 
~~ ~~ ~~ def steal_msg ( self ) : \n 
msg_unread = msgbox . GetUnreadCount ( ) \n 
read_count = msg_count - msg_unread \n 
term_read = self . rosters . get_term_read ( self . get_uid ( ) ) \n 
term_stealed = self . rosters . get_term_stealed ( self . get_uid ( ) ) \n 
all_xmpp = True \n 
new_unread = { } \n 
for i in range ( read_count - 1 , msg_count ) : \n 
~~ msghead = msgbox . LoadMsgHead ( i , all = False ) \n 
if i >= read_count and all_xmpp : \n 
~~~ if msghead . topid == my_pid : \n 
~~~ msgbox . GetUnreadMsg ( ) \n 
~~~ all_xmpp = False \n 
~~ ~~ if msghead . topid == my_pid : \n 
~~~ session = self . rosters . find_session ( self . authJID . bare , msghead . topid ) \n 
if session is None or session . get_mode ( ) != modes . MSG : \n 
~~ if msghead . topid not in new_unread : \n 
new_unread [ msghead . topid ] = i \n 
~~ ~~ final_unread = { } \n 
to_steal = { } \n 
to_steal_begin = msg_count \n 
for pid in term_read : \n 
~~~ if pid in new_unread : \n 
~~~ if new_unread [ pid ] == term_read [ pid ] [ 0 ] : \n 
~~~ final_unread [ pid ] = ( term_read [ pid ] [ 0 ] , term_read [ pid ] [ 1 ] + 1 ) \n 
if final_unread [ pid ] [ 1 ] > STEAL_AFTER_SEEN : \n 
~~~ to_steal [ pid ] = final_unread [ pid ] \n 
if pid in term_stealed : \n 
~~~ steal_begin = max ( final_unread [ pid ] [ 0 ] , term_stealed [ pid ] + 1 ) \n 
~~~ steal_begin = final_unread [ pid ] [ 0 ] \n 
~~ if steal_begin < to_steal_begin : \n 
~~~ to_steal_begin = steal_begin \n 
~~ ~~ ~~ else : \n 
~~~ final_unread [ pid ] = ( new_unread [ pid ] , 1 ) \n 
pass \n 
~~ ~~ for pid in new_unread : \n 
~~~ if pid not in term_read : \n 
final_unread [ pid ] = ( new_unread [ pid ] , 1 ) \n 
~~ ~~ if to_steal : \n 
for i in range ( to_steal_begin , msg_count ) : \n 
msgbox . GetUnreadMsg ( ) \n 
~~ elif msghead . topid in to_steal : \n 
~~~ if msghead . topid not in term_stealed or i > term_stealed [ msghead . topid ] : \n 
msgtext = msgbox . LoadMsgText ( msghead ) \n 
term_stealed [ msghead . topid ] = i \n 
~~ ~~ ~~ ~~ self . rosters . set_term_read ( self . get_uid ( ) , final_unread ) \n 
def presence ( self , elem ) : \n 
if self . authJID == elem . get ( ) : \n 
~~~ if ( elem . get ( ) == None or ( not self . authJID . match_bare ( elem . get ( ) ) ) ) : \n 
~~~ return self . send_presence ( elem ) \n 
~~ ~~ self . recv_presence ( elem ) \n 
~~ def send_presence ( self , elem ) : \n 
direct = elem . get ( ) \n 
if not direct : \n 
~~~ self . rosters . broadcast ( self , elem ) \n 
if elem . get ( ) != : \n 
~~~ self . recv_presence ( elem ) \n 
~~ if not self . probed : \n 
~~~ self . probed = True \n 
self . rosters . probe ( self ) \n 
~~ ~~ elif not self . rosters . send ( self , direct , elem ) : \n 
~~~ self . send ( direct , elem ) \n 
~~ ~~ def recv_presence ( self , elem ) : \n 
if not self . rosters . recv ( self , elem ) : \n 
self . write ( elem ) \n 
~~ ~~ @ xmpp . iq ( ) \n 
def roster ( self , iq ) : \n 
roster = self . rosters . get ( self ) \n 
method = getattr ( self , % iq . get ( ) ) \n 
return method and method ( iq , roster ) \n 
~~ def get_roster ( self , iq , roster ) : \n 
~~~ query = self . E . query ( { : } ) \n 
for item in roster . items ( ) : \n 
~~~ query . append ( item ) \n 
~~ return self . iq ( , iq , query ) \n 
~~ def set_roster ( self , iq , roster ) : \n 
~~~ query = self . E . query ( xmlns = ) \n 
for item in iq [ 0 ] : \n 
~~~ result = roster . set ( item ) \n 
if result is not None : \n 
~~~ query . append ( result ) \n 
~~ ~~ if len ( query ) > 0 : \n 
~~~ self . push ( roster , query ) \n 
~~ return self . iq ( , iq ) \n 
~~ def push ( self , roster , query ) : \n 
for jid in roster . requests ( ) : \n 
~~~ for ( to , route ) in self . routes ( jid ) : \n 
~~~ route . iq ( , self . ignore , query ) \n 
~~ ~~ ~~ def ignore ( self , iq ) : \n 
def vcard ( self , iq ) : \n 
if iq . get ( ) == : \n 
~~~ if ( iq . get ( ) == None ) : \n 
~~~ target = iq . get ( ) \n 
~~ form_target = UCache . UCache . formalize_jid ( target ) \n 
name = form_target . partition ( ) [ 0 ] \n 
user = UserManager . UserManager . LoadUser ( name ) \n 
info = user . GetInfo ( ) \n 
desc = % ( info [ ] , info [ ] , info [ ] , \n 
info [ ] , info [ ] , info [ ] , info [ ] ) \n 
if ( in info ) : \n 
~~~ desc += "Plan:\\r%s" % ( info [ ] . replace ( , ) ) \n 
~~ vcard = self . E . vCard ( { : } , \n 
self . E ( , name ) , \n 
self . E ( , Util . Util . RemoveTags ( info [ ] ) ) , \n 
self . E ( , Util . Util . RemoveTags ( desc ) ) ) \n 
if ( iq . get ( ) == None ) : \n 
~~~ return self . iq ( , iq , vcard ) \n 
~~~ return self . iq ( , iq , vcard , { : iq . get ( ) } ) \n 
~~ ~~ ~~ @ xmpp . iq ( % __disco_info_ns__ ) \n 
def disco_info ( self , iq ) : \n 
target = iq . get ( ) \n 
if ( target . find ( ) < 0 ) : \n 
~~~ query = self . E . query ( { : __disco_info_ns__ } , \n 
self . E . identity ( { : , \n 
: , \n 
: Config . Config . GetString ( , ) , \n 
} ) ) \n 
features = [ __disco_info_ns__ , __disco_items_ns__ , __vcard_ns__ ] \n 
for feature in features : \n 
~~~ query . append ( self . E . feature ( { : feature } ) ) \n 
~~ ~~ return self . iq ( , iq , query , { : target } ) \n 
~~ @ xmpp . iq ( % __disco_items_ns__ ) \n 
def disco_items ( self , iq ) : \n 
~~~ query = self . E . query ( { : __disco_items_ns__ } ) \n 
~~ return self . iq ( , iq , query , { : target } ) \n 
### \n 
~~ ~~ from __future__ import print_function \n 
from __future__ import unicode_literals \n 
from __future__ import division \n 
from __future__ import absolute_import \n 
from builtins import range \n 
from future import standard_library \n 
standard_library . install_aliases ( ) \n 
PYTHON_VERSION = sys . version_info [ : 3 ] \n 
PY2 = ( PYTHON_VERSION [ 0 ] == 2 ) \n 
if PY2 : \n 
~~~ if PYTHON_VERSION < ( 2 , 7 , 9 ) : \n 
~~~ raise Exception ( ) \n 
~~ ~~ elif PYTHON_VERSION < ( 3 , 4 ) : \n 
~~ import hpOneView as hpov \n 
from pprint import pprint \n 
from hpOneView . common import uri \n 
import hpOneView . profile as profile \n 
def acceptEULA ( con ) : \n 
~~~ con . get_eula_status ( ) \n 
~~~ if con . get_eula_status ( ) is True : \n 
~~~ print ( ) \n 
con . set_eula ( ) \n 
~~ ~~ except Exception as e : \n 
print ( e ) \n 
~~ ~~ def login ( con , credential ) : \n 
~~~ con . login ( credential ) \n 
~~ ~~ def get_eg_uri_from_arg ( srv , name ) : \n 
~~~ if srv and name : \n 
~~~ if name . startswith ( ) and uri [ ] in name : \n 
~~~ return name \n 
~~~ egs = srv . get_enclosure_groups ( ) \n 
for eg in egs : \n 
~~~ if eg [ ] == name : \n 
~~~ return eg [ ] \n 
~~ ~~ ~~ ~~ return None \n 
~~ def get_sht_from_arg ( srv , name ) : \n 
~~~ shts = srv . get_server_hardware_types ( ) \n 
for sht in shts : \n 
~~~ if sht [ ] == name : \n 
~~~ return sht \n 
~~ def define_profile_template ( \n 
srv , \n 
name , \n 
desc , \n 
sp_desc , \n 
server_hwt , \n 
enc_group , \n 
affinity , \n 
hide_flexnics , \n 
conn_list , \n 
fw_settings , \n 
boot , \n 
bootmode ) : \n 
~~~ if conn_list : \n 
~~~ conn = json . loads ( open ( conn_list ) . read ( ) ) \n 
~~~ conn = [ ] \n 
~~ profile_template = srv . create_server_profile_template ( \n 
name = name , \n 
description = desc , \n 
serverProfileDescription = sp_desc , \n 
serverHardwareTypeUri = server_hwt , \n 
enclosureGroupUri = enc_group , \n 
affinity = affinity , \n 
hideUnusedFlexNics = hide_flexnics , \n 
profileConnectionV4 = conn , \n 
firmwareSettingsV3 = fw_settings , \n 
bootSettings = boot , \n 
bootModeSetting = bootmode ) \n 
if in profile_template : \n 
~~~ print ( , profile_template [ ] ) \n 
print ( , profile_template [ ] ) \n 
print ( ) \n 
for connection in profile_template [ ] : \n 
~~~ print ( , connection [ ] ) \n 
print ( , connection [ ] ) \n 
~~ print ( ) \n 
print ( , profile_template [ ] [ ] ) \n 
print ( , profile_template [ ] [ ] , ) \n 
~~~ pprint ( profile_template ) \n 
~~ ~~ def main ( ) : \n 
~~~ parser = argparse . ArgumentParser ( add_help = True , \n 
formatter_class = argparse . RawTextHelpFormatter , \n 
description = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
parser . add_argument ( , dest = , \n 
required = True , \n 
required = False , \n 
required = False , choices = [ , ] , \n 
action = , \n 
nargs = , \n 
choices = [ , , ] , \n 
choices = [ , , , \n 
, ] , \n 
args = parser . parse_args ( ) \n 
credential = { : args . user , : args . passwd } \n 
con = hpov . connection ( args . host ) \n 
srv = hpov . servers ( con ) \n 
sts = hpov . settings ( con ) \n 
if args . proxy : \n 
~~~ con . set_proxy ( args . proxy . split ( ) [ 0 ] , args . proxy . split ( ) [ 1 ] ) \n 
~~ if args . cert : \n 
~~~ con . set_trusted_ssl_bundle ( args . cert ) \n 
~~ login ( con , credential ) \n 
acceptEULA ( con ) \n 
eg_uri = get_eg_uri_from_arg ( srv , args . enc_group ) \n 
sht = get_sht_from_arg ( srv , args . server_hwt ) \n 
fw_settings = profile . make_firmware_dict ( sts , args . baseline ) \n 
boot , bootmode = profile . make_boot_settings_dict ( srv , sht , args . disable_manage_boot , \n 
args . boot_order , args . boot_mode , args . pxe ) \n 
define_profile_template ( srv , \n 
args . name , \n 
args . desc , \n 
args . sp_desc , \n 
sht [ ] , \n 
eg_uri , \n 
args . affinity , \n 
args . hide_flexnics , \n 
args . conn_list , \n 
bootmode ) \n 
~~ if __name__ == : \n 
~~~ import argparse \n 
sys . exit ( main ( ) ) \n 
~~ from __future__ import print_function \n 
~~ ~~ def get_address_pools ( con , srv , types ) : \n 
~~~ if types == or types == : \n 
~~~ vmac = srv . get_vmac_pool ( ) \n 
for key in sorted ( vmac ) : \n 
~~~ print ( . format ( key , vmac [ key ] ) ) \n 
~~ if in vmac : \n 
~~~ for uri in vmac [ ] : \n 
~~~ ranges = con . get ( uri ) \n 
print ( , ranges [ ] ) \n 
~~ ~~ ~~ if types == or types == : \n 
~~~ vwwn = srv . get_vwwn_pool ( ) \n 
for key in sorted ( vwwn ) : \n 
~~~ print ( . format ( key , vwwn [ key ] ) ) \n 
~~ if in vwwn : \n 
~~~ for uri in vwwn [ ] : \n 
~~~ vsn = srv . get_vsn_pool ( ) \n 
for key in sorted ( vsn ) : \n 
~~~ print ( . format ( key , vsn [ key ] ) ) \n 
~~ if in vsn : \n 
~~~ for uri in vsn [ ] : \n 
~~ ~~ ~~ ~~ def main ( ) : \n 
choices = [ , , , ] , default = , \n 
credential = { : args . domain . upper ( ) , : args . user , : args . passwd } \n 
get_address_pools ( con , srv , args . types ) \n 
~~~ import sys \n 
import argparse \n 
~~ ~~ def get_managed_sans ( fcs ) : \n 
~~~ sans = fcs . get_managed_sans ( ) \n 
pprint ( sans ) \n 
~~ def main ( ) : \n 
fcs = hpov . fcsans ( con ) \n 
get_managed_sans ( fcs ) \n 
~~ ~~ def getpolicy ( sts ) : \n 
~~~ policy = sts . get_storage_vol_template_policy ( ) \n 
print ( policy [ ] ) \n 
getpolicy ( sts ) \n 
from __future__ import print_function \n 
__title__ = \n 
__version__ = \n 
__copyright__ = \n 
__license__ = \n 
__status__ = \n 
from hpOneView . common import * \n 
from hpOneView . connection import * \n 
from hpOneView . activity import * \n 
from hpOneView . exceptions import * \n 
class servers ( object ) : \n 
~~~ def __init__ ( self , con ) : \n 
~~~ self . _con = con \n 
self . _activity = activity ( con ) \n 
########################################################################### \n 
~~ def get_connections ( self , filter = ) : \n 
return get_members ( self . _con . get ( uri [ ] + filter ) ) \n 
~~ def get_connection ( self , server ) : \n 
body = self . _con . get ( server [ ] ) \n 
return body \n 
~~ def get_server_by_bay ( self , baynum ) : \n 
~~~ servers = get_members ( self . _con . get ( uri [ ] ) ) \n 
for server in servers : \n 
~~~ if server [ ] == baynum : \n 
~~~ return server \n 
~~ ~~ ~~ def get_server_by_name ( self , name ) : \n 
~~~ if server [ ] == name : \n 
~~ ~~ ~~ def get_available_servers ( self , server_hardware_type = None , \n 
enclosure_group = None , server_profile = None ) : \n 
~~~ filters = [ ] \n 
if server_hardware_type : \n 
~~~ filters . append ( + server_hardware_type [ ] ) \n 
~~ if enclosure_group : \n 
~~~ filters . append ( + enclosure_group [ ] ) \n 
~~ if server_profile : \n 
~~~ filters . append ( + server_profile [ ] ) \n 
~~ query_string = \n 
if filters : \n 
~~~ query_string = + . join ( filters ) \n 
~~ return self . _con . get ( uri [ ] + query_string ) \n 
~~ def get_servers ( self ) : \n 
~~~ return get_members ( self . _con . get ( uri [ ] ) ) \n 
~~ def get_utilization ( self , server ) : \n 
body = self . _con . get ( server [ ] + ) \n 
~~ def get_env_conf ( self , server ) : \n 
~~ def set_server_powerstate ( self , server , state , force = False , blocking = True , \n 
verbose = False ) : \n 
~~~ if state == and force is True : \n 
~~~ powerRequest = make_powerstate_dict ( , ) \n 
~~ elif state == and force is False : \n 
~~ elif state == : \n 
~~ task , body = self . _con . put ( server [ ] + , powerRequest ) \n 
if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 60 , verbose = verbose ) \n 
~~ return task \n 
~~ def delete_server ( self , server , force = False , blocking = True , verbose = False ) : \n 
~~~ if force : \n 
~~~ task , body = self . _con . delete ( server [ ] + ) \n 
~~~ task , body = self . _con . delete ( server [ ] ) \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 600 , verbose = verbose ) \n 
~~ def update_server ( self , server ) : \n 
~~~ task , body = self . _con . put ( server [ ] , server ) \n 
~~ def add_server ( self , server , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . post ( uri [ ] , server ) \n 
if in task and task [ ] . startswith ( ) : \n 
~~~ entity = self . _activity . get_task_associated_resource ( task ) \n 
server = self . _con . get ( entity [ ] ) \n 
return server \n 
~~ ~~ return task \n 
~~ def get_server_schema ( self ) : \n 
return self . _con . get ( uri [ ] + ) \n 
~~ def get_bios ( self , server ) : \n 
return self . _con . get ( server [ ] + ) \n 
~~ def get_ilo_sso_url ( self , server ) : \n 
~~ def get_java_remote_console_url ( self , server ) : \n 
~~ def get_remote_console_url ( self , server ) : \n 
~~ def get_server_hardware_types ( self ) : \n 
body = self . _con . get ( uri [ ] ) \n 
return get_members ( body ) \n 
~~ def remove_server_hardware_type ( self , server_hardware_type , force = False , blocking = True , verbose = False ) : \n 
if force : \n 
~~~ task , body = self . _con . delete ( server_hardware_type [ ] + ) \n 
~~~ task , body = self . _con . delete ( server_hardware_type [ ] ) \n 
~~ def get_server_type_schema ( self ) : \n 
~~ def get_server_hardware_type ( self , server_type ) : \n 
return self . _con . get ( server_type [ ] ) \n 
~~ def set_server_hardware_type ( self , server_hardware_type , name , description ) : \n 
request = make_server_type_dict ( name , description ) \n 
task , body = self . _con . put ( server_hardware_type [ ] , request ) \n 
return task \n 
~~ def create_server_profile ( self , \n 
affinity = , \n 
biosSettings = None , \n 
bootSettings = None , \n 
bootModeSetting = None , \n 
profileConnectionV4 = None , \n 
description = None , \n 
firmwareSettingsV3 = None , \n 
hideUnusedFlexNics = True , \n 
localStorageSettingsV3 = None , \n 
macType = , \n 
name = None , \n 
sanStorageV3 = None , \n 
serialNumber = None , \n 
serialNumberType = , \n 
serverHardwareTypeUri = None , \n 
serverHardwareUri = None , \n 
serverProfileTemplateUri = None , \n 
uuid = None , \n 
wwnType = , \n 
blocking = True , verbose = False ) : \n 
profile = make_ServerProfileV5 ( affinity , biosSettings , bootSettings , \n 
bootModeSetting , profileConnectionV4 , \n 
description , firmwareSettingsV3 , \n 
hideUnusedFlexNics , \n 
localStorageSettingsV3 , macType , name , \n 
sanStorageV3 , serialNumber , \n 
serialNumberType , serverHardwareTypeUri , \n 
serverHardwareUri , \n 
serverProfileTemplateUri , uuid , wwnType ) \n 
task , body = self . _con . post ( uri [ ] , profile ) \n 
if profile [ ] is None : \n 
~~~ tout = 600 \n 
~~~ tout = 3600 \n 
~~~ task = self . _activity . wait4task ( task , tout , verbose = verbose ) \n 
profile = self . _con . get ( entity [ ] ) \n 
return profile \n 
~~ def post_server_profile ( self , profile , blocking = True , verbose = False ) : \n 
~~ def remove_server_profile ( self , profile , force = False , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . delete ( profile [ ] + ) \n 
~~~ task , body = self . _con . delete ( profile [ ] ) \n 
~~ def get_server_profiles ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
~~ def update_server_profile ( self , profile , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . put ( profile [ ] , profile ) \n 
~~~ if profile [ ] [ ] is None : \n 
~~ ~~ except Exception : \n 
~~~ task = self . _activity . wait4task ( task , tout = tout , verbose = verbose ) \n 
~~ profileResource = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( profileResource [ ] ) \n 
~~ def update_server_profile_from_template ( self , profile , blocking = True , verbose = False ) : \n 
~~~ patch_request = [ { : , : , : } ] \n 
task , body = self . _con . patch ( profile [ ] , patch_request ) \n 
~~ ~~ def get_server_profile_by_name ( self , name ) : \n 
~~~ body = self . _con . get_entity_byfield ( uri [ ] , , name ) \n 
~~ def get_profile_message ( self , profile ) : \n 
message = self . _con . get ( profile [ ] + ) \n 
return message \n 
~~ def get_profile_compliance_preview ( self , profile ) : \n 
return self . _con . get ( profile [ ] + ) \n 
~~ def create_server_profile_template ( \n 
self , \n 
serverProfileDescription = None , \n 
enclosureGroupUri = None , \n 
affinity = None , \n 
hideUnusedFlexNics = None , \n 
blocking = True , \n 
profile_template = make_ServerProfileTemplateV1 ( name , \n 
description , \n 
serverProfileDescription , \n 
serverHardwareTypeUri , \n 
enclosureGroupUri , \n 
profileConnectionV4 , \n 
firmwareSettingsV3 , \n 
bootSettings , \n 
bootModeSetting ) \n 
task , body = self . _con . post ( uri [ ] , profile_template ) \n 
tout = 600 \n 
profile_template = self . _con . get ( entity [ ] ) \n 
return profile_template \n 
~~ def remove_server_profile_template ( self , profile_template , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . delete ( profile_template [ ] ) \n 
~~ return body \n 
~~ def get_server_profile_templates ( self ) : \n 
~~ def get_server_profile_template_by_name ( self , name ) : \n 
~~ def update_server_profile_template ( self , profile_template , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . put ( profile_template [ ] , profile_template ) \n 
~~ profileTemplateResource = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( profileTemplateResource [ ] ) \n 
~~ def get_server_profile_from_template ( self , profile_template ) : \n 
~~~ profile = self . _con . get ( profile_template [ ] + ) \n 
~~ def get_enclosures ( self ) : \n 
~~ def add_enclosure ( self , enclosure , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . post ( uri [ ] , enclosure ) \n 
if enclosure [ ] is : \n 
~~ elif enclosure [ ] is None : \n 
enclosure = self . _con . get ( entity [ ] ) \n 
return enclosure \n 
~~ def remove_enclosure ( self , enclosure , force = False , blocking = True , \n 
~~~ task , body = self . _con . delete ( enclosure [ ] + ) \n 
~~~ task , body = self . _con . delete ( enclosure [ ] ) \n 
~~ def create_enclosure_group ( self , associatedLIGs , name , \n 
powerMode = ) : \n 
egroup = make_EnclosureGroupV200 ( associatedLIGs , name , powerMode ) \n 
task , body = self . _con . post ( uri [ ] , egroup ) \n 
~~ def delete_enclosure_group ( self , egroup ) : \n 
~~~ self . _con . delete ( egroup [ ] ) \n 
~~ def get_enclosure_groups ( self ) : \n 
~~ def update_enclosure_group ( self , enclosuregroup ) : \n 
~~~ task , body = self . _con . put ( enclosuregroup [ ] , enclosuregroup ) \n 
~~ def get_pool ( self , pooltype ) : \n 
~~~ body = self . _con . get ( uri [ ] + + pooltype ) \n 
~~ def get_vmac_pool ( self ) : \n 
~~ def get_vwwn_pool ( self ) : \n 
~~ def get_vsn_pool ( self ) : \n 
~~ def get_profile_networks ( self ) : \n 
~~ def get_profile_schema ( self ) : \n 
~~~ return self . _con . get ( uri [ ] ) \n 
~~ def get_profile_available_servers ( self ) : \n 
~~ def get_profile_available_storage_systems ( self ) : \n 
~~ def get_profile_ports ( self ) : \n 
~~ def allocate_pool_ids ( self , url , count ) : \n 
~~~ allocatorUrl = % url \n 
allocatorBody = { : count } \n 
task , body = self . _con . put ( allocatorUrl , allocatorBody ) \n 
~~ def release_pool_ids ( self , url , idList ) : \n 
~~~ collectorUrl = % url \n 
collectorBody = { : idList } \n 
task , body = self . _con . put ( collectorUrl , collectorBody ) \n 
~~ def allocate_range_ids ( self , allocatorUrl , count ) : \n 
~~~ task , body = self . _con . put ( allocatorUrl , { : count } ) \n 
~~ def release_range_ids ( self , collectorUrl , idList ) : \n 
~~~ task , body = self . _con . put ( collectorUrl , { : idList } ) \n 
~~ def enable_range ( self , url ) : \n 
~~~ prange = self . _con . get ( url ) \n 
prange [ ] = True \n 
task , body = self . _con . put ( url , prange ) \n 
~~ def disable_range ( self , url ) : \n 
prange [ ] = False \n 
import locale \n 
import zipfile \n 
import logging \n 
import textwrap \n 
import validictory \n 
from . sharedtypes import JSONEncoder \n 
from ilorest . rest . v1_helper import ( RisObject ) \n 
LOGGER = logging . getLogger ( __name__ ) \n 
class ValidationError ( Exception ) : \n 
~~ class SchemaValidationError ( ValidationError ) : \n 
~~ class RegistryValidationError ( ValidationError ) : \n 
def __init__ ( self , msg , regentry = None , selector = None ) : \n 
~~~ super ( RegistryValidationError , self ) . __init__ ( msg ) \n 
self . reg = regentry \n 
self . sel = selector \n 
~~ ~~ class UnknownValidatorError ( Exception ) : \n 
~~ class ValidationManager ( object ) : \n 
def __init__ ( self , local_path , bios_local_path , romfamily = None , biosversion = None , iloversion = None , monolith = None ) : \n 
~~~ super ( ValidationManager , self ) . __init__ ( ) \n 
defaultilopath = None \n 
defaultbiospath = None \n 
schemamainfolder = None \n 
if float ( iloversion ) < 2.10 : \n 
~~~ if os . name == : \n 
~~~ defaultilopath = r".\\hp-rest-classes-ilo4" \n 
defaultbiospath = r".\\hp-rest-classes-bios" \n 
schemamainfolder = os . path . dirname ( sys . executable ) \n 
~~~ defaultilopath = "/usr/share/hprest/hp-rest-classes-ilo4" \n 
defaultbiospath = "/usr/share/hprest/hp-rest-classes-bios" \n 
schemamainfolder = "/usr/share/hprest/" \n 
~~ if not local_path : \n 
~~~ if not os . path . isdir ( defaultilopath ) : \n 
~~~ ilozip = self . getiloziplocation ( schemamainfolder , iloversion ) \n 
if ilozip and os . path . exists ( ilozip ) : \n 
~~~ with zipfile . ZipFile ( os . path . join ( schemamainfolder , ilozip ) , "r" ) as zfile : \n 
~~~ zfile . extractall ( os . path . join ( schemamainfolder , "hp-rest-classes-ilo4" ) ) \n 
~~ local_path = os . path . join ( schemamainfolder , ) \n 
~~~ raise SchemaValidationError ( ) \n 
~~~ local_path = defaultilopath \n 
~~~ if not os . path . isdir ( local_path ) : \n 
~~ ~~ if not bios_local_path : \n 
~~~ if not os . path . isdir ( defaultbiospath ) : \n 
~~~ bioszip = self . getbiosziplocation ( romfamily , schemamainfolder , biosversion ) \n 
if bioszip and os . path . exists ( bioszip ) : \n 
~~~ with zipfile . ZipFile ( \n 
os . path . join ( schemamainfolder , bioszip ) , "r" ) as zfile : \n 
~~~ zfile . extractall ( os . path . join ( schemamainfolder , "hp-rest-classes-bios" ) ) \n 
~~ bios_local_path = os . path . join ( schemamainfolder , ) \n 
~~~ bios_local_path = defaultbiospath \n 
~~~ if not os . path . isdir ( bios_local_path ) : \n 
~~~ if monolith . is_redfish : \n 
~~~ local_path = "/redfish/v1/Schemas/" \n 
bios_local_path = "/redfish/v1/Registries/" \n 
~~~ local_path = "/rest/v1/Schemas" \n 
bios_local_path = "/rest/v1/Registries" \n 
~~ ~~ self . _schema_locations = list ( ) \n 
self . _classes = list ( ) \n 
self . _registry_locations = list ( ) \n 
self . _classes_registry = list ( ) \n 
self . _bios_schema_locations = list ( ) \n 
self . _bios_classes = list ( ) \n 
self . _bios_registry_locations = list ( ) \n 
self . _bios_classes_registry = list ( ) \n 
self . _ilo_messages = list ( ) \n 
self . _base_messages = list ( ) \n 
self . _hpcommon_messages = list ( ) \n 
self . _iloevents_messages = list ( ) \n 
self . _errors = list ( ) \n 
if monolith . is_redfish : \n 
~~~ self . _schemaid = [ "/redfish/v1/schemas" , "Members" ] \n 
self . _regid = [ "/redfish/v1/registries" , "Members" ] \n 
~~~ self . _schemaid = [ "/rest/v1/schemas" , "Items" ] \n 
self . _regid = [ "/rest/v1/registries" , "Items" ] \n 
~~ if local_path : \n 
~~~ self . add_location ( schema_path = local_path , monolith = monolith ) \n 
self . add_location ( registry_path = local_path , monolith = monolith ) \n 
~~ if bios_local_path : \n 
~~~ self . add_location ( schema_path = bios_local_path , biossection = True , monolith = monolith ) \n 
self . add_location ( registry_path = bios_local_path , biossection = True , monolith = monolith ) \n 
~~ ~~ def getbiosziplocation ( self , romfamily , schemadir , biosversion ) : \n 
foundfile = None \n 
currentver = None \n 
tempstr = "hp-rest-classes-bios-" + romfamily + "-" + biosversion \n 
for _ , _ , filenames in os . walk ( schemadir ) : \n 
~~~ for filename in filenames : \n 
~~~ if tempstr in filename : \n 
~~~ regentry = re . compile ( % tempstr ) \n 
mentry = regentry . search ( filename ) \n 
if mentry and currentver : \n 
~~~ if currentver < mentry . group ( 1 ) : \n 
~~~ foundfile = filename \n 
currentver = mentry . group ( 1 ) \n 
~~ ~~ elif mentry and not currentver : \n 
~~ ~~ ~~ ~~ if foundfile : \n 
~~~ return os . path . join ( schemadir , foundfile ) \n 
~~ ~~ def getiloziplocation ( self , schemadir , iloversion ) : \n 
~~~ iloversion = \n 
~~ tempstr = "hp-rest-classes-ilo4-" + iloversion . replace ( "." , "" ) \n 
~~~ return os . path . join ( schemadir , filename ) \n 
~~ ~~ ~~ return None \n 
~~ def add_location ( self , schema_path = None , registry_path = None , \n 
biossection = False , monolith = None ) : \n 
if schema_path : \n 
~~~ if not biossection : \n 
~~~ self . _schema_locations . append ( schema_path ) \n 
self . _update_location_map ( monolith = monolith ) \n 
~~~ self . _bios_schema_locations . append ( schema_path ) \n 
self . _update_location_map ( biossection = True , monolith = monolith ) \n 
~~ ~~ elif registry_path : \n 
~~~ self . _registry_locations . append ( registry_path ) \n 
self . _update_location_map ( registries = True , monolith = monolith ) \n 
~~~ self . _bios_registry_locations . append ( registry_path ) \n 
self . _update_location_map ( biossection = True , registries = True , monolith = monolith ) \n 
~~ ~~ def _update_location_map ( self , biossection = False , registries = False , \n 
monolith = None ) : \n 
locationslist = list ( ) \n 
pathjoinstr = None \n 
if not registries : \n 
~~~ pathjoinstr = "Schemas" \n 
if not biossection : \n 
~~~ locationslist = self . _schema_locations \n 
~~~ locationslist = self . _bios_schema_locations \n 
~~~ pathjoinstr = "Registries" \n 
~~~ locationslist = self . _registry_locations \n 
~~~ locationslist = self . _bios_registry_locations \n 
~~ ~~ for location in locationslist : \n 
~~~ if monolith : \n 
~~~ self . new_load_file ( monolith , root = location , biossection = biossection , registries = registries ) \n 
~~ elif self . _is_local ( location ) : \n 
~~~ for root , _ , filenames in os . walk ( os . path . join ( location , \n 
pathjoinstr ) ) : \n 
~~~ fqpath = os . path . abspath ( os . path . join ( os . path . normpath ( root ) , filename ) ) \n 
if self . load_file ( fqpath , root = location , biossection = biossection , registries = registries ) : \n 
~~ ~~ ~~ ~~ ~~ ~~ def new_load_file ( self , monolith , root = None , biossection = False , registries = False ) : \n 
classesdataholder = [ ] \n 
for itemtype in monolith . types : \n 
~~~ if itemtype . startswith ( "#SchemaFileCollection." ) or itemtype . startswith ( "Collection." ) and in monolith . types [ itemtype ] : \n 
~~~ for instance in monolith . types [ itemtype ] [ ] : \n 
~~~ if self . _schemaid [ 0 ] in instance . resp . request . path . lower ( ) or self . _regid [ 0 ] in instance . resp . request . path . lower ( ) : \n 
~~~ if not registries and self . _schemaid [ 0 ] in instance . resp . request . path . lower ( ) : \n 
~~~ if classesdataholder : \n 
~~~ if self . _schemaid [ 1 ] in instance . resp . dict : \n 
~~~ classesdataholder [ 0 ] [ self . _schemaid [ 1 ] ] . extend ( instance . resp . dict [ self . _schemaid [ 1 ] ] ) \n 
~~~ classesdataholder . append ( instance . resp . dict ) \n 
~~ ~~ elif registries and self . _regid [ 0 ] in instance . resp . request . path . lower ( ) : \n 
~~~ classesdataholder [ 0 ] [ self . _regid [ 1 ] ] . extend ( instance . resp . dict [ self . _regid [ 1 ] ] ) \n 
~~ ~~ ~~ ~~ ~~ ~~ if classesdataholder : \n 
~~~ classesdataholder = classesdataholder [ 0 ] \n 
~~ try : \n 
~~~ if monolith . _typestring in classesdataholder and ( in classesdataholder [ monolith . _typestring ] or ( in classesdataholder [ monolith . _typestring ] and monolith . is_redfish ) ) : \n 
~~~ newclass = Classes . parse ( classesdataholder ) \n 
newclass . set_root ( root ) \n 
~~~ self . _classes . append ( newclass ) \n 
~~~ self . _bios_classes . append ( newclass ) \n 
~~~ self . _classes_registry . append ( newclass ) \n 
~~~ self . _bios_classes_registry . append ( newclass ) \n 
~~ ~~ ~~ ~~ except BaseException : \n 
~~ ~~ def load_file ( self , filepath , root = None , biossection = False , \n 
registries = False , datareturn = False ) : \n 
result = False \n 
if os . path . isfile ( filepath ) : \n 
~~~ filehand = open ( filepath , ) \n 
data = json . load ( filehand ) \n 
if datareturn : \n 
~~~ return data \n 
~~ if in data and data [ ] == : \n 
~~~ if biossection and registries : \n 
~~~ itemsreturn = self . bios_helper_function ( data , root ) \n 
data [ "Items" ] = itemsreturn \n 
~~ newclass = Classes . parse ( data ) \n 
~~ ~~ result = True \n 
~~ ~~ except BaseException : \n 
~~ finally : \n 
~~~ filehand . close ( ) \n 
~~ ~~ return result \n 
~~ def bios_helper_function ( self , data , root ) : \n 
folderentries = data [ "links" ] \n 
datareturn = list ( ) \n 
for entry in folderentries [ "Member" ] : \n 
~~~ joinstr = entry [ "href" ] \n 
if os . name == and joinstr [ 0 ] == "/" : \n 
~~~ joinstr = joinstr . replace ( "/" , "\\\\" ) [ 1 : ] \n 
~~ elif joinstr [ 0 ] == "/" : \n 
~~~ joinstr = joinstr [ 1 : ] \n 
~~ for root , _ , filenames in os . walk ( os . path . join ( root , joinstr ) ) : \n 
datareturn . append ( self . load_file ( fqpath , root = root , biossection = True , registries = True , datareturn = True ) ) \n 
~~ ~~ ~~ return datareturn \n 
~~ def validate ( self , item , selector = None , currdict = None , monolith = None , \n 
newarg = None , checkall = False , regloc = None ) : \n 
if regloc : \n 
~~~ attrreg = RepoRegistryEntry ( regloc ) \n 
~~~ attrreg = self . find_schema ( schname = item [ monolith . _typestring ] ) \n 
~~ if attrreg : \n 
~~~ tempvalue = attrreg . validate ( item , self . _errors , selector = selector , \n 
currdict = currdict , monolith = monolith , \n 
newarg = newarg , checkall = checkall ) \n 
if tempvalue is True : \n 
~~ elif tempvalue : \n 
~~~ self . _errors = tempvalue \n 
~~ ~~ return True \n 
~~ def bios_validate ( self , item , regname , selector = None , currdict = None , \n 
checkall = False , monolith = None ) : \n 
attrreg = self . find_bios_registry ( regname = regname ) \n 
if attrreg : \n 
~~~ tempvalue = attrreg . validate_bios_version ( item , self . _errors , selector = selector , currdict = currdict , checkall = checkall , monolith = monolith ) \n 
if tempvalue == : \n 
~~~ return tempvalue \n 
~~ elif tempvalue == : \n 
~~ def bios_info ( self , item , regname , selector ) : \n 
~~~ if attrreg . validate_bios_version ( item , self . _errors , selector = selector ) : \n 
~~ def find_schema ( self , schname ) : \n 
for cls in self . _classes : \n 
~~~ found = cls . find_schema ( schname = schname ) \n 
if found : \n 
~~~ return found \n 
~~ ~~ return None \n 
~~ def find_registry ( self , regname ) : \n 
for cls in self . _classes_registry : \n 
~~~ found = cls . find_registry ( regname = regname ) \n 
~~ def find_bios_registry ( self , regname ) : \n 
for cls in self . _bios_classes_registry : \n 
~~~ found = cls . find_bios_registry ( regname = regname ) \n 
~~ def get_errors ( self ) : \n 
return self . _errors \n 
~~ def _is_local ( self , path ) : \n 
if in path : \n 
~~ ~~ class Classes ( RisObject ) : \n 
def __init__ ( self , item ) : \n 
~~~ super ( Classes , self ) . __init__ ( item ) \n 
self . _root = None \n 
~~ def set_root ( self , newroot ) : \n 
self . _root = newroot \n 
result = None \n 
if hasattr ( self , ) and isinstance ( self . Items , list ) : \n 
~~~ for entry in self . Items : \n 
~~~ if entry and in entry and entry [ ] . lower ( ) == schname . lower ( ) : \n 
~~~ regentry = RepoRegistryEntry . parse ( entry ) \n 
regentry . set_root ( self . _root ) \n 
result = regentry \n 
break \n 
~~ ~~ ~~ elif hasattr ( self , ) and isinstance ( self . Members , list ) : \n 
~~~ schname = schname . split ( ) [ - 1 ] \n 
for entry in self . Members : \n 
~~~ schlink = entry [ ] . split ( ) \n 
schlink = schlink [ len ( schlink ) - 2 ] \n 
if schname . lower ( ) == schlink . lower ( ) : \n 
~~~ result = entry \n 
~~ ~~ ~~ return result \n 
~~~ if entry and ( in entry and \n 
entry [ ] . lower ( ) . startswith ( regname . lower ( ) ) ) : \n 
~~~ regname = regname . split ( ) [ - 1 ] \n 
~~~ reglink = entry [ ] . split ( ) \n 
reglink = reglink [ len ( reglink ) - 2 ] \n 
if regname . lower ( ) == reglink . lower ( ) : \n 
~~ def find_bios_schema ( self , schname ) : \n 
~~~ if ( in entry and entry [ ] . lower ( ) == \n 
schname . lower ( ) ) : \n 
~~~ if entry and ( in entry and regname . lower ( ) in entry [ ] . lower ( ) ) : \n 
~~ ~~ class RepoBaseEntry ( RisObject ) : \n 
def __init__ ( self , d ) : \n 
~~~ super ( RepoBaseEntry , self ) . __init__ ( d ) \n 
~~ def _read_location_file ( self , currloc , errlist ) : \n 
if in currloc : \n 
~~~ root = os . path . normpath ( self . _root ) \n 
xref = os . path . normpath ( currloc . Uri . extref ) . lstrip ( os . path . sep ) \n 
fqpath = os . path . join ( root , xref ) \n 
if not os . path . isfile ( fqpath ) : \n 
~~~ errlist . append ( SchemaValidationError ( \n 
~~~ result = None \n 
if fqpath . endswith ( ) : \n 
~~~ result = open ( fqpath ) . read ( ) \n 
~~ ~~ class RepoRegistryEntry ( RepoBaseEntry ) : \n 
~~~ super ( RepoRegistryEntry , self ) . __init__ ( d ) \n 
~~ def validate ( self , tdict , errlist = None , selector = None , currdict = None , checkall = False , monolith = None , newarg = None ) : \n 
if not errlist : \n 
~~~ errlist = list ( ) \n 
~~ reg = self . get_registry_model ( errlist = errlist , currdict = currdict , monolith = monolith , newarg = newarg ) \n 
if reg and not checkall : \n 
~~~ if reg [ selector ] . readonly : \n 
~~~ return True \n 
~~ results = reg . validate_attribute_values ( tdict ) \n 
errlist . extend ( results ) \n 
~~ elif checkall and selector is None : \n 
~~~ results = reg . validate_attribute_values ( tdict ) \n 
~~~ errlist . append ( RegistryValidationError ( ) ) \n 
~~ if errlist : \n 
~~~ return errlist \n 
~~ ~~ def validate_bios_version ( self , tdict , errlist = None , selector = None , checkall = False , currdict = None , monolith = None ) : \n 
~~ reg = self . get_registry_model_bios_version ( errlist = errlist , currdict = currdict , monolith = monolith ) \n 
~~~ for item in reg . Attributes : \n 
~~~ if not item [ "Name" ] == selector : \n 
~~ if item [ "ReadOnly" ] is True : \n 
~~~ if item [ "IsSystemUniqueProperty" ] is True : \n 
~~ ~~ results = reg . validate_att_val_bios ( tdict ) \n 
~~~ results = reg . validate_att_val_bios ( tdict ) \n 
~~ ~~ def validate_deprecated ( self , tdict , errlist = None ) : \n 
~~ if not hasattr ( self , ) : \n 
return errlist \n 
~~ currloc = None \n 
defloc = None \n 
langcode = \n 
for loc in self . Location : \n 
~~~ for loclang in loc . keys ( ) : \n 
~~~ if loclang . lower ( ) == langcode . lower ( ) : \n 
~~~ currloc = loc [ loclang ] \n 
~~ elif loclang . lower ( ) == : \n 
~~~ defloc = loc [ loclang ] \n 
~~ ~~ ~~ if not currloc : \n 
~~~ currloc = defloc \n 
~~ if not currloc : \n 
~~ location_file = self . _read_location_file ( currloc , errlist = errlist ) \n 
if not location_file : \n 
~~~ jsonreg = json . loads ( location_file ) \n 
if in jsonreg : \n 
~~~ if in jsonreg and jsonreg [ ] == : \n 
~~~ reg = HpPropertiesRegistry . parse ( jsonreg [ ] ) \n 
results = reg . validate_attribute_values ( tdict ) \n 
~~ ~~ ~~ ~~ def get_registry_model ( self , currdict = None , monolith = None , errlist = None , skipcommit = False , searchtype = None , newarg = None , latestschema = None ) : \n 
~~~ errlist . append ( RegistryValidationError ( \n 
) ) \n 
return None \n 
defloc = "en" \n 
langcode = list ( locale . getdefaultlocale ( ) ) \n 
if not langcode [ 0 ] : \n 
~~~ langcode [ 0 ] = "en" \n 
~~ for loc in self . Location : \n 
~~~ locationlanguage = loc [ "Language" ] . lower ( ) \n 
locationlanguage = locationlanguage . replace ( "-" , "_" ) \n 
if locationlanguage in langcode [ 0 ] . lower ( ) : \n 
~~~ currloc = loc \n 
~~ ~~ if not currloc : \n 
~~ if not searchtype : \n 
~~~ searchtype = "ob" \n 
~~ location_file = None \n 
if currdict and monolith : \n 
~~~ for itemtype in monolith . types : \n 
~~~ if itemtype . lower ( ) . startswith ( searchtype . lower ( ) ) and in monolith . types [ itemtype ] : \n 
~~~ currtype = currdict [ instance . _typestring ] . split ( ) [ - 1 ] \n 
currtype = currtype . split ( ) [ 0 ] + \n 
~~~ currtype = currdict [ instance . _typestring ] \n 
~~ if latestschema : \n 
~~~ currtype = currdict [ instance . _typestring ] . split ( ) [ : 1 ] \n 
insttype = instance . resp . dict [ "title" ] . split ( ) [ : 1 ] \n 
if currtype == insttype or currtype == instance . resp . dict [ "oldtitle" ] . split ( ) [ : 1 ] : \n 
~~~ location_file = instance . resp . dict \n 
~~ ~~ elif searchtype == "ob" and instance . resp . dict [ "title" ] . startswith ( currtype ) or "oldtitle" in instance . resp . dict . keys ( ) and currdict [ instance . _typestring ] == instance . resp . dict [ "oldtitle" ] : \n 
~~ elif searchtype != "ob" and currdict [ instance . _typestring ] in instance . resp . dict [ "RegistryPrefix" ] : \n 
~~ ~~ ~~ if location_file : \n 
~~~ break \n 
~~~ location_file = self . _read_location_file ( currloc , errlist = errlist ) \n 
~~ if not location_file : \n 
~~~ if currdict and monolith : \n 
~~~ jsonreg = json . loads ( json . dumps ( location_file , indent = 2 , cls = JSONEncoder ) ) \n 
~~ if skipcommit : \n 
~~~ return jsonreg [ "Messages" ] \n 
~~ if in jsonreg : \n 
~~~ regitem = jsonreg [ ] \n 
reg = HpPropertiesRegistry . parse ( regitem ) \n 
if newarg : \n 
~~~ regcopy = reg \n 
for arg in newarg [ : - 1 ] : \n 
~~~ if in regcopy [ arg ] . iterkeys ( ) and ( in regcopy [ arg ] . iterkeys ( ) ) : \n 
~~~ regcopy [ arg ] [ ] . update ( regcopy [ arg ] [ ] ) \n 
regcopy = regcopy [ arg ] [ "properties" ] \n 
for pattern in regcopy . iterkeys ( ) : \n 
~~~ test = re . compile ( pattern ) \n 
nextarg = newarg [ newarg . index ( arg ) + 1 ] \n 
match = test . match ( nextarg ) \n 
if match : \n 
~~~ regcopy [ nextarg ] = regcopy . pop ( pattern ) \n 
~~ ~~ ~~ elif in regcopy [ arg ] : \n 
~~~ oneof = regcopy [ arg ] [ ] \n 
for item in oneof : \n 
~~~ regcopy = item [ ] \n 
if not arg == newarg [ - 1 ] : \n 
~~~ nextitem = newarg [ newarg . index ( arg ) + 1 ] \n 
regcopy [ nextitem ] \n 
~~ except Exception : \n 
~~ ~~ ~~ ~~ else : \n 
~~~ regcopy = regcopy [ arg ] [ "properties" ] \n 
~~~ regcopy = regcopy [ arg ] [ ] \n 
~~~ patterninfo = regcopy . pop ( pattern ) \n 
regcopy [ nextarg ] = patterninfo \n 
~~ ~~ ~~ except BaseException : \n 
~~ ~~ ~~ reg = regcopy \n 
~~ ~~ return reg \n 
~~ return None \n 
~~ def get_registry_model_bios_version ( self , currdict = None , monolith = None , errlist = None ) : \n 
~~~ if "HpBiosAttributeRegistrySchema." in itemtype and in monolith . types [ itemtype ] : \n 
~~ ~~ if location_file : \n 
return reg \n 
~~ ~~ class RepoSchemaEntry ( RepoBaseEntry ) : \n 
~~~ super ( RepoSchemaEntry , self ) . __init__ ( item ) \n 
if in currloc and in currloc : \n 
~~~ fqpath = os . path . join ( self . _root , currloc . ArchiveUri . xref . lstrip ( os . path . sep ) ) \n 
~~~ archive_file = currloc . ArchiveFile \n 
archive_fh = None \n 
~~~ archive_fh = zipfile . ZipFile ( fqpath ) \n 
infolist = archive_fh . infolist ( ) \n 
for i in infolist : \n 
~~~ if i . filename . lower ( ) == archive_file . lower ( ) : \n 
~~~ jsonsch_fh = archive_fh . open ( i ) \n 
result = jsonsch_fh . read ( ) \n 
jsonsch_fh . close ( ) \n 
~~ ~~ archive_fh . close ( ) \n 
~~ def validate ( self , tdict , errlist = None ) : \n 
~~ result = list ( ) \n 
if not hasattr ( self , ) : \n 
~~~ result . append ( SchemaValidationError ( ) ) \n 
return result \n 
~~~ result . append ( SchemaValidationError ( \n 
~~ location_file = self . _read_location_file ( currloc , errlist = result ) \n 
~~~ jsonsch = json . loads ( location_file ) \n 
validictory . validate ( tdict , jsonsch ) \n 
~~ ~~ ~~ class HpPropertiesRegistry ( RisObject ) : \n 
~~~ super ( HpPropertiesRegistry , self ) . __init__ ( d ) \n 
~~ def validate_attribute_values ( self , tdict ) : \n 
result = list ( ) \n 
for tkey in tdict : \n 
~~~ if self [ tkey ] and hasattr ( self [ tkey ] , "type" ) : \n 
~~~ temp = self . validate_attribute ( self [ tkey ] , tdict [ tkey ] , tkey ) \n 
for err in temp : \n 
~~~ if isinstance ( err , RegistryValidationError ) : \n 
~~~ if err . reg : \n 
~~~ err . sel = tkey \n 
~~ ~~ ~~ result . extend ( temp ) \n 
~~ def validate_att_val_bios ( self , tdict ) : \n 
~~~ for item in self . Attributes : \n 
~~~ if item [ "Name" ] == tkey and hasattr ( item , "Type" ) : \n 
~~~ temp = self . validate_attribute ( item , tdict [ tkey ] , tkey ) \n 
~~ def get_validator ( self , attrname , newargs = None , oneof = None ) : \n 
if oneof : \n 
~~~ self = oneof \n 
~~ if newargs : \n 
~~~ for arg in newargs : \n 
~~~ self = self [ ] \n 
~~ if not hasattr ( self , arg ) : \n 
~~ elif not arg == newargs [ - 1 ] : \n 
~~~ self = self [ arg ] \n 
~~ ~~ ~~ if not hasattr ( self , attrname ) : \n 
~~ validator = None \n 
if EnumValidator . is_type ( self [ attrname ] ) : \n 
~~~ validator = EnumValidator . parse ( self [ attrname ] ) \n 
~~ elif StringValidator . is_type ( self [ attrname ] ) : \n 
~~~ validator = StringValidator . parse ( self [ attrname ] ) \n 
~~ elif ObjectValidator . is_type ( self [ attrname ] ) : \n 
~~~ validator = ObjectValidator . parse ( self [ attrname ] ) \n 
~~ elif IntegerValidator . is_type ( self [ attrname ] ) : \n 
~~~ validator = IntegerValidator . parse ( self [ attrname ] ) \n 
~~ elif BoolValidator . is_type ( self [ attrname ] ) : \n 
~~~ validator = BoolValidator . parse ( self [ attrname ] ) \n 
~~ elif PasswordValidator . is_type ( self [ attrname ] ) : \n 
~~~ validator = PasswordValidator . parse ( self [ attrname ] ) \n 
~~ elif in self [ attrname ] . keys ( ) : \n 
~~~ for item in self [ attrname ] [ ] : \n 
~~~ validator = self . get_validator ( attrname , newargs , HpPropertiesRegistry ( { attrname : item } ) ) \n 
if validator : \n 
~~ ~~ ~~ return validator \n 
~~ def get_validator_bios ( self , attrname ) : \n 
for item in self . Attributes : \n 
~~~ if item [ "Name" ] == attrname : \n 
~~~ validator = None \n 
if EnumValidator . is_type ( item ) : \n 
~~~ validator = EnumValidator . parse ( item ) \n 
~~ elif StringValidator . is_type ( item ) : \n 
~~~ validator = StringValidator . parse ( item ) \n 
~~ elif IntegerValidator . is_type ( item ) : \n 
~~~ validator = IntegerValidator . parse ( item ) \n 
~~ elif BoolValidator . is_type ( item ) : \n 
~~~ validator = BoolValidator . parse ( item ) \n 
~~ elif ObjectValidator . is_type ( item ) : \n 
~~~ validator = ObjectValidator . parse ( item ) \n 
~~ elif PasswordValidator . is_type ( item ) : \n 
~~~ validator = PasswordValidator . parse ( item ) \n 
~~ return validator \n 
~~ def validate_attribute ( self , attrentry , attrval , name ) : \n 
validator = None \n 
if EnumValidator . is_type ( attrentry ) : \n 
~~~ validator = EnumValidator . parse ( attrentry ) \n 
~~ elif StringValidator . is_type ( attrentry ) : \n 
~~~ validator = StringValidator . parse ( attrentry ) \n 
~~ elif IntegerValidator . is_type ( attrentry ) : \n 
~~~ validator = IntegerValidator . parse ( attrentry ) \n 
~~ elif BoolValidator . is_type ( attrentry ) : \n 
~~~ validator = BoolValidator . parse ( attrentry ) \n 
~~ elif ObjectValidator . is_type ( attrentry ) : \n 
~~~ validator = ObjectValidator . parse ( attrentry ) \n 
~~ elif PasswordValidator . is_type ( attrentry ) : \n 
~~~ validator = PasswordValidator . parse ( attrentry ) \n 
~~~ raise UnknownValidatorError ( attrentry ) \n 
~~ if validator : \n 
~~~ result . extend ( validator . validate ( attrval , name ) ) \n 
~~ ~~ class BaseValidator ( RisObject ) : \n 
~~~ super ( BaseValidator , self ) . __init__ ( d ) \n 
~~ def validate ( self ) : \n 
raise RuntimeError ( ) \n 
~~ ~~ class EnumValidator ( BaseValidator ) : \n 
~~~ super ( EnumValidator , self ) . __init__ ( d ) \n 
def is_type ( attrentry ) : \n 
if in attrentry : \n 
~~~ if isinstance ( attrentry [ ] , list ) : \n 
~~~ for item in attrentry [ ] : \n 
~~~ if item . lower ( ) == : \n 
~~ elif in attrentry and item . lower ( ) == : \n 
~~ ~~ ~~ elif in attrentry and attrentry [ ] == "array" : \n 
~~~ for key , value in attrentry [ ] . iteritems ( ) : \n 
~~~ if key . lower ( ) == "type" and value . lower ( ) == : \n 
~~~ if attrentry [ ] . lower ( ) == : \n 
~~ elif in attrentry and attrentry [ ] . lower ( ) == : \n 
~~ ~~ ~~ elif in attrentry : \n 
~~ ~~ return False \n 
~~ def validate ( self , newval , name ) : \n 
~~~ for possibleval in self . enum : \n 
~~~ if possibleval . lower ( ) == newval . lower ( ) : \n 
~~ ~~ ~~ except Exception : \n 
~~~ for possibleval in self . Value : \n 
~~~ if possibleval . ValueName . lower ( ) == str ( newval ) . lower ( ) : \n 
regentry = self ) ) \n 
~~ def print_help ( self , name , out = sys . stdout ) : \n 
wrapper = textwrap . TextWrapper ( ) \n 
wrapper . initial_indent = * 4 \n 
wrapper . subsequent_indent = * 4 \n 
out . write ( ) \n 
out . write ( % wrapper . fill ( % name ) ) \n 
if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
~~ if in self : \n 
~~ if in self and isinstance ( self [ ] , list ) : \n 
for item in self [ ] : \n 
~~~ out . write ( % wrapper . fill ( % item ) ) \n 
~~ out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( % possibleval ) \n 
~~ ~~ out . write ( ) \n 
~~ ~~ class BoolValidator ( BaseValidator ) : \n 
~~~ super ( BoolValidator , self ) . __init__ ( d ) \n 
~~ ~~ ~~ elif attrentry [ ] == "array" : \n 
if newval is False or newval is True : \n 
~~ result . append ( \n 
RegistryValidationError ( \n 
regentry = self \n 
) \n 
~~ ~~ class StringValidator ( BaseValidator ) : \n 
~~~ super ( StringValidator , self ) . __init__ ( d ) \n 
~~~ if key . lower ( ) == "type" and in value : \n 
~~~ if len ( newval ) < int ( self [ ] ) : \n 
~~~ result . append ( RegistryValidationError ( \n 
( self . Name , int ( self [ ] ) ) , regentry = self ) ) \n 
~~ ~~ if in self : \n 
~~~ if len ( newval ) > int ( self [ ] ) : \n 
~~~ if self [ ] : \n 
~~~ pat = re . compile ( self [ ] ) \n 
if newval and not pat . match ( newval ) : \n 
"\'%(ValueExpression)s\'" % ( self ) , regentry = self ) ) \n 
~~ ~~ ~~ class IntegerValidator ( BaseValidator ) : \n 
~~~ super ( IntegerValidator , self ) . __init__ ( d ) \n 
~~~ if item . lower ( ) == or item . lower ( ) == : \n 
~~~ if key . lower ( ) == "type" : \n 
~~~ if value . lower ( ) == or value . lower ( ) == : \n 
~~~ if attrentry [ ] . lower ( ) == or attrentry [ ] . lower ( ) . lower ( ) == : \n 
intval = int ( newval ) \n 
pat = re . compile ( ) \n 
if newval and not pat . match ( intval ) : \n 
~~~ result . append ( \n 
~~~ if intval < int ( self [ ] ) : \n 
~~~ if intval > int ( self [ ] ) : \n 
~~ ~~ ~~ class ObjectValidator ( BaseValidator ) : \n 
~~~ super ( ObjectValidator , self ) . __init__ ( d ) \n 
~~ elif key . lower ( ) == "anyof" : \n 
~~~ if value [ 0 ] [ ] == : \n 
~~ ~~ ~~ class PasswordValidator ( BaseValidator ) : \n 
~~~ super ( PasswordValidator , self ) . __init__ ( d ) \n 
if newval is None : \n 
~~ ~~ ~~ from . constants import MILLI_MICROS , SECOND_MICROS , MINUTE_MICROS \n 
import calendar \n 
from datetime import datetime \n 
from dateutil import parser \n 
from dateutil . tz import tzlocal \n 
from . error import TimeConstructionError \n 
from . sanedelta import SaneDelta \n 
import pytz \n 
MICROS_TRANSLATIONS = ( \n 
( ( , , , , ) , MINUTE_MICROS ) , \n 
( ( , , , , ) , SECOND_MICROS ) , \n 
( ( , , , , ) , MILLI_MICROS ) , \n 
( ( , , , , ) , 1 ) ) \n 
MICROS_TRANSLATION_HASH = dict ( ( alt , v ) for k , v in MICROS_TRANSLATIONS for alt in k ) \n 
class SaneTime ( object ) : \n 
super ( time , self ) . __init__ ( ) \n 
uss = set ( ) \n 
tzs = set ( ) \n 
naive_dt = None \n 
avoid_localize = False \n 
for k , v in kwargs . iteritems ( ) : \n 
~~~ if k in ( , ) : \n 
~~~ tzs . add ( SaneTime . to_timezone ( v ) ) \n 
~~ elif k in MICROS_TRANSLATION_HASH : \n 
~~~ uss . add ( MICROS_TRANSLATION_HASH [ k ] * v ) \n 
~~ ~~ args = list ( args ) \n 
if len ( args ) > 2 and len ( args ) <= 8 : \n 
~~~ args = [ datetime ( * args ) ] \n 
~~ if len ( args ) == 2 : \n 
~~~ tzs . add ( SaneTime . to_timezone ( args . pop ( ) ) ) \n 
~~ if len ( args ) == 1 : \n 
~~~ arg = args . pop ( ) \n 
if hasattr ( arg , ) : \n 
~~~ uss . add ( int ( arg ) ) \n 
if hasattr ( arg , ) : tzs . add ( arg . tz ) \n 
~~ elif isinstance ( arg , basestring ) : \n 
~~~ parts = arg . strip ( ) . split ( ) \n 
if len ( parts ) > 1 and parts [ - 1 ] . startswith ( ) : \n 
~~~ tzs . add ( SaneTime . to_timezone ( parts [ - 1 ] [ 1 : ] ) ) \n 
arg = . join ( parts [ : - 1 ] ) \n 
~~ except : pass \n 
~~ utc = arg . endswith ( ) or arg . endswith ( ) \n 
arg = parser . parse ( arg ) \n 
~~~ if utc : \n 
~~~ tzs . add ( pytz . utc ) \n 
arg = arg . replace ( tzinfo = None ) \n 
~~~ arg = arg . replace ( tzinfo = None ) \n 
~~~ avoid_localize = True \n 
arg = arg . astimezone ( pytz . utc ) . replace ( tzinfo = None ) \n 
~~ ~~ ~~ if type ( arg ) == datetime : \n 
~~~ naive_dt = arg \n 
if naive_dt . tzinfo : \n 
~~~ tzs . add ( SaneTime . to_timezone ( str ( naive_dt . tzinfo ) ) ) \n 
naive_dt = naive_dt . replace ( tzinfo = None ) \n 
~~ ~~ ~~ if len ( tzs ) > 1 : \n 
~~ self . tz = len ( tzs ) and tzs . pop ( ) or pytz . utc \n 
if naive_dt : \n 
~~~ if avoid_localize : \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( naive_dt ) ) \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( self . tz . localize ( naive_dt ) . astimezone ( pytz . utc ) ) ) \n 
~~ ~~ if len ( uss ) == 0 : \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( datetime . utcnow ( ) ) ) \n 
~~ if len ( uss ) > 1 : \n 
~~ self . us = uss . pop ( ) \n 
if len ( args ) > 0 : \n 
~~ ~~ @ property \n 
def ms ( self ) : return self . us / MILLI_MICROS \n 
epoch_milliseconds = epoch_millis = milliseconds = millis = ms \n 
def s ( self ) : return self . us / SECOND_MICROS \n 
epoch_seconds = epoch_secs = seconds = secs = s \n 
def m ( self ) : return self . us / MINUTE_MICROS \n 
epoch_minutes = epoch_mins = minutes = mins = m \n 
def micros ( self ) : return self . us \n 
epoch_microseconds = epoch_micros = microseconds = micros \n 
def tz_name ( self ) : return self . tz . zone \n 
def tz_abbr ( self ) : return self . tz . _tzname \n 
def set_tz ( self , tz ) : \n 
~~~ self . tz = self . __class__ . to_timezone ( tz ) ; return self \n 
~~ def with_tz ( self , tz ) : \n 
~~~ return self . __class__ ( self . us , tz ) \n 
~~ @ property \n 
def _tuple ( self ) : return ( self . us , self . tz ) \n 
def strftime ( self , * args , ** kwargs ) : return self . datetime . strftime ( * args , ** kwargs ) \n 
def __cmp__ ( self , other ) : \n 
~~~ if not hasattr ( other , ) : other = SaneTime ( other ) \n 
return cmp ( self . us , int ( other ) ) \n 
~~ def __hash__ ( self ) : return self . us . __hash__ ( ) \n 
def __add__ ( self , operand ) : \n 
~~~ if not hasattr ( operand , ) : operand = SaneTime ( operand ) \n 
return self . __class__ ( self . us + int ( operand ) , tz = self . tz ) \n 
~~ def __sub__ ( self , operand ) : \n 
if isinstance ( operand , SaneTime ) : return SaneDelta ( self . us - int ( operand ) ) \n 
return self . __add__ ( - int ( operand ) ) \n 
~~ def __mul__ ( self , operand ) : \n 
~~~ return self . us * int ( operand ) \n 
~~ def __div__ ( self , operand ) : \n 
~~~ return self . us / int ( operand ) \n 
~~ def __int__ ( self ) : return int ( self . us ) \n 
def __long__ ( self ) : return long ( self . us ) \n 
def __repr__ ( self ) : return u"SaneTime(%s,%s)" % ( self . us , repr ( self . tz ) ) \n 
def __str__ ( self ) : return unicode ( self ) . encode ( ) \n 
def __unicode__ ( self ) : \n 
~~~ dt = self . datetime \n 
micros = u".%06d" % dt . microsecond if dt . microsecond else \n 
~~ def clone ( self ) : \n 
return self . __class__ ( self . us , self . tz ) \n 
def ny_str ( self ) : \n 
return self . ny_ndt . strftime ( ) \n 
def utc_datetime ( self ) : return SaneTime . us_to_utc_datetime ( self . us ) \n 
utc_dt = utc_datetime \n 
def utc_naive_datetime ( self ) : return self . utc_datetime . replace ( tzinfo = None ) \n 
utc_ndt = utc_naive_datetime \n 
def to_timezoned_datetime ( self , tz ) : return self . utc_datetime . astimezone ( SaneTime . to_timezone ( tz ) ) \n 
def to_timezoned_naive_datetime ( self , tz ) : return self . to_timezoned_datetime ( tz ) . replace ( tzinfo = None ) \n 
def datetime ( self ) : return self . to_timezoned_datetime ( self . tz ) \n 
dt = datetime \n 
def naive_datetime ( self ) : return self . to_timezoned_naive_datetime ( self . tz ) \n 
ndt = naive_datetime \n 
def ny_datetime ( self ) : return self . to_timezoned_datetime ( ) \n 
ny_dt = ny_datetime \n 
def ny_naive_datetime ( self ) : return self . to_timezoned_naive_datetime ( ) \n 
ny_ndt = ny_naive_datetime \n 
def year ( self ) : return self . dt . year \n 
def month ( self ) : return self . dt . month \n 
def day ( self ) : return self . dt . day \n 
def hour ( self ) : return self . dt . hour \n 
def minute ( self ) : return self . dt . minute \n 
def second ( self ) : return self . dt . second \n 
def microsecond ( self ) : return self . dt . microsecond \n 
@ classmethod \n 
def utc_datetime_to_us ( kls , dt ) : \n 
~~~ return calendar . timegm ( dt . timetuple ( ) ) * 1000 ** 2 + dt . microsecond \n 
~~ @ classmethod \n 
def us_to_utc_datetime ( kls , us ) : \n 
~~~ return pytz . utc . localize ( datetime . utcfromtimestamp ( us / 10 ** 6 ) ) . replace ( microsecond = us % 10 ** 6 ) \n 
def to_timezone ( kls , tz ) : \n 
~~~ if not isinstance ( tz , basestring ) : return tz \n 
return pytz . timezone ( tz ) \n 
~~ ~~ def ntime ( * args , ** kwargs ) : \n 
~~~ if args : \n 
~~~ if args [ 0 ] is None : return None \n 
~~ elif kwargs : \n 
~~~ if None in [ v for k , v in kwargs . iteritems ( ) if k != ] : return None \n 
~~ return SaneTime ( * args , ** kwargs ) \n 
~~ time = sanetime = SaneTime \n 
nsanetime = ntime \n 
from tastypie . authorization import Authorization \n 
from openpds . authentication import OAuth2Authentication \n 
from openpds . core . models import Profile , AuditEntry \n 
import settings \n 
import pdb \n 
class PDSAuthorization ( Authorization ) : \n 
~~~ audit_enabled = True \n 
scope = "" \n 
requester_uuid = "" \n 
def requester ( self ) : \n 
~~~ return self . requester_uuid \n 
~~ def trustWrapper ( self , datastore_owner ) : \n 
~~ def is_authorized ( self , request , object = None ) : \n 
~~~ authenticator = OAuth2Authentication ( self . scope ) \n 
if "datastore_owner__uuid" in request . REQUEST : \n 
~~~ authorized = True \n 
token = request . REQUEST [ "bearer_token" ] if "bearer_token" in request . REQUEST else request . META [ "HTTP_BEARER_TOKEN" ] \n 
datastore_owner_uuid = request . REQUEST [ "datastore_owner__uuid" ] \n 
datastore_owner , ds_owner_created = Profile . objects . get_or_create ( uuid = datastore_owner_uuid ) \n 
self . requester_uuid = authenticator . get_userinfo_from_token ( token , self . scope ) \n 
if self . requester_uuid is False or self . requester_uuid is None or len ( self . requester_uuid ) == 0 : \n 
~~~ self . requester_uuid = "not-specified" \n 
authorized = False \n 
~~ self . trustWrapper ( datastore_owner ) \n 
~~~ if ( self . audit_enabled ) : \n 
#pdb.set_trace() \n 
~~~ audit_entry = AuditEntry ( token = token ) \n 
audit_entry . method = request . method \n 
audit_entry . scope = self . scope \n 
audit_entry . purpose = request . REQUEST [ "purpose" ] if "purpose" in request . REQUEST else "" \n 
audit_entry . system_entity_toggle = request . REQUEST [ "system_entity" ] if "system_entity" in request . REQUEST else False \n 
audit_entry . datastore_owner = datastore_owner \n 
audit_entry . requester , created = Profile . objects . get_or_create ( uuid = self . requester_uuid ) \n 
audit_entry . script = request . path \n 
audit_entry . save ( ) \n 
~~~ print e \n 
~~ return authorized \n 
~~ return False \n 
~~ def __init__ ( self , scope , audit_enabled = True ) : \n 
~~~ self . scope = scope \n 
self . audit_enabled = audit_enabled \n 
from django import template \n 
register = template . Library ( ) \n 
class VerbatimNode ( template . Node ) : \n 
~~~ def __init__ ( self , text ) : \n 
~~~ self . text = text \n 
~~ def render ( self , context ) : \n 
~~~ return self . text \n 
~~ ~~ @ register . tag \n 
def verbatim ( parser , token ) : \n 
~~~ text = [ ] \n 
while 1 : \n 
~~~ token = parser . tokens . pop ( 0 ) \n 
if token . contents == : \n 
~~ if token . token_type == template . TOKEN_VAR : \n 
~~~ text . append ( ) \n 
~~ elif token . token_type == template . TOKEN_BLOCK : \n 
~~ text . append ( token . contents ) \n 
if token . token_type == template . TOKEN_VAR : \n 
~~ ~~ return VerbatimNode ( . join ( text ) ) \n 
~~ from django . shortcuts import render_to_response \n 
from django . template import RequestContext \n 
from werkzeug . utils import cached_property \n 
from base import db , Base \n 
from cluster import Cluster \n 
class Proxy ( Base ) : \n 
~~~ __tablename__ = \n 
host = db . Column ( db . String ( 255 ) , nullable = False ) \n 
port = db . Column ( db . Integer , nullable = False ) \n 
eru_container_id = db . Column ( db . String ( 64 ) , index = True ) \n 
cluster_id = db . Column ( db . ForeignKey ( Cluster . id ) , index = True ) \n 
suppress_alert = db . Column ( db . Integer , nullable = False , default = 1 ) \n 
__table_args__ = ( db . Index ( , , , unique = True ) , ) \n 
@ cached_property \n 
def eru_deployed ( self ) : \n 
~~~ return self . eru_container_id is not None \n 
~~ @ cached_property \n 
def eru_info ( self ) : \n 
~~~ import eru_utils \n 
if eru_utils . eru_client is None or not self . eru_deployed : \n 
~~ return eru_utils . eru_client . get_container ( self . eru_container_id ) \n 
def cluster ( self ) : \n 
~~~ return Cluster . query . get ( self . cluster_id ) \n 
~~ ~~ def get_by_host_port ( host , port ) : \n 
~~~ return db . session . query ( Proxy ) . filter ( \n 
Proxy . host == host , Proxy . port == port ) . first ( ) \n 
~~ def del_by_host_port ( host , port ) : \n 
Proxy . host == host , Proxy . port == port ) . delete ( ) \n 
~~ def get_or_create ( host , port , cluster_id = None ) : \n 
~~~ p = db . session . query ( Proxy ) . filter ( \n 
if p is None : \n 
~~~ p = Proxy ( host = host , port = port , cluster_id = cluster_id ) \n 
db . session . add ( p ) \n 
db . session . flush ( ) \n 
~~ return p \n 
~~ def create_eru_instance ( host , port , cluster_id , eru_container_id ) : \n 
~~~ node = Proxy ( host = host , port = port , eru_container_id = eru_container_id , \n 
cluster_id = cluster_id ) \n 
db . session . add ( node ) \n 
return node \n 
~~ def delete_eru_instance ( eru_container_id ) : \n 
~~~ db . session . query ( Proxy ) . filter ( \n 
Proxy . eru_container_id == eru_container_id ) . delete ( ) \n 
~~ def get_eru_by_container_id ( eru_container_id ) : \n 
Proxy . eru_container_id == eru_container_id ) . first ( ) \n 
~~ def list_all ( ) : \n 
~~~ return db . session . query ( Proxy ) . all ( ) \n 
~~ def list_eru_proxies ( offset , limit ) : \n 
Proxy . eru_container_id != None ) . order_by ( \n 
Proxy . id . desc ( ) ) . offset ( offset ) . limit ( limit ) . all ( ) \n 
~~ def list_ip ( ) : \n 
~~~ return db . session . query ( Proxy . host , Proxy . port ) . all ( ) \n 
~~ from ethereum import tester \n 
import hydrachain . native_contracts as nc \n 
from fungible_contract import IOU \n 
import ethereum . slogging as slogging \n 
log = slogging . get_logger ( ) \n 
def test_iou_template ( ) : \n 
nc . registry . register ( IOU ) \n 
state = tester . state ( ) \n 
logs = [ ] \n 
issuer_address = tester . a0 \n 
issuer_key = tester . k0 \n 
for evt_class in IOU . events : \n 
~~~ nc . listen_logs ( state , evt_class , callback = lambda e : logs . append ( e ) ) \n 
~~ iou_address = nc . tester_create_native_contract_instance ( state , issuer_key , IOU ) \n 
iou_as_issuer = nc . tester_nac ( state , issuer_key , iou_address ) \n 
iou_as_issuer . init ( ) \n 
assert iou_as_issuer . balanceOf ( issuer_address ) == 0 \n 
amount_issued = 200000 \n 
iou_as_issuer . issue_funds ( amount_issued , ) \n 
assert iou_as_issuer . balanceOf ( issuer_address ) == amount_issued \n 
assert iou_as_issuer . balanceOf ( issuer_address ) == 2 * amount_issued \n 
assert iou_as_issuer . get_issued_amount ( issuer_address ) == 2 * amount_issued \n 
print logs \n 
while logs and logs . pop ( ) : \n 
~~ nc . registry . unregister ( IOU ) \n 
from view_controls . view import DrawingTool , Event \n 
from game_objects . item import Item \n 
from game_objects . state import TrackerState , TrackerStateEncoder \n 
from log_parser import LogParser \n 
from options import Options \n 
class IsaacTracker ( object ) : \n 
def __init__ ( self , logging_level = logging . INFO , read_timer = 1 ) : \n 
~~~ self . read_timer = read_timer \n 
self . file_prefix = "../" \n 
self . log = logging . getLogger ( "tracker" ) \n 
self . log . addHandler ( logging . FileHandler ( self . file_prefix + "tracker_log.txt" , mode = ) ) \n 
self . log . setLevel ( logging_level ) \n 
with open ( self . file_prefix + "items.json" , "r" ) as items_file : \n 
~~~ Item . items_info = json . load ( items_file ) \n 
~~ with open ( self . file_prefix + , ) as f : \n 
~~~ self . tracker_version = f . read ( ) \n 
~~ Options ( ) . load_options ( self . file_prefix + "options.json" ) \n 
~~ def __del__ ( self ) : \n 
~~~ Options ( ) . save_options ( self . file_prefix + "options.json" ) \n 
~~ def check_for_update ( self ) : \n 
~~~ latest = "https://api.github.com/repos/Hyphen-ated/RebirthItemTracker/releases/latest" \n 
github_info_json = urllib2 . urlopen ( latest ) . read ( ) \n 
info = json . loads ( github_info_json ) \n 
latest_version = info [ "name" ] \n 
if latest_version != self . tracker_version : \n 
~~ return title_text \n 
~~ return "" \n 
~~ def run ( self ) : \n 
update_notifier = self . check_for_update ( ) \n 
framecount = 0 \n 
drawing_tool = DrawingTool ( self . file_prefix ) \n 
drawing_tool . set_window_title ( update_notifier ) \n 
parser = LogParser ( self . file_prefix , self . tracker_version ) \n 
opt = Options ( ) \n 
log = logging . getLogger ( "tracker" ) \n 
event_result = None \n 
state = None \n 
read_from_server = opt . read_from_server \n 
write_to_server = opt . write_to_server \n 
state_version = - 1 \n 
twitch_username = None \n 
new_states_queue = [ ] \n 
screen_error_message = None \n 
while event_result != Event . DONE : \n 
~~~ event_result = drawing_tool . handle_events ( ) \n 
if opt . read_from_server != read_from_server or opt . twitch_name != twitch_username : \n 
~~~ twitch_username = opt . twitch_name \n 
if read_from_server : \n 
~~~ state_version = - 1 \n 
drawing_tool . set_window_title ( update_notifier , watching_player = twitch_username , updates_queued = len ( new_states_queue ) ) \n 
~~~ drawing_tool . set_window_title ( update_notifier ) \n 
~~ ~~ if opt . write_to_server and opt . write_to_server != write_to_server : \n 
~~~ write_to_server = True \n 
drawing_tool . set_window_title ( update_notifier , uploading = True ) \n 
~~ if not opt . write_to_server : \n 
~~~ write_to_server = False \n 
~~ if opt . read_from_server : \n 
~~~ update_timer = 2 \n 
~~~ update_timer = self . read_timer \n 
~~ if event_result == Event . OPTIONS_UPDATE : \n 
~~~ framecount = 0 \n 
if state is not None : \n 
~~~ state . modified = True \n 
~~ ~~ if ( framecount % int ( Options ( ) . framerate_limit * update_timer ) == 0 ) : \n 
~~~ if opt . read_from_server : \n 
~~~ base_url = opt . trackerserver_url + "/tracker/api/user/" + opt . twitch_name \n 
json_dict = None \n 
~~~ json_version = urllib2 . urlopen ( base_url + "/version" ) . read ( ) \n 
if int ( json_version ) > state_version : \n 
~~~ json_state = urllib2 . urlopen ( base_url ) . read ( ) \n 
json_dict = json . loads ( json_state ) \n 
new_state = TrackerState . from_json ( json_dict ) \n 
if new_state is None : \n 
~~~ raise Exception \n 
~~ state_version = int ( json_version ) \n 
new_states_queue . append ( ( state_version , new_state ) ) \n 
drawing_tool . set_window_title ( update_notifier , watching_player = twitch_username , updates_queued = len ( new_states_queue ) , read_delay = opt . read_delay ) \n 
~~~ state = None \n 
log . error ( traceback . format_exc ( ) ) \n 
if json_dict is not None : \n 
~~~ their_version = "" \n 
if "tracker_version" in json_dict : \n 
~~~ their_version = json_dict [ "tracker_version" ] \n 
~~~ their_version = "0.10-beta1" \n 
~~ if their_version != self . tracker_version : \n 
~~~ force_draw = state and state . modified \n 
state = parser . parse ( ) \n 
if force_draw : \n 
~~ if write_to_server and not opt . trackerserver_authkey : \n 
~~ if state is not None and write_to_server and state . modified and screen_error_message is None : \n 
~~~ opener = urllib2 . build_opener ( urllib2 . HTTPHandler ) \n 
put_url = opt . trackerserver_url + "/tracker/api/update/" + opt . trackerserver_authkey \n 
json_string = json . dumps ( state , cls = TrackerStateEncoder , sort_keys = True ) \n 
request = urllib2 . Request ( put_url , \n 
data = json_string ) \n 
request . add_header ( , ) \n 
request . get_method = lambda : \n 
~~~ result = opener . open ( request ) \n 
result_json = json . loads ( result . read ( ) ) \n 
updated_user = result_json [ "updated_user" ] \n 
if updated_user is None : \n 
~~~ screen_error_message = None \n 
~~~ import traceback \n 
errmsg = traceback . format_exc ( ) \n 
log . error ( errmsg ) \n 
~~ ~~ ~~ ~~ if len ( new_states_queue ) > 0 : \n 
~~~ ( state_timestamp , new_state ) = new_states_queue [ 0 ] \n 
current_timestamp = int ( time . time ( ) ) \n 
if current_timestamp - state_timestamp >= opt . read_delay or state is None : \n 
~~~ state = new_state \n 
new_states_queue . pop ( 0 ) \n 
~~ ~~ if state is None and screen_error_message is None : \n 
~~~ if read_from_server : \n 
~~ ~~ if screen_error_message is not None : \n 
~~~ drawing_tool . write_error_message ( screen_error_message ) \n 
~~~ drawing_tool . draw_state ( state ) \n 
~~ drawing_tool . tick ( ) \n 
framecount += 1 \n 
~~ drawing_tool . save_window_position ( ) \n 
~~~ rt = IsaacTracker ( ) \n 
rt . run ( ) \n 
print ( errmsg ) \n 
logging . getLogger ( "tracker" ) . error ( errmsg ) \n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ main ( ) \n 
~~ from __future__ import division , print_function , unicode_literals \n 
from collections import OrderedDict \n 
from brainstorm . layers . base_layer import Layer \n 
from brainstorm . structure . buffer_structure import ( BufferStructure , \n 
StructureTemplate ) \n 
from brainstorm . structure . construction import ConstructionWrapper \n 
from brainstorm . utils import flatten_all_but_last \n 
def BatchNorm ( name = None , decay = 0.9 , epsilon = 1.0e-5 ) : \n 
return ConstructionWrapper . create ( BatchNormLayerImpl , \n 
decay = decay , \n 
epsilon = epsilon ) \n 
~~ class BatchNormLayerImpl ( Layer ) : \n 
~~~ expected_inputs = { : StructureTemplate ( , , ) } \n 
expected_kwargs = { , } \n 
def setup ( self , kwargs , in_shapes ) : \n 
~~~ self . epsilon = kwargs . get ( , 1.0e-5 ) \n 
self . decay = kwargs . get ( , 0.9 ) \n 
outputs = OrderedDict ( ) \n 
outputs [ ] = in_shapes [ ] \n 
parameters = OrderedDict ( ) \n 
buf = BufferStructure ( self . in_shapes [ ] . feature_shape [ - 1 ] ) \n 
parameters [ ] = buf \n 
internals = OrderedDict ( ) \n 
internals [ ] = buf \n 
internals [ ] = self . in_shapes [ ] \n 
return outputs , parameters , internals \n 
~~ def forward_pass ( self , buffers , training_pass = True ) : \n 
~~~ _h = self . handler \n 
sigma_b , centered , x_hat = buffers . internals \n 
gamma , beta , mu , sigma = buffers . parameters \n 
inputs = flatten_all_but_last ( buffers . inputs . default ) \n 
centered = flatten_all_but_last ( centered ) \n 
x_hat = flatten_all_but_last ( x_hat ) \n 
out = flatten_all_but_last ( buffers . outputs . default ) \n 
m = inputs . shape [ 0 ] \n 
if training_pass : \n 
_h . sum_t ( inputs , 0 , mu_b ) \n 
_h . mult_st ( - 1.0 / m , mu_b , mu_b ) \n 
_h . mult_st ( self . decay , mu , mu ) \n 
_h . mult_add_st ( 1.0 - self . decay , mu_b , mu ) \n 
mu = mu_b \n 
~~ _h . add_mv ( inputs , mu . reshape ( ( 1 , mu . size ) ) , centered ) \n 
_h . mult_tt ( centered , centered , centered2 ) \n 
_h . sum_t ( centered2 , 0 , sigma2 ) \n 
_h . sqrt_t ( sigma2 , sigma_b ) \n 
_h . mult_st ( self . decay , sigma , sigma ) \n 
_h . mult_add_st ( 1.0 - self . decay , sigma_b , sigma ) \n 
sigma = sigma_b \n 
~~ _h . divide_mv ( centered , sigma . reshape ( ( 1 , sigma . size ) ) , x_hat ) \n 
_h . mult_mv ( x_hat , gamma . reshape ( ( 1 , gamma . size ) ) , out ) \n 
_h . add_mv ( out , beta . reshape ( ( 1 , beta . size ) ) , out ) \n 
~~ def backward_pass ( self , buffers ) : \n 
gamma = buffers . parameters . gamma \n 
dgamma = buffers . gradients . gamma \n 
dbeta = buffers . gradients . beta \n 
outdeltas = flatten_all_but_last ( buffers . output_deltas . default ) \n 
indeltas = flatten_all_but_last ( buffers . input_deltas . default ) \n 
m = outdeltas . shape [ 0 ] \n 
tmp = big_tmp \n 
dgamma_tmp = small_tmp \n 
_h . mult_tt ( outdeltas , x_hat , tmp ) \n 
_h . sum_t ( tmp , axis = 0 , out = dgamma_tmp ) \n 
_h . add_tt ( dgamma_tmp , dgamma , dgamma ) \n 
_h . mult_st ( 1 / m , dgamma_tmp , dgamma_tmp ) \n 
term1 = big_tmp \n 
_h . mult_mv ( x_hat , dgamma_tmp . reshape ( ( 1 , gamma . size ) ) , term1 ) \n 
dbeta_tmp = small_tmp \n 
_h . sum_t ( outdeltas , axis = 0 , out = dbeta_tmp ) \n 
_h . add_tt ( dbeta_tmp , dbeta , dbeta ) \n 
_h . mult_st ( 1 / m , dbeta_tmp , dbeta_tmp ) \n 
term2 = big_tmp \n 
term3 = big_tmp \n 
_h . subtract_tt ( outdeltas , term1 , term2 ) \n 
_h . subtract_mv ( term2 , dbeta_tmp . reshape ( ( 1 , dbeta . size ) ) , term3 ) \n 
coeff = small_tmp \n 
_h . divide_tt ( gamma , sigma_b , coeff ) \n 
term4 = big_tmp \n 
_h . mult_mv ( term3 , coeff . reshape ( ( 1 , coeff . size ) ) , term4 ) \n 
_h . add_tt ( term4 , indeltas , indeltas ) \n 
~~ ~~ from __future__ import division , print_function , unicode_literals \n 
import numpy as np \n 
from brainstorm . describable import Describable \n 
class Scorer ( Describable ) : \n 
~~~ def __init__ ( self , out_name = , targets_name = , mask_name = , \n 
name = None ) : \n 
~~~ self . out_name = out_name \n 
self . targets_name = targets_name \n 
self . mask_name = mask_name \n 
self . __name__ = name if name is not None else self . __class__ . __name__ \n 
~~ def __call__ ( self , true_labels , predicted , mask = None ) : \n 
def aggregate ( errors ) : \n 
~~~ errors = np . array ( errors ) \n 
assert errors . ndim == 2 and errors . shape [ 1 ] == 2 \n 
return np . sum ( errors [ : , 1 ] ) / np . sum ( errors [ : , 0 ] ) \n 
~~ ~~ def gather_losses_and_scores ( net , scorers , scores , out_name = , \n 
targets_name = , mask_name = ) : \n 
~~~ ls = net . get_loss_values ( ) \n 
for name , loss in ls . items ( ) : \n 
~~~ scores [ name ] . append ( ( net . _buffer_manager . batch_size , loss ) ) \n 
~~ for sc in scorers : \n 
~~~ name = sc . __name__ \n 
predicted = net . get ( sc . out_name or out_name or net . output_name ) \n 
true_labels = net . get_input ( sc . targets_name ) if sc . targets_name else net . get_input ( targets_name ) \n 
mask = net . get_input ( sc . mask_name ) if sc . mask_name else ( net . get_input ( mask_name ) if mask_name else None ) \n 
predicted = _flatten_all_but_last ( predicted ) \n 
true_labels = _flatten_all_but_last ( true_labels ) \n 
mask = _flatten_all_but_last ( mask ) \n 
weight = mask . sum ( ) if mask is not None else predicted . shape [ 0 ] \n 
scores [ name ] . append ( ( weight , sc ( true_labels , predicted , mask ) ) ) \n 
~~ ~~ def aggregate_losses_and_scores ( scores , net , scorers ) : \n 
~~~ results = OrderedDict ( ) \n 
for name in net . get_loss_values ( ) : \n 
~~~ results [ name ] = _weighted_average ( scores [ name ] ) \n 
~~~ results [ sc . __name__ ] = sc . aggregate ( scores [ sc . __name__ ] ) \n 
~~ return results \n 
~~ class Accuracy ( Scorer ) : \n 
~~~ def __call__ ( self , true_labels , predicted , mask = None ) : \n 
~~~ if predicted . shape [ 1 ] > 1 : \n 
~~~ predicted = predicted . argmax ( 1 ) . reshape ( - 1 , 1 ) \n 
~~ correct = ( predicted == true_labels ) . astype ( np . float ) \n 
if mask is not None : \n 
~~~ correct *= mask \n 
~~ return np . sum ( correct ) \n 
~~ ~~ class Hamming ( Scorer ) : \n 
~~~ def __init__ ( self , threshold = 0.5 , out_name = , targets_name = , \n 
mask_name = , name = None ) : \n 
~~~ super ( Hamming , self ) . __init__ ( out_name , targets_name , mask_name , name ) \n 
self . threshold = threshold \n 
~~~ correct = np . logical_xor ( predicted < self . threshold , \n 
true_labels ) . astype ( np . float ) \n 
~~ return np . sum ( correct ) / true_labels . shape [ 1 ] \n 
~~ ~~ class MeanSquaredError ( Scorer ) : \n 
~~~ errors = ( true_labels - predicted ) ** 2 \n 
~~~ errors *= mask \n 
~~ return 0.5 * np . sum ( errors ) \n 
~~ ~~ def _flatten_all_but_last ( a ) : \n 
~~~ if a is None : \n 
~~ return a . reshape ( - 1 , a . shape [ - 1 ] ) \n 
~~ def _weighted_average ( errors ) : \n 
return np . sum ( errors [ : , 1 ] * errors [ : , 0 ] / np . sum ( errors [ : , 0 ] ) ) \n 
import pytest \n 
import six \n 
from brainstorm . training . schedules import Exponential , Linear , MultiStep \n 
def test_linear ( ) : \n 
~~~ sch = Linear ( initial_value = 1.0 , final_value = 0.5 , num_changes = 5 ) \n 
epochs = [ 0 ] * 2 + [ 1 ] * 2 + [ 2 ] * 2 + [ 3 ] * 2 + [ 4 ] * 2 \n 
updates = range ( 10 ) \n 
values = [ sch ( epoch , update , , 1 , None , None , None ) \n 
for epoch , update in six . moves . zip ( epochs , updates ) ] \n 
assert values == [ 1.0 , 1.0 , 0.9 , 0.9 , 0.8 , 0.8 , 0.7 , 0.7 , 0.6 , 0.6 ] \n 
assert values == [ 1.0 , 0.9 , 0.8 , 0.7 , 0.6 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ] \n 
values = [ sch ( epoch , update , , 3 , None , None , None ) \n 
assert values == [ 1.0 , 1.0 , 1.0 , 0.9 , 0.9 , 0.9 , 0.8 , 0.8 , 0.8 , 0.7 ] \n 
~~ def test_exponential ( ) : \n 
~~~ sch = Exponential ( initial_value = 1.0 , factor = 0.99 , minimum = 0.97 ) \n 
epochs = [ 0 ] * 4 + [ 1 ] * 4 + [ 2 ] * 4 \n 
updates = range ( 12 ) \n 
assert values == [ 1.0 ] * 4 + [ 0.99 ] * 4 + [ 0.99 * 0.99 ] * 4 \n 
assert values == [ 1.0 * ( 0.99 ** x ) for x in range ( 4 ) ] + [ 0.97 ] * 8 \n 
assert values == [ 1.0 ] * 3 + [ 0.99 ] * 3 + [ 0.9801 ] * 3 + [ 0.99 ** 3 ] * 3 \n 
~~ def test_multistep ( ) : \n 
~~~ sch = MultiStep ( initial_value = 1.0 , steps = [ 3 , 5 , 8 ] , \n 
values = [ 0.1 , 0.01 , 0.001 ] ) \n 
assert values == [ 1.0 , 1.0 , 1.0 , 1.0 , 1.0 , 1.0 , 0.1 , 0.1 , 0.1 , 0.1 ] \n 
assert values == [ 1.0 , 1.0 , 1.0 , 0.1 , 0.1 , 0.01 , 0.01 , 0.01 , 0.001 , 0.001 ] \n 
with pytest . raises ( AssertionError ) : \n 
~~~ _ = sch ( 0 , 0 , , 3 , None , None , None ) \n 
~~ ~~ import os \n 
~~~ from unittest . mock import MagicMock \n 
~~ except ImportError : \n 
~~~ from mock import Mock as MagicMock \n 
~~ class Mock ( MagicMock ) : \n 
~~~ @ classmethod \n 
def __getattr__ ( cls , name ) : \n 
~~~ return Mock ( ) \n 
~~ ~~ MOCK_MODULES = [ , ] \n 
sys . modules . update ( ( mod_name , Mock ( ) ) for mod_name in MOCK_MODULES ) \n 
cwd = os . getcwd ( ) \n 
parent = os . path . dirname ( cwd ) \n 
sys . path . insert ( 0 , parent ) \n 
import brainstorm \n 
extensions = [ , , \n 
] \n 
templates_path = [ ] \n 
source_suffix = \n 
master_doc = \n 
project = \n 
copyright = \n 
version = brainstorm . __version__ \n 
release = brainstorm . __version__ \n 
exclude_patterns = [ ] \n 
pygments_style = \n 
on_rtd = os . environ . get ( , None ) == \n 
if not on_rtd : \n 
~~~ import sphinx_rtd_theme \n 
html_theme = \n 
html_theme_path = [ sphinx_rtd_theme . get_html_theme_path ( ) ] \n 
~~~ html_theme = \n 
~~ ~~ html_static_path = [ ] \n 
htmlhelp_basename = \n 
latex_elements = { \n 
latex_documents = [ \n 
( , , , \n 
, ) , \n 
man_pages = [ \n 
[ ] , 1 ) \n 
texinfo_documents = [ \n 
, , , \n 
) , \n 
from __future__ import division , print_function , unicode_literals \n 
from sacred . utils import iter_prefixes , join_paths \n 
class ConfigSummary ( dict ) : \n 
~~~ def __init__ ( self , added = ( ) , modified = ( ) , typechanged = ( ) , \n 
ignored_fallbacks = ( ) ) : \n 
~~~ super ( ConfigSummary , self ) . __init__ ( ) \n 
self . added = set ( added ) \n 
self . typechanged = dict ( typechanged ) \n 
self . ensure_coherence ( ) \n 
~~ def update_from ( self , config_mod , path = ) : \n 
~~~ added = config_mod . added \n 
updated = config_mod . modified \n 
typechanged = config_mod . typechanged \n 
self . added &= { join_paths ( path , a ) for a in added } \n 
self . modified |= { join_paths ( path , u ) for u in updated } \n 
self . typechanged . update ( { join_paths ( path , k ) : v \n 
for k , v in typechanged . items ( ) } ) \n 
~~ def update_add ( self , config_mod , path = ) : \n 
self . added |= { join_paths ( path , a ) for a in added } \n 
~~ def ensure_coherence ( self ) : \n 
~~~ self . modified |= { p for a in self . added for p in iter_prefixes ( a ) } \n 
self . modified |= { p for u in self . modified for p in iter_prefixes ( u ) } \n 
self . modified |= { p for t in self . typechanged \n 
for p in iter_prefixes ( t ) } \n 
self . added -= set ( self . typechanged . keys ( ) ) \n 
self . modified -= set ( self . typechanged . keys ( ) ) \n 
self . modified -= self . added \n 
import sacred . optional as opt \n 
from sacred . config import ConfigDict \n 
from sacred . config . custom_containers import DogmaticDict , DogmaticList \n 
@ pytest . fixture \n 
def conf_dict ( ) : \n 
~~~ cfg = ConfigDict ( { \n 
"a" : 1 , \n 
"b" : 2.0 , \n 
"c" : True , \n 
"d" : , \n 
"e" : [ 1 , 2 , 3 ] , \n 
"f" : { : , : } , \n 
} ) \n 
return cfg \n 
~~ def test_config_dict_returns_dict ( conf_dict ) : \n 
~~~ assert isinstance ( conf_dict ( ) , dict ) \n 
~~ def test_config_dict_result_contains_keys ( conf_dict ) : \n 
~~~ cfg = conf_dict ( ) \n 
assert set ( cfg . keys ( ) ) == { , , , , , } \n 
assert cfg [ ] == 1 \n 
assert cfg [ ] == 2.0 \n 
assert cfg [ ] \n 
assert cfg [ ] == \n 
assert cfg [ ] == [ 1 , 2 , 3 ] \n 
assert cfg [ ] == { : , : } \n 
~~ def test_fixing_values ( conf_dict ) : \n 
~~~ assert conf_dict ( { : 100 } ) [ ] == 100 \n 
def test_config_dict_raises_on_invalid_keys ( key ) : \n 
~~~ with pytest . raises ( KeyError ) : \n 
~~~ ConfigDict ( { key : True } ) \n 
~~ ~~ @ pytest . mark . parametrize ( "value" , [ lambda x : x , pytest , test_fixing_values ] ) \n 
def test_config_dict_raises_on_invalid_values ( value ) : \n 
~~~ with pytest . raises ( ValueError ) : \n 
~~~ ConfigDict ( { "invalid" : value } ) \n 
~~ ~~ def test_fixing_nested_dicts ( conf_dict ) : \n 
~~~ cfg = conf_dict ( { : { : } } ) \n 
assert cfg [ ] [ ] == \n 
~~ def test_adding_values ( conf_dict ) : \n 
~~~ cfg = conf_dict ( { : 23 , : { : 10 } } ) \n 
assert cfg [ ] == 23 \n 
assert cfg [ ] == { : 10 } \n 
assert cfg . added == { , , } \n 
~~ def test_typechange ( conf_dict ) : \n 
~~~ cfg = conf_dict ( { : , : , : 1 } ) \n 
assert cfg . typechanged == { : ( int , type ( ) ) , \n 
: ( float , type ( ) ) , \n 
: ( bool , int ) } \n 
~~ def test_nested_typechange ( conf_dict ) : \n 
~~~ cfg = conf_dict ( { : { : 10 } } ) \n 
assert cfg . typechanged == { : ( type ( ) , int ) } \n 
~~ def is_dogmatic ( a ) : \n 
~~~ if isinstance ( a , ( DogmaticDict , DogmaticList ) ) : \n 
~~ elif isinstance ( a , dict ) : \n 
~~~ return any ( is_dogmatic ( v ) for v in a . values ( ) ) \n 
~~ elif isinstance ( a , ( list , tuple ) ) : \n 
~~~ return any ( is_dogmatic ( v ) for v in a ) \n 
~~ ~~ def test_result_of_conf_dict_is_not_dogmatic ( conf_dict ) : \n 
~~~ cfg = conf_dict ( { : [ 1 , 1 , 1 ] } ) \n 
assert not is_dogmatic ( cfg ) \n 
def test_conf_scope_handles_numpy_bools ( ) : \n 
"a" : opt . np . bool_ ( 1 ) \n 
assert in cfg ( ) \n 
assert cfg ( ) [ ] \n 
~~ def test_conf_scope_contains_presets ( ) : \n 
~~~ conf_dict = ConfigDict ( { \n 
"answer" : 42 \n 
cfg = conf_dict ( preset = { : 21 , : True } ) \n 
assert set ( cfg . keys ( ) ) == { , , } \n 
assert cfg [ ] == 21 \n 
assert cfg [ ] == 42 \n 
assert cfg [ ] is True \n 
~~ def test_conf_scope_does_not_contain_fallback ( ) : \n 
~~~ config_dict = ConfigDict ( { \n 
cfg = config_dict ( fallback = { : 21 , : 10 } ) \n 
assert set ( cfg . keys ( ) ) == { } \n 
~~ def test_fixed_subentry_of_preset ( ) : \n 
~~~ config_dict = ConfigDict ( { } ) \n 
cfg = config_dict ( preset = { : { : 1 , : 2 } } , fixed = { : { : 10 } } ) \n 
assert set ( cfg [ ] . keys ( ) ) == { , } \n 
assert cfg [ ] [ ] == 10 \n 
assert cfg [ ] [ ] == 2 \n 
~~ class PID ( object ) : \n 
~~~ def __init__ ( self ) : \n 
self . kd = 0 \n 
self . ki = 0 \n 
self . kp = 1 \n 
self . previous_error = 0 \n 
self . integral_error = 0 \n 
~~ def set_k_values ( self , kp , kd , ki ) : \n 
~~~ self . kp = kp \n 
self . ki = ki \n 
self . kd = kd \n 
~~ def clear_error ( self ) : \n 
~~~ self . previous_error = 0 \n 
self . integeral_error = 0 \n 
~~ def pid ( self , target , process_var , timestep ) : \n 
~~~ current_error = ( target - process_var ) \n 
p_error = self . kp * current_error \n 
d_error = self . kd * ( current_error - self . previous_error ) / timestep \n 
self . integral_error = ( \n 
current_error + self . previous_error ) / 2 + self . integral_error \n 
i_error = self . ki * self . integral_error \n 
total_error = p_error + d_error + i_error \n 
self . previous_error = current_error \n 
return total_error \n 
import cmd \n 
import bot . client . ctrl_client as ctrl_client_mod \n 
import bot . client . sub_client as sub_client_mod \n 
class CLI ( cmd . Cmd ) : \n 
def __init__ ( self , ctrl_addr , sub_addr ) : \n 
cmd . Cmd . __init__ ( self ) \n 
~~~ self . ctrl_client = ctrl_client_mod . CtrlClient ( ctrl_addr ) \n 
~~ except Exception , e : \n 
sys . exit ( - 1 ) \n 
~~~ self . sub_client = sub_client_mod . SubClient ( sub_addr ) \n 
~~ ~~ def default ( self , raw_args ) : \n 
if obj_name in self . ctrl_client . objects : \n 
if method_name in self . ctrl_client . objects [ obj_name ] : \n 
~~~ param_dict = { } \n 
for param in params . split ( ) : \n 
~~~ key , value = param . split ( ":" ) \n 
~~~ if "." in value : \n 
~~~ value = float ( value ) \n 
~~~ value = int ( value ) \n 
~~ ~~ except ValueError : \n 
~~~ if value == "True" : \n 
~~~ value = True \n 
~~ elif value == "False" : \n 
~~~ value = False \n 
~~ elif value . startswith ( "\'" ) and value . endswith ( "\'" ) : \n 
~~~ value = value [ 1 : - 1 ] \n 
~~ ~~ param_dict [ key ] = value \n 
~~ ~~ except IndexError : \n 
~~ except ValueError : \n 
~~ result = self . ctrl_client . call ( \n 
obj_name , method_name , param_dict ) \n 
print "-->" , result \n 
~~ ~~ def completenames ( self , text , * ignored ) : \n 
cmd_match_names = cmd . Cmd . completenames ( self , text , * ignored ) \n 
obj_names = self . ctrl_client . objects . keys ( ) \n 
api_match_names = [ x for x in obj_names if x . startswith ( text ) ] \n 
return cmd_match_names + api_match_names \n 
~~ def completedefault ( self , text , line , begidx , endidx ) : \n 
if obj in self . ctrl_client . objects : \n 
~~~ method_names = self . ctrl_client . objects [ obj ] \n 
match_names = [ x for x in method_names if x . startswith ( text ) ] \n 
return match_names \n 
~~ ~~ ~~ def do_list ( self , raw_args ) : \n 
print \n 
for obj_name , methods in sorted ( self . ctrl_client . objects . items ( ) ) : \n 
~~~ print "{}:" . format ( obj_name ) \n 
for method in methods : \n 
~~ ~~ print \n 
~~ def help_list ( self ) : \n 
print "list" \n 
~~ def do_ping ( self , raw_args ) : \n 
reply_time = self . ctrl_client . ping ( ) \n 
~~ def help_ping ( self ) : \n 
print "ping" \n 
~~ def do_sub_add ( self , raw_args ) : \n 
~~~ topic = raw_args . split ( ) [ 0 ] \n 
~~ except ( ValueError , IndexError ) : \n 
~~ self . sub_client . add_topic ( topic ) \n 
~~ def help_sub_add ( self ) : \n 
~~ def do_sub_del ( self , raw_args ) : \n 
~~ self . sub_client . del_topic ( topic ) \n 
~~ def help_sub_del ( self ) : \n 
~~ def do_sub ( self , raw_args ) : \n 
self . sub_client . print_msgs ( ) \n 
~~ def help_sub ( self ) : \n 
print "sub" \n 
~~ def do_stop ( self , raw_args ) : \n 
self . ctrl_client . stop_full ( ) \n 
~~ def help_stop ( self ) : \n 
print "stop" \n 
~~ def do_kill ( self , raw_args ) : \n 
self . ctrl_client . exit_server ( ) \n 
~~ def help_kill ( self ) : \n 
print "kill" \n 
~~ def do_die ( self , raw_args ) : \n 
print "Disconnecting..." \n 
self . ctrl_client . clean_up ( ) \n 
self . sub_client . clean_up ( ) \n 
print "Bye!" \n 
return True \n 
~~ def help_die ( self ) : \n 
print "die" \n 
~~ def do_shell ( self , cmd ) : \n 
os . system ( cmd ) \n 
~~ def help_shell ( self ) : \n 
~~ def do_EOF ( self , raw_args ) : \n 
~~ def help_EOF ( self ) : \n 
print "ctrl+d" \n 
~~ def help_help ( self ) : \n 
~~ ~~ if __name__ == : \n 
~~~ if len ( sys . argv ) == 1 : \n 
CLI ( "tcp://localhost:60000" , "tcp://localhost:60001" ) . cmdloop ( ) \n 
~~ elif len ( sys . argv ) == 3 : \n 
~~~ ctrl_addr = sys . argv [ 1 ] \n 
sub_addr = sys . argv [ 2 ] \n 
CLI ( ctrl_addr , sub_addr ) . cmdloop ( ) \n 
from random import randint \n 
from os import path \n 
import bot . lib . lib as lib \n 
import bot . hardware . servo as s_mod \n 
import tests . test_bot as test_bot \n 
class TestPosition ( test_bot . TestBot ) : \n 
def setUp ( self ) : \n 
super ( TestPosition , self ) . setUp ( ) \n 
config = path . dirname ( path . realpath ( __file__ ) ) + "/test_config.yaml" \n 
self . config = lib . get_config ( config ) \n 
self . pwm_num = self . config [ ] \n 
self . setup_pwm ( self . pwm_num , "1\\n" , "150\\n" , "200\\n" , "0\\n" ) \n 
self . servo = s_mod . Servo ( self . pwm_num ) \n 
~~ def tearDown ( self ) : \n 
super ( TestPosition , self ) . tearDown ( ) \n 
~~ def test_0 ( self ) : \n 
self . servo . position = 0 \n 
assert self . servo . position == 0 , self . servo . position \n 
~~ def test_180 ( self ) : \n 
self . servo . position = 180 \n 
assert self . servo . position == 180 , self . servo . position \n 
~~ def test_middle ( self ) : \n 
self . servo . position = 90 \n 
assert self . servo . position == 90 , self . servo . position \n 
~~ def test_series ( self ) : \n 
for position in range ( 0 , 180 , 18 ) : \n 
~~~ self . servo . position = position \n 
assert self . servo . position == position , self . servo . position \n 
~~ ~~ def test_manually_confirm ( self ) : \n 
for i in range ( 10 ) : \n 
~~~ test_pos = randint ( 0 , 180 ) \n 
self . servo . position = test_pos \n 
cur_pwm = self . get_pwm ( self . pwm_num ) \n 
duty = int ( cur_pwm [ "duty_ns" ] ) \n 
read_pos = int ( round ( ( ( duty - 580000 ) / 2320000. ) * 180 ) ) \n 
~~ ~~ def test_over_max ( self ) : \n 
self . servo . position = 181 \n 
~~ def test_under_min ( self ) : \n 
self . servo . position = - 1 \n 
#:coding=utf-8: \n 
~~ ~~ from django . contrib . syndication . views import Feed as SyndicationFeed \n 
from django . core . urlresolvers import reverse \n 
from django . conf import settings \n 
from lifestream . models import Lifestream , Item \n 
class RecentItemsFeed ( SyndicationFeed ) : \n 
def link ( self , obj ) : \n 
~~~ return reverse ( , kwargs = { \n 
: obj . slug , \n 
~~ def get_object ( self , bits ) : \n 
~~~ return Lifestream . objects . get ( slug = bits [ 0 ] ) \n 
~~ def items ( self , obj ) : \n 
~~~ return Item . objects . published ( ) . filter ( feed__lifestream = obj ) [ : 10 ] \n 
~~ def item_pubdate ( self , item ) : \n 
~~~ return item . date \n 
~~ def item_categories ( self , item ) : \n 
~~~ def item_categories ( self , item ) : \n 
~~~ if in settings . INSTALLED_APPS : \n 
~~~ return [ tag . name for tag in item . tag_set ] \n 
from functools import update_wrapper \n 
from google . appengine . api import users \n 
from werkzeug import redirect \n 
from werkzeug . exceptions import Forbidden \n 
from kay . utils import ( \n 
create_login_url , create_logout_url \n 
from kay . utils . decorators import auto_adapt_to_methods \n 
def login_required ( func ) : \n 
~~~ def inner ( request , * args , ** kwargs ) : \n 
~~~ if request . user . is_anonymous ( ) : \n 
~~~ if request . is_xhr : \n 
~~~ return Forbidden ( ) \n 
~~~ return redirect ( create_login_url ( request . url ) ) \n 
~~ ~~ return func ( request , * args , ** kwargs ) \n 
~~ update_wrapper ( inner , func ) \n 
return inner \n 
~~ login_required = auto_adapt_to_methods ( login_required ) \n 
def admin_required ( func ) : \n 
~~~ if not request . user . is_admin : \n 
~~~ raise Forbidden ( \n 
description = \n 
create_logout_url ( request . url ) \n 
~~ admin_required = auto_adapt_to_methods ( admin_required ) \n 
PARSE_ERROR = - 32700 \n 
INVALID_REQUEST = - 32600 \n 
METHOD_NOT_FOUND = - 32601 \n 
INVALID_PARAMS = - 32602 \n 
INTERNAL_ERROR = - 32603 \n 
errors = { } \n 
~~~ import json \n 
~~~ import django . utils . simplejson as json \n 
~~~ import simplejson as json \n 
~~ ~~ import sys \n 
import itertools \n 
from werkzeug import Request , Response \n 
from werkzeug import exceptions \n 
class JsonRpcApplication ( object ) : \n 
~~~ def __init__ ( self , methods = None ) : \n 
~~~ if methods is not None : \n 
~~~ self . methods = methods \n 
~~~ self . methods = { } \n 
~~ ~~ def add_module ( self , mod , namespace = None ) : \n 
~~~ if namespace is None : \n 
~~~ namespace = mod . __name__ \n 
~~ for k , v in ( ( k , v ) for k , v in mod . __dict__ . iteritems ( ) \n 
if not k . startswith ( ) and callable ( v ) ) : \n 
~~~ self . add ( namespace + + k , v ) \n 
~~ ~~ def add ( self , name , func ) : \n 
~~~ self . methods [ name ] = func \n 
~~ def process ( self , data ) : \n 
~~~ if data . get ( ) != "2.0" : \n 
~~~ return { : , \n 
: data . get ( ) , \n 
: { : INVALID_REQUEST , \n 
: errors [ INVALID_REQUEST ] } } \n 
~~ if not in data : \n 
~~ methodname = data [ ] \n 
if not isinstance ( methodname , basestring ) : \n 
~~ if methodname . startswith ( ) : \n 
: { : METHOD_NOT_FOUND , \n 
: errors [ METHOD_NOT_FOUND ] } } \n 
~~ if methodname not in self . methods : \n 
~~ method = self . methods [ methodname ] \n 
~~~ params = data . get ( , [ ] ) \n 
if isinstance ( params , list ) : \n 
~~~ result = method ( * params ) \n 
~~ elif isinstance ( params , dict ) : \n 
~~~ result = method ( ** dict ( [ ( str ( k ) , v ) for k , v in params . iteritems ( ) ] ) ) \n 
~~ resdata = None \n 
if data . get ( ) : \n 
~~~ resdata = { \n 
: result , \n 
~~ return resdata \n 
: { : INTERNAL_ERROR , \n 
: errors [ INTERNAL_ERROR ] , \n 
: str ( e ) } } \n 
~~ ~~ def __call__ ( self , environ , start_response ) : \n 
~~~ request = Request ( environ ) \n 
if request . method != "POST" : \n 
~~~ raise exceptions . MethodNotAllowed \n 
~~ if not request . content_type . startswith ( ) : \n 
~~~ raise exceptions . BadRequest \n 
~~~ data = json . loads ( request . data ) \n 
~~ except ValueError , e : \n 
~~~ resdata = { : , \n 
: None , \n 
: { : PARSE_ERROR , \n 
: errors [ PARSE_ERROR ] } } \n 
~~~ if isinstance ( data , dict ) : \n 
~~~ resdata = self . process ( data ) \n 
~~ elif isinstance ( data , list ) : \n 
~~~ if len ( [ x for x in data if not isinstance ( x , dict ) ] ) : \n 
~~~ resdata = [ d for d in ( self . process ( d ) for d in data ) \n 
if d is not None ] \n 
~~ ~~ ~~ response = Response ( content_type = "application/json" ) \n 
if resdata : \n 
~~~ response . headers [ "Cache-Control" ] = "no-cache" \n 
response . headers [ "Pragma" ] = "no-cache" \n 
response . headers [ "Expires" ] = "-1" \n 
response . data = json . dumps ( resdata ) \n 
~~ return response ( environ , start_response ) \n 
~~ ~~ def getmod ( modname ) : \n 
~~~ __import__ ( modname ) \n 
~~ except ImportError , e : \n 
~~ mod = sys . modules [ modname ] \n 
return mod \n 
~~ def HTTPExceptionMiddleware ( app ) : \n 
~~~ def wrap ( environ , start_response ) : \n 
~~~ return app ( environ , start_response ) \n 
~~ except exceptions . HTTPException , e : \n 
~~~ return e ( environ , start_response ) \n 
~~ ~~ return wrap \n 
~~ def make_application ( methods ) : \n 
~~~ app = JsonRpcApplication ( ) \n 
for name , value in methods . iteritems ( ) : \n 
~~~ if ":" in value : \n 
~~~ modname , funcname = value . split ( ":" , 1 ) \n 
mod = getmod ( modname ) \n 
if mod : \n 
~~~ app . add ( name , getattr ( mod , funcname ) ) \n 
~~~ modname = value \n 
~~~ app . add_module ( mod , name ) \n 
~~ ~~ ~~ app = HTTPExceptionMiddleware ( app ) \n 
return app \n 
from jinja2 . runtime import Undefined \n 
__test__ = False \n 
number_re = re . compile ( ) \n 
regex_type = type ( number_re ) \n 
~~~ test_callable = callable \n 
~~ except NameError : \n 
~~~ def test_callable ( x ) : \n 
~~~ return hasattr ( x , ) \n 
~~ ~~ def test_odd ( value ) : \n 
return value % 2 == 1 \n 
~~ def test_even ( value ) : \n 
return value % 2 == 0 \n 
~~ def test_divisibleby ( value , num ) : \n 
return value % num == 0 \n 
~~ def test_defined ( value ) : \n 
return not isinstance ( value , Undefined ) \n 
~~ def test_undefined ( value ) : \n 
return isinstance ( value , Undefined ) \n 
~~ def test_none ( value ) : \n 
return value is None \n 
~~ def test_lower ( value ) : \n 
return unicode ( value ) . islower ( ) \n 
~~ def test_upper ( value ) : \n 
return unicode ( value ) . isupper ( ) \n 
~~ def test_string ( value ) : \n 
return isinstance ( value , basestring ) \n 
~~ def test_number ( value ) : \n 
return isinstance ( value , ( int , long , float , complex ) ) \n 
~~ def test_sequence ( value ) : \n 
~~~ len ( value ) \n 
value . __getitem__ \n 
~~ def test_sameas ( value , other ) : \n 
return value is other \n 
~~ def test_iterable ( value ) : \n 
~~~ iter ( value ) \n 
~~ except TypeError : \n 
~~ def test_escaped ( value ) : \n 
return hasattr ( value , ) \n 
~~ TESTS = { \n 
: test_odd , \n 
: test_even , \n 
: test_divisibleby , \n 
: test_defined , \n 
: test_undefined , \n 
: test_none , \n 
: test_lower , \n 
: test_upper , \n 
: test_string , \n 
: test_number , \n 
: test_sequence , \n 
: test_iterable , \n 
: test_callable , \n 
: test_sameas , \n 
: test_escaped \n 
import codecs \n 
import mimetypes \n 
from werkzeug . _internal import _proxy_repr , _missing , _empty_stream \n 
_locale_delim_re = re . compile ( ) \n 
def is_immutable ( self ) : \n 
~~~ raise TypeError ( % self . __class__ . __name__ ) \n 
~~ def iter_multi_items ( mapping ) : \n 
if isinstance ( mapping , MultiDict ) : \n 
~~~ for item in mapping . iteritems ( multi = True ) : \n 
~~~ yield item \n 
~~ ~~ elif isinstance ( mapping , dict ) : \n 
~~~ for key , value in mapping . iteritems ( ) : \n 
~~~ if isinstance ( value , ( tuple , list ) ) : \n 
~~~ for value in value : \n 
~~~ yield key , value \n 
~~~ for item in mapping : \n 
~~ ~~ ~~ class ImmutableListMixin ( object ) : \n 
def __reduce_ex__ ( self , protocol ) : \n 
~~~ return type ( self ) , ( list ( self ) , ) \n 
~~ def __delitem__ ( self , key ) : \n 
~~~ is_immutable ( self ) \n 
~~ def __delslice__ ( self , i , j ) : \n 
~~ def __iadd__ ( self , other ) : \n 
~~ __imul__ = __iadd__ \n 
def __setitem__ ( self , key , value ) : \n 
~~ def __setslice__ ( self , i , j , value ) : \n 
~~ def append ( self , item ) : \n 
~~ remove = append \n 
def extend ( self , iterable ) : \n 
~~ def insert ( self , pos , value ) : \n 
~~ def pop ( self , index = - 1 ) : \n 
~~ def reverse ( self ) : \n 
~~ def sort ( self , cmp = None , key = None , reverse = None ) : \n 
~~ ~~ class ImmutableList ( ImmutableListMixin , list ) : \n 
__repr__ = _proxy_repr ( list ) \n 
~~ class ImmutableDictMixin ( object ) : \n 
~~~ return type ( self ) , ( dict ( self ) , ) \n 
~~ def setdefault ( self , key , default = None ) : \n 
~~ def update ( self , * args , ** kwargs ) : \n 
~~ def pop ( self , key , default = None ) : \n 
~~ def popitem ( self ) : \n 
~~ def __setitem__ ( self , key , value ) : \n 
~~ def clear ( self ) : \n 
~~ ~~ class ImmutableMultiDictMixin ( ImmutableDictMixin ) : \n 
~~~ return type ( self ) , ( self . items ( multi = True ) , ) \n 
~~ def add ( self , key , value ) : \n 
~~ def popitemlist ( self ) : \n 
~~ def poplist ( self , key ) : \n 
~~ def setlist ( self , key , new_list ) : \n 
~~ def setlistdefault ( self , key , default_list = None ) : \n 
~~ ~~ class UpdateDictMixin ( object ) : \n 
on_update = None \n 
def calls_update ( name ) : \n 
~~~ def oncall ( self , * args , ** kw ) : \n 
~~~ rv = getattr ( super ( UpdateDictMixin , self ) , name ) ( * args , ** kw ) \n 
if self . on_update is not None : \n 
~~~ self . on_update ( self ) \n 
~~ return rv \n 
~~ oncall . __name__ = name \n 
return oncall \n 
~~ __setitem__ = calls_update ( ) \n 
__delitem__ = calls_update ( ) \n 
clear = calls_update ( ) \n 
pop = calls_update ( ) \n 
popitem = calls_update ( ) \n 
setdefault = calls_update ( ) \n 
update = calls_update ( ) \n 
del calls_update \n 
~~ class TypeConversionDict ( dict ) : \n 
def get ( self , key , default = None , type = None ) : \n 
~~~ rv = self [ key ] \n 
if type is not None : \n 
~~~ rv = type ( rv ) \n 
~~ ~~ except ( KeyError , ValueError ) : \n 
~~~ rv = default \n 
~~ ~~ class ImmutableTypeConversionDict ( ImmutableDictMixin , TypeConversionDict ) : \n 
def copy ( self ) : \n 
return TypeConversionDict ( self ) \n 
~~ def __copy__ ( self ) : \n 
~~ ~~ class MultiDict ( TypeConversionDict ) : \n 
KeyError = None \n 
def __init__ ( self , mapping = None ) : \n 
~~~ if isinstance ( mapping , MultiDict ) : \n 
~~~ dict . __init__ ( self , ( ( k , l [ : ] ) for k , l in mapping . iterlists ( ) ) ) \n 
~~ elif isinstance ( mapping , dict ) : \n 
~~~ tmp = { } \n 
for key , value in mapping . iteritems ( ) : \n 
~~~ value = list ( value ) \n 
~~~ value = [ value ] \n 
~~ tmp [ key ] = value \n 
~~ dict . __init__ ( self , tmp ) \n 
for key , value in mapping or ( ) : \n 
~~~ tmp . setdefault ( key , [ ] ) . append ( value ) \n 
~~ ~~ def __getstate__ ( self ) : \n 
~~~ return dict ( self . lists ( ) ) \n 
~~ def __setstate__ ( self , value ) : \n 
~~~ dict . clear ( self ) \n 
dict . update ( self , value ) \n 
~~ def __iter__ ( self ) : \n 
~~~ return self . iterkeys ( ) \n 
~~ def __getitem__ ( self , key ) : \n 
if key in self : \n 
~~~ return dict . __getitem__ ( self , key ) [ 0 ] \n 
~~ raise self . KeyError ( key ) \n 
dict . __setitem__ ( self , key , [ value ] ) \n 
dict . setdefault ( self , key , [ ] ) . append ( value ) \n 
~~ def getlist ( self , key , type = None ) : \n 
~~~ rv = dict . __getitem__ ( self , key ) \n 
~~ except KeyError : \n 
~~ if type is None : \n 
~~~ return list ( rv ) \n 
for item in rv : \n 
~~~ result . append ( type ( item ) ) \n 
dict . __setitem__ ( self , key , list ( new_list ) ) \n 
if key not in self : \n 
~~~ self [ key ] = default \n 
~~~ default = self [ key ] \n 
~~ return default \n 
~~~ default_list = list ( default_list or ( ) ) \n 
dict . __setitem__ ( self , key , default_list ) \n 
~~~ default_list = dict . __getitem__ ( self , key ) \n 
~~ return default_list \n 
~~ def items ( self , multi = False ) : \n 
return list ( self . iteritems ( multi ) ) \n 
~~ def lists ( self ) : \n 
return list ( self . iterlists ( ) ) \n 
~~ def values ( self ) : \n 
return [ self [ key ] for key in self . iterkeys ( ) ] \n 
~~ def listvalues ( self ) : \n 
return list ( self . iterlistvalues ( ) ) \n 
~~ def iteritems ( self , multi = False ) : \n 
for key , values in dict . iteritems ( self ) : \n 
~~~ if multi : \n 
~~~ for value in values : \n 
~~~ yield key , values [ 0 ] \n 
~~ ~~ ~~ def iterlists ( self ) : \n 
~~~ yield key , list ( values ) \n 
~~ ~~ def itervalues ( self ) : \n 
for values in dict . itervalues ( self ) : \n 
~~~ yield values [ 0 ] \n 
~~ ~~ def iterlistvalues ( self ) : \n 
~~~ yield list ( values ) \n 
~~ ~~ def copy ( self ) : \n 
return self . __class__ ( self ) \n 
~~ def to_dict ( self , flat = True ) : \n 
if flat : \n 
~~~ return dict ( self . iteritems ( ) ) \n 
~~ return dict ( self . lists ( ) ) \n 
~~ def update ( self , other_dict ) : \n 
for key , value in iter_multi_items ( other_dict ) : \n 
~~~ MultiDict . add ( self , key , value ) \n 
~~ ~~ def pop ( self , key , default = _missing ) : \n 
~~~ return dict . pop ( self , key ) [ 0 ] \n 
~~ except KeyError , e : \n 
~~~ if default is not _missing : \n 
~~~ return default \n 
~~ raise self . KeyError ( str ( e ) ) \n 
~~ ~~ def popitem ( self ) : \n 
~~~ item = dict . popitem ( self ) \n 
return ( item [ 0 ] , item [ 1 ] [ 0 ] ) \n 
~~~ raise self . KeyError ( str ( e ) ) \n 
~~ ~~ def poplist ( self , key ) : \n 
return dict . pop ( self , key , [ ] ) \n 
~~~ return dict . popitem ( self ) \n 
~~ ~~ def __repr__ ( self ) : \n 
~~~ return % ( self . __class__ . __name__ , self . items ( multi = True ) ) \n 
~~ ~~ class _omd_bucket ( object ) : \n 
__slots__ = ( , , , ) \n 
def __init__ ( self , omd , key , value ) : \n 
~~~ self . prev = omd . _last_bucket \n 
self . key = key \n 
self . value = value \n 
self . next = None \n 
if omd . _first_bucket is None : \n 
~~~ omd . _first_bucket = self \n 
~~ if omd . _last_bucket is not None : \n 
~~~ omd . _last_bucket . next = self \n 
~~ omd . _last_bucket = self \n 
~~ def unlink ( self , omd ) : \n 
~~~ if self . prev : \n 
~~~ self . prev . next = self . next \n 
~~ if self . next : \n 
~~~ self . next . prev = self . prev \n 
~~ if omd . _first_bucket is self : \n 
~~~ omd . _first_bucket = self . next \n 
~~ if omd . _last_bucket is self : \n 
~~~ omd . _last_bucket = self . prev \n 
~~ ~~ ~~ class OrderedMultiDict ( MultiDict ) : \n 
~~~ dict . __init__ ( self ) \n 
self . _first_bucket = self . _last_bucket = None \n 
if mapping is not None : \n 
~~~ OrderedMultiDict . update ( self , mapping ) \n 
~~ ~~ def __eq__ ( self , other ) : \n 
~~~ if not isinstance ( other , MultiDict ) : \n 
~~~ return NotImplemented \n 
~~ if isinstance ( other , OrderedMultiDict ) : \n 
~~~ iter1 = self . iteritems ( multi = True ) \n 
iter2 = other . iteritems ( multi = True ) \n 
~~~ for k1 , v1 in iter1 : \n 
~~~ k2 , v2 = iter2 . next ( ) \n 
if k1 != k2 or v1 != v2 : \n 
~~ ~~ ~~ except StopIteration : \n 
~~~ iter2 . next ( ) \n 
~~ except StopIteration : \n 
~~ if len ( self ) != len ( other ) : \n 
~~ for key , values in self . iterlists ( ) : \n 
~~~ if other . getlist ( key ) != values : \n 
~~ def __ne__ ( self , other ) : \n 
~~~ return not self . __eq__ ( other ) \n 
~~ def __reduce_ex__ ( self , protocol ) : \n 
~~ def __getstate__ ( self ) : \n 
~~~ return self . items ( multi = True ) \n 
~~ def __setstate__ ( self , values ) : \n 
for key , value in values : \n 
~~~ self . add ( key , value ) \n 
~~ ~~ def __getitem__ ( self , key ) : \n 
~~~ if key in self : \n 
~~~ return dict . __getitem__ ( self , key ) [ 0 ] . value \n 
~~~ self . poplist ( key ) \n 
self . add ( key , value ) \n 
~~~ self . pop ( key ) \n 
~~ def iterkeys ( self ) : \n 
~~~ return ( key for key , value in self . iteritems ( ) ) \n 
~~ def itervalues ( self ) : \n 
~~~ return ( value for key , value in self . iteritems ( ) ) \n 
~~~ ptr = self . _first_bucket \n 
if multi : \n 
~~~ while ptr is not None : \n 
~~~ yield ptr . key , ptr . value \n 
ptr = ptr . next \n 
~~~ returned_keys = set ( ) \n 
while ptr is not None : \n 
~~~ if ptr . key not in returned_keys : \n 
~~~ returned_keys . add ( ptr . key ) \n 
yield ptr . key , ptr . value \n 
~~ ptr = ptr . next \n 
ptr = self . _first_bucket \n 
~~~ yield ptr . key , self . getlist ( ptr . key ) \n 
returned_keys . add ( ptr . key ) \n 
~~~ for key , values in self . iterlists ( ) : \n 
~~~ yield values \n 
~~ ~~ def add ( self , key , value ) : \n 
~~~ dict . setdefault ( self , key , [ ] ) . append ( _omd_bucket ( self , key , value ) ) \n 
~~~ return [ x . value for x in rv ] \n 
~~~ result . append ( type ( item . value ) ) \n 
for value in new_list : \n 
~~ ~~ def setlistdefault ( self , key , default_list = None ) : \n 
~~~ raise TypeError ( \n 
~~ def update ( self , mapping ) : \n 
~~~ for key , value in iter_multi_items ( mapping ) : \n 
~~~ OrderedMultiDict . add ( self , key , value ) \n 
~~~ buckets = dict . pop ( self , key , ( ) ) \n 
for bucket in buckets : \n 
~~~ bucket . unlink ( self ) \n 
~~ return [ x . value for x in buckets ] \n 
~~ def pop ( self , key , default = _missing ) : \n 
~~~ buckets = dict . pop ( self , key ) \n 
~~ for bucket in buckets : \n 
~~ return buckets [ 0 ] . value \n 
~~~ key , buckets = dict . popitem ( self ) \n 
~~ return key , buckets [ 0 ] . value \n 
~~ return key , [ x . value for x in buckets ] \n 
~~ ~~ def _options_header_vkw ( value , kw ) : \n 
~~~ if not kw : \n 
~~~ return value \n 
~~ return dump_options_header ( value , dict ( ( k . replace ( , ) , v ) \n 
for k , v in kw . items ( ) ) ) \n 
~~ class Headers ( object ) : \n 
def __init__ ( self , defaults = None , _list = None ) : \n 
~~~ if _list is None : \n 
~~~ _list = [ ] \n 
~~ self . _list = _list \n 
if defaults is not None : \n 
~~~ if isinstance ( defaults , ( list , Headers ) ) : \n 
~~~ self . _list . extend ( defaults ) \n 
~~~ self . extend ( defaults ) \n 
~~ ~~ ~~ @ classmethod \n 
def linked ( cls , headerlist ) : \n 
return cls ( _list = headerlist ) \n 
~~ def __getitem__ ( self , key , _index_operation = True ) : \n 
~~~ if _index_operation : \n 
~~~ if isinstance ( key , ( int , long ) ) : \n 
~~~ return self . _list [ key ] \n 
~~ elif isinstance ( key , slice ) : \n 
~~~ return self . __class__ ( self . _list [ key ] ) \n 
~~ ~~ ikey = key . lower ( ) \n 
for k , v in self . _list : \n 
~~~ if k . lower ( ) == ikey : \n 
~~~ return v \n 
~~ ~~ raise self . KeyError ( key ) \n 
~~ def __eq__ ( self , other ) : \n 
~~~ return other . __class__ is self . __class__ and set ( other . _list ) == set ( self . _list ) \n 
~~ def get ( self , key , default = None , type = None ) : \n 
~~~ rv = self . __getitem__ ( key , _index_operation = False ) \n 
~~~ return rv \n 
~~~ return type ( rv ) \n 
~~ ~~ def getlist ( self , key , type = None ) : \n 
ikey = key . lower ( ) \n 
result = [ ] \n 
for k , v in self : \n 
~~~ if type is not None : \n 
~~~ v = type ( v ) \n 
~~ ~~ result . append ( v ) \n 
~~ def get_all ( self , name ) : \n 
return self . getlist ( name ) \n 
~~ def iteritems ( self , lower = False ) : \n 
~~~ for key , value in self : \n 
~~~ if lower : \n 
~~~ key = key . lower ( ) \n 
~~ yield key , value \n 
~~ ~~ def iterkeys ( self , lower = False ) : \n 
~~~ for key , _ in self . iteritems ( lower ) : \n 
~~~ yield key \n 
~~~ for _ , value in self . iteritems ( ) : \n 
~~~ yield value \n 
~~ ~~ def keys ( self , lower = False ) : \n 
~~~ return list ( self . iterkeys ( lower ) ) \n 
~~~ return list ( self . itervalues ( ) ) \n 
~~ def items ( self , lower = False ) : \n 
~~~ return list ( self . iteritems ( lower ) ) \n 
~~ def extend ( self , iterable ) : \n 
if isinstance ( iterable , dict ) : \n 
~~~ for key , value in iterable . iteritems ( ) : \n 
~~~ for v in value : \n 
~~~ self . add ( key , v ) \n 
~~~ for key , value in iterable : \n 
~~ ~~ ~~ def __delitem__ ( self , key , _index_operation = True ) : \n 
~~~ if _index_operation and isinstance ( key , ( int , long , slice ) ) : \n 
~~~ del self . _list [ key ] \n 
~~ key = key . lower ( ) \n 
new = [ ] \n 
~~~ if k . lower ( ) != key : \n 
~~~ new . append ( ( k , v ) ) \n 
~~ ~~ self . _list [ : ] = new \n 
~~ def remove ( self , key ) : \n 
return self . __delitem__ ( key , _index_operation = False ) \n 
~~ def pop ( self , key = None , default = _missing ) : \n 
if key is None : \n 
~~~ return self . _list . pop ( ) \n 
~~ if isinstance ( key , ( int , long ) ) : \n 
~~~ return self . _list . pop ( key ) \n 
self . remove ( key ) \n 
~~ raise \n 
return self . pop ( ) \n 
~~ def __contains__ ( self , key ) : \n 
~~~ self . __getitem__ ( key , _index_operation = False ) \n 
~~ has_key = __contains__ \n 
def __iter__ ( self ) : \n 
return iter ( self . _list ) \n 
~~ def __len__ ( self ) : \n 
~~~ return len ( self . _list ) \n 
~~ def add ( self , _key , _value , ** kw ) : \n 
self . _list . append ( ( _key , _options_header_vkw ( _value , kw ) ) ) \n 
~~ def add_header ( self , _key , _value , ** _kw ) : \n 
self . add ( _key , _value , ** _kw ) \n 
del self . _list [ : ] \n 
~~ def set ( self , _key , _value , ** kw ) : \n 
lc_key = _key . lower ( ) \n 
_value = _options_header_vkw ( _value , kw ) \n 
for idx , ( old_key , old_value ) in enumerate ( self . _list ) : \n 
~~~ if old_key . lower ( ) == lc_key : \n 
~~~ self . _list [ idx ] = ( _key , _value ) \n 
~~~ return self . add ( _key , _value ) \n 
~~ self . _list [ idx + 1 : ] = [ ( k , v ) for k , v in self . _list [ idx + 1 : ] \n 
if k . lower ( ) != lc_key ] \n 
~~ def setdefault ( self , key , value ) : \n 
~~~ return self [ key ] \n 
~~ self . set ( key , value ) \n 
return value \n 
if isinstance ( key , ( slice , int , long ) ) : \n 
~~~ self . _list [ key ] = value \n 
~~~ self . set ( key , value ) \n 
~~ ~~ def to_list ( self , charset = ) : \n 
~~~ if isinstance ( v , unicode ) : \n 
~~~ v = v . encode ( charset ) \n 
~~~ v = str ( v ) \n 
~~ result . append ( ( k , v ) ) \n 
~~ def copy ( self ) : \n 
~~~ return self . __class__ ( self . _list ) \n 
~~~ return self . copy ( ) \n 
~~ def __str__ ( self , charset = ) : \n 
strs = [ ] \n 
for key , value in self . to_list ( charset ) : \n 
~~~ strs . append ( % ( key , value ) ) \n 
~~ strs . append ( ) \n 
return . join ( strs ) \n 
~~ def __repr__ ( self ) : \n 
~~~ return % ( \n 
self . __class__ . __name__ , \n 
list ( self ) \n 
~~ ~~ class ImmutableHeadersMixin ( object ) : \n 
def __delitem__ ( self , key ) : \n 
~~ set = __setitem__ \n 
def add ( self , item ) : \n 
~~ remove = add_header = add \n 
~~ def setdefault ( self , key , default ) : \n 
~~ ~~ class EnvironHeaders ( ImmutableHeadersMixin , Headers ) : \n 
def __init__ ( self , environ ) : \n 
~~~ self . environ = environ \n 
def linked ( cls , environ ) : \n 
% cls . __name__ ) \n 
~~~ return self . environ is other . environ \n 
~~ def __getitem__ ( self , key , _index_operation = False ) : \n 
~~~ key = key . upper ( ) . replace ( , ) \n 
if key in ( , ) : \n 
~~~ return self . environ [ key ] \n 
~~ return self . environ [ + key ] \n 
~~~ return len ( list ( iter ( self ) ) ) \n 
~~~ for key , value in self . environ . iteritems ( ) : \n 
~~~ if key . startswith ( ) and key not in ( , ) : \n 
~~~ yield key [ 5 : ] . replace ( , ) . title ( ) , value \n 
~~ elif key in ( , ) : \n 
~~~ yield key . replace ( , ) . title ( ) , value \n 
~~ ~~ ~~ def copy ( self ) : \n 
~~ ~~ class CombinedMultiDict ( ImmutableMultiDictMixin , MultiDict ) : \n 
~~~ return type ( self ) , ( self . dicts , ) \n 
~~ def __init__ ( self , dicts = None ) : \n 
~~~ self . dicts = dicts or [ ] \n 
def fromkeys ( cls ) : \n 
~~~ raise TypeError ( % \n 
cls . __name__ ) \n 
~~~ for d in self . dicts : \n 
~~~ if key in d : \n 
~~~ return d [ key ] \n 
~~~ return type ( d [ key ] ) \n 
~~ ~~ return d [ key ] \n 
~~ ~~ return default \n 
~~~ rv = [ ] \n 
for d in self . dicts : \n 
~~~ rv . extend ( d . getlist ( key , type ) ) \n 
~~ def keys ( self ) : \n 
~~~ rv = set ( ) \n 
~~~ rv . update ( d . keys ( ) ) \n 
~~ return list ( rv ) \n 
~~~ found = set ( ) \n 
~~~ for key , value in d . iteritems ( multi ) : \n 
~~ elif key not in found : \n 
~~~ found . add ( key ) \n 
yield key , value \n 
~~ ~~ ~~ ~~ def itervalues ( self ) : \n 
~~~ for key , value in self . iteritems ( ) : \n 
~~ ~~ def values ( self ) : \n 
~~~ return list ( self . iteritems ( multi ) ) \n 
~~ def iterlists ( self ) : \n 
~~~ rv = { } \n 
~~~ for key , values in d . iterlists ( ) : \n 
~~~ rv . setdefault ( key , [ ] ) . extend ( values ) \n 
~~ ~~ return rv . iteritems ( ) \n 
~~~ return list ( self . iterlists ( ) ) \n 
~~ def iterlistvalues ( self ) : \n 
~~~ return ( x [ 0 ] for x in self . lists ( ) ) \n 
~~~ return list ( self . iterlistvalues ( ) ) \n 
~~~ return iter ( self . keys ( ) ) \n 
~~ __iter__ = iterkeys \n 
return self . __class__ ( self . dicts [ : ] ) \n 
rv = { } \n 
for d in reversed ( self . dicts ) : \n 
~~~ rv . update ( d . to_dict ( flat ) ) \n 
~~~ return len ( self . keys ( ) ) \n 
def __repr__ ( self ) : \n 
~~~ return % ( self . __class__ . __name__ , self . dicts ) \n 
~~ ~~ class FileMultiDict ( MultiDict ) : \n 
def add_file ( self , name , file , filename = None , content_type = None ) : \n 
if isinstance ( file , FileStorage ) : \n 
~~~ self [ name ] = file \n 
~~ if isinstance ( file , basestring ) : \n 
~~~ if filename is None : \n 
~~~ filename = file \n 
~~ file = open ( file , ) \n 
~~ if filename and content_type is None : \n 
~~~ content_type = mimetypes . guess_type ( filename ) [ 0 ] or \n 
~~ self [ name ] = FileStorage ( file , filename , name , content_type ) \n 
~~ ~~ class ImmutableDict ( ImmutableDictMixin , dict ) : \n 
__repr__ = _proxy_repr ( dict ) \n 
return dict ( self ) \n 
~~ ~~ class ImmutableMultiDict ( ImmutableMultiDictMixin , MultiDict ) : \n 
return MultiDict ( self ) \n 
~~ ~~ class ImmutableOrderedMultiDict ( ImmutableMultiDictMixin , OrderedMultiDict ) : \n 
return OrderedMultiDict ( self ) \n 
~~ ~~ class Accept ( ImmutableList ) : \n 
def __init__ ( self , values = ( ) ) : \n 
~~~ if values is None : \n 
~~~ list . __init__ ( self ) \n 
self . provided = False \n 
~~ elif isinstance ( values , Accept ) : \n 
~~~ self . provided = values . provided \n 
list . __init__ ( self , values ) \n 
~~~ self . provided = True \n 
values = [ ( a , b ) for b , a in values ] \n 
values . sort ( ) \n 
values . reverse ( ) \n 
list . __init__ ( self , [ ( a , b ) for b , a in values ] ) \n 
~~ ~~ def _value_matches ( self , value , item ) : \n 
return item == or item . lower ( ) == value . lower ( ) \n 
if isinstance ( key , basestring ) : \n 
~~~ return self . quality ( key ) \n 
~~ return list . __getitem__ ( self , key ) \n 
~~ def quality ( self , key ) : \n 
for item , quality in self : \n 
~~~ if self . _value_matches ( key , item ) : \n 
~~~ return quality \n 
~~ ~~ return 0 \n 
~~ def __contains__ ( self , value ) : \n 
~~~ for item , quality in self : \n 
~~~ if self . _value_matches ( value , item ) : \n 
. join ( % ( x , y ) for x , y in self ) \n 
~~ def index ( self , key ) : \n 
~~~ for idx , ( item , quality ) in enumerate ( self ) : \n 
~~~ return idx \n 
~~ ~~ raise ValueError ( key ) \n 
~~ return list . index ( self , key ) \n 
~~ def find ( self , key ) : \n 
~~~ return self . index ( key ) \n 
return list ( self . itervalues ( ) ) \n 
for item in self : \n 
~~~ yield item [ 0 ] \n 
~~ ~~ def to_header ( self ) : \n 
for value , quality in self : \n 
~~~ if quality != 1 : \n 
~~~ value = % ( value , quality ) \n 
~~ result . append ( value ) \n 
~~ return . join ( result ) \n 
~~ def __str__ ( self ) : \n 
~~~ return self . to_header ( ) \n 
~~ def best_match ( self , matches , default = None ) : \n 
best_quality = - 1 \n 
result = default \n 
for server_item in matches : \n 
~~~ for client_item , quality in self : \n 
~~~ if quality <= best_quality : \n 
~~ if self . _value_matches ( client_item , server_item ) : \n 
~~~ best_quality = quality \n 
result = server_item \n 
def best ( self ) : \n 
if self : \n 
~~~ return self [ 0 ] [ 0 ] \n 
~~ ~~ ~~ class MIMEAccept ( Accept ) : \n 
def _value_matches ( self , value , item ) : \n 
~~~ def _normalize ( x ) : \n 
~~~ x = x . lower ( ) \n 
return x == and ( , ) or x . split ( , 1 ) \n 
~~ if not in value : \n 
~~~ raise ValueError ( % value ) \n 
~~ value_type , value_subtype = _normalize ( value ) \n 
if value_type == and value_subtype != : \n 
~~ if not in item : \n 
~~ item_type , item_subtype = _normalize ( item ) \n 
if item_type == and item_subtype != : \n 
~~ return ( \n 
( item_type == item_subtype == or \n 
value_type == value_subtype == ) or \n 
( item_type == value_type and ( item_subtype == or \n 
value_subtype == or \n 
item_subtype == value_subtype ) ) \n 
def accept_html ( self ) : \n 
return ( \n 
in self or \n 
self . accept_xhtml \n 
def accept_xhtml ( self ) : \n 
in self \n 
~~ ~~ class LanguageAccept ( Accept ) : \n 
~~~ def _normalize ( language ) : \n 
~~~ return _locale_delim_re . split ( language . lower ( ) ) \n 
~~ return item == or _normalize ( value ) == _normalize ( item ) \n 
~~ ~~ class CharsetAccept ( Accept ) : \n 
~~~ def _normalize ( name ) : \n 
~~~ return codecs . lookup ( name ) . name \n 
~~ except LookupError : \n 
~~~ return name . lower ( ) \n 
~~ ~~ return item == or _normalize ( value ) == _normalize ( item ) \n 
~~ ~~ def cache_property ( key , empty , type ) : \n 
return property ( lambda x : x . _get_cache_value ( key , empty , type ) , \n 
lambda x , v : x . _set_cache_value ( key , v , type ) , \n 
lambda x : x . _del_cache_value ( key ) , \n 
% key ) \n 
~~ class _CacheControl ( UpdateDictMixin , dict ) : \n 
no_cache = cache_property ( , , None ) \n 
no_store = cache_property ( , None , bool ) \n 
max_age = cache_property ( , - 1 , int ) \n 
no_transform = cache_property ( , None , None ) \n 
def __init__ ( self , values = ( ) , on_update = None ) : \n 
~~~ dict . __init__ ( self , values or ( ) ) \n 
self . on_update = on_update \n 
self . provided = values is not None \n 
~~ def _get_cache_value ( self , key , empty , type ) : \n 
if type is bool : \n 
~~~ return key in self \n 
~~ if key in self : \n 
~~~ value = self [ key ] \n 
if value is None : \n 
~~~ return empty \n 
~~ elif type is not None : \n 
~~~ value = type ( value ) \n 
~~ ~~ return value \n 
~~ ~~ def _set_cache_value ( self , key , value , type ) : \n 
~~~ if value : \n 
~~~ self [ key ] = None \n 
~~~ self . pop ( key , None ) \n 
~~~ if value is None : \n 
~~ elif value is True : \n 
~~~ self [ key ] = value \n 
~~ ~~ ~~ def _del_cache_value ( self , key ) : \n 
~~~ del self [ key ] \n 
return dump_header ( self ) \n 
self . to_header ( ) \n 
~~ ~~ class RequestCacheControl ( ImmutableDictMixin , _CacheControl ) : \n 
max_stale = cache_property ( , , int ) \n 
min_fresh = cache_property ( , , int ) \n 
only_if_cached = cache_property ( , None , bool ) \n 
~~ class ResponseCacheControl ( _CacheControl ) : \n 
public = cache_property ( , None , bool ) \n 
private = cache_property ( , , None ) \n 
must_revalidate = cache_property ( , None , bool ) \n 
proxy_revalidate = cache_property ( , None , bool ) \n 
s_maxage = cache_property ( , None , None ) \n 
~~ _CacheControl . cache_property = staticmethod ( cache_property ) \n 
class CallbackDict ( UpdateDictMixin , dict ) : \n 
def __init__ ( self , initial = None , on_update = None ) : \n 
~~~ dict . __init__ ( self , initial or ( ) ) \n 
dict . __repr__ ( self ) \n 
~~ ~~ class HeaderSet ( object ) : \n 
def __init__ ( self , headers = None , on_update = None ) : \n 
~~~ self . _headers = list ( headers or ( ) ) \n 
self . _set = set ( [ x . lower ( ) for x in self . _headers ] ) \n 
~~ def add ( self , header ) : \n 
self . update ( ( header , ) ) \n 
~~ def remove ( self , header ) : \n 
key = header . lower ( ) \n 
if key not in self . _set : \n 
~~~ raise KeyError ( header ) \n 
~~ self . _set . remove ( key ) \n 
for idx , key in enumerate ( self . _headers ) : \n 
~~~ if key . lower ( ) == header : \n 
~~~ del self . _headers [ idx ] \n 
~~ ~~ if self . on_update is not None : \n 
~~ ~~ def update ( self , iterable ) : \n 
inserted_any = False \n 
for header in iterable : \n 
~~~ key = header . lower ( ) \n 
~~~ self . _headers . append ( header ) \n 
self . _set . add ( key ) \n 
inserted_any = True \n 
~~ ~~ if inserted_any and self . on_update is not None : \n 
~~ ~~ def discard ( self , header ) : \n 
~~~ return self . remove ( header ) \n 
~~ ~~ def find ( self , header ) : \n 
header = header . lower ( ) \n 
for idx , item in enumerate ( self . _headers ) : \n 
~~~ if item . lower ( ) == header : \n 
~~ ~~ return - 1 \n 
~~ def index ( self , header ) : \n 
rv = self . find ( header ) \n 
if rv < 0 : \n 
~~~ raise IndexError ( header ) \n 
self . _set . clear ( ) \n 
del self . _headers [ : ] \n 
~~ ~~ def as_set ( self , preserve_casing = False ) : \n 
if preserve_casing : \n 
~~~ return set ( self . _headers ) \n 
~~ return set ( self . _set ) \n 
~~ def to_header ( self ) : \n 
return . join ( map ( quote_header_value , self . _headers ) ) \n 
~~ def __getitem__ ( self , idx ) : \n 
~~~ return self . _headers [ idx ] \n 
~~ def __delitem__ ( self , idx ) : \n 
~~~ rv = self . _headers . pop ( idx ) \n 
self . _set . remove ( rv . lower ( ) ) \n 
~~ ~~ def __setitem__ ( self , idx , value ) : \n 
~~~ old = self . _headers [ idx ] \n 
self . _set . remove ( old . lower ( ) ) \n 
self . _headers [ idx ] = value \n 
self . _set . add ( value . lower ( ) ) \n 
~~ ~~ def __contains__ ( self , header ) : \n 
~~~ return header . lower ( ) in self . _set \n 
~~~ return len ( self . _set ) \n 
~~~ return iter ( self . _headers ) \n 
~~ def __nonzero__ ( self ) : \n 
~~~ return bool ( self . _set ) \n 
self . _headers \n 
~~ ~~ class ETags ( object ) : \n 
def __init__ ( self , strong_etags = None , weak_etags = None , star_tag = False ) : \n 
~~~ self . _strong = frozenset ( not star_tag and strong_etags or ( ) ) \n 
self . _weak = frozenset ( weak_etags or ( ) ) \n 
self . star_tag = star_tag \n 
~~ def as_set ( self , include_weak = False ) : \n 
rv = set ( self . _strong ) \n 
if include_weak : \n 
~~~ rv . update ( self . _weak ) \n 
~~ def is_weak ( self , etag ) : \n 
return etag in self . _weak \n 
~~ def contains_weak ( self , etag ) : \n 
return self . is_weak ( etag ) or self . contains ( etag ) \n 
~~ def contains ( self , etag ) : \n 
if self . star_tag : \n 
~~ return etag in self . _strong \n 
~~ def contains_raw ( self , etag ) : \n 
etag , weak = unquote_etag ( etag ) \n 
if weak : \n 
~~~ return self . contains_weak ( etag ) \n 
~~ return self . contains ( etag ) \n 
~~ return . join ( \n 
[ \'"%s"\' % x for x in self . _strong ] + \n 
[ \'w/"%s"\' % x for x in self . _weak ] \n 
~~ def __call__ ( self , etag = None , data = None , include_weak = False ) : \n 
~~~ if [ etag , data ] . count ( None ) != 1 : \n 
~~~ raise TypeError ( ) \n 
~~ if etag is None : \n 
~~~ etag = generate_etag ( data ) \n 
~~ if include_weak : \n 
~~~ if etag in self . _weak : \n 
~~ ~~ return etag in self . _strong \n 
~~~ return bool ( self . star_tag or self . _strong ) \n 
~~~ return iter ( self . _strong ) \n 
~~ def __contains__ ( self , etag ) : \n 
~~~ return self . contains ( etag ) \n 
~~~ return % ( self . __class__ . __name__ , str ( self ) ) \n 
~~ ~~ class Authorization ( ImmutableDictMixin , dict ) : \n 
def __init__ ( self , auth_type , data = None ) : \n 
~~~ dict . __init__ ( self , data or { } ) \n 
self . type = auth_type \n 
~~ username = property ( lambda x : x . get ( ) , doc = ) \n 
password = property ( lambda x : x . get ( ) , doc = ) \n 
realm = property ( lambda x : x . get ( ) , doc = ) \n 
nonce = property ( lambda x : x . get ( ) , doc = ) \n 
uri = property ( lambda x : x . get ( ) , doc = ) \n 
nc = property ( lambda x : x . get ( ) , doc = ) \n 
cnonce = property ( lambda x : x . get ( ) , doc = ) \n 
response = property ( lambda x : x . get ( ) , doc = ) \n 
opaque = property ( lambda x : x . get ( ) , doc = ) \n 
def qop ( self ) : \n 
def on_update ( header_set ) : \n 
~~~ if not header_set and in self : \n 
~~~ del self [ ] \n 
~~ elif header_set : \n 
~~~ self [ ] = header_set . to_header ( ) \n 
~~ ~~ return parse_set_header ( self . get ( ) , on_update ) \n 
~~ ~~ class WWWAuthenticate ( UpdateDictMixin , dict ) : \n 
_require_quoting = frozenset ( [ , , , ] ) \n 
def __init__ ( self , auth_type = None , values = None , on_update = None ) : \n 
if auth_type : \n 
~~~ self [ ] = auth_type \n 
~~ self . on_update = on_update \n 
~~ def set_basic ( self , realm = ) : \n 
dict . clear ( self ) \n 
dict . update ( self , { : , : realm } ) \n 
if self . on_update : \n 
~~ ~~ def set_digest ( self , realm , nonce , qop = ( , ) , opaque = None , \n 
algorithm = None , stale = False ) : \n 
d = { \n 
: realm , \n 
: nonce , \n 
: dump_header ( qop ) \n 
if stale : \n 
~~~ d [ ] = \n 
~~ if opaque is not None : \n 
~~~ d [ ] = opaque \n 
~~ if algorithm is not None : \n 
~~~ d [ ] = algorithm \n 
~~ dict . clear ( self ) \n 
dict . update ( self , d ) \n 
d = dict ( self ) \n 
auth_type = d . pop ( , None ) or \n 
return % ( auth_type . title ( ) , . join ( [ \n 
% ( key , quote_header_value ( value , \n 
allow_token = key not in self . _require_quoting ) ) \n 
for key , value in d . iteritems ( ) \n 
] ) ) \n 
~~ def auth_property ( name , doc = None ) : \n 
def _set_value ( self , value ) : \n 
~~~ self . pop ( name , None ) \n 
~~~ self [ name ] = str ( value ) \n 
~~ ~~ return property ( lambda x : x . get ( name ) , _set_value , doc = doc ) \n 
~~ def _set_property ( name , doc = None ) : \n 
~~~ def fget ( self ) : \n 
~~~ def on_update ( header_set ) : \n 
~~~ if not header_set and name in self : \n 
~~~ del self [ name ] \n 
~~~ self [ name ] = header_set . to_header ( ) \n 
~~ ~~ return parse_set_header ( self . get ( name ) , on_update ) \n 
~~ return property ( fget , doc = doc ) \n 
~~ type = auth_property ( , doc = ) \n 
realm = auth_property ( , doc = ) \n 
domain = _set_property ( , doc = ) \n 
nonce = auth_property ( , doc = ) \n 
opaque = auth_property ( , doc = ) \n 
qop = _set_property ( , doc = ) \n 
def _get_stale ( self ) : \n 
~~~ val = self . get ( ) \n 
if val is not None : \n 
~~~ return val . lower ( ) == \n 
~~ ~~ def _set_stale ( self , value ) : \n 
~~~ self . pop ( , None ) \n 
~~~ self [ ] = value and or \n 
~~ ~~ stale = property ( _get_stale , _set_stale , doc = ) \n 
del _get_stale , _set_stale \n 
auth_property = staticmethod ( auth_property ) \n 
del _set_property \n 
~~ class FileStorage ( object ) : \n 
def __init__ ( self , stream = None , filename = None , name = None , \n 
content_type = , content_length = - 1 , \n 
headers = None ) : \n 
~~~ self . name = name \n 
self . stream = stream or _empty_stream \n 
self . filename = filename or getattr ( stream , , None ) \n 
self . content_type = content_type \n 
self . content_length = content_length \n 
if headers is None : \n 
~~~ headers = Headers ( ) \n 
~~ self . headers = headers \n 
~~ def save ( self , dst , buffer_size = 16384 ) : \n 
from shutil import copyfileobj \n 
close_dst = False \n 
if isinstance ( dst , basestring ) : \n 
~~~ dst = file ( dst , ) \n 
close_dst = True \n 
~~~ copyfileobj ( self . stream , dst , buffer_size ) \n 
~~~ if close_dst : \n 
~~~ dst . close ( ) \n 
~~ ~~ ~~ def close ( self ) : \n 
~~~ self . stream . close ( ) \n 
~~ ~~ def __nonzero__ ( self ) : \n 
~~~ return bool ( self . filename ) \n 
~~ def __getattr__ ( self , name ) : \n 
~~~ return getattr ( self . stream , name ) \n 
~~~ return iter ( self . readline , ) \n 
self . filename , \n 
self . content_type \n 
~~ ~~ from werkzeug . http import dump_options_header , dump_header , generate_etag , quote_header_value , parse_set_header , unquote_etag \n 
from werkzeug . exceptions import BadRequest \n 
for _cls in MultiDict , OrderedMultiDict , CombinedMultiDict , Headers , EnvironHeaders : \n 
~~~ _cls . KeyError = BadRequest . wrap ( KeyError , _cls . __name__ + ) \n 
~~ del _cls \n 
from werkzeug . utils import import_string \n 
from kay . management . shell import ( \n 
rshell , shell , clear_datastore , create_user , \n 
from kay . management . runserver import runserver_passthru_argv \n 
from kay . management . startapp import startapp \n 
from kay . management . startapp import startproject \n 
from kay . management . appcfg import do_appcfg_passthru_argv \n 
from kay . management . bulkloader import ( \n 
do_bulkloader_passthru_argv , dump_all , restore_all , \n 
from kay . management . test import do_runtest \n 
from kay . management . preparse import do_preparse_bundle \n 
from kay . management . preparse import do_preparse_apps \n 
from kay . management . extract_messages import do_extract_messages \n 
from kay . management . add_translations import do_add_translations \n 
from kay . management . update_translations import do_update_translations \n 
from kay . management . compile_translations import do_compile_translations \n 
from kay . management . wxadmin import do_wxadmin \n 
from kay . management . compile_media import do_compile_media \n 
from kay . conf import settings \n 
action_dump_all = dump_all \n 
action_restore_all = restore_all \n 
action_shell = shell \n 
action_rshell = rshell \n 
action_startapp = startapp \n 
action_startproject = startproject \n 
action_test = do_runtest \n 
action_preparse_bundle = do_preparse_bundle \n 
action_preparse_apps = do_preparse_apps \n 
action_extract_messages = do_extract_messages \n 
action_add_translations = do_add_translations \n 
action_update_translations = do_update_translations \n 
action_compile_translations = do_compile_translations \n 
action_appcfg = do_appcfg_passthru_argv \n 
action_runserver = runserver_passthru_argv \n 
action_bulkloader = do_bulkloader_passthru_argv \n 
action_clear_datastore = clear_datastore \n 
action_create_user = create_user \n 
action_wxadmin = do_wxadmin \n 
action_compile_media = do_compile_media \n 
additional_actions = [ ] \n 
for app in settings . INSTALLED_APPS : \n 
~~~ appmod = import_string ( app ) \n 
if not os . path . exists ( os . path . join ( os . path . dirname ( appmod . __file__ ) , \n 
) ) : \n 
~~ management_mod = import_string ( "%s.management" % app ) \n 
for name , val in vars ( management_mod ) . iteritems ( ) : \n 
~~~ if name . startswith ( "action_" ) : \n 
~~~ locals ( ) [ name ] = getattr ( management_mod , name ) \n 
additional_actions . append ( name ) \n 
~~ ~~ ~~ except Exception , e : \n 
sys . stderr . write ( . join ( traceback . format_exception ( * ( sys . exc_info ( ) ) ) ) ) \n 
~~ ~~ __all__ = [ \n 
, , , , \n 
, , , , , \n 
, , \n 
] + additional_actions \n 
def print_status ( msg ) : \n 
~~~ print ( msg ) \n 
sys . stdout . flush ( ) \n 
from kay . routing import ( \n 
ViewGroup , Rule \n 
view_groups = [ \n 
ViewGroup ( \n 
Rule ( , endpoint = , \n 
view = ( , ( ) , { } ) ) , \n 
view = ) , \n 
from google . appengine . ext import db \n 
from kay . utils . forms import ValidationError \n 
from kay . utils . forms . modelform import ModelForm \n 
class MaxLengthValidator ( object ) : \n 
~~~ def __init__ ( self , length ) : \n 
~~~ self . length = length \n 
~~ def __call__ ( self , val ) : \n 
~~~ if len ( val ) > self . length : \n 
~~ ~~ class TestModel ( db . Model ) : \n 
~~~ number = db . IntegerProperty ( required = True ) \n 
data_field = db . StringProperty ( required = True , \n 
validator = MaxLengthValidator ( 20 ) ) \n 
is_active = db . BooleanProperty ( required = True ) \n 
string_list_field = db . StringListProperty ( required = True ) \n 
~~ class TestModel2 ( db . Model ) : \n 
~~ class TestModelForm ( ModelForm ) : \n 
~~~ csrf_protected = False \n 
class Meta ( ) : \n 
~~~ model = TestModel \n 
~~ def __init__ ( self , instance = None , initial = None ) : \n 
~~~ super ( TestModelForm , self ) . __init__ ( instance , initial ) \n 
self . string_list_field . min_size = 1 \n 
~~ ~~ class JsonTestModel ( db . Model ) : \n 
~~~ s = db . StringProperty ( ) \n 
i = db . IntegerProperty ( ) \n 
b = db . BooleanProperty ( ) \n 
l = db . StringListProperty ( ) \n 
r = db . ReferenceProperty ( ) \n 
~~ class ModelFormTestModel ( db . Model ) : \n 
~~~ s_name = db . StringProperty ( ) \n 
zip_code = db . StringProperty ( ) \n 
addr = db . StringProperty ( ) \n 
~~ class ModelFormTestForm ( ModelForm ) : \n 
class Meta : \n 
~~~ model = ModelFormTestModel \n 
fields = ( ) \n 
~~ ~~ class ValidationTestModel ( db . Model ) : \n 
~~~ slist = db . StringListProperty ( ) \n 
~~ class ValidationTestForm ( ModelForm ) : \n 
~~~ model = ValidationTestModel \n 
~~ def context_validate ( self , data ) : \n 
~~~ raise ValidationError ( "Error!" ) \n 
from os import path , listdir , mkdir \n 
def compile_file ( env , src_path , dst_path , encoding = , base_dir = ) : \n 
src_file = file ( src_path , ) \n 
~~~ source = src_file . read ( ) . decode ( encoding ) \n 
raise \n 
~~ src_file . close ( ) \n 
name = src_path . replace ( base_dir , ) \n 
raw = env . compile ( source , name = name , filename = name , raw = True ) \n 
dst_file = open ( dst_path , ) \n 
dst_file . write ( raw ) \n 
dst_file . close ( ) \n 
~~ def compile_dir ( env , src_path , dst_path , pattern = , \n 
encoding = , base_dir = None , \n 
negative_pattern = ) : \n 
if base_dir is None : \n 
~~~ base_dir = src_path \n 
~~ for filename in listdir ( src_path ) : \n 
~~~ if filename . startswith ( "." ) : \n 
~~ src_name = path . join ( src_path , filename ) \n 
dst_name = path . join ( dst_path , filename ) \n 
if path . isdir ( src_name ) : \n 
~~~ if not path . isdir ( dst_name ) : \n 
~~~ mkdir ( dst_name ) \n 
~~ compile_dir ( env , src_name , dst_name , encoding = encoding , \n 
base_dir = base_dir ) \n 
~~ elif path . isfile ( src_name ) and re . match ( pattern , filename ) and not re . match ( negative_pattern , filename ) : \n 
~~~ compile_file ( env , src_name , dst_name , encoding = encoding , \n 
import api \n 
import random \n 
import imp \n 
import shutil \n 
from functools import partial \n 
from bson import json_util \n 
from api . common import InternalException , SevereInternalException \n 
log = api . logger . use ( __name__ ) \n 
modifiable_problem_fields = [ "description" ] \n 
seed = "" \n 
def is_autogen_problem ( pid ) : \n 
return api . problem . get_problem ( pid = pid ) . get ( "autogen" , False ) \n 
~~ def get_metadata_path ( pid , n ) : \n 
return path . join ( get_instance_path ( pid , n = n , public = False ) , "metadata.json" ) \n 
~~ def write_metadata ( pid , n , data ) : \n 
metadata_path = get_metadata_path ( pid , n ) \n 
with open ( metadata_path , "w" ) as f : \n 
~~~ f . write ( json_util . dumps ( data ) ) \n 
~~ ~~ @ api . cache . memoize ( timeout = 120 , fast = True ) \n 
def read_metadata ( pid , n ) : \n 
with open ( metadata_path , "r" ) as f : \n 
~~~ return json_util . loads ( f . read ( ) ) \n 
~~ ~~ def build_problem_instances ( pid , instances ) : \n 
problem = api . problem . get_problem ( pid = pid ) \n 
if not is_autogen_problem ( pid ) : \n 
~~ previous_state = seed_generator ( "INIT" , pid ) \n 
instance_path , static_instance_path = get_instance_path ( pid ) , get_static_instance_path ( pid ) \n 
for autogen_path in [ instance_path , static_instance_path ] : \n 
if not path . isdir ( autogen_path ) : \n 
os . makedirs ( autogen_path ) \n 
~~ ~~ for n in range ( instances ) : \n 
build = get_generator ( pid ) . generate ( random , pid , api . autogen_tools , n ) \n 
autogen_instance_path = get_instance_path ( pid , n = n ) \n 
file_type_paths = { \n 
"resource_files" : { \n 
"public" : get_instance_path ( pid , n = n , public = True ) , \n 
"private" : get_instance_path ( pid , n = n , public = False ) \n 
} , \n 
"static_files" : { \n 
"public" : get_static_instance_path ( pid , public = True ) , \n 
"private" : get_static_instance_path ( pid , public = False ) \n 
for _ , file_types in file_type_paths . items ( ) : \n 
~~~ for _ , autogen_path in file_types . items ( ) : \n 
~~~ if not path . isdir ( autogen_path ) : \n 
~~~ os . makedirs ( autogen_path ) \n 
~~ ~~ ~~ problem_updates = build . get ( "problem_updates" , None ) \n 
if problem_updates is None : \n 
~~ write_metadata ( pid , n , problem_updates ) \n 
for file_type , listings in build . items ( ) : \n 
~~~ destination_type = file_type_paths . get ( file_type , None ) \n 
if destination_type is not None : \n 
~~~ for listing in listings : \n 
~~~ destination = destination_type . get ( listing , None ) \n 
if destination is not None : \n 
~~~ files = listings [ listing ] \n 
for f , name in files : \n 
~~~ if path . isfile ( f ) : \n 
~~~ shutil . copyfile ( f , path . join ( destination , name ) ) \n 
~~ elif path . isdir ( f ) : \n 
~~~ shutil . copytree ( f , autogen_instance_path ) \n 
~~ ~~ ~~ ~~ ~~ ~~ api . autogen_tools . clear_build_directories ( ) \n 
log . debug ( "done!" ) \n 
~~ random . setstate ( previous_state ) \n 
~~ def get_generator_path ( pid ) : \n 
~~ if not problem . get ( "generator" , False ) : \n 
~~ return path . join ( api . problem . grader_base_path , problem [ "generator" ] ) \n 
~~ def get_generator ( pid ) : \n 
generator_path = get_generator_path ( pid ) \n 
if not path . isfile ( generator_path ) : \n 
~~ return imp . load_source ( generator_path [ : - 3 ] , generator_path ) \n 
~~ def get_seed ( pid , tid ) : \n 
return seed + tid + pid \n 
~~ def seed_generator ( pid , tid ) : \n 
previous_state = random . getstate ( ) \n 
random . seed ( get_seed ( pid , tid ) ) \n 
return previous_state \n 
~~ @ api . cache . memoize ( timeout = 120 , fast = True ) \n 
def get_instance_number ( pid , tid ) : \n 
previous_state = seed_generator ( tid , pid ) \n 
total_instances = get_number_of_instances ( pid ) \n 
if total_instances == 0 : \n 
~~ instance_number = random . randint ( 0 , total_instances - 1 ) \n 
random . setstate ( previous_state ) \n 
return instance_number \n 
def get_number_of_instances ( pid ) : \n 
~~~ return [ dirname . isdigit ( ) for dirname in os . listdir ( get_instance_path ( pid , public = False ) ) ] . count ~~ except FileNotFoundError : \n 
~~ ~~ def get_static_instance_path ( pid , public = True ) : \n 
return path . abspath ( path . join ( get_instance_path ( pid , public = public ) , "static" ) ) \n 
~~ def get_instance_path ( pid , n = "" , public = True ) : \n 
name = api . problem . get_problem ( pid ) [ "name" ] \n 
instance_path = path . join ( path . dirname ( generator_path ) , "instances" , name , str ( n ) ) \n 
if public : \n 
~~~ instance_path = path . join ( instance_path , "public" ) \n 
~~ return path . abspath ( instance_path ) \n 
def get_problem_instance ( pid , tid ) : \n 
n = get_instance_number ( pid , tid ) \n 
metadata = read_metadata ( pid , n ) \n 
if not set ( metadata ) . issubset ( modifiable_problem_fields ) : \n 
~~~ invalid_keys = set ( metadata ) . difference ( modifiable_problem_fields ) \n 
~~ problem . update ( metadata ) \n 
return problem \n 
~~ def grade_problem_instance ( pid , tid , key ) : \n 
~~ problem = api . problem . get_problem ( pid ) \n 
grader_problem_instance = GraderProblemInstance ( pid , tid , n ) \n 
grader = api . problem . get_grader ( pid ) \n 
~~~ correct , message = grader . grade ( grader_problem_instance , key ) \n 
~~ return { \n 
"correct" : correct , \n 
"points" : problem [ "score" ] , \n 
"message" : message \n 
~~ class GraderProblemInstance ( object ) : \n 
def __init__ ( self , pid , tid , n ) : \n 
~~~ self . instance = n \n 
self . get_instance_path = partial ( get_instance_path , pid , n = n ) \n 
self . seed_generator = partial ( seed_generator , pid , tid ) \n 
self . write_metadata = partial ( write_metadata , pid , n ) \n 
self . read_metadata = partial ( read_metadata , pid ) \n 
def generate ( random , pid , tools , n ) : \n 
f = open ( "/tmp/key" , "w" ) \n 
k = str ( random . randint ( 0 , 1000 ) ) \n 
f . write ( k ) \n 
f . close ( ) \n 
return { \n 
"public" : [ ( "/tmp/key" , "public_key" ) ] , \n 
"private" : [ ( "/tmp/key" , "private_key" ) ] \n 
"public" : [ ( "/tmp/key" , "public_static" ) ] , \n 
"private" : [ ( "/tmp/key" , "private_static" ) ] \n 
"problem_updates" : { \n 
~~ import IECore \n 
import GafferUI \n 
import GafferScene \n 
import GafferSceneUI \n 
scriptNode = script \n 
scriptWindow = GafferUI . ScriptWindow . acquire ( script ) \n 
layout = eval ( scriptWindow . setLayout ( layout ) \n 
scriptWindow . _Widget__qtWidget . resize ( 995 , 500 ) \n 
for nodeName in [ ] : \n 
~~~ script . selection ( ) . add ( script . descendant ( nodeName ) ) \n 
~~ script . context ( ) [ "ui:scene:expandedPaths" ] = GafferScene . PathMatcherData ( GafferScene . PathMatcher ( [ script . context ( ) [ "ui:scene:selectedPaths" ] = IECore . StringVectorData ( [ "/group/group2/group/sphere" ############################################################## \n 
########################################################################## \n 
import glob \n 
import IECore \n 
class convertAnimCache ( IECore . Op ) : \n 
self . parameters ( ) . addParameters ( \n 
[ \n 
IECore . FileSequenceParameter ( \n 
"inputSequence" , \n 
defaultValue = "" , \n 
allowEmptyString = False , \n 
check = IECore . FileSequenceParameter . CheckType . MustExist , \n 
extensions = "fio" , \n 
"outputSequence" , \n 
] , \n 
~~ def doOperation ( self , args ) : \n 
~~~ src = self . parameters ( ) [ "inputSequence" ] . getFileSequenceValue ( ) \n 
dst = self . parameters ( ) [ "outputSequence" ] . getFileSequenceValue ( ) \n 
if isinstance ( dst . frameList , IECore . EmptyFrameList ) : \n 
~~~ dst . frameList = src . frameList \n 
~~ for ( sf , df ) in zip ( src . fileNames ( ) , dst . fileNames ( ) ) : \n 
~~~ sc = IECore . AttributeCache ( sf , IECore . IndexedIOOpenMode . Read ) \n 
dc = IECore . AttributeCache ( df , IECore . IndexedIOOpenMode . Write ) \n 
combinedBound = IECore . Box3f ( ) \n 
for objectName in sc . objects ( ) : \n 
~~~ p = b = None \n 
with IECore . IgnoredExceptions ( Exception ) : \n 
~~~ p = sc . read ( objectName , "vertCache.P" ) \n 
b = sc . read ( objectName , "vertCache.boundingBox" ) \n 
~~ if p is not None and b is not None : \n 
~~~ combinedBound . extendBy ( b . value ) \n 
dc . write ( "-" + objectName , "primVar:P" , p ) \n 
dc . write ( "-" + objectName , "bound" , b ) \n 
~~ ~~ dc . write ( "-" , "bound" , IECore . Box3fData ( combinedBound ) ) \n 
~~ return args [ "outputSequence" ] . value \n 
~~ ~~ IECore . registerRunTimeTyped ( convertAnimCache ) \n 
import unittest \n 
import subprocess32 as subprocess \n 
import Gaffer \n 
import GafferTest \n 
import GafferAppleseed \n 
import GafferAppleseedTest \n 
class AppleseedRenderTest ( GafferTest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ GafferTest . TestCase . setUp ( self ) \n 
self . __scriptFileName = self . temporaryDirectory ( ) + "/test.gfr" \n 
~~ def testExecute ( self ) : \n 
~~~ s = Gaffer . ScriptNode ( ) \n 
s [ "plane" ] = GafferScene . Plane ( ) \n 
s [ "render" ] = GafferAppleseed . AppleseedRender ( ) \n 
s [ "render" ] [ "mode" ] . setValue ( "generate" ) \n 
s [ "render" ] [ "in" ] . setInput ( s [ "plane" ] [ "out" ] ) \n 
s [ "expression" ] = Gaffer . Expression ( ) \n 
s [ "fileName" ] . setValue ( self . __scriptFileName ) \n 
s . save ( ) \n 
p = subprocess . Popen ( \n 
shell = True , \n 
stderr = subprocess . PIPE , \n 
p . wait ( ) \n 
self . failIf ( p . returncode ) \n 
for i in range ( 1 , 4 ) : \n 
~~~ self . failUnless ( os . path . exists ( self . temporaryDirectory ( ) + "/test.%d.appleseed" % i ) ) \n 
~~ ~~ def testWaitForImage ( self ) : \n 
s [ "outputs" ] = GafferScene . Outputs ( ) \n 
s [ "outputs" ] . addOutput ( \n 
"beauty" , \n 
IECore . Display ( \n 
self . temporaryDirectory ( ) + "/test.exr" , \n 
"exr" , \n 
"rgba" , \n 
{ } \n 
s [ "outputs" ] [ "in" ] . setInput ( s [ "plane" ] [ "out" ] ) \n 
s [ "render" ] [ "in" ] . setInput ( s [ "outputs" ] [ "out" ] ) \n 
s [ "render" ] [ "verbosity" ] . setValue ( "fatal" ) \n 
s [ "render" ] [ "fileName" ] . setValue ( self . temporaryDirectory ( ) + "/test.appleseed" ) \n 
s [ "render" ] [ "task" ] . execute ( ) \n 
self . failUnless ( os . path . exists ( self . temporaryDirectory ( ) + "/test.exr" ) ) \n 
~~ def testExecuteWithStringSubstitutions ( self ) : \n 
s [ "render" ] [ "fileName" ] . setValue ( self . temporaryDirectory ( ) + "/test.####.appleseed" ) \n 
~~~ self . failUnless ( os . path . exists ( self . temporaryDirectory ( ) + "/test.%04d.appleseed" % i ) ) \n 
~~ ~~ def testImageOutput ( self ) : \n 
self . temporaryDirectory ( ) + "/test.####.exr" , \n 
c = Gaffer . Context ( ) \n 
~~~ c . setFrame ( i ) \n 
with c : \n 
~~~ s [ "render" ] [ "task" ] . execute ( ) \n 
~~ ~~ for i in range ( 1 , 4 ) : \n 
~~~ self . failUnless ( os . path . exists ( self . temporaryDirectory ( ) + "/test.%04d.exr" % i ) ) \n 
~~ ~~ def testTypeNamePrefixes ( self ) : \n 
~~~ self . assertTypeNamesArePrefixed ( GafferAppleseed ) \n 
self . assertTypeNamesArePrefixed ( GafferAppleseedTest ) \n 
~~ def testDefaultNames ( self ) : \n 
~~~ self . assertDefaultNamesAreCorrect ( GafferAppleseed ) \n 
self . assertDefaultNamesAreCorrect ( GafferAppleseedTest ) \n 
~~ def testNodesConstructWithDefaultValues ( self ) : \n 
~~~ self . assertNodesConstructWithDefaultValues ( GafferAppleseed ) \n 
self . assertNodesConstructWithDefaultValues ( GafferAppleseedTest ) \n 
~~ def testDirectoryCreation ( self ) : \n 
s [ "variables" ] . addMember ( "renderDirectory" , self . temporaryDirectory ( ) + "/renderTests" ) \n 
s [ "variables" ] . addMember ( "appleseedDirectory" , self . temporaryDirectory ( ) + "/appleseedTests" ) \n 
"$renderDirectory/test.####.exr" , \n 
s [ "render" ] [ "fileName" ] . setValue ( "$appleseedDirectory/test.####.appleseed" ) \n 
self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + "/renderTests" ) ) \n 
self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + "/appleseedTests" ) ) \n 
self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + "/appleseedTests/test.0001.appleseed" self . assertFalse ( os . path . exists ( self . __scriptFileName ) ) \n 
with s . context ( ) : \n 
~~ self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + "/renderTests" ) ) \n 
self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + "/appleseedTests" ) ) \n 
self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + "/appleseedTests/test.0001.appleseed" self . assertTrue ( os . path . exists ( self . __scriptFileName ) ) \n 
self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + "/appleseedTests/test.0001.appleseed" \n 
~~~ unittest . main ( ) \n 
~~ import GafferUITest \n 
import GafferArnold \n 
import GafferArnoldUI \n 
class DocumentationTest ( GafferUITest . TestCase ) : \n 
~~~ def test ( self ) : \n 
~~~ self . maxDiff = None \n 
self . assertNodesAreDocumented ( \n 
GafferArnold , \n 
additionalTerminalPlugTypes = ( GafferScene . ScenePlug , ) \n 
class parameterChangedCallback ( IECore . Parameterised ) : \n 
~~~ IECore . Parameterised . __init__ ( self , "" ) \n 
IECore . IntParameter ( \n 
"driver" , \n 
"" , \n 
0 \n 
"driven" , \n 
self . changes = [ ] \n 
~~ def parameterChanged ( self , parameter ) : \n 
~~~ self . changes . append ( ( parameter , str ( parameter . getValue ( ) ) ) ) \n 
if parameter . isSame ( self . parameters ( ) [ "driver" ] ) : \n 
~~~ self . parameters ( ) [ "driven" ] . setNumericValue ( self . parameters ( ) [ "driver" ] . getNumericValue ( ) * 5 ) \n 
~~ ~~ ~~ IECore . registerRunTimeTyped ( parameterChangedCallback ) \n 
import GafferCortexUI \n 
class ToolParameterValueWidget ( GafferCortexUI . ParameterValueWidget ) : \n 
~~~ def __init__ ( self , parameterHandler , parenting = None ) : \n 
~~~ GafferCortexUI . ParameterValueWidget . __init__ ( \n 
GafferUI . ToolPlugValueWidget ( parameterHandler . plug ( ) ) , \n 
parameterHandler , \n 
parenting = parenting \n 
import GafferDispatch \n 
class TextWriter ( GafferDispatch . ExecutableNode ) : \n 
~~~ def __init__ ( self , name = "TextWriter" , requiresSequenceExecution = False ) : \n 
~~~ GafferDispatch . ExecutableNode . __init__ ( self , name ) \n 
self . __requiresSequenceExecution = requiresSequenceExecution \n 
self . addChild ( Gaffer . StringPlug ( "fileName" , Gaffer . Plug . Direction . In ) ) \n 
self . addChild ( Gaffer . StringPlug ( "mode" , defaultValue = "w" , direction = Gaffer . Plug . Direction . In self . addChild ( Gaffer . StringPlug ( "text" , Gaffer . Plug . Direction . In ) ) \n 
~~ def execute ( self ) : \n 
~~~ context = Gaffer . Context . current ( ) \n 
fileName = self [ "fileName" ] . getValue ( ) \n 
directory = os . path . dirname ( fileName ) \n 
if directory : \n 
~~~ os . makedirs ( directory ) \n 
~~ except OSError : \n 
~~~ if not os . path . isdir ( directory ) : \n 
~~~ raise \n 
~~ ~~ ~~ text = self . __processText ( context ) \n 
with file ( fileName , self [ "mode" ] . getValue ( ) ) as f : \n 
~~~ f . write ( text ) \n 
~~ ~~ def executeSequence ( self , frames ) : \n 
~~~ if not self . __requiresSequenceExecution : \n 
~~~ GafferDispatch . ExecutableNode . executeSequence ( self , frames ) \n 
~~ context = Gaffer . Context ( Gaffer . Context . current ( ) ) \n 
~~~ with context : \n 
~~~ for frame in frames : \n 
~~~ context . setFrame ( frame ) \n 
text = self . __processText ( context ) \n 
f . write ( text ) \n 
~~ ~~ ~~ ~~ def hash ( self , context ) : \n 
~~~ h = GafferDispatch . ExecutableNode . hash ( self , context ) \n 
h . append ( context . getFrame ( ) ) \n 
h . append ( context . get ( "textWriter:replace" , IECore . StringVectorData ( ) ) ) \n 
self [ "fileName" ] . hash ( h ) \n 
self [ "mode" ] . hash ( h ) \n 
self [ "text" ] . hash ( h ) \n 
return h \n 
~~ def requiresSequenceExecution ( self ) : \n 
~~~ return self . __requiresSequenceExecution \n 
~~ def __processText ( self , context ) : \n 
~~~ text = self [ "text" ] . getValue ( ) \n 
replace = context . get ( "textWriter:replace" , IECore . StringVectorData ( ) ) \n 
if replace and len ( replace ) == 2 : \n 
~~~ text = text . replace ( replace [ 0 ] , replace [ 1 ] ) \n 
~~ return text \n 
~~ ~~ IECore . registerRunTimeTyped ( TextWriter , typeName = "GafferDispatchTest::TextWriter" ) \n 
import GafferImage \n 
import GafferImageTest \n 
class CopyImageMetadataTest ( GafferImageTest . ImageTestCase ) : \n 
~~~ checkerFile = os . path . expandvars ( "$GAFFER_ROOT/python/GafferImageTest/images/checker.exr" ) \n 
def test ( self ) : \n 
~~~ r = GafferImage . ImageReader ( ) \n 
r [ "fileName" ] . setValue ( self . checkerFile ) \n 
inMetadata = r [ "out" ] [ "metadata" ] . getValue ( ) \n 
d = GafferImage . DeleteImageMetadata ( ) \n 
d [ "in" ] . setInput ( r [ "out" ] ) \n 
d [ "names" ] . setValue ( "*" ) \n 
m = GafferImage . CopyImageMetadata ( ) \n 
m [ "in" ] . setInput ( d [ "out" ] ) \n 
m [ "copyFrom" ] . setInput ( r [ "out" ] ) \n 
m [ "names" ] . setValue ( "" ) \n 
metadata = m [ "out" ] [ "metadata" ] . getValue ( ) \n 
self . assertEqual ( m [ "out" ] [ "metadata" ] . getValue ( ) , IECore . CompoundObject ( ) ) \n 
self . assertEqual ( m [ "out" ] . image ( ) , d [ "out" ] . image ( ) ) \n 
expected = set ( [ "screenWindowWidth" , "screenWindowCenter" , "compression" ] ) \n 
self . assertEqual ( set ( metadata . keys ( ) ) , expected ) \n 
for key in metadata . keys ( ) : \n 
~~~ self . assertEqual ( metadata [ key ] , inMetadata [ key ] ) \n 
~~ m [ "invertNames" ] . setValue ( True ) \n 
expected = set ( [ "PixelAspectRatio" , "oiio:ColorSpace" ] ) \n 
~~ ~~ def testOverwrite ( self ) : \n 
a = GafferImage . ImageMetadata ( ) \n 
a [ "metadata" ] . addMember ( "compression" , IECore . StringData ( "extraFancyCompressor" ) ) \n 
m [ "in" ] . setInput ( r [ "out" ] ) \n 
m [ "copyFrom" ] . setInput ( a [ "out" ] ) \n 
self . assertEqual ( metadata [ "compression" ] , IECore . StringData ( "zips" ) ) \n 
self . assertEqual ( m [ "out" ] [ "metadata" ] . getValue ( ) , inMetadata ) \n 
self . assertEqual ( m [ "out" ] . image ( ) , r [ "out" ] . image ( ) ) \n 
m [ "names" ] . setValue ( "compression" ) \n 
self . assertTrue ( "compression" in metadata . keys ( ) ) \n 
self . assertEqual ( metadata [ "compression" ] , IECore . StringData ( "extraFancyCompressor" ) ) \n 
~~ def testDirtyPropogation ( self ) : \n 
~~~ c = GafferImage . Constant ( ) \n 
r = GafferImage . ImageReader ( ) \n 
m [ "in" ] . setInput ( c [ "out" ] ) \n 
cs = GafferTest . CapturingSlot ( m . plugDirtiedSignal ( ) ) \n 
m [ "copyFrom" ] . setInput ( c [ "out" ] ) \n 
self . assertTrue ( m [ "out" ] [ "metadata" ] in set ( e [ 0 ] for e in cs ) ) \n 
del cs [ : ] \n 
m [ "names" ] . setValue ( "test" ) \n 
m [ "invertNames" ] . setValue ( True ) \n 
~~ def testPassThrough ( self ) : \n 
i = GafferImage . ImageReader ( ) \n 
i [ "fileName" ] . setValue ( self . checkerFile ) \n 
m [ "in" ] . setInput ( i [ "out" ] ) \n 
m [ "names" ] . setValue ( "*" ) \n 
self . assertEqual ( i [ "out" ] [ "format" ] . hash ( ) , m [ "out" ] [ "format" ] . hash ( ) ) \n 
self . assertEqual ( i [ "out" ] [ "dataWindow" ] . hash ( ) , m [ "out" ] [ "dataWindow" ] . hash ( ) ) \n 
self . assertEqual ( i [ "out" ] [ "channelNames" ] . hash ( ) , m [ "out" ] [ "channelNames" ] . hash ( ) ) \n 
self . assertEqual ( i [ "out" ] [ "format" ] . getValue ( ) , m [ "out" ] [ "format" ] . getValue ( ) ) \n 
self . assertEqual ( i [ "out" ] [ "dataWindow" ] . getValue ( ) , m [ "out" ] [ "dataWindow" ] . getValue ( ) ) \n 
self . assertEqual ( i [ "out" ] [ "channelNames" ] . getValue ( ) , m [ "out" ] [ "channelNames" ] . getValue ( ) ) \n 
context = Gaffer . Context ( ) \n 
context [ "image:tileOrigin" ] = IECore . V2i ( 0 ) \n 
with context : \n 
~~~ for c in [ "G" , "B" , "A" ] : \n 
~~~ context [ "image:channelName" ] = c \n 
self . assertEqual ( i [ "out" ] [ "channelData" ] . hash ( ) , m [ "out" ] [ "channelData" ] . hash ( ) ) \n 
self . assertEqual ( i [ "out" ] [ "channelData" ] . getValue ( ) , m [ "out" ] [ "channelData" ] . getValue ( ) ) \n 
~~ ~~ ~~ ~~ if __name__ == "__main__" : \n 
~~ import os \n 
class ObjectToImageTest ( GafferImageTest . ImageTestCase ) : \n 
~~~ fileName = os . path . expandvars ( "$GAFFER_ROOT/python/GafferImageTest/images/checker.exr" ) \n 
negFileName = os . path . expandvars ( "$GAFFER_ROOT/python/GafferImageTest/images/checkerWithNegativeDataWindow.200x150.exr" \n 
~~~ i = IECore . Reader . create ( self . fileName ) . read ( ) \n 
n = GafferImage . ObjectToImage ( ) \n 
n [ "object" ] . setValue ( i ) \n 
self . assertEqual ( n [ "out" ] . image ( ) , i ) \n 
~~ def testImageWithANegativeDataWindow ( self ) : \n 
~~~ i = IECore . Reader . create ( self . negFileName ) . read ( ) \n 
~~ def testHashVariesPerTileAndChannel ( self ) : \n 
~~~ n = GafferImage . ObjectToImage ( ) \n 
n [ "object" ] . setValue ( IECore . Reader . create ( self . fileName ) . read ( ) ) \n 
self . assertNotEqual ( \n 
n [ "out" ] . channelDataHash ( "R" , IECore . V2i ( 0 ) ) , \n 
n [ "out" ] . channelDataHash ( "G" , IECore . V2i ( 0 ) ) \n 
n [ "out" ] . channelDataHash ( "R" , IECore . V2i ( GafferImage . ImagePlug . tileSize ( ) ) ) \n 
~~ import threading \n 
__all__ = [ ] \n 
Gaffer . Metadata . registerNode ( \n 
GafferImage . Display , \n 
"description" , \n 
plugs = { \n 
"port" : [ \n 
__plugsPendingUpdate = [ ] \n 
__plugsPendingUpdateLock = threading . Lock ( ) \n 
def __scheduleUpdate ( plug , force = False ) : \n 
~~~ if not force : \n 
~~~ global __plugsPendingUpdate \n 
global __plugsPendingUpdateLock \n 
with __plugsPendingUpdateLock : \n 
~~~ for p in __plugsPendingUpdate : \n 
~~~ if plug . isSame ( p ) : \n 
~~ ~~ __plugsPendingUpdate . append ( plug ) \n 
~~ ~~ GafferUI . EventLoop . executeOnUIThread ( lambda : __update ( plug ) ) \n 
~~ def __update ( plug ) : \n 
~~~ node = plug . node ( ) \n 
if node : \n 
~~~ updateCountPlug = node [ "__updateCount" ] \n 
updateCountPlug . setValue ( updateCountPlug . getValue ( ) + 1 ) \n 
~~ global __plugsPendingUpdate \n 
~~~ __plugsPendingUpdate = [ p for p in __plugsPendingUpdate if not p . isSame ( plug ) ] \n 
~~ ~~ __displayDataReceivedConnection = GafferImage . Display . dataReceivedSignal ( ) . connect ( __scheduleUpdate __displayImageReceivedConnection = GafferImage . Display . imageReceivedSignal ( ) . connect ( IECore . curry ( ########################################################################## \n 
from _GafferImageUI import * \n 
import DisplayUI \n 
from FormatPlugValueWidget import FormatPlugValueWidget \n 
from ChannelMaskPlugValueWidget import ChannelMaskPlugValueWidget \n 
import OpenImageIOReaderUI \n 
import ImageReaderUI \n 
import ImageViewToolbar \n 
import ImageTransformUI \n 
import ConstantUI \n 
import ImageSwitchUI \n 
import ColorSpaceUI \n 
import ImageContextVariablesUI \n 
import ImageStatsUI \n 
import DeleteChannelsUI \n 
import ObjectToImageUI \n 
import ClampUI \n 
import ImageWriterUI \n 
import GradeUI \n 
import ImageTimeWarpUI \n 
import ImageSamplerUI \n 
import MergeUI \n 
import ImageNodeUI \n 
import ChannelDataProcessorUI \n 
import ImageProcessorUI \n 
import ImageMetadataUI \n 
import DeleteImageMetadataUI \n 
import CopyImageMetadataUI \n 
import ImageLoopUI \n 
import ShuffleUI \n 
import PremultiplyUI \n 
import UnpremultiplyUI \n 
import CropUI \n 
import ResizeUI \n 
import ResampleUI \n 
import LUTUI \n 
import CDLUI \n 
import DisplayTransformUI \n 
import OffsetUI \n 
import BlurUI \n 
import ShapeUI \n 
import TextUI \n 
import WarpUI \n 
import UVWarpUI \n 
__import__ ( "IECore" ) . loadConfig ( "GAFFER_STARTUP_PATHS" , { } , subdirectory = "GafferImageUI" ) \n 
import GafferSceneTest \n 
import GafferRenderMan \n 
import GafferRenderManTest \n 
class RenderManShaderTest ( GafferRenderManTest . RenderManTestCase ) : \n 
~~~ GafferRenderManTest . RenderManTestCase . setUp ( self ) \n 
GafferRenderMan . RenderManShader . shaderLoader ( ) . clear ( ) \n 
~~ def test ( self ) : \n 
~~~ n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( "plastic" ) \n 
self . failUnless ( isinstance ( n [ "parameters" ] [ "Ks" ] , Gaffer . FloatPlug ) ) \n 
self . failUnless ( isinstance ( n [ "parameters" ] [ "Kd" ] , Gaffer . FloatPlug ) ) \n 
self . failUnless ( isinstance ( n [ "parameters" ] [ "Ka" ] , Gaffer . FloatPlug ) ) \n 
self . failUnless ( isinstance ( n [ "parameters" ] [ "roughness" ] , Gaffer . FloatPlug ) ) \n 
self . failUnless ( isinstance ( n [ "parameters" ] [ "specularcolor" ] , Gaffer . Color3fPlug ) ) \n 
self . assertEqual ( n [ "parameters" ] [ "Ks" ] . getValue ( ) , 0.5 ) \n 
self . assertEqual ( n [ "parameters" ] [ "Kd" ] . getValue ( ) , 0.5 ) \n 
self . assertEqual ( n [ "parameters" ] [ "Ka" ] . getValue ( ) , 1 ) \n 
self . assertAlmostEqual ( n [ "parameters" ] [ "roughness" ] . getValue ( ) , 0.1 ) \n 
self . assertEqual ( n [ "parameters" ] [ "specularcolor" ] . getValue ( ) , IECore . Color3f ( 1 ) ) \n 
~~ def testSerialisation ( self ) : \n 
s [ "n" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "n" ] . loadShader ( "plastic" ) \n 
ss = s . serialise ( ) \n 
s = Gaffer . ScriptNode ( ) \n 
s . execute ( ss ) \n 
st = s [ "n" ] . state ( ) \n 
self . assertEqual ( len ( st ) , 1 ) \n 
self . assertEqual ( st [ 0 ] . type , "ri:surface" ) \n 
self . assertEqual ( st [ 0 ] . name , "plastic" ) \n 
self . failUnless ( isinstance ( s [ "n" ] [ "parameters" ] [ "Ks" ] , Gaffer . FloatPlug ) ) \n 
self . failUnless ( isinstance ( s [ "n" ] [ "parameters" ] [ "Kd" ] , Gaffer . FloatPlug ) ) \n 
self . failUnless ( isinstance ( s [ "n" ] [ "parameters" ] [ "Ka" ] , Gaffer . FloatPlug ) ) \n 
self . failUnless ( isinstance ( s [ "n" ] [ "parameters" ] [ "roughness" ] , Gaffer . FloatPlug ) ) \n 
self . failUnless ( isinstance ( s [ "n" ] [ "parameters" ] [ "specularcolor" ] , Gaffer . Color3fPlug ) ) \n 
self . assertTrue ( "parameters1" not in s [ "n" ] ) \n 
~~ def testShader ( self ) : \n 
s = n . state ( ) \n 
self . assertEqual ( len ( s ) , 1 ) \n 
self . assertEqual ( s [ 0 ] . type , "ri:surface" ) \n 
self . assertEqual ( s [ 0 ] . name , "plastic" ) \n 
self . assertEqual ( s [ 0 ] . parameters [ "Ks" ] , IECore . FloatData ( .5 ) ) \n 
self . assertEqual ( s [ 0 ] . parameters [ "Kd" ] , IECore . FloatData ( .5 ) ) \n 
self . assertEqual ( s [ 0 ] . parameters [ "Ka" ] , IECore . FloatData ( 1 ) ) \n 
self . assertEqual ( s [ 0 ] . parameters [ "roughness" ] , IECore . FloatData ( .1 ) ) \n 
self . assertEqual ( s [ 0 ] . parameters [ "specularcolor" ] , IECore . Color3fData ( IECore . Color3f ( 1 ) ) ) \n 
~~ def testShaderHash ( self ) : \n 
n . loadShader ( "matte" ) \n 
h1 = n . stateHash ( ) \n 
n [ "parameters" ] [ "Kd" ] . setValue ( 0.25 ) \n 
self . assertNotEqual ( n . stateHash ( ) , h1 ) \n 
~~ def testCoshaderHash ( self ) : \n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
self . assertTrue ( "coshaderParameter" in shaderNode [ "parameters" ] ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . typeId ( ) , Gaffer . Plug . staticTypeId \n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
coshaderNode = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode . loadShader ( coshader ) \n 
shaderNode [ "parameters" ] [ "coshaderParameter" ] . setInput ( coshaderNode [ "out" ] ) \n 
h1 = shaderNode . stateHash ( ) \n 
coshaderNode [ "parameters" ] [ "floatParameter" ] . setValue ( 0.25 ) \n 
self . assertNotEqual ( shaderNode . stateHash ( ) , h1 ) \n 
~~ def testParameterOrdering ( self ) : \n 
self . assertEqual ( n [ "parameters" ] [ 0 ] . getName ( ) , "Ks" ) \n 
self . assertEqual ( n [ "parameters" ] [ 1 ] . getName ( ) , "Kd" ) \n 
self . assertEqual ( n [ "parameters" ] [ 2 ] . getName ( ) , "Ka" ) \n 
self . assertEqual ( n [ "parameters" ] [ 3 ] . getName ( ) , "roughness" ) \n 
self . assertEqual ( n [ "parameters" ] [ 4 ] . getName ( ) , "specularcolor" ) \n 
n = GafferRenderMan . RenderManShader ( ) \n 
self . assertEqual ( n [ "parameters" ] [ 0 ] . getName ( ) , "Ka" ) \n 
~~ def testCoshader ( self ) : \n 
s = shaderNode . state ( ) \n 
self . assertEqual ( len ( s ) , 2 ) \n 
self . assertEqual ( s [ 0 ] . name , coshader ) \n 
self . assertEqual ( s [ 1 ] . name , shader ) \n 
self . assertEqual ( s [ 0 ] . parameters [ "__handle" ] , s [ 1 ] . parameters [ "coshaderParameter" ] ) \n 
~~ def testInputAcceptance ( self ) : \n 
random = Gaffer . Random ( ) \n 
self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( coshaderNode [ "out" ] ) self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( random [ "outFloat" ] ) \n 
self . assertTrue ( shaderNode [ "parameters" ] [ "floatParameter" ] . acceptsInput ( random [ "outFloat" ] ) ) \n 
self . assertFalse ( shaderNode [ "parameters" ] [ "floatParameter" ] . acceptsInput ( coshaderNode [ "out" ] ) ) \n 
self . assertTrue ( coshaderNode [ "parameters" ] [ "colorParameter" ] . acceptsInput ( random [ "outColor" ] ) ) self . assertFalse ( coshaderNode [ "parameters" ] [ "colorParameter" ] . acceptsInput ( coshaderNode [ "out" ] ) \n 
~~ def testParameterDefaultValue ( self ) : \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "floatParameter" ] . defaultValue ( ) , 1 ) \n 
~~ def testParameterMinMax ( self ) : \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "floatParameter" ] . minValue ( ) , - 1 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "floatParameter" ] . maxValue ( ) , 10 ) \n 
~~ def testReload ( self ) : \n 
~~~ shader1 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version1.sl" ) \n 
shaderNode . loadShader ( shader1 ) \n 
shaderNode [ "parameters" ] [ "float1" ] . setValue ( 0.1 ) \n 
shaderNode [ "parameters" ] [ "string1" ] . setValue ( "test" ) \n 
shaderNode [ "parameters" ] [ "color1" ] . setValue ( IECore . Color3f ( 1 , 2 , 3 ) ) \n 
self . assertAlmostEqual ( shaderNode [ "parameters" ] [ "float1" ] . getValue ( ) , 0.1 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "string1" ] . getValue ( ) , "test" ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "color1" ] . getValue ( ) , IECore . Color3f ( 1 , 2 , 3 ) ) \n 
shader2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version2.sl" ) \n 
shaderNode . loadShader ( shader2 , keepExistingValues = True ) \n 
self . assertEqual ( shaderNode [ "parameters" ] . keys ( ) , [ "float1" , "string1" , "color1" , "float2" , "string2" self . assertAlmostEqual ( shaderNode [ "parameters" ] [ "float1" ] . getValue ( ) , 0.1 ) \n 
shaderNode . loadShader ( shader1 , keepExistingValues = True ) \n 
self . assertEqual ( shaderNode [ "parameters" ] . keys ( ) , [ "float1" , "string1" , "color1" ] ) \n 
shaderNode . loadShader ( shader1 , keepExistingValues = False ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "float1" ] . getValue ( ) , 1 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "string1" ] . getValue ( ) , "" ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "color1" ] . getValue ( ) , IECore . Color3f ( 1 , 1 , 1 ) ) \n 
~~ def testReloadRemovesOldParameters ( self ) : \n 
~~~ shader2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version2.sl" ) \n 
shaderNode . loadShader ( shader2 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] . keys ( ) , [ "float1" , "string1" , "color1" , "float2" , "string2" \n 
shader3 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version3.sl" ) \n 
shaderNode . loadShader ( shader3 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] . keys ( ) , [ "float1" , "string1" , "color1" , "float2" ] ) \n 
~~ def testAutomaticReloadOnScriptLoad ( self ) : \n 
~~~ shader1 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version1.sl" , shaderName = "unversioned" \n 
s [ "shader" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "shader" ] . loadShader ( shader1 ) \n 
s [ "shader" ] [ "parameters" ] [ "float1" ] . setValue ( 0.1 ) \n 
s [ "shader" ] [ "parameters" ] [ "string1" ] . setValue ( "test" ) \n 
s [ "shader" ] [ "parameters" ] [ "color1" ] . setValue ( IECore . Color3f ( 1 , 2 , 3 ) ) \n 
self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version2.sl" , shaderName = "unversioned" \n 
self . assertEqual ( s [ "shader" ] [ "parameters" ] . keys ( ) , [ "float1" , "string1" , "color1" , "float2" , "string2" self . assertAlmostEqual ( s [ "shader" ] [ "parameters" ] [ "float1" ] . getValue ( ) , 0.1 ) \n 
self . assertEqual ( s [ "shader" ] [ "parameters" ] [ "string1" ] . getValue ( ) , "test" ) \n 
self . assertEqual ( s [ "shader" ] [ "parameters" ] [ "color1" ] . getValue ( ) , IECore . Color3f ( 1 , 2 , 3 ) ) \n 
~~ def testReloadPreservesConnections ( self ) : \n 
n [ "parameters" ] [ "Ks" ] . setInput ( random [ "outFloat" ] ) \n 
n [ "parameters" ] [ "specularcolor" ] . setInput ( random [ "outColor" ] ) \n 
n . loadShader ( "plastic" , keepExistingValues = True ) \n 
self . assertTrue ( n [ "parameters" ] [ "Ks" ] . getInput ( ) . isSame ( random [ "outFloat" ] ) ) \n 
self . assertTrue ( n [ "parameters" ] [ "specularcolor" ] . getInput ( ) . isSame ( random [ "outColor" ] ) ) \n 
~~ def testReloadPreservesConnectionsWhenMinMaxOrDefaultChanges ( self ) : \n 
~~~ shader1 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version1.sl" , shaderName = "unversioned" n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( shader1 ) \n 
self . assertFalse ( n [ "parameters" ] [ "float1" ] . hasMinValue ( ) ) \n 
self . assertFalse ( n [ "parameters" ] [ "float1" ] . hasMaxValue ( ) ) \n 
self . assertEqual ( n [ "parameters" ] [ "string1" ] . defaultValue ( ) , "" ) \n 
nn = Gaffer . Node ( ) \n 
nn [ "outFloat" ] = Gaffer . FloatPlug ( direction = Gaffer . Plug . Direction . Out ) \n 
nn [ "outString" ] = Gaffer . StringPlug ( direction = Gaffer . Plug . Direction . Out ) \n 
n [ "parameters" ] [ "float1" ] . setInput ( nn [ "outFloat" ] ) \n 
n [ "parameters" ] [ "string1" ] . setInput ( nn [ "outString" ] ) \n 
shader2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version2.sl" , shaderName = "unversioned" \n 
n . loadShader ( shader1 , keepExistingValues = True ) \n 
self . assertTrue ( n [ "parameters" ] [ "float1" ] . hasMinValue ( ) ) \n 
self . assertTrue ( n [ "parameters" ] [ "float1" ] . hasMaxValue ( ) ) \n 
self . assertEqual ( n [ "parameters" ] [ "float1" ] . minValue ( ) , - 1 ) \n 
self . assertEqual ( n [ "parameters" ] [ "float1" ] . maxValue ( ) , 2 ) \n 
self . assertEqual ( n [ "parameters" ] [ "string1" ] . defaultValue ( ) , "newDefaultValue" ) \n 
self . assertTrue ( n [ "parameters" ] [ "float1" ] . getInput ( ) . isSame ( nn [ "outFloat" ] ) ) \n 
self . assertTrue ( n [ "parameters" ] [ "string1" ] . getInput ( ) . isSame ( nn [ "outString" ] ) ) \n 
~~ def testReloadPreservesPartialConnectionsWhenMinMaxOrDefaultChanges ( self ) : \n 
n [ "parameters" ] [ "color1" ] [ 0 ] . setInput ( nn [ "outFloat" ] ) \n 
n [ "parameters" ] [ "color1" ] [ 1 ] . setInput ( nn [ "outFloat" ] ) \n 
n [ "parameters" ] [ "color1" ] [ 2 ] . setValue ( 0.75 ) \n 
self . assertTrue ( n [ "parameters" ] [ "color1" ] [ 0 ] . getInput ( ) . isSame ( nn [ "outFloat" ] ) ) \n 
self . assertTrue ( n [ "parameters" ] [ "color1" ] [ 1 ] . getInput ( ) . isSame ( nn [ "outFloat" ] ) ) \n 
self . assertEqual ( n [ "parameters" ] [ "color1" ] [ 2 ] . getValue ( ) , 0.75 ) \n 
~~ def testReloadPreservesValuesWhenMinMaxOrDefaultChanges ( self ) : \n 
n [ "parameters" ] [ "float1" ] . setValue ( 0.25 ) \n 
n [ "parameters" ] [ "string1" ] . setValue ( "dog" ) \n 
n [ "parameters" ] [ "color1" ] . setValue ( IECore . Color3f ( 0.1 , 0.25 , 0.5 ) ) \n 
self . assertEqual ( n [ "parameters" ] [ "float1" ] . getValue ( ) , 0.25 ) \n 
self . assertEqual ( n [ "parameters" ] [ "string1" ] . getValue ( ) , "dog" ) \n 
self . assertEqual ( n [ "parameters" ] [ "color1" ] . getValue ( ) , IECore . Color3f ( 0.1 , 0.25 , 0.5 ) ) \n 
~~ def testOutputParameters ( self ) : \n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version3.sl" ) \n 
n . loadShader ( shader ) \n 
self . failIf ( "outputFloat" in n [ "parameters" ] . keys ( ) ) \n 
~~ def testAssignmentDirtyPropagation ( self ) : \n 
plane = GafferScene . Plane ( ) \n 
assignment = GafferScene . ShaderAssignment ( ) \n 
assignment [ "in" ] . setInput ( plane [ "out" ] ) \n 
assignment [ "shader" ] . setInput ( shaderNode [ "out" ] ) \n 
cs = GafferTest . CapturingSlot ( assignment . plugDirtiedSignal ( ) ) \n 
coshaderNode [ "parameters" ] [ "floatParameter" ] . setValue ( 12 ) \n 
dirtiedNames = [ x [ 0 ] . fullName ( ) for x in cs ] \n 
self . assertEqual ( len ( dirtiedNames ) , 3 ) \n 
self . assertEqual ( dirtiedNames [ 0 ] , "ShaderAssignment.shader" ) \n 
self . assertEqual ( dirtiedNames [ 1 ] , "ShaderAssignment.out.attributes" ) \n 
self . assertEqual ( dirtiedNames [ 2 ] , "ShaderAssignment.out" ) \n 
~~ def testArrayParameters ( self ) : \n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/arrayParameters.sl" ) \n 
expected = { \n 
"dynamicFloatArray" : IECore . FloatVectorData ( [ ] ) , \n 
"fixedFloatArray" : IECore . FloatVectorData ( [ 1 , 2 , 3 , 4 ] ) , \n 
"dynamicStringArray" : IECore . StringVectorData ( [ "dynamic" , "arrays" , "can" , "still" , "have" , "defaults" "fixedStringArray" : IECore . StringVectorData ( [ "hello" , "goodbye" ] ) , \n 
"dynamicColorArray" : IECore . Color3fVectorData ( [ IECore . Color3f ( 1 ) , IECore . Color3f ( 2 ) ] ) , \n 
"fixedColorArray" : IECore . Color3fVectorData ( [ IECore . Color3f ( 1 ) , IECore . Color3f ( 2 ) ] ) , \n 
"dynamicVectorArray" : IECore . V3fVectorData ( [ ] , IECore . GeometricData . Interpretation . Vector ) , \n 
"fixedVectorArray" : IECore . V3fVectorData ( [ IECore . V3f ( x ) for x in range ( 1 , 6 ) ] , IECore . GeometricData "dynamicPointArray" : IECore . V3fVectorData ( [ ] , IECore . GeometricData . Interpretation . Point ) , \n 
"fixedPointArray" : IECore . V3fVectorData ( [ IECore . V3f ( x ) for x in range ( 1 , 6 ) ] , IECore . GeometricData "dynamicNormalArray" : IECore . V3fVectorData ( [ ] , IECore . GeometricData . Interpretation . Normal ) , \n 
"fixedNormalArray" : IECore . V3fVectorData ( [ IECore . V3f ( x ) for x in range ( 1 , 6 ) ] , IECore . GeometricData } \n 
self . assertEqual ( set ( n [ "parameters" ] . keys ( ) ) , set ( expected . keys ( ) ) ) \n 
for name , value in expected . items ( ) : \n 
~~~ self . assertEqual ( n [ "parameters" ] [ name ] . defaultValue ( ) , value ) \n 
self . assertEqual ( n [ "parameters" ] [ name ] . getValue ( ) , value ) \n 
~~ s = n . state ( ) [ 0 ] \n 
~~~ self . assertEqual ( s . parameters [ name ] , value ) \n 
~~ ~~ def testFixedCoshaderArrayParameters ( self ) : \n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" ) n = GafferRenderMan . RenderManShader ( ) \n 
self . assertEqual ( n [ "parameters" ] . keys ( ) , [ "dynamicShaderArray" , "fixedShaderArray" ] ) \n 
self . assertTrue ( isinstance ( n [ "parameters" ] [ "fixedShaderArray" ] , Gaffer . ArrayPlug ) ) \n 
self . assertEqual ( len ( n [ "parameters" ] [ "fixedShaderArray" ] ) , 4 ) \n 
self . assertTrue ( isinstance ( n [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray0" ] , Gaffer . Plug self . assertTrue ( isinstance ( n [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray1" ] , Gaffer . Plug self . assertTrue ( isinstance ( n [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray2" ] , Gaffer . Plug self . assertTrue ( isinstance ( n [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray3" ] , Gaffer . Plug \n 
state = n . state ( ) \n 
self . assertEqual ( state [ 0 ] . parameters [ "fixedShaderArray" ] , IECore . StringVectorData ( [ "" ] * 4 ) ) \n 
n [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray0" ] . setInput ( coshaderNode [ "out" ] ) \n 
self . assertEqual ( state [ 1 ] . parameters [ "fixedShaderArray" ] , IECore . StringVectorData ( [ state [ 0 ] . parameters \n 
~~ def testCoshaderType ( self ) : \n 
~~~ coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
self . assertEqual ( coshaderNode . state ( ) [ 0 ] . type , "ri:shader" ) \n 
~~ def testCantConnectSurfaceShaderIntoCoshaderInput ( self ) : \n 
n1 = GafferRenderMan . RenderManShader ( ) \n 
n1 . loadShader ( shader ) \n 
n2 = GafferRenderMan . RenderManShader ( ) \n 
n2 . loadShader ( "plastic" ) \n 
self . assertFalse ( n1 [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( n2 [ "out" ] ) ) \n 
n3 = GafferRenderMan . RenderManShader ( ) \n 
n3 . loadShader ( coshader ) \n 
self . assertTrue ( n1 [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( n3 [ "out" ] ) ) \n 
arrayShader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" n4 = GafferRenderMan . RenderManShader ( ) \n 
n4 . loadShader ( arrayShader ) \n 
self . assertFalse ( n4 [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray0" ] . acceptsInput ( n2 [ "out" self . assertTrue ( n4 [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray0" ] . acceptsInput ( n3 [ "out" ] \n 
~~ def testConnectionsBetweenParameters ( self ) : \n 
~~~ s = GafferRenderMan . RenderManShader ( ) \n 
s . loadShader ( "plastic" ) \n 
s [ "parameters" ] [ "Kd" ] . setValue ( 0.25 ) \n 
s [ "parameters" ] [ "Ks" ] . setInput ( s [ "parameters" ] [ "Kd" ] ) \n 
shader = s . state ( ) [ 0 ] \n 
self . assertEqual ( shader . parameters [ "Kd" ] . value , 0.25 ) \n 
self . assertEqual ( shader . parameters [ "Ks" ] . value , 0.25 ) \n 
~~ def testFixedCoshaderArrayParameterHash ( self ) : \n 
h2 = n . stateHash ( ) \n 
self . assertNotEqual ( h2 , h1 ) \n 
n [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray1" ] . setInput ( coshaderNode [ "out" ] ) \n 
h3 = n . stateHash ( ) \n 
self . assertNotEqual ( h3 , h2 ) \n 
self . assertNotEqual ( h3 , h1 ) \n 
n [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray1" ] . setInput ( None ) \n 
n [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray2" ] . setInput ( coshaderNode [ "out" ] ) \n 
h4 = n . stateHash ( ) \n 
self . assertNotEqual ( h4 , h3 ) \n 
self . assertNotEqual ( h4 , h2 ) \n 
self . assertNotEqual ( h4 , h1 ) \n 
~~ def testDisabling ( self ) : \n 
stateHash = s . stateHash ( ) \n 
state = s . state ( ) \n 
self . assertEqual ( len ( state ) , 1 ) \n 
self . assertEqual ( state [ 0 ] . name , "plastic" ) \n 
self . assertTrue ( s [ "enabled" ] . isSame ( s . enabledPlug ( ) ) ) \n 
s [ "enabled" ] . setValue ( False ) \n 
stateHash2 = s . stateHash ( ) \n 
self . assertNotEqual ( stateHash2 , stateHash ) \n 
state2 = s . state ( ) \n 
self . assertEqual ( len ( state2 ) , 0 ) \n 
~~ def testDisablingCoshaders ( self ) : \n 
h = shaderNode . stateHash ( ) \n 
coshaderNode [ "enabled" ] . setValue ( False ) \n 
s2 = shaderNode . state ( ) \n 
self . assertEqual ( len ( s2 ) , 1 ) \n 
self . assertEqual ( s2 [ 0 ] . name , shader ) \n 
self . assertTrue ( "coshaderParameter" not in s2 [ 0 ] . parameters ) \n 
self . assertNotEqual ( shaderNode . stateHash ( ) , h ) \n 
~~ def testDisablingCoshaderArrayInputs ( self ) : \n 
coshaderNode1 = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode1 . loadShader ( coshader ) \n 
coshaderNode2 = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode2 . loadShader ( coshader ) \n 
n [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . setInput ( coshaderNode1 [ "out" ] ) \n 
n [ "parameters" ] [ "fixedShaderArray" ] [ 2 ] . setInput ( coshaderNode2 [ "out" ] ) \n 
self . assertEqual ( \n 
state [ 2 ] . parameters [ "fixedShaderArray" ] , \n 
IECore . StringVectorData ( [ \n 
state [ 0 ] . parameters [ "__handle" ] . value , \n 
state [ 1 ] . parameters [ "__handle" ] . value , \n 
"" \n 
] ) \n 
coshaderNode1 [ "enabled" ] . setValue ( False ) \n 
state [ 1 ] . parameters [ "fixedShaderArray" ] , \n 
coshaderNode2 [ "enabled" ] . setValue ( False ) \n 
state [ 0 ] . parameters [ "fixedShaderArray" ] , \n 
self . assertNotEqual ( n . stateHash ( ) , h2 ) \n 
~~ def testCorrespondingInput ( self ) : \n 
self . assertEqual ( coshaderNode . correspondingInput ( coshaderNode [ "out" ] ) , None ) \n 
coshader2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderWithPassThrough.sl" coshaderNode2 = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode2 . loadShader ( coshader2 ) \n 
self . assertTrue ( coshaderNode2 . correspondingInput ( coshaderNode2 [ "out" ] ) . isSame ( coshaderNode2 [ "parameters" \n 
~~ def testCoshaderPassThrough ( self ) : \n 
passThroughCoshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderWithPassThrough.sl" passThroughCoshaderNode = GafferRenderMan . RenderManShader ( ) \n 
passThroughCoshaderNode . loadShader ( passThroughCoshader ) \n 
shaderNode [ "parameters" ] [ "coshaderParameter" ] . setInput ( passThroughCoshaderNode [ "out" ] ) \n 
passThroughCoshaderNode [ "parameters" ] [ "aColorIWillTint" ] . setInput ( coshaderNode [ "out" ] ) \n 
self . assertEqual ( len ( s ) , 3 ) \n 
self . assertEqual ( s [ 2 ] . parameters [ "coshaderParameter" ] , s [ 1 ] . parameters [ "__handle" ] ) \n 
self . assertEqual ( s [ 1 ] . name , passThroughCoshader ) \n 
self . assertEqual ( s [ 1 ] . parameters [ "aColorIWillTint" ] , s [ 0 ] . parameters [ "__handle" ] ) \n 
passThroughCoshaderNode [ "enabled" ] . setValue ( False ) \n 
self . assertEqual ( s [ 1 ] . parameters [ "coshaderParameter" ] , s [ 0 ] . parameters [ "__handle" ] ) \n 
~~ def testSplineParameters ( self ) : \n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/splineParameters.sl" ) \n 
self . assertEqual ( n [ "parameters" ] . keys ( ) , [ "floatSpline" , "colorSpline" , "colorSpline2" ] ) \n 
self . assertTrue ( isinstance ( n [ "parameters" ] [ "floatSpline" ] , Gaffer . SplineffPlug ) ) \n 
self . assertTrue ( isinstance ( n [ "parameters" ] [ "colorSpline" ] , Gaffer . SplinefColor3fPlug ) ) \n 
n [ "parameters" ] [ "floatSpline" ] . defaultValue ( ) , \n 
IECore . Splineff ( \n 
IECore . CubicBasisf . catmullRom ( ) , \n 
( 0 , 0 ) , \n 
( 1 , 1 ) , \n 
n [ "parameters" ] [ "colorSpline" ] . defaultValue ( ) , \n 
IECore . SplinefColor3f ( \n 
( 0 , IECore . Color3f ( 0 ) ) , \n 
( 1 , IECore . Color3f ( 1 ) ) , \n 
floatValue = IECore . Splineff ( \n 
( 1 , 2 ) , \n 
colorValue = IECore . SplinefColor3f ( \n 
( 1 , IECore . Color3f ( .5 ) ) , \n 
n [ "parameters" ] [ "floatSpline" ] . setValue ( floatValue ) \n 
n [ "parameters" ] [ "colorSpline" ] . setValue ( colorValue ) \n 
s = n . state ( ) [ 0 ] \n 
self . assertEqual ( s . parameters [ "floatSpline" ] . value , floatValue ) \n 
self . assertEqual ( s . parameters [ "colorSpline" ] . value , colorValue ) \n 
~~ def testSplineParameterSerialisationKeepsExistingValues ( self ) : \n 
s [ "n" ] . loadShader ( shader ) \n 
s [ "n" ] [ "parameters" ] [ "floatSpline" ] . setValue ( \n 
s [ "n" ] [ "parameters" ] [ "floatSpline" ] . getValue ( ) , \n 
s2 = Gaffer . ScriptNode ( ) \n 
s2 . execute ( ss ) \n 
s2 [ "n" ] [ "parameters" ] [ "floatSpline" ] . getValue ( ) , \n 
~~ def testSplineParameterDefaultValueAnnotation ( self ) : \n 
n [ "parameters" ] [ "colorSpline2" ] . getValue ( ) , \n 
( 0 , IECore . Color3f ( 1 ) ) , \n 
( 0.5 , IECore . Color3f ( 1 , 0.5 , 0.25 ) ) , \n 
( 1 , IECore . Color3f ( 0 ) ) , \n 
~~ def testCoshadersInBox ( self ) : \n 
shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
s [ "shader" ] . loadShader ( shader ) \n 
s [ "coshader" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "coshader" ] . loadShader ( coshader ) \n 
s [ "shader" ] [ "parameters" ] [ "coshaderParameter" ] . setInput ( s [ "coshader" ] [ "out" ] ) \n 
b = Gaffer . Box . create ( s , Gaffer . StandardSet ( [ s [ "coshader" ] ] ) ) \n 
self . assertTrue ( s [ "shader" ] [ "parameters" ] [ "coshaderParameter" ] . getInput ( ) . parent ( ) . isSame ( b ) ) \n 
s = s [ "shader" ] . state ( ) \n 
~~ def testShaderInBoxWithExternalCoshader ( self ) : \n 
b = Gaffer . Box . create ( s , Gaffer . StandardSet ( [ s [ "shader" ] ] ) ) \n 
self . assertTrue ( b [ "shader" ] [ "parameters" ] [ "coshaderParameter" ] . getInput ( ) . parent ( ) . isSame ( b ) ) \n 
s = b [ "shader" ] . state ( ) \n 
~~ def testNumericTypeAnnotations ( self ) : \n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/numericTypeAnnotations.sl" ) \n 
self . assertTrue ( isinstance ( shaderNode [ "parameters" ] [ "floatParameter1" ] , Gaffer . FloatPlug ) ) \n 
self . assertTrue ( isinstance ( shaderNode [ "parameters" ] [ "floatParameter2" ] , Gaffer . FloatPlug ) ) \n 
self . assertTrue ( isinstance ( shaderNode [ "parameters" ] [ "intParameter" ] , Gaffer . IntPlug ) ) \n 
self . assertTrue ( isinstance ( shaderNode [ "parameters" ] [ "boolParameter" ] , Gaffer . BoolPlug ) ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "floatParameter1" ] . defaultValue ( ) , 1.25 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "floatParameter2" ] . defaultValue ( ) , 1.5 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "intParameter" ] . defaultValue ( ) , 10 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "boolParameter" ] . defaultValue ( ) , True ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "floatParameter1" ] . getValue ( ) , 1.25 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "floatParameter2" ] . getValue ( ) , 1.5 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "intParameter" ] . getValue ( ) , 10 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "boolParameter" ] . getValue ( ) , True ) \n 
~~ def testCoshaderTypeAnnotations ( self ) : \n 
coshaderType1 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderType1.sl" ) \n 
coshaderType1Node = GafferRenderMan . RenderManShader ( ) \n 
coshaderType1Node . loadShader ( coshaderType1 ) \n 
coshaderType2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderType2.sl" ) \n 
coshaderType2Node = GafferRenderMan . RenderManShader ( ) \n 
coshaderType2Node . loadShader ( coshaderType2 ) \n 
shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/typedCoshaderParameters.sl" ) shaderNode = GafferRenderMan . RenderManShader ( ) \n 
self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( coshaderNode [ "out" ] ) self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( coshaderType1Node [ "out" self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( coshaderType2Node [ "out" \n 
self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderParameterType1" ] . acceptsInput ( coshaderNode [ "out" self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameterType1" ] . acceptsInput ( coshaderType1Node self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderParameterType1" ] . acceptsInput ( coshaderType2Node \n 
self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderParameterType2" ] . acceptsInput ( coshaderNode [ "out" self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderParameterType2" ] . acceptsInput ( coshaderType1Node self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameterType2" ] . acceptsInput ( coshaderType2Node \n 
self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderArrayParameter" ] [ "coshaderArrayParameter0" ] . acceptsInput self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderArrayParameter" ] [ "coshaderArrayParameter0" ] . acceptsInput self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderArrayParameter" ] [ "coshaderArrayParameter0" ] . acceptsInput \n 
self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderArrayParameterType1" ] [ "coshaderArrayParameterType1_0" self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderArrayParameterType1" ] [ "coshaderArrayParameterType1_0" self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderArrayParameterType1" ] [ "coshaderArrayParameterType1_0" \n 
self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderArrayParameterType2" ] [ 0 ] . acceptsInput ( coshaderNode self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderArrayParameterType2" ] [ 0 ] . acceptsInput ( coshaderType1Node self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderArrayParameterType2" ] [ 0 ] . acceptsInput ( coshaderType2Node \n 
~~ def testMultipleCoshaderTypeAnnotations ( self ) : \n 
~~~ coshaderType1And2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderType1And2.sl" coshaderType1And2Node = GafferRenderMan . RenderManShader ( ) \n 
coshaderType1And2Node . loadShader ( coshaderType1And2 ) \n 
self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( coshaderType1And2Node self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameterType1" ] . acceptsInput ( coshaderType1And2Node self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameterType2" ] . acceptsInput ( coshaderType1And2Node self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderParameterType3" ] . acceptsInput ( coshaderType1And2Node \n 
~~ def testSplitCoshaderPassThrough ( self ) : \n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" ) S = GafferRenderMan . RenderManShader ( ) \n 
S . loadShader ( shader ) \n 
passThroughCoshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderWithPassThrough.sl" D = GafferRenderMan . RenderManShader ( ) \n 
D . loadShader ( passThroughCoshader ) \n 
C = GafferRenderMan . RenderManShader ( ) \n 
C . loadShader ( coshader ) \n 
S [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . setInput ( C [ "out" ] ) \n 
S [ "parameters" ] [ "fixedShaderArray" ] [ 1 ] . setInput ( D [ "out" ] ) \n 
D [ "parameters" ] [ "aColorIWillTint" ] . setInput ( C [ "out" ] ) \n 
h = S . stateHash ( ) \n 
s = S . state ( ) \n 
self . assertEqual ( s [ 2 ] . parameters [ "fixedShaderArray" ] , IECore . StringVectorData ( [ s [ 0 ] . parameters [ self . assertEqual ( s [ 0 ] . name , coshader ) \n 
D [ "enabled" ] . setValue ( False ) \n 
self . assertNotEqual ( S . stateHash ( ) , h ) \n 
self . assertEqual ( s [ 1 ] . parameters [ "fixedShaderArray" ] , IECore . StringVectorData ( [ s [ 0 ] . parameters [ self . assertEqual ( s [ 0 ] . name , coshader ) \n 
~~ def testSerialDisabledShaders ( self ) : \n 
S = GafferRenderMan . RenderManShader ( ) \n 
passThroughCoshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderWithPassThrough.sl" D1 = GafferRenderMan . RenderManShader ( ) \n 
D1 . loadShader ( passThroughCoshader ) \n 
D2 = GafferRenderMan . RenderManShader ( ) \n 
D2 . loadShader ( passThroughCoshader ) \n 
S [ "parameters" ] [ "coshaderParameter" ] . setInput ( D2 [ "out" ] ) \n 
D2 [ "parameters" ] [ "aColorIWillTint" ] . setInput ( D1 [ "out" ] ) \n 
D1 [ "parameters" ] [ "aColorIWillTint" ] . setInput ( C [ "out" ] ) \n 
h1 = S . stateHash ( ) \n 
self . assertEqual ( len ( s ) , 4 ) \n 
self . assertEqual ( s [ 2 ] . name , passThroughCoshader ) \n 
self . assertEqual ( s [ 3 ] . name , shader ) \n 
self . assertEqual ( s [ 3 ] . parameters [ "coshaderParameter" ] , s [ 2 ] . parameters [ "__handle" ] ) \n 
self . assertEqual ( s [ 2 ] . parameters [ "aColorIWillTint" ] , s [ 1 ] . parameters [ "__handle" ] ) \n 
D2 [ "enabled" ] . setValue ( False ) \n 
h2 = S . stateHash ( ) \n 
self . assertNotEqual ( h1 , h2 ) \n 
self . assertEqual ( s [ 2 ] . name , shader ) \n 
D1 [ "enabled" ] . setValue ( False ) \n 
h3 = S . stateHash ( ) \n 
~~ def testDynamicCoshaderArrayParameters ( self ) : \n 
shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" ) shaderNode = GafferRenderMan . RenderManShader ( ) \n 
self . assertEqual ( len ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] ) , 1 ) \n 
self . assertTrue ( isinstance ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] , Gaffer . Plug ) ) \n 
self . assertTrue ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] . getInput ( ) is None ) \n 
shaderNode [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] . setInput ( coshaderNode [ "out" ] ) \n 
self . assertEqual ( len ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] ) , 2 ) \n 
self . assertTrue ( isinstance ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] [ 1 ] , Gaffer . Plug ) ) \n 
self . assertTrue ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] . getInput ( ) . isSame ( coshaderNode self . assertTrue ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] [ 1 ] . getInput ( ) is None ) \n 
shaderNode [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] . setInput ( None ) \n 
~~ def testSerialiseDynamicCoshaderArrayParameters ( self ) : \n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" ) coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
s [ "c" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "c" ] . loadShader ( coshader ) \n 
s [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] . setInput ( s [ "c" ] [ "out" ] ) \n 
s [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 1 ] . setInput ( s [ "c" ] [ "out" ] ) \n 
s [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 2 ] . setInput ( s [ "c" ] [ "out" ] ) \n 
s [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 1 ] . setInput ( None ) \n 
self . assertEqual ( len ( s [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] ) , 4 ) \n 
s2 . execute ( s . serialise ( ) ) \n 
self . assertEqual ( len ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] ) , 4 ) \n 
self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] . getInput ( ) . isSame ( s2 [ "c" ] [ "out" ] self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 1 ] . getInput ( ) is None ) \n 
self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 2 ] . getInput ( ) . isSame ( s2 [ "c" ] [ "out" ] self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 3 ] . getInput ( ) is None ) \n 
s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 3 ] . setInput ( s2 [ "c" ] [ "out" ] ) \n 
self . assertEqual ( len ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] ) , 5 ) \n 
self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 2 ] . getInput ( ) . isSame ( s2 [ "c" ] [ "out" ] self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 3 ] . getInput ( ) . isSame ( s2 [ "c" ] [ "out" ] self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 4 ] . getInput ( ) is None ) \n 
~~ def testConvertFixedCoshaderArrayToDynamic ( self ) : \n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" ) shaderV2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParametersV2.sl" coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . setInput ( s [ "c" ] [ "out" ] ) \n 
self . assertTrue ( len ( s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] ) , 4 ) \n 
s [ "n" ] . loadShader ( shaderV2 , keepExistingValues = True ) \n 
self . assertTrue ( s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . getInput ( ) . isSame ( s [ "c" ] [ "out" ] ) ) \n 
self . assertTrue ( s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 1 ] . getInput ( ) is None ) \n 
s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . setInput ( None ) \n 
self . assertEqual ( len ( s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] ) , 1 ) \n 
self . assertTrue ( s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . getInput ( ) is None ) \n 
~~ def testConvertFixedCoshaderArrayToDynamicWithFirstPlugUnconnected ( self ) : \n 
s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 1 ] . setInput ( s [ "c" ] [ "out" ] ) \n 
self . assertTrue ( s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 1 ] . getInput ( ) . isSame ( s [ "c" ] [ "out" ] ) ) \n 
s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 1 ] . setInput ( None ) \n 
~~ def testConvertFixedCoshaderArrayToDynamicDuringLoading ( self ) : \n 
shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParametersV2.sl" \n 
self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 1 ] . getInput ( ) . isSame ( s2 [ "c" ] [ "out" ] ) self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . getInput ( ) is None ) \n 
s2 [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 1 ] . setInput ( None ) \n 
self . assertEqual ( len ( s2 [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] ) , 1 ) \n 
self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . getInput ( ) is None ) \n 
~~ def testHashThroughBox ( self ) : \n 
b = Gaffer . Box ( ) \n 
b . addChild ( Gaffer . Plug ( "in" ) ) \n 
b . addChild ( Gaffer . Plug ( "out" , direction = Gaffer . Plug . Direction . Out ) ) \n 
intermediateCoshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderWithPassThrough.sl" intermediateCoshaderNode = GafferRenderMan . RenderManShader ( ) \n 
intermediateCoshaderNode . loadShader ( intermediateCoshader ) \n 
b [ "in" ] . setInput ( coshaderNode [ "out" ] ) \n 
intermediateCoshaderNode [ "parameters" ] [ "aColorIWillTint" ] . setInput ( b [ "in" ] ) \n 
b [ "out" ] . setInput ( intermediateCoshaderNode [ "out" ] ) \n 
shaderNode [ "parameters" ] [ "coshaderParameter" ] . setInput ( b [ "out" ] ) \n 
~~ def testDanglingBoxConnection ( self ) : \n 
shaderNode1 = GafferRenderMan . RenderManShader ( ) \n 
shaderNode1 . loadShader ( shader ) \n 
shaderNode2 = GafferRenderMan . RenderManShader ( ) \n 
shaderNode2 . loadShader ( shader ) \n 
b [ "shader1" ] = shaderNode1 \n 
shaderNode1 [ "parameters" ] [ "coshaderParameter" ] . setInput ( b [ "in" ] ) \n 
shaderNode2 [ "parameters" ] [ "coshaderParameter" ] . setInput ( b [ "out" ] ) \n 
~~ def testUnconnectedCustomBoxInput ( self ) : \n 
~~~ class CustomBox ( Gaffer . Box ) : \n 
~~~ def __init__ ( self , name = "CustomBox" ) : \n 
~~~ Gaffer . Box . __init__ ( self , name ) \n 
~~ ~~ IECore . registerRunTimeTyped ( CustomBox ) \n 
b = CustomBox ( ) \n 
b [ "s" ] = GafferRenderMan . RenderManShader ( ) \n 
b [ "s" ] . loadShader ( shader ) \n 
b [ "in" ] = b [ "s" ] [ "parameters" ] [ "coshaderParameter" ] . createCounterpart ( "in" , Gaffer . Plug . Direction \n 
b [ "s" ] [ "parameters" ] [ "coshaderParameter" ] . setInput ( b [ "in" ] ) \n 
s = b [ "s" ] . state ( ) \n 
self . assertEqual ( s [ 0 ] . name , shader ) \n 
self . assertTrue ( b [ "s" ] [ "parameters" ] [ "coshaderParameter" ] . getInput ( ) . isSame ( b [ "in" ] ) ) \n 
c = GafferRenderMan . RenderManShader ( ) \n 
c . loadShader ( coshader ) \n 
self . assertTrue ( b [ "in" ] . acceptsInput ( c [ "out" ] ) ) \n 
b [ "in" ] . setInput ( c [ "out" ] ) \n 
n = Gaffer . Node ( ) \n 
n [ "out" ] = b [ "in" ] . createCounterpart ( "out" , Gaffer . Plug . Direction . Out ) \n 
self . assertFalse ( b [ "in" ] . acceptsInput ( n [ "out" ] ) ) \n 
self . assertRaises ( RuntimeError , b [ "in" ] . setInput , n [ "out" ] ) \n 
b [ "s" ] [ "parameters" ] [ "coshaderParameter" ] . setInput ( None ) \n 
self . assertTrue ( b [ "in" ] . acceptsInput ( n [ "out" ] ) ) \n 
b [ "in" ] . setInput ( n [ "out" ] ) \n 
self . assertTrue ( b [ "in" ] . getInput ( ) . isSame ( n [ "out" ] ) ) \n 
self . assertFalse ( b [ "s" ] [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( b [ "in" ] ) ) \n 
self . assertRaises ( RuntimeError , b [ "s" ] [ "parameters" ] [ "coshaderParameter" ] . setInput , b [ "in" ] ) \n 
~~ def testCoshaderSwitching ( self ) : \n 
coshaderNode0 = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode0 . loadShader ( coshader ) \n 
coshaderNode0 [ "parameters" ] [ "floatParameter" ] . setValue ( 0 ) \n 
coshaderNode1 [ "parameters" ] [ "floatParameter" ] . setValue ( 1 ) \n 
switch = GafferScene . ShaderSwitch ( ) \n 
switch [ "in" ] . setInput ( coshaderNode0 [ "out" ] ) \n 
switch [ "in1" ] . setInput ( coshaderNode1 [ "out" ] ) \n 
shaderNode [ "parameters" ] [ "coshaderParameter" ] . setInput ( switch [ "out" ] ) \n 
self . assertEqual ( shaderNode . state ( ) [ 0 ] . parameters [ "floatParameter" ] . value , 0 ) \n 
switch [ "index" ] . setValue ( 1 ) \n 
self . assertEqual ( shaderNode . state ( ) [ 0 ] . parameters [ "floatParameter" ] . value , 1 ) \n 
switch [ "enabled" ] . setValue ( False ) \n 
~~ def testCoshaderTypingPreventsNewInvalidSwitchInputs ( self ) : \n 
~~~ coshaderType1 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderType1.sl" ) \n 
shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/typedCoshaderParameters.sl" shaderNode = GafferRenderMan . RenderManShader ( ) \n 
switch [ "in" ] . setInput ( coshaderType1Node [ "out" ] ) \n 
shaderNode [ "parameters" ] [ "coshaderParameterType1" ] . setInput ( switch [ "out" ] ) \n 
self . assertFalse ( switch [ "in1" ] . acceptsInput ( coshaderType2Node [ "out" ] ) ) \n 
self . assertTrue ( switch [ "in1" ] . acceptsInput ( coshaderType1Node [ "out" ] ) ) \n 
~~ def testAcceptInputFromEmptySwitch ( self ) : \n 
self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( switch [ "out" ] ) ) \n 
~~ def testCoshaderSwitchingInBox ( self ) : \n 
script = Gaffer . ScriptNode ( ) \n 
script [ "coshaderNode0" ] = GafferRenderMan . RenderManShader ( ) \n 
script [ "coshaderNode0" ] . loadShader ( coshader ) \n 
script [ "coshaderNode1" ] = GafferRenderMan . RenderManShader ( ) \n 
script [ "coshaderNode1" ] . loadShader ( coshader ) \n 
script [ "coshaderNode0" ] [ "parameters" ] [ "floatParameter" ] . setValue ( 0 ) \n 
script [ "coshaderNode1" ] [ "parameters" ] [ "floatParameter" ] . setValue ( 1 ) \n 
script [ "shaderNode" ] = GafferRenderMan . RenderManShader ( ) \n 
script [ "shaderNode" ] . loadShader ( shader ) \n 
script [ "switch" ] = GafferScene . ShaderSwitch ( ) \n 
script [ "switch" ] [ "in" ] . setInput ( script [ "coshaderNode0" ] [ "out" ] ) \n 
script [ "switch" ] [ "in1" ] . setInput ( script [ "coshaderNode1" ] [ "out" ] ) \n 
script [ "shaderNode" ] [ "parameters" ] [ "coshaderParameter" ] . setInput ( script [ "switch" ] [ "out" ] ) \n 
self . assertEqual ( script [ "shaderNode" ] . state ( ) [ 0 ] . parameters [ "floatParameter" ] . value , 0 ) \n 
box = Gaffer . Box . create ( script , Gaffer . StandardSet ( script . children ( Gaffer . Node ) ) ) \n 
self . assertEqual ( box [ "shaderNode" ] . state ( ) [ 0 ] . parameters [ "floatParameter" ] . value , 0 ) \n 
promotedIndex = box . promotePlug ( box [ "switch" ] [ "index" ] ) \n 
promotedIndex . setValue ( 1 ) \n 
self . assertEqual ( box [ "shaderNode" ] . state ( ) [ 0 ] . parameters [ "floatParameter" ] . value , 1 ) \n 
~~ def testRepeatability ( self ) : \n 
~~~ s1 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
s2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
sn1 = GafferRenderMan . RenderManShader ( ) \n 
sn2 = GafferRenderMan . RenderManShader ( ) \n 
sn1 . loadShader ( s1 ) \n 
sn2 . loadShader ( s2 ) \n 
sn2 [ "parameters" ] [ "coshaderParameter" ] . setInput ( sn1 [ "out" ] ) \n 
self . assertEqual ( sn2 . stateHash ( ) , sn2 . stateHash ( ) ) \n 
self . assertEqual ( sn2 . state ( ) , sn2 . state ( ) ) \n 
~~ def testHandlesAreHumanReadable ( self ) : \n 
sn1 = GafferRenderMan . RenderManShader ( "Shader1" ) \n 
sn2 = GafferRenderMan . RenderManShader ( "Shader2" ) \n 
state = sn2 . state ( ) \n 
self . assertTrue ( "Shader1" in state [ 0 ] . parameters [ "__handle" ] . value ) \n 
~~ def testHandlesAreUniqueEvenIfNodeNamesArent ( self ) : \n 
s2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" ) \n 
script [ "in1" ] = GafferRenderMan . RenderManShader ( ) \n 
script [ "in1" ] . loadShader ( s1 ) \n 
script [ "in2" ] = GafferRenderMan . RenderManShader ( ) \n 
script [ "in2" ] . loadShader ( s1 ) \n 
script [ "shader" ] = GafferRenderMan . RenderManShader ( ) \n 
script [ "shader" ] . loadShader ( s2 ) \n 
script [ "shader" ] [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . setInput ( script [ "in1" ] [ "out" ] ) \n 
script [ "shader" ] [ "parameters" ] [ "fixedShaderArray" ] [ 1 ] . setInput ( script [ "in2" ] [ "out" ] ) \n 
box = Gaffer . Box . create ( script , Gaffer . StandardSet ( [ script [ "in1" ] ] ) ) \n 
box [ "in1" ] . setName ( "notUnique" ) \n 
script [ "in2" ] . setName ( "notUnique" ) \n 
state = script [ "shader" ] . state ( ) \n 
self . assertNotEqual ( state [ 0 ] . parameters [ "__handle" ] , state [ 1 ] . parameters [ "__handle" ] ) \n 
~~ def testShaderTypesInState ( self ) : \n 
state = shaderNode . state ( ) \n 
self . assertEqual ( state [ 0 ] . type , "ri:shader" ) \n 
self . assertEqual ( state [ 1 ] . type , "ri:surface" ) \n 
~~ def testAssignmentAttributeName ( self ) : \n 
~~~ p = GafferScene . Plane ( ) \n 
s = GafferRenderMan . RenderManShader ( ) \n 
a = GafferScene . ShaderAssignment ( ) \n 
a [ "in" ] . setInput ( p [ "out" ] ) \n 
a [ "shader" ] . setInput ( s [ "out" ] ) \n 
self . assertEqual ( a [ "out" ] . attributes ( "/plane" ) . keys ( ) , [ "ri:surface" ] ) \n 
~~ def testVolumeShader ( self ) : \n 
s . loadShader ( "fog" ) \n 
self . assertEqual ( s [ "type" ] . getValue ( ) , "ri:atmosphere" ) \n 
s [ "type" ] . setValue ( "ri:interior" ) \n 
s . loadShader ( "fog" , keepExistingValues = True ) \n 
self . assertEqual ( s [ "type" ] . getValue ( ) , "ri:interior" ) \n 
s . loadShader ( "fog" , keepExistingValues = False ) \n 
~~ def testInputAcceptanceFromDots ( self ) : \n 
dot = Gaffer . Dot ( ) \n 
dot . setup ( coshaderNode [ "out" ] ) \n 
self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( dot [ "out" ] ) ) \n 
~~ def testShaderTypeOverride ( self ) : \n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/shaderTypeOverride.sl" ) \n 
self . assertEqual ( shaderNode [ ] . getValue ( ) , "ri:overrideType" ) \n 
~~ def testReferencePromotedCoshader ( self ) : \n 
s [ "b" ] = Gaffer . Box ( ) \n 
s [ "b" ] [ "s" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "b" ] [ "s" ] . loadShader ( shader ) \n 
p = s [ "b" ] . promotePlug ( s [ "b" ] [ "s" ] [ "parameters" ] [ "coshaderParameter" ] ) \n 
p . setName ( "p" ) \n 
self . assertTrue ( s [ "b" ] [ "p" ] . acceptsInput ( s [ "c" ] [ "out" ] ) ) \n 
s [ "b" ] . exportForReference ( self . temporaryDirectory ( ) + "/test.grf" ) \n 
s [ "r" ] = Gaffer . Reference ( ) \n 
s [ "r" ] . load ( self . temporaryDirectory ( ) + "/test.grf" ) \n 
self . assertTrue ( s [ "r" ] [ "p" ] . acceptsInput ( s [ "c" ] [ "out" ] ) ) \n 
~~ def testLoadAndGIL ( self ) : \n 
~~~ script = Gaffer . ScriptNode ( ) \n 
script [ "plane" ] = GafferScene . Plane ( ) \n 
script [ "plane" ] [ "divisions" ] . setValue ( IECore . V2i ( 20 ) ) \n 
script [ "sphere" ] = GafferScene . Sphere ( ) \n 
script [ "expression" ] = Gaffer . Expression ( ) \n 
script [ "instancer" ] = GafferScene . Instancer ( ) \n 
script [ "instancer" ] [ "in" ] . setInput ( script [ "plane" ] [ "out" ] ) \n 
script [ "instancer" ] [ "instance" ] . setInput ( script [ "sphere" ] [ "out" ] ) \n 
script [ "instancer" ] [ "parent" ] . setValue ( "/plane" ) \n 
script [ "assignment" ] = GafferScene . ShaderAssignment ( ) \n 
script [ "assignment" ] [ "in" ] . setInput ( script [ "instancer" ] [ "out" ] ) \n 
script [ "assignment" ] [ "shader" ] . setInput ( script [ "shader" ] [ "out" ] ) \n 
traverseConnection = Gaffer . ScopedConnection ( GafferSceneTest . connectTraverseSceneToPlugDirtiedSignal script [ "shader" ] . loadShader ( "matte" ) \n 
class OpenGLRenderTest ( GafferSceneTest . SceneTestCase ) : \n 
~~~ self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + "/test.exr" ) ) \n 
s [ "plane" ] [ "transform" ] [ "translate" ] . setValue ( IECore . V3f ( 0 , 0 , - 5 ) ) \n 
s [ "image" ] = GafferImage . ImageReader ( ) \n 
s [ "image" ] [ "fileName" ] . setValue ( os . path . expandvars ( "$GAFFER_ROOT/python/GafferImageTest/images/checker.exr" \n 
s [ "shader" ] = GafferScene . OpenGLShader ( ) \n 
s [ "shader" ] . loadShader ( "Texture" ) \n 
s [ "shader" ] [ "parameters" ] [ "texture" ] . setInput ( s [ "image" ] [ "out" ] ) \n 
s [ "shader" ] [ "parameters" ] [ "mult" ] . setValue ( 1 ) \n 
s [ "shader" ] [ "parameters" ] [ "tint" ] . setValue ( IECore . Color4f ( 1 ) ) \n 
s [ "assignment" ] = GafferScene . ShaderAssignment ( ) \n 
s [ "assignment" ] [ "in" ] . setInput ( s [ "plane" ] [ "out" ] ) \n 
s [ "assignment" ] [ "shader" ] . setInput ( s [ "shader" ] [ "out" ] ) \n 
s [ "outputs" ] [ "in" ] . setInput ( s [ "assignment" ] [ "out" ] ) \n 
s [ "render" ] = GafferScene . OpenGLRender ( ) \n 
s [ "fileName" ] . setValue ( self . temporaryDirectory ( ) + "/test.gfr" ) \n 
self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + "/test.exr" ) ) \n 
i = IECore . EXRImageReader ( self . temporaryDirectory ( ) + "/test.exr" ) . read ( ) \n 
e = IECore . ImagePrimitiveEvaluator ( i ) \n 
r = e . createResult ( ) \n 
e . pointAtUV ( IECore . V2f ( 0.5 ) , r ) \n 
self . assertAlmostEqual ( r . floatPrimVar ( e . R ( ) ) , 0.666666 , 5 ) \n 
self . assertAlmostEqual ( r . floatPrimVar ( e . G ( ) ) , 0.666666 , 5 ) \n 
self . assertEqual ( r . floatPrimVar ( e . B ( ) ) , 0 ) \n 
~~ def testOutputDirectoryCreation ( self ) : \n 
s [ "variables" ] . addMember ( "renderDirectory" , self . temporaryDirectory ( ) + "/openGLRenderTest" ) \n 
self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + "/openGLRenderTest" ) ) \n 
self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + "/openGLRenderTest/test.0001.exr" ) \n 
s [ "fileName" ] . setValue ( "/tmp/test.gfr" ) \n 
~~ self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + "/openGLRenderTest" ) ) \n 
self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + "/openGLRenderTest/test.0001.exr" ) ) \n 
~~ def testHash ( self ) : \n 
~~~ c = Gaffer . Context ( ) \n 
c . setFrame ( 1 ) \n 
c2 = Gaffer . Context ( ) \n 
c2 . setFrame ( 2 ) \n 
s [ "outputs" ] . addOutput ( "beauty" , IECore . Display ( "$renderDirectory/test.####.exr" , "exr" , "rgba" , s [ "render" ] = GafferScene . OpenGLRender ( ) \n 
self . assertEqual ( s [ "render" ] . hash ( c ) , IECore . MurmurHash ( ) ) \n 
self . assertNotEqual ( s [ "render" ] . hash ( c ) , IECore . MurmurHash ( ) ) \n 
self . assertNotEqual ( s [ "render" ] . hash ( c ) , s [ "render" ] . hash ( c2 ) ) \n 
current = s [ "render" ] . hash ( c ) \n 
c [ "renderDirectory" ] = self . temporaryDirectory ( ) + "/openGLRenderTest" \n 
self . assertNotEqual ( s [ "render" ] . hash ( c ) , current ) \n 
c [ "renderDirectory" ] = self . temporaryDirectory ( ) + "/openGLRenderTest2" \n 
c [ "ui:something" ] = "alterTheUI" \n 
self . assertEqual ( s [ "render" ] . hash ( c ) , current ) \n 
~~ import unittest \n 
class SceneTimeWarpTest ( GafferSceneTest . SceneTestCase ) : \n 
~~~ def testConstruct ( self ) : \n 
s [ "n" ] = GafferScene . SceneTimeWarp ( ) \n 
self . assertEqual ( s [ "n" ] [ "speed" ] . getValue ( ) , 1 ) \n 
self . assertEqual ( s [ "n" ] [ "offset" ] . getValue ( ) , 0 ) \n 
~~ def testRunTimeTyped ( self ) : \n 
~~~ n = GafferScene . SceneTimeWarp ( ) \n 
self . failUnless ( n . isInstanceOf ( GafferScene . SceneTimeWarp . staticTypeId ( ) ) ) \n 
self . failUnless ( n . isInstanceOf ( GafferScene . SceneContextProcessor . staticTypeId ( ) ) ) \n 
self . failUnless ( n . isInstanceOf ( GafferScene . SceneProcessor . staticTypeId ( ) ) ) \n 
self . failUnless ( n . isInstanceOf ( GafferScene . SceneNode . staticTypeId ( ) ) ) \n 
self . failUnless ( n . isInstanceOf ( Gaffer . Node . staticTypeId ( ) ) ) \n 
baseTypeIds = IECore . RunTimeTyped . baseTypeIds ( n . typeId ( ) ) \n 
self . failUnless ( GafferScene . SceneContextProcessor . staticTypeId ( ) in baseTypeIds ) \n 
self . failUnless ( GafferScene . SceneProcessor . staticTypeId ( ) in baseTypeIds ) \n 
self . failUnless ( GafferScene . SceneNode . staticTypeId ( ) in baseTypeIds ) \n 
self . failUnless ( Gaffer . Node . staticTypeId ( ) in baseTypeIds ) \n 
~~ def testAffects ( self ) : \n 
c = GafferTest . CapturingSlot ( n . plugDirtiedSignal ( ) ) \n 
n [ "speed" ] . setValue ( 2 ) \n 
found = False \n 
for cc in c : \n 
~~~ if cc [ 0 ] . isSame ( n [ "out" ] ) : \n 
~~~ found = True \n 
~~ ~~ self . failUnless ( found ) \n 
del c [ : ] \n 
n [ "offset" ] . setValue ( 2 ) \n 
~~ def testNoExtraInputs ( self ) : \n 
n = GafferScene . SceneTimeWarp ( ) \n 
n [ "in" ] . setInput ( p [ "out" ] ) \n 
self . assertTrue ( "in1" not in n ) \n 
~~ import Gaffer \n 
GafferScene . Cube , \n 
"dimensions" : [ \n 
GafferScene . ObjectSource , \n 
"name" : [ \n 
"transform" : [ \n 
"layout:section" , "Transform" , \n 
"sets" : [ \n 
import functools \n 
GafferSceneUI . SceneView , \n 
"shadingMode" : [ \n 
"toolbarLayout:index" , 2 , \n 
"toolbarLayout:divider" , True , \n 
"plugValueWidget:type" , "GafferSceneUI.SceneViewToolbar._ShadingModePlugValueWidget" , \n 
"minimumExpansionDepth" : [ \n 
"plugValueWidget:type" , "GafferSceneUI.SceneViewToolbar._ExpansionPlugValueWidget" , \n 
"lookThrough" : [ \n 
"plugValueWidget:type" , "GafferSceneUI.SceneViewToolbar._LookThroughPlugValueWidget" , \n 
"toolbarLayout:label" , "" , \n 
"lookThrough.enabled" : [ \n 
"lookThrough.camera" : [ \n 
"grid" : [ \n 
"plugValueWidget:type" , "GafferSceneUI.SceneViewToolbar._GridPlugValueWidget" , \n 
"gnomon" : [ \n 
"plugValueWidget:type" , "" , \n 
class _ShadingModePlugValueWidget ( GafferUI . PlugValueWidget ) : \n 
~~~ def __init__ ( self , plug , parenting = None ) : \n 
~~~ menuButton = GafferUI . MenuButton ( \n 
image = "shading.png" , \n 
menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __menuDefinition ) ) , \n 
hasFrame = False , \n 
GafferUI . PlugValueWidget . __init__ ( self , menuButton , plug , parenting = parenting ) \n 
~~ def hasLabel ( self ) : \n 
~~ def _updateFromPlug ( self ) : \n 
~~ def __menuDefinition ( self ) : \n 
~~~ m = IECore . MenuDefinition ( ) \n 
currentName = self . getPlug ( ) . getValue ( ) \n 
for name in [ "" ] + GafferSceneUI . SceneView . registeredShadingModes ( ) : \n 
~~~ m . append ( \n 
"/" + name if name else "Default" , \n 
{ \n 
"checkBox" : name == currentName , \n 
"command" : functools . partial ( Gaffer . WeakMethod ( self . __setValue ) , name if name != currentName } \n 
if not name : \n 
~~~ m . append ( "/DefaultDivider" , { "divider" : True } ) \n 
~~ ~~ return m \n 
~~ def __setValue ( self , value , * unused ) : \n 
~~~ self . getPlug ( ) . setValue ( value ) \n 
~~ ~~ class _ExpansionPlugValueWidget ( GafferUI . PlugValueWidget ) : \n 
~~~ menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __menuDefinition ) ) \n 
menuButton = GafferUI . MenuButton ( menu = menu , image = "expansion.png" , hasFrame = False ) \n 
~~~ expandAll = bool ( self . getPlug ( ) . getValue ( ) ) \n 
m = IECore . MenuDefinition ( ) \n 
return m \n 
~~ def __toggleMinimumExpansionDepth ( self , * unused ) : \n 
~~~ self . getPlug ( ) . setValue ( 0 if self . getPlug ( ) . getValue ( ) else 999 ) \n 
~~ ~~ class _LookThroughPlugValueWidget ( GafferUI . PlugValueWidget ) : \n 
~~~ row = GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Horizontal ) \n 
GafferUI . PlugValueWidget . __init__ ( self , row , plug , parenting = parenting ) \n 
with row : \n 
~~~ self . __enabledWidget = GafferUI . BoolPlugValueWidget ( plug [ "enabled" ] , displayMode = GafferUI . BoolWidget self . __cameraWidget = GafferSceneUI . ScenePathPlugValueWidget ( \n 
plug [ "camera" ] , \n 
path = GafferScene . ScenePath ( \n 
plug . node ( ) [ "in" ] , \n 
plug . node ( ) . getContext ( ) , \n 
"/" , \n 
self . __cameraWidget . pathWidget ( ) . setFixedCharacterWidth ( 13 ) \n 
if hasattr ( self . __cameraWidget . pathWidget ( ) . _qtWidget ( ) , "setPlaceholderText" ) : \n 
~~ ~~ self . _updateFromPlug ( ) \n 
~~~ with self . getContext ( ) : \n 
~~~ self . __cameraWidget . setEnabled ( self . getPlug ( ) [ "enabled" ] . getValue ( ) ) \n 
~~ ~~ ~~ class _GridPlugValueWidget ( GafferUI . PlugValueWidget ) : \n 
menuButton = GafferUI . MenuButton ( menu = menu , image = "grid.png" , hasFrame = False ) \n 
m . append ( \n 
"checkBox" : self . getPlug ( ) [ "visible" ] . getValue ( ) , \n 
"command" : self . getPlug ( ) [ "visible" ] . setValue , \n 
"checkBox" : self . getPlug ( ) . node ( ) [ "gnomon" ] [ "visible" ] . getValue ( ) , \n 
"command" : self . getPlug ( ) . node ( ) [ "gnomon" ] [ "visible" ] . setValue , \n 
~~ ~~ import IECore \n 
class AddNode ( Gaffer . ComputeNode ) : \n 
~~~ def __init__ ( self , name = "AddNode" ) : \n 
~~~ Gaffer . ComputeNode . __init__ ( self , name ) \n 
p1 = Gaffer . IntPlug ( "op1" , Gaffer . Plug . Direction . In ) \n 
p2 = Gaffer . IntPlug ( "op2" , Gaffer . Plug . Direction . In ) \n 
self . addChild ( Gaffer . BoolPlug ( "enabled" , defaultValue = True ) ) \n 
self . addChild ( p1 ) \n 
self . addChild ( p2 ) \n 
p3 = Gaffer . IntPlug ( "sum" , Gaffer . Plug . Direction . Out ) \n 
self . addChild ( p3 ) \n 
self . numHashCalls = 0 \n 
self . numComputeCalls = 0 \n 
~~ def enabledPlug ( self ) : \n 
~~~ return self [ "enabled" ] \n 
~~ def correspondingInput ( self , output ) : \n 
~~~ if output . isSame ( self [ "sum" ] ) : \n 
~~~ return self [ "op1" ] \n 
~~ return Gaffer . ComputeNode . correspondingInput ( self , output ) \n 
~~ def affects ( self , input ) : \n 
~~~ outputs = Gaffer . ComputeNode . affects ( self , input ) \n 
if input . getName ( ) in ( "enabled" , "op1" , "op2" ) : \n 
~~~ outputs . append ( self . getChild ( "sum" ) ) \n 
~~ return outputs \n 
~~ def hash ( self , output , context , h ) : \n 
~~~ assert ( output . isSame ( self . getChild ( "sum" ) ) or plug . getFlags ( ) & plug . Flags . Dynamic ) \n 
self . getChild ( "enabled" ) . hash ( h ) \n 
self . getChild ( "op1" ) . hash ( h ) \n 
self . getChild ( "op2" ) . hash ( h ) \n 
self . numHashCalls += 1 \n 
~~ def compute ( self , plug , context ) : \n 
~~~ assert ( plug . isSame ( self . getChild ( "sum" ) ) or plug . getFlags ( ) & plug . Flags . Dynamic ) \n 
assert ( isinstance ( context , Gaffer . Context ) ) \n 
assert ( plug . settable ( ) ) \n 
assert ( not self [ "op1" ] . settable ( ) ) \n 
assert ( not self [ "op2" ] . settable ( ) ) \n 
if self [ "enabled" ] . getValue ( ) : \n 
~~~ plug . setValue ( self . getChild ( "op1" ) . getValue ( ) + self . getChild ( "op2" ) . getValue ( ) ) \n 
~~~ plug . setValue ( self . getChild ( "op1" ) . getValue ( ) ) \n 
~~ self . numComputeCalls += 1 \n 
~~ ~~ IECore . registerRunTimeTyped ( AddNode , typeName = "GafferTest::AddNode" ) \n 
from __future__ import with_statement \n 
import pwd \n 
import grp \n 
class FileSystemPathTest ( GafferTest . TestCase ) : \n 
~~~ p = Gaffer . FileSystemPath ( __file__ ) \n 
self . assert_ ( p . isValid ( ) ) \n 
self . assert_ ( p . isLeaf ( ) ) \n 
while len ( p ) : \n 
~~~ del p [ - 1 ] \n 
self . assert_ ( not p . isLeaf ( ) ) \n 
~~ ~~ def testIsLeaf ( self ) : \n 
~~~ path = Gaffer . FileSystemPath ( "/this/path/doesnt/exist" ) \n 
self . assert_ ( not path . isLeaf ( ) ) \n 
~~ def testConstructWithFilter ( self ) : \n 
self . failUnless ( p . getFilter ( ) is None ) \n 
f = Gaffer . FileNamePathFilter ( [ "*.exr" ] ) \n 
p = Gaffer . FileSystemPath ( __file__ , filter = f ) \n 
self . failUnless ( p . getFilter ( ) . isSame ( f ) ) \n 
~~ def testBrokenSymbolicLinks ( self ) : \n 
~~~ os . symlink ( self . temporaryDirectory ( ) + "/nonExistent" , self . temporaryDirectory ( ) + "/broken" ) \n 
d = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) ) \n 
c = d . children ( ) \n 
self . assertEqual ( len ( c ) , 1 ) \n 
l = c [ 0 ] \n 
self . assertEqual ( str ( l ) , self . temporaryDirectory ( ) + "/broken" ) \n 
self . assertEqual ( l . isValid ( ) , True ) \n 
info = l . info ( ) \n 
self . failUnless ( info is not None ) \n 
~~ def testSymLinkInfo ( self ) : \n 
~~~ with open ( self . temporaryDirectory ( ) + "/a" , "w" ) as f : \n 
~~~ f . write ( "AAAA" ) \n 
~~ os . symlink ( self . temporaryDirectory ( ) + "/a" , self . temporaryDirectory ( ) + "/l" ) \n 
a = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) + "/a" ) \n 
l = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) + "/l" ) \n 
aInfo = a . info ( ) \n 
self . assertEqual ( aInfo [ "fileSystem:size" ] , l . info ( ) [ "fileSystem:size" ] ) \n 
os . remove ( str ( a ) ) \n 
self . assertNotEqual ( aInfo [ "fileSystem:size" ] , l . info ( ) [ "fileSystem:size" ] ) \n 
~~ def testCopy ( self ) : \n 
~~~ p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) ) \n 
p2 = p . copy ( ) \n 
self . assertEqual ( p , p2 ) \n 
self . assertEqual ( str ( p ) , str ( p2 ) ) \n 
~~ def testEmptyPath ( self ) : \n 
~~~ p = Gaffer . FileSystemPath ( ) \n 
self . assertEqual ( str ( p ) , "" ) \n 
self . assertTrue ( p . isEmpty ( ) ) \n 
self . assertFalse ( p . isValid ( ) ) \n 
~~ def testRelativePath ( self ) : \n 
~~~ os . chdir ( self . temporaryDirectory ( ) ) \n 
with open ( self . temporaryDirectory ( ) + "/a" , "w" ) as f : \n 
~~ p = Gaffer . FileSystemPath ( "a" ) \n 
self . assertEqual ( str ( p ) , "a" ) \n 
self . assertFalse ( p . isEmpty ( ) ) \n 
self . assertTrue ( p . isValid ( ) ) \n 
p2 = Gaffer . FileSystemPath ( "nonexistent" ) \n 
self . assertEqual ( str ( p2 ) , "nonexistent" ) \n 
self . assertFalse ( p2 . isEmpty ( ) ) \n 
self . assertFalse ( p2 . isValid ( ) ) \n 
~~ def testRelativePathChildren ( self ) : \n 
os . mkdir ( "dir" ) \n 
with open ( self . temporaryDirectory ( ) + "/dir/a" , "w" ) as f : \n 
~~ p = Gaffer . FileSystemPath ( "dir" ) \n 
c = p . children ( ) \n 
self . assertEqual ( str ( c [ 0 ] ) , "dir/a" ) \n 
self . assertTrue ( c [ 0 ] . isValid ( ) ) \n 
~~ def testChildrenOfFile ( self ) : \n 
self . assertEqual ( p . children ( ) , [ ] ) \n 
~~ def testModificationTimes ( self ) : \n 
p . append ( "t" ) \n 
with open ( str ( p ) , "w" ) as f : \n 
~~ mt = p . property ( "fileSystem:modificationTime" ) \n 
self . assertTrue ( isinstance ( mt , datetime . datetime ) ) \n 
self . assertLess ( ( datetime . datetime . utcnow ( ) - mt ) . total_seconds ( ) , 2 ) \n 
time . sleep ( 1 ) \n 
~~~ f . write ( "BBBB" ) \n 
~~ def testOwner ( self ) : \n 
~~ o = p . property ( "fileSystem:owner" ) \n 
self . assertTrue ( isinstance ( o , str ) ) \n 
self . assertEqual ( o , pwd . getpwuid ( os . stat ( str ( p ) ) . st_uid ) . pw_name ) \n 
~~ def testGroup ( self ) : \n 
~~ g = p . property ( "fileSystem:group" ) \n 
self . assertTrue ( isinstance ( g , str ) ) \n 
self . assertEqual ( g , grp . getgrgid ( os . stat ( str ( p ) ) . st_gid ) . gr_name ) \n 
~~ def testPropertyNames ( self ) : \n 
a = p . propertyNames ( ) \n 
self . assertTrue ( isinstance ( a , list ) ) \n 
self . assertTrue ( "fileSystem:group" in a ) \n 
self . assertTrue ( "fileSystem:owner" in a ) \n 
self . assertTrue ( "fileSystem:modificationTime" in a ) \n 
self . assertTrue ( "fileSystem:size" in a ) \n 
self . assertTrue ( "fileSystem:frameRange" not in a ) \n 
p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) , includeSequences = True ) \n 
self . assertTrue ( "fileSystem:frameRange" in p . propertyNames ( ) ) \n 
~~ def testSequences ( self ) : \n 
~~~ os . mkdir ( self . temporaryDirectory ( ) + "/dir" ) \n 
for n in [ "singleFile.txt" , "a.001.txt" , "a.002.txt" , "a.004.txt" , "b.003.txt" ] : \n 
~~~ with open ( self . temporaryDirectory ( ) + "/" + n , "w" ) as f : \n 
~~ ~~ p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) , includeSequences = True ) \n 
self . assertTrue ( p . getIncludeSequences ( ) ) \n 
self . assertEqual ( len ( c ) , 8 ) \n 
s = sorted ( c , key = str ) \n 
self . assertEqual ( str ( s [ 0 ] ) , self . temporaryDirectory ( ) + "/a.###.txt" ) \n 
self . assertEqual ( str ( s [ 1 ] ) , self . temporaryDirectory ( ) + "/a.001.txt" ) \n 
self . assertEqual ( str ( s [ 2 ] ) , self . temporaryDirectory ( ) + "/a.002.txt" ) \n 
self . assertEqual ( str ( s [ 3 ] ) , self . temporaryDirectory ( ) + "/a.004.txt" ) \n 
self . assertEqual ( str ( s [ 4 ] ) , self . temporaryDirectory ( ) + "/b.###.txt" ) \n 
self . assertEqual ( str ( s [ 5 ] ) , self . temporaryDirectory ( ) + "/b.003.txt" ) \n 
self . assertEqual ( str ( s [ 6 ] ) , self . temporaryDirectory ( ) + "/dir" ) \n 
self . assertEqual ( str ( s [ 7 ] ) , self . temporaryDirectory ( ) + "/singleFile.txt" ) \n 
for x in s : \n 
~~~ self . assertTrue ( x . isValid ( ) ) \n 
if not os . path . isdir ( str ( x ) ) : \n 
~~~ self . assertTrue ( x . isLeaf ( ) ) \n 
~~ self . assertEqual ( x . property ( "fileSystem:owner" ) , pwd . getpwuid ( os . stat ( str ( p ) ) . st_uid ) . pw_name self . assertEqual ( x . property ( "fileSystem:group" ) , grp . getgrgid ( os . stat ( str ( p ) ) . st_gid ) . gr_name self . assertLess ( ( datetime . datetime . utcnow ( ) - x . property ( "fileSystem:modificationTime" ) ) . total_seconds if "###" not in str ( x ) : \n 
~~~ self . assertFalse ( x . isFileSequence ( ) ) \n 
self . assertEqual ( x . fileSequence ( ) , None ) \n 
self . assertEqual ( x . property ( "fileSystem:frameRange" ) , "" ) \n 
if os . path . isdir ( str ( x ) ) : \n 
~~~ self . assertEqual ( x . property ( "fileSystem:size" ) , 0 ) \n 
~~~ self . assertEqual ( x . property ( "fileSystem:size" ) , 4 ) \n 
~~ ~~ ~~ self . assertEqual ( s [ 0 ] . property ( "fileSystem:frameRange" ) , "1-2,4" ) \n 
self . assertTrue ( s [ 0 ] . isFileSequence ( ) ) \n 
self . assertTrue ( isinstance ( s [ 0 ] . fileSequence ( ) , IECore . FileSequence ) ) \n 
self . assertEqual ( s [ 0 ] . fileSequence ( ) , IECore . FileSequence ( str ( s [ 0 ] ) , IECore . frameListFromList ( [ self . assertEqual ( s [ 0 ] . property ( "fileSystem:size" ) , 4 * 3 ) \n 
self . assertEqual ( s [ 4 ] . property ( "fileSystem:frameRange" ) , "3" ) \n 
self . assertTrue ( s [ 4 ] . isFileSequence ( ) ) \n 
self . assertTrue ( isinstance ( s [ 4 ] . fileSequence ( ) , IECore . FileSequence ) ) \n 
self . assertEqual ( s [ 4 ] . fileSequence ( ) , IECore . FileSequence ( str ( s [ 4 ] ) , IECore . frameListFromList ( [ self . assertEqual ( s [ 4 ] . property ( "fileSystem:size" ) , 4 ) \n 
self . assertTrue ( p2 . getIncludeSequences ( ) ) \n 
self . assertEqual ( len ( p2 . children ( ) ) , 8 ) \n 
p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) , includeSequences = False ) \n 
self . assertFalse ( p . getIncludeSequences ( ) ) \n 
self . assertEqual ( len ( c ) , 6 ) \n 
self . assertEqual ( str ( s [ 0 ] ) , self . temporaryDirectory ( ) + "/a.001.txt" ) \n 
self . assertEqual ( str ( s [ 1 ] ) , self . temporaryDirectory ( ) + "/a.002.txt" ) \n 
self . assertEqual ( str ( s [ 2 ] ) , self . temporaryDirectory ( ) + "/a.004.txt" ) \n 
self . assertEqual ( str ( s [ 3 ] ) , self . temporaryDirectory ( ) + "/b.003.txt" ) \n 
self . assertEqual ( str ( s [ 4 ] ) , self . temporaryDirectory ( ) + "/dir" ) \n 
self . assertEqual ( str ( s [ 5 ] ) , self . temporaryDirectory ( ) + "/singleFile.txt" ) \n 
p . setIncludeSequences ( True ) \n 
~~ def setUp ( self ) : \n 
self . __originalCWD = os . getcwd ( ) \n 
~~~ GafferTest . TestCase . tearDown ( self ) \n 
os . chdir ( self . __originalCWD ) \n 
class SequencePathTest ( GafferTest . TestCase ) : \n 
~~~ def __dictPath ( self ) : \n 
~~~ dict = { } \n 
dict [ "dir" ] = { } \n 
~~~ dict [ "dir" ] [ f ] = 1 \n 
~~ return Gaffer . DictPath ( dict , "/" ) \n 
~~~ path = Gaffer . SequencePath ( self . __dictPath ( ) ) \n 
self . failUnless ( path . isValid ( ) ) \n 
self . failUnless ( not path . isLeaf ( ) ) \n 
path . append ( "dir" ) \n 
path [ 0 ] = "oops!" \n 
self . failIf ( path . isValid ( ) ) \n 
self . failIf ( path . isLeaf ( ) ) \n 
path [ : ] = [ "dir" ] \n 
children = path . children ( ) \n 
for child in children : \n 
~~~ self . failUnless ( isinstance ( child , Gaffer . SequencePath ) ) \n 
~~ self . assertEqual ( len ( children ) , 2 ) \n 
childrenStrings = [ str ( c ) for c in children ] \n 
~~ def testNonLeafChildren ( self ) : \n 
~~ self . assertEqual ( len ( children ) , 1 ) \n 
self . assertEqual ( str ( children [ 0 ] ) , "/dir" ) \n 
path2 = path . copy ( ) \n 
self . failUnless ( isinstance ( path2 , Gaffer . SequencePath ) ) \n 
self . assertEqual ( path [ : ] , path2 [ : ] ) \n 
self . failUnless ( path . getFilter ( ) is path2 . getFilter ( ) ) \n 
c = [ str ( p ) for p in path . children ( ) ] \n 
c2 = [ str ( p ) for p in path2 . children ( ) ] \n 
self . assertEqual ( c , c2 ) \n 
~~ def testInfo ( self ) : \n 
~~~ dictPath = self . __dictPath ( ) \n 
path = Gaffer . SequencePath ( dictPath ) \n 
self . assertEqual ( dictPath . info ( ) , path . info ( ) ) \n 
~~ def testInfoOfInvalidPath ( self ) : \n 
~~~ fp = Gaffer . FileSystemPath ( "/iSurelyDontExist" ) \n 
self . assertEqual ( fp . isValid ( ) , False ) \n 
self . assertEqual ( fp . info ( ) , None ) \n 
sp = Gaffer . SequencePath ( fp ) \n 
self . assertEqual ( sp . isValid ( ) , False ) \n 
self . assertEqual ( sp . info ( ) , None ) \n 
~~ def testFilter ( self ) : \n 
~~ def testIsEmpty ( self ) : \n 
path . setFromString ( "" ) \n 
self . assertTrue ( path . isEmpty ( ) ) \n 
self . assertTrue ( path2 . isEmpty ( ) ) \n 
~~ def testProperties ( self ) : \n 
self . assertEqual ( dictPath . propertyNames ( ) , path . propertyNames ( ) ) \n 
self . assertEqual ( dictPath . property ( "dict:value" ) , path . property ( "dict:value" ) ) \n 
~~ assert ( __name__ == "__main__" ) \n 
class CompoundDataPlugValueWidget ( GafferUI . PlugValueWidget ) : \n 
~~~ self . __column = GafferUI . ListContainer ( spacing = 6 ) \n 
GafferUI . PlugValueWidget . __init__ ( self , self . __column , plug , parenting = parenting ) \n 
with self . __column : \n 
~~~ self . __layout = GafferUI . PlugLayout ( plug ) \n 
with GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Horizontal ) as self . __editRow : \n 
~~~ GafferUI . Spacer ( IECore . V2i ( GafferUI . PlugWidget . labelWidth ( ) , 1 ) ) \n 
GafferUI . MenuButton ( \n 
image = "plus.png" , \n 
menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __addMenuDefinition ) ) \n 
GafferUI . Spacer ( IECore . V2i ( 1 ) , IECore . V2i ( 999999 , 1 ) , parenting = { "expand" : True } ) \n 
~~ def setPlug ( self , plug ) : \n 
~~~ GafferUI . PlugValueWidget . setPlug ( self , plug ) \n 
self . __layout = GafferUI . PlugLayout ( plug ) \n 
self . __column [ 0 ] = self . __layout \n 
~~ def setReadOnly ( self , readOnly ) : \n 
~~~ if readOnly == self . getReadOnly ( ) : \n 
~~ GafferUI . PlugValueWidget . setReadOnly ( self , readOnly ) \n 
self . __layout . setReadOnly ( readOnly ) \n 
~~ def childPlugValueWidget ( self , childPlug , lazy = True ) : \n 
~~~ return self . __layout . plugValueWidget ( childPlug , lazy ) \n 
~~~ editable = True \n 
if self . getPlug ( ) is not None : \n 
~~~ editable = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "compoundDataPlugValueWidget:editable" ) \n 
editable = editable if editable is not None else True \n 
~~ self . __editRow . setVisible ( editable ) \n 
~~ def __addMenuDefinition ( self ) : \n 
~~~ result = IECore . MenuDefinition ( ) \n 
result . append ( "/Add/Bool" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" , IECore result . append ( "/Add/Float" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" , result . append ( "/Add/Int" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" , IECore result . append ( "/Add/NumericDivider" , { "divider" : True } ) \n 
result . append ( "/Add/String" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" , result . append ( "/Add/StringDivider" , { "divider" : True } ) \n 
result . append ( "/Add/V2i" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" , IECore result . append ( "/Add/V3i" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" , IECore result . append ( "/Add/V2f" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" , IECore result . append ( "/Add/V3f" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" , IECore result . append ( "/Add/VectorDivider" , { "divider" : True } ) \n 
result . append ( "/Add/Color3f" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" result . append ( "/Add/Color4f" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" \n 
~~ def __addItem ( self , name , value ) : \n 
~~~ with Gaffer . UndoContext ( self . getPlug ( ) . ancestor ( Gaffer . ScriptNode . staticTypeId ( ) ) ) : \n 
~~~ self . getPlug ( ) . addOptionalMember ( name , value , enabled = True ) \n 
~~ ~~ ~~ class _MemberPlugValueWidget ( GafferUI . PlugValueWidget ) : \n 
~~~ def __init__ ( self , childPlug ) : \n 
~~~ self . __row = GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Horizontal , spacing = 4 ) \n 
GafferUI . PlugValueWidget . __init__ ( self , self . __row , childPlug ) \n 
if not childPlug . getFlags ( Gaffer . Plug . Flags . Dynamic ) : \n 
~~~ nameWidget = GafferUI . LabelPlugValueWidget ( \n 
childPlug , \n 
horizontalAlignment = GafferUI . Label . HorizontalAlignment . Right , \n 
verticalAlignment = GafferUI . Label . VerticalAlignment . Center , \n 
nameWidget . label ( ) . _qtWidget ( ) . setFixedWidth ( GafferUI . PlugWidget . labelWidth ( ) ) \n 
nameWidget . label ( ) . _qtWidget ( ) . setFixedHeight ( 20 ) \n 
~~~ nameWidget = GafferUI . StringPlugValueWidget ( childPlug [ "name" ] ) \n 
nameWidget . textWidget ( ) . _qtWidget ( ) . setFixedWidth ( GafferUI . PlugWidget . labelWidth ( ) ) \n 
~~ self . __row . append ( nameWidget , \n 
verticalAlignment = GafferUI . Label . VerticalAlignment . Top \n 
if "enabled" in childPlug : \n 
~~~ self . __row . append ( \n 
GafferUI . BoolPlugValueWidget ( \n 
childPlug [ "enabled" ] , \n 
displayMode = GafferUI . BoolWidget . DisplayMode . Switch \n 
verticalAlignment = GafferUI . Label . VerticalAlignment . Top , \n 
~~ self . __row . append ( GafferUI . PlugValueWidget . create ( childPlug [ "value" ] ) , expand = True ) \n 
self . _updateFromPlug ( ) \n 
if isinstance ( self . __row [ 0 ] , GafferUI . LabelPlugValueWidget ) : \n 
~~~ self . __row [ 0 ] . setPlug ( plug ) \n 
~~~ self . __row [ 0 ] . setPlug ( plug [ "name" ] ) \n 
~~ if "enabled" in plug : \n 
~~~ self . __row [ 1 ] . setPlug ( plug [ "enabled" ] ) \n 
~~ self . __row [ - 1 ] . setPlug ( plug [ "value" ] ) \n 
~~~ for w in self . __row : \n 
~~~ if w . getPlug ( ) . isSame ( childPlug ) : \n 
~~~ return w \n 
for w in self . __row : \n 
~~~ w . setReadOnly ( readOnly ) \n 
~~ ~~ def _updateFromPlug ( self ) : \n 
~~~ if "enabled" in self . getPlug ( ) : \n 
~~~ enabled = self . getPlug ( ) [ "enabled" ] . getValue ( ) \n 
~~ if isinstance ( self . __row [ 0 ] , GafferUI . StringPlugValueWidget ) : \n 
~~~ self . __row [ 0 ] . setEnabled ( enabled ) \n 
~~ self . __row [ - 1 ] . setEnabled ( enabled ) \n 
~~ ~~ ~~ GafferUI . PlugValueWidget . registerType ( Gaffer . CompoundDataPlug , CompoundDataPlugValueWidget ) \n 
GafferUI . PlugValueWidget . registerType ( Gaffer . CompoundDataPlug . MemberPlug , _MemberPlugValueWidget ) \n 
def __deletePlug ( plug ) : \n 
~~~ with Gaffer . UndoContext ( plug . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ plug . parent ( ) . removeChild ( plug ) \n 
~~ ~~ def __plugPopupMenu ( menuDefinition , plugValueWidget ) : \n 
~~~ plug = plugValueWidget . getPlug ( ) \n 
memberPlug = plug if isinstance ( plug , Gaffer . CompoundDataPlug . MemberPlug ) else None \n 
memberPlug = memberPlug if memberPlug is not None else plug . ancestor ( Gaffer . CompoundDataPlug . MemberPlug if memberPlug is None : \n 
~~ if not memberPlug . getFlags ( Gaffer . Plug . Flags . Dynamic ) : \n 
~~ menuDefinition . append ( "/DeleteDivider" , { "divider" : True } ) \n 
menuDefinition . append ( "/Delete" , { "command" : IECore . curry ( __deletePlug , memberPlug ) , "active" \n 
~~ __plugPopupMenuConnection = GafferUI . PlugValueWidget . popupMenuSignal ( ) . connect ( __plugPopupMenu ) \n 
class FileSystemPathPlugValueWidget ( GafferUI . PathPlugValueWidget ) : \n 
~~~ def __init__ ( self , plug , path = None , parenting = None ) : \n 
~~~ GafferUI . PathPlugValueWidget . __init__ ( \n 
plug , \n 
path , \n 
self . __plugMetadataChangedConnection = Gaffer . Metadata . plugValueChangedSignal ( ) . connect ( Gaffer . WeakMethod \n 
~~ def getToolTip ( self ) : \n 
~~~ result = GafferUI . PathPlugValueWidget . getToolTip ( self ) \n 
extensions = self . __extensions ( ) \n 
if extensions : \n 
~~ def _pathChooserDialogue ( self ) : \n 
~~~ dialogue = GafferUI . PathPlugValueWidget . _pathChooserDialogue ( self ) \n 
if Gaffer . Metadata . plugValue ( self . getPlug ( ) , "fileSystemPathPlugValueWidget:includeSequences" ) : \n 
~~~ columns = dialogue . pathChooserWidget ( ) . pathListingWidget ( ) . getColumns ( ) \n 
~~ return dialogue \n 
~~~ GafferUI . PathPlugValueWidget . _updateFromPlug ( self ) \n 
includeSequences = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "fileSystemPathPlugValueWidget:includeSequences" \n 
self . path ( ) . setFilter ( \n 
Gaffer . FileSystemPath . createStandardFilter ( \n 
self . __extensions ( ) , \n 
Gaffer . Metadata . plugValue ( self . getPlug ( ) , "fileSystemPathPlugValueWidget:extensionsLabel" ) or includeSequenceFilter = includeSequences , \n 
self . path ( ) . setIncludeSequences ( includeSequences ) \n 
~~ def _setPlugFromPath ( self , path ) : \n 
~~~ if Gaffer . Metadata . plugValue ( self . getPlug ( ) , "fileSystemPathPlugValueWidget:includeSequenceFrameRange" ~~~ sequence = path . fileSequence ( ) \n 
if sequence : \n 
~~~ self . getPlug ( ) . setValue ( str ( sequence ) ) \n 
~~ ~~ GafferUI . PathPlugValueWidget . _setPlugFromPath ( self , path ) \n 
~~ def __plugMetadataChanged ( self , nodeTypeId , plugPath , key , plug ) : \n 
~~~ if self . getPlug ( ) is None : \n 
~~ if plug is not None and not plug . isSame ( self . getPlug ( ) ) : \n 
~~ if not self . getPlug ( ) . node ( ) . isInstanceOf ( nodeTypeId ) : \n 
~~ if key . startswith ( "fileSystemPathPlugValueWidget:" ) : \n 
~~~ self . _updateFromPlug ( ) \n 
~~ ~~ def __extensions ( self ) : \n 
~~ extensions = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "fileSystemPathPlugValueWidget:extensions" if isinstance ( extensions , str ) : \n 
~~~ extensions = extensions . split ( ) \n 
~~~ extensions = list ( extensions ) \n 
~~ return extensions \n 
class NameLabel ( GafferUI . Label ) : \n 
~~~ def __init__ ( self , graphComponent , horizontalAlignment = GafferUI . Label . HorizontalAlignment . Left , verticalAlignment \n 
~~~ GafferUI . Label . __init__ ( self , "" , horizontalAlignment , verticalAlignment , parenting = parenting ) \n 
self . __formatter = formatter if formatter is not None else self . defaultFormatter \n 
self . __numComponents = numComponents \n 
self . __connections = [ ] \n 
self . setGraphComponent ( graphComponent ) \n 
self . __buttonPressConnection = self . buttonPressSignal ( ) . connect ( Gaffer . WeakMethod ( self . __buttonPress self . __dragBeginConnection = self . dragBeginSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragBegin ) self . __dragEndConnection = self . dragEndSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragEnd ) ) \n 
~~ def setText ( self , text ) : \n 
~~~ GafferUI . Label . setText ( self , text ) \n 
~~ def setGraphComponent ( self , graphComponent ) : \n 
~~~ if graphComponent is not None and self . __graphComponent is not False : \n 
~~~ if graphComponent . isSame ( self . __graphComponent ) : \n 
~~ ~~ elif self . __graphComponent is None : \n 
~~ self . __graphComponent = graphComponent \n 
self . __setupConnections ( ) \n 
self . __setText ( ) \n 
~~ def getGraphComponent ( self ) : \n 
~~~ return self . __graphComponent \n 
~~ def setNumComponents ( self , numComponents ) : \n 
~~~ assert ( numComponents > 0 ) \n 
if numComponents == self . __numComponents : \n 
~~ self . __numComponents = numComponents \n 
~~ def getNumComponents ( self ) : \n 
~~~ return self . __numComponents \n 
~~ def setFormatter ( self , formatter ) : \n 
~~~ self . __formatter = formatter \n 
~~ def getFormatter ( self ) : \n 
~~~ return self . __formatter \n 
def defaultFormatter ( graphComponents ) : \n 
~~~ return "." . join ( IECore . CamelCase . toSpaced ( g . getName ( ) ) for g in graphComponents ) \n 
~~ def __setupConnections ( self , reuseUntil = None ) : \n 
~~~ if self . __graphComponent is None : \n 
~~~ self . __connections = [ ] \n 
~~ updatedConnections = [ ] \n 
n = 0 \n 
g = self . __graphComponent \n 
reuse = reuseUntil is not None \n 
while g is not None and n < self . __numComponents : \n 
~~~ if reuse : \n 
~~~ updatedConnections . extend ( self . __connections [ n * 2 : n * 2 + 2 ] ) \n 
~~~ updatedConnections . append ( g . nameChangedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __setText ) ) if n < self . __numComponents - 1 : \n 
~~~ updatedConnections . append ( g . parentChangedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __parentChanged \n 
~~ ~~ if g . isSame ( reuseUntil ) : \n 
~~~ reuse = False \n 
~~ g = g . parent ( ) \n 
n += 1 \n 
~~ self . __connections = updatedConnections \n 
~~ def __parentChanged ( self , child , oldParent ) : \n 
~~~ self . __setText ( ) \n 
self . __setupConnections ( reuseUntil = child ) \n 
~~ def __setText ( self , * unwantedArgs ) : \n 
~~~ graphComponents = [ ] \n 
~~~ graphComponents . append ( g ) \n 
g = g . parent ( ) \n 
~~ graphComponents . reverse ( ) \n 
GafferUI . Label . setText ( self , self . __formatter ( graphComponents ) ) \n 
~~ def __buttonPress ( self , widget , event ) : \n 
~~~ return self . getGraphComponent ( ) is not None and event . buttons & ( event . Buttons . Left | event . Buttons \n 
~~ def __dragBegin ( self , widget , event ) : \n 
~~~ if event . buttons & ( event . Buttons . Left | event . Buttons . Middle ) : \n 
~~~ GafferUI . Pointer . setCurrent ( "nodes" ) \n 
return self . getGraphComponent ( ) \n 
~~ def __dragEnd ( self , widget , event ) : \n 
~~~ GafferUI . Pointer . setCurrent ( None ) \n 
~~ ~~ import functools \n 
class PresetsPlugValueWidget ( GafferUI . PlugValueWidget ) : \n 
~~~ self . __menuButton = GafferUI . MenuButton ( "" , menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __menuDefinition GafferUI . PlugValueWidget . __init__ ( self , self . __menuButton , plug , parenting = parenting ) \n 
self . _addPopupMenu ( self . __menuButton ) \n 
~~~ self . __menuButton . setEnabled ( self . _editable ( ) ) \n 
text = "" \n 
~~~ text = Gaffer . NodeAlgo . currentPreset ( self . getPlug ( ) ) or "Invalid" \n 
~~ ~~ self . __menuButton . setText ( text ) \n 
if self . getPlug ( ) is None : \n 
~~ currentPreset = Gaffer . NodeAlgo . currentPreset ( self . getPlug ( ) ) \n 
for n in Gaffer . NodeAlgo . presets ( self . getPlug ( ) ) : \n 
"/" + n , \n 
"command" : functools . partial ( Gaffer . WeakMethod ( self . __applyPreset ) , preset = n ) , \n 
"checkBox" : n == currentPreset , \n 
~~ def __applyPreset ( self , unused , preset ) : \n 
~~~ with Gaffer . UndoContext ( self . getPlug ( ) . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ Gaffer . NodeAlgo . applyPreset ( self . getPlug ( ) , preset ) \n 
~~ ~~ def __plugMetadataChanged ( self , nodeTypeId , plugPath , key , plug ) : \n 
~~ if key . startswith ( "preset:" ) : \n 
~~ ~~ ~~ import weakref \n 
import types \n 
import collections \n 
class UIEditor ( GafferUI . NodeSetEditor ) : \n 
~~~ def __init__ ( self , scriptNode , parenting = None ) : \n 
~~~ self . __frame = GafferUI . Frame ( borderWidth = 4 , borderStyle = GafferUI . Frame . BorderStyle . None ) \n 
GafferUI . NodeSetEditor . __init__ ( self , self . __frame , scriptNode , parenting = parenting ) \n 
###################################################################### \n 
self . __nodeMetadataWidgets = [ ] \n 
self . __plugMetadataWidgets = [ ] \n 
with self . __frame : \n 
~~~ self . __tabbedContainer = GafferUI . TabbedContainer ( ) \n 
~~ with self . __tabbedContainer : \n 
~~~ with GafferUI . ListContainer ( spacing = 4 , borderWidth = 8 , parenting = { "label" : "Node" } ) as \n 
~~~ with _Row ( ) : \n 
~~~ _Label ( "Name" ) \n 
self . __nodeNameWidget = GafferUI . NameWidget ( None ) \n 
~~ with _Row ( ) : \n 
~~~ _Label ( "Description" , parenting = { "verticalAlignment" : GafferUI . ListContainer . VerticalAlignment \n 
self . __nodeMetadataWidgets . append ( \n 
_MultiLineStringMetadataWidget ( key = "description" ) \n 
~~~ _Label ( "Color" ) \n 
_ColorSwatchMetadataWidget ( key = "nodeGadget:color" ) \n 
~~ ~~ with GafferUI . SplitContainer ( orientation = GafferUI . SplitContainer . Orientation . Horizontal , borderWidth \n 
~~~ self . __plugListing = _PlugListing ( ) \n 
self . __plugListingSelectionChangedConnection = self . __plugListing . selectionChangedSignal ( ) . connect \n 
with GafferUI . TabbedContainer ( ) as self . __plugAndSectionEditorsContainer : \n 
~~~ self . __plugEditor = _PlugEditor ( ) \n 
self . __sectionEditor = _SectionEditor ( ) \n 
self . __sectionEditorNameChangedConnection = self . __sectionEditor . nameChangedSignal ( ) . connect ( Gaffer \n 
~~ self . __plugAndSectionEditorsContainer . setTabsVisible ( False ) \n 
~~ self . __plugTab . setSizes ( [ 0.3 , 0.7 ] ) \n 
~~ self . __node = None \n 
self . __selectedPlug = None \n 
self . __updateFromSetInternal ( lazy = False ) \n 
~~ def setSelection ( self , selection ) : \n 
~~~ self . __plugListing . setSelection ( selection ) \n 
~~ def getSelection ( self ) : \n 
~~~ return self . __plugListing . getSelection ( ) \n 
~~ def nodeEditor ( self ) : \n 
~~~ return self . __nodeTab \n 
~~ def plugEditor ( self ) : \n 
~~~ return self . __plugTab \n 
def appendNodeContextMenuDefinitions ( cls , nodeGraph , node , menuDefinition ) : \n 
~~~ menuDefinition . append ( "/UIEditorDivider" , { "divider" : True } ) \n 
def appendNodeEditorToolMenuDefinitions ( cls , nodeEditor , node , menuDefinition ) : \n 
~~~ menuDefinition . append ( \n 
"command" : functools . partial ( GafferUI . UIEditor . acquire , node ) , \n 
"active" : isinstance ( node , Gaffer . Box ) or nodeEditor . nodeUI ( ) . plugValueWidget ( node [ "user" ] ) } \n 
~~ def _updateFromSet ( self ) : \n 
~~~ GafferUI . NodeSetEditor . _updateFromSet ( self ) \n 
self . __updateFromSetInternal ( ) \n 
~~ def __updateFromSetInternal ( self , lazy = True ) : \n 
~~~ node = self . _lastAddedNode ( ) \n 
if lazy and node == self . __node : \n 
~~ self . __node = node \n 
self . __nodeNameWidget . setGraphComponent ( self . __node ) \n 
self . __nodeTab . setEnabled ( self . __node is not None ) \n 
if self . __node is None : \n 
~~~ self . __plugListing . setPlugParent ( None ) \n 
self . __sectionEditor . setPlugParent ( None ) \n 
~~~ plugParent = self . __node [ "user" ] \n 
if isinstance ( self . __node , Gaffer . Box ) : \n 
~~~ plugParent = self . __node \n 
~~ self . __plugListing . setPlugParent ( plugParent ) \n 
self . __sectionEditor . setPlugParent ( plugParent ) \n 
~~ for widget in self . __nodeMetadataWidgets : \n 
~~~ widget . setTarget ( self . __node ) \n 
~~ self . setSelection ( None ) \n 
~~ def __plugListingSelectionChanged ( self , listing ) : \n 
~~~ selection = listing . getSelection ( ) \n 
if selection is None or isinstance ( selection , Gaffer . Plug ) : \n 
~~~ self . __plugEditor . setPlug ( selection ) \n 
self . __plugAndSectionEditorsContainer . setCurrent ( self . __plugEditor ) \n 
~~ elif isinstance ( selection , basestring ) : \n 
~~~ self . __plugEditor . setPlug ( None ) \n 
self . __sectionEditor . setSection ( selection ) \n 
self . __plugAndSectionEditorsContainer . setCurrent ( self . __sectionEditor ) \n 
~~ ~~ def __sectionEditorNameChanged ( self , sectionEditor , oldName , newName ) : \n 
~~~ self . __plugListing . setSelection ( newName ) \n 
def __setColor ( cls , menu , node ) : \n 
~~~ color = Gaffer . Metadata . nodeValue ( node , "nodeGadget:color" ) or IECore . Color3f ( 1 ) \n 
dialogue = GafferUI . ColorChooserDialogue ( color = color , useDisplayTransform = False ) \n 
color = dialogue . waitForColor ( parentWindow = menu . ancestor ( GafferUI . Window ) ) \n 
if color is not None : \n 
~~~ with Gaffer . UndoContext ( node . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ Gaffer . Metadata . registerNodeValue ( node , "nodeGadget:color" , color ) \n 
~~ ~~ ~~ ~~ GafferUI . EditorWidget . registerType ( "UIEditor" , UIEditor ) \n 
def __editPlugUI ( node , plug ) : \n 
~~~ editor = GafferUI . UIEditor . acquire ( node ) \n 
editor . setSelection ( plug ) \n 
editor . plugEditor ( ) . reveal ( ) \n 
~~ def __plugPopupMenu ( menuDefinition , plugValueWidget ) : \n 
node = plug . node ( ) \n 
if node is None : \n 
~~ if isinstance ( node , Gaffer . Box ) : \n 
~~~ if not plug . parent ( ) . isSame ( node ) : \n 
~~~ if not plug . parent ( ) . isSame ( node [ "user" ] ) : \n 
~~ ~~ menuDefinition . append ( "/EditUIDivider" , { "divider" : True } ) \n 
class _Label ( GafferUI . Label ) : \n 
~~~ def __init__ ( self , * args , ** kw ) : \n 
~~~ GafferUI . Label . __init__ ( \n 
* args , ** kw \n 
self . _qtWidget ( ) . setFixedWidth ( 110 ) \n 
~~ ~~ class _Row ( GafferUI . ListContainer ) : \n 
~~~ GafferUI . ListContainer . __init__ ( self , GafferUI . ListContainer . Orientation . Horizontal , spacing = 4 , \n 
~~ ~~ class _MetadataWidget ( GafferUI . Widget ) : \n 
~~~ def __init__ ( self , topLevelWidget , key , target = None , parenting = None ) : \n 
~~~ GafferUI . Widget . __init__ ( self , topLevelWidget , parenting = parenting ) \n 
self . __key = key \n 
self . __target = None \n 
self . setTarget ( target ) \n 
~~ def setTarget ( self , target ) : \n 
~~~ assert ( isinstance ( target , ( Gaffer . Node , Gaffer . Plug , type ( None ) ) ) ) \n 
self . __target = target \n 
self . setEnabled ( self . __target is not None ) \n 
if isinstance ( self . __target , Gaffer . Node ) : \n 
~~~ self . __metadataChangedConnection = Gaffer . Metadata . nodeValueChangedSignal ( ) . connect ( \n 
Gaffer . WeakMethod ( self . __nodeMetadataChanged ) \n 
~~ elif isinstance ( self . __target , Gaffer . Plug ) : \n 
~~~ self . __metadataChangedConnection = Gaffer . Metadata . plugValueChangedSignal ( ) . connect ( \n 
Gaffer . WeakMethod ( self . __plugMetadataChanged ) \n 
~~~ self . __metadataChangedConnection = None \n 
~~ self . __update ( ) \n 
~~ def getTarget ( self ) : \n 
~~~ return self . __target \n 
~~ def setKey ( self , key ) : \n 
~~~ if key == self . __key : \n 
~~ self . __key = key \n 
self . __update ( ) \n 
~~ def getKey ( self , key ) : \n 
~~~ return self . __key \n 
~~ def _updateFromValue ( self , value ) : \n 
~~~ raise NotImplementedError \n 
~~ def _updateFromWidget ( self , value ) : \n 
~~~ if self . __target is None : \n 
~~ with Gaffer . UndoContext ( self . __target . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ _registerMetadata ( self . __target , self . __key , value ) \n 
~~ ~~ def _deregisterValue ( self ) : \n 
~~~ _deregisterMetadata ( self . __target , self . __key ) \n 
~~ ~~ def __update ( self ) : \n 
~~~ if isinstance ( self . __target , Gaffer . Node ) : \n 
~~~ self . _updateFromValue ( Gaffer . Metadata . nodeValue ( self . __target , self . __key ) ) \n 
~~~ self . _updateFromValue ( Gaffer . Metadata . plugValue ( self . __target , self . __key ) ) \n 
~~~ self . _updateFromValue ( None ) \n 
~~ ~~ def __nodeMetadataChanged ( self , nodeTypeId , key , node ) : \n 
~~~ if self . __key != key : \n 
~~ if node is not None and not node . isSame ( self . __target ) : \n 
~~ if not self . __target . isInstanceOf ( nodeTypeId ) : \n 
~~ if plug is not None and not plug . isSame ( self . __target ) : \n 
~~ if not self . __target . node ( ) . isInstanceOf ( nodeTypeId ) : \n 
~~ if not Gaffer . match ( self . __target . relativeName ( self . __target . node ( ) ) , plugPath ) : \n 
~~ ~~ class _BoolMetadataWidget ( _MetadataWidget ) : \n 
~~~ def __init__ ( self , key , target = None , parenting = None ) : \n 
~~~ self . __boolWidget = GafferUI . BoolWidget ( ) \n 
_MetadataWidget . __init__ ( self , self . __boolWidget , key , target , parenting = parenting ) \n 
self . __stateChangedConnection = self . __boolWidget . stateChangedSignal ( ) . connect ( \n 
Gaffer . WeakMethod ( self . __stateChanged ) \n 
~~~ self . __boolWidget . setState ( value if value is not None else False ) \n 
~~ def __stateChanged ( self , * unused ) : \n 
~~~ self . _updateFromWidget ( self . __boolWidget . getState ( ) ) \n 
~~ ~~ class _StringMetadataWidget ( _MetadataWidget ) : \n 
~~~ def __init__ ( self , key , target = None , acceptEmptyString = True , parenting = None ) : \n 
~~~ self . __textWidget = GafferUI . TextWidget ( ) \n 
_MetadataWidget . __init__ ( self , self . __textWidget , key , target , parenting = None ) \n 
self . __acceptEmptyString = acceptEmptyString \n 
self . __editingFinishedConnection = self . __textWidget . editingFinishedSignal ( ) . connect ( \n 
Gaffer . WeakMethod ( self . __editingFinished ) \n 
~~ def textWidget ( self ) : \n 
~~~ return self . __textWidget \n 
~~~ self . __textWidget . setText ( value if value is not None else "" ) \n 
~~ def __editingFinished ( self , * unused ) : \n 
~~~ text = self . __textWidget . getText ( ) \n 
if text or self . __acceptEmptyString : \n 
~~~ self . _updateFromWidget ( text ) \n 
~~~ self . _deregisterValue ( ) \n 
~~ ~~ ~~ class _MultiLineStringMetadataWidget ( _MetadataWidget ) : \n 
~~~ self . __textWidget = GafferUI . MultiLineTextWidget ( ) \n 
~~~ self . _updateFromWidget ( self . __textWidget . getText ( ) ) \n 
~~ ~~ class _ColorSwatchMetadataWidget ( _MetadataWidget ) : \n 
~~~ self . __swatch = GafferUI . ColorSwatch ( useDisplayTransform = False ) \n 
_MetadataWidget . __init__ ( self , self . __swatch , key , target , parenting = parenting ) \n 
self . __swatch . _qtWidget ( ) . setFixedHeight ( 18 ) \n 
self . __swatch . _qtWidget ( ) . setMaximumWidth ( 40 ) \n 
self . __value = None \n 
self . __buttonReleaseConnection = self . __swatch . buttonReleaseSignal ( ) . connect ( Gaffer . WeakMethod ( self \n 
~~~ if value is not None : \n 
~~~ self . __swatch . setColor ( value ) \n 
~~~ self . __swatch . setColor ( IECore . Color4f ( 0 , 0 , 0 , 0 ) ) \n 
~~ self . __value = value \n 
~~ def __buttonRelease ( self , swatch , event ) : \n 
~~~ if event . button != event . Buttons . Left : \n 
~~ color = self . __value if self . __value is not None else IECore . Color3f ( 1 ) \n 
color = dialogue . waitForColor ( parentWindow = self . ancestor ( GafferUI . Window ) ) \n 
~~~ self . _updateFromWidget ( color ) \n 
~~ ~~ ~~ class _MenuMetadataWidget ( _MetadataWidget ) : \n 
~~~ def __init__ ( self , key , labelsAndValues , target = None , parenting = None ) : \n 
~~~ self . __menuButton = GafferUI . MenuButton ( \n 
menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __menuDefinition ) ) \n 
self . __labelsAndValues = labelsAndValues \n 
self . __currentValue = None \n 
_MetadataWidget . __init__ ( self , self . __menuButton , key , target , parenting = parenting ) \n 
~~~ self . __currentValue = value \n 
buttonText = str ( value ) \n 
for label , value in self . __labelsAndValues : \n 
~~~ if value == self . __currentValue : \n 
~~~ buttonText = label \n 
~~ ~~ self . __menuButton . setText ( buttonText ) \n 
"/" + label , \n 
"command" : functools . partial ( Gaffer . WeakMethod ( self . __setValue ) , value = value ) , \n 
"checkBox" : value == self . __currentValue \n 
~~ def __setValue ( self , unused , value ) : \n 
~~~ self . _updateFromWidget ( value ) \n 
~~ ~~ class _LayoutItem ( object ) : \n 
~~~ self . __parent = None \n 
self . __children = [ ] \n 
~~ def parent ( self ) : \n 
~~~ if self . __parent is None : \n 
~~~ return self . __parent ( ) \n 
~~ ~~ def child ( self , name ) : \n 
~~~ for c in self . __children : \n 
~~~ if c . name ( ) == name : \n 
~~~ return c \n 
~~ def isAncestorOf ( self , item ) : \n 
~~~ while item is not None : \n 
~~~ parent = item . parent ( ) \n 
if parent is self : \n 
~~ item = parent \n 
~~ def append ( self , child ) : \n 
~~~ self . insert ( len ( self ) , child ) \n 
~~ def insert ( self , index , child ) : \n 
~~~ assert ( child . parent ( ) is None ) \n 
self . __children . insert ( index , child ) \n 
child . __parent = weakref . ref ( self ) \n 
~~ def remove ( self , child ) : \n 
~~~ assert ( child . parent ( ) is self ) \n 
self . __children . remove ( child ) \n 
child . __parent = None \n 
~~ def index ( self , child ) : \n 
~~~ return self . __children . index ( child ) \n 
~~ def name ( self ) : \n 
~~ def fullName ( self ) : \n 
~~~ result = "" \n 
item = self \n 
while item . parent ( ) is not None : \n 
~~~ if result : \n 
~~~ result = item . name ( ) + "." + result \n 
~~~ result = item . name ( ) \n 
~~ item = item . parent ( ) \n 
~~~ return len ( self . __children ) \n 
~~ def __getitem__ ( self , index ) : \n 
~~~ return self . __children [ index ] \n 
~~ ~~ class _SectionLayoutItem ( _LayoutItem ) : \n 
~~~ def __init__ ( self , sectionName ) : \n 
~~~ _LayoutItem . __init__ ( self ) \n 
self . __sectionName = sectionName \n 
~~~ return self . __sectionName \n 
~~ ~~ class _PlugLayoutItem ( _LayoutItem ) : \n 
~~~ def __init__ ( self , plug ) : \n 
self . plug = plug \n 
self . __name = plug . getName ( ) \n 
~~~ return self . __name \n 
~~ ~~ class _PlugListing ( GafferUI . Widget ) : \n 
~~~ class __LayoutPath ( Gaffer . Path ) : \n 
~~~ def __init__ ( self , rootItem , path , root = "/" , filter = None ) : \n 
~~~ Gaffer . Path . __init__ ( self , path , root , filter ) \n 
self . __rootItem = rootItem \n 
~~ def rootItem ( self ) : \n 
~~~ return self . __rootItem \n 
~~ def item ( self ) : \n 
~~~ result = self . __rootItem \n 
for name in self : \n 
~~~ result = result . child ( name ) \n 
if result is None : \n 
~~~ return self . __class__ ( self . __rootItem , self [ : ] , self . root ( ) , self . getFilter ( ) ) \n 
~~ def isLeaf ( self ) : \n 
~~~ return not isinstance ( self . item ( ) , _SectionLayoutItem ) \n 
~~ def isValid ( self ) : \n 
~~~ return self . item ( ) is not None \n 
~~ def _children ( self ) : \n 
~~~ item = self . item ( ) \n 
if item is None : \n 
~~ result = [ \n 
self . __class__ ( self . __rootItem , self [ : ] + [ c . name ( ) ] , self . root ( ) , self . getFilter ( ) ) \n 
for c in item \n 
if len ( result ) == 0 and isinstance ( item , _SectionLayoutItem ) : \n 
~~ ~~ def __init__ ( self , parenting = None ) : \n 
~~~ column = GafferUI . ListContainer ( spacing = 4 ) \n 
GafferUI . Widget . __init__ ( self , column , parenting = parenting ) \n 
with column : \n 
~~~ self . __pathListing = GafferUI . PathListingWidget ( \n 
self . __LayoutPath ( _SectionLayoutItem ( "" ) , "/" ) , \n 
columns = ( GafferUI . PathListingWidget . defaultNameColumn , ) , \n 
displayMode = GafferUI . PathListingWidget . DisplayMode . Tree , \n 
self . __pathListing . setDragPointer ( "" ) \n 
self . __pathListing . setSortable ( False ) \n 
self . __pathListing . setHeaderVisible ( False ) \n 
with GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Horizontal , spacing = 4 ) : \n 
~~~ GafferUI . MenuButton ( \n 
menu = GafferUI . Menu ( \n 
definition = Gaffer . WeakMethod ( self . __addMenuDefinition ) \n 
self . __deleteButton = GafferUI . Button ( image = "minus.png" , hasFrame = False ) \n 
self . __deleteButtonClickedConnection = self . __deleteButton . clickedSignal ( ) . connect ( Gaffer . WeakMethod \n 
~~ ~~ self . __parent = None \n 
self . __dragItem = None \n 
self . __selectionChangedSignal = Gaffer . Signal1 ( ) \n 
self . __dragEnterConnection = self . __pathListing . dragEnterSignal ( ) . connect ( Gaffer . WeakMethod ( self self . __dragMoveConnection = self . __pathListing . dragMoveSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragMove self . __dragEndConnection = self . __pathListing . dragEndSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragEnd self . __selectionChangedConnection = self . __pathListing . selectionChangedSignal ( ) . connect ( Gaffer . WeakMethod self . __keyPressConnection = self . keyPressSignal ( ) . connect ( Gaffer . WeakMethod ( self . __keyPress ) ) \n 
self . __nodeMetadataChangedConnection = Gaffer . Metadata . nodeValueChangedSignal ( ) . connect ( Gaffer . WeakMethod self . __plugMetadataChangedConnection = Gaffer . Metadata . plugValueChangedSignal ( ) . connect ( Gaffer . WeakMethod \n 
~~ def setPlugParent ( self , parent ) : \n 
~~~ assert ( isinstance ( parent , ( Gaffer . Plug , Gaffer . Node , types . NoneType ) ) ) \n 
self . __parent = parent \n 
self . __childAddedConnection = None \n 
self . __childRemovedConnection = None \n 
self . __childNameChangedConnections = { } \n 
if self . __parent is not None : \n 
~~~ self . __childAddedConnection = self . __parent . childAddedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __childAddedOrRemoved self . __childRemovedConnection = self . __parent . childRemovedSignal ( ) . connect ( Gaffer . WeakMethod ( self for child in self . __parent . children ( ) : \n 
~~~ self . __updateChildNameChangedConnection ( child ) \n 
~~ ~~ self . __updatePath ( ) \n 
~~ def getPlugParent ( self ) : \n 
~~~ return self . __parent \n 
~~~ self . __updatePathLazily . flush ( self ) \n 
def findPlugPath ( path , plug ) : \n 
~~~ item = path . item ( ) \n 
if isinstance ( item , _PlugLayoutItem ) and item . plug . isSame ( plug ) : \n 
~~~ return path \n 
~~~ for child in path . children ( ) : \n 
~~~ r = findPlugPath ( child , plug ) \n 
if r is not None : \n 
~~~ return r \n 
~~ ~~ if isinstance ( selection , Gaffer . Plug ) : \n 
~~~ path = findPlugPath ( self . __pathListing . getPath ( ) , selection ) \n 
if path is None : \n 
~~~ self . __pathListing . setSelectedPaths ( [ ] ) \n 
~~~ self . __pathListing . setSelectedPaths ( [ path ] ) \n 
~~ ~~ elif isinstance ( selection , basestring ) : \n 
~~~ path = self . __pathListing . getPath ( ) . copy ( ) \n 
path [ : ] = selection . split ( "." ) \n 
self . __pathListing . setSelectedPaths ( [ path ] ) \n 
~~~ assert ( selection is None ) \n 
self . __pathListing . setSelectedPaths ( [ ] ) \n 
~~ ~~ def getSelection ( self ) : \n 
~~~ item = self . __selectedItem ( ) \n 
~~ elif isinstance ( item , _PlugLayoutItem ) : \n 
~~~ return item . plug \n 
~~ elif isinstance ( item , _SectionLayoutItem ) : \n 
~~~ return item . fullName ( ) \n 
~~ ~~ def selectionChangedSignal ( self ) : \n 
~~~ return self . __selectionChangedSignal \n 
~~ def __updatePath ( self ) : \n 
~~~ self . __pathListing . setPath ( self . __LayoutPath ( _SectionLayoutItem ( "" ) , "/" ) ) \n 
~~ def section ( rootLayoutItem , sectionPath ) : \n 
~~~ sectionItem = rootLayoutItem \n 
if sectionPath != "" : \n 
~~~ for sectionName in sectionPath . split ( "." ) : \n 
~~~ childSectionItem = sectionItem . child ( sectionName ) \n 
if childSectionItem is None : \n 
~~~ childSectionItem = _SectionLayoutItem ( sectionName ) \n 
sectionItem . append ( childSectionItem ) \n 
~~ sectionItem = childSectionItem \n 
~~ ~~ return sectionItem \n 
~~ layout = _SectionLayoutItem ( "" ) \n 
for sectionPath in GafferUI . PlugLayout . layoutSections ( self . __parent ) : \n 
~~~ if sectionPath == "User" and isinstance ( self . __parent , Gaffer . Node ) : \n 
~~ sectionItem = section ( layout , sectionPath ) \n 
for plug in GafferUI . PlugLayout . layoutOrder ( self . __parent , section = sectionPath ) : \n 
~~~ sectionItem . append ( _PlugLayoutItem ( plug ) ) \n 
~~ ~~ emptySections = _metadata ( self . getPlugParent ( ) , "uiEditor:emptySections" ) \n 
emptySectionIndices = _metadata ( self . getPlugParent ( ) , "uiEditor:emptySectionIndices" ) \n 
if emptySections and emptySectionIndices : \n 
~~~ for sectionPath , sectionIndex in zip ( emptySections , emptySectionIndices ) : \n 
~~~ parentPath , unused , sectionName = sectionPath . rpartition ( "." ) \n 
parentSection = section ( layout , parentPath ) \n 
if parentSection . child ( sectionName ) is None : \n 
~~~ parentSection . insert ( sectionIndex , _SectionLayoutItem ( sectionName ) ) \n 
~~ ~~ ~~ if len ( layout ) == 0 and isinstance ( self . __parent , Gaffer . Node ) : \n 
~~~ layout . append ( _SectionLayoutItem ( "Settings" ) ) \n 
~~ expandedPaths = self . __pathListing . getExpandedPaths ( ) \n 
self . __pathListing . setPath ( self . __LayoutPath ( layout , "/" ) ) \n 
self . __pathListing . setExpandedPaths ( expandedPaths ) \n 
~~ @ GafferUI . LazyMethod ( ) \n 
def __updatePathLazily ( self ) : \n 
~~~ self . __updatePath ( ) \n 
~~ def __updateMetadata ( self ) : \n 
~~~ emptySections = IECore . StringVectorData ( ) \n 
emptySectionIndices = IECore . IntVectorData ( ) \n 
def walk ( layoutItem , path = "" , index = 0 ) : \n 
~~~ for childItem in layoutItem : \n 
~~~ if isinstance ( childItem , _PlugLayoutItem ) : \n 
~~~ Gaffer . Metadata . registerPlugValue ( childItem . plug , "layout:section" , path ) \n 
Gaffer . Metadata . registerPlugValue ( childItem . plug , "layout:index" , index ) \n 
index += 1 \n 
~~ elif isinstance ( childItem , _SectionLayoutItem ) : \n 
~~~ childPath = path + "." + childItem . name ( ) if path else childItem . name ( ) \n 
if len ( childItem ) : \n 
~~~ index = walk ( childItem , childPath , index ) \n 
~~~ emptySections . append ( childPath ) \n 
emptySectionIndices . append ( layoutItem . index ( childItem ) ) \n 
~~ ~~ ~~ return index \n 
~~ with Gaffer . BlockedConnection ( self . __plugMetadataChangedConnection ) : \n 
~~~ walk ( self . __pathListing . getPath ( ) . copy ( ) . setFromString ( "/" ) . item ( ) ) \n 
_registerMetadata ( self . getPlugParent ( ) , "uiEditor:emptySections" , emptySections ) \n 
_registerMetadata ( self . getPlugParent ( ) , "uiEditor:emptySectionIndices" , emptySectionIndices ) \n 
~~ ~~ def __childAddedOrRemoved ( self , parent , child ) : \n 
~~~ assert ( parent . isSame ( self . __parent ) ) \n 
self . __updateChildNameChangedConnection ( child ) \n 
self . __updatePathLazily ( ) \n 
~~ def __childNameChanged ( self , child ) : \n 
~~~ selection = self . getSelection ( ) \n 
self . __updatePath ( ) \n 
if isinstance ( selection , Gaffer . Plug ) and child . isSame ( selection ) : \n 
~~~ self . setSelection ( selection ) \n 
~~ ~~ def __updateChildNameChangedConnection ( self , child ) : \n 
~~~ if self . __parent . isSame ( child . parent ( ) ) : \n 
~~~ if child not in self . __childNameChangedConnections : \n 
~~~ self . __childNameChangedConnections [ child ] = child . nameChangedSignal ( ) . connect ( Gaffer . WeakMethod ~~ ~~ else : \n 
~~~ if child in self . __childNameChangedConnections : \n 
~~~ del self . __childNameChangedConnections [ child ] \n 
~~ ~~ ~~ def __dragEnter ( self , listing , event ) : \n 
~~~ if event . sourceWidget is not self . __pathListing : \n 
~~ if not isinstance ( event . data , IECore . StringVectorData ) : \n 
~~ dragPath = self . __pathListing . getPath ( ) . copy ( ) . setFromString ( event . data [ 0 ] ) \n 
self . __dragItem = dragPath . item ( ) \n 
self . __pathListing . setPathExpanded ( dragPath , False ) \n 
~~ def __dragMove ( self , listing , event ) : \n 
~~~ if self . __dragItem is None : \n 
################################################# \n 
~~ targetPath = self . __pathListing . pathAt ( event . line . p0 ) \n 
if targetPath is not None : \n 
~~~ targetItem = targetPath . item ( ) \n 
if targetItem is not None : \n 
~~~ if isinstance ( targetItem , _SectionLayoutItem ) and self . __pathListing . getPathExpanded ( targetPath ~~~ newParent = targetItem \n 
newIndex = 0 \n 
~~~ newParent = targetItem . parent ( ) \n 
newIndex = newParent . index ( targetItem ) \n 
~~~ newParent = targetPath . copy ( ) . truncateUntilValid ( ) . item ( ) \n 
~~~ newParent = self . __pathListing . getPath ( ) . rootItem ( ) \n 
newIndex = 0 if event . line . p0 . y < 1 else len ( newParent ) \n 
~~ if newParent is self . __dragItem or self . __dragItem . isAncestorOf ( newParent ) : \n 
~~ firstNonPlugIndex = next ( \n 
( x [ 0 ] for x in enumerate ( newParent ) if not isinstance ( x [ 1 ] , _PlugLayoutItem ) ) , \n 
len ( newParent ) \n 
if self . __dragItem . parent ( ) is newParent and newParent . index ( self . __dragItem ) < firstNonPlugIndex ~~~ firstNonPlugIndex -= 1 \n 
~~ if isinstance ( self . __dragItem , _PlugLayoutItem ) : \n 
~~~ if newIndex > firstNonPlugIndex : \n 
~~~ if newIndex < firstNonPlugIndex : \n 
~~~ newIndex = max ( newIndex , firstNonPlugIndex ) \n 
~~ ~~ self . __dragItem . parent ( ) . remove ( self . __dragItem ) \n 
newParent . insert ( newIndex , self . __dragItem ) \n 
############################################################## \n 
self . __pathListing . getPath ( ) . pathChangedSignal ( ) ( self . __pathListing . getPath ( ) ) \n 
selection = self . __pathListing . getPath ( ) . copy ( ) \n 
selection [ : ] = self . __dragItem . fullName ( ) . split ( "." ) \n 
self . __pathListing . setSelectedPaths ( [ selection ] , scrollToFirst = False , expandNonLeaf = False ) \n 
~~ def __dragEnd ( self , listing , event ) : \n 
~~ with Gaffer . UndoContext ( self . __parent . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ self . __updateMetadata ( ) \n 
~~ self . __dragItem = None \n 
~~ def __selectionChanged ( self , pathListing ) : \n 
~~~ self . __deleteButton . setEnabled ( bool ( pathListing . getSelectedPaths ( ) ) ) \n 
self . __selectionChangedSignal ( self ) \n 
~~ def __deleteButtonClicked ( self , button ) : \n 
~~~ self . __deleteSelected ( ) \n 
~~ def __nodeMetadataChanged ( self , nodeTypeId , key , node ) : \n 
~~ if node is not None and not self . __parent . isSame ( node ) : \n 
~~ if not self . __parent . isInstanceOf ( nodeTypeId ) : \n 
~~ if key in ( "uiEditor:emptySections" , "uiEditor:emptySectionIndices" ) : \n 
~~~ self . __updatePathLazily ( ) \n 
~~ if plug is not None and not self . __parent . isSame ( plug ) and not self . __parent . isSame ( plug . parent ~~~ return \n 
~~ node = self . __parent . node ( ) if isinstance ( self . __parent , Gaffer . Plug ) else self . __parent \n 
if not node . isInstanceOf ( nodeTypeId ) : \n 
~~ if key in ( "layout:index" , "layout:section" , "uiEditor:emptySections" , "uiEditor:emptySectionIndices" ~~~ self . __updatePathLazily ( ) \n 
~~ ~~ def __keyPress ( self , widget , event ) : \n 
~~~ assert ( widget is self ) \n 
if event . key == "Backspace" or event . key == "Delete" : \n 
~~ def __addPlug ( self , plugType ) : \n 
~~~ plug = plugType ( flags = Gaffer . Plug . Flags . Default | Gaffer . Plug . Flags . Dynamic ) \n 
_registerMetadata ( plug , "nodule:type" , "" ) \n 
parentItem = self . __selectedItem ( ) \n 
if parentItem is not None : \n 
~~~ while not isinstance ( parentItem , _SectionLayoutItem ) : \n 
~~~ parentItem = parentItem . parent ( ) \n 
~~~ parentItem = self . __pathListing . getPath ( ) . rootItem ( ) \n 
parentItem = next ( \n 
( c for c in parentItem if isinstance ( c , _SectionLayoutItem ) ) , \n 
parentItem \n 
~~ _registerMetadata ( plug , "layout:section" , parentItem . fullName ( ) ) \n 
with Gaffer . UndoContext ( self . __parent . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ self . getPlugParent ( ) . addChild ( plug ) \n 
~~ self . __updatePathLazily . flush ( self ) \n 
self . setSelection ( plug ) \n 
~~ def __addSection ( self ) : \n 
~~~ rootItem = self . __pathListing . getPath ( ) . rootItem ( ) \n 
existingSectionNames = set ( c . name ( ) for c in rootItem if isinstance ( c , _SectionLayoutItem ) ) \n 
index = 1 \n 
while name in existingSectionNames : \n 
~~ rootItem . append ( _SectionLayoutItem ( name ) ) \n 
~~ self . __pathListing . setSelectedPaths ( \n 
self . __pathListing . getPath ( ) . copy ( ) . setFromString ( "/" + name ) \n 
~~ def __selectedItem ( self ) : \n 
~~~ selectedPaths = self . __pathListing . getSelectedPaths ( ) \n 
if not len ( selectedPaths ) : \n 
~~ assert ( len ( selectedPaths ) == 1 ) \n 
return selectedPaths [ 0 ] . item ( ) \n 
~~ def __deleteSelected ( self ) : \n 
~~~ selectedItem = self . __selectedItem ( ) \n 
if selectedItem is None : \n 
~~ selectedItem . parent ( ) . remove ( selectedItem ) \n 
def deletePlugsWalk ( item ) : \n 
~~~ if isinstance ( item , _PlugLayoutItem ) : \n 
~~~ item . plug . parent ( ) . removeChild ( item . plug ) \n 
~~~ for childItem in item : \n 
~~~ deletePlugsWalk ( childItem ) \n 
~~ ~~ ~~ with Gaffer . UndoContext ( self . __parent . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ deletePlugsWalk ( selectedItem ) \n 
self . __updateMetadata ( ) \n 
~~ ~~ ~~ class _PresetsEditor ( GafferUI . Widget ) : \n 
~~~ def __init__ ( self , parenting = None ) : \n 
~~~ row = GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Horizontal , spacing = 8 ) \n 
GafferUI . Widget . __init__ ( self , row , parenting = parenting ) \n 
~~~ with GafferUI . ListContainer ( spacing = 4 ) : \n 
Gaffer . DictPath ( collections . OrderedDict ( ) , "/" ) , \n 
self . __pathListing . _qtWidget ( ) . setFixedWidth ( 200 ) \n 
self . __pathListing . _qtWidget ( ) . setFixedHeight ( 200 ) \n 
self . __pathListingSelectionChangedConnection = self . __pathListing . selectionChangedSignal ( ) . connect self . __dragEnterConnection = self . __pathListing . dragEnterSignal ( ) . connect ( Gaffer . WeakMethod ( self self . __dragMoveConnection = self . __pathListing . dragMoveSignal ( ) . connect ( Gaffer . WeakMethod ( self self . __dragEndConnection = self . __pathListing . dragEndSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragEnd \n 
~~~ self . __addButton = GafferUI . Button ( image = "plus.png" , hasFrame = False ) \n 
self . __addButtonClickedConnection = self . __addButton . clickedSignal ( ) . connect ( Gaffer . WeakMethod \n 
~~ ~~ with GafferUI . ListContainer ( spacing = 4 ) as self . __editingColumn : \n 
~~~ GafferUI . Label ( "Name" ) \n 
self . __nameWidget = GafferUI . TextWidget ( ) \n 
self . __nameEditingFinishedConnection = self . __nameWidget . editingFinishedSignal ( ) . connect ( Gaffer \n 
GafferUI . Spacer ( IECore . V2i ( 4 ) , maximumSize = IECore . V2i ( 4 ) ) \n 
GafferUI . Label ( "Value" ) \n 
~~ ~~ self . __valueNode = Gaffer . Node ( "PresetEditor" ) \n 
self . __valuePlugSetConnection = self . __valueNode . plugSetSignal ( ) . connect ( Gaffer . WeakMethod ( self . \n 
~~~ self . __plug = plug \n 
self . __plugMetadataChangedConnection = None \n 
del self . __editingColumn [ 4 : ] \n 
plugValueWidget = None \n 
if self . __plug is not None : \n 
~~~ self . __plugMetadataChangedConnection = Gaffer . Metadata . plugValueChangedSignal ( ) . connect ( Gaffer . WeakMethod self . __valueNode [ "presetValue" ] = plug . createCounterpart ( "presetValue" , plug . Direction . In ) \n 
if hasattr ( self . __plug , "getValue" ) : \n 
~~~ plugValueWidget = GafferUI . PlugValueWidget . create ( self . __valueNode [ "presetValue" ] , useTypeOnly \n 
~~ ~~ self . __editingColumn . append ( plugValueWidget if plugValueWidget is not None else GafferUI . TextWidget \n 
self . __editingColumn . append ( GafferUI . Spacer ( IECore . V2i ( 0 ) , parenting = { "expand" : True } ) ) \n 
self . __addButton . setEnabled ( hasattr ( self . __plug , "getValue" ) ) \n 
~~ def getPlug ( self ) : \n 
~~~ return self . __plug \n 
~~~ d = self . __pathListing . getPath ( ) . dict ( ) \n 
d . clear ( ) \n 
~~~ for name in _registeredMetadata ( self . __plug , instanceOnly = True , persistentOnly = True ) : \n 
~~~ if name . startswith ( "preset:" ) : \n 
~~~ d [ name [ 7 : ] ] = _metadata ( self . __plug , name ) \n 
~~ ~~ ~~ self . __pathListing . getPath ( ) . pathChangedSignal ( ) ( self . __pathListing . getPath ( ) ) \n 
~~~ if plug is None or not plug . isSame ( self . __plug ) : \n 
~~ ~~ def __selectionChanged ( self , listing ) : \n 
~~~ selectedPaths = listing . getSelectedPaths ( ) \n 
self . __nameWidget . setText ( selectedPaths [ 0 ] [ 0 ] if selectedPaths else "" ) \n 
if selectedPaths : \n 
~~~ with Gaffer . BlockedConnection ( self . __valuePlugSetConnection ) : \n 
~~~ self . __valueNode [ "presetValue" ] . setValue ( \n 
Gaffer . Metadata . plugValue ( self . getPlug ( ) , "preset:" + selectedPaths [ 0 ] [ 0 ] ) \n 
~~ ~~ self . __editingColumn . setEnabled ( bool ( selectedPaths ) ) \n 
self . __deleteButton . setEnabled ( bool ( selectedPaths ) ) \n 
~~ def __dragEnter ( self , listing , event ) : \n 
srcPath = self . __pathListing . getPath ( ) . copy ( ) . setFromString ( event . data [ 0 ] ) \n 
srcIndex = d . keys ( ) . index ( srcPath [ 0 ] ) \n 
targetPath = self . __pathListing . pathAt ( event . line . p0 ) \n 
~~~ targetIndex = d . keys ( ) . index ( targetPath [ 0 ] ) \n 
~~~ targetIndex = 0 if event . line . p0 . y < 1 else len ( d ) \n 
~~ if srcIndex == targetIndex : \n 
~~ items = d . items ( ) \n 
item = items [ srcIndex ] \n 
del items [ srcIndex ] \n 
items . insert ( targetIndex , item ) \n 
d . update ( items ) \n 
with Gaffer . BlockedConnection ( self . __plugMetadataChangedConnection ) : \n 
~~~ for item in d . items ( ) : \n 
~~~ Gaffer . Metadata . deregisterPlugValue ( self . getPlug ( ) , "preset:" + item [ 0 ] ) \n 
~~ for item in d . items ( ) : \n 
~~~ Gaffer . Metadata . registerPlugValue ( self . getPlug ( ) , "preset:" + item [ 0 ] , item [ 1 ] ) \n 
~~ ~~ ~~ self . __updatePath ( ) \n 
~~ def __addButtonClicked ( self , button ) : \n 
~~~ existingNames = [ p [ 0 ] for p in self . __pathListing . getPath ( ) . children ( ) ] \n 
while name in existingNames : \n 
~~ with Gaffer . UndoContext ( self . __plug . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ Gaffer . Metadata . registerPlugValue ( self . __plug , "preset:" + name , self . __plug . getValue ( ) ) \n 
self . __nameWidget . grabFocus ( ) \n 
self . __nameWidget . setSelection ( 0 , len ( name ) ) \n 
~~~ paths = self . __pathListing . getPath ( ) . children ( ) \n 
selectedPreset = self . __pathListing . getSelectedPaths ( ) [ 0 ] [ 0 ] \n 
selectedIndex = [ p [ 0 ] for p in paths ] . index ( selectedPreset ) \n 
with Gaffer . UndoContext ( self . __plug . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ Gaffer . Metadata . deregisterPlugValue ( self . __plug , "preset:" + selectedPreset ) \n 
~~ del paths [ selectedIndex ] \n 
if len ( paths ) : \n 
~~~ self . __pathListing . setSelectedPaths ( [ paths [ min ( selectedIndex , len ( paths ) - 1 ) ] ] ) \n 
~~ def __nameEditingFinished ( self , nameWidget ) : \n 
~~ oldName = selectedPaths [ 0 ] [ 0 ] \n 
newName = nameWidget . getText ( ) \n 
items = self . __pathListing . getPath ( ) . dict ( ) . items ( ) \n 
~~~ for item in items : \n 
~~ for item in items : \n 
~~~ Gaffer . Metadata . registerPlugValue ( self . getPlug ( ) , "preset:" + ( item [ 0 ] if item [ 0 ] != oldName else \n 
self . __pathListing . setSelectedPaths ( [ self . __pathListing . getPath ( ) . copy ( ) . setFromString ( "/" + newName \n 
~~ def __valuePlugSet ( self , plug ) : \n 
~~~ if not plug . isSame ( self . __valueNode [ "presetValue" ] ) : \n 
~~ selectedPaths = self . __pathListing . getSelectedPaths ( ) \n 
preset = selectedPaths [ 0 ] [ 0 ] \n 
with Gaffer . UndoContext ( self . getPlug ( ) . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ Gaffer . Metadata . registerPlugValue ( self . getPlug ( ) , "preset:" + preset , plug . getValue ( ) ) \n 
~~ ~~ ~~ class _PlugEditor ( GafferUI . Widget ) : \n 
~~~ scrolledContainer = GafferUI . ScrolledContainer ( horizontalMode = GafferUI . ScrolledContainer . ScrollMode GafferUI . Widget . __init__ ( self , scrolledContainer , parenting = parenting ) \n 
self . __metadataWidgets = { } \n 
scrolledContainer . setChild ( GafferUI . ListContainer ( spacing = 4 ) ) \n 
with scrolledContainer . getChild ( ) : \n 
self . __nameWidget = GafferUI . NameWidget ( None ) \n 
~~~ _Label ( "Label" ) \n 
self . __metadataWidgets [ "label" ] = _StringMetadataWidget ( key = "label" , acceptEmptyString = False \n 
~~~ _Label ( "Description" , parenting = { "verticalAlignment" : GafferUI . ListContainer . VerticalAlignment self . __metadataWidgets [ "description" ] = _MultiLineStringMetadataWidget ( key = "description" ) \n 
self . __metadataWidgets [ "description" ] . textWidget ( ) . setFixedLineHeight ( 10 ) \n 
~~~ _Label ( "Widget" ) \n 
self . __widgetMenu = GafferUI . MenuButton ( \n 
menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __widgetMenuDefinition ) ) \n 
~~ with GafferUI . Collapsible ( "Presets" , collapsed = True ) : \n 
~~~ _Label ( "" ) \n 
self . __presetsEditor = _PresetsEditor ( ) \n 
~~~ _Label ( "Divider" ) \n 
self . __metadataWidgets [ "divider" ] = _BoolMetadataWidget ( key = "divider" ) \n 
~~ for m in self . __metadataDefinitions : \n 
~~~ _Label ( m . label ) \n 
self . __metadataWidgets [ m . key ] = m . metadataWidgetType ( key = m . key ) \n 
~~~ with GafferUI . ListContainer ( spacing = 4 ) as self . __nodeGraphSection : \n 
~~~ _Label ( "Gadget" ) \n 
self . __gadgetMenu = GafferUI . MenuButton ( \n 
menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __gadgetMenuDefinition ) ) \n 
~~~ _Label ( "Position" ) \n 
self . __metadataWidgets [ "nodeGadget:nodulePosition" ] = _MenuMetadataWidget ( \n 
key = "nodeGadget:nodulePosition" , \n 
labelsAndValues = [ \n 
( "Default" , None ) , \n 
( "Top" , "top" ) , \n 
( "Bottom" , "bottom" ) , \n 
( "Left" , "left" ) , \n 
( "Right" , "right" ) , \n 
self . __metadataWidgets [ "nodule:color" ] = _ColorSwatchMetadataWidget ( key = "nodule:color" ) \n 
self . __metadataWidgets [ "connectionGadget:color" ] = _ColorSwatchMetadataWidget ( key = "connectionGadget:color" \n 
~~ ~~ ~~ GafferUI . Spacer ( IECore . V2i ( 0 ) , parenting = { "expand" : True } ) \n 
~~ self . __plugMetadataChangedConnection = Gaffer . Metadata . plugValueChangedSignal ( ) . connect ( Gaffer . WeakMethod \n 
self . __plug = None \n 
self . __nameWidget . setGraphComponent ( self . __plug ) \n 
for widget in self . __metadataWidgets . values ( ) : \n 
~~~ widget . setTarget ( self . __plug ) \n 
~~ self . __updateWidgetMenuText ( ) \n 
self . __updateWidgetSettings ( ) \n 
self . __updateGadgetMenuText ( ) \n 
self . __presetsEditor . setPlug ( plug ) \n 
self . __nodeGraphSection . setEnabled ( self . __plug is not None and self . __plug . parent ( ) . isSame ( self . \n 
self . setEnabled ( self . __plug is not None ) \n 
~~ if key == "plugValueWidget:type" : \n 
~~~ self . __updateWidgetMenuText ( ) \n 
~~ elif key == "nodule:type" : \n 
~~~ self . __updateGadgetMenuText ( ) \n 
~~ ~~ def __updateWidgetMenuText ( self ) : \n 
~~~ self . __widgetMenu . setText ( "" ) \n 
~~ metadata = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "plugValueWidget:type" ) \n 
for w in self . __widgetDefinitions : \n 
~~~ if w . metadata == metadata : \n 
~~~ self . __widgetMenu . setText ( w . label ) \n 
~~ ~~ self . __widgetMenu . setText ( metadata ) \n 
~~ def __updateWidgetSettings ( self ) : \n 
~~~ widgetType = None \n 
~~~ widgetType = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "plugValueWidget:type" ) \n 
~~~ widget = self . __metadataWidgets [ m . key ] \n 
widget . parent ( ) . setEnabled ( m . plugValueWidgetType == widgetType ) \n 
~~ self . __metadataWidgets [ "connectionGadget:color" ] . parent ( ) . setEnabled ( \n 
self . getPlug ( ) is not None and self . getPlug ( ) . direction ( ) == Gaffer . Plug . Direction . In \n 
~~ def __widgetMenuDefinition ( self ) : \n 
~~~ if not isinstance ( self . getPlug ( ) , w . plugType ) : \n 
"/" + w . label , \n 
"command" : functools . partial ( Gaffer . WeakMethod ( self . __registerOrDeregisterMetadata ) , key = "checkBox" : metadata == w . metadata , \n 
~~ def __updateGadgetMenuText ( self ) : \n 
~~~ self . __gadgetMenu . setText ( "" ) \n 
~~ metadata = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "nodule:type" ) \n 
metadata = None if metadata == "GafferUI::StandardNodule" else metadata \n 
for g in self . __gadgetDefinitions : \n 
~~~ if g . metadata == metadata : \n 
~~~ self . __gadgetMenu . setText ( g . label ) \n 
~~ ~~ self . __gadgetMenu . setText ( metadata ) \n 
~~ def __gadgetMenuDefinition ( self ) : \n 
~~~ if not isinstance ( self . getPlug ( ) , g . plugType ) : \n 
"/" + g . label , \n 
"command" : functools . partial ( Gaffer . WeakMethod ( self . __registerOrDeregisterMetadata ) , key = "checkBox" : metadata == g . metadata , \n 
~~ def __registerOrDeregisterMetadata ( self , unused , key , value ) : \n 
~~~ Gaffer . Metadata . registerPlugValue ( self . getPlug ( ) , key , value ) \n 
~~~ Gaffer . Metadata . deregisterPlugValue ( self . getPlug ( ) , key ) \n 
~~ ~~ ~~ __WidgetDefinition = collections . namedtuple ( "WidgetDefinition" , ( "label" , "plugType" , "metadata" __widgetDefinitions = ( \n 
__WidgetDefinition ( "Default" , Gaffer . Plug , None ) , \n 
__WidgetDefinition ( "Checkbox" , Gaffer . IntPlug , "GafferUI.BoolPlugValueWidget" ) , \n 
__WidgetDefinition ( "Connection" , Gaffer . Plug , "GafferUI.ConnectionPlugValueWidget" ) , \n 
__WidgetDefinition ( "None" , Gaffer . Plug , "" ) , \n 
__MetadataDefinition = collections . namedtuple ( "MetadataDefinition" , ( "key" , "label" , "metadataWidgetType" __metadataDefinitions = ( \n 
__GadgetDefinition = collections . namedtuple ( "GadgetDefinition" , ( "label" , "plugType" , "metadata" __gadgetDefinitions = ( \n 
__GadgetDefinition ( "Default" , Gaffer . Plug , None ) , \n 
__GadgetDefinition ( "Array" , Gaffer . ArrayPlug , "GafferUI::CompoundNodule" ) , \n 
__GadgetDefinition ( "None" , Gaffer . Plug , "" ) , \n 
~~ class _SectionEditor ( GafferUI . Widget ) : \n 
~~~ column = GafferUI . ListContainer ( spacing = 4 , borderWidth = 8 ) \n 
self . __nameWidgetEditingFinishedConnection = self . __nameWidget . editingFinishedSignal ( ) . connect ( \n 
~~~ _Label ( "Summary" , parenting = { "verticalAlignment" : GafferUI . ListContainer . VerticalAlignment . \n 
self . __summaryMetadataWidget = _MultiLineStringMetadataWidget ( key = "" ) \n 
~~ ~~ self . __section = "" \n 
self . __plugParent = None \n 
self . __nameChangedSignal = Gaffer . Signal3 ( ) \n 
~~ def setPlugParent ( self , plugParent ) : \n 
~~~ self . __plugParent = plugParent \n 
self . __summaryMetadataWidget . setTarget ( self . __plugParent ) \n 
~~~ return self . __plugParent \n 
~~ def setSection ( self , section ) : \n 
~~~ assert ( isinstance ( section , basestring ) ) \n 
self . __section = section \n 
self . __nameWidget . setText ( section . rpartition ( "." ) [ - 1 ] ) \n 
self . __summaryMetadataWidget . setKey ( "layout:section:" + self . __section + ":summary" ) \n 
~~ def getSection ( self ) : \n 
~~~ return self . __section \n 
~~ def nameChangedSignal ( self ) : \n 
~~~ return self . __nameChangedSignal \n 
~~ def __nameWidgetEditingFinished ( self , nameWidget ) : \n 
~~~ if nameWidget . getText ( ) == "" : \n 
~~~ self . setSection ( self . __section ) \n 
~~ oldSectionPath = self . __section . split ( "." ) \n 
newSectionPath = oldSectionPath [ : ] \n 
newSectionPath [ - 1 ] = nameWidget . getText ( ) . replace ( "." , "" ) \n 
if oldSectionPath == newSectionPath : \n 
~~ def newSection ( oldSection ) : \n 
~~~ s = oldSection . split ( "." ) \n 
if s [ : len ( oldSectionPath ) ] == oldSectionPath : \n 
~~~ s [ : len ( oldSectionPath ) ] = newSectionPath \n 
return "." . join ( s ) \n 
~~~ return oldSection \n 
~~ ~~ with Gaffer . UndoContext ( self . __plugParent . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ for plug in self . __plugParent . children ( Gaffer . Plug ) : \n 
~~~ s = _metadata ( plug , "layout:section" ) \n 
if s is not None : \n 
~~~ _registerMetadata ( plug , "layout:section" , newSection ( s ) ) \n 
if emptySections : \n 
~~~ for i in range ( 0 , len ( emptySections ) ) : \n 
~~~ emptySections [ i ] = newSection ( emptySections [ i ] ) \n 
~~ _registerMetadata ( self . getPlugParent ( ) , "uiEditor:emptySections" , emptySections ) \n 
~~ for name in _registeredMetadata ( self . getPlugParent ( ) , instanceOnly = True , persistentOnly = True ~~~ m = re . match ( "(layout:section:)(.*)(:.*)" , name ) \n 
if m : \n 
~~~ if newSection ( m . group ( 2 ) ) != m . group ( 2 ) : \n 
~~~ _registerMetadata ( \n 
self . getPlugParent ( ) , \n 
m . group ( 1 ) + newSection ( m . group ( 2 ) ) + m . group ( 3 ) , \n 
_metadata ( self . getPlugParent ( ) , name ) \n 
_deregisterMetadata ( self . getPlugParent ( ) , name ) \n 
~~ ~~ ~~ ~~ self . setSection ( "." . join ( newSectionPath ) ) \n 
self . nameChangedSignal ( ) ( self , "." . join ( oldSectionPath ) , "." . join ( newSectionPath ) ) \n 
~~ ~~ def _registerMetadata ( target , name , value ) : \n 
~~~ if isinstance ( target , Gaffer . Node ) : \n 
~~~ Gaffer . Metadata . registerNodeValue ( target , name , value ) \n 
~~~ Gaffer . Metadata . registerPlugValue ( target , name , value ) \n 
~~ ~~ def _registeredMetadata ( target , inherit = True , instanceOnly = False , persistentOnly = False ) : \n 
~~~ return Gaffer . Metadata . registeredNodeValues ( target , inherit , instanceOnly , persistentOnly ) \n 
~~~ return Gaffer . Metadata . registeredPlugValues ( target , inherit , instanceOnly , persistentOnly ) \n 
~~ ~~ def _metadata ( target , name ) : \n 
~~~ return Gaffer . Metadata . nodeValue ( target , name ) \n 
~~~ return Gaffer . Metadata . plugValue ( target , name ) \n 
~~ ~~ def _deregisterMetadata ( target , name ) : \n 
~~~ return Gaffer . Metadata . deregisterNodeValue ( target , name ) \n 
~~~ return Gaffer . Metadata . deregisterPlugValue ( target , name ) \n 
~~ ~~ import unittest \n 
class NumericSliderTest ( unittest . TestCase ) : \n 
~~~ def testConstruction ( self ) : \n 
~~~ s = GafferUI . NumericSlider ( value = 0 , min = 0 , max = 1 ) \n 
self . assertEqual ( s . getPosition ( ) , 0 ) \n 
self . assertEqual ( s . getValue ( ) , 0 ) \n 
self . assertEqual ( s . getRange ( ) , ( 0 , 1 , 0 , 1 ) ) \n 
~~ def testSetValue ( self ) : \n 
~~~ s = GafferUI . NumericSlider ( value = 0 , min = 0 , max = 2 ) \n 
s . setValue ( 0.5 ) \n 
self . assertEqual ( s . getPosition ( ) , 0.25 ) \n 
self . assertEqual ( s . getValue ( ) , 0.5 ) \n 
~~ def testSetRange ( self ) : \n 
~~~ s = GafferUI . NumericSlider ( value = 1 , min = 0 , max = 2 ) \n 
self . assertEqual ( s . getPosition ( ) , 0.5 ) \n 
self . assertEqual ( s . getValue ( ) , 1 ) \n 
s . setRange ( 0 , 1 ) \n 
self . assertEqual ( s . getPosition ( ) , 1 ) \n 
~~ def testSetZeroRange ( self ) : \n 
~~~ s = GafferUI . NumericSlider ( value = 1 , min = 1 , max = 2 ) \n 
s . setRange ( 1 , 1 ) \n 
~~ def testSetPosition ( self ) : \n 
s . setPosition ( 0.5 ) \n 
~~ def testValuesOutsideRangeAreClamped ( self ) : \n 
~~~ s = GafferUI . NumericSlider ( value = 0.1 , min = 0 , max = 2 ) \n 
cs = GafferTest . CapturingSlot ( s . valueChangedSignal ( ) , s . positionChangedSignal ( ) ) \n 
s . setValue ( 3 ) \n 
self . assertEqual ( s . getValue ( ) , 2 ) \n 
self . assertEqual ( len ( cs ) , 2 ) \n 
~~ def testPositionsOutsideRangeAreClamped ( self ) : \n 
s . setPosition ( 2 ) \n 
~~ def testHardRange ( self ) : \n 
~~~ s = GafferUI . NumericSlider ( value = 0.1 , min = 0 , max = 2 , hardMin = - 1 , hardMax = 3 ) \n 
self . assertEqual ( s . getRange ( ) , ( 0 , 2 , - 1 , 3 ) ) \n 
self . assertEqual ( s . getValue ( ) , 3 ) \n 
self . assertEqual ( s . getPosition ( ) , 1.5 ) \n 
s . setValue ( 3.5 ) \n 
s . setValue ( - 1 ) \n 
self . assertEqual ( s . getValue ( ) , - 1 ) \n 
self . assertEqual ( s . getPosition ( ) , - 0.5 ) \n 
self . assertEqual ( len ( cs ) , 4 ) \n 
s . setValue ( - 2 ) \n 
~~ def testSetRangeClampsValue ( self ) : \n 
~~~ s = GafferUI . NumericSlider ( value = 0.5 , min = 0 , max = 2 ) \n 
s . setRange ( 1 , 2 ) \n 
~~ def testMultipleValues ( self ) : \n 
~~~ self . assertRaises ( Exception , GafferUI . NumericSlider , value = 0 , values = [ 1 , 2 ] ) \n 
s = GafferUI . NumericSlider ( values = [ 1 , 1.5 ] , min = 0 , max = 2 ) \n 
self . assertEqual ( s . getValues ( ) , [ 1 , 1.5 ] ) \n 
self . assertEqual ( s . getPositions ( ) , [ 0.5 , 0.75 ] ) \n 
self . assertRaises ( ValueError , s . getValue ) \n 
import GafferUITest \n 
QtCore = GafferUI . _qtImport ( "QtCore" ) \n 
QtGui = GafferUI . _qtImport ( "QtGui" ) \n 
class TestWidget ( GafferUI . Widget ) : \n 
~~~ def __init__ ( self , ** kw ) : \n 
~~~ GafferUI . Widget . __init__ ( self , QtGui . QLabel ( "hello" ) , ** kw ) \n 
~~ ~~ class TestWidget2 ( GafferUI . Widget ) : \n 
~~~ self . topLevelGafferWidget = TestWidget ( ) \n 
GafferUI . Widget . __init__ ( self , self . topLevelGafferWidget ) \n 
~~ ~~ class WidgetTest ( GafferUITest . TestCase ) : \n 
~~~ def testOwner ( self ) : \n 
~~~ w = TestWidget ( ) \n 
self . assert_ ( GafferUI . Widget . _owner ( w . _qtWidget ( ) ) is w ) \n 
~~ def testParent ( self ) : \n 
self . assert_ ( w . parent ( ) is None ) \n 
~~ def testCanDie ( self ) : \n 
wr1 = weakref . ref ( w ) \n 
wr2 = weakref . ref ( w . _qtWidget ( ) ) \n 
del w \n 
self . assert_ ( wr1 ( ) is None ) \n 
self . assert_ ( wr2 ( ) is None ) \n 
~~ def testAncestor ( self ) : \n 
~~~ w = GafferUI . Window ( "test" ) \n 
l = GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Vertical ) \n 
p = GafferUI . SplitContainer ( ) \n 
l . append ( p ) \n 
w . setChild ( l ) \n 
self . assert_ ( p . ancestor ( GafferUI . ListContainer ) is l ) \n 
self . assert_ ( p . ancestor ( GafferUI . Window ) is w ) \n 
self . assert_ ( p . ancestor ( GafferUI . Menu ) is None ) \n 
~~ def testIsAncestorOf ( self ) : \n 
~~~ with GafferUI . Window ( "test" ) as w : \n 
~~~ with GafferUI . SplitContainer ( ) as p : \n 
~~~ with GafferUI . ListContainer ( ) as l1 : \n 
~~~ b1 = GafferUI . Button ( ) \n 
~~ with GafferUI . ListContainer ( ) as l2 : \n 
~~~ b2 = GafferUI . Button ( ) \n 
~~ ~~ ~~ self . assertTrue ( l2 . isAncestorOf ( b2 ) ) \n 
self . assertFalse ( l1 . isAncestorOf ( b2 ) ) \n 
self . assertTrue ( p . isAncestorOf ( b2 ) ) \n 
self . assertTrue ( w . isAncestorOf ( b2 ) ) \n 
self . assertFalse ( b2 . isAncestorOf ( b1 ) ) \n 
self . assertFalse ( b2 . isAncestorOf ( l1 ) ) \n 
self . assertFalse ( b2 . isAncestorOf ( l2 ) ) \n 
self . assertFalse ( b2 . isAncestorOf ( p ) ) \n 
self . assertFalse ( b2 . isAncestorOf ( w ) ) \n 
self . assertTrue ( l1 . isAncestorOf ( b1 ) ) \n 
self . assertFalse ( l2 . isAncestorOf ( b1 ) ) \n 
self . assertTrue ( p . isAncestorOf ( b1 ) ) \n 
self . assertTrue ( w . isAncestorOf ( b1 ) ) \n 
~~ def testGafferWidgetAsTopLevel ( self ) : \n 
~~~ w = TestWidget2 ( ) \n 
self . assert_ ( w . topLevelGafferWidget . parent ( ) is w ) \n 
self . assert_ ( GafferUI . Widget . _owner ( w . topLevelGafferWidget . _qtWidget ( ) ) is not w ) \n 
~~ def testToolTip ( self ) : \n 
self . assertEqual ( w . getToolTip ( ) , "" ) \n 
w = TestWidget ( toolTip = "hi" ) \n 
self . assertEqual ( w . getToolTip ( ) , "hi" ) \n 
w . setToolTip ( "a" ) \n 
self . assertEqual ( w . getToolTip ( ) , "a" ) \n 
~~ def testEnabledState ( self ) : \n 
self . assertEqual ( w . getEnabled ( ) , True ) \n 
self . assertEqual ( w . enabled ( ) , True ) \n 
w . setEnabled ( False ) \n 
self . assertEqual ( w . getEnabled ( ) , False ) \n 
self . assertEqual ( w . enabled ( ) , False ) \n 
w . setEnabled ( True ) \n 
~~ def testDisabledWidgetsDontGetSignals ( self ) : \n 
def f ( w , event ) : \n 
~~~ WidgetTest . signalsEmitted += 1 \n 
~~ c = w . buttonPressSignal ( ) . connect ( f ) \n 
WidgetTest . signalsEmitted = 0 \n 
event = QtGui . QMouseEvent ( QtCore . QEvent . MouseButtonPress , QtCore . QPoint ( 0 , 0 ) , QtCore . Qt . LeftButton \n 
QtGui . QApplication . instance ( ) . sendEvent ( w . _qtWidget ( ) , event ) \n 
self . assertEqual ( WidgetTest . signalsEmitted , 1 ) \n 
self . assertEqual ( WidgetTest . signalsEmitted , 2 ) \n 
~~ def testCanDieAfterUsingSignals ( self ) : \n 
w . buttonPressSignal ( ) \n 
w . buttonReleaseSignal ( ) \n 
w . mouseMoveSignal ( ) \n 
w . wheelSignal ( ) \n 
~~ def testVisibility ( self ) : \n 
~~~ with GafferUI . Window ( ) as w : \n 
~~~ with GafferUI . ListContainer ( ) as l : \n 
~~~ t = TestWidget ( ) \n 
~~ ~~ self . assertEqual ( w . getVisible ( ) , False ) \n 
self . assertEqual ( l . getVisible ( ) , True ) \n 
self . assertEqual ( t . getVisible ( ) , True ) \n 
self . assertEqual ( w . visible ( ) , False ) \n 
self . assertEqual ( l . visible ( ) , False ) \n 
self . assertEqual ( t . visible ( ) , False ) \n 
w . setVisible ( True ) \n 
self . assertEqual ( w . getVisible ( ) , True ) \n 
self . assertEqual ( w . visible ( ) , True ) \n 
self . assertEqual ( l . visible ( ) , True ) \n 
self . assertEqual ( t . visible ( ) , True ) \n 
w . setVisible ( False ) \n 
self . assertEqual ( w . getVisible ( ) , False ) \n 
self . assertEqual ( t . visible ( relativeTo = l ) , True ) \n 
self . assertEqual ( t . visible ( relativeTo = w ) , True ) \n 
t . setVisible ( False ) \n 
self . assertEqual ( t . getVisible ( ) , False ) \n 
self . assertEqual ( t . visible ( relativeTo = l ) , False ) \n 
~~ def testGetVisibleForNewWidgets ( self ) : \n 
~~ def testVisibilityOfParentlessWidgets ( self ) : \n 
~~~ w = GafferUI . Window ( ) \n 
t = TestWidget ( ) \n 
w . setChild ( t ) \n 
w . removeChild ( t ) \n 
self . assertEqual ( t . parent ( ) , None ) \n 
~~ def testVisibilityWhenTransferringWidgets ( self ) : \n 
~~~ w1 = GafferUI . Window ( ) \n 
w1 . setVisible ( True ) \n 
w2 = GafferUI . Window ( ) \n 
w2 . setVisible ( True ) \n 
v = TestWidget ( ) \n 
self . assertEqual ( v . getVisible ( ) , True ) \n 
self . assertEqual ( v . visible ( ) , False ) \n 
h = TestWidget ( ) \n 
self . assertEqual ( h . getVisible ( ) , True ) \n 
h . setVisible ( False ) \n 
self . assertEqual ( h . getVisible ( ) , False ) \n 
self . assertEqual ( h . visible ( ) , False ) \n 
w1 . setChild ( v ) \n 
self . assertEqual ( v . visible ( ) , True ) \n 
w2 . setChild ( v ) \n 
w1 . setChild ( h ) \n 
w2 . setChild ( h ) \n 
~~ def testSignals ( self ) : \n 
for s in [ \n 
( "keyPressSignal" , GafferUI . WidgetEventSignal ) , \n 
( "keyReleaseSignal" , GafferUI . WidgetEventSignal ) , \n 
( "buttonPressSignal" , GafferUI . WidgetEventSignal ) , \n 
( "buttonReleaseSignal" , GafferUI . WidgetEventSignal ) , \n 
( "buttonDoubleClickSignal" , GafferUI . WidgetEventSignal ) , \n 
( "mouseMoveSignal" , GafferUI . WidgetEventSignal ) , \n 
( "enterSignal" , GafferUI . WidgetSignal ) , \n 
( "leaveSignal" , GafferUI . WidgetSignal ) , \n 
( "wheelSignal" , GafferUI . WidgetEventSignal ) , \n 
( "visibilityChangedSignal" , GafferUI . WidgetSignal ) , \n 
( "contextMenuSignal" , GafferUI . WidgetSignal ) , \n 
( "parentChangedSignal" , GafferUI . WidgetSignal ) , \n 
] : \n 
~~~ self . failUnless ( isinstance ( getattr ( w , s [ 0 ] ) ( ) , s [ 1 ] ) ) \n 
self . failUnless ( getattr ( w , s [ 0 ] ) ( ) is getattr ( w , s [ 0 ] ) ( ) ) \n 
~~ ~~ def testBound ( self ) : \n 
~~~ w = GafferUI . Window ( borderWidth = 8 ) \n 
b = GafferUI . Button ( ) \n 
w . setChild ( b ) \n 
w . setPosition ( IECore . V2i ( 100 ) ) \n 
self . waitForIdle ( 1000 ) \n 
wb = w . bound ( ) \n 
bb = b . bound ( ) \n 
bbw = b . bound ( relativeTo = w ) \n 
self . failUnless ( isinstance ( wb , IECore . Box2i ) ) \n 
self . failUnless ( isinstance ( bb , IECore . Box2i ) ) \n 
self . failUnless ( isinstance ( bbw , IECore . Box2i ) ) \n 
self . assertEqual ( bb . size ( ) , bbw . size ( ) ) \n 
self . assertEqual ( bbw . min , bb . min - wb . min ) \n 
self . assertEqual ( b . size ( ) , bb . size ( ) ) \n 
~~ def testParentChangedSignal ( self ) : \n 
window = GafferUI . Window ( ) \n 
cs = GafferTest . CapturingSlot ( w . parentChangedSignal ( ) ) \n 
self . assertEqual ( len ( cs ) , 0 ) \n 
window . setChild ( w ) \n 
self . assertEqual ( len ( cs ) , 1 ) \n 
self . assertEqual ( cs [ 0 ] , ( w , ) ) \n 
window . setChild ( None ) \n 
self . assertEqual ( cs [ 1 ] , ( w , ) ) \n 
~~ def testHighlighting ( self ) : \n 
self . assertEqual ( w . getHighlighted ( ) , False ) \n 
w . setHighlighted ( True ) \n 
self . assertEqual ( w . getHighlighted ( ) , True ) \n 
w . setHighlighted ( False ) \n 
~~ def testWidgetAt ( self ) : \n 
~~~ with GafferUI . Window ( ) as w1 : \n 
~~~ t1 = GafferUI . TextWidget ( "hello" ) \n 
~~ with GafferUI . Window ( ) as w2 : \n 
~~~ t2 = GafferUI . TextWidget ( "hello" ) \n 
~~ w1 . setVisible ( True ) \n 
w1 . setPosition ( IECore . V2i ( 100 ) ) \n 
w2 . setPosition ( IECore . V2i ( 300 ) ) \n 
self . assertTrue ( GafferUI . Widget . widgetAt ( w1 . bound ( ) . center ( ) ) is t1 ) \n 
self . assertTrue ( GafferUI . Widget . widgetAt ( w2 . bound ( ) . center ( ) ) is t2 ) \n 
self . assertTrue ( GafferUI . Widget . widgetAt ( w1 . bound ( ) . center ( ) , widgetType = GafferUI . Window ) is w1 self . assertTrue ( GafferUI . Widget . widgetAt ( w2 . bound ( ) . center ( ) , widgetType = GafferUI . Window ) is w2 \n 
~~ def testMousePosition ( self ) : \n 
mouseGlobal = GafferUI . Widget . mousePosition ( ) \n 
mouseLocal = GafferUI . Widget . mousePosition ( relativeTo = b ) \n 
self . assertEqual ( mouseGlobal , mouseLocal + b . bound ( ) . min ) \n 
~~ def testAddressAndObject ( self ) : \n 
~~~ button = GafferUI . Button ( ) \n 
address = GafferUI . _qtAddress ( button . _qtWidget ( ) ) \n 
self . assertTrue ( isinstance ( address , int ) ) \n 
widget = GafferUI . _qtObject ( address , QtGui . QPushButton ) \n 
self . assertTrue ( isinstance ( widget , QtGui . QPushButton ) ) \n 
~~ def testSetVisibleWithNonBool ( self ) : \n 
self . assertTrue ( w . getVisible ( ) is True ) \n 
w . setVisible ( 0 ) \n 
self . assertTrue ( w . getVisible ( ) is False ) \n 
w . setVisible ( 1 ) \n 
~~ import GafferUI \n 
def __toolMenu ( nodeEditor , node , menuDefinition ) : \n 
~~~ GafferUI . UIEditor . appendNodeEditorToolMenuDefinitions ( nodeEditor , node , menuDefinition ) \n 
GafferUI . BoxUI . appendNodeEditorToolMenuDefinitions ( nodeEditor , node , menuDefinition ) \n 
GafferSceneUI . FilteredSceneProcessorUI . appendNodeEditorToolMenuDefinitions ( nodeEditor , node , menuDefinition \n 
~~ __nodeEditorToolMenuConnection = GafferUI . NodeEditor . toolMenuSignal ( ) . connect ( __toolMenu ) \n 
VERSION = ( 0 , 1 , 0 , , 1 ) \n 
__version__ = . join ( map ( str , VERSION ) ) \n 
def get_version ( ) : \n 
~~~ version = % ( VERSION [ 0 ] , VERSION [ 1 ] ) \n 
if VERSION [ 2 ] : \n 
~~~ version = % ( version , VERSION [ 2 ] ) \n 
~~ if VERSION [ 3 : ] == ( , 0 ) : \n 
~~~ version = % version \n 
~~~ if VERSION [ 3 ] != : \n 
~~~ version = % ( version , VERSION [ 3 ] , VERSION [ 4 ] ) \n 
~~ ~~ return version \n 
~~ import yappi \n 
from totalimpact import backend \n 
rootdir = "." \n 
logfile = \n 
yappi . clear_stats ( ) \n 
yappi . start ( ) \n 
backend . main ( logfile ) \n 
yappi . stop ( ) \n 
yappi . print_stats ( sort_type = yappi . SORTTYPE_TTOT , limit = 30 , thread_stats_on = False ) \n 
import os , collections , simplejson \n 
from totalimpact import db , app \n 
from totalimpact . providers import pmc \n 
from test . unit_tests . providers import common \n 
from test . unit_tests . providers . common import ProviderTestCase \n 
from totalimpact . providers . provider import Provider , ProviderContentMalformedError , ProviderFactory \n 
from totalimpact import provider_batch_data \n 
from test . utils import http \n 
from test . utils import setup_postgres_for_unittests , teardown_postgres_for_unittests \n 
from nose . tools import assert_equals , raises , nottest , assert_items_equal \n 
datadir = os . path . join ( os . path . split ( __file__ ) [ 0 ] , "../../../extras/sample_provider_pages/pmc" ) \n 
SAMPLE_EXTRACT_METRICS_PAGE = os . path . join ( datadir , "monthly_download" ) \n 
SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH = os . path . join ( datadir , "monthly_download_different_month" \n 
TEST_PMID = "23066504" \n 
class TestPmc ( ProviderTestCase ) : \n 
~~~ provider_name = "pmc" \n 
testitem_aliases = ( "pmid" , TEST_PMID ) \n 
testitem_metrics = ( "pmid" , TEST_PMID ) \n 
~~~ ProviderTestCase . setUp ( self ) \n 
self . db = setup_postgres_for_unittests ( db , app ) \n 
sample_data_dump = open ( SAMPLE_EXTRACT_METRICS_PAGE , "r" ) . read ( ) \n 
sample_data_dump_different_month = open ( SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH , "r" ) . read \n 
test_monthly_data = [ \n 
{ "_id" : "abc" , \n 
"type" : "provider_data_dump" , \n 
"provider" : "pmc" , \n 
"raw" : sample_data_dump , \n 
"provider_raw_version" : 1.0 , \n 
"created" : "2012-11-29T07:34:01.126892" , \n 
"aliases" : { "pmid" : [ "111" , "222" ] } , \n 
"min_event_date" : "2012-10-01T07:34:01.126892" , \n 
"max_event_date" : "2012-10-31T07:34:01.126892" \n 
{ "_id" : "def" , \n 
"raw" : sample_data_dump_different_month , \n 
"created" : "2012-11-29T08:34:01.126892" , \n 
"aliases" : { "pmid" : [ "111" ] } , \n 
"min_event_date" : "2012-01-01T07:34:01.126892" , \n 
"max_event_date" : "2012-01-31T07:34:01.126892" \n 
"_id" : "abc123" , \n 
"raw" : "max_event_date" : "2012-10-31T07:34:01.126892" , \n 
"aliases" : { \n 
"pmid" : [ \n 
"23066504" , \n 
"23066507" , \n 
"23066508" , \n 
"23066509" , \n 
"23066506" , \n 
"23066503" , \n 
"23066510" , \n 
"23071903" , \n 
"23110253" , \n 
"23066505" , \n 
"23110254" , \n 
"23110255" , \n 
"23110252" \n 
"provider_raw_version" : 1 , \n 
"min_event_date" : "2012-10-02T07:34:01.126892" , \n 
"created" : "2012-11-29T09:34:01.126892" \n 
for doc in test_monthly_data : \n 
~~~ new_object = provider_batch_data . create_objects_from_doc ( doc ) \n 
print new_object \n 
~~ self . provider = pmc . Pmc ( ) \n 
~~~ teardown_postgres_for_unittests ( self . db ) \n 
~~ def test_has_applicable_batch_data_true ( self ) : \n 
~~~ response = self . provider . has_applicable_batch_data ( "pmid" , "111" ) \n 
assert_equals ( response , True ) \n 
~~ def test_has_applicable_batch_data_false ( self ) : \n 
~~~ response = self . provider . has_applicable_batch_data ( "pmid" , "notapmidintheview" ) \n 
assert_equals ( response , False ) \n 
~~ def test_build_batch_data_dict ( self ) : \n 
~~~ response = self . provider . build_batch_data_dict ( ) \n 
print response . keys ( ) \n 
expected = [ ( , ) , ( , ) , ( , ) , ( , assert_items_equal ( response . keys ( ) , expected ) \n 
~~ def test_is_relevant_alias ( self ) : \n 
~~~ assert_equals ( self . provider . is_relevant_alias ( self . testitem_aliases ) , True ) \n 
~~ def test_extract_metrics_success ( self ) : \n 
~~~ f = open ( SAMPLE_EXTRACT_METRICS_PAGE , "r" ) \n 
good_page = f . read ( ) \n 
metrics_dict = self . provider . _extract_metrics ( good_page , id = "222" ) \n 
print metrics_dict \n 
expected = { : 514 , : 230 , : 606 , assert_equals ( metrics_dict , expected ) \n 
~~ def test_provider_metrics_500 ( self ) : \n 
~~ def test_provider_metrics_400 ( self ) : \n 
~~ def test_provider_metrics_nonsense_xml ( self ) : \n 
~~ def test_provider_metrics_nonsense_txt ( self ) : \n 
~~ def test_provider_metrics_empty ( self ) : \n 
~~ @ http \n 
def test_metrics ( self ) : \n 
~~~ metrics_dict = self . provider . metrics ( [ ( "pmid" , "222" ) ] ) \n 
expected = { : ( 514 , ) , : ( 230 , ) , print metrics_dict \n 
for key in expected : \n 
~~~ assert metrics_dict [ key ] [ 0 ] >= expected [ key ] [ 0 ] , [ key , metrics_dict [ key ] , expected [ key ] ] assert metrics_dict [ key ] [ 1 ] == expected [ key ] [ 1 ] , [ key , metrics_dict [ key ] , expected [ key ] ] \n 
~~ ~~ @ http \n 
def test_metrics_multiple_months ( self ) : \n 
~~~ metrics_dict = self . provider . metrics ( [ ( "pmid" , "111" ) ] ) \n 
expected = { : ( 218 , ) , : ( 810 , ) , print metrics_dict \n 
def test_metrics_real ( self ) : \n 
~~~ metrics_dict = self . provider . metrics ( [ ( "pmid" , "23066504" ) ] ) \n 
expected = { : ( 119 , ) , : ( 722 , ) , print metrics_dict \n 
~~ ~~ ~~ import os \n 
import hashlib \n 
from cPickle import PicklingError \n 
import redis \n 
from totalimpact import REDIS_CACHE_DATABASE_NUMBER \n 
logger = logging . getLogger ( "ti.cache" ) \n 
cache_client = redis . from_url ( os . getenv ( "REDIS_URL" ) , REDIS_CACHE_DATABASE_NUMBER ) \n 
MAX_CACHE_SIZE_BYTES = 100 * 1000 * 1000 #100mb \n 
class CacheException ( Exception ) : \n 
~~ class Cache ( object ) : \n 
def _build_hash_key ( self , key ) : \n 
~~~ json_key = json . dumps ( key ) \n 
hash_key = hashlib . md5 ( json_key . encode ( "utf-8" ) ) . hexdigest ( ) \n 
return hash_key \n 
~~ def _get_client ( self ) : \n 
~~~ return cache_client \n 
~~~ self . max_cache_age = max_cache_age \n 
self . flush_cache ( ) \n 
~~ def flush_cache ( self ) : \n 
~~~ mc = self . _get_client ( ) \n 
~~ def get_cache_entry ( self , key ) : \n 
mc = self . _get_client ( ) \n 
hash_key = self . _build_hash_key ( key ) \n 
response = mc . get ( hash_key ) \n 
if response : \n 
~~~ response = json . loads ( response ) \n 
~~ return response \n 
~~ def set_cache_entry ( self , key , data ) : \n 
if sys . getsizeof ( data [ "text" ] ) > MAX_PAYLOAD_SIZE_BYTES : \n 
~~ mc = self . _get_client ( ) \n 
if mc . info ( ) [ "used_memory" ] >= MAX_CACHE_SIZE_BYTES : \n 
~~ hash_key = self . _build_hash_key ( key ) \n 
set_response = mc . set ( hash_key , json . dumps ( data ) ) \n 
mc . expire ( hash_key , self . max_cache_age ) \n 
if not set_response : \n 
~~ return set_response \n 
~~ ~~ from totalimpact . providers import provider \n 
from totalimpact . providers . provider import Provider , ProviderContentMalformedError \n 
import simplejson , os , re , urllib \n 
logger = logging . getLogger ( ) \n 
class Plosalm ( Provider ) : \n 
~~~ example_id = ( "doi" , "10.1371/journal.pcbi.1000361" ) \n 
url = "http://www.plos.org/" \n 
metrics_url_template = "http://alm.plos.org/api/v3/articles?ids=%s&source=citations,counter&api_key=" provenance_url_template = "http://dx.doi.org/%s" \n 
PLOS_ICON = "http://www.plos.org/wp-content/themes/plos_new/favicon.ico" \n 
static_meta_dict = { \n 
"html_views" : { \n 
"provider" : "PLOS" , \n 
"provider_url" : "http://www.plos.org/" , \n 
"icon" : PLOS_ICON , \n 
"pdf_views" : { \n 
def __init__ ( self ) : \n 
~~~ super ( Plosalm , self ) . __init__ ( ) \n 
~~ def is_relevant_alias ( self , alias ) : \n 
~~~ ( namespace , nid ) = alias \n 
relevant = ( ( "doi" == namespace ) and ( "10.1371/" in nid ) ) \n 
return ( relevant ) \n 
~~ def _extract_metrics ( self , page , status_code = 200 , id = None ) : \n 
~~~ if status_code != 200 : \n 
~~~ if status_code == 404 : \n 
~~~ return { } \n 
~~~ raise ( self . _get_error ( status_code ) ) \n 
~~ ~~ if not "sources" in page : \n 
~~~ raise ProviderContentMalformedError \n 
~~ json_response = provider . _load_json ( page ) \n 
this_article = json_response [ 0 ] [ "sources" ] [ 0 ] [ "metrics" ] \n 
dict_of_keylists = { \n 
: [ ] , \n 
: [ ] \n 
metrics_dict = provider . _extract_from_data_dict ( this_article , dict_of_keylists ) \n 
return metrics_dict \n 
import urlparse \n 
from kombu import Exchange , Queue \n 
sys . path . append ( ) \n 
redis_url = os . environ . get ( , "redis://127.0.0.1:6379/" ) \n 
if not redis_url . endswith ( "/" ) : \n 
~~~ redis_url += "/" \n 
REDIS_CONNECT_RETRY = True \n 
: True , \n 
CELERY_DEFAULT_QUEUE = \n 
CELERY_QUEUES = [ \n 
Queue ( , routing_key = ) , \n 
Queue ( , routing_key = ) \n 
BROKER_POOL_LIMIT = None \n 
CELERY_CREATE_MISSING_QUEUES = True \n 
CELERY_ACCEPT_CONTENT = [ , ] \n 
CELERY_ENABLE_UTC = True \n 
CELERY_ACKS_LATE = True \n 
CELERYD_FORCE_EXECV = True \n 
CELERY_TRACK_STARTED = True \n 
CELERYD_PREFETCH_MULTIPLIER = 1 \n 
CELERY_IMPORTS = ( "core_tasks" , ) \n 
CELERY_ANNOTATIONS = { \n 
from totalimpact . providers import provider \n 
from totalimpact . providers . provider import Provider , ProviderFactory \n 
from totalimpactwebapp import app , db \n 
from nose . tools import assert_equals , nottest \n 
from xml . dom import minidom \n 
import simplejson , BeautifulSoup \n 
from sqlalchemy . sql import text \n 
sampledir = os . path . join ( os . path . split ( __file__ ) [ 0 ] , "../../../extras/sample_provider_pages/" ) \n 
class Test_Provider ( ) : \n 
~~~ TEST_PROVIDER_CONFIG = [ \n 
( "pubmed" , { "workers" : 1 } ) , \n 
( "wikipedia" , { "workers" : 3 } ) , \n 
( "mendeley" , { "workers" : 3 } ) , \n 
TEST_XML = open ( os . path . join ( sampledir , "facebook" , "metrics" ) ) . read ( ) \n 
~~~ self . db = setup_postgres_for_unittests ( db , app ) \n 
~~ def test_get_provider ( self ) : \n 
~~~ provider = ProviderFactory . get_provider ( "wikipedia" ) \n 
assert_equals ( provider . __class__ . __name__ , "Wikipedia" ) \n 
~~ def test_get_providers ( self ) : \n 
~~~ providers = ProviderFactory . get_providers ( self . TEST_PROVIDER_CONFIG ) \n 
provider_names = [ provider . __class__ . __name__ for provider in providers ] \n 
assert_equals ( set ( provider_names ) , set ( [ , , "Pubmed" ] ) ) \n 
~~ def test_get_providers_filters_by_metrics ( self ) : \n 
~~~ providers = ProviderFactory . get_providers ( self . TEST_PROVIDER_CONFIG , "metrics" ) \n 
~~ def test_get_providers_filters_by_biblio ( self ) : \n 
~~~ providers = ProviderFactory . get_providers ( self . TEST_PROVIDER_CONFIG , "biblio" ) \n 
assert_equals ( set ( provider_names ) , set ( [ , ] ) ) \n 
~~ def test_get_providers_filters_by_aliases ( self ) : \n 
~~~ providers = ProviderFactory . get_providers ( self . TEST_PROVIDER_CONFIG , "aliases" ) \n 
~~ def test_lookup_json ( self ) : \n 
~~~ page = self . TEST_JSON \n 
data = simplejson . loads ( page ) \n 
response = provider . _lookup_json ( data , [ , ] ) \n 
assert_equals ( response , ) \n 
~~ def test_extract_json ( self ) : \n 
: [ , ] , \n 
: [ , ] } \n 
response = provider . _extract_from_json ( page , dict_of_keylists ) \n 
assert_equals ( response , { : , : } ) \n 
~~ def test_lookup_xml_from_dom ( self ) : \n 
~~~ page = self . TEST_XML \n 
doc = minidom . parseString ( page . strip ( ) ) \n 
response = provider . _lookup_xml_from_dom ( doc , [ ] ) \n 
assert_equals ( response , 17 ) \n 
~~ def test_lookup_xml_from_soup ( self ) : \n 
doc = BeautifulSoup . BeautifulStoneSoup ( page ) \n 
response = provider . _lookup_xml_from_soup ( doc , [ ] ) \n 
~~ def test_extract_xml ( self ) : \n 
: [ ] } \n 
response = provider . _extract_from_xml ( page , dict_of_keylists ) \n 
assert_equals ( response , { : 17 } ) \n 
~~ def test_doi_from_url_string ( self ) : \n 
~~~ test_url = "https://knb.ecoinformatics.org/knb/d1/mn/v1/object/doi:10.5063%2FAA%2Fnrs.373.1" expected = "10.5063/AA/nrs.373.1" \n 
response = provider . doi_from_url_string ( test_url ) \n 
assert_equals ( response , expected ) \n 
~~ def test_is_issn_in_doaj_false ( self ) : \n 
~~~ response = provider . is_issn_in_doaj ( "invalidissn" ) \n 
~~ def test_is_issn_in_doaj_true ( self ) : \n 
response = provider . is_issn_in_doaj ( zookeys_issn ) \n 
~~ def test_import_products ( self ) : \n 
~~~ response = provider . import_products ( "product_id_strings" , \n 
{ "product_id_strings" : [ "123456" , "HTTPS://starbucks.com" , "arXiv:1305.3328" , "http://doi.org/10.123/ABC" expected = [ ( , ) , ( , ) , ( , ) , ( assert_equals ( response , expected ) \n 
~~ def test_import_products_bad_providername ( self ) : \n 
~~~ response = provider . import_products ( "nonexistant" , { } ) \n 
expected = [ ] \n 
~~ ~~ class TestProviderFactory ( ) : \n 
def test_get_all_static_meta ( self ) : \n 
~~~ sm = ProviderFactory . get_all_static_meta ( self . TEST_PROVIDER_CONFIG ) \n 
expected = \n 
assert_equals ( sm [ "pubmed:pmc_citations" ] [ "description" ] , expected ) \n 
~~ def test_get_all_metric_names ( self ) : \n 
~~~ response = ProviderFactory . get_all_metric_names ( self . TEST_PROVIDER_CONFIG ) \n 
expected = [ , , , assert_equals ( response , expected ) \n 
~~ def test_get_all_metadata ( self ) : \n 
~~~ md = ProviderFactory . get_all_metadata ( self . TEST_PROVIDER_CONFIG ) \n 
print md [ "pubmed" ] \n 
assert_equals ( md [ "pubmed" ] [ ] , ) \n 
~~ ~~ import datetime \n 
import copy \n 
import unicode_helpers \n 
from util import cached_property \n 
from util import dict_from_dir \n 
from totalimpactwebapp import db \n 
logger = logging . getLogger ( "ti.aliases" ) \n 
def clean_id ( nid ) : \n 
nid = unicode_helpers . remove_nonprinting_characters ( nid ) \n 
~~ except ( TypeError , AttributeError ) : \n 
~~ return ( nid ) \n 
~~ def normalize_alias_tuple ( ns , nid ) : \n 
~~~ ns = clean_id ( ns ) \n 
ns = ns . lower ( ) \n 
if ns == "biblio" : \n 
~~~ return ( ns , nid ) \n 
~~ nid = clean_id ( nid ) \n 
from totalimpact . providers import crossref \n 
from totalimpact . providers import pubmed \n 
from totalimpact . providers import arxiv \n 
from totalimpact . providers import webpage \n 
from totalimpact import importer \n 
clean_nid = None \n 
if ns == "doi" or importer . is_doi ( nid ) : \n 
~~~ ns = "doi" \n 
clean_nid = crossref . clean_doi ( nid ) \n 
~~ elif ns == "pmid" or importer . is_pmid ( nid ) : \n 
~~~ ns = "pmid" \n 
clean_nid = pubmed . clean_pmid ( nid ) \n 
~~ elif ns == "arxiv" or importer . is_arxiv ( nid ) : \n 
~~~ ns = "arxiv" \n 
clean_nid = arxiv . clean_arxiv_id ( nid ) \n 
~~ elif ns == "url" or importer . is_url ( nid ) : \n 
~~~ ns = "url" \n 
clean_nid = webpage . clean_url ( nid ) \n 
~~ elif ns not in [ "doi" , "pmid" , "arxiv" , "url" ] : \n 
~~~ clean_nid = nid \n 
~~ if not clean_nid : \n 
~~ return ( ns , clean_nid ) \n 
~~ def clean_alias_tuple_for_comparing ( ns , nid ) : \n 
~~~ alias_tuple = normalize_alias_tuple ( ns , nid ) \n 
if not alias_tuple : \n 
~~~ ( ns , nid ) = alias_tuple \n 
cleaned_alias = ( ns . lower ( ) , nid . lower ( ) ) \n 
~~ except AttributeError : \n 
ns = ns , nid = nid ) ) \n 
cleaned_alias = ( ns , nid ) \n 
~~ return cleaned_alias \n 
~~ def alias_tuples_from_dict ( aliases_dict ) : \n 
alias_tuples = [ ] \n 
for ns , ids in aliases_dict . iteritems ( ) : \n 
~~~ if isinstance ( ids , basestring ) : \n 
~~~ alias_tuples . append ( ( ns , ids ) ) \n 
~~~ for id in ids : \n 
~~~ alias_tuples . append ( ( ns , id ) ) \n 
~~ ~~ ~~ return alias_tuples \n 
~~ def alias_dict_from_tuples ( aliases_tuples ) : \n 
~~~ alias_dict = { } \n 
for ( ns , ids ) in aliases_tuples : \n 
~~~ if ns in alias_dict : \n 
~~~ alias_dict [ ns ] += [ ids ] \n 
~~~ alias_dict [ ns ] = [ ids ] \n 
~~ ~~ return alias_dict \n 
~~ def canonical_aliases ( orig_aliases_dict ) : \n 
~~~ lowercase_aliases_dict = { } \n 
for orig_namespace in orig_aliases_dict : \n 
~~~ lowercase_namespace = clean_id ( orig_namespace . lower ( ) ) \n 
if lowercase_namespace == "doi" : \n 
~~~ lowercase_aliases_dict [ lowercase_namespace ] = [ clean_id ( doi . lower ( ) ) for doi in orig_aliases_dict ~~ else : \n 
~~~ lowercase_aliases_dict [ lowercase_namespace ] = [ clean_id ( nid ) for nid in orig_aliases_dict ~~ ~~ return lowercase_aliases_dict \n 
~~ def merge_alias_dicts ( aliases1 , aliases2 ) : \n 
~~~ merged_aliases = copy . deepcopy ( aliases1 ) \n 
for ns , nid_list in aliases2 . iteritems ( ) : \n 
~~~ for nid in nid_list : \n 
~~~ if not nid in merged_aliases [ ns ] : \n 
~~~ merged_aliases [ ns ] . append ( nid ) \n 
~~~ merged_aliases [ ns ] = [ nid ] \n 
~~ ~~ ~~ return merged_aliases \n 
~~ def matches_alias ( product1 , product2 , exclude = [ ] ) : \n 
~~~ alias_tuple_list1 = [ alias_row . my_alias_tuple_for_comparing for alias_row in product1 . alias_rows alias_tuple_list2 = [ alias_row . my_alias_tuple_for_comparing for alias_row in product2 . alias_rows has_matches = False \n 
for alias_tuple1 in alias_tuple_list1 : \n 
~~~ if alias_tuple1 : \n 
~~~ ( ns , nid ) = alias_tuple1 \n 
if alias_tuple1 in alias_tuple_list2 and ns not in exclude : \n 
~~~ has_matches = True \n 
~~ ~~ ~~ return has_matches \n 
~~ class AliasRow ( db . Model ) : \n 
tiid = db . Column ( db . Text , db . ForeignKey ( ) , primary_key = True ) \n 
namespace = db . Column ( db . Text , primary_key = True ) \n 
nid = db . Column ( db . Text , primary_key = True ) \n 
collected_date = db . Column ( db . DateTime ( ) ) \n 
def __init__ ( self , ** kwargs ) : \n 
~~~ if "collected_date" not in kwargs : \n 
~~~ self . collected_date = datetime . datetime . utcnow ( ) \n 
~~ super ( AliasRow , self ) . __init__ ( ** kwargs ) \n 
def alias_tuple ( self ) : \n 
~~~ return ( self . namespace , self . nid ) \n 
def my_alias_tuple_for_comparing ( self ) : \n 
~~~ return clean_alias_tuple_for_comparing ( self . namespace , self . nid ) \n 
~~ def is_equivalent_alias ( self , given_namespace , given_nid ) : \n 
~~~ if not given_nid : \n 
~~ given_clean_alias = clean_alias_tuple_for_comparing ( given_namespace , given_nid ) \n 
if not given_clean_alias : \n 
~~ return given_clean_alias == self . my_alias_tuple_for_comparing \n 
~~ ~~ class Aliases ( object ) : \n 
~~~ def __init__ ( self , alias_rows ) : \n 
~~~ ignore_namepaces = [ "biblio" ] \n 
self . tiid = None \n 
for alias_row in alias_rows : \n 
~~~ if alias_row . namespace not in ignore_namepaces : \n 
~~~ self . tiid = alias_row . tiid \n 
~~~ getattr ( self , alias_row . namespace ) . append ( alias_row . nid ) \n 
~~~ setattr ( self , alias_row . namespace , [ alias_row . nid ] ) \n 
~~ ~~ ~~ ~~ @ cached_property \n 
def best_url ( self ) : \n 
~~~ if self . display_doi : \n 
~~~ return u"http://doi.org/" + self . display_doi \n 
~~ if self . display_pmid : \n 
~~~ return u"http://www.ncbi.nlm.nih.gov/pubmed/" + self . display_pmid \n 
~~ if self . display_pmc : \n 
~~~ return u"http://www.ncbi.nlm.nih.gov/pmc/articles/" + self . display_pmc \n 
~~ if self . resolved_url : \n 
~~~ return self . resolved_url \n 
~~~ return self . url [ 0 ] \n 
~~ ~~ @ cached_property \n 
~~~ return self . best_url \n 
def display_pmid ( self ) : \n 
~~~ return self . pmid [ 0 ] \n 
def display_pmc ( self ) : \n 
~~~ return self . pmc [ 0 ] \n 
def display_doi ( self ) : \n 
~~~ return self . doi [ 0 ] \n 
def display_arxiv ( self ) : \n 
~~~ return self . arxiv [ 0 ] \n 
def has_formal_alias ( self ) : \n 
~~~ if self . display_arxiv or self . display_doi or self . display_pmid or self . display_pmc : \n 
def resolved_url ( self ) : \n 
~~~ for url in self . url : \n 
~~~ if "doi.org" in url : \n 
~~ elif "ncbi.nlm.nih.gov/" in url : \n 
~~ elif "europepmc.org" in url : \n 
~~ elif "mendeley.com" in url : \n 
~~ elif "scopus.com" in url : \n 
~~~ return url \n 
~~ ~~ return self . url [ 0 ] \n 
~~ ~~ def get_genre ( self ) : \n 
~~~ return self . _guess_genre_and_host_from_aliases ( ) [ 0 ] \n 
~~ def get_host ( self ) : \n 
~~~ return self . _guess_genre_and_host_from_aliases ( ) [ 1 ] \n 
~~ def _guess_genre_and_host_from_aliases ( self ) : \n 
genre = "unknown" \n 
host = "unknown" \n 
if hasattr ( self , "doi" ) : \n 
~~~ joined_doi_string = "" . join ( self . doi ) . lower ( ) \n 
if "10.5061/dryad." in joined_doi_string : \n 
~~~ genre = "dataset" \n 
host = "dryad" \n 
~~ elif ".figshare." in joined_doi_string : \n 
~~~ host = "figshare" \n 
genre = "dataset" \n 
~~~ genre = "article" \n 
~~ ~~ elif hasattr ( self , "pmid" ) : \n 
~~ elif hasattr ( self , "arxiv" ) : \n 
host = "arxiv" \n 
~~ elif hasattr ( self , "blog" ) : \n 
~~~ genre = "blog" \n 
host = "wordpresscom" \n 
~~ elif hasattr ( self , "blog_post" ) : \n 
host = "blog_post" \n 
~~ elif hasattr ( self , "url" ) : \n 
~~~ joined_url_string = "" . join ( self . url ) . lower ( ) \n 
if "slideshare.net" in joined_url_string : \n 
~~~ genre = "slides" \n 
host = "slideshare" \n 
~~ elif "github.com" in joined_url_string : \n 
~~~ genre = "software" \n 
host = "github" \n 
~~ elif ( "youtube.com" in joined_url_string ) or ( "youtu.be" in joined_url_string ) : \n 
~~~ genre = "video" \n 
host = "youtube" \n 
~~ elif "vimeo.com" in joined_url_string : \n 
host = "vimeo" \n 
~~~ genre = "webpage" \n 
~~ ~~ return genre , host \n 
~~ def to_dict ( self ) : \n 
~~~ ret = dict_from_dir ( self ) \n 
return ret \n 
~~ ~~ from totalimpactwebapp import json_sqlalchemy \n 
from util import commit \n 
from util import as_int_or_float_if_possible \n 
from totalimpactwebapp . tweeter import Tweeter \n 
from birdy . twitter import AppClient , TwitterApiError , TwitterRateLimitError , TwitterClientError \n 
from collections import defaultdict \n 
from sqlalchemy import case \n 
def tweets_from_tiids ( tiids ) : \n 
~~~ if not tiids : \n 
~~ tweets = db . session . query ( Tweet ) . filter ( Tweet . tiid . in_ ( tiids ) ) . all ( ) \n 
return tweets \n 
~~ def get_product_tweets_for_profile ( profile_id ) : \n 
~~~ tweets = db . session . query ( Tweet ) . filter ( Tweet . profile_id == profile_id ) . all ( ) \n 
response = defaultdict ( list ) \n 
for tweet in tweets : \n 
~~~ if tweet . tiid and tweet . tweet_text : \n 
~~~ response [ tweet . tiid ] . append ( tweet ) \n 
~~ ~~ return response \n 
~~ def store_tweet_payload_and_tweeter_from_twitter ( payload_dicts_from_twitter , tweets ) : \n 
~~~ tweets_by_tweet_id = defaultdict ( list ) \n 
~~~ tweets_by_tweet_id [ tweet . tweet_id ] . append ( tweet ) \n 
~~ for payload_dict in payload_dicts_from_twitter : \n 
~~~ tweet_id = payload_dict [ "id_str" ] \n 
tweet_id = tweet_id ) ) \n 
for tweet in tweets_by_tweet_id [ tweet_id ] : \n 
~~~ if not tweet . payload : \n 
~~~ tweet . payload = payload_dict \n 
tweet_id = tweet_id , tiid = tweet . tiid ) ) \n 
if "user" in payload_dict : \n 
~~~ tweet . tweeter . set_attributes_from_twitter_data ( payload_dict [ "user" ] ) \n 
~~~ tweeter = Tweeter . query . get ( tweet . screen_name ) \n 
if not tweeter : \n 
~~~ tweeter = Tweeter ( screen_name = tweet . screen_name ) \n 
db . session . add ( tweeter ) \n 
~~ tweeter . set_attributes_from_twitter_data ( payload_dict [ "user" ] ) \n 
tweet . tweeter = tweeter \n 
commit ( db ) \n 
~~ if tweet . tweeter : \n 
screen_name = tweet . tweeter . screen_name ) ) \n 
~~ ~~ ~~ ~~ ~~ ~~ def flag_deleted_tweets ( tweet_ids ) : \n 
~~~ if not tweet_ids : \n 
~~ for tweet in Tweet . query . filter ( Tweet . tweet_id . in_ ( tweet_ids ) ) . all ( ) : \n 
~~~ tweet . is_deleted = True \n 
db . session . merge ( tweet ) \n 
~~ ~~ def handle_all_tweets ( data , tweets ) : \n 
~~~ store_tweet_payload_and_tweeter_from_twitter ( data , tweets ) \n 
tweet_ids = [ tweet . tweet_id for tweet in tweets ] \n 
tweet_ids_with_response = [ tweet [ "id_str" ] for tweet in data ] \n 
tweet_ids_without_response = [ tweet for tweet in tweet_ids if tweet not in tweet_ids_with_response flag_deleted_tweets ( tweet_ids_without_response ) \n 
~~ class AppDictClient ( AppClient ) : \n 
def get_json_object_hook ( data ) : \n 
~~ ~~ def get_and_save_tweet_text_and_tweeter_followers ( tweets ) : \n 
~~~ client = AppDictClient ( \n 
os . getenv ( "TWITTER_CONSUMER_KEY" ) , \n 
os . getenv ( "TWITTER_CONSUMER_SECRET" ) , \n 
access_token = os . getenv ( "TWITTER_ACCESS_TOKEN" ) \n 
num = len ( tweets ) ) ) \n 
group_size = 100 \n 
list_of_groups = [ tweets [ i : i + group_size ] for i in range ( 0 , len ( tweets ) , group_size ) ] \n 
for tweet_subset in list_of_groups : \n 
~~~ tweet_id_string = "," . join ( [ tweet . tweet_id for tweet in tweet_subset ] ) \n 
~~~ response = client . api . statuses . lookup . post ( id = tweet_id_string , trim_user = False ) \n 
handle_all_tweets ( response . data , tweet_subset ) \n 
~~ except TwitterApiError , e : \n 
~~ except TwitterClientError , e : \n 
~~ except TwitterRateLimitError , e : \n 
~~ ~~ return \n 
~~ def hydrate_twitter_text_and_followers ( profile_id , altmetric_twitter_posts ) : \n 
profile_id = profile_id ) ) \n 
tweets_to_hydrate_from_twitter = [ ] \n 
tweets = Tweet . query . filter ( Tweet . profile_id == profile_id ) \n 
tweet_dict = dict ( [ ( ( tweet . tweet_id , tweet . tiid ) , tweet ) for tweet in tweets ] ) \n 
for tiid , post_list in altmetric_twitter_posts . iteritems ( ) : \n 
~~~ for post in post_list : \n 
~~~ tweet_id = post [ "tweet_id" ] \n 
screen_name = post [ "author" ] [ "id_on_source" ] \n 
if ( tweet_id , tiid ) in tweet_dict . keys ( ) : \n 
~~~ tweet = tweet_dict [ ( tweet_id , tiid ) ] \n 
if not tweet . tweet_text and not tweet . is_deleted : \n 
~~~ tweets_to_hydrate_from_twitter . append ( tweet ) \n 
~~~ if not Tweet . query . get ( ( tweet_id , tiid ) ) : \n 
~~~ tweet = Tweet ( tweet_id = tweet_id , tiid = tiid ) \n 
tweet . set_attributes_from_altmetric_post ( post ) \n 
tweet . profile_id = profile_id \n 
tweets_to_hydrate_from_twitter . append ( tweet ) \n 
db . session . add ( tweet ) \n 
~~ if not tweet . tweeter : \n 
~~~ tweeter = Tweeter . query . get ( screen_name ) \n 
~~~ tweeter = Tweeter ( screen_name = screen_name ) \n 
~~ tweeter . set_attributes_from_altmetric_post ( post ) \n 
~~ commit ( db ) \n 
if tweets_to_hydrate_from_twitter : \n 
~~~ commit ( db ) \n 
tweet_ids = [ tweet . tweet_id for tweet in tweets_to_hydrate_from_twitter ] \n 
get_and_save_tweet_text_and_tweeter_followers ( tweets_to_hydrate_from_twitter ) \n 
~~ return \n 
class Tweet ( db . Model ) : \n 
~~~ tweet_id = db . Column ( db . Text , primary_key = True ) \n 
profile_id = db . Column ( db . Integer , db . ForeignKey ( ) ) \n 
screen_name = db . Column ( db . Text , db . ForeignKey ( ) ) \n 
tweet_timestamp = db . Column ( db . DateTime ( ) ) \n 
payload = db . Column ( json_sqlalchemy . JSONAlchemy ( db . Text ) ) \n 
tweeter = db . relationship ( \n 
, \n 
lazy = , \n 
cascade = , \n 
backref = db . backref ( "tweet" ) , \n 
uselist = False , \n 
primaryjoin = handle_workaround_join_string \n 
~~~ if "payload" in kwargs : \n 
~~~ payload_dict = kwargs [ "payload" ] \n 
kwargs [ "tweet_id" ] = payload_dict [ "id_str" ] \n 
kwargs [ "screen_name" ] = payload_dict [ "user" ] [ "screen_name" ] \n 
kwargs [ "payload" ] = payload_dict \n 
~~~ kwargs [ "country" ] = payload_dict [ "place" ] [ "country_code" ] \n 
~~ except ( AttributeError , TypeError ) : \n 
~~ ~~ ~~ super ( Tweet , self ) . __init__ ( ** kwargs ) \n 
def most_recent_tweet_id ( cls , screen_name ) : \n 
~~~ screen_name = screen_name . replace ( "@" , "" ) \n 
q = db . session . query ( Tweet ) . filter ( Tweet . screen_name == screen_name ) . order_by ( Tweet . tweet_timestamp tweet = q . first ( ) \n 
~~~ tweet_id = tweet . tweet_id \n 
~~~ tweet_id = None \n 
~~ return tweet_id \n 
def tweet_text ( self ) : \n 
~~~ return self . payload [ "text" ] \n 
def tweet_text_with_links ( self ) : \n 
~~~ if self . tweet_text is None : \n 
~~ ret = self . tweet_text \n 
ret = re . sub ( r"(http://.+?)(\\s|$)" , r"<link>" , ret ) \n 
for url_info in self . urls : \n 
url = url_info [ "expanded_url" ] , \n 
display_url = url_info [ "display_url" ] \n 
ret = re . sub ( r"<link>" , my_link , ret , 1 ) \n 
def urls ( self ) : \n 
~~~ return self . payload [ "entities" ] [ "urls" ] \n 
def has_country ( self ) : \n 
~~~ return self . country != None \n 
~~ def set_attributes_from_altmetric_post ( self , post ) : \n 
~~~ self . tweet_id = post [ "tweet_id" ] \n 
self . screen_name = post [ "author" ] [ "id_on_source" ] \n 
self . tweet_timestamp = post [ "posted_on" ] \n 
if "geo" in post [ "author" ] : \n 
~~~ self . country = post [ "author" ] [ "geo" ] . get ( "country" , None ) \n 
~~ return self \n 
~~~ return . format ( \n 
tweet_id = self . tweet_id , \n 
profile_id = self . profile_id , \n 
screen_name = self . screen_name , \n 
timestamp = self . tweet_timestamp ) \n 
~~~ attributes_to_ignore = [ \n 
"payload" \n 
ret = dict_from_dir ( self , attributes_to_ignore ) \n 
def load_gender_data ( ntrain = 10000 , ntest = 10000 ) : \n 
~~~ import pandas as pd \n 
file_loc = os . path . dirname ( os . path . realpath ( __file__ ) ) \n 
fullpath = os . path . join ( file_loc , relative_path ) \n 
data = pd . read_csv ( fullpath , nrows = ntrain + ntest ) \n 
X = data [ ] . values \n 
Y = data [ ] . values \n 
trX = X [ : - ntest ] \n 
teX = X [ - ntest : ] \n 
trY = Y [ : - ntest ] \n 
teY = Y [ - ntest : ] \n 
return trX , teX , trY , teY \n 
~~ def load_mnist ( data_dir = None ) : \n 
~~~ if data_dir is None : \n 
~~~ import urllib \n 
import gzip \n 
url = \n 
fnames = [ \n 
for fname in fnames : \n 
~~~ if not os . path . isfile ( fname ) : \n 
~~~ print , fname \n 
urllib . urlretrieve ( url + fname , fname ) \n 
~~ ~~ data_dir = \n 
~~ fd = gzip . open ( os . path . join ( data_dir , ) ) \n 
loaded = np . fromstring ( fd . read ( ) , dtype = np . uint8 ) \n 
trX = loaded [ 16 : ] . reshape ( ( 60000 , - 1 ) ) \n 
fd = gzip . open ( os . path . join ( data_dir , ) ) \n 
trY = loaded [ 8 : ] . reshape ( ( 60000 ) ) \n 
teX = loaded [ 16 : ] . reshape ( ( 10000 , - 1 ) ) \n 
teY = loaded [ 8 : ] . reshape ( ( 10000 ) ) \n 
trX = trX / 255. \n 
teX = teX / 255. \n 
trX = trX . reshape ( - 1 , 28 , 28 ) \n 
teX = teX . reshape ( - 1 , 28 , 28 ) \n 
import commands \n 
from utils import get_temporary_location \n 
from utils import delete_repository \n 
from gitpy import LocalRepository \n 
from gitpy import find_repository \n 
from gitpy . exceptions import GitException \n 
class EmptyRepositoryTest ( unittest . TestCase ) : \n 
~~~ self . dirname = get_temporary_location ( ) \n 
self . repo = LocalRepository ( self . dirname ) \n 
self . assertFalse ( os . path . exists ( self . dirname ) ) \n 
self . assertFalse ( self . repo . isValid ( ) ) \n 
~~~ if os . path . exists ( self . dirname ) : \n 
~~~ delete_repository ( self . repo ) \n 
~~ ~~ ~~ class BasicRepositories ( EmptyRepositoryTest ) : \n 
~~~ def testRepositoryInit ( self ) : \n 
~~~ self . repo . init ( ) \n 
self . assertTrue ( self . repo . isValid ( ) ) \n 
self . failUnless ( os . path . isdir ( self . dirname ) ) \n 
self . failUnless ( os . path . isdir ( os . path . join ( self . dirname , ".git" ) ) ) \n 
~~ def testConfiguration ( self ) : \n 
self . repo . config . setParameter ( , 2 ) \n 
self . assertEquals ( self . repo . config . getParameter ( ) , ) \n 
~~ def testRepositoryInitWhenExists ( self ) : \n 
~~~ os . mkdir ( self . dirname ) \n 
self . repo . init ( ) \n 
~~ ~~ class ModifiedRepositoryTest ( EmptyRepositoryTest ) : \n 
~~~ FILENAME = "test.txt" \n 
~~~ super ( ModifiedRepositoryTest , self ) . setUp ( ) \n 
with open ( os . path . join ( self . repo . path , self . FILENAME ) , "wb" ) as f : \n 
~~~ print >> f , "Hey!" \n 
~~ self . assertFalse ( self . repo . isWorkingDirectoryClean ( ) ) \n 
~~ ~~ class ModifiedRepositories ( ModifiedRepositoryTest ) : \n 
~~~ def testStatus ( self ) : \n 
~~~ untracked = self . repo . getUntrackedFiles ( ) \n 
self . assertEquals ( untracked , [ self . FILENAME ] ) \n 
~~ def testAdding ( self ) : \n 
~~~ untracked_files = self . repo . getUntrackedFiles ( ) \n 
for u in untracked_files : \n 
~~~ self . repo . add ( u ) \n 
~~ self . assertEquals ( self . repo . getStagedFiles ( ) , untracked_files ) \n 
self . assertFalse ( self . repo . isWorkingDirectoryClean ( ) ) \n 
~~ def testCommitting ( self ) : \n 
~~~ self . repo . addAll ( ) \n 
self . assertNotEquals ( self . repo . getStagedFiles ( ) , [ ] ) \n 
self . assertTrue ( self . repo . isWorkingDirectoryClean ( ) ) \n 
self . assertEquals ( self . repo . getStagedFiles ( ) , [ ] ) \n 
~~ ~~ class CleaningUntrackedFiles ( ModifiedRepositoryTest ) : \n 
~~~ def _clean ( self ) : \n 
~~~ self . repo . cleanUntrackedFiles ( ) \n 
self . failIf ( self . repo . getUntrackedFiles ( ) ) \n 
~~ def testCleaningUpUntrackedFiles ( self ) : \n 
~~~ with open ( os . path . join ( self . repo . path , "dirty_file" ) , "wb" ) as f : \n 
~~~ print >> f , "data" \n 
~~ self . failUnless ( self . repo . getUntrackedFiles ( ) ) \n 
self . _clean ( ) \n 
dirpath = os . path . join ( self . repo . path , "unused_directory" ) \n 
os . mkdir ( dirpath ) \n 
self . failIf ( os . path . exists ( dirpath ) ) \n 
~~ ~~ class TestAPI ( ModifiedRepositoryTest ) : \n 
~~~ def test_find_repository ( self ) : \n 
~~~ prev_path = os . path . realpath ( "." ) \n 
subpath = os . path . join ( self . repo . path , "a" , "b" , "c" ) \n 
os . makedirs ( subpath ) \n 
os . chdir ( subpath ) \n 
~~~ repo = find_repository ( ) \n 
~~~ os . chdir ( prev_path ) \n 
~~ self . failUnless ( repo . path == self . repo . path ) \n 
~~ import logging \n 
from okcupyd . db import model , txn , with_txn \n 
log = logging . getLogger ( __name__ ) \n 
class UserAdapter ( object ) : \n 
~~~ def __init__ ( self , profile ) : \n 
~~~ self . profile = profile \n 
~~ def build ( self , session ) : \n 
~~~ found = model . User . query_no_txn ( session , model . User . handle == \n 
self . profile . username ) \n 
~~~ return found [ 0 ] \n 
~~~ return model . User ( okc_id = self . profile . id , \n 
handle = self . profile . username , \n 
age = self . profile . age , \n 
location = self . profile . location ) \n 
~~ ~~ def get_no_txn ( self , session ) : \n 
~~~ return model . User . upsert_one_no_txn ( session , self . build ( session ) , \n 
id_key = ) \n 
~~ get = with_txn ( get_no_txn ) \n 
~~ class ThreadAdapter ( object ) : \n 
~~~ def __init__ ( self , thread ) : \n 
~~~ self . thread = thread \n 
~~ def _get_thread ( self , session ) : \n 
~~~ initiator = UserAdapter ( self . thread . initiator ) . get_no_txn ( session ) \n 
respondent = UserAdapter ( self . thread . respondent ) . get_no_txn ( session ) \n 
message_thread = model . MessageThread ( okc_id = self . thread . id , \n 
initiator = initiator , \n 
respondent = respondent ) \n 
return model . MessageThread . upsert_one_no_txn ( session , message_thread , \n 
~~ def _add_messages ( self , thread_model ) : \n 
~~~ existing_message_ids = set ( [ m . okc_id for m in thread_model . messages ] ) \n 
new_messages = [ message for message in self . thread . messages \n 
if message . id not in existing_message_ids ] \n 
new_message_models = [ ] \n 
for new_message in new_messages : \n 
~~~ from_initiator = thread_model . initiator . handle . lower ( ) == new_message . sender . username . lower ( ) \n 
sender , recipient = ( thread_model . initiator , \n 
thread_model . respondent ) if from_initiator else ( thread_model . respondent , \n 
thread_model . initiator ) \n 
new_message_model = model . Message ( okc_id = new_message . id , \n 
text = new_message . content , \n 
sender = sender , \n 
recipient = recipient , \n 
time_sent = new_message . time_sent ) \n 
new_message_models . append ( new_message_model ) \n 
thread_model . messages . append ( new_message_model ) \n 
~~ return new_message_models \n 
~~ def add_messages ( self ) : \n 
~~~ with txn ( ) as session : \n 
~~~ thread_model = model . MessageThread . find_no_txn ( session , \n 
self . thread . id , \n 
return self . _add_messages ( thread_model ) \n 
~~ ~~ def get_thread ( self ) : \n 
~~~ thread_model = self . _get_thread ( session ) \n 
return thread_model , self . _add_messages ( thread_model ) \n 
~~ ~~ ~~ import logging \n 
from invoke import task \n 
import IPython \n 
from okcupyd import db \n 
from okcupyd import util \n 
from okcupyd . db import mailbox , model \n 
from okcupyd . user import User \n 
@ task ( default = True ) \n 
def session ( ) : \n 
~~~ with db . txn ( ) as session : \n 
~~~ IPython . embed ( ) \n 
~~ ~~ @ task \n 
def reset ( ) : \n 
~~~ util . enable_logger ( __name__ ) \n 
log . info ( db . Base . metadata . bind ) \n 
db . Base . metadata . drop_all ( ) \n 
db . Base . metadata . create_all ( ) \n 
~~ @ task \n 
def sync ( ) : \n 
~~~ user = User ( ) \n 
mailbox . Sync ( user ) . all ( ) \n 
log . info ( model . Message . query ( model . User . okc_id == user . profile . id ) ) \n 
def make ( ) : \n 
user_model = model . User . from_profile ( user . profile ) \n 
user_model . upsert_model ( id_key = ) \n 
okcupyd_user = model . OKCupydUser ( user_id = user_model . id ) \n 
okcupyd_user . upsert_model ( id_key = ) \n 
return okcupyd_user \n 
~~ from . import util \n 
from okcupyd import User , photo \n 
@ util . use_cassette ( path = , \n 
match_on = util . match_on_no_body ) \n 
def test_photo_upload ( ) : \n 
~~~ uploader = photo . PhotoUploader ( ) \n 
upload_response_dict = uploader . upload_and_confirm ( ) \n 
assert int ( upload_response_dict [ ] ) > 0 \n 
~~ @ util . use_cassette ( path = , match_on = util . match_on_no_body ) \n 
def test_photo_delete ( ) : \n 
response_dict = user . photo . upload_and_confirm ( user . quickmatch ( ) . photo_infos [ 0 ] ) \n 
before_delete_photos = user . profile . photo_infos \n 
user . photo . delete ( response_dict [ ] ) \n 
user . profile . refresh ( ) \n 
assert len ( before_delete_photos ) - 1 == len ( user . profile . photo_infos ) \n 
~~ def test_make_photo_uri_from_https_link ( ) : \n 
~~~ photo_info = photo . Info . from_cdn_uri ( \n 
assert photo_info . id == 2254475731855279447 \n 
assert photo_info . thumb_nail_top == 21 \n 
~~ @ util . use_cassette \n 
def test_photo_info_upload ( vcr_live_sleep ) : \n 
response = user . photo . upload_and_confirm ( user . quickmatch ( ) . photo_infos [ 0 ] ) \n 
vcr_live_sleep ( 2 ) \n 
assert int ( response [ ] ) in [ pi . id for pi in user . profile . photo_infos ] \n 
~~ import theano \n 
import theano . tensor as T \n 
from theano . sandbox . rng_mrg import MRG_RandomStreams \n 
from theano . tensor . nnet . conv import conv2d \n 
from theano . tensor . signal . downsample import max_pool_2d \n 
from theano . tensor . shared_randomstreams import RandomStreams \n 
from toolbox import * \n 
from modelbase import * \n 
class LM_gru ( ModelLMBase ) : \n 
~~~ def __init__ ( self , data , hp ) : \n 
~~~ super ( LM_gru , self ) . __init__ ( self . __class__ . __name__ , data , hp ) \n 
self . n_h = 256 \n 
self . dropout = 0.5 \n 
self . params = Parameters ( ) \n 
self . hiddenstates = Parameters ( ) \n 
n_tokens = self . data [ ] \n 
n_h = self . n_h \n 
scale = hp . init_scale \n 
gates = 3 \n 
with self . hiddenstates : \n 
~~~ b1_h = shared_zeros ( ( self . hp . batch_size , n_h ) ) \n 
b2_h = shared_zeros ( ( self . hp . batch_size , n_h ) ) \n 
~~ if hp . load_model and os . path . isfile ( self . filename ) : \n 
~~~ self . params . load ( self . filename ) \n 
~~~ with self . params : \n 
~~~ W_emb = shared_normal ( ( n_tokens , n_h ) , scale = scale ) \n 
W1 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
V1 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
b1 = shared_zeros ( ( n_h * gates ) ) \n 
W2 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
V2 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
b2 = shared_zeros ( ( n_h * gates , ) ) \n 
~~ ~~ def lstm ( X , h , c , W , U , b ) : \n 
~~~ g_on = T . dot ( X , W ) + T . dot ( h , U ) + b \n 
i_on = T . nnet . sigmoid ( g_on [ : , : n_h ] ) \n 
f_on = T . nnet . sigmoid ( g_on [ : , n_h : 2 * n_h ] ) \n 
o_on = T . nnet . sigmoid ( g_on [ : , 2 * n_h : 3 * n_h ] ) \n 
c = f_on * c + i_on * T . tanh ( g_on [ : , 3 * n_h : ] ) \n 
h = o_on * T . tanh ( c ) \n 
return h , c \n 
~~ def gru ( X , h , W , U , b ) : \n 
~~~ z_t = T . nnet . sigmoid ( T . dot ( X , W [ : , : n_h ] ) + T . dot ( h , U [ : , : n_h ] ) + b [ : n_h ] ) \n 
r_t = T . nnet . sigmoid ( T . dot ( X , W [ : , n_h : 2 * n_h ] ) + T . dot ( h , U [ : , n_h : 2 * n_h ] ) + b [ n_h : 2 * n_h ] ) \n 
h_t = T . tanh ( T . dot ( X , W [ : , 2 * n_h : 3 * n_h ] ) + r_t * T . dot ( h , U [ : , 2 * n_h : 3 * n_h ] ) + b [ 2 * n_h : 3 * n_h return ( 1 - z_t ) * h + z_t * h_t \n 
~~ def sgru ( X , h , W , U , b ) : \n 
~~~ z_t = T . tanh ( T . dot ( X , W [ : , : n_h ] ) + T . dot ( h , U [ : , : n_h ] ) + b [ : n_h ] ) \n 
h_t = T . tanh ( T . dot ( X , W [ : , 1 * n_h : 2 * n_h ] ) + T . dot ( h , U [ : , 1 * n_h : 2 * n_h ] ) + b [ 1 * n_h : 2 * n_h ] ) \n 
return z_t * h_t \n 
~~ def model ( x , p , p_dropout ) : \n 
~~~ input_size = x . shape [ 1 ] \n 
h0 = dropout ( h0 , p_dropout ) \n 
cost , h1 , h2 = [ 0. , b1_h , b2_h ] \n 
for t in xrange ( 0 , self . hp . seq_size ) : \n 
~~~ if t >= self . hp . warmup_size : \n 
~~~ pyx = softmax ( T . dot ( dropout ( h2 , p_dropout ) , T . transpose ( p . W_emb ) ) ) \n 
cost += T . sum ( T . nnet . categorical_crossentropy ( pyx , theano_one_hot ( x [ t ] , n_tokens \n 
~~ h1 = gru ( h0 [ t ] , h1 , p . W1 , p . V1 , p . b1 ) \n 
h2 = gru ( dropout ( h1 , p_dropout ) , h2 , p . W2 , p . V2 , p . b2 ) \n 
~~ h_updates = [ ( b1_h , h1 ) , ( b2_h , h2 ) ] \n 
return cost , h_updates \n 
~~ cost , h_updates = model ( self . X , self . params , self . dropout ) \n 
te_cost , te_h_updates = model ( self . X , self . params , 0. ) \n 
self . compile ( cost , te_cost , h_updates , te_h_updates ) \n 
#!/usr/bin/python \n 
import csv \n 
def csvOutput ( queryResult , separator = , quote = \'"\' ) : \n 
csvWriter = csv . writer ( sys . stdout , delimiter = separator , quotechar = quote , \n 
quoting = csv . QUOTE_MINIMAL ) \n 
for line in queryResult : \n 
~~~ csvWriter . writerow ( line ) \n 
~~ ~~ import sys , os , stat \n 
import pythoncom \n 
from win32com . shell import shell , shellcon \n 
import commctrl \n 
import winerror \n 
from win32com . server . util import wrap \n 
from pywintypes import IID \n 
IPersist_Methods = [ "GetClassID" ] \n 
IColumnProvider_Methods = IPersist_Methods + [ "Initialize" , "GetColumnInfo" , "GetItemData" ] \n 
class ColumnProvider : \n 
~~~ _reg_progid_ = "Python.ShellExtension.ColumnProvider" \n 
_reg_clsid_ = IID ( "{0F14101A-E05E-4070-BD54-83DFA58C3D68}" ) \n 
_com_interfaces_ = [ pythoncom . IID_IPersist , \n 
shell . IID_IColumnProvider , \n 
_public_methods_ = IColumnProvider_Methods \n 
def GetClassID ( self ) : \n 
~~~ return self . _reg_clsid_ \n 
~~ def Initialize ( self , colInit ) : \n 
~~~ flags , reserved , name = colInit \n 
~~ def GetColumnInfo ( self , index ) : \n 
~~~ if index in [ 0 , 1 ] : \n 
~~~ if index == 0 : \n 
~~~ ext = ".pyc" \n 
~~~ ext = ".pyo" \n 
col_info = ( \n 
20 , #cChars \n 
title , \n 
description ) \n 
return col_info \n 
~~ def GetItemData ( self , colid , colData ) : \n 
~~~ fmt_id , pid = colid \n 
fmt_id == self . _reg_clsid_ \n 
flags , attr , reserved , ext , name = colData \n 
if ext . lower ( ) not in [ ".py" , ".pyw" ] : \n 
~~ if pid == 0 : \n 
~~ check_file = os . path . splitext ( name ) [ 0 ] + ext \n 
~~~ st = os . stat ( check_file ) \n 
return st [ stat . ST_SIZE ] \n 
~~ ~~ ~~ def DllRegisterServer ( ) : \n 
~~~ import _winreg \n 
key = _winreg . CreateKey ( _winreg . HKEY_CLASSES_ROOT , \n 
"Folder\\\\ShellEx\\\\ColumnHandlers\\\\" + str ( ColumnProvider . _reg_clsid_ ) ) \n 
_winreg . SetValueEx ( key , None , 0 , _winreg . REG_SZ , ColumnProvider . _reg_desc_ ) \n 
~~ def DllUnregisterServer ( ) : \n 
~~~ key = _winreg . DeleteKey ( _winreg . HKEY_CLASSES_ROOT , \n 
~~ except WindowsError , details : \n 
~~~ import errno \n 
if details . errno != errno . ENOENT : \n 
~~~ from win32com . server import register \n 
register . UseCommandLine ( ColumnProvider , \n 
finalize_register = DllRegisterServer , \n 
finalize_unregister = DllUnregisterServer ) \n 
~~ def __load ( ) : \n 
~~~ import imp , os , sys \n 
~~~ dirname = os . path . dirname ( __loader__ . archive ) \n 
~~~ dirname = sys . prefix \n 
~~ path = os . path . join ( dirname , ) \n 
mod = imp . load_dynamic ( __name__ , path ) \n 
~~ __load ( ) \n 
del __load \n 
class LoggerFactory ( object ) : \n 
~~~ _isSetup = False \n 
def __init__ ( self , level = logging . DEBUG ) : \n 
~~~ if LoggerFactory . _isSetup is False : \n 
~~~ logger = logging . getLogger ( "openob" ) \n 
logger . setLevel ( level ) \n 
formatter = logging . Formatter ( ) \n 
ch = logging . StreamHandler ( ) \n 
ch . setLevel ( level ) \n 
ch . setFormatter ( formatter ) \n 
logger . addHandler ( ch ) \n 
LoggerFactory . _isSetup = True \n 
~~ ~~ def getLogger ( self , name , level = logging . DEBUG ) : \n 
~~~ logger = logging . getLogger ( "openob.%s" % name ) \n 
return logger \n 
~~ ~~ from . functions import * \n 
from pysd import functions \n 
def time ( ) : \n 
~~~ return _t \n 
~~ def flowa ( ) : \n 
return 0.1 \n 
~~ def stocka ( ) : \n 
~~~ return _state [ ] \n 
~~ def _stocka_init ( ) : \n 
~~~ return - 5 \n 
~~ def _dstocka_dt ( ) : \n 
~~~ return flowa ( ) \n 
~~ def test_exp ( ) : \n 
return np . exp ( stocka ( ) ) \n 
~~ def final_time ( ) : \n 
return 100 \n 
~~ def initial_time ( ) : \n 
return 0 \n 
~~ def saveper ( ) : \n 
return time_step ( ) \n 
~~ def time_step ( ) : \n 
__all__ = [ \n 
from myapp import utils \n 
module_name = utils . getFinalName ( __name__ ) \n 
module = utils . getModule ( __name__ , subdomain = module_name ) \n 
import views \n 
import subprocess \n 
def perform_testing ( config ) : \n 
~~~ requirements = { \n 
"MakeMKV" : "makemkvcon" , \n 
"Filebot" : "filebot" , \n 
"HandBrake" : "HandBrakeCLI" , \n 
print "" \n 
for req in requirements : \n 
~~~ print checkcommand ( requirements [ req ] ) , req \n 
~~ sys . exit ( 0 ) \n 
~~ def canwrite ( path ) : \n 
~~~ ret = booltostatus ( os . access ( path , os . W_OK | os . X_OK ) ) \n 
~~~ ret = False \n 
~~~ return ret \n 
~~ ~~ def booltostatus ( inbool ) : \n 
~~~ if inbool : \n 
~~ ~~ def checkcommand ( com ) : \n 
~~~ proc = subprocess . Popen ( \n 
str ( com ) \n 
stdout = subprocess . PIPE \n 
return booltostatus ( len ( proc . stdout . read ( ) ) > 0 ) \n 
from collections import namedtuple \n 
from uuid import uuid4 \n 
from django . http import HttpResponse \n 
from django . contrib . gis . db . models . query import GeoQuerySet \n 
from django . contrib . gis . db . models import GeometryField \n 
from django import forms as f \n 
from django . shortcuts import render_to_response \n 
from ga_ows . views import common \n 
from ga_ows . utils import MultipleValueField , BBoxField , CaseInsensitiveDict \n 
from lxml import etree \n 
from ga_ows . views . common import RequestForm , CommonParameters , GetCapabilitiesMixin \n 
from osgeo import ogr \n 
from tempfile import gettempdir \n 
from django . db import connections \n 
class InputParameters ( RequestForm ) : \n 
srs_name = f . CharField ( ) \n 
srs_format = f . CharField ( required = False ) \n 
def from_request ( cls , request ) : \n 
~~~ request [ ] = request . get ( , ) \n 
~~ ~~ class PresentationParameters ( RequestForm ) : \n 
~~~ count = f . IntegerField ( ) \n 
start_index = f . IntegerField ( ) \n 
max_features = f . IntegerField ( ) \n 
output_format = f . CharField ( ) \n 
~~~ request [ ] = int ( request . get ( , ) ) \n 
request [ ] = int ( request . get ( , ) ) \n 
~~ ~~ class AdHocQueryParameters ( RequestForm ) : \n 
~~~ type_names = MultipleValueField ( ) \n 
aliases = MultipleValueField ( required = False ) \n 
filter = f . CharField ( required = False ) \n 
filter_language = f . CharField ( required = False ) \n 
resource_id = f . CharField ( required = False ) \n 
bbox = BBoxField ( ) \n 
sort_by = f . CharField ( required = False ) \n 
~~~ request [ ] = request . getlist ( ) \n 
request [ ] = request . getlist ( ) \n 
request [ ] = request . get ( ) \n 
~~ ~~ class StoredQueryParameters ( RequestForm ) : \n 
~~~ stored_query_id = f . CharField ( required = False ) \n 
~~~ request [ ] = request . get ( ) \n 
~~ ~~ class GetFeatureByIdParameters ( RequestForm ) : \n 
~~~ feature_id = f . CharField ( ) \n 
~~ ~~ class ResolveParameters ( RequestForm ) : \n 
~~~ resolve = f . CharField ( required = False ) \n 
resolve_depth = f . IntegerField ( ) \n 
resolve_timeout = f . FloatField ( ) \n 
request [ ] = float ( request . get ( , ) ) \n 
~~ ~~ class CannotLockAllFeatures ( common . OWSException ) : \n 
~~ class DuplicateStoredQueryIdValue ( common . OWSException ) : \n 
~~ class DuplicateStoredQueryParameterName ( common . OWSException ) : \n 
~~ class FeaturesNotLocked ( common . OWSException ) : \n 
~~ class InvalidLockId ( common . OWSException ) : \n 
~~ class InvalidValue ( common . OWSException ) : \n 
~~ class LockHasExpired ( common . OWSException ) : \n 
~~ class OperationParsingFailed ( common . OWSException ) : \n 
~~ class OperationProcessingFailed ( common . OWSException ) : \n 
~~ class ResponseCacheExpired ( common . OWSException ) : \n 
~~ class OperationNotSupported ( common . OWSException ) : \n 
######################################################################################################################## \n 
~~ FeatureDescription = namedtuple ( , ( , , , , , \n 
#: \n 
StoredQueryParameter = namedtuple ( "StoredQueryParameter" , ( , , , , \n 
class WFSAdapter ( object ) : \n 
def get_feature_descriptions ( self , request , * types ) : \n 
~~~ raise OperationNotSupported . at ( , \n 
~~ def list_stored_queries ( self , request ) : \n 
queries = dict ( [ ( q [ 3 : ] , [ ] ) for q in filter ( lambda x : x . startswith ( "SQ_" ) , \n 
reduce ( \n 
list . __add__ , \n 
[ c . __dict__ . keys ( ) for c in self . __class__ . mro ( ) ] \n 
) ] ) \n 
return queries \n 
~~ def get_features ( self , request , parms ) : \n 
~~ def supports_feature_versioning ( self ) : \n 
~~ ~~ class GeoDjangoWFSAdapter ( WFSAdapter ) : \n 
~~~ def __init__ ( self , models ) : \n 
~~~ self . models = { } \n 
self . srids = { } \n 
for model in models : \n 
~~~ self . models [ model . _meta . app_label + ":" + model . _meta . object_name ] = model \n 
for field in model . _meta . fields : \n 
~~~ if isinstance ( field , GeometryField ) : \n 
~~~ self . geometries [ model . _meta . app_label + ":" + model . _meta . object_name ] = field \n 
self . srids [ model . _meta . app_label + ":" + model . _meta . object_name ] = field . srid \n 
~~ ~~ ~~ ~~ def list_stored_queries ( self , request ) : \n 
~~~ sq = super ( GeoDjangoWFSAdapter , self ) . list_stored_queries ( request ) \n 
fts = list ( self . models . keys ( ) ) \n 
for k in sq . keys ( ) : \n 
~~~ sq [ k ] = StoredQueryDescription ( name = k , feature_types = fts , title = k , parameters = [ ] ) \n 
~~ return sq \n 
~~ def get_feature_descriptions ( self , request , * types ) : \n 
for model in self . models . values ( ) : \n 
~~~ if model . objects . count ( ) > 0 : \n 
~~~ extent = model . objects . extent ( ) \n 
~~~ extent = ( 0 , 0 , 0 , 0 ) \n 
~~ yield FeatureDescription ( \n 
ns = namespace , \n 
ns_name = model . _meta . app_label , \n 
name = model . _meta . object_name , \n 
abstract = model . __doc__ , \n 
title = model . _meta . verbose_name , \n 
keywords = [ ] , \n 
srs = self . srids [ model . _meta . app_label + ":" + model . _meta . object_name ] , \n 
bbox = extent , \n 
schema = namespace \n 
~~ ~~ def get_features ( self , request , parms ) : \n 
~~~ if parms . cleaned_data [ ] : \n 
~~~ squid = "SQ_" + parms . cleaned_data [ ] \n 
~~~ return self . __getattribute__ ( squid ) ( request , parms ) \n 
~~~ raise OperationNotSupported . at ( , . format ( squid ~~ ~~ else : \n 
#try: \n 
~~~ return self . AdHocQuery ( request , parms ) \n 
~~ ~~ def AdHocQuery ( self , request , parms ) : \n 
bbox = parms . cleaned_data [ ] \n 
sort_by = parms . cleaned_data [ ] \n 
count = parms . cleaned_data [ ] \n 
if not count : \n 
~~~ count = parms . cleaned_data [ ] \n 
~~ start_index = parms . cleaned_data [ ] \n 
geometry_field = self . geometries [ type_names [ 0 ] ] \n 
query_set = model . objects . all ( ) \n 
if bbox : \n 
~~~ mnx , mny , mxx , mxy = bbox \n 
query_set . filter ( ** { geometry_field . name + "__bboverlaps" : \n 
mny = mny , \n 
mxx = mxx , \n 
mxy = mxy ) \n 
~~ if flt : \n 
~~~ flt = json . loads ( flt ) \n 
query_set = query_set . filter ( ** flt ) \n 
~~ if sort_by and in sort_by : \n 
~~~ sort_by = sort_by . split ( ) \n 
query_set = query_set . order_by ( * sort_by ) \n 
~~ elif sort_by : \n 
~~~ query_set = query_set . order_by ( sort_by ) \n 
~~ if start_index and count : \n 
~~~ query_set = query_set [ start_index : start_index + count ] \n 
~~ elif start_index : \n 
~~~ query_set = query_set [ start_index : ] \n 
~~ elif count : \n 
~~~ query_set = query_set [ : count ] \n 
~~ if srs_name : \n 
~~~ if ( not srs_format or srs_format == ) and srs_name != geometry_field . srid : \n 
~~~ if srs_name . lower ( ) . startswith ( ) : \n 
~~~ srs_name = srs_name [ 5 : ] \n 
~~ query_set . transform ( int ( srs_name ) ) \n 
~~ ~~ return query_set \n 
~~ def SQ_GetFeatureById ( self , request , parms ) : \n 
~~~ my_parms = GetFeatureByIdParameters . create ( request . REQUEST ) \n 
typename , pk = my_parms . cleaned_data [ ] . split ( ) \n 
return self . models [ typename ] . objects . filter ( pk = int ( pk ) ) \n 
~~ ~~ class WFSBase ( object ) : \n 
adapter = None \n 
~~ class DescribeFeatureTypeMixin ( WFSBase ) : \n 
class Parameters ( \n 
CommonParameters \n 
) : \n 
~~~ request [ ] = request . getlist ( ) + request . getlist ( ) \n 
~~ ~~ def _parse_xml_DescribeFeatureType ( self , request ) : \n 
def add_ns ( it , ns ) : \n 
~~~ x = it . split ( ) \n 
if len ( x ) > 1 : \n 
~~~ return ns [ x [ 0 ] ] , x [ 1 ] \n 
~~~ return , x \n 
~~ ~~ root = etree . fromstring ( request ) \n 
xmlns = root . get ( ) \n 
output_format = root . get ( , ) \n 
if xmlns is not None : \n 
~~~ xmlns = "{" + xmlns + "}" \n 
~~~ xmlns = "" \n 
~~ namespaces = { } \n 
for name , value in root . attrib . items ( ) : \n 
~~~ if name . startswith ( xmlns ) : \n 
~~~ namespaces [ value ] = name [ len ( xmlns ) : ] \n 
~~ ~~ type_names = root . get ( ) \n 
if type_names is not None : \n 
~~~ type_names = [ add_ns ( n , namespaces ) for n in type_names . split ( ) ] \n 
~~~ type_names = [ ] \n 
for elt in root : \n 
~~~ if elt . tag . endswith ( "TypeName" ) : \n 
~~~ namespace , name = elt . text . split ( ":" ) \n 
namespace = namespaces [ namespace ] \n 
type_names . append ( ( namespace , name ) ) \n 
~~ ~~ ~~ if not len ( type_names ) : \n 
~~~ type_names = \n 
~~ return DescribeFeatureTypeMixin . Parameters . create ( CaseInsensitiveDict ( { "typenames" : type_names \n 
~~ def _response_xml_DescribeFeatureType ( self , response ) : \n 
~~~ return render_to_response ( "ga_ows/WFS_DescribeFeature.template.xml" , { "feature_types" : list \n 
~~ def _response_json_DescribeFeatureType ( self , response , callback = None ) : \n 
~~~ rsp = [ ] \n 
for feature_type in response : \n 
~~~ rsp . append ( { \n 
"schema" : feature_type . schema , \n 
"name" : feature_type . name , \n 
"abstract" : feature_type . abstract , \n 
"title" : feature_type . title , \n 
"ns_name" : feature_type . ns_name \n 
~~ if callback is not None : \n 
~~~ return HttpResponse ( callback + "(" + json . dumps ( rsp ) + ")" , mimetype = ) ~~ else : \n 
~~~ return HttpResponse ( json . dumps ( rsp ) , mimetype = ) \n 
~~ ~~ def DescribeFeatureType ( self , request , kwargs ) : \n 
if in kwargs : \n 
~~~ parms = self . _parse_xml_DescribeFeatureType ( kwargs [ ] ) \n 
~~~ parms = DescribeFeatureTypeMixin . Parameters . create ( kwargs ) \n 
~~ response = self . adapter . get_feature_descriptions ( request , * parms . cleaned_data [ ] ) \n 
if parms . cleaned_data [ ] . endswith ( ) : \n 
~~~ if in kwargs : \n 
~~~ return self . _response_json_DescribeFeatureType ( response , callback = kwargs [ ] ~~ elif in kwargs : \n 
~~~ return self . _response_json_DescribeFeatureType ( response , callback = kwargs [ ] ) \n 
~~~ return self . _response_json_DescribeFeatureType ( response ) \n 
~~~ return self . _response_xml_DescribeFeatureType ( response ) \n 
~~ ~~ ~~ class GetFeatureMixin ( WFSBase ) : \n 
CommonParameters , \n 
InputParameters , \n 
PresentationParameters , \n 
AdHocQueryParameters , \n 
StoredQueryParameters \n 
~~ def _parse_xml_GetFeature ( self , request ) : \n 
~~ def GetFeature ( self , request , kwargs ) : \n 
mimetypes = { \n 
: \n 
~~~ parms = self . _parse_xml_GetFeature ( kwargs [ ] ) \n 
~~~ parms = GetFeatureMixin . Parameters . create ( kwargs ) \n 
~~ response = self . adapter . get_features ( request , parms ) \n 
if isinstance ( response , GeoQuerySet ) : \n 
~~~ layer = None \n 
db_params = settings . DATABASES [ response . db ] \n 
if db_params [ ] . endswith ( ) : \n 
~~~ from psycopg2 . extensions import adapt \n 
query , parameters = response . query . get_compiler ( response . db ) . as_sql ( ) \n 
parameters = tuple ( [ adapt ( p ) for p in parameters ] ) \n 
query = query % parameters \n 
drv = ogr . GetDriverByName ( "PostgreSQL" ) \n 
connection_string = "PG:dbname=\'{db}\'" . format ( db = db_params [ ] ) \n 
if in db_params and db_params [ ] : \n 
~~ if in db_params and db_params [ ] : \n 
layer = conn . ExecuteSQL ( query . encode ( ) ) \n 
~~ elif db_params [ ] . endswith ( ) : \n 
drv = ogr . GetDriverByName ( "Spatialite" ) \n 
conn = drv . Open ( db_params [ ] ) \n 
layer = conn . ExecuteSQL ( query ) \n 
~~~ layer = response . GetLayerByIndex ( 0 ) \n 
~~ drivers = dict ( [ ( ogr . GetDriver ( drv ) . GetName ( ) , ogr . GetDriver ( drv ) ) for drv in range ( ogr . GetDriverCount output_format = parms . cleaned_data [ ] . decode ( ) \n 
if in output_format or in output_format : \n 
~~~ tmpname = "{tmpdir}{sep}{uuid}.{output_format}" . format ( tmpdir = gettempdir ( ) , uuid = uuid4 ( ) drv = ogr . GetDriverByName ( "GML" ) \n 
ds = drv . CreateDataSource ( tmpname ) \n 
l2 = ds . CopyLayer ( layer , ) \n 
l2 . SyncToDisk ( ) \n 
del ds \n 
responsef = open ( tmpname ) \n 
rdata = responsef . read ( ) \n 
responsef . close ( ) \n 
os . unlink ( tmpname ) \n 
return HttpResponse ( rdata , mimetype = output_format ) \n 
~~ elif output_format in drivers : \n 
~~~ tmpname = "{tmpdir}{sep}{uuid}.{output_format}" . format ( tmpdir = gettempdir ( ) , uuid = uuid4 ( ) drv = drivers [ output_format ] \n 
return HttpResponse ( rdata , mimetype = mimetypes . get ( output_format , ) ) \n 
~~~ raise OperationProcessingFailed . at ( , \n 
~~ ~~ ~~ class ListStoredQueriesMixin ( WFSBase ) : \n 
def ListStoredQueries ( self , request , kwargs ) : \n 
queries = self . adapter . list_stored_queries ( request ) \n 
response = etree . Element ( "ListStoredQueriesResponse" ) \n 
for query , description in queries . items ( ) : \n 
~~~ sub = etree . SubElement ( response , "StoredQuery" ) \n 
etree . SubElement ( sub , "Title" ) . text = query \n 
for feature_type in description . feature_types : \n 
~~~ etree . SubElement ( sub , ) . text = feature_type \n 
~~ ~~ return HttpResponse ( etree . tostring ( response , pretty_print = True ) , mimetype = ) \n 
~~ ~~ class DescribeStoredQueriesMixin ( WFSBase ) : \n 
~~~ class Parameters ( CommonParameters ) : \n 
~~~ stored_query_id = MultipleValueField ( ) \n 
~~ ~~ def DescribeStoredQueries ( self , request , kwargs ) : \n 
~~~ parms = DescribeStoredQueriesMixin . Parameters . create ( kwargs ) \n 
inspected_queries = parms . cleaned_data [ ] \n 
response = etree . Element ( ) \n 
for query , description in filter ( lambda ( x , y ) : x in inspected_queries , self . adapter . list_stored_queries ~~~ desc = etree . SubElement ( response , "StoredQueryDescription" ) \n 
etree . SubElement ( desc , ) . text = query \n 
for parameter in description . parameters : \n 
~~~ p = etree . SubElement ( desc , "Parameter" , attrib = { "name" : parameter . name , "type" : parameter etree . SubElement ( p , ) . text = parameter . title \n 
etree . SubElement ( p , ) . text = parameter . abstractS \n 
if parameter . query_expression : \n 
~~~ etree . SubElement ( p , "QueryExpressionText" , attrib = { \n 
"isPrivate" : parameter . query_expression . private == True , \n 
"language" : parameter . query_expression . language , \n 
"returnFeatureTypes" : . join ( parameter . query_expression . return_feature_types } ) . text = parameter . query_expression . text \n 
~~ ~~ ~~ return HttpResponse ( etree . tostring ( response , pretty_print = True ) , mimetype = ) \n 
~~ ~~ class CreateStoredQuery ( WFSBase ) : \n 
~~~ def CreateStoredQuery ( self , request , kwargs ) : \n 
~~~ raise OperationNotSupported . at ( "CreateStoredQuery" ) \n 
~~ ~~ class DropStoredQuery ( WFSBase ) : \n 
~~~ def DropStoredQuery ( self , request , kwargs ) : \n 
~~~ raise OperationNotSupported . at ( "DropStoredQuery" ) \n 
~~ ~~ class TransactionMixin ( WFSBase ) : \n 
~~~ def Transaction ( self , request , kwargs ) : \n 
raise OperationNotSupported . at ( ) \n 
~~ ~~ class GetFeatureWithLockMixin ( WFSBase ) : \n 
~~~ def GetFeatureWithLock ( self , request , kwargs ) : \n 
~~~ raise OperationNotSupported . at ( "GetFeatureWithLock" ) \n 
~~ ~~ class LockFeatureMixin ( WFSBase ) : \n 
~~~ def LockFeature ( self , request , kwargs ) : \n 
~~~ raise OperationNotSupported . at ( ) \n 
~~ ~~ class GetPropertyValueMixin ( WFSBase ) : \n 
~~~ class Parameters ( StoredQueryParameters , AdHocQueryParameters ) : \n 
~~~ value_reference = f . CharField ( ) \n 
resolve_path = f . CharField ( required = False ) \n 
~~~ request [ ] = request [ ] \n 
request [ ] = request [ ] \n 
~~ ~~ def GetPropertyValue ( self , request , kwargs ) : \n 
~~ ~~ class WFS ( \n 
common . OWSView , \n 
GetCapabilitiesMixin , \n 
DescribeFeatureTypeMixin , \n 
DescribeStoredQueriesMixin , \n 
GetFeatureMixin , \n 
ListStoredQueriesMixin , \n 
GetPropertyValueMixin \n 
models = None \n 
title = None \n 
keywords = [ ] \n 
fees = None \n 
access_constraints = None \n 
provider_name = None \n 
addr_street = None \n 
addr_city = None \n 
addr_admin_area = None \n 
addr_postcode = None \n 
addr_country = None \n 
addr_email = None \n 
~~~ common . OWSView . __init__ ( self , ** kwargs ) \n 
if self . models : \n 
~~~ self . adapter = GeoDjangoWFSAdapter ( self . models ) \n 
~~ ~~ def get_capabilities_response ( self , request , params ) : \n 
~~~ return render_to_response ( , { \n 
"title" : self . title , \n 
"keywords" : self . keywords , \n 
"fees" : self . fees , \n 
"access_constraints" : self . access_constraints , \n 
"endpoint" : request . build_absolute_uri ( ) . split ( ) [ 0 ] , \n 
"output_formats" : [ ogr . GetDriver ( drv ) . GetName ( ) for drv in range ( ogr . GetDriverCount ( ) ) "addr_street" : self . addr_street , \n 
"addr_city" : self . addr_city , \n 
"addr_admin_area" : self . addr_admin_area , \n 
"addr_postcode" : self . addr_postcode , \n 
"addr_country" : self . addr_country , \n 
"feature_versioning" : False , \n 
"transactional" : False , \n 
: self . adapter . get_feature_descriptions ( request ) \n 
~~ ~~ class WFST ( WFS , TransactionMixin , GetFeatureWithLockMixin , LockFeatureMixin ) : \n 
def get_capabilities_response ( self , request , params ) : \n 
"feature_versioning" : self . adapter . supports_feature_versioning ( ) , \n 
"transactional" : True , \n 
~~ ~~ from sondra . document . valuehandlers import DateTime , Geometry , Now \n 
from shapely . geometry import Point \n 
import rethinkdb as r \n 
from sondra . tests . api import * \n 
from sondra . auth import Auth \n 
s = ConcreteSuite ( ) \n 
api = SimpleApp ( s ) \n 
auth = Auth ( s ) \n 
AuthenticatedApp ( s ) \n 
AuthorizedApp ( s ) \n 
s . ensure_database_objects ( ) \n 
@ pytest . fixture ( scope = ) \n 
def simple_doc ( request ) : \n 
~~~ simple_doc = s [ ] [ ] . create ( { \n 
"date" : datetime . now ( ) , \n 
"value" : 0 \n 
def teardown ( ) : \n 
~~~ simple_doc . delete ( ) \n 
~~ request . addfinalizer ( teardown ) \n 
return simple_doc \n 
~~ @ pytest . fixture ( scope = ) \n 
def fk_doc ( request , simple_doc ) : \n 
~~~ fk_doc = s [ ] [ ] . create ( { \n 
: simple_doc , \n 
: [ simple_doc ] \n 
~~~ fk_doc . delete ( ) \n 
return fk_doc \n 
~~ def test_foreignkey ( fk_doc , simple_doc ) : \n 
~~~ retr_doc = s [ ] [ ] [ ] \n 
assert isinstance ( fk_doc . obj [ ] , str ) \n 
assert fk_doc . obj [ ] == simple_doc . url \n 
assert isinstance ( retr_doc . obj [ ] , str ) \n 
assert retr_doc . obj [ ] == simple_doc . url \n 
storage_repr = fk_doc . rql_repr ( ) \n 
assert storage_repr [ ] == simple_doc . id \n 
assert isinstance ( fk_doc [ ] , SimpleDocument ) import os \n 
~~ from PySide . QtGui import * \n 
from PySide . QtCore import * \n 
from ui_Event import Ui_Event \n 
class EventWindow ( QDialog , Ui_Event ) : \n 
~~~ def __init__ ( self , parent , eventId ) : \n 
~~~ super ( EventWindow , self ) . __init__ ( parent ) \n 
self . rent = parent \n 
self . data = parent . eventData [ eventId ] \n 
self . deckAssignment = [ ] \n 
self . setupUi ( self ) \n 
self . assignWidgets ( ) \n 
~~ def savePressed ( self ) : \n 
~~~ self . data [ "Notes" ] = self . notesText . toPlainText ( ) \n 
self . data [ "Deck" ] = self . deckText . text ( ) \n 
self . data [ "Place" ] = self . placeText . text ( ) \n 
self . data [ "Type" ] = self . eventTypeText . text ( ) \n 
self . data [ "Players" ] = self . playersText . text ( ) \n 
self . data [ "Format" ] = self . formatText . text ( ) \n 
self . data [ "Location" ] = self . locationText . text ( ) \n 
self . data [ "Date" ] = self . dateText . text ( ) \n 
ourCounter = 0 \n 
for ourRound in self . deckAssignment : \n 
~~~ self . data [ "Opponents" ] [ self . deckAssignment [ ourCounter ] [ 0 ] ] [ 2 ] = self . deckAssignment [ ourCounter ourCounter += 1 \n 
~~ self . rent . updateGUI ( ) \n 
~~ def closePressed ( self ) : \n 
~~~ self . hide ( ) \n 
~~ def roundSelected ( self , ourRound , ourColumn ) : \n 
~~~ ourIndex = int ( ourRound . text ( 0 ) ) - 1 \n 
if ok and deckName : \n 
~~~ self . data [ "Opponents" ] [ ourIndex ] [ 3 ] . setData ( 3 , 0 , deckName ) \n 
self . deckAssignment . append ( [ ourIndex , deckName ] ) \n 
~~ ~~ def assignWidgets ( self ) : \n 
~~~ self . saveChangesButton . clicked . connect ( self . savePressed ) \n 
self . closeButton . clicked . connect ( self . closePressed ) \n 
self . roundTree . itemDoubleClicked . connect ( self . roundSelected ) \n 
self . notesText . setPlainText ( self . data [ "Notes" ] ) \n 
self . deckText . setText ( self . data [ "Deck" ] ) \n 
self . placeText . setText ( self . data [ "Place" ] ) \n 
self . eventTypeText . setText ( self . data [ "Type" ] ) \n 
self . playersText . setText ( self . data [ "Players" ] ) \n 
self . formatText . setText ( self . data [ "Format" ] ) \n 
self . locationText . setText ( self . data [ "Location" ] ) \n 
self . dateText . setText ( self . data [ "Date" ] ) \n 
matchItem = TreeWidgetItem ( self . resultsTree ) \n 
matchItem . setText ( 0 , unicode ( self . data [ "Wins" ] ) ) \n 
matchItem . setText ( 1 , unicode ( self . data [ "Losses" ] ) ) \n 
matchItem . setText ( 2 , unicode ( self . data [ "Draws" ] ) ) \n 
matchItem . setText ( 3 , unicode ( self . data [ "Matches" ] ) ) \n 
self . resultsTree . addTopLevelItem ( matchItem ) \n 
for i in range ( 4 ) : \n 
~~~ self . resultsTree . resizeColumnToContents ( i ) \n 
~~ roundCounter = 1 \n 
for opponent in self . data [ "Opponents" ] : \n 
~~~ roundItem = TreeWidgetItem ( self . roundTree ) \n 
roundItem . setText ( 0 , unicode ( roundCounter ) ) \n 
roundItem . setText ( 1 , unicode ( opponent [ 0 ] ) ) \n 
roundItem . setText ( 2 , unicode ( opponent [ 1 ] ) ) \n 
roundItem . setText ( 3 , unicode ( opponent [ 2 ] ) ) \n 
opponent [ 3 ] = roundItem \n 
self . roundTree . addTopLevelItem ( roundItem ) \n 
roundCounter += 1 \n 
~~ for i in range ( 4 ) : \n 
~~~ self . roundTree . resizeColumnToContents ( i ) \n 
~~ ~~ ~~ class TreeWidgetItem ( QTreeWidgetItem ) : \n 
~~~ def __init__ ( self , parent = None ) : \n 
~~~ QTreeWidgetItem . __init__ ( self , parent ) \n 
~~ def __lt__ ( self , otherItem ) : \n 
~~~ column = self . treeWidget ( ) . sortColumn ( ) \n 
~~~ return float ( self . text ( column ) ) > float ( otherItem . text ( column ) ) \n 
~~~ return self . text ( column ) > otherItem . text ( column ) \n 
from pylatex import Document , Section , Subsection , Math , Matrix , VectorName \n 
if __name__ == : \n 
~~~ a = np . array ( [ [ 100 , 10 , 20 ] ] ) . T \n 
doc = Document ( ) \n 
section = Section ( ) \n 
subsection = Subsection ( ) \n 
vec = Matrix ( a ) \n 
vec_name = VectorName ( ) \n 
math = Math ( data = [ vec_name , , vec ] ) \n 
subsection . append ( math ) \n 
section . append ( subsection ) \n 
M = np . matrix ( [ [ 2 , 3 , 4 ] , \n 
[ 0 , 0 , 1 ] , \n 
[ 0 , 0 , 2 ] ] ) \n 
matrix = Matrix ( M , mtype = ) \n 
math = Math ( data = [ , matrix ] ) \n 
math = Math ( data = [ , vec_name , , Matrix ( M * a ) ] ) \n 
doc . append ( section ) \n 
doc . generate_pdf ( ) \n 
~~ import quantities as pq \n 
from pylatex . quantities import _dimensionality_to_siunitx , Quantity \n 
def test_quantity ( ) : \n 
~~~ v = 1 * pq . m / pq . s \n 
q1 = Quantity ( v ) \n 
assert q1 . dumps ( ) == \n 
q2 = Quantity ( v , format_cb = lambda x : str ( int ( x ) ) ) \n 
assert q2 . dumps ( ) == \n 
q3 = Quantity ( v , options = { : } ) \n 
ref = \n 
assert q3 . dumps ( ) == ref \n 
~~ def test_quantity_float ( ) : \n 
~~~ q1 = Quantity ( 42.0 ) \n 
~~ def test_quantity_uncertain ( ) : \n 
~~~ t = pq . UncertainQuantity ( 7. , pq . second , 1. ) \n 
q1 = Quantity ( t ) \n 
~~ def test_dimensionality_to_siunitx ( ) : \n 
~~~ assert _dimensionality_to_siunitx ( ( pq . volt / pq . kelvin ) . dimensionality ) == \n 
~~~ test_quantity ( ) \n 
test_dimensionality_to_siunitx ( ) \n 
~~ from supervisor . medusa import asyncore_25 as asyncore \n 
from supervisor . medusa import default_handler \n 
from supervisor . medusa import http_server \n 
from supervisor . medusa import put_handler \n 
from supervisor . medusa import auth_handler \n 
from supervisor . medusa import filesys \n 
users = { : , : } \n 
fs = filesys . os_filesystem ( ) \n 
dh = default_handler . default_handler ( fs ) \n 
ph = put_handler . put_handler ( fs , ) \n 
ah = auth_handler . auth_handler ( users , ph ) \n 
hs = http_server . http_server ( ip = , port = 8080 ) \n 
asyncore . loop ( ) \n 
import socket \n 
import string \n 
from supervisor . medusa import asyncore_25 as asyncore \n 
from supervisor . medusa import asynchat_25 as asynchat \n 
class test_client ( asynchat . async_chat ) : \n 
~~~ ac_in_buffer_size = 16384 \n 
ac_out_buffer_size = 16384 \n 
total_in = 0 \n 
concurrent = 0 \n 
max_concurrent = 0 \n 
def __init__ ( self , addr , chain ) : \n 
~~~ asynchat . async_chat . __init__ ( self ) \n 
self . create_socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
self . set_terminator ( ) \n 
self . connect ( addr ) \n 
self . push ( chain ) \n 
~~ def handle_connect ( self ) : \n 
~~~ test_client . concurrent = test_client . concurrent + 1 \n 
if ( test_client . concurrent > test_client . max_concurrent ) : \n 
~~~ test_client . max_concurrent = test_client . concurrent \n 
~~ ~~ def handle_expt ( self ) : \n 
~~~ print \n 
self . close ( ) \n 
~~~ test_client . concurrent = test_client . concurrent - 1 \n 
asynchat . async_chat . close ( self ) \n 
~~ def collect_incoming_data ( self , data ) : \n 
~~~ test_client . total_in = test_client . total_in + len ( data ) \n 
~~ def found_terminator ( self ) : \n 
~~ def log ( self , * args ) : \n 
class timer : \n 
~~~ self . start = time . time ( ) \n 
~~ def end ( self ) : \n 
~~~ return time . time ( ) - self . start \n 
~~ ~~ def build_request_chain ( num , host , request_size ) : \n 
~~~ s = % ( request_size , host ) \n 
sl = [ s ] * ( num - 1 ) \n 
sl . append ( \n 
% ( \n 
request_size , host \n 
return string . join ( sl , ) \n 
~~~ import string \n 
if len ( sys . argv ) != 6 : \n 
~~~ print % sys . argv ~~ else : \n 
~~~ host = sys . argv [ 1 ] \n 
ip = socket . gethostbyname ( host ) \n 
[ port , request_size , num_requests , num_conns ] = map ( \n 
string . atoi , sys . argv [ 2 : ] \n 
chain = build_request_chain ( num_requests , host , request_size ) \n 
t = timer ( ) \n 
for i in range ( num_conns ) : \n 
~~~ test_client ( ( host , port ) , chain ) \n 
~~ asyncore . loop ( ) \n 
total_time = t . end ( ) \n 
total_bytes = test_client . total_in \n 
num_trans = num_requests * num_conns \n 
throughput = float ( total_bytes ) / total_time \n 
trans_per_sec = num_trans / total_time \n 
sys . stderr . write ( % total_time ) \n 
sys . stderr . write ( % num_trans ) \n 
sys . stderr . write ( % total_bytes ) \n 
sys . stderr . write ( % throughput ) \n 
sys . stderr . write ( % trans_per_sec ) \n 
sys . stderr . write ( % test_client . max_concurrent ) \n 
sys . stdout . write ( \n 
string . join ( \n 
map ( str , ( num_conns , num_requests , request_size , throughput , trans_per_sec ) \n 
) + \n 
~~ ~~ from os import * \n 
from os import _exit \n 
class FakeOS : \n 
~~~ self . orig_uid = os . getuid ( ) \n 
self . orig_gid = os . getgid ( ) \n 
~~ def setgroups ( * args ) : \n 
~~ def getuid ( ) : \n 
~~ def setuid ( arg ) : \n 
~~~ self . uid = arg \n 
self . setuid_called = 1 \n 
~~ def setgid ( arg ) : \n 
~~~ self . gid = arg \n 
self . setgid_called = 1 \n 
~~ def clear ( ) : \n 
~~~ self . uid = orig_uid \n 
self . gid = orig_gid \n 
self . setuid_called = 0 \n 
self . setgid_called = 0 \n 
~~ ~~ fake = FakeOS ( ) \n 
setgroups = fake . setgroups \n 
getuid = fake . getuid \n 
setuid = fake . setuid \n 
setgid = fake . setgid \n 
clear = fake . clear \n 
import toto \n 
from toto . invocation import * \n 
from tornado . ioloop import IOLoop \n 
@ asynchronous \n 
def invoke ( handler , params ) : \n 
~~~ def receive_message ( message ) : \n 
~~~ handler . respond ( result = { : message } ) \n 
~~ handler . register_event_handler ( , receive_message , deregister_on_finish = True ) \n 
from time import time , sleep \n 
from toto . tasks import TaskQueue , AwaitableInstance , InstancePool \n 
from tornado . gen import coroutine \n 
class _Instance ( object ) : \n 
~~~ self . counter = 0 \n 
~~ def increment ( self ) : \n 
~~~ self . counter += 1 \n 
return self . counter \n 
~~ def value ( self ) : \n 
~~~ return self . counter \n 
~~ ~~ class TestTasks ( unittest . TestCase ) : \n 
~~~ def test_add_task ( self ) : \n 
~~~ queue = TaskQueue ( ) \n 
self . assertEquals ( len ( queue ) , 0 ) \n 
task_results = [ ] \n 
task = lambda x : task_results . append ( x ) \n 
queue . add_task ( task , 1 ) \n 
queue . add_task ( task , 2 ) \n 
queue . add_task ( task , 3 ) \n 
start = time ( ) \n 
~~~ if len ( task_results ) == 3 : \n 
~~ if time ( ) - start > 5 : \n 
~~ sleep ( 0.01 ) \n 
~~ self . assertEquals ( len ( task_results ) , 3 ) \n 
self . assertEquals ( task_results , [ 1 , 2 , 3 ] ) \n 
~~ def test_yield_task ( self ) : \n 
@ coroutine \n 
def yield_tasks ( ) : \n 
~~~ task = lambda x : x \n 
futures = [ ] \n 
futures . append ( queue . yield_task ( task , 1 ) ) \n 
futures . append ( queue . yield_task ( task , 2 ) ) \n 
futures . append ( queue . yield_task ( task , 3 ) ) \n 
res = yield futures \n 
task_results [ : ] = res \n 
~~ loop = IOLoop ( ) \n 
loop . make_current ( ) \n 
loop . run_sync ( yield_tasks ) \n 
self . assertEquals ( len ( task_results ) , 3 ) \n 
~~ def test_add_task_exception ( self ) : \n 
def task ( x ) : \n 
~~~ task_results . append ( x ) \n 
raise Exception ( ) \n 
~~ queue . add_task ( task , 1 ) \n 
~~ def test_yield_task_exception ( self ) : \n 
~~~ def task ( x ) : \n 
~~ futures = [ ] \n 
for f in futures : \n 
~~~ yield f \n 
~~~ task_results . append ( e ) \n 
~~ ~~ ~~ loop = IOLoop ( ) \n 
for e in task_results : \n 
~~~ self . assertEquals ( e . message , ) \n 
~~ ~~ def test_awaitable ( self ) : \n 
~~~ instance = _Instance ( ) \n 
instance . increment ( ) \n 
self . assertEquals ( instance . value ( ) , 1 ) \n 
awaitable = AwaitableInstance ( instance ) \n 
~~~ self . assertEquals ( ( yield awaitable . increment ( ) ) , 2 ) \n 
self . assertEquals ( ( yield awaitable . increment ( ) ) , 3 ) \n 
self . assertEquals ( ( yield awaitable . increment ( ) ) , 4 ) \n 
self . assertEquals ( ( yield awaitable . value ( ) ) , 4 ) \n 
self . assertEquals ( instance . value ( ) , 4 ) \n 
~~ def test_instance_pool ( self ) : \n 
~~~ instance1 = _Instance ( ) \n 
instance2 = _Instance ( ) \n 
pool = InstancePool ( [ instance1 , instance2 ] ) \n 
pool . increment ( ) \n 
self . assertEquals ( instance1 . value ( ) , 1 ) \n 
self . assertEquals ( instance2 . value ( ) , 1 ) \n 
pool . transaction ( lambda i : i . increment ( ) ) \n 
self . assertEquals ( instance1 . value ( ) , 2 ) \n 
self . assertEquals ( instance2 . value ( ) , 2 ) \n 
~~~ self . assertEquals ( ( yield pool . await ( ) . increment ( ) ) , 3 ) \n 
self . assertEquals ( ( yield pool . await ( ) . increment ( ) ) , 3 ) \n 
self . assertEquals ( instance1 . value ( ) , 3 ) \n 
self . assertEquals ( instance2 . value ( ) , 3 ) \n 
self . assertEquals ( ( yield pool . await_transaction ( lambda i : i . increment ( ) ) ) , 4 ) \n 
self . assertEquals ( instance1 . value ( ) , 4 ) \n 
self . assertEquals ( instance2 . value ( ) , 4 ) \n 
~~ ~~ \n 
import cPickle as pickle \n 
from threading import Thread \n 
from collections import deque \n 
from tornado . web import * \n 
from traceback import format_exc \n 
from tornado . options import options \n 
import zmq \n 
import zlib \n 
from random import choice , shuffle \n 
class EventManager ( ) : \n 
~~~ \n 
def __init__ ( self , address = None ) : \n 
~~~ self . __handlers = { } \n 
self . address = address \n 
self . __zmq_context = zmq . Context ( ) \n 
self . __remote_servers = { } \n 
self . __thread = None \n 
self . __queued_servers = deque ( ) \n 
~~ def register_server ( self , address ) : \n 
if address in self . __remote_servers : \n 
~~~ raise Exception ( , address ) \n 
~~ socket = self . __zmq_context . socket ( zmq . PUSH ) \n 
socket . connect ( address ) \n 
self . __remote_servers [ address ] = socket \n 
self . refresh_server_queue ( ) \n 
~~ def remove_server ( self , address ) : \n 
del self . __remote_servers [ address ] \n 
~~ def remove_all_servers ( self ) : \n 
self . __remote_servers . clear ( ) \n 
~~ def refresh_server_queue ( self ) : \n 
self . __queued_servers . clear ( ) \n 
self . __queued_servers . extend ( self . __remote_servers . itervalues ( ) ) \n 
shuffle ( self . __queued_servers ) \n 
~~ def register_handler ( self , event_name , event_handler , run_on_main_loop = False , request_handler = None ~~~ \n 
if not event_name in self . __handlers : \n 
~~~ self . __handlers [ event_name ] = set ( ) \n 
~~ handler_tuple = ( event_handler , run_on_main_loop , request_handler , persist ) \n 
self . __handlers [ event_name ] . add ( handler_tuple ) \n 
return ( event_name , handler_tuple ) \n 
~~ def remove_handler ( self , handler_sig ) : \n 
self . __handlers [ handler_sig [ 0 ] ] . discard ( handler_sig [ 1 ] ) \n 
~~ def start_listening ( self ) : \n 
if self . __thread : \n 
~~ def receive ( ) : \n 
~~~ context = zmq . Context ( ) \n 
socket = context . socket ( zmq . PULL ) \n 
socket . bind ( self . address ) \n 
while True : \n 
~~~ event = pickle . loads ( zlib . decompress ( socket . recv ( ) ) ) \n 
event_name = event [ ] \n 
event_args = event [ ] \n 
if event_name in self . __handlers : \n 
~~~ handlers = self . __handlers [ event_name ] \n 
for handler in list ( handlers ) : \n 
~~~ if not handler [ 3 ] : \n 
~~~ handlers . remove ( handler ) \n 
~~~ if handler [ 2 ] and handler [ 2 ] . _finished : \n 
~~ if handler [ 1 ] : \n 
~~~ ( lambda h : IOLoop . instance ( ) . add_callback ( lambda : h [ 0 ] ( event_args ) ) ) ( handler ) \n 
~~~ handler [ 0 ] ( event_args ) \n 
~~~ logging . error ( format_exc ( ) ) \n 
~~ ~~ ~~ ~~ ~~ self . __thread = Thread ( target = receive ) \n 
self . __thread . daemon = True \n 
self . __thread . start ( ) \n 
~~ def send_to_server ( self , address , event_name , event_args ) : \n 
event = { : event_name , : event_args } \n 
event_data = zlib . compress ( pickle . dumps ( event ) ) \n 
self . __remote_servers [ address ] . send ( event_data ) \n 
~~ def send ( self , event_name , event_args , broadcast = True ) : \n 
if not self . __remote_servers : \n 
~~ event = { : event_name , : event_args } \n 
if not broadcast : \n 
~~~ self . __queued_servers [ 0 ] . send ( event_data ) \n 
self . __queued_servers . rotate ( - 1 ) \n 
~~ for socket in self . __queued_servers : \n 
~~~ socket . send ( event_data ) \n 
~~ ~~ @ classmethod \n 
def instance ( cls ) : \n 
if not hasattr ( cls , ) : \n 
~~~ cls . _instance = cls ( ) \n 
~~ return cls . _instance \n 
~~ ~~ import toto \n 
from tornado . gen import Task \n 
from time import time \n 
from toto . options import safe_define \n 
WORKER_SOCKET_CONNECT = \n 
WORKER_SOCKET_DISCONNECT = \n 
class WorkerConnection ( object ) : \n 
def __getattr__ ( self , path ) : \n 
~~~ return WorkerInvocation ( path , self ) \n 
~~ def log_error ( self , error ) : \n 
~~~ logging . error ( repr ( error ) ) \n 
~~ def enable_traceback_logging ( self ) : \n 
~~~ from new import instancemethod \n 
def log_error ( self , e ) : \n 
~~ self . log_error = instancemethod ( log_error , self ) \n 
~~~ if options . worker_transport == : \n 
~~~ from toto . httpworkerconnection import HTTPWorkerConnection \n 
cls . _instance = HTTPWorkerConnection . instance ( ) \n 
~~~ from toto . zmqworkerconnection import ZMQWorkerConnection \n 
cls . _instance = ZMQWorkerConnection . instance ( ) \n 
~~ ~~ return cls . _instance \n 
~~ ~~ class WorkerInvocation ( object ) : \n 
~~~ def __init__ ( self , path , connection ) : \n 
~~~ self . _path = path \n 
self . _connection = connection \n 
~~ def __call__ ( self , * args , ** kwargs ) : \n 
~~~ return self . _connection . invoke ( self . _path , * args , ** kwargs ) \n 
~~ def __getattr__ ( self , path ) : \n 
~~~ return getattr ( self . _connection , self . _path + + path ) \n 
~~ ~~ from . import multiarray \n 
__all__ = [ ] #!/usr/bin/python2.4 \n 
__author__ = \n 
import urllib \n 
from pyactiveresource import connection \n 
from pyactiveresource import formats \n 
class Error ( Exception ) : \n 
~~ class FakeConnection ( object ) : \n 
def __init__ ( self , format = formats . XMLFormat ) : \n 
self . format = format \n 
self . _request_map = { } \n 
self . _debug_only = False \n 
~~ def _split_path ( self , path ) : \n 
path_only , query_string = urllib . splitquery ( path ) \n 
if query_string : \n 
~~~ query_dict = dict ( [ i . split ( ) for i in query_string . split ( ) ] ) \n 
~~~ query_dict = { } \n 
~~ return path_only , query_dict \n 
~~ def debug_only ( self , debug = True ) : \n 
~~~ self . _debug_only = debug \n 
~~ def respond_to ( self , method , path , headers , data , body , \n 
response_headers = None ) : \n 
path_only , query = self . _split_path ( path ) \n 
if response_headers is None : \n 
~~~ response_headers = { } \n 
~~ self . _request_map . setdefault ( method , [ ] ) . append ( \n 
( ( path_only , query , headers , data ) , ( body , response_headers ) ) ) \n 
~~ def _lookup_response ( self , method , path , headers , data ) : \n 
~~~ path_only , query = self . _split_path ( path ) \n 
for key , value in self . _request_map . get ( method , { } ) : \n 
~~~ if key == ( path_only , query , headers , data ) : \n 
~~~ response_body , response_headers = value \n 
return connection . Response ( 200 , response_body , response_headers ) \n 
~~ ~~ raise Error ( % \n 
( path , headers , data ) ) \n 
~~ def get ( self , path , headers = None ) : \n 
return self . format . decode ( \n 
self . _lookup_response ( , path , headers , None ) . body ) \n 
~~ def post ( self , path , headers = None , data = None ) : \n 
return self . _lookup_response ( , path , headers , data ) \n 
~~ def put ( self , path , headers = None , data = None ) : \n 
~~ def delete ( self , path , headers = None ) : \n 
return self . _lookup_response ( , path , headers , None ) \n 
~~ ~~ from trac . env import Environment \n 
from trac . attachment import Attachment \n 
from tracLib import * \n 
from ConfigParser import ConfigParser \n 
import tracLib \n 
import tracLib . timetracking \n 
class Client ( object ) : \n 
~~~ def __init__ ( self , env_path ) : \n 
~~~ self . env_path = env_path \n 
self . env = Environment ( env_path ) \n 
self . db_cnx = self . env . get_db_cnx ( ) \n 
self . _registered_users_logins = [ ] \n 
self . _timetracking_plugins = self . _get_timetracking_plugins ( ) \n 
~~ def _get_timetracking_plugins ( self ) : \n 
~~~ plugins = { } \n 
if tracLib . SUPPORT_TIME_TRACKING == : \n 
~~~ for plugin in tracLib . timetracking . plugins : \n 
~~~ plugin_name = plugin . get_name ( ) \n 
for com_name , com_enabled in self . env . _component_rules . items ( ) : \n 
~~~ if com_name . startswith ( plugin_name ) and com_enabled and plugin_name not in plugins ~~~ plugins [ plugin_name ] = plugin ( self . env ) \n 
if plugin_name == tracLib . SUPPORT_TIME_TRACKING : \n 
~~~ plugins [ plugin_name ] = plugin ( self . env ) \n 
break ; \n 
~~ ~~ ~~ for plugin_name in plugins . keys ( ) : \n 
~~ return plugins . values ( ) \n 
~~ def get_project_description ( self ) : \n 
~~~ return self . env . project_description \n 
~~ def get_users ( self ) : \n 
~~~ result = self . env . get_known_users ( ) \n 
trac_users = list ( [ ] ) \n 
for user in result : \n 
~~~ user_login = user [ 0 ] . lower ( ) \n 
if user_login in self . _registered_users_logins : \n 
~~ u = TracUser ( user_login ) \n 
u . email = user [ 2 ] \n 
trac_users . append ( u ) \n 
self . _registered_users_logins . append ( user_login ) \n 
~~ if not tracLib . ACCEPT_NON_AUTHORISED_USERS : \n 
~~~ return trac_users \n 
~~ user_fields = [ ( "owner" , "component" ) , ( "reporter" , "ticket" ) , ( "owner" , "ticket" ) , ( "author" first = True \n 
request = "" \n 
for column_name , table_name in user_fields : \n 
~~~ if first : \n 
~~~ first = False \n 
~~ cursor = self . db_cnx . cursor ( ) \n 
cursor . execute ( request ) \n 
for row in cursor : \n 
~~~ if row [ 0 ] not in self . _registered_users_logins : \n 
~~~ trac_user = self . _get_non_authorised_user ( row [ 0 ] ) \n 
if trac_user is not None : \n 
~~~ trac_users . append ( trac_user ) \n 
self . _registered_users_logins . append ( trac_user . name ) \n 
~~ ~~ ~~ return trac_users \n 
~~ def _get_non_authorised_user ( self , user_name ) : \n 
~~~ if user_name is None : \n 
~~ start = user_name . find ( "<" ) \n 
end = user_name . rfind ( ">" ) \n 
if ( start > - 1 ) and ( end > start + 1 ) : \n 
~~~ if user_name . find ( "@" , start , end ) > 0 : \n 
return user \n 
~~ def _get_user_login ( self , user_name ) : \n 
~~ if user_name in self . _registered_users_logins : \n 
~~~ return user_name \n 
~~ user = self . _get_non_authorised_user ( user_name ) \n 
if ( user is None ) or ( user . name not in self . _registered_users_logins ) : \n 
~~ return user . name \n 
~~ def get_severities ( self ) : \n 
~~~ return self . _get_data_from_enum ( "severity" ) \n 
~~ def get_issue_types ( self ) : \n 
~~~ return self . _get_data_from_enum ( "ticket_type" ) \n 
~~ def get_issue_priorities ( self ) : \n 
~~~ return self . _get_data_from_enum ( "priority" ) \n 
~~ def get_issue_resolutions ( self ) : \n 
~~~ return [ TracResolution ( name ) for name in self . _get_data_from_enum ( "resolution" ) ] \n 
~~ def get_components ( self ) : \n 
~~~ cursor = self . db_cnx . cursor ( ) \n 
trac_components = list ( [ ] ) \n 
~~~ component = TracComponent ( row [ 0 ] ) \n 
component . owner = self . _get_user_login ( component . owner ) \n 
if row [ 2 ] is not None : \n 
~~~ component . description = row [ 2 ] \n 
~~ trac_components . append ( component ) \n 
~~ return trac_components \n 
~~ def get_versions ( self ) : \n 
trac_versions = list ( [ ] ) \n 
~~~ version = TracVersion ( row [ 0 ] ) \n 
if row [ 1 ] : \n 
~~~ version . time = to_unix_time ( row [ 1 ] ) \n 
~~ if row [ 2 ] is not None : \n 
~~~ version . description = row [ 2 ] \n 
~~ trac_versions . append ( version ) \n 
~~ return trac_versions \n 
~~ def get_issues ( self ) : \n 
~~~ issue = TracIssue ( row [ 0 ] ) \n 
issue . time = to_unix_time ( row [ 2 ] ) \n 
issue . changetime = to_unix_time ( row [ 3 ] ) \n 
issue . reporter = self . _get_user_login ( row [ 8 ] ) \n 
if row [ 9 ] is not None : \n 
~~~ cc = row [ 9 ] . split ( "," ) \n 
for c in cc : \n 
~~~ if len ( c ) > 0 : \n 
~~~ cc_name = self . _get_user_login ( c . strip ( ) ) \n 
if cc_name is not None : \n 
~~~ issue . cc . add ( cc_name ) \n 
~~ ~~ ~~ ~~ issue . summary = row [ 13 ] \n 
issue . description = row [ 14 ] \n 
issue . custom_fields [ "Type" ] = row [ 1 ] \n 
issue . custom_fields [ "Component" ] = row [ 4 ] \n 
issue . custom_fields [ "Severity" ] = row [ 5 ] \n 
issue . custom_fields [ "Priority" ] = row [ 6 ] \n 
issue . custom_fields [ "Owner" ] = self . _get_user_login ( row [ 7 ] ) \n 
issue . custom_fields [ "Version" ] = row [ 10 ] \n 
issue . custom_fields [ "Status" ] = row [ 11 ] \n 
issue . custom_fields [ "Resolution" ] = row [ 12 ] \n 
if row [ 15 ] is not None : \n 
~~~ keywords = row [ 15 ] . rsplit ( "," ) \n 
for kw in keywords : \n 
~~~ if len ( kw ) > 0 : \n 
~~~ issue . keywords . add ( kw . strip ( ) ) \n 
~~ ~~ ~~ custom_field_cursor = self . db_cnx . cursor ( ) \n 
~~~ issue . custom_fields [ cf [ 0 ] . capitalize ( ) ] = cf [ 1 ] \n 
~~ attachment_cursor = self . db_cnx . cursor ( ) \n 
for elem in attachment_cursor : \n 
~~~ at = TracAttachment ( Attachment . _get_path ( self . env . path , , str ( issue . id ) , elem at . name = elem [ 0 ] \n 
at . size = elem [ 1 ] \n 
at . time = to_unix_time ( elem [ 2 ] ) \n 
at . description = elem [ 3 ] \n 
at . author_name = elem [ 4 ] \n 
issue . attachment . add ( at ) \n 
~~ trac_issues . append ( issue ) \n 
change_cursor = self . db_cnx . cursor ( ) \n 
~~~ if ( elem [ 2 ] is None ) or ( not len ( elem [ 2 ] . lstrip ( ) ) ) : \n 
~~ comment = TracComment ( to_unix_time ( elem [ 0 ] ) ) \n 
comment . author = str ( elem [ 1 ] ) \n 
comment . content = unicode ( elem [ 2 ] ) \n 
comment . id = elem [ 3 ] \n 
issue . comments . add ( comment ) \n 
~~ for ttp in self . _timetracking_plugins : \n 
~~~ issue . workitems . update ( set ( ttp [ row [ 0 ] ] ) ) \n 
~~ ~~ return trac_issues \n 
~~ def get_custom_fields_declared ( self ) : \n 
~~~ ini_file_path = self . env_path + "/conf/trac.ini" \n 
parser = ConfigParser ( ) \n 
parser . read ( ini_file_path ) \n 
if not ( "ticket-custom" in parser . sections ( ) ) : \n 
~~~ return set ( [ ] ) \n 
~~ result = parser . items ( "ticket-custom" ) \n 
items = dict ( [ ] ) \n 
for elem in result : \n 
~~~ items [ elem [ 0 ] ] = elem [ 1 ] \n 
~~ keys = items . keys ( ) \n 
custom_fields = list ( [ ] ) \n 
for k in keys : \n 
~~~ if not ( "." in k ) : \n 
~~~ field = TracCustomFieldDeclaration ( k . capitalize ( ) ) \n 
field . type = items [ k ] \n 
options_key = k + ".options" \n 
if options_key in items : \n 
~~~ opts_str = items [ options_key ] \n 
opts = opts_str . rsplit ( "|" ) \n 
for o in opts : \n 
~~~ field . options . append ( o ) \n 
~~ ~~ value_key = k + ".value" \n 
if value_key in items : \n 
~~~ field . value = items [ value_key ] \n 
~~ label_key = k + ".label" \n 
if label_key in items : \n 
~~~ field . label = items [ label_key ] \n 
~~ custom_fields . append ( field ) \n 
~~ ~~ return custom_fields \n 
~~ def _get_data_from_enum ( self , type_name ) : \n 
return [ row [ 0 ] for row in cursor ] \n 
from bacpypes . debugging import Logging , function_debugging , ModuleLogger \n 
from bacpypes . consolelogging import ConsoleLogHandler \n 
from bacpypes . pdu import Address \n 
from bacpypes . analysis import trace , strftimestamp , Tracer \n 
from bacpypes . npdu import WhoIsRouterToNetwork \n 
_debug = 0 \n 
_log = ModuleLogger ( globals ( ) ) \n 
filterSource = None \n 
filterDestination = None \n 
filterHost = None \n 
requests = defaultdict ( int ) \n 
networks = defaultdict ( list ) \n 
@ function_debugging \n 
def Match ( addr1 , addr2 ) : \n 
if ( addr2 . addrType == Address . localBroadcastAddr ) : \n 
~~~ return ( addr1 . addrType == Address . localStationAddr ) or ( addr1 . addrType == Address . localBroadcastAddr ~~ elif ( addr2 . addrType == Address . localStationAddr ) : \n 
~~~ return ( addr1 . addrType == Address . localStationAddr ) and ( addr1 . addrAddr == addr2 . addrAddr ) \n 
~~ elif ( addr2 . addrType == Address . remoteBroadcastAddr ) : \n 
~~~ return ( ( addr1 . addrType == Address . remoteStationAddr ) or ( addr1 . addrType == Address . remoteBroadcastAddr and ( addr1 . addrNet == addr2 . addrNet ) \n 
~~ elif ( addr2 . addrType == Address . remoteStationAddr ) : \n 
~~~ return ( addr1 . addrType == Address . remoteStationAddr ) and ( addr1 . addrNet == addr2 . addrNet ) and ( addr1 . addrAddr == addr2 . addrAddr ) \n 
~~ elif ( addr2 . addrType == Address . globalBroadcastAddr ) : \n 
~~~ return ( addr1 . addrType == Address . globalBroadcastAddr ) \n 
~~ ~~ class WhoIsRouterToNetworkSummary ( Tracer , Logging ) : \n 
~~~ if _debug : IAmRouterToNetworkSummary . _debug ( "__init__" ) \n 
Tracer . __init__ ( self , self . Filter ) \n 
~~ def Filter ( self , pkt ) : \n 
global requests , networks \n 
if not isinstance ( pkt , WhoIsRouterToNetwork ) : \n 
~~ if filterSource : \n 
~~~ if not Match ( pkt . pduSource , filterSource ) : \n 
~~ ~~ if filterDestination : \n 
~~~ if not Match ( pkt . pduDestination , filterDestination ) : \n 
~~ ~~ if filterHost : \n 
~~ ~~ requests [ pkt . pduSource ] += 1 \n 
networks [ pkt . pduSource ] . append ( pkt . wirtnNetwork ) \n 
~~ ~~ try : \n 
~~~ if ( in sys . argv ) : \n 
~~~ indx = sys . argv . index ( ) \n 
for i in range ( indx + 1 , len ( sys . argv ) ) : \n 
~~~ ConsoleLogHandler ( sys . argv [ i ] ) \n 
~~ del sys . argv [ indx : ] \n 
~~ if _debug : _log . debug ( "initialization" ) \n 
if ( in sys . argv ) : \n 
~~~ i = sys . argv . index ( ) \n 
filterSource = Address ( sys . argv [ i + 1 ] ) \n 
del sys . argv [ i : i + 2 ] \n 
~~ if ( in sys . argv ) : \n 
filterDestination = Address ( sys . argv [ i + 1 ] ) \n 
filterHost = Address ( sys . argv [ i + 1 ] ) \n 
~~ for fname in sys . argv [ 1 : ] : \n 
~~~ trace ( fname , [ WhoIsRouterToNetworkSummary ] ) \n 
~~ items = requests . items ( ) \n 
items . sort ( lambda x , y : cmp ( y [ 1 ] , x [ 1 ] ) ) \n 
for key , count in items : \n 
net_count = defaultdict ( int ) \n 
for net in networks [ key ] : \n 
~~~ net_count [ net ] += 1 \n 
~~ net_count = net_count . items ( ) \n 
net_count . sort ( lambda x , y : cmp ( y [ 1 ] , x [ 1 ] ) ) \n 
for net , count in net_count : \n 
~~ ~~ ~~ except KeyboardInterrupt : \n 
~~~ if _debug : _log . debug ( "finally" ) \n 
import asyncore \n 
from time import time as _time , sleep as _sleep \n 
from StringIO import StringIO \n 
from . debugging import ModuleLogger , DebugContents , bacpypes_debugging \n 
from . core import deferred \n 
from . task import FunctionTask , OneShotFunction \n 
from . comm import PDU , Client , Server \n 
from . comm import ServiceAccessPoint , ApplicationServiceElement \n 
REBIND_SLEEP_INTERVAL = 2.0 \n 
class PickleActorMixIn : \n 
~~~ def __init__ ( self , * args ) : \n 
super ( PickleActorMixIn , self ) . __init__ ( * args ) \n 
self . pickleBuffer = \n 
~~ def indication ( self , pdu ) : \n 
pdu . pduData = pickle . dumps ( pdu . pduData ) \n 
super ( PickleActorMixIn , self ) . indication ( pdu ) \n 
~~ def response ( self , pdu ) : \n 
self . pickleBuffer += pdu . pduData \n 
strm = StringIO ( self . pickleBuffer ) \n 
pos = 0 \n 
while ( pos < strm . len ) : \n 
~~~ msg = pickle . load ( strm ) \n 
~~ rpdu = PDU ( msg ) \n 
rpdu . update ( pdu ) \n 
super ( PickleActorMixIn , self ) . response ( rpdu ) \n 
pos = strm . tell ( ) \n 
~~ if ( pos < strm . len ) : \n 
~~~ self . pickleBuffer = self . pickleBuffer [ pos : ] \n 
~~~ self . pickleBuffer = \n 
~~ ~~ ~~ bacpypes_debugging ( PickleActorMixIn ) \n 
class TCPClient ( asyncore . dispatcher ) : \n 
~~~ def __init__ ( self , peer ) : \n 
asyncore . dispatcher . __init__ ( self ) \n 
self . peer = peer \n 
self . request = \n 
self . socketError = None \n 
self . connect ( peer ) \n 
~~~ if _debug : deferred ( TCPClient . _debug , "handle_connect" ) \n 
~~ def handle_expt ( self ) : \n 
~~ def readable ( self ) : \n 
~~~ return 1 \n 
~~ def handle_read ( self ) : \n 
~~~ if _debug : deferred ( TCPClient . _debug , "handle_read" ) \n 
~~~ msg = self . recv ( 65536 ) \n 
if not self . socket : \n 
~~~ deferred ( self . response , PDU ( msg ) ) \n 
~~ ~~ except socket . error , err : \n 
~~~ if ( err . args [ 0 ] == 111 ) : \n 
~~ self . socketError = err \n 
~~ ~~ def writable ( self ) : \n 
~~~ return ( len ( self . request ) != 0 ) \n 
~~ def handle_write ( self ) : \n 
~~~ if _debug : deferred ( TCPClient . _debug , "handle_write" ) \n 
~~~ sent = self . send ( self . request ) \n 
self . request = self . request [ sent : ] \n 
~~ except socket . error , err : \n 
~~ ~~ def handle_close ( self ) : \n 
~~~ if _debug : deferred ( TCPClient . _debug , "handle_close" ) \n 
self . socket = None \n 
self . request += pdu . pduData \n 
~~ ~~ bacpypes_debugging ( TCPClient ) \n 
class TCPClientActor ( TCPClient ) : \n 
~~~ def __init__ ( self , director , peer ) : \n 
TCPClient . __init__ ( self , peer ) \n 
self . director = director \n 
self . timeout = director . timeout \n 
if self . timeout > 0 : \n 
~~~ self . timer = FunctionTask ( self . idle_timeout ) \n 
self . timer . install_task ( _time ( ) + self . timeout ) \n 
~~~ self . timer = None \n 
~~ self . flushTask = None \n 
self . director . add_actor ( self ) \n 
~~ def handle_close ( self ) : \n 
~~~ if _debug : TCPClientActor . _debug ( "handle_close" ) \n 
if self . flushTask : \n 
~~~ self . flushTask . suspend_task ( ) \n 
~~ if self . timer : \n 
~~~ self . timer . suspend_task ( ) \n 
~~ self . director . remove_actor ( self ) \n 
TCPClient . handle_close ( self ) \n 
~~ def idle_timeout ( self ) : \n 
~~~ if _debug : TCPClientActor . _debug ( "idle_timeout" ) \n 
self . handle_close ( ) \n 
~~~ self . timer . install_task ( _time ( ) + self . timeout ) \n 
~~ TCPClient . indication ( self , pdu ) \n 
pdu . pduSource = self . peer \n 
if self . timer : \n 
~~ self . director . response ( pdu ) \n 
~~ def flush ( self ) : \n 
~~~ if _debug : TCPClientActor . _debug ( "flush" ) \n 
self . flushTask = None \n 
if self . request : \n 
~~~ self . flushTask = OneShotFunction ( self . flush ) \n 
~~ self . handle_close ( ) \n 
~~ ~~ bacpypes_debugging ( TCPClientActor ) \n 
class TCPPickleClientActor ( PickleActorMixIn , TCPClientActor ) : \n 
~~ class TCPClientDirector ( Server , ServiceAccessPoint , DebugContents ) : \n 
~~~ _debug_contents = ( , , , ) \n 
def __init__ ( self , timeout = 0 , actorClass = TCPClientActor , sid = None , sapID = None ) : \n 
ServiceAccessPoint . __init__ ( self , sapID ) \n 
if not issubclass ( actorClass , TCPClientActor ) : \n 
~~ self . actorClass = actorClass \n 
self . timeout = timeout \n 
self . clients = { } \n 
self . reconnect = { } \n 
~~ def add_actor ( self , actor ) : \n 
self . clients [ actor . peer ] = actor \n 
if self . serviceElement : \n 
~~~ self . sap_request ( addPeer = actor . peer ) \n 
~~ ~~ def remove_actor ( self , actor ) : \n 
del self . clients [ actor . peer ] \n 
~~~ self . sap_request ( delPeer = actor . peer ) \n 
~~ if actor . peer in self . reconnect : \n 
~~~ connect_task = FunctionTask ( self . connect , actor . peer ) \n 
connect_task . install_task ( _time ( ) + self . reconnect [ actor . peer ] ) \n 
~~ ~~ def get_actor ( self , address ) : \n 
return self . clients . get ( address , None ) \n 
~~ def connect ( self , address , reconnect = 0 ) : \n 
if address in self . clients : \n 
~~ client = self . actorClass ( self , address ) \n 
if reconnect : \n 
~~~ self . reconnect [ address ] = reconnect \n 
~~ ~~ def disconnect ( self , address ) : \n 
if address not in self . clients : \n 
~~ if address in self . reconnect : \n 
~~~ del self . reconnect [ address ] \n 
~~ self . clients [ address ] . handle_close ( ) \n 
addr = pdu . pduDestination \n 
client = self . clients . get ( addr , None ) \n 
if not client : \n 
~~~ client = self . actorClass ( self , addr ) \n 
~~ client . indication ( pdu ) \n 
~~ ~~ bacpypes_debugging ( TCPClientDirector ) \n 
class TCPServer ( asyncore . dispatcher ) : \n 
~~~ def __init__ ( self , sock , peer ) : \n 
asyncore . dispatcher . __init__ ( self , sock ) \n 
~~~ if _debug : deferred ( TCPServer . _debug , "handle_connect" ) \n 
~~~ if _debug : deferred ( TCPServer . _debug , "handle_read" ) \n 
~~~ if _debug : deferred ( TCPServer . _debug , "handle_write" ) \n 
~~ except socket . error , why : \n 
~~~ if ( why . args [ 0 ] == 111 ) : \n 
~~ self . socketError = why \n 
~~~ if _debug : deferred ( TCPServer . _debug , "handle_close" ) \n 
if not self : \n 
~~ if not self . socket : \n 
~~ self . close ( ) \n 
~~ ~~ bacpypes_debugging ( TCPServer ) \n 
class TCPServerActor ( TCPServer ) : \n 
~~~ def __init__ ( self , director , sock , peer ) : \n 
TCPServer . __init__ ( self , sock , peer ) \n 
~~~ if _debug : TCPServerActor . _debug ( "handle_close" ) \n 
TCPServer . handle_close ( self ) \n 
~~~ if _debug : TCPServerActor . _debug ( "idle_timeout" ) \n 
~~ TCPServer . indication ( self , pdu ) \n 
~~ pdu . pduSource = self . peer \n 
~~~ if _debug : TCPServerActor . _debug ( "flush" ) \n 
~~ ~~ bacpypes_debugging ( TCPServerActor ) \n 
class TCPPickleServerActor ( PickleActorMixIn , TCPServerActor ) : \n 
~~ class TCPServerDirector ( asyncore . dispatcher , Server , ServiceAccessPoint , DebugContents ) : \n 
def __init__ ( self , address , listeners = 5 , timeout = 0 , reuse = False , actorClass = TCPServerActor , cid = ~~~ if _debug : \n 
~~ Server . __init__ ( self , cid ) \n 
self . port = address \n 
if not issubclass ( actorClass , TCPServerActor ) : \n 
self . servers = { } \n 
if reuse : \n 
~~~ self . set_reuse_addr ( ) \n 
~~ hadBindErrors = False \n 
for i in range ( 30 ) : \n 
~~~ self . bind ( address ) \n 
~~~ hadBindErrors = True \n 
TCPServerDirector . _warning ( , err ) \n 
_sleep ( REBIND_SLEEP_INTERVAL ) \n 
~~~ TCPServerDirector . _error ( ) \n 
~~ if hadBindErrors : \n 
~~~ TCPServerDirector . _info ( ) \n 
~~ self . listen ( listeners ) \n 
~~ def handle_accept ( self ) : \n 
~~~ if _debug : TCPServerDirector . _debug ( "handle_accept" ) \n 
~~~ client , addr = self . accept ( ) \n 
~~ except socket . error : \n 
~~~ TCPServerDirector . _warning ( ) \n 
server = self . actorClass ( self , client , addr ) \n 
self . servers [ addr ] = server \n 
~~~ if _debug : TCPServerDirector . _debug ( "handle_close" ) \n 
self . servers [ actor . peer ] = actor \n 
~~~ del self . servers [ actor . peer ] \n 
~~ if self . serviceElement : \n 
return self . servers . get ( address , None ) \n 
server = self . servers . get ( addr , None ) \n 
if not server : \n 
~~ server . indication ( pdu ) \n 
~~ ~~ bacpypes_debugging ( TCPServerDirector ) \n 
class StreamToPacket ( Client , Server ) : \n 
~~~ def __init__ ( self , fn , cid = None , sid = None ) : \n 
Client . __init__ ( self , cid ) \n 
Server . __init__ ( self , sid ) \n 
self . packetFn = fn \n 
self . upstreamBuffer = { } \n 
self . downstreamBuffer = { } \n 
~~ def packetize ( self , pdu , streamBuffer ) : \n 
def chop ( addr ) : \n 
buff = streamBuffer . get ( addr , ) + pdu . pduData \n 
~~~ packet = self . packetFn ( buff ) \n 
if packet is None : \n 
~~ yield PDU ( packet [ 0 ] , \n 
source = pdu . pduSource , \n 
destination = pdu . pduDestination , \n 
user_data = pdu . pduUserData , \n 
buff = packet [ 1 ] \n 
~~ streamBuffer [ addr ] = buff \n 
~~ if pdu . pduSource : \n 
~~~ for pdu in chop ( pdu . pduSource ) : \n 
~~~ yield pdu \n 
~~ ~~ if pdu . pduDestination : \n 
~~~ for pdu in chop ( pdu . pduDestination ) : \n 
~~ ~~ ~~ def indication ( self , pdu ) : \n 
for packet in self . packetize ( pdu , self . downstreamBuffer ) : \n 
~~~ self . request ( packet ) \n 
~~ ~~ def confirmation ( self , pdu ) : \n 
for packet in self . packetize ( pdu , self . upstreamBuffer ) : \n 
~~~ self . response ( packet ) \n 
~~ ~~ ~~ bacpypes_debugging ( StreamToPacket ) \n 
class StreamToPacketSAP ( ApplicationServiceElement , ServiceAccessPoint ) : \n 
~~~ def __init__ ( self , stp , aseID = None , sapID = None ) : \n 
ApplicationServiceElement . __init__ ( self , aseID ) \n 
self . stp = stp \n 
~~ def indication ( self , addPeer = None , delPeer = None ) : \n 
if addPeer : \n 
~~~ self . stp . upstreamBuffer [ addPeer ] = \n 
self . stp . downstreamBuffer [ addPeer ] = \n 
~~ if delPeer : \n 
~~~ del self . stp . upstreamBuffer [ delPeer ] \n 
del self . stp . downstreamBuffer [ delPeer ] \n 
~~~ self . sap_request ( addPeer = addPeer , delPeer = delPeer ) \n 
~~ ~~ ~~ bacpypes_debugging ( StreamToPacketSAP ) \n 
from copy import deepcopy \n 
from . errors import ConfigurationError \n 
from . debugging import ModuleLogger , bacpypes_debugging \n 
from . pdu import Address \n 
from . comm import Server \n 
@ bacpypes_debugging \n 
class Network : \n 
~~~ def __init__ ( self , dropPercent = 0.0 ) : \n 
self . nodes = [ ] \n 
self . dropPercent = dropPercent \n 
~~ def add_node ( self , node ) : \n 
self . nodes . append ( node ) \n 
node . lan = self \n 
~~ def remove_node ( self , node ) : \n 
self . nodes . remove ( node ) \n 
node . lan = None \n 
~~ def process_pdu ( self , pdu ) : \n 
if self . dropPercent != 0.0 : \n 
~~~ if ( random . random ( ) * 100.0 ) < self . dropPercent : \n 
~~ ~~ if not pdu . pduDestination or not isinstance ( pdu . pduDestination , Address ) : \n 
~~ elif pdu . pduDestination . addrType == Address . localBroadcastAddr : \n 
~~~ for n in self . nodes : \n 
~~~ if ( pdu . pduSource != n . address ) : \n 
~~~ n . response ( deepcopy ( pdu ) ) \n 
~~ ~~ ~~ elif pdu . pduDestination . addrType == Address . localStationAddr : \n 
~~~ if n . promiscuous or ( pdu . pduDestination == n . address ) : \n 
~~ ~~ def __len__ ( self ) : \n 
if _debug : Network . _debug ( "__len__" ) \n 
return len ( self . nodes ) \n 
~~ ~~ @ bacpypes_debugging \n 
class Node ( Server ) : \n 
~~~ def __init__ ( self , addr , lan = None , promiscuous = False , spoofing = False , sid = None ) : \n 
~~~ if _debug : \n 
addr , lan , promiscuous , spoofing , sid \n 
~~ Server . __init__ ( self , sid ) \n 
if not isinstance ( addr , Address ) : \n 
~~ self . lan = None \n 
self . address = addr \n 
if lan : \n 
~~~ self . bind ( lan ) \n 
~~ self . promiscuous = promiscuous \n 
self . spoofing = spoofing \n 
~~ def bind ( self , lan ) : \n 
lan . add_node ( self ) \n 
if not self . lan : \n 
~~ if pdu . pduSource is None : \n 
~~~ pdu . pduSource = self . address \n 
~~ elif ( not self . spoofing ) and ( pdu . pduSource != self . address ) : \n 
~~ deferred ( self . lan . process_pdu , pdu ) \n 
from bacpypes . debugging import bacpypes_debugging , ModuleLogger \n 
from bacpypes . consolelogging import ConfigArgumentParser \n 
from bacpypes . consolecmd import ConsoleCmd \n 
from bacpypes . core import run \n 
from bacpypes . app import LocalDeviceObject , BIPSimpleApplication \n 
from bacpypes . apdu import Error , AbortPDU , AtomicReadFileRequest , AtomicReadFileRequestAccessMethodChoice , AtomicReadFileRequestAccessMethodChoiceRecordAccess , AtomicReadFileRequestAccessMethodChoiceStreamAccess , AtomicReadFileACK , AtomicWriteFileRequest , AtomicWriteFileRequestAccessMethodChoice , AtomicWriteFileRequestAccessMethodChoiceRecordAccess , AtomicWriteFileRequestAccessMethodChoiceStreamAccess , AtomicWriteFileACK \n 
from bacpypes . basetypes import ServicesSupported \n 
this_application = None \n 
class TestApplication ( BIPSimpleApplication ) : \n 
~~~ def request ( self , apdu ) : \n 
self . _request = apdu \n 
BIPSimpleApplication . request ( self , apdu ) \n 
~~ def confirmation ( self , apdu ) : \n 
if isinstance ( apdu , Error ) : \n 
~~ elif isinstance ( apdu , AbortPDU ) : \n 
~~~ apdu . debug_contents ( ) \n 
~~~ if apdu . accessMethod . recordAccess : \n 
~~~ value = apdu . accessMethod . recordAccess . fileRecordData \n 
~~ elif apdu . accessMethod . streamAccess : \n 
~~~ value = apdu . accessMethod . streamAccess . fileData \n 
sys . stdout . write ( repr ( value ) + ) \n 
~~~ if apdu . fileStartPosition is not None : \n 
~~~ value = apdu . fileStartPosition \n 
~~ elif apdu . fileStartRecord is not None : \n 
~~~ value = apdu . fileStartRecord \n 
~~ ~~ ~~ @ bacpypes_debugging \n 
class TestConsoleCmd ( ConsoleCmd ) : \n 
~~~ def do_readrecord ( self , args ) : \n 
args = args . split ( ) \n 
~~~ addr , obj_inst , start_record , record_count = args \n 
obj_type = \n 
obj_inst = int ( obj_inst ) \n 
start_record = int ( start_record ) \n 
record_count = int ( record_count ) \n 
request = AtomicReadFileRequest ( \n 
fileIdentifier = ( obj_type , obj_inst ) , \n 
accessMethod = AtomicReadFileRequestAccessMethodChoice ( \n 
recordAccess = AtomicReadFileRequestAccessMethodChoiceRecordAccess ( \n 
fileStartRecord = start_record , \n 
requestedRecordCount = record_count , \n 
request . pduDestination = Address ( addr ) \n 
this_application . request ( request ) \n 
~~ ~~ def do_readstream ( self , args ) : \n 
~~~ addr , obj_inst , start_position , octet_count = args \n 
start_position = int ( start_position ) \n 
octet_count = int ( octet_count ) \n 
streamAccess = AtomicReadFileRequestAccessMethodChoiceStreamAccess ( \n 
fileStartPosition = start_position , \n 
requestedOctetCount = octet_count , \n 
~~ ~~ def do_writerecord ( self , args ) : \n 
~~~ addr , obj_inst , start_record , record_count = args [ 0 : 4 ] \n 
record_data = list ( args [ 4 : ] ) \n 
request = AtomicWriteFileRequest ( \n 
accessMethod = AtomicWriteFileRequestAccessMethodChoice ( \n 
recordAccess = AtomicWriteFileRequestAccessMethodChoiceRecordAccess ( \n 
recordCount = record_count , \n 
fileRecordData = record_data , \n 
~~ ~~ def do_writestream ( self , args ) : \n 
~~~ addr , obj_inst , start_position , data = args \n 
streamAccess = AtomicWriteFileRequestAccessMethodChoiceStreamAccess ( \n 
fileData = data , \n 
~~ ~~ ~~ try : \n 
~~~ args = ConfigArgumentParser ( description = __doc__ ) . parse_args ( ) \n 
if _debug : _log . debug ( "initialization" ) \n 
this_device = LocalDeviceObject ( \n 
objectName = args . ini . objectname , \n 
objectIdentifier = int ( args . ini . objectidentifier ) , \n 
maxApduLengthAccepted = int ( args . ini . maxapdulengthaccepted ) , \n 
segmentationSupported = args . ini . segmentationsupported , \n 
vendorIdentifier = int ( args . ini . vendoridentifier ) , \n 
this_application = TestApplication ( this_device , args . ini . address ) \n 
services_supported = this_application . get_services_supported ( ) \n 
this_device . protocolServicesSupported = services_supported . value \n 
this_console = TestConsoleCmd ( ) \n 
_log . debug ( "running" ) \n 
run ( ) \n 
~~~ _log . debug ( "finally" ) \n 
from . import test_address \n 
from __future__ import division , unicode_literals \n 
from . bencode import bencode , bdecode \n 
from . humanize import humanize_bytes \n 
from . utils import is_unsplitable , get_root_of_unsplitable , Pieces \n 
class Color : \n 
~~~ BLACK = \n 
RED = \n 
GREEN = \n 
YELLOW = \n 
BLUE = \n 
PINK = \n 
CYAN = \n 
WHITE = \n 
ENDC = \n 
~~ COLOR_OK = Color . GREEN \n 
COLOR_MISSING_FILES = Color . RED \n 
COLOR_ALREADY_SEEDING = Color . BLUE \n 
COLOR_FOLDER_EXIST_NOT_SEEDING = Color . YELLOW \n 
COLOR_FAILED_TO_ADD_TO_CLIENT = Color . PINK \n 
class Status : \n 
~~~ OK = 0 \n 
MISSING_FILES = 1 \n 
ALREADY_SEEDING = 2 \n 
FOLDER_EXIST_NOT_SEEDING = 3 \n 
FAILED_TO_ADD_TO_CLIENT = 4 \n 
~~ status_messages = { \n 
Status . OK : % ( COLOR_OK , Color . ENDC ) , \n 
Status . MISSING_FILES : % ( COLOR_MISSING_FILES , Color . ENDC ) , \n 
Status . ALREADY_SEEDING : % ( COLOR_ALREADY_SEEDING , Color . ENDC ) , \n 
Status . FOLDER_EXIST_NOT_SEEDING : % ( COLOR_FOLDER_EXIST_NOT_SEEDING , Color . ENDC ) , \n 
Status . FAILED_TO_ADD_TO_CLIENT : % ( COLOR_FAILED_TO_ADD_TO_CLIENT , Color . ENDC ) , \n 
CHUNK_SIZE = 65536 \n 
class UnknownLinkTypeException ( Exception ) : \n 
~~ class IllegalPathException ( Exception ) : \n 
~~ class AutoTorrent ( object ) : \n 
~~~ def __init__ ( self , db , client , store_path , add_limit_size , add_limit_percent , delete_torrents , link_type ~~~ self . db = db \n 
self . client = client \n 
self . store_path = store_path \n 
self . add_limit_size = add_limit_size \n 
self . add_limit_percent = add_limit_percent \n 
self . delete_torrents = delete_torrents \n 
self . link_type = link_type \n 
self . torrents_seeded = set ( ) \n 
~~ def try_decode ( self , value ) : \n 
~~~ return value . decode ( ) \n 
~~ except UnicodeDecodeError : \n 
~~~ logger . debug ( % value ) \n 
~~ return value . decode ( ) \n 
~~ def is_legal_path ( self , path ) : \n 
~~~ for p in path : \n 
~~~ if p in [ , ] or in p : \n 
~~ def populate_torrents_seeded ( self ) : \n 
self . torrents_seeded = set ( x . lower ( ) for x in self . client . get_torrents ( ) ) \n 
~~ def get_info_hash ( self , torrent ) : \n 
return hashlib . sha1 ( bencode ( torrent [ ] ) ) . hexdigest ( ) \n 
~~ def find_hash_checks ( self , torrent , result ) : \n 
modified_result = False \n 
pieces = Pieces ( torrent ) \n 
if self . db . hash_slow_mode : \n 
~~~ logger . info ( ) \n 
self . db . build_hash_size_table ( ) \n 
~~ start_size = 0 \n 
end_size = 0 \n 
logger . info ( ) \n 
for f in result : \n 
~~~ start_size = end_size \n 
end_size += f [ ] \n 
if f [ ] : \n 
~~ files_to_check = [ ] \n 
logger . debug ( ) \n 
if self . db . hash_size_mode : \n 
~~~ logger . debug ( ) \n 
files_to_check += self . db . find_hash_size ( f [ ] ) \n 
~~ if self . db . hash_name_mode : \n 
name = f [ ] [ - 1 ] \n 
files_to_check += self . db . find_hash_name ( name ) \n 
~~ if self . db . hash_slow_mode : \n 
files_to_check += self . db . find_hash_varying_size ( f [ ] ) \n 
~~ logger . debug ( % len ( files_to_check ) ) \n 
checked_files = set ( ) \n 
for db_file in files_to_check : \n 
~~~ if db_file in checked_files : \n 
~~~ logger . debug ( % db_file ) \n 
~~ checked_files . add ( db_file ) \n 
logger . info ( % db_file ) \n 
match_start , match_end = pieces . match_file ( db_file , start_size , end_size ) \n 
logger . info ( % ( db_file , match_start , match_end \n 
~~~ size = os . path . getsize ( db_file ) \n 
if match_start and match_end : \n 
modification_point = pieces . find_piece_breakpoint ( db_file , start_size , end_size ~~ elif match_start : \n 
modification_point = min ( f [ ] , size ) \n 
~~ elif match_end : \n 
modification_point = 0 \n 
~~ if size > f [ ] : \n 
~~~ modification_action = \n 
~~ f [ ] = False \n 
f [ ] = ( , modification_action , modification_point ) \n 
modified_result = True \n 
f [ ] = True \n 
~~ f [ ] = db_file \n 
~~ ~~ ~~ return modified_result , result \n 
~~ def index_torrent ( self , torrent ) : \n 
torrent_name = torrent [ ] [ ] \n 
logger . debug ( % ( torrent_name , ) ) \n 
torrent_name = self . try_decode ( torrent_name ) \n 
if not self . is_legal_path ( [ torrent_name ] ) : \n 
~~~ raise IllegalPathException ( % torrent_name \n 
~~ logger . info ( % torrent_name ) \n 
if self . db . exact_mode : \n 
~~~ prefix = if in torrent [ ] else \n 
paths = self . db . find_exact_file_path ( prefix , torrent_name ) \n 
if paths : \n 
~~~ for path in paths : \n 
~~~ logger . debug ( % path ) \n 
if prefix == : \n 
size = os . path . getsize ( path ) \n 
if torrent [ ] [ ] != size : \n 
~~ return { : , \n 
: os . path . dirname ( path ) , \n 
: [ { \n 
: path , \n 
: size , \n 
: [ torrent_name ] , \n 
} ] } \n 
~~~ result = [ ] \n 
for f in torrent [ ] [ ] : \n 
~~~ orig_path = [ self . try_decode ( x ) for x in f [ ] ] \n 
p = os . path . join ( path , * orig_path ) \n 
if not os . path . isfile ( p ) : \n 
~~~ logger . debug ( % p ) \n 
~~ size = os . path . getsize ( p ) \n 
if size != f [ ] : \n 
~~~ logger . debug ( break \n 
~~ result . append ( { \n 
: p , \n 
: f [ ] , \n 
: orig_path , \n 
return { : , \n 
: result } \n 
~~ ~~ ~~ ~~ ~~ result = [ ] \n 
~~~ files_sorted = { } \n 
files = { } \n 
if in torrent [ ] : \n 
~~~ i = 0 \n 
path_files = defaultdict ( list ) \n 
~~~ logger . debug ( % ( f , ) ) \n 
~~~ raise IllegalPathException ( % \n 
~~ path = [ torrent_name ] + orig_path \n 
name = path . pop ( ) \n 
path_files [ os . path . join ( * path ) ] . append ( { \n 
files_sorted [ . join ( orig_path ) ] = i \n 
i += 1 \n 
~~ ~~ if self . db . unsplitable_mode : \n 
~~~ unsplitable_paths = set ( ) \n 
for path , files in path_files . items ( ) : \n 
~~~ if is_unsplitable ( f [ ] [ - 1 ] for f in files ) : \n 
~~~ path = path . split ( os . sep ) \n 
name = get_root_of_unsplitable ( path ) \n 
~~ while path [ - 1 ] != name : \n 
~~~ path . pop ( ) \n 
~~ unsplitable_paths . add ( os . path . join ( * path ) ) \n 
~~ ~~ ~~ for path , files in path_files . items ( ) : \n 
~~~ if self . db . unsplitable_mode : \n 
while path and os . path . join ( * path ) not in unsplitable_paths : \n 
~~~ path = None \n 
~~ if path : \n 
~~~ name = path [ - 1 ] \n 
for f in files : \n 
~~~ actual_path = self . db . find_unsplitable_file_path ( name , f [ ] , f [ f [ ] = actual_path \n 
f [ ] = actual_path is not None \n 
~~ result += files \n 
~~~ for f in files : \n 
~~~ actual_path = self . db . find_file_path ( f [ ] [ - 1 ] , f [ ] ) \n 
f [ ] = actual_path \n 
~~ ~~ result = sorted ( result , key = lambda x : files_sorted [ . join ( x [ ] ) ] ) \n 
~~~ length = torrent [ ] [ ] \n 
actual_path = self . db . find_file_path ( torrent_name , length ) \n 
result . append ( { \n 
: actual_path , \n 
: length , \n 
: actual_path is not None , \n 
~~ mode = \n 
if self . db . hash_mode : \n 
~~~ modified_result , result = self . find_hash_checks ( torrent , result ) \n 
if modified_result : \n 
~~~ mode = \n 
~~ ~~ return { : mode , : result } \n 
~~ def parse_torrent ( self , torrent ) : \n 
files = self . index_torrent ( torrent ) \n 
found_size , missing_size = 0 , 0 \n 
for f in files [ ] : \n 
~~~ if f [ ] or f . get ( ) : \n 
~~~ found_size += f [ ] \n 
~~~ missing_size += f [ ] \n 
~~ ~~ return found_size , missing_size , files \n 
~~ def link_files ( self , destination_path , files ) : \n 
if not os . path . isdir ( destination_path ) : \n 
~~~ os . makedirs ( destination_path ) \n 
~~ for f in files : \n 
~~~ if f [ ] : \n 
~~~ destination = os . path . join ( destination_path , * f [ ] ) \n 
file_path = os . path . dirname ( destination ) \n 
if not os . path . isdir ( file_path ) : \n 
~~~ logger . debug ( % file_path ) \n 
os . makedirs ( file_path ) \n 
~~ logger . debug ( % ( self . link_type , f [ ] , destination \n 
if self . link_type == : \n 
~~~ os . symlink ( f [ ] , destination ) \n 
~~ elif self . link_type == : \n 
~~~ os . link ( f [ ] , destination ) \n 
~~~ raise UnknownLinkTypeException ( % self . link_type ) \n 
~~ ~~ ~~ ~~ def rewrite_hashed_files ( self , destination_path , files ) : \n 
~~~ if not f [ ] and in f : \n 
~~ logger . debug ( % ( f [ ] , destination ) ) \n 
_ , modification_action , modification_point = f [ ] \n 
current_size = os . path . getsize ( f [ ] ) \n 
expected_size = f [ ] \n 
diff = abs ( current_size - expected_size ) \n 
modified = False \n 
bytes_written = 0 \n 
with open ( destination , ) as output_fp : \n 
~~~ with open ( f [ ] , ) as input_fp : \n 
~~~ logger . debug ( while True : \n 
~~~ if not modified and bytes_written == modification_point : \n 
~~~ logger . debug ( % ( modification_action modified = True \n 
if modification_action == : \n 
~~~ seek_point = bytes_written + diff \n 
logger . debug ( input_fp . seek ( seek_point ) \n 
~~ elif modification_action == : \n 
~~~ logger . debug ( % diff ) \n 
while diff > 0 : \n 
~~~ write_bytes = min ( CHUNK_SIZE , diff ) \n 
output_fp . write ( * write_bytes ) \n 
diff -= write_bytes \n 
~~ ~~ ~~ read_bytes = CHUNK_SIZE \n 
if not modified : \n 
~~~ read_bytes = min ( read_bytes , modification_point - bytes_written ) \n 
~~ logger . debug ( % ( read_bytes , ) ) \n 
data = input_fp . read ( read_bytes ) \n 
if not data : \n 
~~ output_fp . write ( data ) \n 
bytes_written += read_bytes \n 
~~ ~~ ~~ logger . debug ( ) \n 
~~ ~~ ~~ def handle_torrentfile ( self , path , dry_run = False ) : \n 
logger . info ( % path ) \n 
torrent = self . open_torrentfile ( path ) \n 
if self . check_torrent_in_client ( torrent ) : \n 
~~~ self . print_status ( Status . ALREADY_SEEDING , path , ) \n 
if self . delete_torrents : \n 
~~~ logger . info ( % path ) \n 
os . remove ( path ) \n 
~~ return Status . ALREADY_SEEDING \n 
~~ found_size , missing_size , files = self . parse_torrent ( torrent ) \n 
missing_percent = ( missing_size / ( found_size + missing_size ) ) * 100 \n 
found_percent = 100 - missing_percent \n 
would_not_add = missing_size and missing_percent > self . add_limit_percent or missing_size > \n 
if dry_run : \n 
~~~ return found_size , missing_size , would_not_add , [ f [ ] for f in files [ \n 
~~ if would_not_add : \n 
~~~ logger . info ( % ( path , found_percent self . print_status ( Status . MISSING_FILES , path , return Status . MISSING_FILES \n 
~~ if files [ ] == or files [ ] == : \n 
destination_path = os . path . join ( self . store_path , os . path . splitext ( os . path . basename ( path ) \n 
if os . path . isdir ( destination_path ) : \n 
~~~ logger . info ( % destination_path ) \n 
self . print_status ( Status . FOLDER_EXIST_NOT_SEEDING , path , return Status . FOLDER_EXIST_NOT_SEEDING \n 
~~ self . link_files ( destination_path , files [ ] ) \n 
~~ elif files [ ] == : \n 
destination_path = files [ ] \n 
~~ fast_resume = True \n 
if files [ ] == : \n 
~~~ fast_resume = False \n 
self . rewrite_hashed_files ( destination_path , files [ ] ) \n 
~~ if self . delete_torrents : \n 
~~ if self . client . add_torrent ( torrent , destination_path , files [ ] , fast_resume ) : \n 
~~~ self . print_status ( Status . OK , path , ) \n 
return Status . OK \n 
~~~ self . print_status ( Status . FAILED_TO_ADD_TO_CLIENT , path , return Status . FAILED_TO_ADD_TO_CLIENT \n 
~~ ~~ def check_torrent_in_client ( self , torrent ) : \n 
info_hash = self . get_info_hash ( torrent ) \n 
return info_hash in self . torrents_seeded \n 
~~ def open_torrentfile ( self , path ) : \n 
with open ( path , ) as f : \n 
~~~ return bdecode ( f . read ( ) ) \n 
~~ ~~ def print_status ( self , status , torrentfile , message ) : \n 
~~~ print ( % ( % status_messages [ status ] , os . path . splitext ( os . path . basename ( ~~ ~~ import pytest \n 
import exceptions \n 
def test_exceptions ( ) : \n 
~~~ with pytest . raises ( Exception ) : \n 
~~~ raise exceptions . CardinalException \n 
~~ with pytest . raises ( exceptions . CardinalException ) : \n 
~~~ raise exceptions . InternalError \n 
~~~ raise exceptions . PluginError \n 
~~~ raise exceptions . CommandNotFoundError \n 
~~~ raise exceptions . ConfigNotFoundError \n 
~~~ raise exceptions . AmbiguousConfigError \n 
~~~ raise exceptions . EventAlreadyExistsError \n 
~~~ raise exceptions . EventDoesNotExistError \n 
~~~ raise exceptions . EventCallbackError \n 
~~~ raise exceptions . EventRejectedMessage \n 
import legofy \n 
import tkinter as tk \n 
import tkinter . ttk as ttk \n 
from tkinter import filedialog \n 
import tkinter . messagebox as tkmsg \n 
LEGO_PALETTE = ( , , , , , , ) \n 
class LegofyGui ( tk . Tk ) : \n 
~~~ def __init__ ( self , * args , ** kwargs ) : \n 
~~~ super ( ) . __init__ ( * args , ** kwargs ) \n 
self . wm_title ( "Legofy!" ) \n 
self . iconbitmap ( os . path . dirname ( os . path . realpath ( __file__ ) ) + ) \n 
self . resizable ( False , False ) \n 
self . body = LegofyGuiMainFrame ( self ) \n 
self . body . grid ( row = 0 , column = 0 , padx = 10 , pady = 10 ) \n 
~~ ~~ class LegofyGuiMainFrame ( tk . Frame ) : \n 
self . chosenFile = None \n 
self . chosenFilePath = tk . StringVar ( ) \n 
self . pathField = tk . Entry ( self , width = 40 , textvariable = self . chosenFilePath , state = tk . DISABLED self . pathField . grid ( row = 0 , column = 0 , padx = 10 ) \n 
self . selectFile . grid ( row = 0 , column = 1 ) \n 
self . groupFrame = tk . LabelFrame ( self , text = "Params" , padx = 5 , pady = 5 ) \n 
self . groupFrame . grid ( row = 1 , column = 0 , columnspan = 2 , ) \n 
self . colorPaletteLabel = tk . Label ( self . groupFrame , text = ) \n 
self . colorPaletteLabel . grid ( row = 0 , column = 0 ) \n 
self . colorPalette = ttk . Combobox ( self . groupFrame ) \n 
self . colorPalette [ ] = LEGO_PALETTE \n 
self . colorPalette . current ( 0 ) \n 
self . colorPalette . grid ( row = 0 , column = 1 ) \n 
self . brickNumberScale = tk . Scale ( self . groupFrame , from_ = 1 , to = 200 , orient = tk . HORIZONTAL , label self . brickNumberScale . set ( 30 ) \n 
self . brickNumberScale . grid ( row = 1 , column = 0 , columnspan = 2 , ) \n 
self . convertFile . grid ( row = 2 , column = 0 , columnspan = 2 ) \n 
~~ def choose_a_file ( self ) : \n 
~~~ options = { } \n 
options [ ] = \n 
options [ ] = [ ( , ) , \n 
( , ) , \n 
( , ) , ] \n 
options [ ] = os . path . realpath ( "\\\\" ) \n 
options [ ] = self \n 
self . chosenFile = filedialog . askopenfile ( mode = , ** options ) \n 
if self . chosenFile : \n 
~~~ self . chosenFilePath . set ( self . chosenFile . name ) \n 
~~ ~~ def convert_file ( self ) : \n 
~~~ if self . chosenFile is not None : \n 
~~~ palette = self . colorPalette . get ( ) \n 
if palette in LEGO_PALETTE and palette != : \n 
~~~ legofy . main ( self . chosenFile . name , size = self . brickNumberScale . get ( ) , palette_mode ~~ else : \n 
~~~ legofy . main ( self . chosenFile . name , size = self . brickNumberScale . get ( ) ) \n 
~~~ tkmsg . showerror ( "Error" , str ( e ) ) \n 
~~ ~~ ~~ if __name__ == : \n 
~~~ app = LegofyGui ( ) \n 
app . mainloop ( ) \n 
~~ from distutils . core import setup \n 
from condent import __version__ \n 
with open ( "README.rst" ) as readme : \n 
~~~ long_description = readme . read ( ) \n 
~~ classifiers = [ \n 
setup ( \n 
name = "condent" , \n 
version = __version__ , \n 
py_modules = [ "condent" ] , \n 
scripts = [ "bin/condent" ] , \n 
author_email = "Julian@GrayVines.com" , \n 
classifiers = classifiers , \n 
license = "MIT/X" , \n 
long_description = long_description , \n 
url = "http://github.com/Julian/condent" , \n 
from pyvi import window \n 
from pyvi . modes import normal \n 
class Editor ( object ) : \n 
~~~ _command = None \n 
active_tab = None \n 
def __init__ ( self , tabs = None , config = None , normal = normal ) : \n 
~~~ self . config = config \n 
self . mode = self . normal = normal \n 
self . count = None \n 
if tabs is None : \n 
~~~ tabs = self . tabs = [ window . Tab ( self ) ] \n 
~~~ tabs = self . tabs = list ( tabs ) \n 
~~ if tabs : \n 
~~~ self . active_tab = tabs [ 0 ] \n 
def active_window ( self ) : \n 
~~~ return self . active_tab . active_window \n 
~~ def keypress ( self , keys ) : \n 
~~~ return self . mode . keypress ( self , keys ) \n 
~~ ~~ from collections import deque \n 
from contextlib import contextmanager \n 
from jsonschema import FormatChecker , ValidationError \n 
from jsonschema . tests . compat import mock , unittest \n 
from jsonschema . validators import ( \n 
RefResolutionError , UnknownType , Draft3Validator , \n 
Draft4Validator , RefResolver , create , extend , validator_for , validate , \n 
class TestCreateAndExtend ( unittest . TestCase ) : \n 
~~~ self . meta_schema = { u"properties" : { u"smelly" : { } } } \n 
self . smelly = mock . MagicMock ( ) \n 
self . validators = { u"smelly" : self . smelly } \n 
self . types = { u"dict" : dict } \n 
self . Validator = create ( \n 
meta_schema = self . meta_schema , \n 
validators = self . validators , \n 
default_types = self . types , \n 
self . validator_value = 12 \n 
self . schema = { u"smelly" : self . validator_value } \n 
self . validator = self . Validator ( self . schema ) \n 
~~ def test_attrs ( self ) : \n 
~~~ self . assertEqual ( self . Validator . VALIDATORS , self . validators ) \n 
self . assertEqual ( self . Validator . META_SCHEMA , self . meta_schema ) \n 
self . assertEqual ( self . Validator . DEFAULT_TYPES , self . types ) \n 
~~ def test_init ( self ) : \n 
~~~ self . assertEqual ( self . validator . schema , self . schema ) \n 
~~ def test_iter_errors ( self ) : \n 
~~~ instance = "hello" \n 
self . smelly . return_value = [ ] \n 
self . assertEqual ( list ( self . validator . iter_errors ( instance ) ) , [ ] ) \n 
error = mock . Mock ( ) \n 
self . smelly . return_value = [ error ] \n 
self . assertEqual ( list ( self . validator . iter_errors ( instance ) ) , [ error ] ) \n 
self . smelly . assert_called_with ( \n 
self . validator , self . validator_value , instance , self . schema , \n 
~~ def test_if_a_version_is_provided_it_is_registered ( self ) : \n 
~~~ with mock . patch ( "jsonschema.validators.validates" ) as validates : \n 
~~~ validates . side_effect = lambda version : lambda cls : cls \n 
self . assertEqual ( Validator . __name__ , "MyVersionValidator" ) \n 
~~ def test_if_a_version_is_not_provided_it_is_not_registered ( self ) : \n 
~~~ create ( meta_schema = { u"id" : "id" } ) \n 
~~ self . assertFalse ( validates . called ) \n 
~~ def test_extend ( self ) : \n 
~~~ validators = dict ( self . Validator . VALIDATORS ) \n 
new = mock . Mock ( ) \n 
self . assertEqual ( Extended . VALIDATORS , validators ) \n 
self . assertEqual ( Extended . META_SCHEMA , self . Validator . META_SCHEMA ) \n 
self . assertEqual ( Extended . DEFAULT_TYPES , self . Validator . DEFAULT_TYPES ) \n 
~~ ~~ class TestIterErrors ( unittest . TestCase ) : \n 
~~~ self . validator = Draft3Validator ( { } ) \n 
~~~ instance = [ 1 , 2 ] \n 
schema = { \n 
u"disallow" : u"array" , \n 
u"enum" : [ [ "a" , "b" , "c" ] , [ "d" , "e" , "f" ] ] , \n 
u"minItems" : 3 \n 
got = ( e . message for e in self . validator . iter_errors ( instance , schema ) ) \n 
expected = [ \n 
self . assertEqual ( sorted ( got ) , sorted ( expected ) ) \n 
~~ def test_iter_errors_multiple_failures_one_validator ( self ) : \n 
~~~ instance = { "foo" : 2 , "bar" : [ 1 ] , "baz" : 15 , "quux" : "spam" } \n 
u"properties" : { \n 
"foo" : { u"type" : "string" } , \n 
"bar" : { u"minItems" : 2 } , \n 
"baz" : { u"maximum" : 10 , u"enum" : [ 2 , 4 , 6 , 8 ] } , \n 
errors = list ( self . validator . iter_errors ( instance , schema ) ) \n 
self . assertEqual ( len ( errors ) , 4 ) \n 
~~ ~~ class TestValidationErrorMessages ( unittest . TestCase ) : \n 
~~~ def message_for ( self , instance , schema , * args , ** kwargs ) : \n 
~~~ kwargs . setdefault ( "cls" , Draft3Validator ) \n 
with self . assertRaises ( ValidationError ) as e : \n 
~~~ validate ( instance , schema , * args , ** kwargs ) \n 
~~ return e . exception . message \n 
~~ def test_single_type_failure ( self ) : \n 
~~~ message = self . message_for ( instance = 1 , schema = { u"type" : u"string" } ) \n 
~~ def test_single_type_list_failure ( self ) : \n 
~~~ message = self . message_for ( instance = 1 , schema = { u"type" : [ u"string" ] } ) \n 
~~ def test_multiple_type_failure ( self ) : \n 
~~~ types = u"string" , u"object" \n 
message = self . message_for ( instance = 1 , schema = { u"type" : list ( types ) } ) \n 
~~ def test_object_without_title_type_failure ( self ) : \n 
~~~ type = { u"type" : [ { u"minimum" : 3 } ] } \n 
message = self . message_for ( instance = 1 , schema = { u"type" : [ type ] } ) \n 
~~ def test_object_with_name_type_failure ( self ) : \n 
~~~ name = "Foo" \n 
schema = { u"type" : [ { u"name" : name , u"minimum" : 3 } ] } \n 
message = self . message_for ( instance = 1 , schema = schema ) \n 
~~ def test_minimum ( self ) : \n 
~~~ message = self . message_for ( instance = 1 , schema = { "minimum" : 2 } ) \n 
~~ def test_maximum ( self ) : \n 
~~~ message = self . message_for ( instance = 1 , schema = { "maximum" : 0 } ) \n 
~~ def test_dependencies_failure_has_single_element_not_list ( self ) : \n 
~~~ depend , on = "bar" , "foo" \n 
schema = { u"dependencies" : { depend : on } } \n 
message = self . message_for ( { "bar" : 2 } , schema ) \n 
~~ def test_additionalItems_single_failure ( self ) : \n 
~~~ message = self . message_for ( \n 
[ 2 ] , { u"items" : [ ] , u"additionalItems" : False } , \n 
~~ def test_additionalItems_multiple_failures ( self ) : \n 
[ 1 , 2 , 3 ] , { u"items" : [ ] , u"additionalItems" : False } \n 
~~ def test_additionalProperties_single_failure ( self ) : \n 
~~~ additional = "foo" \n 
schema = { u"additionalProperties" : False } \n 
message = self . message_for ( { additional : 2 } , schema ) \n 
~~ def test_additionalProperties_multiple_failures ( self ) : \n 
~~~ schema = { u"additionalProperties" : False } \n 
message = self . message_for ( dict . fromkeys ( [ "foo" , "bar" ] ) , schema ) \n 
self . assertIn ( repr ( "foo" ) , message ) \n 
self . assertIn ( repr ( "bar" ) , message ) \n 
~~ def test_invalid_format_default_message ( self ) : \n 
~~~ checker = FormatChecker ( formats = ( ) ) \n 
check_fn = mock . Mock ( return_value = False ) \n 
checker . checks ( u"thing" ) ( check_fn ) \n 
schema = { u"format" : u"thing" } \n 
message = self . message_for ( "bla" , schema , format_checker = checker ) \n 
self . assertIn ( repr ( "bla" ) , message ) \n 
self . assertIn ( repr ( "thing" ) , message ) \n 
~~ ~~ class TestValidationErrorDetails ( unittest . TestCase ) : \n 
~~~ def test_anyOf ( self ) : \n 
~~~ instance = 5 \n 
"anyOf" : [ \n 
{ "minimum" : 20 } , \n 
{ "type" : "string" } \n 
validator = Draft4Validator ( schema ) \n 
errors = list ( validator . iter_errors ( instance ) ) \n 
self . assertEqual ( len ( errors ) , 1 ) \n 
e = errors [ 0 ] \n 
self . assertEqual ( e . validator , "anyOf" ) \n 
self . assertEqual ( e . validator_value , schema [ "anyOf" ] ) \n 
self . assertEqual ( e . instance , instance ) \n 
self . assertEqual ( e . schema , schema ) \n 
self . assertIsNone ( e . parent ) \n 
self . assertEqual ( e . path , deque ( [ ] ) ) \n 
self . assertEqual ( e . relative_path , deque ( [ ] ) ) \n 
self . assertEqual ( e . absolute_path , deque ( [ ] ) ) \n 
self . assertEqual ( e . schema_path , deque ( [ "anyOf" ] ) ) \n 
self . assertEqual ( e . relative_schema_path , deque ( [ "anyOf" ] ) ) \n 
self . assertEqual ( e . absolute_schema_path , deque ( [ "anyOf" ] ) ) \n 
self . assertEqual ( len ( e . context ) , 2 ) \n 
e1 , e2 = sorted_errors ( e . context ) \n 
self . assertEqual ( e1 . validator , "minimum" ) \n 
self . assertEqual ( e1 . validator_value , schema [ "anyOf" ] [ 0 ] [ "minimum" ] ) \n 
self . assertEqual ( e1 . instance , instance ) \n 
self . assertEqual ( e1 . schema , schema [ "anyOf" ] [ 0 ] ) \n 
self . assertIs ( e1 . parent , e ) \n 
self . assertEqual ( e1 . path , deque ( [ ] ) ) \n 
self . assertEqual ( e1 . absolute_path , deque ( [ ] ) ) \n 
self . assertEqual ( e1 . relative_path , deque ( [ ] ) ) \n 
self . assertEqual ( e1 . schema_path , deque ( [ 0 , "minimum" ] ) ) \n 
self . assertEqual ( e1 . relative_schema_path , deque ( [ 0 , "minimum" ] ) ) \n 
e1 . absolute_schema_path , deque ( [ "anyOf" , 0 , "minimum" ] ) , \n 
self . assertFalse ( e1 . context ) \n 
self . assertEqual ( e2 . validator , "type" ) \n 
self . assertEqual ( e2 . validator_value , schema [ "anyOf" ] [ 1 ] [ "type" ] ) \n 
self . assertEqual ( e2 . instance , instance ) \n 
self . assertEqual ( e2 . schema , schema [ "anyOf" ] [ 1 ] ) \n 
self . assertIs ( e2 . parent , e ) \n 
self . assertEqual ( e2 . path , deque ( [ ] ) ) \n 
self . assertEqual ( e2 . relative_path , deque ( [ ] ) ) \n 
self . assertEqual ( e2 . absolute_path , deque ( [ ] ) ) \n 
self . assertEqual ( e2 . schema_path , deque ( [ 1 , "type" ] ) ) \n 
self . assertEqual ( e2 . relative_schema_path , deque ( [ 1 , "type" ] ) ) \n 
self . assertEqual ( e2 . absolute_schema_path , deque ( [ "anyOf" , 1 , "type" ] ) ) \n 
self . assertEqual ( len ( e2 . context ) , 0 ) \n 
~~ def test_type ( self ) : \n 
~~~ instance = { "foo" : 1 } \n 
"type" : [ \n 
{ "type" : "integer" } , \n 
"type" : "object" , \n 
"properties" : { \n 
"foo" : { "enum" : [ 2 ] } \n 
validator = Draft3Validator ( schema ) \n 
self . assertEqual ( e . validator , "type" ) \n 
self . assertEqual ( e . validator_value , schema [ "type" ] ) \n 
self . assertEqual ( e . schema_path , deque ( [ "type" ] ) ) \n 
self . assertEqual ( e . relative_schema_path , deque ( [ "type" ] ) ) \n 
self . assertEqual ( e . absolute_schema_path , deque ( [ "type" ] ) ) \n 
self . assertEqual ( e1 . validator , "type" ) \n 
self . assertEqual ( e1 . validator_value , schema [ "type" ] [ 0 ] [ "type" ] ) \n 
self . assertEqual ( e1 . schema , schema [ "type" ] [ 0 ] ) \n 
self . assertEqual ( e1 . schema_path , deque ( [ 0 , "type" ] ) ) \n 
self . assertEqual ( e1 . relative_schema_path , deque ( [ 0 , "type" ] ) ) \n 
self . assertEqual ( e1 . absolute_schema_path , deque ( [ "type" , 0 , "type" ] ) ) \n 
self . assertEqual ( e2 . validator , "enum" ) \n 
self . assertEqual ( e2 . validator_value , [ 2 ] ) \n 
self . assertEqual ( e2 . instance , 1 ) \n 
self . assertEqual ( e2 . schema , { u"enum" : [ 2 ] } ) \n 
self . assertEqual ( e2 . path , deque ( [ "foo" ] ) ) \n 
self . assertEqual ( e2 . relative_path , deque ( [ "foo" ] ) ) \n 
self . assertEqual ( e2 . absolute_path , deque ( [ "foo" ] ) ) \n 
e2 . schema_path , deque ( [ 1 , "properties" , "foo" , "enum" ] ) , \n 
e2 . relative_schema_path , deque ( [ 1 , "properties" , "foo" , "enum" ] ) , \n 
e2 . absolute_schema_path , \n 
deque ( [ "type" , 1 , "properties" , "foo" , "enum" ] ) , \n 
self . assertFalse ( e2 . context ) \n 
~~ def test_single_nesting ( self ) : \n 
"foo" : { "type" : "string" } , \n 
"bar" : { "minItems" : 2 } , \n 
"baz" : { "maximum" : 10 , "enum" : [ 2 , 4 , 6 , 8 ] } , \n 
errors = validator . iter_errors ( instance ) \n 
e1 , e2 , e3 , e4 = sorted_errors ( errors ) \n 
self . assertEqual ( e1 . path , deque ( [ "bar" ] ) ) \n 
self . assertEqual ( e2 . path , deque ( [ "baz" ] ) ) \n 
self . assertEqual ( e3 . path , deque ( [ "baz" ] ) ) \n 
self . assertEqual ( e4 . path , deque ( [ "foo" ] ) ) \n 
self . assertEqual ( e1 . relative_path , deque ( [ "bar" ] ) ) \n 
self . assertEqual ( e2 . relative_path , deque ( [ "baz" ] ) ) \n 
self . assertEqual ( e3 . relative_path , deque ( [ "baz" ] ) ) \n 
self . assertEqual ( e4 . relative_path , deque ( [ "foo" ] ) ) \n 
self . assertEqual ( e1 . absolute_path , deque ( [ "bar" ] ) ) \n 
self . assertEqual ( e2 . absolute_path , deque ( [ "baz" ] ) ) \n 
self . assertEqual ( e3 . absolute_path , deque ( [ "baz" ] ) ) \n 
self . assertEqual ( e4 . absolute_path , deque ( [ "foo" ] ) ) \n 
self . assertEqual ( e1 . validator , "minItems" ) \n 
self . assertEqual ( e3 . validator , "maximum" ) \n 
self . assertEqual ( e4 . validator , "type" ) \n 
~~ def test_multiple_nesting ( self ) : \n 
~~~ instance = [ 1 , { "foo" : 2 , "bar" : { "baz" : [ 1 ] } } , "quux" ] \n 
"type" : "string" , \n 
"items" : { \n 
"type" : [ "string" , "object" ] , \n 
"foo" : { "enum" : [ 1 , 3 ] } , \n 
"bar" : { \n 
"type" : "array" , \n 
"bar" : { "required" : True } , \n 
"baz" : { "minItems" : 2 } , \n 
e1 , e2 , e3 , e4 , e5 , e6 = sorted_errors ( errors ) \n 
self . assertEqual ( e2 . path , deque ( [ 0 ] ) ) \n 
self . assertEqual ( e3 . path , deque ( [ 1 , "bar" ] ) ) \n 
self . assertEqual ( e4 . path , deque ( [ 1 , "bar" , "bar" ] ) ) \n 
self . assertEqual ( e5 . path , deque ( [ 1 , "bar" , "baz" ] ) ) \n 
self . assertEqual ( e6 . path , deque ( [ 1 , "foo" ] ) ) \n 
self . assertEqual ( e1 . schema_path , deque ( [ "type" ] ) ) \n 
self . assertEqual ( e2 . schema_path , deque ( [ "items" , "type" ] ) ) \n 
list ( e3 . schema_path ) , [ "items" , "properties" , "bar" , "type" ] , \n 
list ( e4 . schema_path ) , \n 
[ "items" , "properties" , "bar" , "properties" , "bar" , "required" ] , \n 
list ( e5 . schema_path ) , \n 
[ "items" , "properties" , "bar" , "properties" , "baz" , "minItems" ] \n 
list ( e6 . schema_path ) , [ "items" , "properties" , "foo" , "enum" ] , \n 
self . assertEqual ( e3 . validator , "type" ) \n 
self . assertEqual ( e4 . validator , "required" ) \n 
self . assertEqual ( e5 . validator , "minItems" ) \n 
self . assertEqual ( e6 . validator , "enum" ) \n 
~~ def test_recursive ( self ) : \n 
~~~ schema = { \n 
"definitions" : { \n 
"node" : { \n 
"anyOf" : [ { \n 
"required" : [ "name" , "children" ] , \n 
"name" : { \n 
"children" : { \n 
"patternProperties" : { \n 
"^.*$" : { \n 
"$ref" : "#/definitions/node" , \n 
} ] , \n 
"required" : [ "root" ] , \n 
"root" : { "$ref" : "#/definitions/node" } , \n 
instance = { \n 
"root" : { \n 
"name" : "root" , \n 
"a" : { \n 
"name" : "a" , \n 
"ab" : { \n 
"name" : "ab" , \n 
e , = validator . iter_errors ( instance ) \n 
self . assertEqual ( e . absolute_path , deque ( [ "root" ] ) ) \n 
e . absolute_schema_path , deque ( [ "properties" , "root" , "anyOf" ] ) , \n 
e1 , = e . context \n 
self . assertEqual ( e1 . absolute_path , deque ( [ "root" , "children" , "a" ] ) ) \n 
e1 . absolute_schema_path , deque ( \n 
"properties" , \n 
"root" , \n 
"anyOf" , \n 
0 , \n 
"children" , \n 
"patternProperties" , \n 
"^.*$" , \n 
e2 , = e1 . context \n 
e2 . absolute_path , deque ( \n 
[ "root" , "children" , "a" , "children" , "ab" ] , \n 
e2 . absolute_schema_path , deque ( \n 
"anyOf" \n 
~~ def test_additionalProperties ( self ) : \n 
~~~ instance = { "bar" : "bar" , "foo" : 2 } \n 
"additionalProperties" : { "type" : "integer" , "minimum" : 5 } \n 
e1 , e2 = sorted_errors ( errors ) \n 
self . assertEqual ( e2 . validator , "minimum" ) \n 
~~ def test_patternProperties ( self ) : \n 
~~~ instance = { "bar" : 1 , "foo" : 2 } \n 
"bar" : { "type" : "string" } , \n 
"foo" : { "minimum" : 5 } \n 
~~ def test_additionalItems ( self ) : \n 
~~~ instance = [ "foo" , 1 ] \n 
"items" : [ ] , \n 
"additionalItems" : { "type" : "integer" , "minimum" : 5 } \n 
self . assertEqual ( e1 . path , deque ( [ 0 ] ) ) \n 
self . assertEqual ( e2 . path , deque ( [ 1 ] ) ) \n 
~~ def test_additionalItems_with_items ( self ) : \n 
~~~ instance = [ "foo" , "bar" , 1 ] \n 
"items" : [ { } ] , \n 
self . assertEqual ( e1 . path , deque ( [ 1 ] ) ) \n 
self . assertEqual ( e2 . path , deque ( [ 2 ] ) ) \n 
~~ ~~ class ValidatorTestMixin ( object ) : \n 
~~~ self . instance = mock . Mock ( ) \n 
self . schema = { } \n 
self . resolver = mock . Mock ( ) \n 
self . validator = self . validator_class ( self . schema ) \n 
~~ def test_valid_instances_are_valid ( self ) : \n 
~~~ errors = iter ( [ ] ) \n 
with mock . patch . object ( \n 
self . validator , "iter_errors" , return_value = errors , \n 
~~~ self . assertTrue ( \n 
self . validator . is_valid ( self . instance , self . schema ) \n 
~~ ~~ def test_invalid_instances_are_not_valid ( self ) : \n 
~~~ errors = iter ( [ mock . Mock ( ) ] ) \n 
~~~ self . assertFalse ( \n 
~~ ~~ def test_non_existent_properties_are_ignored ( self ) : \n 
~~~ instance , my_property , my_value = mock . Mock ( ) , mock . Mock ( ) , mock . Mock ( ) \n 
validate ( instance = instance , schema = { my_property : my_value } ) \n 
~~ def test_it_creates_a_ref_resolver_if_not_provided ( self ) : \n 
~~~ self . assertIsInstance ( self . validator . resolver , RefResolver ) \n 
~~ def test_it_delegates_to_a_ref_resolver ( self ) : \n 
~~~ resolver = RefResolver ( "" , { } ) \n 
schema = { "$ref" : mock . Mock ( ) } \n 
with mock . patch . object ( resolver , "resolve" ) as resolve : \n 
~~~ resolve . return_value = "url" , { "type" : "integer" } \n 
with self . assertRaises ( ValidationError ) : \n 
~~~ self . validator_class ( schema , resolver = resolver ) . validate ( None ) \n 
~~ ~~ resolve . assert_called_once_with ( schema [ "$ref" ] ) \n 
~~ def test_it_delegates_to_a_legacy_ref_resolver ( self ) : \n 
class LegacyRefResolver ( object ) : \n 
~~~ @ contextmanager \n 
def resolving ( this , ref ) : \n 
yield { "type" : "integer" } \n 
~~ ~~ resolver = LegacyRefResolver ( ) \n 
~~ ~~ def test_is_type_is_true_for_valid_type ( self ) : \n 
~~~ self . assertTrue ( self . validator . is_type ( "foo" , "string" ) ) \n 
~~ def test_is_type_is_false_for_invalid_type ( self ) : \n 
~~~ self . assertFalse ( self . validator . is_type ( "foo" , "array" ) ) \n 
~~ def test_is_type_evades_bool_inheriting_from_int ( self ) : \n 
~~~ self . assertFalse ( self . validator . is_type ( True , "integer" ) ) \n 
self . assertFalse ( self . validator . is_type ( True , "number" ) ) \n 
~~ def test_is_type_raises_exception_for_unknown_type ( self ) : \n 
~~~ with self . assertRaises ( UnknownType ) : \n 
~~~ self . validator . is_type ( "foo" , object ( ) ) \n 
~~ ~~ ~~ class TestDraft3Validator ( ValidatorTestMixin , unittest . TestCase ) : \n 
~~~ validator_class = Draft3Validator \n 
def test_is_type_is_true_for_any_type ( self ) : \n 
~~~ self . assertTrue ( self . validator . is_valid ( mock . Mock ( ) , { "type" : "any" } ) ) \n 
~~ def test_is_type_does_not_evade_bool_if_it_is_being_tested ( self ) : \n 
~~~ self . assertTrue ( self . validator . is_type ( True , "boolean" ) ) \n 
self . assertTrue ( self . validator . is_valid ( True , { "type" : "any" } ) ) \n 
~~ def test_non_string_custom_types ( self ) : \n 
~~~ schema = { : [ None ] } \n 
cls = self . validator_class ( schema , types = { None : type ( None ) } ) \n 
cls . validate ( None , schema ) \n 
~~ ~~ class TestDraft4Validator ( ValidatorTestMixin , unittest . TestCase ) : \n 
~~~ validator_class = Draft4Validator \n 
~~ class TestBuiltinFormats ( unittest . TestCase ) : \n 
~~ for format in FormatChecker . checkers : \n 
~~~ def test ( self , format = format ) : \n 
~~~ v = Draft4Validator ( { "format" : format } , format_checker = FormatChecker ( ) ) \n 
v . validate ( 123 ) \n 
~~ name = "test_{0}_ignores_non_strings" . format ( format ) \n 
test . __name__ = name \n 
setattr ( TestBuiltinFormats , name , test ) \n 
~~ class TestValidatorFor ( unittest . TestCase ) : \n 
~~~ def test_draft_3 ( self ) : \n 
~~~ schema = { "$schema" : "http://json-schema.org/draft-03/schema" } \n 
self . assertIs ( validator_for ( schema ) , Draft3Validator ) \n 
schema = { "$schema" : "http://json-schema.org/draft-03/schema#" } \n 
~~ def test_draft_4 ( self ) : \n 
~~~ schema = { "$schema" : "http://json-schema.org/draft-04/schema" } \n 
self . assertIs ( validator_for ( schema ) , Draft4Validator ) \n 
schema = { "$schema" : "http://json-schema.org/draft-04/schema#" } \n 
~~ def test_custom_validator ( self ) : \n 
self . assertIs ( validator_for ( schema ) , Validator ) \n 
~~ def test_validator_for_jsonschema_default ( self ) : \n 
~~~ self . assertIs ( validator_for ( { } ) , Draft4Validator ) \n 
~~ def test_validator_for_custom_default ( self ) : \n 
~~~ self . assertIs ( validator_for ( { } , default = None ) , None ) \n 
~~ ~~ class TestValidate ( unittest . TestCase ) : \n 
~~~ def test_draft3_validator_is_chosen ( self ) : \n 
~~~ schema = { "$schema" : "http://json-schema.org/draft-03/schema#" } \n 
with mock . patch . object ( Draft3Validator , "check_schema" ) as chk_schema : \n 
~~~ validate ( { } , schema ) \n 
chk_schema . assert_called_once_with ( schema ) \n 
~~ schema = { "$schema" : "http://json-schema.org/draft-03/schema" } \n 
~~ ~~ def test_draft4_validator_is_chosen ( self ) : \n 
~~~ schema = { "$schema" : "http://json-schema.org/draft-04/schema#" } \n 
with mock . patch . object ( Draft4Validator , "check_schema" ) as chk_schema : \n 
~~ ~~ def test_draft4_validator_is_the_default ( self ) : \n 
~~~ with mock . patch . object ( Draft4Validator , "check_schema" ) as chk_schema : \n 
~~~ validate ( { } , { } ) \n 
chk_schema . assert_called_once_with ( { } ) \n 
~~ ~~ ~~ class TestRefResolver ( unittest . TestCase ) : \n 
~~~ base_uri = "" \n 
stored_uri = "foo://stored" \n 
stored_schema = { "stored" : "schema" } \n 
~~~ self . referrer = { } \n 
self . store = { self . stored_uri : self . stored_schema } \n 
self . resolver = RefResolver ( self . base_uri , self . referrer , self . store ) \n 
~~ def test_it_does_not_retrieve_schema_urls_from_the_network ( self ) : \n 
~~~ ref = Draft3Validator . META_SCHEMA [ "id" ] \n 
with mock . patch . object ( self . resolver , "resolve_remote" ) as remote : \n 
~~~ with self . resolver . resolving ( ref ) as resolved : \n 
~~~ self . assertEqual ( resolved , Draft3Validator . META_SCHEMA ) \n 
~~ ~~ self . assertFalse ( remote . called ) \n 
~~ def test_it_resolves_local_refs ( self ) : \n 
~~~ ref = "#/properties/foo" \n 
self . referrer [ "properties" ] = { "foo" : object ( ) } \n 
with self . resolver . resolving ( ref ) as resolved : \n 
~~~ self . assertEqual ( resolved , self . referrer [ "properties" ] [ "foo" ] ) \n 
~~ ~~ def test_it_resolves_local_refs_with_id ( self ) : \n 
~~~ schema = { "id" : "http://bar/schema#" , "a" : { "foo" : "bar" } } \n 
resolver = RefResolver . from_schema ( schema ) \n 
with resolver . resolving ( "#/a" ) as resolved : \n 
~~~ self . assertEqual ( resolved , schema [ "a" ] ) \n 
~~ with resolver . resolving ( "http://bar/schema#/a" ) as resolved : \n 
~~ ~~ def test_it_retrieves_stored_refs ( self ) : \n 
~~~ with self . resolver . resolving ( self . stored_uri ) as resolved : \n 
~~~ self . assertIs ( resolved , self . stored_schema ) \n 
~~ self . resolver . store [ "cached_ref" ] = { "foo" : 12 } \n 
with self . resolver . resolving ( "cached_ref#/foo" ) as resolved : \n 
~~~ self . assertEqual ( resolved , 12 ) \n 
~~ ~~ def test_it_retrieves_unstored_refs_via_requests ( self ) : \n 
~~~ ref = "http://bar#baz" \n 
schema = { "baz" : 12 } \n 
with mock . patch ( "jsonschema.validators.requests" ) as requests : \n 
~~~ requests . get . return_value . json . return_value = schema \n 
~~ ~~ requests . get . assert_called_once_with ( "http://bar" ) \n 
~~ def test_it_retrieves_unstored_refs_via_urlopen ( self ) : \n 
with mock . patch ( "jsonschema.validators.requests" , None ) : \n 
~~~ with mock . patch ( "jsonschema.validators.urlopen" ) as urlopen : \n 
~~~ urlopen . return_value . read . return_value = ( \n 
json . dumps ( schema ) . encode ( "utf8" ) ) \n 
~~ ~~ ~~ urlopen . assert_called_once_with ( "http://bar" ) \n 
~~ def test_it_can_construct_a_base_uri_from_a_schema ( self ) : \n 
~~~ schema = { "id" : "foo" } \n 
self . assertEqual ( resolver . base_uri , "foo" ) \n 
self . assertEqual ( resolver . resolution_scope , "foo" ) \n 
with resolver . resolving ( "" ) as resolved : \n 
~~~ self . assertEqual ( resolved , schema ) \n 
~~ with resolver . resolving ( "#" ) as resolved : \n 
~~ with resolver . resolving ( "foo" ) as resolved : \n 
~~ with resolver . resolving ( "foo#" ) as resolved : \n 
~~ ~~ def test_it_can_construct_a_base_uri_from_a_schema_without_id ( self ) : \n 
~~~ schema = { } \n 
self . assertEqual ( resolver . base_uri , "" ) \n 
self . assertEqual ( resolver . resolution_scope , "" ) \n 
~~ ~~ def test_custom_uri_scheme_handlers ( self ) : \n 
~~~ schema = { "foo" : "bar" } \n 
ref = "foo://bar" \n 
foo_handler = mock . Mock ( return_value = schema ) \n 
resolver = RefResolver ( "" , { } , handlers = { "foo" : foo_handler } ) \n 
with resolver . resolving ( ref ) as resolved : \n 
~~ foo_handler . assert_called_once_with ( ref ) \n 
~~ def test_cache_remote_on ( self ) : \n 
~~~ ref = "foo://bar" \n 
foo_handler = mock . Mock ( ) \n 
resolver = RefResolver ( \n 
"" , { } , cache_remote = True , handlers = { "foo" : foo_handler } , \n 
with resolver . resolving ( ref ) : \n 
~~ with resolver . resolving ( ref ) : \n 
~~ def test_cache_remote_off ( self ) : \n 
"" , { } , cache_remote = False , handlers = { "foo" : foo_handler } , \n 
~~ self . assertEqual ( foo_handler . call_count , 1 ) \n 
~~ def test_if_you_give_it_junk_you_get_a_resolution_error ( self ) : \n 
with self . assertRaises ( RefResolutionError ) as err : \n 
~~~ with resolver . resolving ( ref ) : \n 
~~ def test_helpful_error_message_on_failed_pop_scope ( self ) : \n 
resolver . pop_scope ( ) \n 
with self . assertRaises ( RefResolutionError ) as exc : \n 
~~~ resolver . pop_scope ( ) \n 
~~ ~~ class UniqueTupleItemsMixin ( object ) : \n 
def test_it_properly_formats_an_error_message ( self ) : \n 
~~~ validator = self . validator_class ( \n 
schema = { "uniqueItems" : True } , \n 
types = { "array" : ( tuple , ) } , \n 
~~~ validator . validate ( ( 1 , 1 ) ) \n 
~~ ~~ class TestDraft4UniqueTupleItems ( UniqueTupleItemsMixin , unittest . TestCase ) : \n 
~~ class TestDraft3UniqueTupleItems ( UniqueTupleItemsMixin , unittest . TestCase ) : \n 
~~ def sorted_errors ( errors ) : \n 
~~~ def key ( error ) : \n 
~~~ return ( \n 
[ str ( e ) for e in error . path ] , \n 
[ str ( e ) for e in error . schema_path ] \n 
~~ return sorted ( errors , key = key ) \n 
~~ \n 
from jnpr . openclos . report import ResourceAllocationReport , L2Report , L3Report \n 
from test_dao import InMemoryDao \n 
class Test ( unittest . TestCase ) : \n 
self . __conf = { } \n 
self . __conf [ ] = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ ) ) , ) \n 
self . __conf [ ] = \n 
self . __conf [ ] = { \n 
self . __conf [ ] = { : , : [ , , ] } \n 
"qfx5100-24q-2p" : { \n 
"ports" : \n 
"qfx5100-48s-6q" : { \n 
"uplinkPorts" : , \n 
"downlinkPorts" : \n 
self . _dao = InMemoryDao . getInstance ( ) \n 
~~~ self . _dao = None \n 
InMemoryDao . _destroy ( ) \n 
def testGenerateL2Report ( self ) : \n 
~~~ l2Report = L2Report ( self . __conf , self . _dao ) \n 
from test_model import createPod \n 
with self . _dao . getReadSession ( ) as session : \n 
~~~ pod = createPod ( "test" , session ) \n 
l2Report . generateReport ( pod . id , True , False ) \n 
~~ ~~ def testGenerateL3Report ( self ) : \n 
~~~ l3Report = L3Report ( self . __conf , self . _dao ) \n 
l3Report . generateReport ( pod . id , True , False ) \n 
~~ ~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) import yaml \n 
~~ import os . path \n 
from jnpr . junos . factory . factory_loader import FactoryLoader \n 
__all__ = [ , ] \n 
def loadyaml ( path ) : \n 
if os . path . splitext ( path ) [ 1 ] == : \n 
~~~ path += \n 
~~ return FactoryLoader ( ) . load ( yaml . load ( open ( path , ) ) ) \n 
from jnpr . junos . factory import loadyaml \n 
from os . path import splitext \n 
_YAML_ = splitext ( __file__ ) [ 0 ] + \n 
globals ( ) . update ( loadyaml ( _YAML_ ) ) \n 
from nose . plugins . attrib import attr \n 
from jnpr . junos import Device \n 
@ attr ( ) \n 
class TestDeviceSsh ( unittest . TestCase ) : \n 
~~~ def tearDown ( self ) : \n 
~~~ self . dev . close ( ) \n 
~~ def test_device_open_default_key ( self ) : \n 
~~~ self . dev = Device ( ) \n 
self . dev . open ( ) \n 
self . assertEqual ( self . dev . connected , True ) \n 
~~ def test_device_open_key_pass ( self ) : \n 
~~~ self . dev = Device ( host = , ssh_private_key_file = self . dev . open ( ) \n 
from jnpr . junos . utils . util import Util \n 
from mock import patch \n 
class TestUtil ( unittest . TestCase ) : \n 
~~~ @ patch ( ) \n 
def setUp ( self , mock_connect ) : \n 
~~~ self . dev = Device ( host = , user = , password = , \n 
gather_facts = False ) \n 
self . util = Util ( self . dev ) \n 
~~ def test_repr ( self ) : \n 
~~~ self . assertEqual ( repr ( self . util ) , ) \n 
~~ def test_dev_setter_exception ( self ) : \n 
~~~ def mod_dev ( ) : \n 
~~~ self . util . dev = \n 
~~ self . assertRaises ( RuntimeError , mod_dev ) \n 
~~ def test_rpc_setter_exception ( self ) : \n 
~~~ def mod_rpc ( ) : \n 
~~~ self . util . rpc = \n 
~~ self . assertRaises ( RuntimeError , mod_rpc ) \n 
from openmdao . main . api import set_as_top , Assembly \n 
from openmdao . util . testutil import assert_rel_error \n 
from openmdao . lib . drivers . api import BroydenSolver \n 
from hyperloop . tube_wall_temp import TubeWallTemp \n 
class TubeHeatBalance ( Assembly ) : \n 
~~~ def configure ( self ) : \n 
~~~ tm = self . add ( , TubeWallTemp ( ) ) \n 
#tm.bearing_air.setTotalTP() \n 
driver = self . add ( , BroydenSolver ( ) ) \n 
driver . add_parameter ( , low = 0. , high = 10000. ) \n 
driver . add_constraint ( ) \n 
driver . workflow . add ( [ ] ) \n 
~~ ~~ class TubeWallTestCase ( unittest . TestCase ) : \n 
~~~ def test_tube_temp ( self ) : \n 
~~~ test = set_as_top ( TubeHeatBalance ( ) ) \n 
test . tm . nozzle_air . setTotalTP ( 1710 , 0.304434211 ) \n 
test . tm . nozzle_air . W = 1.08 \n 
test . tm . bearing_air . W = 0. \n 
test . tm . diameter_outer_tube = 2.22504 test . tm . length_tube = 482803. test . tm . num_pods = 34. test . tm . temp_boundary = 322.361 test . tm . temp_outside_ambient = 305.6 \n 
test . run ( ) \n 
assert_rel_error ( self , test . tm . heat_rate_pod , 353244. , 0.02 ) \n 
assert_rel_error ( self , test . tm . total_heat_rate_pods , 12010290. , 0.02 ) \n 
assert_rel_error ( self , test . tm . GrDelTL3 , 123775609 , 0.02 ) \n 
assert_rel_error ( self , test . tm . Pr , 0.707 , 0.02 ) \n 
assert_rel_error ( self , test . tm . Gr , 23163846280. , 0.02 ) \n 
assert_rel_error ( self , test . tm . Ra , 16369476896. , 0.02 ) \n 
assert_rel_error ( self , test . tm . Nu , 281.6714 , 0.02 ) #http://www.egr.msu.edu/~somerton/Nusselt/ii/ii_a/ii_a_3/ii_a_3_a.html assert_rel_error ( self , test . tm . k , 0.02655 , 0.02 ) \n 
assert_rel_error ( self , test . tm . h , 3.3611 , 0.02 ) \n 
assert_rel_error ( self , test . tm . area_convection , 3374876 , 0.02 ) \n 
assert_rel_error ( self , test . tm . q_per_area_nat_conv , 57.10 , 0.02 ) \n 
assert_rel_error ( self , test . tm . total_q_nat_conv , 192710349 , 0.02 ) \n 
assert_rel_error ( self , test . tm . area_viewing , 1074256. , 0.02 ) \n 
assert_rel_error ( self , test . tm . q_per_area_solar , 350. , 0.02 ) \n 
assert_rel_error ( self , test . tm . q_total_solar , 375989751. , 0.02 ) \n 
assert_rel_error ( self , test . tm . area_rad , 3374876.115 , 0.02 ) \n 
assert_rel_error ( self , test . tm . q_rad_per_area , 59.7 , 0.02 ) \n 
assert_rel_error ( self , test . tm . q_rad_tot , 201533208 , 0.02 ) \n 
assert_rel_error ( self , test . tm . q_total_out , 394673364. , 0.02 ) \n 
from setuptools import setup , find_packages \n 
with open ( os . path . join ( os . path . dirname ( __file__ ) , ) ) as f : \n 
~~~ required = f . read ( ) . splitlines ( ) \n 
~~ setup ( \n 
name = , \n 
version = , \n 
description = , \n 
author = , \n 
author_email = , \n 
packages = find_packages ( exclude = [ , , ] ) , \n 
include_package_data = True , \n 
setup_requires = [ \n 
install_requires = required , \n 
entry_points = { \n 
: [ \n 
del required \n 
import hmac \n 
import base64 \n 
import threading \n 
import requests \n 
from yubico_client . otp import OTP \n 
from yubico_client . yubico_exceptions import ( StatusCodeError , \n 
InvalidClientIdError , \n 
InvalidValidationResponse , \n 
SignatureVerificationError ) \n 
from yubico_client . py3 import b \n 
from yubico_client . py3 import urlencode \n 
from yubico_client . py3 import unquote \n 
COMMON_CA_LOCATIONS = [ \n 
DEFAULT_API_URLS = ( , \n 
DEFAULT_TIMEOUT = 10 \n 
DEFAULT_MAX_TIME_WINDOW = 5 \n 
BAD_STATUS_CODES = [ , , , \n 
class Yubico ( object ) : \n 
~~~ def __init__ ( self , client_id , key = None , verify_cert = True , \n 
translate_otp = True , api_urls = DEFAULT_API_URLS , \n 
ca_certs_bundle_path = None ) : \n 
~~~ if ca_certs_bundle_path and not self . _is_valid_ca_bundle_file ( ca_certs_bundle_path ) : \n 
~~~ raise ValueError ( ( \n 
~~ self . client_id = client_id \n 
if key is not None : \n 
~~~ key = base64 . b64decode ( key . encode ( ) ) \n 
~~ self . key = key \n 
self . verify_cert = verify_cert \n 
self . translate_otp = translate_otp \n 
self . api_urls = self . _init_request_urls ( api_urls = api_urls ) \n 
self . ca_certs_bundle_path = ca_certs_bundle_path \n 
~~ def verify ( self , otp , timestamp = False , sl = None , timeout = None , \n 
return_response = False ) : \n 
ca_bundle_path = self . _get_ca_bundle_path ( ) \n 
otp = OTP ( otp , self . translate_otp ) \n 
rand_str = b ( os . urandom ( 30 ) ) \n 
nonce = base64 . b64encode ( rand_str , b ( ) ) [ : 25 ] . decode ( ) \n 
query_string = self . generate_query_string ( otp . otp , nonce , timestamp , \n 
sl , timeout ) \n 
threads = [ ] \n 
timeout = timeout or DEFAULT_TIMEOUT \n 
for url in self . api_urls : \n 
~~~ thread = URLThread ( % ( url , query_string ) , timeout , \n 
self . verify_cert , ca_bundle_path ) \n 
thread . start ( ) \n 
threads . append ( thread ) \n 
~~ start_time = time . time ( ) \n 
while threads and ( start_time + timeout ) > time . time ( ) : \n 
~~~ for thread in threads : \n 
~~~ if not thread . is_alive ( ) : \n 
~~~ if thread . exception : \n 
~~~ raise thread . exception \n 
~~ elif thread . response : \n 
~~~ status = self . verify_response ( thread . response , \n 
otp . otp , nonce , \n 
return_response ) \n 
if status : \n 
~~~ if return_response : \n 
~~~ return status \n 
~~ ~~ ~~ threads . remove ( thread ) \n 
~~ ~~ time . sleep ( 0.1 ) \n 
~~ raise Exception ( ) \n 
~~ def verify_multi ( self , otp_list , max_time_window = DEFAULT_MAX_TIME_WINDOW , \n 
sl = None , timeout = None ) : \n 
otps = [ ] \n 
for otp in otp_list : \n 
~~~ otps . append ( OTP ( otp , self . translate_otp ) ) \n 
~~ if len ( otp_list ) < 2 : \n 
~~~ raise ValueError ( ) \n 
~~ device_ids = set ( ) \n 
for otp in otps : \n 
~~~ device_ids . add ( otp . device_id ) \n 
~~ if len ( device_ids ) != 1 : \n 
~~ for otp in otps : \n 
~~~ response = self . verify ( otp . otp , True , sl , timeout , \n 
return_response = True ) \n 
if not response : \n 
~~ otp . timestamp = int ( response [ ] ) \n 
~~ count = len ( otps ) \n 
delta = otps [ count - 1 ] . timestamp - otps [ 0 ] . timestamp \n 
delta = delta / 8 \n 
if delta < 0 : \n 
~~~ raise Exception ( \n 
~~ if delta > max_time_window : \n 
~~~ raise Exception ( ( \n 
) % \n 
( max_time_window ) ) \n 
~~ def verify_response ( self , response , otp , nonce , return_response = False ) : \n 
~~~ status = re . search ( , response ) . groups ( ) \n 
if len ( status ) > 1 : \n 
~~~ message = \n 
raise InvalidValidationResponse ( message , response ) \n 
~~ status = status [ 0 ] \n 
~~ except ( AttributeError , IndexError ) : \n 
~~ signature , parameters = self . parse_parameters_from_response ( response ) \n 
if self . key : \n 
~~~ generated_signature = self . generate_message_signature ( parameters ) \n 
if signature != generated_signature : \n 
raise SignatureVerificationError ( generated_signature , \n 
signature ) \n 
~~ ~~ param_dict = self . get_parameters_as_dictionary ( parameters ) \n 
if in param_dict and param_dict [ ] != otp : \n 
raise InvalidValidationResponse ( message , response , param_dict ) \n 
~~ if in param_dict and param_dict [ ] != nonce : \n 
~~ if status == : \n 
~~~ return param_dict \n 
~~ ~~ elif status == : \n 
~~~ raise InvalidClientIdError ( self . client_id ) \n 
~~ elif status == : \n 
~~~ raise StatusCodeError ( status ) \n 
~~ def generate_query_string ( self , otp , nonce , timestamp = False , sl = None , \n 
timeout = None ) : \n 
data = [ ( , self . client_id ) , \n 
( , otp ) , \n 
( , nonce ) ] \n 
if timestamp : \n 
~~~ data . append ( ( , ) ) \n 
~~ if sl is not None : \n 
~~~ if sl not in range ( 0 , 101 ) and sl not in [ , ] : \n 
~~ data . append ( ( , sl ) ) \n 
~~ if timeout : \n 
~~~ data . append ( ( , timeout ) ) \n 
~~ query_string = urlencode ( data ) \n 
~~~ hmac_signature = self . generate_message_signature ( query_string ) \n 
hmac_signature = hmac_signature \n 
query_string += % ( hmac_signature . replace ( , ) ) \n 
~~ return query_string \n 
~~ def generate_message_signature ( self , query_string ) : \n 
pairs = query_string . split ( ) \n 
pairs = [ pair . split ( , 1 ) for pair in pairs ] \n 
pairs_sorted = sorted ( pairs ) \n 
pairs_string = . join ( [ . join ( pair ) for pair in pairs_sorted ] ) \n 
digest = hmac . new ( self . key , b ( pairs_string ) , hashlib . sha1 ) . digest ( ) \n 
signature = base64 . b64encode ( digest ) . decode ( ) \n 
return signature \n 
~~ def parse_parameters_from_response ( self , response ) : \n 
lines = response . splitlines ( ) \n 
pairs = [ line . strip ( ) . split ( , 1 ) for line in lines if in line ] \n 
pairs = sorted ( pairs ) \n 
signature = ( [ unquote ( v ) for k , v in pairs if k == ] or [ None ] ) [ 0 ] \n 
query_string = . join ( [ k + + v for k , v in pairs if k != ] ) \n 
return ( signature , query_string ) \n 
~~ def get_parameters_as_dictionary ( self , query_string ) : \n 
pairs = ( x . split ( , 1 ) for x in query_string . split ( ) ) \n 
return dict ( ( k , unquote ( v ) ) for k , v in pairs ) \n 
~~ def _init_request_urls ( self , api_urls ) : \n 
if not isinstance ( api_urls , ( str , list , tuple ) ) : \n 
~~ if isinstance ( api_urls , str ) : \n 
~~~ api_urls = ( api_urls , ) \n 
~~ api_urls = list ( api_urls ) \n 
for url in api_urls : \n 
~~~ if not url . startswith ( ) and not url . startswith ( ) : \n 
% ( url ) ) ) \n 
~~ ~~ return list ( api_urls ) \n 
~~ def _get_ca_bundle_path ( self ) : \n 
if self . ca_certs_bundle_path : \n 
~~~ return self . ca_certs_bundle_path \n 
~~ for file_path in COMMON_CA_LOCATIONS : \n 
~~~ if self . _is_valid_ca_bundle_file ( file_path = file_path ) : \n 
~~~ return file_path \n 
~~ def _is_valid_ca_bundle_file ( self , file_path ) : \n 
~~~ return os . path . exists ( file_path ) and os . path . isfile ( file_path ) \n 
~~ ~~ class URLThread ( threading . Thread ) : \n 
~~~ def __init__ ( self , url , timeout , verify_cert , ca_bundle_path = None ) : \n 
~~~ super ( URLThread , self ) . __init__ ( ) \n 
self . url = url \n 
self . ca_bundle_path = ca_bundle_path \n 
self . exception = None \n 
self . request = None \n 
self . response = None \n 
~~~ logger . debug ( % ( self . url , \n 
self . name ) ) \n 
verify = self . verify_cert \n 
if self . ca_bundle_path is not None : \n 
~~~ verify = self . ca_bundle_path \n 
logger . debug ( % ( self . ca_bundle_path ) ) \n 
~~~ self . request = requests . get ( url = self . url , timeout = self . timeout , \n 
verify = verify ) \n 
self . response = self . request . content . decode ( ) \n 
~~ except requests . exceptions . SSLError : \n 
~~~ e = sys . exc_info ( ) [ 1 ] \n 
self . exception = e \n 
logger . error ( + str ( e ) ) \n 
~~ args = ( self . url , self . name , self . response ) \n 
logger . debug ( % args ) \n 
~~ ~~ import logging \n 
from app import app , logger \n 
root = logging . getLogger ( ) \n 
root . setLevel ( logging . DEBUG ) \n 
logging . getLogger ( "sqlalchemy.engine" ) . setLevel ( logging . INFO ) \n 
~~ from setuptools import setup \n 
py_modules = [ ] , \n 
install_requires = [ \n 
entry_points = , \n 
from nose . tools import ok_ , raises \n 
from linot import config \n 
from linot . interfaces . line_interface import LineClientP , LineInterface \n 
class TestLineClientP : \n 
~~~ self . line_cfg = config [ ] [ ] \n 
self . lineclient = LineClientP ( self . line_cfg [ ] , \n 
self . line_cfg [ ] ) \n 
~~ def test_find_contact_by_id ( self ) : \n 
~~~ contact = self . lineclient . find_contact_by_id ( self . line_cfg [ ] ) \n 
ok_ ( contact . id == self . line_cfg [ ] ) \n 
~~ @ raises ( ValueError ) \n 
def test_find_contact_by_id_exception ( self ) : \n 
~~~ self . lineclient . find_contact_by_id ( self . line_cfg [ ] [ : - 2 ] ) \n 
~~ ~~ class TestLineInterface : \n 
~~~ self . line_interface = LineInterface ( ) \n 
~~ def test_polling_command ( self ) : \n 
~~~ test_str = \n 
me = self . line_interface . _client . getProfile ( ) \n 
me . sendMessage ( test_str ) \n 
result = self . line_interface . polling_command ( ) \n 
ok_ ( len ( result ) == 1 , result ) \n 
submitter , msg = result [ 0 ] \n 
ok_ ( submitter . code == me . id , submitter ) \n 
ok_ ( msg == test_str , \n 
. format ( msg , test_str ) ) \n 
~~ def test_get_contact_by_id ( self ) : \n 
~~~ me = self . line_interface . _client . getProfile ( ) \n 
contact = self . line_interface . _get_contact_by_id ( me . id ) \n 
ok_ ( me . id == contact . id , . format ( me . id , contact . id ) ) \n 
~~ def test_send_message ( self ) : \n 
me , msg = result [ 0 ] \n 
self . line_interface . send_message ( me , test_str ) \n 
ok_ ( msg == test_str , . format ( msg , test_str ) ) \n 
~~ def test_send_message_to_id ( self ) : \n 
self . line_interface . _send_message_to_id ( me . code , test_str ) \n 
~~ def test_get_display_name ( self ) : \n 
me_submitter , msg = result [ 0 ] \n 
me_display_name = self . line_interface . get_display_name ( me_submitter ) \n 
ok_ ( me_display_name == me . name ) \n 
~~ ~~ import pytest \n 
from aiohttp . parsers import StreamWriter , CORK \n 
from unittest import mock \n 
def test_nodelay_default ( loop ) : \n 
~~~ transport = mock . Mock ( ) \n 
s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
transport . get_extra_info . return_value = s \n 
proto = mock . Mock ( ) \n 
reader = mock . Mock ( ) \n 
writer = StreamWriter ( transport , proto , reader , loop ) \n 
assert not writer . tcp_nodelay \n 
assert not s . getsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY ) \n 
~~ def test_set_nodelay_no_change ( loop ) : \n 
writer . set_tcp_nodelay ( False ) \n 
~~ def test_set_nodelay_enable ( loop ) : \n 
writer . set_tcp_nodelay ( True ) \n 
assert writer . tcp_nodelay \n 
assert s . getsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY ) \n 
~~ def test_set_nodelay_enable_and_disable ( loop ) : \n 
~~ def test_set_nodelay_enable_ipv6 ( loop ) : \n 
s = socket . socket ( socket . AF_INET6 , socket . SOCK_STREAM ) \n 
~~ @ pytest . mark . skipif ( not hasattr ( socket , ) , \n 
def test_set_nodelay_enable_unix ( loop ) : \n 
s = socket . socket ( socket . AF_UNIX , socket . SOCK_STREAM ) \n 
~~ def test_set_nodelay_enable_no_socket ( loop ) : \n 
transport . get_extra_info . return_value = None \n 
assert writer . _socket is None \n 
def test_cork_default ( loop ) : \n 
assert not writer . tcp_cork \n 
assert not s . getsockopt ( socket . IPPROTO_TCP , CORK ) \n 
def test_set_cork_no_change ( loop ) : \n 
writer . set_tcp_cork ( False ) \n 
def test_set_cork_enable ( loop ) : \n 
writer . set_tcp_cork ( True ) \n 
assert writer . tcp_cork \n 
assert s . getsockopt ( socket . IPPROTO_TCP , CORK ) \n 
def test_set_cork_enable_and_disable ( loop ) : \n 
def test_set_cork_enable_ipv6 ( loop ) : \n 
def test_set_cork_enable_unix ( loop ) : \n 
def test_set_cork_enable_no_socket ( loop ) : \n 
def test_set_enabling_cork_disables_nodelay ( loop ) : \n 
def test_set_enabling_nodelay_disables_cork ( loop ) : \n 
from . import placeholder \n 
ShortenerSettings = namedtuple ( , [ \n 
Settings = namedtuple ( , [ \n 
def default_settings ( ) : \n 
~~~ return Settings ( \n 
verbose = False , \n 
strict = True , \n 
force = False , \n 
source = , \n 
destination = , \n 
templates = , \n 
images = , \n 
right_to_left = [ , ] , \n 
pattern = , \n 
shortener = { } , \n 
exclusive = None , \n 
default_locale = , \n 
workers_pool = 10 , \n 
local_images = , \n 
save = None , \n 
cms_service_host = "http://localhost:5001" \n 
~~ def read_args ( argsargs = argparse . ArgumentParser ) : \n 
~~~ settings = default_settings ( ) \n 
args = argsargs ( epilog = ) \n 
args . add_argument ( , , help = % settings . source ) args . add_argument ( \n 
, , help = % settings args . add_argument ( , , \n 
help = % settings . destination ) \n 
args . add_argument ( , , help = % settings . templates args . add_argument ( , , \n 
help = % settings . right_to_left args . add_argument ( , , help = % settings . images ) args . add_argument ( , , help = % settings . pattern \n 
args . add_argument ( , , \n 
help = , \n 
action = ) \n 
args . add_argument ( , , help = , action = ) \n 
help = % settings . workers_pool , type = int ) \n 
args . add_argument ( , , help = , action = args . add_argument ( , , help = , action = ) \n 
subparsers = args . add_subparsers ( help = , dest = ) \n 
template_parser = subparsers . add_parser ( ) \n 
template_parser . add_argument ( , help = ) \n 
template_parser . add_argument ( , \n 
config_parser = subparsers . add_parser ( ) \n 
config_parser . add_argument ( , help = ) \n 
gui_parser = subparsers . add_parser ( ) \n 
gui_parser . add_argument ( , , type = int , help = , default = 8080 ) \n 
gui_parser . add_argument ( , , type = str , help = , \n 
default = ) \n 
gui_parser . add_argument ( , type = str , help = ) \n 
gui_parser . add_argument ( , , type = str , help = ) \n 
return args . parse_args ( ) \n 
~~ def read_settings ( args ) : \n 
~~~ args = vars ( args ) \n 
settings = default_settings ( ) . _asdict ( ) \n 
for k in settings : \n 
~~~ if k in args and args [ k ] is not None : \n 
~~~ settings [ k ] = args [ k ] \n 
~~ ~~ return Settings ( ** settings ) \n 
~~ def print_version ( ) : \n 
~~~ import pkg_resources \n 
version = pkg_resources . require ( ) [ 0 ] . version \n 
print ( version ) \n 
~~ def generate_config ( args ) : \n 
~~~ if args . config_name == : \n 
settings = read_settings ( args ) \n 
placeholder . generate_config ( settings ) \n 
~~ def execute_command ( args ) : \n 
~~~ if args . command == : \n 
~~~ return generate_config ( args ) \n 
~~ elif args . command == : \n 
~~~ from . gui . gui import serve \n 
serve ( args ) \n 
~~ from ldap3 import Server , Connection , ALL \n 
def rotate ( record , newpassword ) : \n 
~~~ result = False \n 
host = record . get ( ) \n 
user_dn = record . get ( ) \n 
~~~ server = Server ( \n 
host = host , \n 
use_ssl = True , \n 
get_info = ALL ) \n 
conn = Connection ( \n 
server = server , \n 
user = user_dn , \n 
password = record . password , \n 
auto_bind = True ) \n 
changePwdResult = conn . extend . microsoft . modify_password ( user_dn , newpassword ) \n 
if ( changePwdResult == True ) : \n 
record . password = newpassword \n 
result = True \n 
~~ conn . unbind ( ) \n 
~~ from keepercommander . record import Record \n 
def sample_record ( ) : \n 
~~~ record = Record ( ) \n 
record . folder = \n 
record . title = \n 
record . login = \n 
record . password = \n 
record . login_url = \n 
record . notes = \n 
record . custom_fields = [ \n 
{ : , : , : } , \n 
{ : , : , : } ] \n 
return record \n 
~~ class TestRecord : \n 
~~~ def test_to_tab_delimited ( self ) : \n 
~~~ assert sample_record ( ) . to_tab_delimited ( ) == \n 
~~ def test_to_tab_dictionary ( self ) : \n 
~~~ assert sample_record ( ) . to_dictionary ( ) == { \n 
{ : , : , : } ] , \n 
~~ ~~ from filesize import size \n 
from filesize import traditional , alternative , verbose , iec , si \n 
import logbook \n 
import pyshark \n 
def caps_directory ( ) : \n 
~~~ return os . path . join ( os . path . dirname ( __file__ ) , ) \n 
~~ @ pytest . fixture \n 
def lazy_simple_capture ( request , caps_directory ) : \n 
cap_path = os . path . join ( caps_directory , ) \n 
cap = pyshark . FileCapture ( cap_path ) \n 
cap . log . level = logbook . DEBUG \n 
def finalizer ( ) : \n 
~~~ cap . close ( ) \n 
cap . eventloop . stop ( ) \n 
~~ request . addfinalizer ( finalizer ) \n 
return cap \n 
def simple_capture ( lazy_simple_capture ) : \n 
lazy_simple_capture . load_packets ( ) \n 
return lazy_simple_capturefrom cornice import Service \n 
~~ from pyramid import httpexceptions \n 
from pyramid . security import NO_PERMISSION_REQUIRED \n 
from kinto . events import ServerFlushed \n 
flush = Service ( name = , \n 
path = ) \n 
@ flush . post ( permission = NO_PERMISSION_REQUIRED ) \n 
def flush_post ( request ) : \n 
~~~ request . registry . storage . flush ( ) \n 
request . registry . permission . flush ( ) \n 
request . registry . cache . flush ( ) \n 
event = ServerFlushed ( request ) \n 
request . registry . notify ( event ) \n 
return httpexceptions . HTTPAccepted ( ) \n 
############################################################################### \n 
from tests import base \n 
def setUpModule ( ) : \n 
base . enabledPlugins . append ( ) \n 
base . startServer ( False ) \n 
~~ def tearDownModule ( ) : \n 
base . stopServer ( ) \n 
~~ class SourceTestCase ( base . TestCase ) : \n 
super ( SourceTestCase , self ) . setUp ( ) \n 
self . _user = self . model ( ) . createUser ( \n 
~~ def testSource ( self ) : \n 
path = \n 
params = { \n 
: self . _user [ ] , \n 
response = self . request ( path = path , method = , params = params ) \n 
self . assertStatusOk ( response ) \n 
folder = response . json [ ] \n 
self . assertEquals ( folder , None ) \n 
response = self . request ( path = path , method = , params = params , user = self . _user ) \n 
self . assertNotEquals ( folder , None ) \n 
self . assertEquals ( folder [ ] , ) \n 
self . assertEquals ( folder [ ] , str ( self . _user [ ] ) ) \n 
: folder [ ] \n 
response = self . request ( path = , method = , params = params , \n 
user = self . _user ) \n 
item1Id = response . json [ ] \n 
item2Id = response . json [ ] \n 
self . assertEquals ( len ( response . json ) , 0 ) \n 
self . assertEquals ( len ( response . json ) , 2 ) \n 
sourceIds = [ d [ ] for d in response . json ] \n 
~~ ~~ from girder . api import access \n 
from girder . api . describe import Description \n 
from girder . api . rest import loadmodel , RestException \n 
from girder . constants import AccessType \n 
from girder . plugins . minerva . rest . dataset import Dataset \n 
from girder . plugins . minerva . utility . minerva_utility import findDatasetFolder , updateMinervaMetadata \n 
class GeojsonDataset ( Dataset ) : \n 
~~~ self . resourceName = \n 
self . route ( , ( ) , self . createGeojsonDataset ) \n 
~~ @ access . user \n 
@ loadmodel ( map = { : } , model = , \n 
level = AccessType . WRITE ) \n 
def createGeojsonDataset ( self , item , params ) : \n 
~~~ user = self . getCurrentUser ( ) \n 
folder = findDatasetFolder ( user , user , create = True ) \n 
if folder is None : \n 
~~~ raise RestException ( ) \n 
~~ if folder [ ] != item [ ] : \n 
"folder." ) \n 
~~ minerva_metadata = { \n 
for file in self . model ( ) . childFiles ( item = item , limit = 0 ) : \n 
~~~ if in file [ ] or in file [ ] : \n 
~~~ minerva_metadata [ ] = [ { \n 
: file [ ] , : file [ ] } ] \n 
minerva_metadata [ ] = { \n 
: file [ ] , : file [ ] } \n 
~~ ~~ if not in minerva_metadata : \n 
~~ updateMinervaMetadata ( item , minerva_metadata ) \n 
return item \n 
~~ createGeojsonDataset . description = ( \n 
Description ( ) \n 
. responseClass ( ) \n 
. param ( , , required = True ) \n 
. errorResponse ( ) \n 
. errorResponse ( , 403 ) ) \n 
~~ from setuptools import setup , find_packages \n 
from os . path import join as opj \n 
curdir = os . path . dirname ( os . path . realpath ( __file__ ) ) \n 
def read ( fname ) : \n 
~~~ contents = \n 
with open ( fname ) as f : \n 
~~~ contents = f . read ( ) \n 
~~ return contents \n 
~~ package_name = \n 
def version ( ) : \n 
~~~ text = read ( opj ( curdir , package_name , ) ) \n 
matches = re . findall ( "(\'|\\")(\\S+)(\'|\\")" , text ) \n 
return matches [ 0 ] [ 1 ] \n 
~~ install_requires = [ \n 
test_requires = [ \n 
~~~ setup ( \n 
name = package_name , \n 
packages = [ package_name ] , \n 
version = version ( ) , \n 
long_description = read ( opj ( curdir , ) ) , \n 
url = , \n 
install_requires = install_requires , \n 
license = , \n 
classifiers = [ \n 
package_data = { : [ ] } , \n 
zip_safe = False , \n 
tests_require = test_requires , \n 
import utils \n 
import sdk \n 
change_file_permissions = [ ] \n 
change_folder_permissions = [ , ] \n 
list_permissions = [ , , , ] \n 
readonly_permissions = [ , ] \n 
class Permissions ( unittest . TestCase ) : \n 
~~~ new_roles = { } \n 
@ utils . allow ( services = list_permissions ) \n 
~~~ acc = self . account \n 
if acc . service in list_permissions : \n 
~~~ self . test_folder = utils . create_or_get_test_folder ( acc ) \n 
self . test_file = utils . create_test_file ( acc ) \n 
~~ new_roles = { \n 
"kloudless.nose.tester+1@gmail.com" : "reader" , \n 
"kloudless.nose.tester+2@gmail.com" : "writer" \n 
if acc . service in change_folder_permissions : \n 
~~~ self . new_roles = new_roles \n 
self . test_folder . permissions . create ( data = self . new_roles ) \n 
~~ if acc . service in change_file_permissions : \n 
self . test_file . permissions . create ( data = self . new_roles ) \n 
~~ ~~ def list_helper ( self , data ) : \n 
~~~ result = data . permissions . all ( ) \n 
self . assertIsInstance ( result , sdk . resources . AnnotatedList ) \n 
owner_exists = False \n 
for perm in result : \n 
~~~ self . assertIsInstance ( perm , sdk . resources . Permission ) \n 
if self . account . service not in readonly_permissions : \n 
~~~ if perm . role == "owner" : \n 
~~~ owner_exists = True \n 
~~~ self . assertIn ( perm . email , self . new_roles ) \n 
self . assertEqual ( perm . role , self . new_roles . get ( perm . email ) ) \n 
~~ self . assertTrue ( owner_exists ) \n 
~~ ~~ ~~ def test_folder_permissions_list ( self ) : \n 
~~~ if self . account . service in list_permissions : \n 
~~~ self . list_helper ( self . test_folder ) \n 
~~ ~~ def test_folder_permissions_set ( self ) : \n 
~~~ if self . account . service in change_folder_permissions : \n 
~~~ self . new_roles = { \n 
"kloudless.nose.tester+3@gmail.com" : "reader" , \n 
"kloudless.nose.tester+4@gmail.com" : "writer" \n 
result = self . test_folder . permissions . create ( data = self . new_roles ) \n 
self . assertIsInstance ( result . permissions , list ) \n 
self . list_helper ( self . test_folder ) \n 
~~ ~~ def test_folder_permissions_update ( self ) : \n 
~~~ self . new_roles . update ( { \n 
"kloudless.nose.tester+1@gmail.com" : "writer" , \n 
"kloudless.nose.tester+2@gmail.com" : "reader" , \n 
"kloudless.nose.tester+3@gmail.com" : "writer" , \n 
"kloudless.nose.tester+4@gmail.com" : "reader" \n 
result = self . test_folder . permissions . update ( data = self . new_roles ) \n 
~~ ~~ def test_file_permissions_list ( self ) : \n 
~~~ self . list_helper ( self . test_file ) \n 
~~ ~~ def test_file_permissions_set ( self ) : \n 
~~~ if self . account . service in change_file_permissions : \n 
result = self . test_file . permissions . create ( data = self . new_roles ) \n 
self . list_helper ( self . test_file ) \n 
~~ ~~ def test_file_permissions_update ( self ) : \n 
result = self . test_file . permissions . update ( data = self . new_roles ) \n 
~~ ~~ ~~ def test_cases ( ) : \n 
~~~ return [ utils . create_test_case ( acc , Permissions ) for acc in utils . accounts ] \n 
~~~ suite = utils . create_suite ( test_cases ( ) ) \n 
unittest . TextTestRunner ( verbosity = 2 ) . run ( suite ) \n 
~~~ basestring = basestring \n 
~~~ basestring = str \n 
~~ from . nmea import NMEASentence \n 
class NMEAFile ( object ) : \n 
def __init__ ( self , f , * args , ** kwargs ) : \n 
~~~ super ( NMEAFile , self ) . __init__ ( ) \n 
if isinstance ( f , basestring ) or args or kwargs : \n 
~~~ self . _file = self . open ( f , * args , ** kwargs ) \n 
~~~ self . _file = f \n 
~~ self . _context = None \n 
~~ def open ( self , fp , mode = ) : \n 
self . _file = open ( fp , mode = mode ) \n 
return self . _file \n 
self . _file . close ( ) \n 
for line in self . _file : \n 
~~~ yield self . parse ( line ) \n 
~~ ~~ def __enter__ ( self ) : \n 
~~~ if hasattr ( self . _file , ) : \n 
~~~ self . _context = self . _file . __enter__ ( ) \n 
~~ def __exit__ ( self , exc_type , exc_val , exc_tb ) : \n 
~~~ if self . _context : \n 
~~~ ctx = self . _context \n 
self . _context = None \n 
ctx . __exit__ ( exc_type , exc_val , exc_tb ) \n 
~~ ~~ def next ( self ) : \n 
data = self . _file . readline ( ) \n 
return self . parse ( data ) \n 
~~ def parse ( self , s ) : \n 
~~~ return NMEASentence . parse ( s ) \n 
~~ def readline ( self ) : \n 
s = self . parse ( data ) \n 
return s \n 
~~ def read ( self ) : \n 
return [ s for s in self ] \n 
~~ ~~ from optparse import make_option \n 
from django . core . management . base import BaseCommand \n 
from django . utils . translation import ugettext as _ \n 
from django_q . cluster import Cluster \n 
class Command ( BaseCommand ) : \n 
option_list = BaseCommand . option_list + ( \n 
make_option ( , \n 
dest = , \n 
default = False , \n 
help = ) , \n 
def handle ( self , * args , ** options ) : \n 
~~~ q = Cluster ( ) \n 
q . start ( ) \n 
if options . get ( , False ) : \n 
~~~ q . stop ( ) \n 
class DictImporter ( object ) : \n 
~~~ def __init__ ( self , sources ) : \n 
~~~ self . sources = sources \n 
~~ def find_module ( self , fullname , path = None ) : \n 
~~~ if fullname == "argparse" and sys . version_info >= ( 2 , 7 ) : \n 
~~ if fullname in self . sources : \n 
~~ if fullname + in self . sources : \n 
~~ def load_module ( self , fullname ) : \n 
~~~ from types import ModuleType \n 
~~~ s = self . sources [ fullname ] \n 
is_pkg = False \n 
~~~ s = self . sources [ fullname + ] \n 
is_pkg = True \n 
~~ co = compile ( s , fullname , ) \n 
module = sys . modules . setdefault ( fullname , ModuleType ( fullname ) ) \n 
module . __file__ = "%s/%s" % ( __file__ , fullname ) \n 
module . __loader__ = self \n 
if is_pkg : \n 
~~~ module . __path__ = [ fullname ] \n 
return sys . modules [ fullname ] \n 
~~ def get_source ( self , name ) : \n 
~~~ res = self . sources . get ( name ) \n 
if res is None : \n 
~~~ res = self . sources . get ( name + ) \n 
~~ return res \n 
sys . exit ( 2 ) \n 
~~ if sys . version_info >= ( 3 , 0 ) : \n 
import pickle \n 
sources = pickle . loads ( zlib . decompress ( base64 . decodebytes ( sources ) ) ) \n 
~~~ import cPickle as pickle \n 
sources = pickle . loads ( zlib . decompress ( base64 . decodestring ( sources ) ) ) \n 
~~ importer = DictImporter ( sources ) \n 
sys . meta_path . insert ( 0 , importer ) \n 
~~ from __future__ import with_statement \n 
from UserDict import DictMixin \n 
import bcrypt \n 
from pyramid . location import lineage \n 
from pyramid . security import view_execution_permitted \n 
from six import string_types \n 
from sqlalchemy import Boolean , bindparam \n 
from sqlalchemy import Column \n 
from sqlalchemy import DateTime \n 
from sqlalchemy import func \n 
from sqlalchemy import Integer \n 
from sqlalchemy import Unicode \n 
from sqlalchemy . orm . exc import NoResultFound \n 
from sqlalchemy . sql . expression import and_ \n 
from sqlalchemy . sql . expression import or_ \n 
from zope . deprecation . deprecation import deprecated \n 
from kotti import Base \n 
from kotti import DBSession \n 
from kotti import get_settings \n 
from kotti . sqla import bakery \n 
from kotti . sqla import JsonType \n 
from kotti . sqla import MutationList \n 
from kotti . util import _ \n 
from kotti . util import request_cache \n 
from kotti . util import DontCache \n 
def get_principals ( ) : \n 
~~~ return get_settings ( ) [ ] [ 0 ] ( ) \n 
~~ @ request_cache ( lambda request : None ) \n 
def get_user ( request ) : \n 
~~~ userid = request . unauthenticated_userid \n 
return get_principals ( ) . get ( userid ) \n 
~~ def has_permission ( permission , context , request ) : \n 
return request . has_permission ( permission , context ) \n 
~~ deprecated ( , \n 
class Principal ( Base ) : \n 
id = Column ( Integer , primary_key = True ) \n 
name = Column ( Unicode ( 100 ) , unique = True ) \n 
password = Column ( Unicode ( 100 ) ) \n 
active = Column ( Boolean ) \n 
confirm_token = Column ( Unicode ( 100 ) ) \n 
title = Column ( Unicode ( 100 ) , nullable = False ) \n 
email = Column ( Unicode ( 100 ) , unique = True ) \n 
groups = Column ( MutationList . as_mutable ( JsonType ) , nullable = False ) \n 
creation_date = Column ( DateTime ( ) , nullable = False ) \n 
last_login_date = Column ( DateTime ( ) ) \n 
__tablename__ = \n 
__mapper_args__ = dict ( \n 
order_by = name , \n 
def __init__ ( self , name , password = None , active = True , confirm_token = None , \n 
title = u"" , email = None , groups = None ) : \n 
if password is not None : \n 
~~~ password = get_principals ( ) . hash_password ( password ) \n 
~~ self . password = password \n 
self . active = active \n 
self . confirm_token = confirm_token \n 
self . title = title \n 
self . email = email \n 
if groups is None : \n 
~~~ groups = [ ] \n 
~~ self . groups = groups \n 
self . creation_date = datetime . now ( ) \n 
self . last_login_date = None \n 
~~~ return . format ( self . name ) \n 
~~ ~~ class AbstractPrincipals ( object ) : \n 
def __getitem__ ( self , name ) : \n 
~~ def __setitem__ ( self , name , principal ) : \n 
~~ def __delitem__ ( self , name ) : \n 
~~ def search ( self , ** kwargs ) : \n 
~~ def hash_password ( self , password ) : \n 
~~ def validate_password ( self , clear , hashed ) : \n 
~~ ~~ ROLES = { \n 
: Principal ( , title = _ ( ) ) , \n 
_DEFAULT_ROLES = ROLES . copy ( ) \n 
SHARING_ROLES = [ , , ] \n 
USER_MANAGEMENT_ROLES = SHARING_ROLES + [ ] \n 
_DEFAULT_SHARING_ROLES = SHARING_ROLES [ : ] \n 
_DEFAULT_USER_MANAGEMENT_ROLES = USER_MANAGEMENT_ROLES [ : ] \n 
SITE_ACL = [ \n 
[ , , [ ] ] , \n 
[ , , [ , , , ] ] , \n 
[ , , [ , , , , ] ] , \n 
def set_roles ( roles_dict ) : \n 
~~~ ROLES . clear ( ) \n 
ROLES . update ( roles_dict ) \n 
~~ def set_sharing_roles ( role_names ) : \n 
~~~ SHARING_ROLES [ : ] = role_names \n 
~~ def set_user_management_roles ( role_names ) : \n 
~~~ USER_MANAGEMENT_ROLES [ : ] = role_names \n 
~~ def reset_roles ( ) : \n 
ROLES . update ( _DEFAULT_ROLES ) \n 
~~ def reset_sharing_roles ( ) : \n 
~~~ SHARING_ROLES [ : ] = _DEFAULT_SHARING_ROLES \n 
~~ def reset_user_management_roles ( ) : \n 
~~~ USER_MANAGEMENT_ROLES [ : ] = _DEFAULT_USER_MANAGEMENT_ROLES \n 
~~ def reset ( ) : \n 
~~~ reset_roles ( ) \n 
reset_sharing_roles ( ) \n 
reset_user_management_roles ( ) \n 
~~ class PersistentACLMixin ( object ) : \n 
~~~ def _get_acl ( self ) : \n 
~~~ if self . _acl is None : \n 
~~~ raise AttributeError ( ) \n 
~~ return self . _acl \n 
~~ def _set_acl ( self , value ) : \n 
~~~ self . _acl = value \n 
~~ def _del_acl ( self ) : \n 
~~~ self . _acl = None \n 
~~ __acl__ = property ( _get_acl , _set_acl , _del_acl ) \n 
~~ def _cachekey_list_groups_raw ( name , context ) : \n 
~~~ context_id = context is not None and getattr ( context , , id ( context ) ) \n 
return name , context_id \n 
~~ @ request_cache ( _cachekey_list_groups_raw ) \n 
def list_groups_raw ( name , context ) : \n 
from kotti . resources import Node \n 
if isinstance ( context , Node ) : \n 
~~~ return set ( \n 
r . group_name for r in context . local_groups \n 
if r . principal_name == name \n 
~~ return set ( ) \n 
~~ def list_groups ( name , context = None ) : \n 
return list_groups_ext ( name , context ) [ 0 ] \n 
~~ def _cachekey_list_groups_ext ( name , context = None , _seen = None , _inherited = None ) : \n 
~~~ if _seen is not None or _inherited is not None : \n 
~~~ raise DontCache \n 
~~~ context_id = getattr ( context , , id ( context ) ) \n 
return unicode ( name ) , context_id \n 
~~ ~~ @ request_cache ( _cachekey_list_groups_ext ) \n 
def list_groups_ext ( name , context = None , _seen = None , _inherited = None ) : \n 
~~~ name = unicode ( name ) \n 
groups = set ( ) \n 
recursing = _inherited is not None \n 
_inherited = _inherited or set ( ) \n 
principal = get_principals ( ) . get ( name ) \n 
if principal is not None : \n 
~~~ groups . update ( principal . groups ) \n 
if context is not None or ( context is None and _seen is not None ) : \n 
~~~ _inherited . update ( principal . groups ) \n 
~~ ~~ if _seen is None : \n 
~~~ _seen = { name } \n 
~~ if context is not None : \n 
~~~ items = lineage ( context ) \n 
for idx , item in enumerate ( items ) : \n 
~~~ group_names = [ i for i in list_groups_raw ( name , item ) \n 
if i not in _seen ] \n 
groups . update ( group_names ) \n 
if recursing or idx != 0 : \n 
~~~ _inherited . update ( group_names ) \n 
~~ ~~ ~~ new_groups = groups - _seen \n 
_seen . update ( new_groups ) \n 
for group_name in new_groups : \n 
~~~ g , i = list_groups_ext ( \n 
group_name , context , _seen = _seen , _inherited = _inherited ) \n 
groups . update ( g ) \n 
_inherited . update ( i ) \n 
~~ return list ( groups ) , list ( _inherited ) \n 
~~ def set_groups ( name , context , groups_to_set = ( ) ) : \n 
from kotti . resources import LocalGroup \n 
name = unicode ( name ) \n 
context . local_groups = [ \n 
lg for lg in context . local_groups \n 
if lg . principal_name != name \n 
] + [ \n 
LocalGroup ( context , name , unicode ( group_name ) ) \n 
for group_name in groups_to_set \n 
~~ def list_groups_callback ( name , request ) : \n 
if not is_user ( name ) : \n 
~~ if name in get_principals ( ) : \n 
~~~ context = request . environ . get ( \n 
, getattr ( request , , None ) ) \n 
if context is None : \n 
~~~ from kotti . resources import get_root \n 
context = get_root ( request ) \n 
~~ return list_groups ( name , context ) \n 
~~ ~~ @ contextmanager \n 
def authz_context ( context , request ) : \n 
~~~ before = request . environ . pop ( , None ) \n 
request . environ [ ] = context \n 
~~~ yield \n 
~~~ del request . environ [ ] \n 
if before is not None : \n 
~~~ request . environ [ ] = before \n 
~~ ~~ ~~ @ contextmanager \n 
def request_method ( request , method ) : \n 
~~~ before = request . method \n 
request . method = method \n 
~~~ request . method = before \n 
~~ ~~ def view_permitted ( context , request , name = , method = ) : \n 
~~~ with authz_context ( context , request ) : \n 
~~~ with request_method ( request , method ) : \n 
~~~ return view_execution_permitted ( context , request , name ) \n 
~~ ~~ ~~ def principals_with_local_roles ( context , inherit = True ) : \n 
principals = set ( ) \n 
items = [ context ] \n 
if inherit : \n 
~~~ principals . update ( \n 
r . principal_name for r in item . local_groups \n 
if not r . principal_name . startswith ( ) \n 
~~ return list ( principals ) \n 
~~ def map_principals_with_local_roles ( context ) : \n 
~~~ principals = get_principals ( ) \n 
value = [ ] \n 
for principal_name in principals_with_local_roles ( context ) : \n 
~~~ principal = principals [ principal_name ] \n 
~~~ all , inherited = list_groups_ext ( principal_name , context ) \n 
value . append ( ( principal , ( all , inherited ) ) ) \n 
~~ ~~ return sorted ( value , key = lambda t : t [ 0 ] . name ) \n 
~~ def is_user ( principal ) : \n 
~~~ if not isinstance ( principal , string_types ) : \n 
~~~ principal = principal . name \n 
~~ return not in principal \n 
~~ class Principals ( DictMixin ) : \n 
factory = Principal \n 
def _principal_by_name ( cls , name ) : \n 
~~~ query = bakery ( lambda session : session . query ( cls . factory ) . filter ( \n 
cls . factory . name == bindparam ( ) ) ) \n 
return query ( DBSession ( ) ) . params ( name = name ) . one ( ) \n 
~~ @ request_cache ( lambda self , name : unicode ( name ) ) \n 
if name . startswith ( ) : \n 
~~~ raise KeyError ( name ) \n 
~~~ return self . _principal_by_name ( name ) \n 
~~ except NoResultFound : \n 
~~ ~~ def __setitem__ ( self , name , principal ) : \n 
if isinstance ( principal , dict ) : \n 
~~~ principal = self . factory ( ** principal ) \n 
~~ DBSession . add ( principal ) \n 
~~~ principal = self . _principal_by_name ( name ) \n 
DBSession . delete ( principal ) \n 
~~ ~~ def iterkeys ( self ) : \n 
~~~ for ( principal_name , ) in DBSession . query ( self . factory . name ) : \n 
~~~ yield principal_name \n 
~~ ~~ def keys ( self ) : \n 
~~~ return list ( self . iterkeys ( ) ) \n 
~~ def search ( self , match = , ** kwargs ) : \n 
if not kwargs : \n 
~~ filters = [ ] \n 
for key , value in kwargs . items ( ) : \n 
~~~ col = getattr ( self . factory , key ) \n 
if isinstance ( value , string_types ) and in value : \n 
~~~ value = value . replace ( , ) . lower ( ) \n 
filters . append ( func . lower ( col ) . like ( value ) ) \n 
~~~ filters . append ( col == value ) \n 
~~ ~~ query = DBSession . query ( self . factory ) \n 
if match == : \n 
~~~ query = query . filter ( or_ ( * filters ) ) \n 
~~ elif match == : \n 
~~~ query = query . filter ( and_ ( * filters ) ) \n 
~~ return query \n 
~~ log_rounds = 10 \n 
def hash_password ( self , password , hashed = None ) : \n 
~~~ if hashed is None : \n 
~~~ hashed = bcrypt . gensalt ( self . log_rounds ) \n 
~~ return unicode ( \n 
bcrypt . hashpw ( password . encode ( ) , hashed . encode ( ) ) ) \n 
~~~ return self . hash_password ( clear , hashed ) == hashed \n 
~~ ~~ ~~ def principals_factory ( ) : \n 
~~~ return Principals ( ) \n 
~~ import json \n 
from mechanize . _mechanize import LinkNotFoundError \n 
from pytest import raises \n 
from kotti . testing import BASE_URL \n 
from kotti . testing import user \n 
from kotti . views . edit . upload import UploadView \n 
def test_upload_anonymous ( root , dummy_request , browser ) : \n 
~~~ view = UploadView ( root , dummy_request ) \n 
assert view . factories == [ ] \n 
link = browser . getLink \n 
browser . open ( . format ( BASE_URL ) ) \n 
with raises ( LinkNotFoundError ) : \n 
~~~ link ( ) . click ( ) \n 
~~ browser . open ( . format ( BASE_URL ) ) \n 
assert browser . url . startswith ( . format ( BASE_URL ) ) \n 
~~ @ user ( ) \n 
def test_upload_authenticated_wo_mimetype ( root , dummy_request , browser ) : \n 
~~~ with raises ( KeyError ) : \n 
~~~ browser . open ( . format ( BASE_URL ) ) \n 
~~ ~~ @ user ( ) \n 
def test_upload_authenticated_text ( root , dummy_request , browser ) : \n 
j = json . loads ( browser . contents ) \n 
assert in j \n 
types = j [ ] \n 
assert len ( types ) == 1 \n 
assert types [ 0 ] [ ] == \n 
from setuptools import setup \n 
from setuptools import find_packages \n 
here = os . path . abspath ( os . path . dirname ( __file__ ) ) \n 
~~~ README = open ( os . path . join ( here , ) ) . read ( ) \n 
AUTHORS = open ( os . path . join ( here , ) ) . read ( ) \n 
CHANGES = open ( os . path . join ( here , ) ) . read ( ) \n 
~~~ README = AUTHORS = CHANGES = \n 
tests_require = [ \n 
development_requires = [ ] \n 
docs_require = [ \n 
if sys . version_info [ : 3 ] < ( 2 , 7 , 0 ) : \n 
~~~ install_requires . append ( ) \n 
~~ setup ( name = , \n 
keywords = , \n 
packages = find_packages ( ) , \n 
tests_require = tests_require , \n 
dependency_links = [ ] , \n 
extras_require = { \n 
: tests_require , \n 
: development_requires , \n 
: docs_require , \n 
from . markers import list_marker_layout \n 
from . min_max import handle_min_max_width \n 
from . percentages import resolve_percentages , resolve_position_percentages \n 
from . preferred import shrink_to_fit \n 
from . tables import table_wrapper_width \n 
from . . formatting_structure import boxes \n 
@ handle_min_max_width \n 
def float_width ( box , context , containing_block ) : \n 
~~~ if box . width == : \n 
~~~ box . width = shrink_to_fit ( context , box , containing_block . width ) \n 
~~ ~~ def float_layout ( context , box , containing_block , device_size , absolute_boxes , \n 
fixed_boxes ) : \n 
from . blocks import block_container_layout \n 
from . inlines import inline_replaced_box_width_height \n 
resolve_percentages ( box , ( containing_block . width , containing_block . height ) ) \n 
resolve_position_percentages ( \n 
box , ( containing_block . width , containing_block . height ) ) \n 
if box . margin_left == : \n 
~~~ box . margin_left = 0 \n 
~~ if box . margin_right == : \n 
~~~ box . margin_right = 0 \n 
~~ if box . margin_top == : \n 
~~~ box . margin_top = 0 \n 
~~ if box . margin_bottom == : \n 
~~~ box . margin_bottom = 0 \n 
~~ clearance = get_clearance ( context , box ) \n 
if clearance is not None : \n 
~~~ box . position_y += clearance \n 
~~ if isinstance ( box , boxes . BlockReplacedBox ) : \n 
~~~ inline_replaced_box_width_height ( box , device_size = None ) \n 
~~ elif box . width == : \n 
~~~ float_width ( box , context , containing_block ) \n 
~~ if box . is_table_wrapper : \n 
~~~ table_wrapper_width ( \n 
context , box , ( containing_block . width , containing_block . height ) ) \n 
~~ if isinstance ( box , boxes . BlockBox ) : \n 
~~~ context . create_block_formatting_context ( ) \n 
box , _ , _ , _ , _ = block_container_layout ( \n 
context , box , max_position_y = float ( ) , \n 
skip_stack = None , device_size = device_size , page_is_empty = False , \n 
absolute_boxes = absolute_boxes , fixed_boxes = fixed_boxes , \n 
adjoining_margins = None ) \n 
list_marker_layout ( context , box ) \n 
context . finish_block_formatting_context ( box ) \n 
~~~ assert isinstance ( box , boxes . BlockReplacedBox ) \n 
~~ box = find_float_position ( context , box , containing_block ) \n 
context . excluded_shapes . append ( box ) \n 
return box \n 
~~ def find_float_position ( context , box , containing_block ) : \n 
if context . excluded_shapes : \n 
~~~ highest_y = context . excluded_shapes [ - 1 ] . position_y \n 
if box . position_y < highest_y : \n 
~~~ box . translate ( 0 , highest_y - box . position_y ) \n 
~~ ~~ position_x , position_y , available_width = avoid_collisions ( \n 
context , box , containing_block ) \n 
if box . style . float == : \n 
~~~ position_x += available_width - box . margin_width ( ) \n 
~~ box . translate ( position_x - box . position_x , position_y - box . position_y ) \n 
~~ def get_clearance ( context , box , collapsed_margin = 0 ) : \n 
clearance = None \n 
hypothetical_position = box . position_y + collapsed_margin \n 
for excluded_shape in context . excluded_shapes : \n 
~~~ if box . style . clear in ( excluded_shape . style . float , ) : \n 
~~~ y , h = excluded_shape . position_y , excluded_shape . margin_height ( ) \n 
if hypothetical_position < y + h : \n 
~~~ clearance = max ( \n 
( clearance or 0 ) , y + h - hypothetical_position ) \n 
~~ ~~ ~~ return clearance \n 
~~ def avoid_collisions ( context , box , containing_block , outer = True ) : \n 
~~~ excluded_shapes = context . excluded_shapes \n 
position_y = box . position_y if outer else box . border_box_y ( ) \n 
box_width = box . margin_width ( ) if outer else box . border_width ( ) \n 
box_height = box . margin_height ( ) if outer else box . border_height ( ) \n 
if box . border_height ( ) == 0 and box . is_floated ( ) : \n 
~~~ return 0 , 0 , containing_block . width \n 
~~ while True : \n 
~~~ colliding_shapes = [ \n 
shape for shape in excluded_shapes \n 
if ( shape . position_y < position_y < \n 
shape . position_y + shape . margin_height ( ) ) or \n 
( shape . position_y < position_y + box_height < \n 
( shape . position_y >= position_y and \n 
shape . position_y + shape . margin_height ( ) <= \n 
position_y + box_height ) \n 
left_bounds = [ \n 
shape . position_x + shape . margin_width ( ) \n 
for shape in colliding_shapes \n 
if shape . style . float == ] \n 
right_bounds = [ \n 
shape . position_x \n 
max_left_bound = containing_block . content_box_x ( ) \n 
max_right_bound = containing_block . content_box_x ( ) + containing_block . width \n 
if not outer : \n 
~~~ max_left_bound += box . margin_left \n 
max_right_bound -= box . margin_right \n 
~~ if left_bounds or right_bounds : \n 
~~~ if left_bounds : \n 
~~~ max_left_bound = max ( max ( left_bounds ) , max_left_bound ) \n 
~~ if right_bounds : \n 
~~~ max_right_bound = min ( min ( right_bounds ) , max_right_bound ) \n 
~~ if box_width > max_right_bound - max_left_bound : \n 
~~~ new_positon_y = min ( \n 
shape . position_y + shape . margin_height ( ) \n 
for shape in colliding_shapes ) \n 
if new_positon_y > position_y : \n 
~~~ position_y = new_positon_y \n 
continue \n 
~~ ~~ ~~ break \n 
~~ position_x = max_left_bound \n 
available_width = max_right_bound - max_left_bound \n 
~~~ position_x -= box . margin_left \n 
position_y -= box . margin_top \n 
~~ return position_x , position_y , available_width \n 
VERSION = \n 
options = dict ( \n 
name = "brigit" , \n 
version = VERSION , \n 
long_description = __doc__ , \n 
author_email = "florian.mounier@kozea.fr" , \n 
license = "BSD" , \n 
platforms = "Any" , \n 
install_requires = [ ] , \n 
provides = [ ] , \n 
use_2to3 = True , \n 
setup ( ** options ) \n 
from django . conf . urls import patterns , include , url \n 
import health_check \n 
health_check . autodiscover ( ) \n 
urlpatterns = patterns ( , \n 
url ( , , name = ) , \n 
import django \n 
from django . db import connection \n 
from django . db . models import Count \n 
from django . db . models . query_utils import Q \n 
from django . utils import translation \n 
from hvad . test_utils . data import NORMAL , STANDARD \n 
from hvad . test_utils . testcase import HvadTestCase , minimumDjangoVersion \n 
from hvad . test_utils . project . app . models import Normal , AggregateModel , Standard , SimpleRelated \n 
from hvad . test_utils . fixtures import NormalFixture , StandardFixture \n 
class FilterTests ( HvadTestCase , NormalFixture ) : \n 
~~~ normal_count = 2 \n 
def test_simple_filter ( self ) : \n 
~~~ qs = Normal . objects . language ( ) . filter ( shared_field__contains = ) \n 
self . assertEqual ( qs . count ( ) , 1 ) \n 
obj = qs [ 0 ] \n 
self . assertEqual ( obj . shared_field , NORMAL [ 2 ] . shared_field ) \n 
self . assertEqual ( obj . translated_field , NORMAL [ 2 ] . translated_field [ ] ) \n 
qs = Normal . objects . language ( ) . filter ( shared_field__contains = ) \n 
self . assertEqual ( obj . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( obj . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
~~ def test_translated_filter ( self ) : \n 
~~~ qs = Normal . objects . language ( ) . filter ( translated_field__contains = ) \n 
self . assertEqual ( qs . count ( ) , self . normal_count ) \n 
obj1 , obj2 = qs \n 
self . assertEqual ( obj1 . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( obj1 . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
self . assertEqual ( obj2 . shared_field , NORMAL [ 2 ] . shared_field ) \n 
self . assertEqual ( obj2 . translated_field , NORMAL [ 2 ] . translated_field [ ] ) \n 
~~ def test_fallbacks_filter ( self ) : \n 
~~~ ( Normal . objects . language ( ) \n 
. filter ( shared_field = NORMAL [ 1 ] . shared_field ) \n 
. delete_translations ( ) ) \n 
with translation . override ( ) : \n 
~~~ qs = Normal . objects . language ( ) . fallbacks ( ) \n 
with self . assertNumQueries ( 2 ) : \n 
~~~ self . assertEqual ( qs . count ( ) , self . normal_count ) \n 
self . assertEqual ( len ( qs ) , self . normal_count ) \n 
~~ with self . assertNumQueries ( 0 ) : \n 
~~~ self . assertCountEqual ( ( obj . pk for obj in qs ) , tuple ( self . normal_id . values ( ) ) ) \n 
self . assertCountEqual ( ( obj . language_code for obj in qs ) , self . translations ) \n 
~~ ~~ ~~ def test_all_languages_filter ( self ) : \n 
~~~ with self . assertNumQueries ( 2 ) : \n 
self . assertEqual ( qs . count ( ) , self . normal_count * len ( self . translations ) ) \n 
self . assertCountEqual ( ( obj . shared_field for obj in qs ) , \n 
( NORMAL [ 1 ] . shared_field , \n 
NORMAL [ 2 ] . shared_field ) * 2 ) \n 
self . assertCountEqual ( ( obj . translated_field for obj in qs ) , \n 
( NORMAL [ 1 ] . translated_field [ ] , \n 
NORMAL [ 1 ] . translated_field [ ] , \n 
NORMAL [ 2 ] . translated_field [ ] , \n 
NORMAL [ 2 ] . translated_field [ ] ) ) \n 
~~ with self . assertNumQueries ( 2 ) : \n 
NORMAL [ 2 ] . shared_field ) ) \n 
~~ ~~ def test_deferred_language_filter ( self ) : \n 
~~~ with translation . override ( ) : \n 
~~ with translation . override ( ) : \n 
~~ ~~ ~~ class ExtraTests ( HvadTestCase , NormalFixture ) : \n 
def test_simple_extra ( self ) : \n 
~~~ qs = Normal . objects . language ( ) . extra ( select = { : } ) \n 
self . assertEqual ( int ( qs [ 0 ] . test_extra ) , 4 ) \n 
~~ ~~ class QueryCachingTests ( HvadTestCase , NormalFixture ) : \n 
def _try_all_cache_using_methods ( self , qs , length ) : \n 
~~~ with self . assertNumQueries ( 0 ) : \n 
~~~ x = 0 \n 
for obj in qs : x += 1 \n 
self . assertEqual ( x , length ) \n 
~~~ qs [ 0 ] \n 
~~~ self . assertEqual ( qs . exists ( ) , length != 0 ) \n 
~~~ self . assertEqual ( qs . count ( ) , length ) \n 
~~~ self . assertEqual ( len ( qs ) , length ) \n 
~~~ self . assertEqual ( bool ( qs ) , length != 0 ) \n 
~~ ~~ def test_iter_caches ( self ) : \n 
~~~ index = 0 \n 
qs = Normal . objects . language ( ) . filter ( pk = self . normal_id [ 1 ] ) \n 
for obj in qs : \n 
~~~ index += 1 \n 
~~ self . assertEqual ( index , 1 ) \n 
self . _try_all_cache_using_methods ( qs , 1 ) \n 
~~ ~~ def test_pickling_caches ( self ) : \n 
~~~ import pickle \n 
~~~ qs = Normal . objects . language ( ) . filter ( pk = self . normal_id [ 1 ] ) \n 
pickle . dumps ( qs ) \n 
~~ ~~ def test_len_caches ( self ) : \n 
self . assertEqual ( len ( qs ) , 1 ) \n 
~~ ~~ def test_bool_caches ( self ) : \n 
self . assertTrue ( qs ) \n 
~~ ~~ ~~ class IterTests ( HvadTestCase , NormalFixture ) : \n 
def test_simple_iter ( self ) : \n 
~~~ with self . assertNumQueries ( 1 ) : \n 
~~~ for index , obj in enumerate ( Normal . objects . language ( ) , 1 ) : \n 
~~~ self . assertEqual ( obj . shared_field , NORMAL [ index ] . shared_field ) \n 
self . assertEqual ( obj . translated_field , NORMAL [ index ] . translated_field [ ] ) \n 
~~ ~~ ~~ with translation . override ( ) : \n 
~~ ~~ ~~ ~~ def test_iter_unique_reply ( self ) : \n 
~~~ self . assertEqual ( len ( Normal . objects . all ( ) ) , len ( Normal . objects . untranslated ( ) ) ) \n 
~~ ~~ def test_iter_deferred_language ( self ) : \n 
~~~ qs = Normal . objects . language ( ) \n 
~~~ for index , obj in enumerate ( qs , 1 ) : \n 
~~ ~~ ~~ ~~ class UpdateTests ( HvadTestCase , NormalFixture ) : \n 
def test_update_shared ( self ) : \n 
~~~ NEW_SHARED = \n 
n1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
n2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
ja1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
ja2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
with self . assertNumQueries ( 1 if connection . features . update_can_self_select else 2 ) : \n 
~~~ Normal . objects . language ( ) . update ( shared_field = NEW_SHARED ) \n 
~~ new1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
new2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
self . assertEqual ( new1 . shared_field , NEW_SHARED ) \n 
self . assertEqual ( new1 . translated_field , n1 . translated_field ) \n 
self . assertEqual ( new2 . shared_field , NEW_SHARED ) \n 
self . assertEqual ( new2 . translated_field , n2 . translated_field ) \n 
newja1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
newja2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
self . assertEqual ( newja1 . shared_field , NEW_SHARED ) \n 
self . assertEqual ( newja2 . shared_field , NEW_SHARED ) \n 
self . assertEqual ( newja1 . translated_field , ja1 . translated_field ) \n 
self . assertEqual ( newja2 . translated_field , ja2 . translated_field ) \n 
~~ def test_update_translated ( self ) : \n 
~~~ NEW_TRANSLATED = \n 
with self . assertNumQueries ( 1 ) : \n 
~~~ Normal . objects . language ( ) . update ( translated_field = NEW_TRANSLATED ) \n 
self . assertEqual ( new1 . shared_field , n1 . shared_field ) \n 
self . assertEqual ( new2 . shared_field , n2 . shared_field ) \n 
self . assertEqual ( new1 . translated_field , NEW_TRANSLATED ) \n 
self . assertEqual ( new2 . translated_field , NEW_TRANSLATED ) \n 
self . assertEqual ( newja1 . shared_field , ja1 . shared_field ) \n 
self . assertEqual ( newja2 . shared_field , ja2 . shared_field ) \n 
~~ def test_update_mixed ( self ) : \n 
NEW_TRANSLATED = \n 
with self . assertNumQueries ( 2 if connection . features . update_can_self_select else 3 ) : \n 
~~~ Normal . objects . language ( ) . update ( \n 
shared_field = NEW_SHARED , translated_field = NEW_TRANSLATED \n 
~~ def test_update_deferred_language ( self ) : \n 
~~~ qs . update ( translated_field = NEW_TRANSLATED ) \n 
~~ ~~ new1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
~~ def test_update_fallbacks ( self ) : \n 
~~~ qs . filter ( shared_field = NORMAL [ 1 ] . shared_field ) . update ( shared_field = ) \n 
~~ self . assertEqual ( Normal . objects . language ( ) . get ( shared_field = ) . pk , self . normal_id self . assertEqual ( Normal . objects . language ( ) . get ( shared_field = ) . pk , self . normal_id \n 
~~ ~~ class ValuesListTests ( HvadTestCase , NormalFixture ) : \n 
def test_values_list_translated ( self ) : \n 
~~~ values = Normal . objects . language ( ) . values_list ( , flat = True ) \n 
values_list = list ( values ) \n 
self . assertCountEqual ( values_list , [ NORMAL [ 1 ] . translated_field [ ] , \n 
NORMAL [ 2 ] . translated_field [ ] ] ) \n 
~~ def test_values_list_shared ( self ) : \n 
self . assertCountEqual ( values_list , [ NORMAL [ 1 ] . shared_field , \n 
NORMAL [ 2 ] . shared_field ] ) \n 
~~ def test_values_list_mixed ( self ) : \n 
~~~ values = Normal . objects . language ( ) . values_list ( , ) \n 
check = [ \n 
( NORMAL [ 1 ] . shared_field , NORMAL [ 1 ] . translated_field [ ] ) , \n 
( NORMAL [ 2 ] . shared_field , NORMAL [ 2 ] . translated_field [ ] ) , \n 
self . assertCountEqual ( values_list , check ) \n 
~~ def test_values_list_deferred_language ( self ) : \n 
~~~ values = qs . values_list ( , ) \n 
~~ check = [ \n 
~~ def test_values_list_language_all ( self ) : \n 
~~~ values = ( Normal . objects . language ( ) . filter ( shared_field = NORMAL [ 1 ] . shared_field ) \n 
. values_list ( , ) ) \n 
~~ ~~ class ValuesTests ( HvadTestCase , NormalFixture ) : \n 
def test_values_shared ( self ) : \n 
~~~ values = Normal . objects . language ( ) . values ( ) \n 
{ : NORMAL [ 1 ] . shared_field } , \n 
{ : NORMAL [ 2 ] . shared_field } , \n 
~~ def test_values_translated ( self ) : \n 
{ : NORMAL [ 1 ] . translated_field [ ] } , \n 
{ : NORMAL [ 2 ] . translated_field [ ] } , \n 
~~ def test_values_mixed ( self ) : \n 
~~~ values = Normal . objects . language ( ) . values ( , ) \n 
{ : NORMAL [ 1 ] . translated_field [ ] , \n 
: NORMAL [ 1 ] . shared_field } , \n 
{ : NORMAL [ 2 ] . translated_field [ ] , \n 
: NORMAL [ 2 ] . shared_field } , \n 
~~ def test_values_post_language ( self ) : \n 
~~~ values = Normal . objects . language ( ) . values ( ) . language ( ) \n 
~~ def test_values_post_filter ( self ) : \n 
~~~ qs = Normal . objects . language ( ) . values ( ) \n 
values = qs . filter ( shared_field = NORMAL [ 1 ] . shared_field ) \n 
~~ def test_values_deferred_language ( self ) : \n 
~~~ values = qs . values ( ) \n 
~~ def test_values_language_all ( self ) : \n 
. values ( , ) ) \n 
{ : NORMAL [ 1 ] . shared_field , \n 
: NORMAL [ 1 ] . translated_field [ ] } , \n 
~~ ~~ class InBulkTests ( HvadTestCase , NormalFixture ) : \n 
def test_empty_in_bulk ( self ) : \n 
~~~ result = Normal . objects . language ( ) . in_bulk ( [ ] ) \n 
self . assertEqual ( len ( result ) , 0 ) \n 
~~ ~~ def test_in_bulk ( self ) : \n 
~~~ pk1 , pk2 = self . normal_id [ 1 ] , self . normal_id [ 2 ] \n 
~~~ result = Normal . objects . language ( ) . in_bulk ( [ pk1 , pk2 ] ) \n 
self . assertCountEqual ( ( pk1 , pk2 ) , result ) \n 
self . assertEqual ( result [ pk1 ] . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( result [ pk1 ] . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
self . assertEqual ( result [ pk1 ] . language_code , ) \n 
self . assertEqual ( result [ pk2 ] . shared_field , NORMAL [ 2 ] . shared_field ) \n 
self . assertEqual ( result [ pk2 ] . translated_field , NORMAL [ 2 ] . translated_field [ ] ) \n 
self . assertEqual ( result [ pk2 ] . language_code , ) \n 
~~ ~~ def test_untranslated_in_bulk ( self ) : \n 
~~~ pk1 = self . normal_id [ 1 ] \n 
~~~ result = Normal . objects . untranslated ( ) . in_bulk ( [ pk1 ] ) \n 
self . assertCountEqual ( ( pk1 , ) , result ) \n 
~~ ~~ ~~ def test_fallbacks_in_bulk ( self ) : \n 
. filter ( shared_field = NORMAL [ 2 ] . shared_field ) \n 
result = Normal . objects . language ( ) . fallbacks ( , ) . in_bulk ( [ pk1 , pk2 ] ) \n 
~~ ~~ def test_all_languages_in_bulk ( self ) : \n 
~~~ with self . assertRaises ( ValueError ) : \n 
~~~ Normal . objects . language ( ) . in_bulk ( [ self . normal_id [ 1 ] ] ) \n 
~~ ~~ def test_in_bulk_deferred_language ( self ) : \n 
~~~ result = qs . in_bulk ( [ pk1 ] ) \n 
~~ ~~ ~~ class DeleteTests ( HvadTestCase , NormalFixture ) : \n 
def test_delete_all ( self ) : \n 
~~~ Normal . objects . all ( ) . delete ( ) \n 
self . assertEqual ( Normal . objects . count ( ) , 0 ) \n 
self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , 0 ) \n 
~~ def test_delete_translation ( self ) : \n 
~~~ self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , 4 ) \n 
Normal . objects . language ( ) . delete_translations ( ) \n 
self . assertEqual ( Normal . objects . untranslated ( ) . count ( ) , 2 ) \n 
self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , 2 ) \n 
~~ def test_filtered_delete_translation ( self ) : \n 
( Normal . objects . language ( ) \n 
self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , 3 ) \n 
. filter ( translated_field = NORMAL [ 2 ] . translated_field [ ] ) \n 
~~ def test_delete_translation_deferred_language ( self ) : \n 
~~~ qs . delete_translations ( ) \n 
~~ self . assertEqual ( Normal . objects . language ( ) . count ( ) , 2 ) \n 
self . assertEqual ( Normal . objects . language ( ) . count ( ) , 0 ) \n 
~~ def test_delete_fallbacks ( self ) : \n 
qs . filter ( shared_field = NORMAL [ 1 ] . shared_field ) . delete ( ) \n 
self . assertEqual ( Normal . objects . language ( ) . count ( ) , self . normal_count - 1 ) \n 
~~ ~~ class GetTranslationFromInstanceTests ( HvadTestCase , NormalFixture ) : \n 
~~~ normal_count = 1 \n 
def test_simple ( self ) : \n 
~~~ en = Normal . objects . language ( ) . get ( ) \n 
ja_trans = en . translations . get_language ( ) \n 
ja = Normal . objects . language ( ) . get ( pk = en . pk ) \n 
self . assertEqual ( en . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( en . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
self . assertRaises ( AttributeError , getattr , ja_trans , ) \n 
self . assertEqual ( ja_trans . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
self . assertEqual ( ja . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( ja . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
~~ def test_cached ( self ) : \n 
~~~ en = Normal . objects . untranslated ( ) . prefetch_related ( ) . get ( ) \n 
with self . assertNumQueries ( 0 ) : \n 
~~~ ja_trans = en . translations . get_language ( ) \n 
~~ ja = Normal . objects . language ( ) . get ( pk = en . pk ) \n 
~~ def test_not_exist ( self ) : \n 
~~~ en = Normal . objects . untranslated ( ) . get ( ) \n 
with self . assertRaises ( Normal . DoesNotExist ) : \n 
~~~ en . translations . get_language ( ) \n 
~~ en = Normal . objects . untranslated ( ) . prefetch_related ( ) . get ( ) \n 
~~ ~~ ~~ class AggregateTests ( HvadTestCase ) : \n 
~~~ def test_aggregate ( self ) : \n 
~~~ from django . db . models import Avg \n 
AggregateModel . objects . language ( "en" ) . create ( number = 10 , translated_number = 20 ) \n 
AggregateModel . objects . language ( "en" ) . create ( number = 0 , translated_number = 0 ) \n 
self . assertEqual ( AggregateModel . objects . language ( "en" ) . aggregate ( Avg ( "number" ) ) , { self . assertEqual ( AggregateModel . objects . language ( "en" ) . aggregate ( Avg ( "translated_number" ) ) , \n 
self . assertEqual ( AggregateModel . objects . language ( "en" ) . aggregate ( num = Avg ( "number" ) ) , { : self . assertEqual ( AggregateModel . objects . language ( "en" ) . aggregate ( tnum = Avg ( "translated_number" \n 
~~ ~~ class AnnotateTests ( HvadTestCase , StandardFixture , NormalFixture ) : \n 
standard_count = 4 \n 
def test_annotate ( self ) : \n 
~~~ qs = Normal . objects . language ( ) . annotate ( Count ( ) ) \n 
self . assertEqual ( qs [ 0 ] . standards__count , 2 ) \n 
self . assertEqual ( qs [ 1 ] . standards__count , 2 ) \n 
qs = Normal . objects . language ( ) . annotate ( foo = Count ( ) ) \n 
self . assertEqual ( qs [ 0 ] . foo , 2 ) \n 
self . assertEqual ( qs [ 1 ] . foo , 2 ) \n 
with self . assertRaises ( ValueError ) : \n 
~~~ qs = Normal . objects . language ( ) . annotate ( Count ( ) , standards__count = Count ( \n 
~~ ~~ ~~ class NotImplementedTests ( HvadTestCase ) : \n 
~~~ def test_notimplemented ( self ) : \n 
~~~ baseqs = SimpleRelated . objects . language ( ) \n 
self . assertRaises ( NotImplementedError , baseqs . defer , ) \n 
self . assertRaises ( NotImplementedError , baseqs . only ) \n 
self . assertRaises ( NotImplementedError , baseqs . bulk_create , [ ] ) \n 
self . assertRaises ( NotImplementedError , baseqs . select_related ) \n 
if django . VERSION >= ( 1 , 7 ) : \n 
~~~ self . assertRaises ( NotImplementedError , baseqs . update_or_create ) \n 
~~ ~~ ~~ class MinimumVersionTests ( HvadTestCase ) : \n 
~~~ def test_versions ( self ) : \n 
~~~ qs = SimpleRelated . objects . language ( ) \n 
if django . VERSION < ( 1 , 7 ) : \n 
~~~ self . assertRaises ( AttributeError , getattr , qs , ) \n 
~~ ~~ ~~ class ExcludeTests ( HvadTestCase , NormalFixture ) : \n 
def test_defer ( self ) : \n 
~~~ qs = Normal . objects . language ( ) . exclude ( translated_field = NORMAL [ 1 ] . translated_field [ ] self . assertEqual ( qs . count ( ) , 0 ) \n 
~~ def test_fallbacks_exclude ( self ) : \n 
qs = ( Normal . objects . language ( ) \n 
. fallbacks ( , ) \n 
. exclude ( shared_field = NORMAL [ 1 ] . shared_field ) ) \n 
self . assertEqual ( qs . count ( ) , 0 ) \n 
~~ def test_all_languages_exclude ( self ) : \n 
~~~ qs = Normal . objects . language ( ) . exclude ( translated_field = NORMAL [ 1 ] . translated_field [ self . assertEqual ( qs . count ( ) , 1 ) \n 
self . assertEqual ( qs [ 0 ] . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
~~ def test_invalid_all_languages_exclude ( self ) : \n 
~~~ Normal . objects . language ( ) . exclude ( language_code = ) \n 
~~ ~~ ~~ class ComplexFilterTests ( HvadTestCase , StandardFixture , NormalFixture ) : \n 
standard_count = 2 \n 
def test_qobject_filter ( self ) : \n 
~~~ shared_contains_one = Q ( shared_field__contains = ) \n 
shared_contains_two = Q ( shared_field__contains = ) \n 
qs = Normal . objects . language ( ) . filter ( shared_contains_two ) \n 
qs = ( Normal . objects . language ( ) . filter ( Q ( shared_contains_one | shared_contains_two ) ) \n 
. order_by ( ) ) \n 
self . assertEqual ( qs . count ( ) , 2 ) \n 
obj = qs [ 1 ] \n 
~~ def test_aware_qobject_filter ( self ) : \n 
~~~ from hvad . utils import get_translation_aware_manager \n 
manager = get_translation_aware_manager ( Standard ) \n 
normal_one = Q ( normal_field = STANDARD [ 1 ] . normal_field ) \n 
normal_two = Q ( normal_field = STANDARD [ 2 ] . normal_field ) \n 
shared_one = Q ( normal__shared_field = NORMAL [ STANDARD [ 1 ] . normal ] . shared_field ) \n 
translated_one_en = Q ( normal__translated_field = NORMAL [ STANDARD [ 1 ] . normal ] . translated_field [ translated_two_en = Q ( normal__translated_field = NORMAL [ STANDARD [ 2 ] . normal ] . translated_field [ \n 
~~~ qs = manager . filter ( shared_one ) \n 
self . assertEqual ( obj . normal_field , STANDARD [ 1 ] . normal_field ) \n 
qs = manager . filter ( translated_one_en ) \n 
qs = manager . filter ( Q ( normal_one & shared_one & translated_one_en ) ) \n 
qs = manager . filter ( Q ( normal_one & translated_two_en ) ) \n 
qs = manager . filter ( Q ( shared_one & translated_two_en ) ) \n 
qs = manager . filter ( Q ( translated_one_en & translated_two_en ) ) \n 
qs = manager . filter ( Q ( normal_one | translated_one_en ) ) \n 
qs = manager . filter ( Q ( shared_one | translated_one_en ) ) \n 
qs = manager . filter ( Q ( normal_one | translated_two_en ) ) \n 
qs = manager . filter ( Q ( shared_one | translated_two_en ) ) \n 
qs = manager . filter ( Q ( translated_one_en | translated_two_en ) ) \n 
qs = manager . filter ( Q ( normal_one & ( translated_one_en | translated_two_en ) ) ) \n 
qs = manager . filter ( Q ( normal_two & ( translated_one_en | translated_two_en ) ) ) \n 
qs = manager . filter ( shared_one & ~ translated_one_en ) \n 
qs = manager . filter ( shared_one & ~ translated_two_en ) \n 
~~ ~~ def test_defer ( self ) : \n 
~~~ qs = Normal . objects . language ( ) . complex_filter ( { } ) \n 
self . assertRaises ( NotImplementedError , \n 
Normal . objects . language ( ) . complex_filter , \n 
Q ( shared_field = NORMAL [ 1 ] . shared_field ) ) \n 
~~ ~~ import json \n 
import sublime \n 
import sublime_plugin \n 
import analytics \n 
import uuid \n 
from elasticsearch import Elasticsearch \n 
from elasticsearch_connections import CustomHeadersConnection \n 
from abc import ABCMeta , abstractmethod \n 
from . . panel import IndexListPanel \n 
from . . panel import DocTypeListPanel \n 
from . . panel import SwitchServerListPanel \n 
from . . panel import AnalyzerListPanel \n 
from . . panel import ScriptListPanel \n 
from . . panel import SearchTemplateListPanel \n 
from . . panel import AliasListPanel \n 
from . . panel import IndexTemplateListPanel \n 
from . . panel import WarmerListPanel \n 
from . . panel import FieldListPanel \n 
from . . panel import RepositoryListPanel \n 
from . . panel import SnapshotListPanel \n 
ANALYTICS_WRITE_KEY = "phc2hsUe48Dfw1iwsYQs2W7HH9jcwrws" \n 
def track_command ( user_id , command_name ) : \n 
~~~ analytics . write_key = ANALYTICS_WRITE_KEY \n 
analytics . identify ( user_id ) \n 
"category" : "ST3" , \n 
"label" : command_name , \n 
~~ def track_activate ( user_id ) : \n 
analytics . track ( user_id , "Activate" , { \n 
"label" : sublime . platform ( ) , \n 
~~ class Settings ( object ) : \n 
~~~ SETTINGS_FILE = \n 
~~~ self . settings = sublime . load_settings ( self . SETTINGS_FILE ) \n 
def base_url ( self ) : \n 
~~~ base_url = self . settings . get ( "base_url" , "http://localhost:9200" ) \n 
if base_url . endswith ( "/" ) : \n 
~~~ return base_url [ : - 1 ] \n 
~~ return base_url \n 
def index ( self ) : \n 
~~~ return self . settings . get ( "index" , "blog-ja" ) \n 
def doc_type ( self ) : \n 
~~~ return self . settings . get ( "doc_type" , "posts" ) \n 
def scroll_size ( self ) : \n 
~~~ return self . settings . get ( "scroll_size" , "1m" ) \n 
def headers ( self ) : \n 
~~~ return self . settings . get ( "headers" , { } ) \n 
def servers ( self ) : \n 
~~~ def _normalize_servers ( servers ) : \n 
~~~ items = [ ] \n 
for name , server in servers . items ( ) : \n 
~~~ server [ "name" ] = name \n 
items . append ( server ) \n 
~~ servers = sorted ( items , key = lambda k : k [ "name" ] ) \n 
return servers \n 
~~ servers = self . settings . get ( "servers" , [ ] ) \n 
if isinstance ( servers , dict ) : \n 
~~~ servers = _normalize_servers ( servers ) \n 
~~ return servers \n 
def active_server ( self ) : \n 
~~~ return dict ( \n 
base_url = self . base_url , \n 
index = self . index , \n 
doc_type = self . doc_type , \n 
scroll_size = self . scroll_size , \n 
def ab_command ( self ) : \n 
~~~ return self . settings . get ( "ab_command" ) \n 
def ab_requests ( self ) : \n 
~~~ return str ( self . settings . get ( "ab_requests" ) ) \n 
def ab_concurrency ( self ) : \n 
~~~ return str ( self . settings . get ( "ab_concurrency" ) ) \n 
def analytics ( self ) : \n 
~~~ return self . settings . get ( "analytics" , True ) \n 
def user_id ( self ) : \n 
~~~ return self . settings . get ( "user_id" , None ) \n 
def dump_file ( self ) : \n 
~~~ return self . settings . get ( "dump_file" , "" ) \n 
def chunk_size ( self ) : \n 
~~~ return self . settings . get ( "chunk_size" , 500 ) \n 
~~ def set ( self , key , value ) : \n 
~~~ self . settings . set ( key , value ) \n 
~~ def save ( self ) : \n 
~~~ sublime . save_settings ( self . SETTINGS_FILE ) \n 
~~ ~~ class BaseCommand ( sublime_plugin . WindowCommand ) : \n 
~~~ __metaclass__ = ABCMeta \n 
command_name = None \n 
~~~ self . settings = Settings ( ) \n 
sublime_plugin . WindowCommand . __init__ ( self , * args , ** kwargs ) \n 
def view ( self ) : \n 
~~~ return self . window . active_view ( ) \n 
~~ def is_valid_json ( self ) : \n 
~~~ json . loads ( self . get_text ( ) ) \n 
~~ def is_enabled ( self ) : \n 
~~~ return self . is_valid_json ( ) \n 
~~ def get_text ( self ) : \n 
~~~ return self . view . substr ( sublime . Region ( 0 , self . view . size ( ) ) ) \n 
~~ def init_client ( self ) : \n 
~~~ self . _client = Elasticsearch ( \n 
self . settings . base_url , \n 
send_get_body_as = , \n 
connection_class = CustomHeadersConnection , \n 
headers = self . settings . headers \n 
return self . _client \n 
~~ def save_settings ( self ) : \n 
~~~ self . settings . save ( ) \n 
self . init_client ( ) \n 
def client ( self ) : \n 
~~~ return self . init_client ( ) \n 
~~ def track_command ( self ) : \n 
~~~ if self . settings . analytics : \n 
~~~ user_id = self . settings . user_id \n 
if not user_id : \n 
~~~ user_id = str ( uuid . uuid4 ( ) ) \n 
self . settings . set ( "user_id" , user_id ) \n 
self . settings . save ( ) \n 
track_activate ( user_id ) \n 
~~ track_command ( user_id , self . command_name ) \n 
~~ ~~ def show_input_panel ( self , label , default , callback ) : \n 
~~~ self . window . show_input_panel ( label , default , callback , None , None ) \n 
~~ def show_response ( self , response , title = "" ) : \n 
~~~ title = title or self . command_name \n 
text = json . dumps ( response , indent = 2 , ensure_ascii = False ) \n 
self . window . run_command ( \n 
"show_response" , { "title" : title , "text" : text } ) \n 
~~ def show_index_list_panel ( self , callback ) : \n 
~~~ list_panel = IndexListPanel ( \n 
self . window , self . client , self . settings . index ) \n 
list_panel . show ( callback ) \n 
~~ def show_doc_type_list_panel ( self , callback ) : \n 
~~~ list_panel = DocTypeListPanel ( \n 
~~ def show_analyzer_list_panel ( self , callback ) : \n 
~~~ list_panel = AnalyzerListPanel ( \n 
~~ def show_switch_server_list_panel ( self , callback ) : \n 
~~~ list_panel = SwitchServerListPanel ( self . window , self . settings . servers ) \n 
~~ def show_script_list_panel ( self , callback ) : \n 
~~~ list_panel = ScriptListPanel ( self . window , self . client ) \n 
~~ def show_search_template_list_panel ( self , callback ) : \n 
~~~ list_panel = SearchTemplateListPanel ( self . window , self . client ) \n 
~~ def show_alias_list_panel ( self , callback ) : \n 
~~~ list_panel = AliasListPanel ( \n 
~~ def show_index_template_list_panel ( self , callback ) : \n 
~~~ list_panel = IndexTemplateListPanel ( self . window , self . client ) \n 
~~ def show_warmer_list_panel ( self , callback ) : \n 
~~~ list_panel = WarmerListPanel ( \n 
~~ def show_field_list_panel ( self , callback ) : \n 
~~~ list_panel = FieldListPanel ( \n 
self . window , self . client , \n 
self . settings . index , self . settings . doc_type ) \n 
~~ def show_repository_list_panel ( self , callback ) : \n 
~~~ list_panel = RepositoryListPanel ( self . window , self . client ) \n 
~~ def show_snapshot_list_panel ( self , repository , callback ) : \n 
~~~ list_panel = SnapshotListPanel ( self . window , self . client , repository ) \n 
~~ def show_output_panel ( self , text , syntax = None ) : \n 
~~~ self . window . run_command ( \n 
"show_output_panel" , { "text" : text , "syntax" : syntax } ) \n 
~~ def show_object_output_panel ( self , obj ) : \n 
~~~ options = dict ( \n 
indent = 4 , \n 
ensure_ascii = False \n 
self . show_output_panel ( \n 
json . dumps ( obj , ** options ) , \n 
syntax = "Packages/JavaScript/JSON.tmLanguage" ) \n 
~~ def show_active_server ( self ) : \n 
~~~ self . window . run_command ( "settings_show_active_server" ) \n 
~~ @ abstractmethod \n 
def run_request ( self , * args , ** kwargs ) : \n 
~~~ raise NotImplementedError ( ) \n 
~~ def run_request_wrapper ( self , * args , ** kwargs ) : \n 
~~~ response = self . run_request ( * args , ** kwargs ) \n 
~~ if response is not None : \n 
~~~ self . show_response ( response ) \n 
self . track_command ( ) \n 
~~ ~~ def request_thread ( self , * args , ** kwargs ) : \n 
~~~ thread = threading . Thread ( \n 
target = self . run_request_wrapper , args = args , kwargs = kwargs ) \n 
~~ def run ( self , * args , ** kwargs ) : \n 
~~~ self . request_thread ( * args , ** kwargs ) \n 
~~ ~~ class CreateBaseCommand ( BaseCommand ) : \n 
~~~ def run_request_wrapper ( self , * args , ** kwargs ) : \n 
~~~ self . show_object_output_panel ( response ) \n 
~~ ~~ ~~ class DeleteBaseCommand ( CreateBaseCommand ) : \n 
~~ class CatBaseCommand ( CreateBaseCommand ) : \n 
~~~ def is_enabled ( self ) : \n 
~~~ self . show_output_panel ( response ) \n 
~~ ~~ ~~ class SearchBaseCommand ( BaseCommand ) : \n 
~~~ def extend_options ( self , options , search_type = None ) : \n 
~~~ if search_type : \n 
~~~ self . command_name = "{base}-{search_type}" . format ( \n 
base = self . command_name , \n 
search_type = search_type . lower ( ) \n 
~~ if search_type == "scan" : \n 
~~~ options [ "params" ] = dict ( \n 
search_type = search_type , \n 
scroll = self . settings . scroll_size \n 
~~ elif search_type is not None : \n 
search_type = search_type \n 
~~ return options \n 
~~ ~~ class SettingsBaseCommand ( BaseCommand ) : \n 
~~ ~~ import sublime \n 
from . base import DeleteBaseCommand \n 
class DeleteDocumentCommand ( DeleteBaseCommand ) : \n 
~~~ command_name = "elasticsearch:delete-document" \n 
def is_enabled ( self ) : \n 
~~ def run_request ( self , id = None ) : \n 
~~~ if not id : \n 
~~~ self . show_input_panel ( , , self . run ) \n 
~~ options = dict ( \n 
index = self . settings . index , \n 
doc_type = self . settings . doc_type , \n 
id = id \n 
~~~ return self . client . delete ( ** options ) \n 
~~ ~~ ~~ import sublime \n 
class IndicesDeleteAliasCommand ( DeleteBaseCommand ) : \n 
~~~ command_name = "elasticsearch:indices-delete-alias" \n 
~~ def run_request ( self , index = None , name = None ) : \n 
~~~ if not index or not name : \n 
~~~ self . show_alias_list_panel ( self . run ) \n 
index = index , \n 
name = name \n 
~~~ return self . client . indices . delete_alias ( ** options ) \n 
~~ ~~ ~~ from . base import BaseCommand \n 
class IndicesStatsCommand ( BaseCommand ) : \n 
~~~ command_name = "elasticsearch:indices-stats" \n 
~~ def run_request ( self , index = None ) : \n 
~~~ if index is None : \n 
~~~ self . show_index_list_panel ( self . run ) \n 
params = dict ( human = True ) \n 
return self . client . indices . stats ( ** options ) \n 
~~ ~~ import sublime_plugin \n 
class ShowOutputPanelCommand ( sublime_plugin . WindowCommand ) : \n 
def run ( self , text , syntax = None ) : \n 
~~~ if syntax is None : \n 
~~~ syntax = self . default_syntax \n 
~~ panel = self . window . create_output_panel ( "elasticsearch" ) \n 
"show_panel" , { "panel" : "output.elasticsearch" } ) \n 
panel . set_syntax_file ( syntax ) \n 
panel . settings ( ) . set ( , True ) \n 
panel . settings ( ) . set ( , False ) \n 
panel . set_read_only ( False ) \n 
panel . run_command ( , { : text } ) \n 
panel . set_read_only ( True ) \n 
__all__ = [ "easter" , "EASTER_JULIAN" , "EASTER_ORTHODOX" , "EASTER_WESTERN" ] \n 
EASTER_JULIAN = 1 \n 
EASTER_ORTHODOX = 2 \n 
EASTER_WESTERN = 3 \n 
def easter ( year , method = EASTER_WESTERN ) : \n 
if not ( 1 <= method <= 3 ) : \n 
~~ y = year \n 
g = y % 19 \n 
e = 0 \n 
if method < 3 : \n 
~~~ i = ( 19 * g + 15 ) % 30 \n 
j = ( y + y // 4 + i ) % 7 \n 
if method == 2 : \n 
~~~ e = 10 \n 
if y > 1600 : \n 
~~~ e = e + y // 100 - 16 - ( y // 100 - 16 ) // 4 \n 
~~~ c = y // 100 \n 
h = ( c - c // 4 - ( 8 * c + 13 ) // 25 + 19 * g + 15 ) % 30 \n 
i = h - ( h // 28 ) * ( 1 - ( h // 28 ) * ( 29 // ( h + 1 ) ) * ( ( 21 - g ) // 11 ) ) \n 
j = ( y + y // 4 + i + 2 - c + c // 4 ) % 7 \n 
~~ p = i - j + e \n 
d = 1 + ( p + 27 + ( p + 6 ) // 40 ) % 31 \n 
m = 3 + ( p + 26 ) // 30 \n 
return datetime . date ( int ( y ) , int ( m ) , int ( d ) ) \n 
~~ __all__ = [ \n 
class ImproperlyConfigured ( Exception ) : \n 
~~ class ElasticsearchException ( Exception ) : \n 
~~ class SerializationError ( ElasticsearchException ) : \n 
~~ class TransportError ( ElasticsearchException ) : \n 
def status_code ( self ) : \n 
return self . args [ 0 ] \n 
def error ( self ) : \n 
return self . args [ 1 ] \n 
def info ( self ) : \n 
return self . args [ 2 ] \n 
~~~ return % ( self . status_code , self . error ) \n 
~~ ~~ class ConnectionError ( TransportError ) : \n 
def __str__ ( self ) : \n 
self . error , self . info . __class__ . __name__ , self . info ) \n 
~~ ~~ class SSLError ( ConnectionError ) : \n 
~~ class ConnectionTimeout ( ConnectionError ) : \n 
self . info . __class__ . __name__ , self . info ) \n 
~~ ~~ class NotFoundError ( TransportError ) : \n 
~~ class ConflictError ( TransportError ) : \n 
~~ class RequestError ( TransportError ) : \n 
~~ class AuthenticationException ( TransportError ) : \n 
~~ class AuthorizationException ( TransportError ) : \n 
~~ HTTP_EXCEPTIONS = { \n 
400 : RequestError , \n 
401 : AuthenticationException , \n 
403 : AuthorizationException , \n 
404 : NotFoundError , \n 
409 : ConflictError , \n 
from . alias_list_panel import AliasListPanel \n 
from . analyzer_list_panel import AnalyzerListPanel \n 
from . doc_type_list_panel import DocTypeListPanel \n 
from . field_list_panel import FieldListPanel \n 
from . index_list_panel import IndexListPanel \n 
from . index_template_list_panel import IndexTemplateListPanel \n 
from . repository_list_panel import RepositoryListPanel \n 
from . script_list_panel import ScriptListPanel \n 
from . search_template_list_panel import SearchTemplateListPanel \n 
from . snapshot_list_panel import SnapshotListPanel \n 
from . switch_server_list_panel import SwitchServerListPanel \n 
from . warmer_list_panel import WarmerListPanel \n 
"AliasListPanel" , \n 
"AnalyzerListPanel" , \n 
"DocTypeListPanel" , \n 
"FieldListPanel" , \n 
"IndexListPanel" , \n 
"IndexTemplateListPanel" , \n 
"RepositoryListPanel" , \n 
"ScriptListPanel" , \n 
"SearchTemplateListPanel" , \n 
"SnapshotListPanel" , \n 
"SwitchServerListPanel" , \n 
"WarmerListPanel" , \n 
from test . asserting . policy import PolicyAssertion , get_fixture_path \n 
from vint . linting . level import Level \n 
from vint . linting . policy . prohibit_command_with_unintended_side_effect import ProhibitCommandWithUnintendedSideEffect \n 
PATH_VALID_VIM_SCRIPT = get_fixture_path ( ) \n 
PATH_INVALID_VIM_SCRIPT = get_fixture_path ( \n 
class TestProhibitCommandWithUnintendedSideEffect ( PolicyAssertion , unittest . TestCase ) : \n 
~~~ def _create_violation_by_line_number ( self , line_number ) : \n 
~~~ return { \n 
: Level . WARNING , \n 
: { \n 
: line_number , \n 
: 1 , \n 
: PATH_INVALID_VIM_SCRIPT \n 
~~ def test_get_violation_if_found_with_valid_file ( self ) : \n 
~~~ self . assertFoundNoViolations ( PATH_VALID_VIM_SCRIPT , \n 
ProhibitCommandWithUnintendedSideEffect ) \n 
~~ def test_get_violation_if_found_with_invalid_file ( self ) : \n 
~~~ expected_violations = [ self . _create_violation_by_line_number ( line_number ) \n 
for line_number in range ( 1 , 14 ) ] \n 
expected_violations [ 3 ] [ ] [ ] = 2 \n 
expected_violations [ 4 ] [ ] [ ] = 6 \n 
self . assertFoundViolationsEqual ( PATH_INVALID_VIM_SCRIPT , \n 
ProhibitCommandWithUnintendedSideEffect , \n 
expected_violations ) \n 
from test . asserting . config_source import ConfigSourceAssertion \n 
from test . asserting . config_source import get_fixture_path \n 
from vint . linting . config . config_file_source import ConfigFileSource \n 
FIXTURE_CONFIG_FILE = get_fixture_path ( ) \n 
class TestConfigFileSource ( ConfigSourceAssertion , unittest . TestCase ) : \n 
~~~ class ConcreteConfigFileSource ( ConfigFileSource ) : \n 
~~~ def get_file_path ( self , env ) : \n 
~~~ return FIXTURE_CONFIG_FILE \n 
~~ ~~ def test_get_config_dict ( self ) : \n 
~~~ expected_config_dict = { \n 
: 10 , \n 
: False , \n 
config_source = self . initialize_config_source_with_env ( \n 
TestConfigFileSource . ConcreteConfigFileSource ) \n 
self . assertConfigDict ( config_source , expected_config_dict ) \n 
~~ from vint . ast . plugin . scope_plugin . scope_detector import ( \n 
detect_scope_visibility , \n 
normalize_variable_name , \n 
is_builtin_variable , \n 
from vint . ast . plugin . scope_plugin . scope_linker import ScopeLinker \n 
from vint . ast . plugin . scope_plugin . identifier_classifier import ( \n 
IdentifierClassifier , \n 
is_function_identifier , \n 
REACHABILITY_FLAG = \n 
REFERECED_FLAG = \n 
class ReferenceReachabilityTester ( object ) : \n 
class TwoWayScopeReferenceAttacher ( object ) : \n 
def attach ( cls , root_scope_tree ) : \n 
~~~ root_scope_tree [ ] = None \n 
return cls . _attach_recursively ( root_scope_tree ) \n 
def _attach_recursively ( cls , scope_tree ) : \n 
~~~ for child_scope in scope_tree [ ] : \n 
~~~ child_scope [ ] = scope_tree \n 
cls . _attach_recursively ( child_scope ) \n 
~~ return scope_tree \n 
~~ ~~ def process ( self , ast ) : \n 
~~~ scope_linker = ScopeLinker ( ) \n 
scope_linker . process ( ast ) \n 
id_collector = IdentifierClassifier . IdentifierCollector ( ) \n 
classified_id_group = id_collector . collect_identifiers ( ast ) \n 
dec_id_nodes = classified_id_group [ ] \n 
ref_id_nodes = classified_id_group [ ] \n 
self . _scope_tree = scope_linker . scope_tree \n 
self . _link_registry = scope_linker . link_registry \n 
ReferenceReachabilityTester . TwoWayScopeReferenceAttacher . attach ( self . _scope_tree ) \n 
for dec_id_node in dec_id_nodes : \n 
~~~ dec_id_node [ REFERECED_FLAG ] = False \n 
~~ for ref_id_node in ref_id_nodes : \n 
~~~ is_reachable = self . check_reachability ( ref_id_node ) \n 
ref_id_node [ REACHABILITY_FLAG ] = is_reachable \n 
~~ ~~ def get_objective_scope_visibility ( self , node ) : \n 
context_scope = self . _link_registry . get_scope_by_referencing_identifier ( node ) \n 
return detect_scope_visibility ( node , context_scope ) [ ] \n 
~~ def _reset_referenced_flag ( self , scope_tree ) : \n 
~~~ for functions in child_scope [ ] . values ( ) : \n 
~~~ for function in functions : \n 
~~~ function [ REFERECED_FLAG ] = False \n 
~~ ~~ for variables in child_scope [ ] . values ( ) : \n 
~~~ for variable in variables : \n 
~~~ variable [ REFERECED_FLAG ] = False \n 
~~ ~~ self . _reset_referenced_flag ( child_scope ) \n 
~~ ~~ def check_reachability ( self , ref_id_node ) : \n 
~~~ scope = self . _link_registry . get_context_scope_by_identifier ( ref_id_node ) \n 
var_name = normalize_variable_name ( ref_id_node , scope ) \n 
is_func_id = is_function_identifier ( ref_id_node ) \n 
while scope is not None : \n 
~~~ if is_func_id : \n 
~~~ functions_list = scope [ ] \n 
if var_name in functions_list : \n 
~~~ for variable in functions_list [ var_name ] : \n 
~~~ declaring_id_node = self . _link_registry . get_declarative_identifier_by_variable ( variable ) \n 
declaring_id_node [ REFERECED_FLAG ] = True \n 
~~ ~~ variables_list = scope [ ] \n 
if var_name in variables_list : \n 
~~~ for variable in variables_list [ var_name ] : \n 
~~ scope = scope [ ] \n 
~~ return is_builtin_variable ( ref_id_node ) \n 
~~ ~~ def is_reference_identifier ( node ) : \n 
~~~ return REACHABILITY_FLAG in node \n 
~~ def is_reachable_reference_identifier ( node ) : \n 
~~~ return node . get ( REACHABILITY_FLAG , False ) \n 
~~ def is_declarative_identifier ( node ) : \n 
~~~ return REFERECED_FLAG in node \n 
~~ def is_referenced_declarative_identifier ( node ) : \n 
~~~ return node . get ( REFERECED_FLAG , False ) \n 
~~ import re \n 
from vint . ast . node_type import NodeType \n 
from vint . linting . policy . abstract_policy import AbstractPolicy \n 
from vint . linting . policy_registry import register_policy \n 
from vint . ast . dictionary . abbreviations import ( \n 
Abbreviations , \n 
AbbreviationsIncludingInvertPrefix , \n 
SetCommandFamily = { \n 
@ register_policy \n 
class ProhibitAbbreviationOption ( AbstractPolicy ) : \n 
~~~ super ( ProhibitAbbreviationOption , self ) . __init__ ( ) \n 
self . description = \n 
self . reference = \n 
self . level = Level . STYLE_PROBLEM \n 
self . was_scriptencoding_found = False \n 
self . has_encoding_opt_after_scriptencoding = False \n 
~~ def listen_node_types ( self ) : \n 
~~~ return [ NodeType . EXCMD , NodeType . OPTION ] \n 
~~ def is_valid ( self , node , lint_context ) : \n 
node_type = NodeType ( node [ ] ) \n 
if node_type is NodeType . OPTION : \n 
~~~ option_name = node [ ] [ 1 : ] \n 
is_valid = option_name not in Abbreviations \n 
if not is_valid : \n 
~~~ self . _make_description_by_option_name ( option_name ) \n 
~~ return is_valid \n 
~~ excmd_node = node \n 
is_set_cmd = excmd_node [ ] [ ] . get ( ) in SetCommandFamily \n 
if not is_set_cmd : \n 
~~ option_expr = excmd_node [ ] . split ( ) [ 1 ] \n 
option_name = re . match ( , option_expr ) . group ( 0 ) \n 
is_valid = option_name not in AbbreviationsIncludingInvertPrefix \n 
~~ def _make_description_by_option_name ( self , option_name ) : \n 
~~~ param = { \n 
: AbbreviationsIncludingInvertPrefix [ option_name ] , \n 
: option_name , \n 
self . description = ( \n 
. format ( ** param ) ) \n 
~~ ~~ from serfclient import result \n 
class TestSerfResult ( object ) : \n 
~~~ def test_initialises_to_none ( self ) : \n 
~~~ r = result . SerfResult ( ) \n 
assert r . head is None \n 
assert r . body is None \n 
~~ def test_provides_a_pretty_printed_form_for_repl_use ( self ) : \n 
~~~ r = result . SerfResult ( head = { "a" : 1 } , body = ( , ) ) \n 
~~ def test_can_convert_to_list ( self ) : \n 
~~~ r = result . SerfResult ( head = 1 , body = 2 ) \n 
assert sorted ( list ( r ) ) == [ 1 , 2 ] \n 
~~ def test_can_convert_to_tuple ( self ) : \n 
assert sorted ( tuple ( r ) ) == [ 1 , 2 ] \n 
class AttrDict ( dict ) : \n 
def __getattr__ ( self , name ) : \n 
~~~ if name in self : \n 
~~~ return self [ name ] \n 
~~ raise AttributeError ( % name ) \n 
~~ def __setattr__ ( self , name , val ) : \n 
~~~ self [ name ] = val \n 
~~ ~~ def get_logger ( name , level = None ) : \n 
logger = logging . getLogger ( name ) \n 
if not logger . handlers : \n 
~~~ stderr = logging . StreamHandler ( ) \n 
stderr . setFormatter ( logging . Formatter ( \n 
logger . addHandler ( stderr ) \n 
level = level if level else os . environ . get ( , ) \n 
logger . setLevel ( getattr ( logging , level ) ) \n 
~~ return logger \n 
~~ from hacksport . problem import Challenge \n 
class Problem ( Challenge ) : \n 
~~~ def setup ( self ) : \n 
~~~ self . flag = \n 
from math import * \n 
def AirDensity ( RH , Tc , P = 101.2 ) : \n 
return rho_a \n 
~~ def PsychConst ( P , cP = 1.013 , lambda_v = 2.26e3 ) : \n 
~~~ gamma = ( cP * P / ( 0.622 * lambda_v ) ) \n 
return gamma \n 
~~ def SatVaporPress ( Tc ) : \n 
~~~ eSat = 0.61 * exp ( 17.27 * Tc / ( 237.3 + Tc ) ) \n 
return eSat \n 
~~ def SlopeSatVaporPress ( Tc ) : \n 
~~~ delta = 4098.0 * SatVaporPress ( Tc ) / ( 237.3 + Tc ) ** 2 \n 
return delta \n 
~~ def AeroReist ( um , zm , z0 , d , zmp = zm ) : \n 
~~~ k = 0.4 \n 
r_a = 1.0 / ( k ** 2 * um ) * log ( ( zm - d ) / z0 ) * log ( ( zmp - d ) / ( z0 / 10.0 ) ) \n 
return r_a \n 
~~ def SurfResist ( g0 , S , D , Tc , SM , SM0 ) : \n 
~~~ g_c = Gee_C ( ) \n 
g_R = Gee_R ( S ) \n 
g_D = Gee_D ( D ) \n 
g_T = Gee_T ( Tc + 273.15 ) \n 
g_M = Gee_M ( SM , SM0 ) \n 
g_s = g0 * g_c * g_R * g_D * g_T * g_M \n 
r_s = 1.0 / g_s \n 
return r_s \n 
~~ def Gee_c ( ) : \n 
~~~ g_c = 1.0 \n 
return g_c \n 
~~ def Gee_R ( S , K_R = 200.0 ) : \n 
~~~ g_R = ( S * ( 1000.0 + K_R ) ) / ( 1000.0 * ( S + K_R ) ) \n 
return g_R \n 
~~ def Gee_D ( D , K_D1 = - 0.307 , K_D2 = 0.019 ) : \n 
~~~ g_D = 1.0 + K_D1 * D + K_D2 * D ** 2 \n 
return g_D \n 
~~ def Gee_T ( TK , TL = 273.0 , TH = 313.0 , T0 = 293.0 ) : \n 
~~~ alpha_T = ( TH - T0 ) / ( T0 - TL ) \n 
g_T = ( ( TK - TL ) * ( TH - TK ) ** alpha_T ) / ( ( T0 - TL ) * ( TH - T0 ) ** alpha_T ) \n 
return g_T \n 
~~ def Gee_M ( SM , SM0 , K_M1 , K_M2 ) : \n 
~~~ g_SM = 1.0 - K_M1 * exp ( K_M2 * ( SM - SM0 ) ) \n 
return g_SM \n 
~~ def PenmanMonteithPET ( Tc , RH , Rn , S , SM , um , z0 , d , g0 , SM0 , P = 101.2 , zm = 2.0 ) : \n 
rho_a = AirDensity ( RH , Tc , P ) \n 
D = ( 1.0 - RH ) * SatVaporPress ( Tc ) \n 
delta = SlopeSatVaporPress ( Tc ) \n 
gamma = PsychConst ( P ) \n 
r_a = AeroReist ( um , zm , z0 , d ) \n 
r_s = SurfResist ( g0 , S , D , Tc , SM , SM0 ) \n 
LE = ( delta * Rn + ( rho_a * cP * D ) / r_a ) / ( delta + gamma * ( 1.0 + r_s / r_a ) ) \n 
return LE \n 
~~ from contextlib import contextmanager \n 
from OpenGL . GL import * \n 
@ contextmanager \n 
def glSection ( type ) : \n 
~~~ glBegin ( type ) \n 
yield \n 
glEnd ( ) \n 
~~ @ contextmanager \n 
def glMatrix ( ) : \n 
~~~ glPushMatrix ( ) \n 
glPopMatrix ( ) \n 
def glModeMatrix ( type ) : \n 
~~~ glMatrixMode ( type ) \n 
glPushMatrix ( ) \n 
glMatrixMode ( type ) \n 
def attributes ( * glBits ) : \n 
~~~ for bit in glBits : \n 
~~~ glPushAttrib ( bit ) \n 
~~ yield \n 
for bit in glBits : \n 
~~~ glPopAttrib ( ) \n 
def enabled ( * glBits ) : \n 
~~~ glEnable ( bit ) \n 
~~~ glDisable ( bit ) \n 
def disabled ( * glBits ) : \n 
def overlays2D ( width , height , background_color ) : \n 
glDisable ( GL_LIGHTING ) \n 
glDisable ( GL_LIGHT0 ) \n 
glDisable ( GL_BLEND ) \n 
glEnable ( GL_SCISSOR_TEST ) \n 
with glModeMatrix ( GL_PROJECTION ) : \n 
~~ glViewport ( 0 , 0 , width , height ) \n 
glDisable ( GL_SCISSOR_TEST ) \n 
glMatrixMode ( GL_MODELVIEW ) \n 
glLoadIdentity ( ) \n 
glEnable ( GL_LIGHTING ) \n 
glEnable ( GL_LIGHT0 ) \n 
glEnable ( GL_BLEND ) \n 
glClearColor ( * background_color ) \n 
~~ def setup_overlay2D ( x , y , width , height ) : \n 
glMatrixMode ( GL_PROJECTION ) \n 
glScissor ( x , y , width , height ) \n 
glViewport ( x , y , width , height ) \n 
glOrtho ( x , x + width , y , y + height , - 1 , 1 ) \n 
~~ cyltrigs = [ 0.587785 , 0.809017 , 0.951057 , 0.309017 ] ; \n 
def notGlePolyCylinder ( points , color , radius ) : \n 
~~~ trigs = [ radius * x for x in cyltrigs ] ; \n 
if abs ( points [ 1 ] [ 0 ] - points [ 2 ] [ 0 ] ) > 1e-6 : \n 
~~~ with glSection ( GL_QUAD_STRIP ) : \n 
~~~ glNormal3f ( 0 , 0 , 1. ) \n 
glVertex ( points [ 1 ] [ 0 ] , 0 , radius ) \n 
glVertex ( points [ 2 ] [ 0 ] , 0 , radius ) \n 
glNormal3f ( 0 , cyltrigs [ 0 ] , cyltrigs [ 1 ] ) \n 
glVertex ( points [ 1 ] [ 0 ] , trigs [ 0 ] , trigs [ 1 ] ) \n 
glVertex ( points [ 2 ] [ 0 ] , trigs [ 0 ] , trigs [ 1 ] ) \n 
glNormal3f ( 0 , cyltrigs [ 2 ] , cyltrigs [ 3 ] ) \n 
glVertex ( points [ 1 ] [ 0 ] , trigs [ 2 ] , trigs [ 3 ] ) \n 
glVertex ( points [ 2 ] [ 0 ] , trigs [ 2 ] , trigs [ 3 ] ) \n 
glNormal3f ( 0 , cyltrigs [ 2 ] , - cyltrigs [ 3 ] ) \n 
glVertex ( points [ 1 ] [ 0 ] , trigs [ 2 ] , - trigs [ 3 ] ) \n 
glVertex ( points [ 2 ] [ 0 ] , trigs [ 2 ] , - trigs [ 3 ] ) \n 
glNormal3f ( 0 , cyltrigs [ 0 ] , - cyltrigs [ 1 ] ) \n 
glVertex ( points [ 1 ] [ 0 ] , trigs [ 0 ] , - trigs [ 1 ] ) \n 
glVertex ( points [ 2 ] [ 0 ] , trigs [ 0 ] , - trigs [ 1 ] ) \n 
glNormal3f ( 0 , 0 , - 1. ) \n 
glVertex ( points [ 1 ] [ 0 ] , 0 , - radius ) \n 
glVertex ( points [ 2 ] [ 0 ] , 0 , - radius ) \n 
glNormal3f ( 0 , - cyltrigs [ 0 ] , - cyltrigs [ 1 ] ) \n 
glVertex ( points [ 1 ] [ 0 ] , - trigs [ 0 ] , - trigs [ 1 ] ) \n 
glVertex ( points [ 2 ] [ 0 ] , - trigs [ 0 ] , - trigs [ 1 ] ) \n 
glNormal3f ( 0 , - cyltrigs [ 2 ] , - cyltrigs [ 3 ] ) \n 
glVertex ( points [ 1 ] [ 0 ] , - trigs [ 2 ] , - trigs [ 3 ] ) \n 
glVertex ( points [ 2 ] [ 0 ] , - trigs [ 2 ] , - trigs [ 3 ] ) \n 
glNormal3f ( 0 , - cyltrigs [ 2 ] , cyltrigs [ 3 ] ) \n 
glVertex ( points [ 1 ] [ 0 ] , - trigs [ 2 ] , trigs [ 3 ] ) \n 
glVertex ( points [ 2 ] [ 0 ] , - trigs [ 2 ] , trigs [ 3 ] ) \n 
glNormal3f ( 0 , - cyltrigs [ 0 ] , cyltrigs [ 1 ] ) \n 
glVertex ( points [ 1 ] [ 0 ] , - trigs [ 0 ] , trigs [ 1 ] ) \n 
glVertex ( points [ 2 ] [ 0 ] , - trigs [ 0 ] , trigs [ 1 ] ) \n 
glNormal3f ( 0 , 0 , 1. ) \n 
~~ ~~ elif abs ( points [ 1 ] [ 1 ] - points [ 2 ] [ 1 ] ) > 1e-6 : \n 
~~~ p1 = points [ 1 ] [ 1 ] \n 
p2 = points [ 2 ] [ 1 ] \n 
with glSection ( GL_QUAD_STRIP ) : \n 
glVertex ( 0 , p1 , radius ) \n 
glVertex ( 0 , p2 , radius ) \n 
glNormal3f ( cyltrigs [ 0 ] , 0 , cyltrigs [ 1 ] ) \n 
glVertex ( trigs [ 0 ] , p1 , trigs [ 1 ] ) \n 
glVertex ( trigs [ 0 ] , p2 , trigs [ 1 ] ) \n 
glNormal3f ( cyltrigs [ 2 ] , 0 , cyltrigs [ 3 ] ) \n 
glVertex ( trigs [ 2 ] , p1 , trigs [ 3 ] ) \n 
glVertex ( trigs [ 2 ] , p2 , trigs [ 3 ] ) \n 
glNormal3f ( cyltrigs [ 2 ] , 0 , - cyltrigs [ 3 ] ) \n 
glVertex ( trigs [ 2 ] , p1 , - trigs [ 3 ] ) \n 
glVertex ( trigs [ 2 ] , p2 , - trigs [ 3 ] ) \n 
glNormal3f ( cyltrigs [ 0 ] , 0 , - cyltrigs [ 1 ] ) \n 
glVertex ( trigs [ 0 ] , p1 , - trigs [ 1 ] ) \n 
glVertex ( trigs [ 0 ] , p2 , - trigs [ 1 ] ) \n 
glVertex ( 0 , p1 , - radius ) \n 
glVertex ( 0 , p2 , - radius ) \n 
glNormal3f ( - cyltrigs [ 0 ] , 0 , - cyltrigs [ 1 ] ) \n 
glVertex ( - trigs [ 0 ] , p1 , - trigs [ 1 ] ) \n 
glVertex ( - trigs [ 0 ] , p2 , - trigs [ 1 ] ) \n 
glNormal3f ( - cyltrigs [ 2 ] , 0 , - cyltrigs [ 3 ] ) \n 
glVertex ( - trigs [ 2 ] , p1 , - trigs [ 3 ] ) \n 
glVertex ( - trigs [ 2 ] , p2 , - trigs [ 3 ] ) \n 
glNormal3f ( - cyltrigs [ 2 ] , 0 , cyltrigs [ 3 ] ) \n 
glVertex ( - trigs [ 2 ] , p1 , trigs [ 3 ] ) \n 
glVertex ( - trigs [ 2 ] , p2 , trigs [ 3 ] ) \n 
glNormal3f ( - cyltrigs [ 0 ] , 0 , cyltrigs [ 1 ] ) \n 
glVertex ( - trigs [ 0 ] , p1 , trigs [ 1 ] ) \n 
glVertex ( - trigs [ 0 ] , p2 , trigs [ 1 ] ) \n 
~~~ p1 = points [ 1 ] [ 2 ] \n 
p2 = points [ 2 ] [ 2 ] \n 
~~~ glNormal3f ( 0 , 1. , 0 ) \n 
glVertex ( 0 , radius , p1 ) \n 
glVertex ( 0 , radius , p2 ) \n 
glNormal3f ( cyltrigs [ 0 ] , cyltrigs [ 1 ] , 0 ) \n 
glVertex ( trigs [ 0 ] , trigs [ 1 ] , p1 ) \n 
glVertex ( trigs [ 0 ] , trigs [ 1 ] , p2 ) \n 
glNormal3f ( cyltrigs [ 2 ] , cyltrigs [ 3 ] , 0 ) \n 
glVertex ( trigs [ 2 ] , trigs [ 3 ] , p1 ) \n 
glVertex ( trigs [ 2 ] , trigs [ 3 ] , p2 ) \n 
glNormal3f ( cyltrigs [ 2 ] , - cyltrigs [ 3 ] , 0 ) \n 
glVertex ( trigs [ 2 ] , - trigs [ 3 ] , p1 ) \n 
glVertex ( trigs [ 2 ] , - trigs [ 3 ] , p2 ) \n 
glNormal3f ( cyltrigs [ 0 ] , - cyltrigs [ 1 ] , 0 ) \n 
glVertex ( trigs [ 0 ] , - trigs [ 1 ] , p1 ) \n 
glVertex ( trigs [ 0 ] , - trigs [ 1 ] , p2 ) \n 
glNormal3f ( 0 , - 1. , 0 ) \n 
glVertex ( 0 , - radius , p1 ) \n 
glVertex ( 0 , - radius , p2 ) \n 
glNormal3f ( - cyltrigs [ 0 ] , - cyltrigs [ 1 ] , 0 ) \n 
glVertex ( - trigs [ 0 ] , - trigs [ 1 ] , p1 ) \n 
glVertex ( - trigs [ 0 ] , - trigs [ 1 ] , p2 ) \n 
glNormal3f ( - cyltrigs [ 2 ] , - cyltrigs [ 3 ] , 0 ) \n 
glVertex ( - trigs [ 2 ] , - trigs [ 3 ] , p1 ) \n 
glVertex ( - trigs [ 2 ] , - trigs [ 3 ] , p2 ) \n 
glNormal3f ( - cyltrigs [ 2 ] , cyltrigs [ 3 ] , 0 ) \n 
glVertex ( - trigs [ 2 ] , trigs [ 3 ] , p1 ) \n 
glVertex ( - trigs [ 2 ] , trigs [ 3 ] , p2 ) \n 
glNormal3f ( - cyltrigs [ 0 ] , cyltrigs [ 1 ] , 0 ) \n 
glVertex ( - trigs [ 0 ] , trigs [ 1 ] , p1 ) \n 
glVertex ( - trigs [ 0 ] , trigs [ 1 ] , p2 ) \n 
glNormal3f ( 0 , 1. , 0 ) \n 
~~ ~~ ~~ def notGlutSolidCube ( size ) : \n 
~~~ p = size / 2 \n 
n = - 1 * p \n 
glVertex ( n , p , n ) \n 
glVertex ( n , n , n ) \n 
glVertex ( p , n , n ) \n 
glVertex ( p , p , n ) \n 
glVertex ( n , p , p ) \n 
glVertex ( p , p , p ) \n 
~~~ glNormal3f ( 1. , 0 , 0 ) \n 
glVertex ( p , n , p ) \n 
~~~ glNormal3f ( 0 , 0 , - 1. ) \n 
glVertex ( n , n , p ) \n 
~~~ glNormal3f ( 0 , - 1. , 0 ) \n 
~~~ glNormal3f ( - 1. , 0 , 0 ) \n 
~~ ~~ class DisplayList ( object ) : \n 
def __init__ ( self , renderFunction ) : \n 
~~~ self . renderFunction = renderFunction \n 
self . needsUpdate = True \n 
self . listId = None \n 
~~ def update ( self ) : \n 
~~~ self . needsUpdate = True \n 
~~ def __call__ ( self , * args ) : \n 
~~~ if self . needsUpdate : \n 
~~~ if self . listId : \n 
~~~ glDeleteLists ( self . listId , 1 ) \n 
~~ self . listId = glGenLists ( 1 ) \n 
glNewList ( self . listId , GL_COMPILE_AND_EXECUTE ) \n 
self . renderFunction ( * args ) \n 
glEndList ( ) \n 
self . needsUpdate = False \n 
~~~ glCallList ( self . listId ) \n 
from lantz import Q_ \n 
from lantz . drivers . examples . dummydrivers import DummyOsci , DummyFunGen , DummyShutter \n 
from myapps import AmplitudeScannerShutter \n 
fungen = DummyFunGen ( ) \n 
osci = DummyOsci ( ) \n 
shutter = DummyShutter ( ) \n 
with AmplitudeScannerShutter ( fungen = fungen , osci = osci , shutter = shutter ) as app : \n 
data = list ( app . scan_amplitude ( Q_ ( range ( 1 , 10 ) , ) ) ) \n 
~~ print ( data ) \n 
from . cobolt0601 import Cobolt0601 \n 
import warnings \n 
from . import Q_ \n 
from . log import LOGGER as _LOG \n 
from stringparser import Parser \n 
class DimensionalityWarning ( Warning ) : \n 
~~ def _do_nothing ( value ) : \n 
~~ def _getitem ( a , b ) : \n 
~~~ return a [ b ] \n 
~~~ return a [ type ( b ) ] \n 
~~ ~~ getitem = _getitem \n 
def convert_to ( units , on_dimensionless = , on_incompatible = , \n 
return_float = False ) : \n 
if on_dimensionless not in ( , , ) : \n 
~~ if isinstance ( units , str ) : \n 
~~~ units = Q_ ( 1 , units ) \n 
~~ elif not isinstance ( units , Q_ ) : \n 
~~ if return_float : \n 
~~~ def _inner ( value ) : \n 
~~~ if isinstance ( value , Q_ ) : \n 
~~~ return value . to ( units ) . magnitude \n 
~~ except ValueError as e : \n 
~~~ if on_incompatible == : \n 
~~~ raise ValueError ( e ) \n 
~~ elif on_incompatible == : \n 
~~~ msg = . format ( value , units warnings . warn ( msg , DimensionalityWarning ) \n 
_LOG . warn ( msg ) \n 
~~ ~~ return value . magnitude \n 
~~~ if not units . dimensionless : \n 
~~~ if on_dimensionless == : \n 
~~~ raise ValueError ( . format ( value , units ) ) \n 
~~ elif on_dimensionless == : \n 
~~~ msg = . format ( value , units ) \n 
warnings . warn ( msg , DimensionalityWarning ) \n 
~~ ~~ return float ( value ) \n 
~~ ~~ return _inner \n 
~~~ return value . to ( units ) \n 
~~ ~~ return float ( value . magnitude ) * units \n 
~~ ~~ return float ( value ) * units \n 
~~ ~~ class Processor ( object ) : \n 
def __new__ ( cls , processors ) : \n 
~~~ if isinstance ( processors , ( tuple , list ) ) : \n 
~~~ if len ( processors ) > 1 : \n 
~~~ inst = super ( ) . __new__ ( cls ) \n 
inst . processors = tuple ( cls . _to_callable ( processor ) \n 
for processor in processors ) \n 
return inst \n 
~~~ return cls . _to_callable ( processors [ 0 ] ) \n 
~~~ return cls . _to_callable ( processors ) \n 
~~ ~~ def __call__ ( self , values ) : \n 
~~~ return tuple ( processor ( value ) \n 
for processor , value in zip ( self . processors , values ) ) \n 
def _to_callable ( cls , obj ) : \n 
~~~ if callable ( obj ) : \n 
~~~ return obj \n 
~~ if obj is None : \n 
~~~ return _do_nothing \n 
~~ return cls . to_callable ( obj ) \n 
def to_callable ( cls , obj ) : \n 
~~~ raise TypeError ( . format ( obj ) ) \n 
~~~ if isinstance ( self . processors , tuple ) : \n 
~~~ return len ( self . processors ) \n 
~~ return 1 \n 
~~ ~~ class FromQuantityProcessor ( Processor ) : \n 
~~~ if isinstance ( obj , ( str , Q_ ) ) : \n 
~~~ return convert_to ( obj , return_float = True ) \n 
~~ raise TypeError ( \n 
. format ( obj ) ) \n 
~~ ~~ class ToQuantityProcessor ( Processor ) : \n 
~~~ return convert_to ( obj , on_dimensionless = ) \n 
~~ ~~ class ParseProcessor ( Processor ) : \n 
~~~ if isinstance ( obj , str ) : \n 
~~~ return Parser ( obj ) \n 
~~ ~~ class MapProcessor ( Processor ) : \n 
~~~ if isinstance ( obj , dict ) : \n 
~~~ return get_mapping ( obj ) \n 
~~ if isinstance ( obj , set ) : \n 
~~~ return check_membership ( obj ) \n 
~~ ~~ class ReverseMapProcessor ( Processor ) : \n 
__reversed_cache = { } \n 
~~~ obj = cls . __reversed_cache . setdefault ( id ( obj ) , \n 
{ value : key for key , value \n 
in obj . items ( ) } ) \n 
return get_mapping ( obj ) \n 
~~ ~~ class RangeProcessor ( Processor ) : \n 
~~~ if not isinstance ( obj , ( list , tuple ) ) : \n 
~~ if not len ( obj ) in ( 1 , 2 , 3 ) : \n 
. format ( len ( obj ) ) ) \n 
~~ if len ( obj ) == 1 : \n 
~~~ return check_range_and_coerce_step ( 0 , * obj ) \n 
~~ return check_range_and_coerce_step ( * obj ) \n 
~~ ~~ def check_range_and_coerce_step ( low , high , step = None ) : \n 
def _inner ( value ) : \n 
~~~ if not ( low <= value <= high ) : \n 
~~~ raise ValueError ( . format ( value , low , high ) ) \n 
~~ if step : \n 
~~~ value = round ( ( value - low ) / step ) * step + low \n 
~~ return value \n 
~~ return _inner \n 
~~ def check_membership ( container ) : \n 
~~~ if value not in container : \n 
~~~ raise ValueError ( . format ( value , container ) ) \n 
~~ def get_mapping ( container ) : \n 
def _inner ( key ) : \n 
~~~ if key not in container : \n 
~~ return container [ key ] \n 
~~~ from setuptools import setup \n 
sys . exit ( 1 ) \n 
def read ( filename ) : \n 
~~~ return codecs . open ( filename , encoding = ) . read ( ) \n 
~~ long_description = . join ( [ read ( ) , \n 
read ( ) , \n 
read ( ) ] ) \n 
__doc__ = long_description \n 
requirements = [ ] \n 
if sys . version_info < ( 3 , 4 ) : \n 
~~~ requirements . append ( ) \n 
~~ root_folder = os . path . dirname ( os . path . abspath ( __file__ ) ) \n 
folder = os . path . join ( root_folder , , ) \n 
paths = os . listdir ( folder ) \n 
companies = [ path for path in paths \n 
if os . path . isdir ( os . path . join ( folder , path ) ) \n 
and os . path . exists ( os . path . join ( folder , path , ) ) ] \n 
folder = os . path . join ( root_folder , , , ) \n 
legacy_companies = [ path for path in paths \n 
setup ( name = , \n 
packages = [ , \n 
] + \n 
[ + company for company in companies ] + \n 
[ + company for company in legacy_companies ] , \n 
test_suite = , \n 
install_requires = [ , \n 
] + requirements , \n 
platforms = , \n 
scripts = [ , \n 
from learnpy . Problem import Problem \n 
pro = Problem ( "BinaryClassification" , "./data/iris_training.csv" ) \n 
pro . set_label ( ) \n 
pro . set_model ( "NaiveBayes" ) \n 
pro . model . fit ( None ) \n 
pro . set_testing ( "./data/iris_testing.csv" ) \n 
pro . predict ( ) \n 
pro2 = Problem ( "BinaryClassification" , "./data/lin_training.csv" ) \n 
pro2 . set_label ( ) \n 
pro2 . set_model ( "NaiveBayes" ) \n 
pro2 . model . fit ( None ) \n 
pro2 . set_testing ( "./data/lin_testing.csv" ) \n 
pro2 . predict ( ) from collections import OrderedDict \n 
from . . import utils \n 
"Layer" , \n 
"MergeLayer" , \n 
class Layer ( object ) : \n 
def __init__ ( self , incoming , name = None ) : \n 
~~~ if isinstance ( incoming , tuple ) : \n 
~~~ self . input_shape = incoming \n 
self . input_layer = None \n 
~~~ self . input_shape = incoming . output_shape \n 
self . input_layer = incoming \n 
~~ self . name = name \n 
self . params = OrderedDict ( ) \n 
self . get_output_kwargs = [ ] \n 
if any ( d is not None and d <= 0 for d in self . input_shape ) : \n 
self . input_shape , self . name ) ) \n 
def output_shape ( self ) : \n 
~~~ shape = self . get_output_shape_for ( self . input_shape ) \n 
if any ( isinstance ( s , T . Variable ) for s in shape ) : \n 
"dimensions." % ( self . __class__ . __name__ , shape ) ) \n 
~~ return shape \n 
~~ def get_params ( self , ** tags ) : \n 
result = list ( self . params . keys ( ) ) \n 
only = set ( tag for tag , value in tags . items ( ) if value ) \n 
if only : \n 
~~~ result = [ param for param in result \n 
if not ( only - self . params [ param ] ) ] \n 
~~ exclude = set ( tag for tag , value in tags . items ( ) if not value ) \n 
if exclude : \n 
if not ( self . params [ param ] & exclude ) ] \n 
~~ return utils . collect_shared_vars ( result ) \n 
~~ def get_output_shape_for ( self , input_shape ) : \n 
return input_shape \n 
~~ def get_output_for ( self , input , ** kwargs ) : \n 
raise NotImplementedError \n 
~~ def add_param ( self , spec , shape , name = None , ** tags ) : \n 
if name is not None : \n 
~~~ if self . name is not None : \n 
~~~ name = "%s.%s" % ( self . name , name ) \n 
~~ ~~ param = utils . create_param ( spec , shape , name ) \n 
tags [ ] = tags . get ( , True ) \n 
self . params [ param ] = set ( tag for tag , value in tags . items ( ) if value ) \n 
return param \n 
~~ ~~ class MergeLayer ( Layer ) : \n 
def __init__ ( self , incomings , name = None ) : \n 
~~~ self . input_shapes = [ incoming if isinstance ( incoming , tuple ) \n 
else incoming . output_shape \n 
for incoming in incomings ] \n 
self . input_layers = [ None if isinstance ( incoming , tuple ) \n 
else incoming \n 
self . name = name \n 
~~ @ Layer . output_shape . getter \n 
~~~ shape = self . get_output_shape_for ( self . input_shapes ) \n 
~~ def get_output_shape_for ( self , input_shapes ) : \n 
~~ def get_output_for ( self , inputs , ** kwargs ) : \n 
~~ ~~ from mock import Mock \n 
import numpy \n 
import theano \n 
class TestAutocrop : \n 
~~~ def test_autocrop_array_shapes ( self ) : \n 
~~~ from lasagne . layers . merge import autocrop_array_shapes \n 
crop0 = None \n 
crop1 = [ None , , , ] \n 
crop2 = [ , ] \n 
crop_bad = [ , , , ] \n 
assert autocrop_array_shapes ( \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop0 ) == [ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop1 ) == [ ( 1 , 2 , 3 , 2 ) , ( 5 , 2 , 3 , 2 ) , ( 5 , 2 , 3 , 2 ) ] \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop2 ) == [ ( 1 , 2 , 3 , 4 ) , ( 1 , 2 , 7 , 8 ) , ( 1 , 2 , 3 , 2 ) ] \n 
with pytest . raises ( ValueError ) : \n 
~~~ autocrop_array_shapes ( \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop_bad ) \n 
~~ with pytest . raises ( ValueError ) : \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 ) , ( 5 , 4 , 3 , 2 , 10 ) ] , crop1 ) \n 
~~ ~~ def test_crop_inputs ( self ) : \n 
~~~ from lasagne . layers . merge import autocrop \n 
from numpy . testing import assert_array_equal \n 
crop_0 = None \n 
crop_1 = [ None , , , ] \n 
crop_l = [ , , , ] \n 
crop_c = [ , , , ] \n 
crop_u = [ , , , ] \n 
crop_x = [ , ] \n 
x0 = numpy . random . random ( ( 2 , 3 , 5 , 7 ) ) \n 
x1 = numpy . random . random ( ( 1 , 2 , 3 , 4 ) ) \n 
x2 = numpy . random . random ( ( 6 , 3 , 4 , 2 ) ) \n 
def crop_test ( cropping , inputs , expected ) : \n 
~~~ inputs = [ theano . shared ( x ) for x in inputs ] \n 
outs = autocrop ( inputs , cropping ) \n 
outs = [ o . eval ( ) for o in outs ] \n 
assert len ( outs ) == len ( expected ) \n 
for o , e in zip ( outs , expected ) : \n 
~~~ assert_array_equal ( o , e ) \n 
~~ ~~ crop_test ( crop_0 , [ x0 , x1 ] , \n 
[ x0 , x1 ] ) \n 
crop_test ( crop_1 , [ x0 , x1 ] , \n 
[ x0 [ : , : 2 , 1 : 4 , 3 : ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_l , [ x0 , x1 ] , \n 
[ x0 [ : 1 , : 2 , : 3 , : 4 ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_c , [ x0 , x1 ] , \n 
[ x0 [ : 1 , : 2 , 1 : 4 , 1 : 5 ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_u , [ x0 , x1 ] , \n 
[ x0 [ 1 : , 1 : , 2 : , 3 : ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_0 , [ x0 , x2 ] , \n 
[ x0 , x2 ] ) \n 
crop_test ( crop_1 , [ x0 , x2 ] , \n 
[ x0 [ : , : , : 4 , 5 : ] , x2 [ : , : , : , : ] ] ) \n 
crop_test ( crop_l , [ x0 , x2 ] , \n 
[ x0 [ : , : , : 4 , : 2 ] , x2 [ : 2 , : , : , : ] ] ) \n 
crop_test ( crop_c , [ x0 , x2 ] , \n 
[ x0 [ : , : , : 4 , 2 : 4 ] , x2 [ 2 : 4 , : , : , : ] ] ) \n 
crop_test ( crop_u , [ x0 , x2 ] , \n 
[ x0 [ : , : , 1 : , 5 : ] , x2 [ 4 : , : , : , : ] ] ) \n 
crop_test ( crop_0 , [ x0 , x1 , x2 ] , \n 
[ x0 , x1 , x2 ] ) \n 
crop_test ( crop_1 , [ x0 , x1 , x2 ] , \n 
[ x0 [ : , : 2 , 1 : 4 , 5 : ] , x1 [ : , : , : , 2 : ] , x2 [ : , : 2 , : 3 , : ] ] ) \n 
crop_test ( crop_l , [ x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , : 3 , : 2 ] , x1 [ : , : , : , : 2 ] , x2 [ : 1 , : 2 , : 3 , : ] ] ) \n 
crop_test ( crop_c , [ x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , 1 : 4 , 2 : 4 ] , x1 [ : , : , : , 1 : 3 ] , x2 [ 2 : 3 , : 2 , : 3 , : ] ] ) \n 
crop_test ( crop_u , [ x0 , x1 , x2 ] , \n 
[ x0 [ 1 : , 1 : , 2 : , 5 : ] , x1 [ : , : , : , 2 : ] , x2 [ 5 : , 1 : , 1 : , : ] ] ) \n 
crop_test ( crop_x , [ x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , : , : ] , x1 [ : 1 , : 2 , : , : ] , x2 [ : 1 , : 2 , : , : ] ] ) \n 
crop_test ( crop_x , [ x0 , x1 , x2 , x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , : , : ] , x1 [ : 1 , : 2 , : , : ] , x2 [ : 1 , : 2 , : , : ] , \n 
x0 [ : 1 , : 2 , : , : ] , x1 [ : 1 , : 2 , : , : ] , x2 [ : 1 , : 2 , : , : ] ] ) \n 
~~~ crop_test ( crop_bad , [ x0 , x1 , x2 ] , \n 
~~~ crop_test ( crop_bad , [ x0 [ : , : , : , 0 ] , x1 , x2 [ : , : , : , : , None ] ] , \n 
~~ ~~ ~~ class TestConcatLayer : \n 
~~~ @ pytest . fixture \n 
def layer ( self ) : \n 
~~~ from lasagne . layers . merge import ConcatLayer \n 
return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = 1 ) \n 
def crop_layer_0 ( self ) : \n 
return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = 0 , \n 
cropping = [ ] * 2 ) \n 
def crop_layer_1 ( self ) : \n 
return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = 1 , \n 
~~ def test_get_output_shape_for ( self , layer ) : \n 
~~~ assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , 5 ) ] ) == ( 3 , 7 ) \n 
assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , None ) ] ) == ( 3 , None ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 5 ) ] ) == ( 3 , 7 ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( None , 5 ) ] ) == ( None , 7 ) \n 
~~~ layer . get_output_shape_for ( [ ( 4 , None ) , ( 3 , 5 ) ] ) \n 
~~~ layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 4 , None ) ] ) \n 
~~~ layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 5 ) , ( 4 , 5 ) ] ) \n 
~~ ~~ def test_get_output_shape_for_cropped ( self , crop_layer_0 , crop_layer_1 ) : \n 
~~~ input_shapes = [ ( 3 , 2 ) , ( 4 , 5 ) ] \n 
result_0 = crop_layer_0 . get_output_shape_for ( input_shapes ) \n 
result_1 = crop_layer_1 . get_output_shape_for ( input_shapes ) \n 
assert result_0 == ( 7 , 2 ) \n 
assert result_1 == ( 3 , 7 ) \n 
~~ def test_get_output_for ( self , layer ) : \n 
~~~ inputs = [ theano . shared ( numpy . ones ( ( 3 , 3 ) ) ) , \n 
theano . shared ( numpy . ones ( ( 3 , 2 ) ) ) ] \n 
result = layer . get_output_for ( inputs ) \n 
result_eval = result . eval ( ) \n 
desired_result = numpy . hstack ( [ input . get_value ( ) for input in inputs ] ) \n 
assert ( result_eval == desired_result ) . all ( ) \n 
~~ def test_get_output_for_cropped ( self , crop_layer_0 , crop_layer_1 ) : \n 
~~~ x0 = numpy . random . random ( ( 5 , 3 ) ) \n 
x1 = numpy . random . random ( ( 4 , 2 ) ) \n 
inputs = [ theano . shared ( x0 ) , \n 
theano . shared ( x1 ) ] \n 
result_0 = crop_layer_0 . get_output_for ( inputs ) . eval ( ) \n 
result_1 = crop_layer_1 . get_output_for ( inputs ) . eval ( ) \n 
desired_result_0 = numpy . concatenate ( [ x0 [ : , : 2 ] , x1 [ : , : 2 ] ] , axis = 0 ) \n 
desired_result_1 = numpy . concatenate ( [ x0 [ : 4 , : ] , x1 [ : 4 , : ] ] , axis = 1 ) \n 
assert ( result_0 == desired_result_0 ) . all ( ) \n 
assert ( result_1 == desired_result_1 ) . all ( ) \n 
~~ ~~ class TestElemwiseSumLayer : \n 
~~~ from lasagne . layers . merge import ElemwiseSumLayer \n 
return ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ 2 , - 1 ] ) \n 
def crop_layer ( self ) : \n 
return ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ 2 , - 1 ] , \n 
~~~ assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , 2 ) ] ) == ( 3 , 2 ) \n 
assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , None ) ] ) == ( 3 , 2 ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 2 ) ] ) == ( 3 , 2 ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( None , 2 ) ] ) == ( None , 2 ) \n 
~~~ layer . get_output_shape_for ( [ ( 3 , None ) , ( 4 , 2 ) ] ) \n 
~~~ layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 2 ) , ( 4 , 2 ) ] ) \n 
~~ ~~ def test_get_output_for ( self , layer ) : \n 
~~~ a = numpy . array ( [ [ 0 , 1 ] , [ 2 , 3 ] ] ) \n 
b = numpy . array ( [ [ 1 , 2 ] , [ 4 , 5 ] ] ) \n 
inputs = [ theano . shared ( a ) , \n 
theano . shared ( b ) ] \n 
desired_result = 2 * a - b \n 
~~ def test_get_output_for_cropped ( self , crop_layer ) : \n 
~~~ from numpy . testing import assert_array_almost_equal as aeq \n 
x0 = numpy . random . random ( ( 5 , 3 ) ) \n 
result = crop_layer . get_output_for ( inputs ) . eval ( ) \n 
desired_result = 2 * x0 [ : 4 , : 2 ] - x1 [ : 4 , : 2 ] \n 
aeq ( result , desired_result ) \n 
~~ def test_bad_coeffs_fails ( self , layer ) : \n 
~~~ ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ 2 , 3 , - 1 ] ) \n 
~~ ~~ ~~ class TestElemwiseMergeLayerMul : \n 
~~~ import theano . tensor as T \n 
from lasagne . layers . merge import ElemwiseMergeLayer \n 
return ElemwiseMergeLayer ( [ Mock ( ) , Mock ( ) ] , merge_function = T . mul ) \n 
desired_result = a * b \n 
~~ ~~ class TestElemwiseMergeLayerMaximum : \n 
return ElemwiseMergeLayer ( [ Mock ( ) , Mock ( ) ] , merge_function = T . maximum ) \n 
desired_result = numpy . maximum ( a , b ) \n 
~~ ~~ from gevent import monkey ; monkey . patch_all ( ) \n 
import gevent \n 
from ws4py . client . geventclient import WebSocketClient \n 
~~~ ws = WebSocketClient ( , protocols = [ , ] ) \n 
ws . connect ( ) \n 
print ( ( ws . receive ( ) , ) ) \n 
def incoming ( ) : \n 
~~~ while True : \n 
~~~ m = ws . receive ( ) \n 
if m is not None : \n 
~~~ m = str ( m ) \n 
print ( ( m , len ( m ) ) ) \n 
if len ( m ) == 35 : \n 
~~~ ws . close ( ) \n 
~~ def outgoing ( ) : \n 
~~~ for i in range ( 0 , 40 , 5 ) : \n 
~~~ ws . send ( "*" * i ) \n 
~~ ws . send ( "Foobar" ) \n 
~~ greenlets = [ \n 
gevent . spawn ( incoming ) , \n 
gevent . spawn ( outgoing ) , \n 
gevent . joinall ( greenlets ) \n 
from ws4py . framing import Frame , OPCODE_CONTINUATION , OPCODE_TEXT , OPCODE_BINARY , OPCODE_CLOSE , OPCODE_PING , OPCODE_PONG \n 
from ws4py . compat import unicode , py3k \n 
__all__ = [ , , , , \n 
, ] \n 
class Message ( object ) : \n 
~~~ def __init__ ( self , opcode , data = , encoding = ) : \n 
self . opcode = opcode \n 
self . _completed = False \n 
self . encoding = encoding \n 
if isinstance ( data , unicode ) : \n 
~~~ if not encoding : \n 
~~ data = data . encode ( encoding ) \n 
~~ elif isinstance ( data , bytearray ) : \n 
~~~ data = bytes ( data ) \n 
~~ elif not isinstance ( data , bytes ) : \n 
~~ self . data = data \n 
~~ def single ( self , mask = False ) : \n 
mask = os . urandom ( 4 ) if mask else None \n 
return Frame ( body = self . data , opcode = self . opcode , \n 
masking_key = mask , fin = 1 ) . build ( ) \n 
~~ def fragment ( self , first = False , last = False , mask = False ) : \n 
fin = 1 if last is True else 0 \n 
opcode = self . opcode if first is True else OPCODE_CONTINUATION \n 
return Frame ( body = self . data , \n 
opcode = opcode , masking_key = mask , \n 
fin = fin ) . build ( ) \n 
def completed ( self ) : \n 
return self . _completed \n 
~~ @ completed . setter \n 
def completed ( self , state ) : \n 
self . _completed = state \n 
~~ def extend ( self , data ) : \n 
if isinstance ( data , bytes ) : \n 
~~~ self . data += data \n 
~~~ self . data += bytes ( data ) \n 
~~ elif isinstance ( data , unicode ) : \n 
~~~ self . data += data . encode ( self . encoding ) \n 
~~~ return len ( self . __unicode__ ( ) ) \n 
~~~ if py3k : \n 
~~~ return self . data . decode ( self . encoding ) \n 
~~ return self . data \n 
~~ def __unicode__ ( self ) : \n 
~~ ~~ class TextMessage ( Message ) : \n 
~~~ def __init__ ( self , text = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_TEXT , text ) \n 
def is_binary ( self ) : \n 
def is_text ( self ) : \n 
~~ ~~ class BinaryMessage ( Message ) : \n 
~~~ def __init__ ( self , bytes = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_BINARY , bytes , encoding = None ) \n 
~~~ return len ( self . data ) \n 
~~ ~~ class CloseControlMessage ( Message ) : \n 
~~~ def __init__ ( self , code = 1000 , reason = ) : \n 
~~~ data = b"" \n 
if code : \n 
~~~ data += struct . pack ( "!H" , code ) \n 
~~ if reason is not None : \n 
~~~ if isinstance ( reason , unicode ) : \n 
~~~ reason = reason . encode ( ) \n 
~~ data += reason \n 
~~ Message . __init__ ( self , OPCODE_CLOSE , data , ) \n 
self . code = code \n 
self . reason = reason \n 
~~~ return self . reason . decode ( ) \n 
~~ return self . reason \n 
~~~ return self . reason . decode ( self . encoding ) \n 
~~ ~~ class PingControlMessage ( Message ) : \n 
~~~ def __init__ ( self , data = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_PING , data ) \n 
~~ ~~ class PongControlMessage ( Message ) : \n 
~~~ def __init__ ( self , data ) : \n 
~~~ Message . __init__ ( self , OPCODE_PONG , data ) \n 
from struct import unpack \n 
from threading import Lock \n 
from binascii import hexlify \n 
from urlparse import urlparse \n 
from mod_python import apache \n 
from PyAuthenNTLM2 . ntlm_dc_proxy import NTLM_DC_Proxy \n 
from PyAuthenNTLM2 . ntlm_ad_proxy import NTLM_AD_Proxy \n 
use_basic_auth = True \n 
~~~ from PyAuthenNTLM2 . ntlm_client import NTLM_Client \n 
~~~ use_basic_auth = False \n 
~~ class CacheConnections : \n 
~~~ self . _mutex = Lock ( ) \n 
self . _cache = { } \n 
~~~ return len ( self . _cache ) \n 
~~ def remove ( self , id ) : \n 
~~~ self . _mutex . acquire ( ) \n 
( proxy , ts ) = self . _cache . get ( id , ( None , None ) ) \n 
if proxy : \n 
~~~ proxy . close ( ) \n 
del self . _cache [ id ] \n 
~~ self . _mutex . release ( ) \n 
~~ def add ( self , id , proxy ) : \n 
self . _cache [ id ] = ( proxy , int ( time . time ( ) ) ) \n 
self . _mutex . release ( ) \n 
~~ def clean ( self ) : \n 
~~~ now = int ( time . time ( ) ) \n 
self . _mutex . acquire ( ) \n 
for id , conn in self . _cache . items ( ) : \n 
~~~ if conn [ 1 ] + 60 < now : \n 
~~~ conn [ 0 ] . close ( ) \n 
~~ ~~ self . _mutex . release ( ) \n 
~~ def has_key ( self , id ) : \n 
~~~ return self . _cache . has_key ( id ) \n 
~~ def get_proxy ( self , id ) : \n 
proxy = self . _cache [ id ] [ 0 ] \n 
return proxy \n 
~~ ~~ class CacheGroups : \n 
~~ def add ( self , group , user ) : \n 
if not self . _cache . has_key ( group ) : \n 
~~~ self . _cache [ group ] = { } \n 
~~ self . _cache [ group ] [ user ] = int ( time . time ( ) ) \n 
old = [ ] \n 
for group , members in self . _cache . items ( ) : \n 
~~~ for user in members : \n 
~~~ if members [ user ] + 3 * 60 * 60 < now : \n 
~~~ old . append ( ( group , user ) ) \n 
~~ ~~ ~~ for group , user in old : \n 
~~~ del self . _cache [ group ] [ user ] \n 
~~ def has ( self , group , user ) : \n 
~~~ if not self . _cache . has_key ( group ) : \n 
~~ return self . _cache [ group ] . has_key ( user ) \n 
~~ ~~ cache = CacheConnections ( ) \n 
cacheGroups = CacheGroups ( ) \n 
def ntlm_message_type ( msg ) : \n 
~~~ if not msg . startswith ( ) or len ( msg ) < 12 : \n 
~~ msg_type = unpack ( , msg [ 8 : 8 + 4 ] ) [ 0 ] \n 
if msg_type not in ( 1 , 2 , 3 ) : \n 
~~ return msg_type \n 
~~ def parse_ntlm_authenticate ( msg ) : \n 
NTLMSSP_NEGOTIATE_UNICODE = 0x00000001 \n 
idx = 28 \n 
length , offset = unpack ( , msg [ idx : idx + 8 ] ) \n 
domain = msg [ offset : offset + length ] \n 
idx += 8 \n 
username = msg [ offset : offset + length ] \n 
idx += 24 \n 
flags = unpack ( , msg [ idx : idx + 4 ] ) [ 0 ] \n 
if flags & NTLMSSP_NEGOTIATE_UNICODE : \n 
~~~ domain = str ( domain . decode ( ) ) \n 
username = str ( username . decode ( ) ) \n 
~~ return username , domain \n 
~~ def set_remote_user ( req , username , domain ) : \n 
~~~ format = req . get_options ( ) . get ( , ) . lower ( ) \n 
if format == : \n 
~~~ req . user = domain + + username \n 
~~~ req . user = username \n 
~~ ~~ def decode_http_authorization_header ( auth ) : \n 
ah = auth . split ( ) \n 
if len ( ah ) == 2 : \n 
~~~ b64 = base64 . b64decode ( ah [ 1 ] ) \n 
if ah [ 0 ] == : \n 
~~~ return ( , b64 ) \n 
~~ elif ah [ 0 ] == and use_basic_auth : \n 
~~~ ( user , password ) = b64 . split ( ) \n 
return ( , user , password ) \n 
~~ def handle_unauthorized ( req ) : \n 
req . err_headers_out . add ( , ) \n 
if use_basic_auth : \n 
~~ req . err_headers_out . add ( , ) \n 
return apache . HTTP_UNAUTHORIZED \n 
~~ def connect_to_proxy ( req , type1 ) : \n 
~~~ domain = req . get_options ( ) [ ] \n 
pdc = req . get_options ( ) [ ] \n 
bdc = req . get_options ( ) . get ( , False ) \n 
~~~ req . log_error ( % str ( e ) , apache . APLOG_CRIT ) raise \n 
~~ ntlm_challenge = None \n 
for server in ( pdc , bdc ) : \n 
~~~ if not server : continue \n 
~~~ if server . startswith ( ) : \n 
~~~ url = urlparse ( server ) \n 
decoded_path = urllib . unquote ( url . path ) [ 1 : ] \n 
proxy = NTLM_AD_Proxy ( url . netloc , domain , base = decoded_path ) \n 
~~~ req . log_error ( ( server , domain ) , apache . APLOG_INFO ) \n 
proxy = NTLM_DC_Proxy ( server , domain ) \n 
~~ ntlm_challenge = proxy . negotiate ( type1 ) \n 
~~~ req . log_error ( % ( server ~~ if ntlm_challenge : break \n 
proxy . close ( ) \n 
~~ return ( proxy , ntlm_challenge ) \n 
~~ def handle_type1 ( req , ntlm_message ) : \n 
cache . remove ( req . connection . id ) \n 
cache . clean ( ) \n 
~~~ ( proxy , ntlm_challenge ) = connect_to_proxy ( req , ntlm_message ) \n 
~~~ return apache . HTTP_INTERNAL_SERVER_ERROR \n 
~~ cache . add ( req . connection . id , proxy ) \n 
~~ def check_authorization ( req , username , proxy ) : \n 
rules = . join ( req . requires ( ) ) . strip ( ) \n 
if rules == or cacheGroups . has ( rules , username ) : \n 
~~ groups = [ ] \n 
for r in req . requires ( ) : \n 
~~~ users = [ u . strip ( ) for u in r [ 5 : ] . split ( "," ) ] \n 
if username in users : \n 
~~~ req . log_error ( % \n 
( username , req . unparsed_uri ) , apache . APLOG_INFO ) \n 
~~~ groups += [ g . strip ( ) for g in r [ 6 : ] . split ( "," ) ] \n 
~~ ~~ if groups : \n 
~~~ res = proxy . check_membership ( username , groups ) \n 
~~~ req . log_error ( ~~ if res : \n 
~~~ cacheGroups . add ( rules , username ) \n 
req . log_error ( % \n 
( username , str ( groups ) , req . unparsed_uri ) , apache . APLOG_INFO ) \n 
~~ req . log_error ( % \n 
( username , str ( groups ) , req . unparsed_uri ) ) \n 
( username , req . unparsed_uri ) ) \n 
~~ def handle_type3 ( req , ntlm_message ) : \n 
proxy = cache . get_proxy ( req . connection . id ) \n 
~~~ user , domain = parse_ntlm_authenticate ( ntlm_message ) \n 
if not domain : \n 
~~~ domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
~~ result = proxy . authenticate ( ntlm_message ) \n 
~~~ req . log_error ( % str ( e ) , apache user , domain = , \n 
~~ if not result : \n 
~~~ cache . remove ( req . connection . id ) \n 
req . log_error ( % ( \n 
domain , user , req . unparsed_uri ) ) \n 
return handle_unauthorized ( req ) \n 
~~ req . log_error ( % ( user , domain , req . unparsed_uri set_remote_user ( req , user , domain ) \n 
result = check_authorization ( req , user , proxy ) \n 
if not result : \n 
~~~ return apache . HTTP_FORBIDDEN \n 
~~ req . connection . notes . add ( , req . user ) \n 
return apache . OK \n 
~~ def handle_basic ( req , user , password ) : \n 
req . log_error ( % ( req . unparsed_uri ) ) \n 
domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
client = NTLM_Client ( user , domain , password ) \n 
type1 = client . make_ntlm_negotiate ( ) \n 
~~~ ( proxy , type2 ) = connect_to_proxy ( req , type1 ) \n 
~~ client . parse_ntlm_challenge ( type2 ) \n 
type3 = client . make_ntlm_authenticate ( ) \n 
if not proxy . authenticate ( type3 ) : \n 
user , domain , req . unparsed_uri ) ) \n 
~~ req . log_error ( % ( user , domain set_remote_user ( req , user , domain ) \n 
~~ req . connection . notes . add ( , user + password ) \n 
~~ def authenhandler ( req ) : \n 
auth_headers = req . headers_in . get ( , [ ] ) \n 
if not isinstance ( auth_headers , list ) : \n 
~~~ auth_headers = [ auth_headers ] \n 
~~ user = req . connection . notes . get ( , None ) \n 
if user : \n 
~~~ req . user = user \n 
if auth_headers : \n 
~~~ req . log_error ( req . connection . id , req . method , req . clength , auth_headers ) , apache . APLOG_INFO ) \n 
if req . method != or req . clength > 0 : \n 
~~~ return apache . OK \n 
~~ ~~ if not auth_headers : \n 
~~~ return handle_unauthorized ( req ) \n 
~~~ for ah in auth_headers : \n 
~~~ ah_data = decode_http_authorization_header ( ah ) \n 
if ah_data : \n 
~~ ~~ ~~ except : \n 
~~~ ah_data = False \n 
~~ if not ah_data : \n 
~~~ req . log_error ( % req . unparsed_uri , apache return apache . HTTP_BAD_REQUEST \n 
~~ if ah_data [ 0 ] == : \n 
~~~ userpwd = req . connection . notes . get ( , None ) \n 
if userpwd : \n 
~~~ if userpwd != ah_data [ 1 ] + ah_data [ 2 ] : \n 
~~ domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
set_remote_user ( req , ah_data [ 1 ] , domain ) \n 
~~ return handle_basic ( req , ah_data [ 1 ] , ah_data [ 2 ] ) \n 
~~~ ntlm_version = ntlm_message_type ( ah_data [ 1 ] ) \n 
if ntlm_version == 1 : \n 
~~~ return handle_type1 ( req , ah_data [ 1 ] ) \n 
~~ if ntlm_version == 3 : \n 
~~~ if cache . has_key ( req . connection . id ) : \n 
~~~ return handle_type3 ( req , ah_data [ 1 ] ) \n 
( req . unparsed_uri ) , apache . APLOG_INFO ) \n 
~~ error = \n 
~~~ error = str ( e ) \n 
( req . unparsed_uri , error ) , apache . APLOG_ERR ) \n 
return apache . HTTP_BAD_REQUEST \n 
~~ from celery import Celery \n 
def create_celery_app ( app ) : \n 
~~~ if app . config . get ( ) : \n 
~~~ app . celery = Celery ( __name__ , broker = app . config [ ] ) \n 
app . celery . conf . update ( app . config ) \n 
taskbase = app . celery . Task \n 
class ContextTask ( taskbase ) : \n 
~~~ abstract = True \n 
def __call__ ( self , * args , ** kwargs ) : \n 
~~~ with app . app_context ( ) : \n 
~~~ return taskbase . __call__ ( self , * args , ** kwargs ) \n 
~~ ~~ ~~ app . celery . Task = ContextTask \n 
sys . path . append ( os . path . dirname ( os . path . realpath ( __file__ ) . rsplit ( , 2 ) [ 0 ] ) ) \n 
from app import create_app \n 
app = create_app ( ) \n 
class TestUsers ( unittest . TestCase ) : \n 
~~~ self . app = app . test_client ( ) \n 
~~ def test_01_add ( self ) : \n 
~~~ rv = self . app . post ( , data = add_data , \n 
content_type = "application/json" ) \n 
assert rv . status_code == 201 \n 
~~ def test_02_read_update ( self ) : \n 
~~~ request = self . app . get ( ) \n 
dict = json . loads ( request . data . decode ( ) ) \n 
id = dict [ ] [ 0 ] [ ] \n 
rv = self . app . patch ( . format ( id ) , \n 
data = update_data , content_type = "application/json" ) \n 
assert rv . status_code == 200 \n 
~~ def test_03_delete ( self ) : \n 
rv = self . app . delete ( . format ( id ) ) \n 
assert rv . status_code == 204 \n 
def tokenProgressFunc ( state = "update" , action = None , text = None , tick = 0 ) : \n 
~~ def build ( \n 
documentPath , \n 
outputUFOFormatVersion = 2 , \n 
roundGeometry = True , \n 
verbose = True , \n 
logPath = None , \n 
progressFunc = None , \n 
from mutatorMath . ufo . document import DesignSpaceDocumentReader \n 
import os , glob \n 
if os . path . isdir ( documentPath ) : \n 
~~~ todo = glob . glob ( os . path . join ( documentPath , "*.designspace" ) ) \n 
~~~ todo = [ documentPath ] \n 
~~ results = [ ] \n 
for path in todo : \n 
~~~ reader = DesignSpaceDocumentReader ( \n 
ufoVersion = outputUFOFormatVersion , \n 
roundGeometry = roundGeometry , \n 
verbose = verbose , \n 
logPath = logPath , \n 
progressFunc = progressFunc \n 
reader . process ( ) \n 
results . append ( reader . results ) \n 
~~ reader = None \n 
return results \n 
~~ from django . core . management . base import BaseCommand \n 
from chronam . core . management . commands import configure_logging \n 
from chronam . core . index import index_pages \n 
configure_logging ( "index_pages_logging.config" , "index_pages.log" ) \n 
~~~ def handle ( self , ** options ) : \n 
~~~ index_pages ( ) \n 
class HttpResponseServiceUnavailable ( HttpResponse ) : \n 
~~~ status_code = 503 \n 
~~ class TooBusyMiddleware ( object ) : \n 
~~~ def process_request ( self , request ) : \n 
~~~ one , five , fifteen = os . getloadavg ( ) \n 
if one > settings . TOO_BUSY_LOAD_AVERAGE : \n 
~~ ~~ from os . path import dirname , join \n 
from django . test import TestCase \n 
from chronam . core . ocr_extractor import ocr_extractor \n 
class OcrExtractorTests ( TestCase ) : \n 
~~~ def test_extractor ( self ) : \n 
~~~ dir = join ( dirname ( dirname ( __file__ ) ) , ) \n 
ocr_file = join ( dir , ) \n 
text , coord_info = ocr_extractor ( ocr_file ) \n 
coords = coord_info [ "coords" ] \n 
expected_text = { "eng" : file ( join ( dir , ) ) . read ( ) . decode ( ) } \n 
self . assertEqual ( text , expected_text ) \n 
self . assertEqual ( len ( coords . keys ( ) ) , 2150 ) \n 
self . assertEqual ( len ( coords [ ] ) , 3 ) \n 
self . assertTrue ( coords . has_key ( ) ) \n 
self . assertTrue ( not coords . has_key ( ) ) \n 
from . tests import * \n 
logger = logging . getLogger ( "human_curl" ) \n 
logger . setLevel ( logging . DEBUG ) \n 
handler = logging . StreamHandler ( ) \n 
handler . setFormatter ( formatter ) \n 
logger . addHandler ( handler ) \n 
import multiprocessing \n 
from PIL import Image \n 
import six . moves . cPickle as pickle \n 
from six . moves import queue \n 
import chainer \n 
from chainer import computational_graph \n 
from chainer import cuda \n 
from chainer import optimizers \n 
from chainer import serializers \n 
parser = argparse . ArgumentParser ( \n 
parser . add_argument ( , help = ) \n 
parser . add_argument ( , , default = , \n 
parser . add_argument ( , , type = int , default = 32 , \n 
parser . add_argument ( , , type = int , default = 250 , \n 
parser . add_argument ( , , default = 10 , type = int , \n 
parser . add_argument ( , , default = - 1 , type = int , \n 
parser . add_argument ( , , default = 20 , type = int , \n 
parser . add_argument ( , default = , \n 
if args . gpu >= 0 : \n 
~~~ cuda . check_cuda_available ( ) \n 
~~ xp = cuda . cupy if args . gpu >= 0 else np \n 
assert 50000 % args . val_batchsize == 0 \n 
def load_image_list ( path , root ) : \n 
~~~ tuples = [ ] \n 
for line in open ( path ) : \n 
~~~ pair = line . strip ( ) . split ( ) \n 
tuples . append ( ( os . path . join ( root , pair [ 0 ] ) , np . int32 ( pair [ 1 ] ) ) ) \n 
~~ return tuples \n 
~~ train_list = load_image_list ( args . train , args . root ) \n 
val_list = load_image_list ( args . val , args . root ) \n 
mean_image = pickle . load ( open ( args . mean , ) ) \n 
if args . arch == : \n 
~~~ import nin \n 
model = nin . NIN ( ) \n 
~~ elif args . arch == : \n 
~~~ import alex \n 
model = alex . Alex ( ) \n 
~~~ import alexbn \n 
model = alexbn . AlexBN ( ) \n 
~~~ import googlenet \n 
model = googlenet . GoogLeNet ( ) \n 
~~~ import googlenetbn \n 
model = googlenetbn . GoogLeNetBN ( ) \n 
~~ if args . gpu >= 0 : \n 
~~~ cuda . get_device ( args . gpu ) . use ( ) \n 
model . to_gpu ( ) \n 
~~ optimizer = optimizers . MomentumSGD ( lr = 0.01 , momentum = 0.9 ) \n 
optimizer . setup ( model ) \n 
if args . initmodel : \n 
~~~ print ( , args . initmodel ) \n 
serializers . load_hdf5 ( args . initmodel , model ) \n 
~~ if args . resume : \n 
~~~ print ( , args . resume ) \n 
serializers . load_hdf5 ( args . resume , optimizer ) \n 
~~ data_q = queue . Queue ( maxsize = 1 ) \n 
res_q = queue . Queue ( ) \n 
cropwidth = 256 - model . insize \n 
def read_image ( path , center = False , flip = False ) : \n 
~~~ image = np . asarray ( Image . open ( path ) ) . transpose ( 2 , 0 , 1 ) \n 
if center : \n 
~~~ top = left = cropwidth / 2 \n 
~~~ top = random . randint ( 0 , cropwidth - 1 ) \n 
left = random . randint ( 0 , cropwidth - 1 ) \n 
~~ bottom = model . insize + top \n 
right = model . insize + left \n 
image = image [ : , top : bottom , left : right ] . astype ( np . float32 ) \n 
image -= mean_image [ : , top : bottom , left : right ] \n 
image /= 255 \n 
if flip and random . randint ( 0 , 1 ) == 0 : \n 
~~~ return image [ : , : , : : - 1 ] \n 
~~~ return image \n 
~~ ~~ def feed_data ( ) : \n 
count = 0 \n 
x_batch = np . ndarray ( \n 
( args . batchsize , 3 , model . insize , model . insize ) , dtype = np . float32 ) \n 
y_batch = np . ndarray ( ( args . batchsize , ) , dtype = np . int32 ) \n 
val_x_batch = np . ndarray ( \n 
( args . val_batchsize , 3 , model . insize , model . insize ) , dtype = np . float32 ) \n 
val_y_batch = np . ndarray ( ( args . val_batchsize , ) , dtype = np . int32 ) \n 
batch_pool = [ None ] * args . batchsize \n 
val_batch_pool = [ None ] * args . val_batchsize \n 
pool = multiprocessing . Pool ( args . loaderjob ) \n 
data_q . put ( ) \n 
for epoch in six . moves . range ( 1 , 1 + args . epoch ) : \n 
~~~ print ( , epoch , file = sys . stderr ) \n 
print ( , optimizer . lr , file = sys . stderr ) \n 
perm = np . random . permutation ( len ( train_list ) ) \n 
for idx in perm : \n 
~~~ path , label = train_list [ idx ] \n 
batch_pool [ i ] = pool . apply_async ( read_image , ( path , False , True ) ) \n 
y_batch [ i ] = label \n 
if i == args . batchsize : \n 
~~~ for j , x in enumerate ( batch_pool ) : \n 
~~~ x_batch [ j ] = x . get ( ) \n 
~~ data_q . put ( ( x_batch . copy ( ) , y_batch . copy ( ) ) ) \n 
i = 0 \n 
~~ count += 1 \n 
if count % 1000 == 0 : \n 
~~~ data_q . put ( ) \n 
j = 0 \n 
for path , label in val_list : \n 
~~~ val_batch_pool [ j ] = pool . apply_async ( \n 
read_image , ( path , True , False ) ) \n 
val_y_batch [ j ] = label \n 
j += 1 \n 
if j == args . val_batchsize : \n 
~~~ for k , x in enumerate ( val_batch_pool ) : \n 
~~~ val_x_batch [ k ] = x . get ( ) \n 
~~ data_q . put ( ( val_x_batch . copy ( ) , val_y_batch . copy ( ) ) ) \n 
~~ ~~ data_q . put ( ) \n 
~~ ~~ optimizer . lr *= 0.97 \n 
~~ pool . close ( ) \n 
pool . join ( ) \n 
~~ def log_result ( ) : \n 
~~~ train_count = 0 \n 
train_cur_loss = 0 \n 
train_cur_accuracy = 0 \n 
begin_at = time . time ( ) \n 
val_begin_at = None \n 
~~~ result = res_q . get ( ) \n 
if result == : \n 
~~~ print ( file = sys . stderr ) \n 
~~ elif result == : \n 
train = True \n 
if val_begin_at is not None : \n 
~~~ begin_at += time . time ( ) - val_begin_at \n 
~~ continue \n 
train = False \n 
val_count = val_loss = val_accuracy = 0 \n 
val_begin_at = time . time ( ) \n 
~~ loss , accuracy = result \n 
if train : \n 
~~~ train_count += 1 \n 
duration = time . time ( ) - begin_at \n 
throughput = train_count * args . batchsize / duration \n 
sys . stderr . write ( \n 
. format ( train_count , train_count * args . batchsize , \n 
datetime . timedelta ( seconds = duration ) , throughput ) ) \n 
train_cur_loss += loss \n 
train_cur_accuracy += accuracy \n 
if train_count % 1000 == 0 : \n 
~~~ mean_loss = train_cur_loss / 1000 \n 
mean_error = 1 - train_cur_accuracy / 1000 \n 
print ( file = sys . stderr ) \n 
print ( json . dumps ( { : , : train_count , \n 
: mean_error , : mean_loss } ) ) \n 
~~~ val_count += args . val_batchsize \n 
duration = time . time ( ) - val_begin_at \n 
throughput = val_count / duration \n 
. format ( val_count / args . val_batchsize , val_count , \n 
val_loss += loss \n 
val_accuracy += accuracy \n 
if val_count == 50000 : \n 
~~~ mean_loss = val_loss * args . val_batchsize / 50000 \n 
mean_error = 1 - val_accuracy * args . val_batchsize / 50000 \n 
~~ ~~ ~~ ~~ def train_loop ( ) : \n 
~~~ graph_generated = False \n 
~~~ while data_q . empty ( ) : \n 
~~~ time . sleep ( 0.1 ) \n 
~~ inp = data_q . get ( ) \n 
~~~ res_q . put ( ) \n 
model . train = True \n 
serializers . save_hdf5 ( args . out , model ) \n 
serializers . save_hdf5 ( args . outstate , optimizer ) \n 
model . train = False \n 
~~ volatile = if model . train else \n 
x = chainer . Variable ( xp . asarray ( inp [ 0 ] ) , volatile = volatile ) \n 
t = chainer . Variable ( xp . asarray ( inp [ 1 ] ) , volatile = volatile ) \n 
if model . train : \n 
~~~ optimizer . update ( model , x , t ) \n 
if not graph_generated : \n 
~~~ with open ( , ) as o : \n 
~~~ o . write ( computational_graph . build_computational_graph ( \n 
( model . loss , ) ) . dump ( ) ) \n 
~~ print ( , file = sys . stderr ) \n 
graph_generated = True \n 
~~~ model ( x , t ) \n 
~~ res_q . put ( ( float ( model . loss . data ) , float ( model . accuracy . data ) ) ) \n 
del x , t \n 
~~ ~~ feeder = threading . Thread ( target = feed_data ) \n 
feeder . daemon = True \n 
feeder . start ( ) \n 
logger = threading . Thread ( target = log_result ) \n 
logger . daemon = True \n 
logger . start ( ) \n 
train_loop ( ) \n 
feeder . join ( ) \n 
logger . join ( ) \n 
import sys , os \n 
sys . path . insert ( 0 , os . path . abspath ( ) ) \n 
import local_settings \n 
extensions = [ , ] \n 
html_show_copyright = False \n 
import djoauth2 \n 
version = djoauth2 . __version__ \n 
release = djoauth2 . __version__ \n 
html_static_path = [ ] \n 
epub_title = \n 
epub_author = \n 
epub_publisher = \n 
epub_copyright = \n 
import asyncio \n 
from zeroservices import ZeroMQMedium , ResourceService \n 
from zeroservices . services import get_http_interface \n 
from zeroservices . discovery import UdpDiscoveryMedium \n 
~~~ loop = asyncio . get_event_loop ( ) \n 
medium = ZeroMQMedium ( loop , UdpDiscoveryMedium ) \n 
service = ResourceService ( , medium ) \n 
application = get_http_interface ( service , loop , port = 5001 , allowed_origins = "*" ) \n 
application = loop . run_until_complete ( application ) \n 
loop . run_until_complete ( service . start ( ) ) \n 
loop . run_forever ( ) \n 
from trello import TrelloClient \n 
from slugify import slugify \n 
from matterllo . utils import config \n 
from matterllo . utils import logger \n 
SETTINGS = config ( ) \n 
LOGGING = logger ( ) \n 
parser . add_argument ( , dest = , action = , help = parser . add_argument ( , dest = , action = , help = parser . add_argument ( , dest = , action = , help = \n 
if not args . cleanup and not args . update and not args . init : \n 
~~~ print parser . print_help ( ) \n 
sys . exit ( 0 ) \n 
~~ client = TrelloClient ( api_key = SETTINGS [ ] , token = SETTINGS [ ] trello_boards = client . list_boards ( ) \n 
boards_name = [ slugify ( b [ ] ) for b in SETTINGS . get ( , { } ) . values ( ) ] \n 
if args . cleanup or args . init : \n 
~~~ result = [ h . delete ( ) for h in client . list_hooks ( ) ] \n 
LOGGING . info ( . format ( len ( result ) ) ) \n 
~~ if args . update or args . init : \n 
~~~ for board in trello_boards : \n 
~~~ board_name = slugify ( board . name ) \n 
if board_name not in boards_name : \n 
~~ LOGGING . info ( . format ( board_name ) ) \n 
url = SETTINGS [ ] + \n 
result = client . create_hook ( url , board . id ) \n 
LOGGING . info ( . format ( board_name , result ) ) \n 
~~ ~~ ~~ except Exception as e : \n 
~~~ LOGGING . error ( . format ( e ) ) \n 
~~~ from . import ssl_compat \n 
~~~ ssl_compat = None \n 
~~ _ver = sys . version_info \n 
is_py2 = _ver [ 0 ] == 2 \n 
is_py2_7_9_or_later = _ver [ 0 ] >= 2 and _ver [ 1 ] >= 7 and _ver [ 2 ] >= 9 \n 
is_py3 = _ver [ 0 ] == 3 \n 
is_py3_3 = is_py3 and _ver [ 1 ] == 3 \n 
def ignore_missing ( ) : \n 
~~ ~~ if is_py2 : \n 
~~~ if is_py2_7_9_or_later : \n 
~~~ import ssl \n 
~~~ ssl = ssl_compat \n 
~~ from urllib import urlencode \n 
from urlparse import urlparse , urlsplit \n 
from itertools import imap \n 
def to_byte ( char ) : \n 
~~~ return ord ( char ) \n 
~~ def decode_hex ( b ) : \n 
~~~ return b . decode ( ) \n 
~~ def write_to_stdout ( data ) : \n 
~~~ sys . stdout . write ( data + ) \n 
~~ def zlib_compressobj ( level = 6 , method = zlib . DEFLATED , wbits = 15 , memlevel = 8 , \n 
strategy = zlib . Z_DEFAULT_STRATEGY ) : \n 
~~~ return zlib . compressobj ( level , method , wbits , memlevel , strategy ) \n 
~~ unicode = unicode \n 
bytes = str \n 
~~ elif is_py3 : \n 
~~~ from urllib . parse import urlencode , urlparse , urlsplit \n 
imap = map \n 
~~~ return char \n 
~~~ return bytes . fromhex ( b ) \n 
~~~ sys . stdout . buffer . write ( data + ) \n 
sys . stdout . buffer . flush ( ) \n 
~~ zlib_compressobj = zlib . compressobj \n 
if is_py3_3 : \n 
~~ unicode = str \n 
bytes = bytes \n 
from hyper import HTTP20Connection \n 
from hyper . compat import ssl \n 
from hyper . http11 . connection import HTTP11Connection \n 
from hpack . hpack import Encoder \n 
from hpack . huffman import HuffmanEncoder \n 
from hpack . huffman_constants import ( \n 
REQUEST_CODES , REQUEST_CODES_LENGTH \n 
from hyper . tls import NPN_PROTOCOL \n 
class SocketServerThread ( threading . Thread ) : \n 
def __init__ ( self , \n 
socket_handler , \n 
host = , \n 
ready_event = None , \n 
h2 = True , \n 
secure = True ) : \n 
~~~ threading . Thread . __init__ ( self ) \n 
self . socket_handler = socket_handler \n 
self . host = host \n 
self . secure = secure \n 
self . ready_event = ready_event \n 
self . daemon = True \n 
if self . secure : \n 
~~~ self . cxt = ssl . SSLContext ( ssl . PROTOCOL_SSLv23 ) \n 
if ssl . HAS_NPN and h2 : \n 
~~~ self . cxt . set_npn_protocols ( [ NPN_PROTOCOL ] ) \n 
~~ self . cxt . load_cert_chain ( certfile = , \n 
keyfile = ) \n 
~~ ~~ def _start_server ( self ) : \n 
~~~ sock = socket . socket ( ) \n 
if sys . platform != : \n 
~~~ sock . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) \n 
~~ if self . secure : \n 
~~~ sock = self . cxt . wrap_socket ( sock , server_side = True ) \n 
~~ sock . bind ( ( self . host , 0 ) ) \n 
self . port = sock . getsockname ( ) [ 1 ] \n 
sock . listen ( 1 ) \n 
if self . ready_event : \n 
~~~ self . ready_event . set ( ) \n 
~~ self . socket_handler ( sock ) \n 
sock . close ( ) \n 
~~ def _wrap_socket ( self , sock ) : \n 
~~~ self . server = self . _start_server ( ) \n 
~~ ~~ class SocketLevelTest ( object ) : \n 
def set_up ( self , secure = True , proxy = False ) : \n 
~~~ self . host = None \n 
self . port = None \n 
self . secure = secure if not proxy else False \n 
self . proxy = proxy \n 
self . server_thread = None \n 
~~ def _start_server ( self , socket_handler ) : \n 
ready_event = threading . Event ( ) \n 
self . server_thread = SocketServerThread ( \n 
socket_handler = socket_handler , \n 
ready_event = ready_event , \n 
h2 = self . h2 , \n 
secure = self . secure \n 
self . server_thread . start ( ) \n 
ready_event . wait ( ) \n 
self . host = self . server_thread . host \n 
self . port = self . server_thread . port \n 
self . secure = self . server_thread . secure \n 
~~ def get_connection ( self ) : \n 
~~~ if self . h2 : \n 
~~~ if not self . proxy : \n 
~~~ return HTTP20Connection ( self . host , self . port , self . secure ) \n 
~~~ return HTTP20Connection ( , secure = self . secure , \n 
proxy_host = self . host , \n 
proxy_port = self . port ) \n 
~~~ return HTTP11Connection ( self . host , self . port , self . secure ) \n 
~~~ return HTTP11Connection ( , secure = self . secure , \n 
~~ ~~ ~~ def get_encoder ( self ) : \n 
e = Encoder ( ) \n 
e . huffman_coder = HuffmanEncoder ( REQUEST_CODES , REQUEST_CODES_LENGTH ) \n 
return e \n 
~~ def tear_down ( self ) : \n 
self . server_thread . join ( 0.1 ) \n 
~~ ~~ import numpy as np \n 
import lxmls . readers . pos_corpus as pcc \n 
corpus = pcc . PostagCorpus ( ) \n 
input_data = path . join ( \n 
path . dirname ( __file__ ) , \n 
"../../data/train-02-21.conll" ) \n 
train_seq = corpus . read_sequence_list_conll ( input_data , max_sent_len = 15 , max_nr_sent = 1000 ) \n 
pickle . dump ( ( corpus . word_dict , corpus . tag_dict ) , open ( , ) ) \n 
with open ( , ) as output : \n 
~~~ for seq in train_seq : \n 
~~~ words = [ corpus . word_dict . get_label_name ( seq . x [ i ] ) for i in xrange ( len ( seq ) ) ] \n 
tags = [ corpus . tag_dict . get_label_name ( seq . y [ i ] ) for i in xrange ( len ( seq ) ) ] \n 
s = . join ( [ . join ( [ word , tag ] ) for word , tag in zip ( words , tags ) ] ) \n 
output . write ( s + ) \n 
from lxmls . parsing . dependency_reader import * \n 
from lxmls . parsing . dependency_writer import * \n 
from lxmls . parsing . dependency_features import * \n 
from lxmls . parsing . dependency_decoder import * \n 
from lxmls . util . my_math_utils import * \n 
class DependencyParser ( ) : \n 
~~~ self . trained = False \n 
self . projective = False \n 
self . language = "" \n 
self . weights = [ ] \n 
self . decoder = DependencyDecoder ( ) \n 
self . reader = DependencyReader ( ) \n 
self . writer = DependencyWriter ( ) \n 
self . features = DependencyFeatures ( ) \n 
~~ def read_data ( self , language ) : \n 
~~~ self . language = language \n 
self . reader . load ( language ) \n 
self . features . create_dictionary ( self . reader . train_instances ) \n 
~~ def train_perceptron ( self , n_epochs ) : \n 
self . weights = np . zeros ( self . features . n_feats ) \n 
total = np . zeros ( self . features . n_feats ) \n 
for epoch in range ( n_epochs ) : \n 
n_mistakes = 0 \n 
n_tokens = 0 \n 
n_instances = 0 \n 
for instance in self . reader . train_instances : \n 
~~~ feats = self . features . create_features ( instance ) \n 
scores = self . features . compute_scores ( feats , self . weights ) \n 
if self . projective : \n 
~~~ heads_pred = self . decoder . parse_proj ( scores ) \n 
~~~ heads_pred = self . decoder . parse_nonproj ( scores ) \n 
~~ for m in range ( np . size ( heads_pred ) ) : \n 
~~~ for f in feats [ instance . heads [ m ] ] [ m ] : \n 
~~~ if f < 0 : \n 
~~ self . weights [ f ] += 1.0 \n 
~~ for f in feats [ heads_pred [ m ] ] [ m ] : \n 
~~ self . weights [ f ] -= 1.0 \n 
~~ n_mistakes += 1 \n 
~~ n_tokens += 1 \n 
~~ n_instances += 1 \n 
~~ self . weights = total / np . double ( n_epochs ) \n 
~~ def train_crf_sgd ( self , n_epochs , sigma , eta0 = 0.001 ) : \n 
t = 0 \n 
t0 = 1.0 / ( sigma * eta0 ) \n 
objective = 0.0 \n 
~~~ eta = 1.0 / ( sigma * ( t + t0 ) ) \n 
feats = self . features . create_features ( instance ) \n 
marginals , logZ = self . decoder . parse_marginals_nonproj ( scores ) \n 
for h in range ( np . size ( marginals , 0 ) ) : \n 
~~~ for m in range ( 1 , np . size ( marginals , 1 ) ) : \n 
~~~ if feats [ h ] [ m ] == None : \n 
~~ for f in feats [ h ] [ m ] : \n 
~~ self . weights [ f ] -= eta * marginals [ h , m ] \n 
for m in range ( 1 , np . size ( instance . heads ) ) : \n 
~~~ h = instance . heads [ m ] \n 
score_corr += scores [ h , m ] \n 
for f in feats [ h ] [ m ] : \n 
~~ self . weights [ f ] += eta \n 
~~ ~~ objective += 0.5 * sigma * np . dot ( self . weights , self . weights ) - score_corr + logZ \n 
n_instances += 1 \n 
t += 1 \n 
~~ ~~ def test ( self ) : \n 
~~~ n_mistakes = 0 \n 
arr_heads_pred = [ ] ; \n 
for instance in self . reader . test_instances : \n 
~~ ~~ for f in feats [ heads_pred [ m ] ] [ m ] : \n 
~~ ~~ n_mistakes += 1 \n 
arr_heads_pred . append ( heads_pred ) \n 
self . writer . save ( self . language , arr_heads_pred ) \n 
~~ ~~ from lxmls . sequences . label_dictionary import * \n 
################# \n 
class IDFeatures : \n 
def __init__ ( self , dataset ) : \n 
self . feature_dict = LabelDictionary ( ) \n 
self . feature_list = [ ] \n 
self . add_features = False \n 
self . dataset = dataset \n 
self . node_feature_cache = { } \n 
self . initial_state_feature_cache = { } \n 
self . final_state_feature_cache = { } \n 
self . edge_feature_cache = { } \n 
~~ def get_num_features ( self ) : \n 
~~~ return len ( self . feature_dict ) \n 
~~ def build_features ( self ) : \n 
self . add_features = True \n 
for sequence in self . dataset . seq_list : \n 
~~~ initial_features , transition_features , final_features , emission_features = self . get_sequence_features ( sequence ) \n 
self . feature_list . append ( [ initial_features , transition_features , final_features , emission_features ~~ self . add_features = False \n 
~~ def get_sequence_features ( self , sequence ) : \n 
emission_features = [ ] \n 
initial_features = [ ] \n 
transition_features = [ ] \n 
final_features = [ ] \n 
features = [ ] \n 
features = self . add_initial_features ( sequence , sequence . y [ 0 ] , features ) \n 
initial_features . append ( features ) \n 
for pos , tag in enumerate ( sequence . y ) : \n 
~~~ features = [ ] \n 
features = self . add_emission_features ( sequence , pos , sequence . y [ pos ] , features ) \n 
emission_features . append ( features ) \n 
if pos > 0 : \n 
~~~ prev_tag = sequence . y [ pos - 1 ] \n 
features = self . add_transition_features ( sequence , pos - 1 , tag , prev_tag , features ) transition_features . append ( features ) \n 
~~ ~~ features = [ ] \n 
features = self . add_final_features ( sequence , sequence . y [ - 1 ] , features ) \n 
final_features . append ( features ) \n 
return initial_features , transition_features , final_features , emission_features \n 
#f(t,y_t,X) \n 
~~ def get_emission_features ( self , sequence , pos , y ) : \n 
~~~ all_feat = [ ] \n 
x = sequence . x [ pos ] \n 
if ( x not in self . node_feature_cache ) : \n 
~~~ self . node_feature_cache [ x ] = { } \n 
~~ if ( y not in self . node_feature_cache [ x ] ) : \n 
~~~ node_idx = [ ] \n 
node_idx = self . add_emission_features ( sequence , pos , y , node_idx ) \n 
self . node_feature_cache [ x ] [ y ] = node_idx \n 
~~ idx = self . node_feature_cache [ x ] [ y ] \n 
all_feat = idx [ : ] \n 
return all_feat \n 
#f(t,y_t,y_(t-1),X) \n 
~~ def get_transition_features ( self , sequence , pos , y , y_prev ) : \n 
~~~ assert ( pos >= 0 and pos < len ( sequence . x ) ) , pdb . set_trace ( ) \n 
if ( y not in self . edge_feature_cache ) : \n 
~~~ self . edge_feature_cache [ y ] = { } \n 
~~ if ( y_prev not in self . edge_feature_cache [ y ] ) : \n 
~~~ edge_idx = [ ] \n 
edge_idx = self . add_transition_features ( sequence , pos , y , y_prev , edge_idx ) \n 
self . edge_feature_cache [ y ] [ y_prev ] = edge_idx \n 
~~ return self . edge_feature_cache [ y ] [ y_prev ] \n 
~~ def get_initial_features ( self , sequence , y ) : \n 
~~~ if ( y not in self . initial_state_feature_cache ) : \n 
edge_idx = self . add_initial_features ( sequence , y , edge_idx ) \n 
self . initial_state_feature_cache [ y ] = edge_idx \n 
~~ return self . initial_state_feature_cache [ y ] \n 
~~ def get_final_features ( self , sequence , y_prev ) : \n 
~~~ if ( y_prev not in self . final_state_feature_cache ) : \n 
edge_idx = self . add_final_features ( sequence , y_prev , edge_idx ) \n 
self . final_state_feature_cache [ y_prev ] = edge_idx \n 
~~ return self . final_state_feature_cache [ y_prev ] \n 
~~ def add_initial_features ( self , sequence , y , features ) : \n 
~~~ y_name = self . dataset . y_dict . get_label_name ( y ) \n 
feat_name = "init_tag:%s" % ( y_name ) \n 
feat_id = self . add_feature ( feat_name ) \n 
if ( feat_id != - 1 ) : \n 
~~~ features . append ( feat_id ) \n 
~~ return features \n 
~~ def add_final_features ( self , sequence , y_prev , features ) : \n 
~~~ y_name = self . dataset . y_dict . get_label_name ( y_prev ) \n 
feat_name = "final_prev_tag:%s" % ( y_name ) \n 
~~ def add_emission_features ( self , sequence , pos , y , features ) : \n 
y_name = self . dataset . y_dict . get_label_name ( y ) \n 
x_name = self . dataset . x_dict . get_label_name ( x ) \n 
feat_name = "id:%s::%s" % ( x_name , y_name ) \n 
if feat_id != - 1 : \n 
~~ def add_transition_features ( self , sequence , pos , y , y_prev , features ) : \n 
assert pos < len ( sequence . x ) - 1 , pdb . set_trace ( ) \n 
y_prev_name = self . dataset . y_dict . get_label_name ( y_prev ) \n 
feat_name = "prev_tag:%s::%s" % ( y_prev_name , y_name ) \n 
~~ def add_feature ( self , feat_name ) : \n 
if ( feat_name in self . feature_dict ) : \n 
~~~ return self . feature_dict [ feat_name ] \n 
~~ if not self . add_features : \n 
~~ return self . feature_dict . add ( feat_name ) \n 
~~~ import ogr \n 
~~~ from osgeo import ogr \n 
~~ line_shp_file = "../static_files/shapefile/rivers_lake_centerlines/ne_50m_rivers_lake_centerlines.shp" \n 
line_datasource = ogr . Open ( line_shp_file ) \n 
driver = ogr . GetDriverByName ( ) \n 
point_shp_file = \n 
layer_name = \n 
if os . path . exists ( point_shp_file ) : \n 
~~~ driver . DeleteDataSource ( point_shp_file ) \n 
~~ point_datasource = driver . CreateDataSource ( point_shp_file ) \n 
layer_count = line_datasource . GetLayerCount ( ) \n 
for each_layer in range ( layer_count ) : \n 
~~~ layer = line_datasource . GetLayerByIndex ( each_layer ) \n 
srs = layer . GetSpatialRef ( ) \n 
point_shp_layer = point_datasource . CreateLayer ( layer_name , srs , ogr . wkbPoint ) \n 
feature_count = layer . GetFeatureCount ( ) \n 
for each_feature in range ( feature_count ) : \n 
~~~ line_feature = layer . GetFeature ( each_feature ) \n 
feature_geom = line_feature . GetGeometryRef ( ) \n 
if feature_geom . GetGeometryName ( ) != : \n 
~~~ points = feature_geom . GetPoints ( ) \n 
for point in points : \n 
~~~ point_geom = ogr . Geometry ( ogr . wkbPoint ) \n 
point_geom . AddPoint ( point [ 0 ] , point [ 1 ] ) \n 
point_feature = ogr . Feature ( point_shp_layer . GetLayerDefn ( ) ) \n 
point_feature . SetGeometry ( point_geom ) \n 
point_shp_layer . CreateFeature ( point_feature ) \n 
~~ ~~ ~~ ~~ \n 
~~ latitudes = [ 50 , 51 , 52 , 53 , 54 ] \n 
longitudes = [ 100 , 110 , 120 , 130 , 140 ] \n 
elevation = 0 \n 
points = ogr . Geometry ( ogr . wkbMultiPoint ) \n 
point_1 = ogr . Geometry ( ogr . wkbPoint ) \n 
point_1 . AddPoint ( longitudes [ 0 ] , latitudes [ 0 ] , elevation ) \n 
points . AddGeometry ( point_1 ) \n 
point_2 = ogr . Geometry ( ogr . wkbPoint ) \n 
point_2 . AddPoint ( longitudes [ 1 ] , latitudes [ 1 ] , elevation ) \n 
points . AddGeometry ( point_2 ) \n 
point_3 = ogr . Geometry ( ogr . wkbPoint ) \n 
point_3 . AddPoint ( longitudes [ 2 ] , latitudes [ 2 ] , elevation ) \n 
points . AddGeometry ( point_3 ) \n 
point_4 = ogr . Geometry ( ogr . wkbPoint ) \n 
point_4 . AddPoint ( longitudes [ 3 ] , latitudes [ 3 ] , elevation ) \n 
points . AddGeometry ( point_4 ) \n 
point_5 = ogr . Geometry ( ogr . wkbPoint ) \n 
point_5 . AddPoint ( longitudes [ 4 ] , latitudes [ 4 ] , elevation ) \n 
points . AddGeometry ( point_5 ) \n 
points . ExportToWkt ( ) \n 
print points \n 
from sklearn import cluster , preprocessing , manifold \n 
class KeplerMapper ( object ) : \n 
~~~ def __init__ ( self , verbose = 2 ) : \n 
~~~ self . verbose = verbose \n 
self . chunk_dist = [ ] \n 
self . overlap_dist = [ ] \n 
self . d = [ ] \n 
self . nr_cubes = 0 \n 
self . overlap_perc = 0 \n 
self . clusterer = False \n 
~~ def fit_transform ( self , X , projection = "sum" , scaler = preprocessing . MinMaxScaler ( ) ) : \n 
~~~ self . scaler = scaler \n 
self . projection = str ( projection ) \n 
~~~ reducer = projection \n 
if self . verbose > 0 : \n 
~~~ projection . set_params ( ** { "verbose" : self . verbose } ) \n 
~~ X = reducer . fit_transform ( X ) \n 
~~ if isinstance ( projection , str ) : \n 
~~~ if self . verbose > 0 : \n 
~~~ X = np . sum ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . mean ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . median ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . max ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . min ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . std ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X_mean = np . mean ( X , axis = 0 ) \n 
X = np . sum ( np . sqrt ( ( X - X_mean ) ** 2 ) , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~ ~~ if isinstance ( projection , list ) : \n 
~~ X = X [ : , np . array ( projection ) ] \n 
~~ if scaler is not None : \n 
~~ X = scaler . fit_transform ( X ) \n 
~~ return X \n 
~~~ start = datetime . now ( ) \n 
def cube_coordinates_all ( nr_cubes , nr_dimensions ) : \n 
~~~ l = [ ] \n 
for x in range ( nr_cubes ) : \n 
~~~ l += [ x ] * nr_dimensions \n 
~~ return [ np . array ( list ( f ) ) for f in sorted ( set ( itertools . permutations ( l , nr_dimensions ) ) ) ] \n 
~~ nodes = defaultdict ( list ) \n 
links = defaultdict ( list ) \n 
complex = { } \n 
self . nr_cubes = nr_cubes \n 
self . clusterer = clusterer \n 
self . overlap_perc = overlap_perc \n 
~~~ inverse_X = projected_X \n 
~~ self . chunk_dist = ( np . max ( projected_X , axis = 0 ) - np . min ( projected_X , axis = 0 ) ) / nr_cubes \n 
self . overlap_dist = self . overlap_perc * self . chunk_dist \n 
self . d = np . min ( projected_X , axis = 0 ) \n 
di = np . array ( [ x for x in range ( projected_X . shape [ 1 ] ) ] ) \n 
ids = np . array ( [ x for x in range ( projected_X . shape [ 0 ] ) ] ) \n 
projected_X = np . c_ [ ids , projected_X ] \n 
inverse_X = np . c_ [ ids , inverse_X ] \n 
~~~ total_cubes = len ( cube_coordinates_all ( nr_cubes , projected_X . shape [ 1 ] ) ) \n 
~~ for i , coor in enumerate ( cube_coordinates_all ( nr_cubes , di . shape [ 0 ] ) ) : \n 
~~~ hypercube = projected_X [ np . invert ( np . any ( ( projected_X [ : , di + 1 ] >= self . d [ di ] + ( coor * self . chunk_dist ( projected_X [ : , di + 1 ] < self . d [ di ] + ( coor * self . chunk_dist [ di ] ) + self . chunk_dist [ di ] + self \n 
if self . verbose > 1 : \n 
( hypercube . shape [ 0 ] , i , total_cubes , self . d [ di ] + ( coor * self . chunk_dist [ di ] ) ) ) \n 
~~ if hypercube . shape [ 0 ] > 0 : \n 
clusterer . fit ( inverse_x [ : , 1 : ] ) \n 
~~ for a in np . c_ [ hypercube [ : , 0 ] , clusterer . labels_ ] : \n 
~~~ cluster_id = str ( coor [ 0 ] ) + "_" + str ( i ) + "_" + str ( a [ 1 ] ) + "_" + str ( coor ) + "_" + str ( self . d [ di ] + ( coor nodes [ cluster_id ] . append ( int ( a [ 0 ] ) ) \n 
~~~ if self . verbose > 1 : \n 
~~ ~~ ~~ candidates = itertools . combinations ( nodes . keys ( ) , 2 ) \n 
for candidate in candidates : \n 
~~~ if len ( nodes [ candidate [ 0 ] ] + nodes [ candidate [ 1 ] ] ) != len ( set ( nodes [ candidate [ 0 ] ] + nodes [ candidate ~~~ links [ candidate [ 0 ] ] . append ( candidate [ 1 ] ) \n 
~~ ~~ if self . verbose > 0 : \n 
~~~ nr_links = 0 \n 
for k in links : \n 
~~~ nr_links += len ( links [ k ] ) \n 
~~ complex [ "nodes" ] = nodes \n 
complex [ "links" ] = links \n 
complex [ "meta" ] = self . projection \n 
return complex \n 
~~ def visualize ( self , complex , color_function = "" , path_html = "mapper_visualization_output.html" , title graph_link_distance = 30 , graph_gravity = 0.1 , graph_charge = - 120 , custom_tooltips = None , width_html height_html = 0 , show_tooltips = True , show_title = True , show_meta = True ) : \n 
~~~ json_s = { } \n 
json_s [ "nodes" ] = [ ] \n 
json_s [ "links" ] = [ ] \n 
k2e = { } \n 
for e , k in enumerate ( complex [ "nodes" ] ) : \n 
~~~ if custom_tooltips is not None : \n 
~~~ tooltip_i = int ( ( ( sum ( [ f for f in custom_tooltips [ complex [ "nodes" ] [ k ] ] ] ) / len ( custom_tooltips json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( ~~ else : \n 
~~~ json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( ~~ ~~ else : \n 
json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( complex ~~ k2e [ k ] = e \n 
~~ for k in complex [ "links" ] : \n 
~~~ for link in complex [ "links" ] [ k ] : \n 
~~~ json_s [ "links" ] . append ( { "source" : k2e [ k ] , "target" : k2e [ link ] , "value" : 1 } ) \n 
~~ ~~ if width_html == 0 : \n 
~~~ width_css = "100%" \n 
width_js = \'document.getElementById("holder").offsetWidth-20\' \n 
~~~ width_css = "%spx" % width_html \n 
width_js = "%s" % width_html \n 
~~ if height_html == 0 : \n 
~~~ height_css = "100%" \n 
height_js = \'document.getElementById("holder").offsetHeight-20\' \n 
~~~ height_css = "%spx" % height_html \n 
height_js = "%s" % height_html \n 
~~ if show_tooltips == False : \n 
~~~ tooltips_display = "" \n 
~~ if show_meta == False : \n 
~~~ meta_display = "" \n 
~~ if show_title == False : \n 
~~~ title_display = "" \n 
~~ with open ( path_html , "wb" ) as outfile : \n 
~~ if self . verbose > 0 : \n 
from SystemConfiguration import * \n 
class SCPreferences ( object ) : \n 
session = None \n 
~~~ super ( SCPreferences , self ) . __init__ ( ) \n 
self . session = SCPreferencesCreate ( None , "set-proxy" , None ) \n 
~~~ if not self . session : \n 
~~ if not SCPreferencesCommitChanges ( self . session ) : \n 
~~ if not SCPreferencesApplyChanges ( self . session ) : \n 
~~ ~~ def set_proxy ( self , enable = True , protocol = "HTTP" , server = "localhost" , port = 3128 ) : \n 
~~~ new_settings = SCPreferencesPathGetValue ( self . session , ) \n 
for interface in new_settings : \n 
~~~ new_settings [ interface ] [ ] [ "%sEnable" % protocol ] = 1 if enable else 0 \n 
if enable : \n 
~~~ new_settings [ interface ] [ ] [ % protocol ] = int ( port ) \n 
new_settings [ interface ] [ ] [ % protocol ] = server \n 
~~ ~~ SCPreferencesPathSetValue ( self . session , , new_settings ) \n 
~~ ~~ class SCPreferencesTests ( unittest . TestCase ) : \n 
~~ import sublime , sublime_plugin , os , re \n 
class StyleSheetSetup : \n 
~~~ def __init__ ( self , extensions , regex , partials = None , index = None ) : \n 
~~~ if partials is None : \n 
~~~ self . partials = False \n 
~~~ self . partials = partials \n 
~~ if index is None : \n 
~~~ self . index = False \n 
~~~ self . index = index \n 
~~ self . extensions = extensions \n 
self . regex = regex \n 
~~ ~~ class ListStylesheetVariables ( sublime_plugin . TextCommand ) : \n 
~~~ def run ( self , edit ) : \n 
~~~ settings = sublime . load_settings ( ) \n 
handle_imports = settings . get ( "readImported" ) \n 
read_all_views = settings . get ( "readAllViews" ) \n 
setups = ( less_setup , sass_setup , stylus_setup , sass_erb_setup ) \n 
chosen_setup = None \n 
self . edit = edit \n 
fn = self . view . file_name ( ) . encode ( "utf_8" ) \n 
for setup in setups : \n 
~~~ for ext in setup . extensions : \n 
~~~ if fn . endswith ( ext ) : \n 
~~~ chosen_setup = setup \n 
~~ ~~ ~~ if chosen_setup == None : \n 
~~ imports = [ ] \n 
imported_vars = [ ] \n 
compiled_regex = re . compile ( chosen_setup . regex , re . MULTILINE ) \n 
if handle_imports : \n 
file_dir = os . path . dirname ( fn ) . decode ( "utf-8" ) \n 
for i , filename in enumerate ( imports ) : \n 
~~~ has_extension = False \n 
for ext in chosen_setup . extensions : \n 
~~~ if filename . endswith ( ext . decode ( "utf-8" ) ) : \n 
~~~ has_extension = True \n 
~~ ~~ if has_extension == False : \n 
~~~ for ext in chosen_setup . extensions : \n 
~~~ ext = ext . decode ( "utf-8" ) \n 
if os . path . isfile ( os . path . normpath ( file_dir + + filename + ext ) ) : \n 
~~~ filename += ext \n 
~~ if chosen_setup . partials : \n 
~~~ fn_split = os . path . split ( filename ) \n 
partial_filename = fn_split [ 0 ] + "/_" + fn_split [ 1 ] \n 
if os . path . isfile ( os . path . normpath ( file_dir + partial_filename + ext ) ) : \n 
~~~ filename = "_" + filename + ext \n 
~~ ~~ if chosen_setup . index and os . path . isfile ( os . path . normpath ( file_dir + "/" + filename ~~~ filename += "/index" + ext \n 
~~~ f = open ( os . path . normpath ( file_dir + + filename ) , ) \n 
contents = f . read ( ) \n 
m = re . findall ( compiled_regex , contents ) \n 
imported_vars = imported_vars + m \n 
~~~ print ( + filename ) \n 
~~ ~~ imported_vars = [ list ( item ) for item in imported_vars ] \n 
~~ self . variables = [ ] \n 
vars_from_views = [ ] \n 
if read_all_views : \n 
~~~ for view in self . view . window ( ) . views ( ) : \n 
~~~ viewfn = self . view . file_name ( ) . encode ( "utf-8" ) \n 
compatible_view = False \n 
~~~ if viewfn . endswith ( ext ) : \n 
~~~ viewvars = [ ] \n 
view . find_all ( chosen_setup . regex , 0 , "$1|$2" , viewvars ) \n 
vars_from_views += viewvars \n 
~~~ self . view . find_all ( chosen_setup . regex , 0 , "$1|$2" , self . variables ) \n 
~~ self . variables += vars_from_views \n 
self . variables = list ( set ( self . variables ) ) \n 
for i , val in enumerate ( self . variables ) : \n 
~~~ self . variables [ i ] = val . split ( "|" ) \n 
~~ self . variables = imported_vars + self . variables \n 
self . variables . sort ( ) \n 
self . view . window ( ) . show_quick_panel ( self . variables , self . insert_variable , sublime . MONOSPACE_FONT \n 
~~ def insert_variable ( self , choice ) : \n 
~~~ if choice == - 1 : \n 
~~ self . view . run_command ( , { : self . variables [ choice ] [ 0 ] } ) \n 
~~ ~~ class InsertText ( sublime_plugin . TextCommand ) : \n 
~~~ def run ( self , edit , string = ) : \n 
~~~ for selection in self . view . sel ( ) : \n 
~~~ self . view . insert ( edit , selection . begin ( ) , string ) \n 
~~ ~~ ~~ from driver_base import DriverBase \n 
os . sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) \n 
import log \n 
class CMDTYPE : \n 
PIXEL_DATA = 2 \n 
BRIGHTNESS = 3 \n 
~~ class RETURN_CODES : \n 
~~ class DriverNetwork ( DriverBase ) : \n 
def __init__ ( self , num = 0 , width = 0 , height = 0 , host = "localhost" , port = 3142 ) : \n 
~~~ super ( DriverNetwork , self ) . __init__ ( num , width , height ) \n 
self . _host = host \n 
self . _port = port \n 
~~ def _generateHeader ( self , cmd , size ) : \n 
~~~ packet = bytearray ( ) \n 
packet . append ( cmd ) \n 
packet . append ( size & 0xFF ) \n 
packet . append ( size >> 8 ) \n 
return packet \n 
~~ def _connect ( self ) : \n 
~~~ s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
s . connect ( ( self . _host , self . _port ) ) \n 
~~ except socket . gaierror : \n 
self . _host ) \n 
log . error ( error ) \n 
raise IOError ( error ) \n 
~~ ~~ def update ( self , data ) : \n 
~~~ s = self . _connect ( ) \n 
count = self . bufByteCount \n 
packet = self . _generateHeader ( CMDTYPE . PIXEL_DATA , count ) \n 
packet . extend ( data ) \n 
s . sendall ( packet ) \n 
resp = ord ( s . recv ( 1 ) ) \n 
s . close ( ) \n 
if resp != RETURN_CODES . SUCCESS : \n 
~~~ log . exception ( e ) \n 
~~ ~~ def setMasterBrightness ( self , brightness ) : \n 
~~~ packet = self . _generateHeader ( CMDTYPE . BRIGHTNESS , 1 ) \n 
packet . append ( brightness ) \n 
s = self . _connect ( ) \n 
~~ ~~ ~~ MANIFEST = [ \n 
"id" : "network" , \n 
"class" : DriverNetwork , \n 
"type" : "driver" , \n 
"display" : "Network" , \n 
"params" : [ { \n 
"id" : "num" , \n 
"type" : "int" , \n 
"default" : 0 , \n 
"min" : 0 , \n 
} , { \n 
"id" : "width" , \n 
"label" : "Width" , \n 
"id" : "height" , \n 
"label" : "Height" , \n 
"id" : "host" , \n 
"type" : "str" , \n 
"default" : "localhost" , \n 
"id" : "port" , \n 
"label" : "Port" , \n 
"default" : 3142 , \n 
} ] \n 
_ID = \n 
API = \n 
API_CALL_TIMEOUT = \n 
API_VERSION = \n 
API_VERSION_MAXIMUM = \n 
API_VERSION_MINIMUM = \n 
AREA = \n 
AREA_MAX = \n 
BBOX = \n 
BOUNDS = \n 
CFGSLAB = \n 
CFGVERSION = 1 \n 
CHANGESET = \n 
CHANGESETS = \n 
CHANGESETS_INLINE_SIZE = \n 
CHANGESETS_PER_SLAB = \n 
CHANGESETS_MAX = \n 
CONFIGURATION_SCHEMA_VERSION = \n 
CONTENT_TYPE = \n 
COUCHDB = \n 
DATASTORE = \n 
DATASTORE_BACKEND = \n 
DATASTORE_CONFIG = \n 
DATASTORE_ENCODING = \n 
DBHOST = \n 
DBJOB_ADDELEM = \n 
DBJOB_QUIT = \n 
DBNAME = \n 
DBPORT = \n 
DBURL = \n 
DEFAULT = \n 
ELEMENT = \n 
FRONT_END = \n 
GENERATOR = \n 
GEODOC = \n 
GEODOC_LRU_SIZE = \n 
GEODOC_LRU_THREADS = \n 
GEOHASH_LENGTH = \n 
ID = \n 
JSON = \n 
K = \n 
LAT = \n 
LAT_MAX = + 90.0 \n 
LAT_MIN = - 90.0 \n 
LON = \n 
LON_MAX = + 180.0 \n 
LON_MIN = - 180.0 \n 
MAXIMUM = \n 
MAXIMUM_ELEMENTS = \n 
MAXGHLAT = 89.999999999999992 \n 
MAXLAT = \n 
MAXLON = \n 
MEMBASE = \n 
MEMBASE_MAX_VALUE_LENGTH = 20 * 1024 * 1024 \n 
MEMBER = \n 
MEMBERS = \n 
MINIMUM = \n 
MINLAT = \n 
MINLON = \n 
ND = \n 
NODE = \n 
NODES = \n 
NODES_INLINE_SIZE = \n 
NODES_PER_SLAB = \n 
OSM = \n 
PER_PAGE = \n 
PORT = \n 
PROJECT_DOC = \n 
PROTOBUF = \n 
REF = \n 
REFERENCES = \n 
RELATION = \n 
RELATIONS = \n 
RELATIONS_INLINE_SIZE = \n 
RELATIONS_PER_SLAB = \n 
ROLE = \n 
SCALE_FACTOR = \n 
SECONDS = \n 
SERVER_NAME = \n 
SERVER_VERSION = \n 
SLAB_LRU_SIZE = \n 
SLAB_LRU_THREADS = \n 
SOURCE_REPOSITORY = \n 
STATUS = \n 
TAG = \n 
TAGS = \n 
TEXT_XML = \n 
TIMEOUT = \n 
TRACEPOINTS = \n 
TRACEPOINTS_PER_PAGE = \n 
TYPE = \n 
UTF8 = \n 
V = \n 
WAY = \n 
WAYS = \n 
WAYS_INLINE_SIZE = \n 
WAYS_PER_SLAB = \n 
WAYNODES = \n 
WAYNODES_MAX = \n 
import webapp2 \n 
from urllib import urlencode \n 
import json , urllib2 \n 
from secret import client_id , client_secret \n 
import config \n 
class AuthRedirector ( webapp2 . RequestHandler ) : \n 
~~~ def get ( self ) : \n 
~~~ args = self . request . GET \n 
args [ "client_id" ] = client_id \n 
args [ "redirect_uri" ] = config . auth_redir_uri \n 
url = "https://accounts.google.com/o/oauth2/auth?" + urlencode ( args ) \n 
self . response . location = url \n 
self . response . status_int = 302 \n 
~~ ~~ def query_json ( url , data ) : \n 
if not ( data is str ) : \n 
~~~ data = urlencode ( data ) \n 
~~~ return json . loads ( urllib2 . urlopen ( url , data ) . read ( ) ) \n 
~~~ return json . loads ( e . read ( ) ) \n 
~~ ~~ def json_compactify ( data ) : \n 
~~ class AuthCallback ( webapp2 . RequestHandler ) : \n 
def get ( self ) : \n 
~~~ state = self . request . get ( "state" ) \n 
code = self . request . get ( "code" ) \n 
error = self . request . get ( "error" ) \n 
q = { \n 
"code" : code , \n 
"client_id" : client_id , \n 
"client_secret" : client_secret , \n 
"redirect_uri" : config . auth_redir_uri , \n 
"grant_type" : "authorization_code" , \n 
result = query_json ( "https://accounts.google.com/o/oauth2/token" , q ) \n 
url = ( config . auth_success_page if "access_token" in result \n 
else config . auth_failure_page ) + "#" + urlencode ( result ) \n 
~~ ~~ class AuthRefresh ( webapp2 . RequestHandler ) : \n 
~~~ refresh_token = self . request . get ( "refresh_token" ) \n 
if not refresh_token : \n 
~~~ self . response . status_int = 400 \n 
~~ q = { \n 
"refresh_token" : refresh_token , \n 
"grant_type" : "refresh_token" , \n 
self . response . write ( json_compactify ( result ) ) \n 
~~ ~~ application = webapp2 . WSGIApplication ( [ \n 
( , AuthRedirector ) , \n 
( , AuthCallback ) , \n 
( , AuthRefresh ) , \n 
] , debug = True ) \n 
from django . db import migrations , models \n 
class Migration ( migrations . Migration ) : \n 
~~~ dependencies = [ \n 
operations = [ \n 
migrations . AddField ( \n 
model_name = , \n 
field = models . EmailField ( help_text = , max_length = 75 , null preserve_default = True , \n 
__email__ = "martin@martineve.com" \n 
from debug import Debuggable \n 
from difflib import SequenceMatcher \n 
class Interactive ( Debuggable ) : \n 
~~~ def __init__ ( self , debug ) : \n 
~~~ self . debug = debug \n 
Debuggable . __init__ ( self , ) \n 
self . COLOR_ESCAPE = "\\x1b[" \n 
self . DARK_COLORS = [ "black" , "darkred" , "darkgreen" , "brown" , "darkblue" , \n 
"purple" , "teal" , "lightgray" ] \n 
self . LIGHT_COLORS = [ "darkgray" , "red" , "green" , "yellow" , "blue" , \n 
"fuchsia" , "turquoise" , "white" ] \n 
self . RESET_COLOR = self . COLOR_ESCAPE + "39;49;00m" \n 
~~ def input_options ( self , options , require = False , prompt = None , fallback_prompt = None , \n 
numrange = None , default = None , max_width = 72 ) : \n 
letters = { } \n 
display_letters = [ ] \n 
capitalized = [ ] \n 
first = True \n 
for option in options : \n 
~~~ for letter in option : \n 
~~~ if letter . isalpha ( ) and letter . upper ( ) == letter : \n 
~~~ found_letter = letter \n 
~~~ if not letter . isalpha ( ) : \n 
~~ if letter not in letters : \n 
~~ ~~ letters [ found_letter . lower ( ) ] = option \n 
index = option . index ( found_letter ) \n 
if not require and ( ( default is None and not numrange and first ) or \n 
( isinstance ( default , basestring ) and \n 
found_letter . lower ( ) == default . lower ( ) ) ) : \n 
~~~ show_letter = % found_letter . upper ( ) \n 
is_default = True \n 
~~~ show_letter = found_letter . upper ( ) \n 
is_default = False \n 
~~ show_letter = self . colorize ( if is_default else , \n 
show_letter ) \n 
capitalized . append ( \n 
option [ : index ] + show_letter + option [ index + 1 : ] \n 
display_letters . append ( found_letter . upper ( ) ) \n 
first = False \n 
~~ if require : \n 
~~~ default = None \n 
~~ elif default is None : \n 
~~~ if numrange : \n 
~~~ default = numrange [ 0 ] \n 
~~~ default = display_letters [ 0 ] . lower ( ) \n 
~~ ~~ if not prompt : \n 
~~~ prompt_parts = [ ] \n 
prompt_part_lengths = [ ] \n 
if numrange : \n 
~~~ if isinstance ( default , int ) : \n 
~~~ default_name = str ( default ) \n 
default_name = self . colorize ( , default_name ) \n 
tmpl = \n 
prompt_parts . append ( tmpl % default_name ) \n 
prompt_part_lengths . append ( len ( tmpl % str ( default ) ) ) \n 
~~~ prompt_parts . append ( ) \n 
prompt_part_lengths . append ( len ( prompt_parts [ - 1 ] ) ) \n 
~~ ~~ prompt_parts += capitalized \n 
prompt_part_lengths += [ len ( s ) for s in options ] \n 
prompt = \n 
line_length = 0 \n 
for i , ( part , length ) in enumerate ( zip ( prompt_parts , \n 
prompt_part_lengths ) ) : \n 
~~~ if i == len ( prompt_parts ) - 1 : \n 
~~~ part += \n 
~~ length += 1 \n 
if line_length + length + 1 > max_width : \n 
~~~ prompt += \n 
~~ if line_length != 0 : \n 
~~~ part = + part \n 
length += 1 \n 
~~ prompt += part \n 
line_length += length \n 
~~ ~~ if not fallback_prompt : \n 
~~~ fallback_prompt = \n 
~~~ fallback_prompt += % numrange \n 
~~ fallback_prompt += . join ( display_letters ) + \n 
~~ resp = self . input_ ( prompt ) \n 
~~~ resp = resp . strip ( ) . lower ( ) \n 
if default is not None and not resp : \n 
~~~ resp = default \n 
~~ if numrange : \n 
~~~ resp = int ( resp ) \n 
~~~ low , high = numrange \n 
if low <= resp <= high : \n 
~~~ return resp \n 
~~~ resp = None \n 
~~ ~~ ~~ if resp : \n 
~~~ resp = resp [ 0 ] \n 
if resp in letters : \n 
~~ ~~ resp = self . input_ ( fallback_prompt ) \n 
~~ ~~ def input_ ( self , prompt = None ) : \n 
if prompt : \n 
~~~ if isinstance ( prompt , unicode ) : \n 
~~~ prompt = prompt . encode ( self . _encoding ( ) , ) \n 
~~ print ( prompt , end = ) \n 
~~~ resp = raw_input ( ) \n 
~~ except EOFError : \n 
~~~ self . debug . print_debug ( ) \n 
~~ return resp . decode ( sys . stdin . encoding or , ) \n 
~~ def _encoding ( self ) : \n 
~~~ return locale . getdefaultlocale ( ) [ 1 ] or \n 
~~ ~~ def _colorize ( self , color , text ) : \n 
if color in self . DARK_COLORS : \n 
~~~ escape = self . COLOR_ESCAPE + "%im" % ( self . DARK_COLORS . index ( color ) + 30 ) \n 
~~ elif color in self . LIGHT_COLORS : \n 
~~~ escape = self . COLOR_ESCAPE + "%i;01m" % ( self . LIGHT_COLORS . index ( color ) + 30 ) \n 
~~~ raise ValueError ( , color ) \n 
~~ return escape + text + self . RESET_COLOR \n 
~~ def colorize ( self , color , text ) : \n 
return self . _colorize ( color , text ) \n 
~~ def _colordiff ( self , a , b , highlight = , minor_highlight = ) : \n 
if not isinstance ( a , basestring ) or not isinstance ( b , basestring ) : \n 
~~~ a = unicode ( a ) \n 
b = unicode ( b ) \n 
if a == b : \n 
~~~ return a , b \n 
~~~ return self . colorize ( highlight , a ) , self . colorize ( highlight , b ) \n 
~~ ~~ if isinstance ( a , bytes ) or isinstance ( b , bytes ) : \n 
~~~ a = self . displayable_path ( a ) \n 
b = self . displayable_path ( b ) \n 
~~ a_out = [ ] \n 
b_out = [ ] \n 
matcher = SequenceMatcher ( lambda x : False , a , b ) \n 
for op , a_start , a_end , b_start , b_end in matcher . get_opcodes ( ) : \n 
~~~ if op == : \n 
~~~ a_out . append ( a [ a_start : a_end ] ) \n 
b_out . append ( b [ b_start : b_end ] ) \n 
~~ elif op == : \n 
~~~ b_out . append ( self . colorize ( highlight , b [ b_start : b_end ] ) ) \n 
~~~ a_out . append ( self . colorize ( highlight , a [ a_start : a_end ] ) ) \n 
~~~ if a [ a_start : a_end ] . lower ( ) != b [ b_start : b_end ] . lower ( ) : \n 
~~~ color = highlight \n 
~~~ color = minor_highlight \n 
~~ a_out . append ( self . colorize ( color , a [ a_start : a_end ] ) ) \n 
b_out . append ( self . colorize ( color , b [ b_start : b_end ] ) ) \n 
~~~ assert ( False ) \n 
~~ ~~ return . join ( a_out ) , . join ( b_out ) \n 
~~ def displayable_path ( self , path , separator = ) : \n 
if isinstance ( path , ( list , tuple ) ) : \n 
~~~ return separator . join ( self . displayable_path ( p ) for p in path ) \n 
~~ elif isinstance ( path , unicode ) : \n 
~~ elif not isinstance ( path , str ) : \n 
~~~ return unicode ( path ) \n 
~~~ return path . decode ( self . _fsencoding ( ) , ) \n 
~~ except ( UnicodeError , LookupError ) : \n 
~~~ return path . decode ( , ) \n 
~~ ~~ def _fsencoding ( self ) : \n 
encoding = sys . getfilesystemencoding ( ) or sys . getdefaultencoding ( ) \n 
if encoding == : \n 
~~~ encoding = \n 
~~ return encoding \n 
~~ def colordiff ( self , a , b , highlight = ) : \n 
if self . gv . settings . get_setting ( , self ) == : \n 
~~~ return self . _colordiff ( a , b , highlight ) \n 
~~~ return unicode ( a ) , unicode ( b ) \n 
~~ ~~ def print_ ( self , * strings ) : \n 
if strings : \n 
~~~ if isinstance ( strings [ 0 ] , unicode ) : \n 
~~~ txt = . join ( strings ) \n 
~~~ txt = \n 
~~ if isinstance ( txt , unicode ) : \n 
~~~ txt = txt . encode ( self . _encoding ( ) , ) \n 
~~ print ( txt ) \n 
~~ def color_diff_suffix ( self , a , b , highlight = ) : \n 
a , b = unicode ( a ) , unicode ( b ) \n 
if not self . gv . settings . get_setting ( , self ) == : \n 
~~ if a == b : \n 
~~ first_diff = None \n 
for i in range ( min ( len ( a ) , len ( b ) ) ) : \n 
~~~ if a [ i ] != b [ i ] : \n 
~~~ first_diff = i \n 
~~~ first_diff = min ( len ( a ) , len ( b ) ) \n 
~~ return a [ : first_diff ] + self . colorize ( highlight , a [ first_diff : ] ) , b [ : first_diff ] + self . colorize ( highlight , b [ first_diff : ] ) \n 
~~ def choose_candidate ( self , candidates , manipulate , opts , item = None , itemcount = None ) : \n 
~~~ self . print_ ( ) \n 
for i , match in enumerate ( candidates ) : \n 
~~~ line = [ \n 
. format ( i + 1 ) , \n 
. format ( manipulate . get_stripped_text ( match . reference_to_link ) \n 
self . print_ ( . join ( line ) ) \n 
~~ sel = self . input_options ( opts , numrange = ( 1 , len ( candidates ) ) ) \n 
return sel # \n 
from jsbeautifier . unpackers import UnpackingError \n 
PRIORITY = 1 \n 
def detect ( source ) : \n 
return source . replace ( , ) . startswith ( ) \n 
~~ def unpack ( source ) : \n 
payload , symtab , radix , count = _filterargs ( source ) \n 
if count != len ( symtab ) : \n 
~~~ raise UnpackingError ( ) \n 
~~~ unbase = Unbaser ( radix ) \n 
~~ def lookup ( match ) : \n 
word = match . group ( 0 ) \n 
return symtab [ unbase ( word ) ] or word \n 
~~ source = re . sub ( , lookup , payload ) \n 
return _replacestrings ( source ) \n 
~~ def _filterargs ( source ) : \n 
args = re . search ( argsregex , source , re . DOTALL ) . groups ( ) \n 
~~~ return args [ 0 ] , args [ 3 ] . split ( ) , int ( args [ 1 ] ) , int ( args [ 2 ] ) \n 
~~ ~~ def _replacestrings ( source ) : \n 
~~~ varname , strings = match . groups ( ) \n 
startpoint = len ( match . group ( 0 ) ) \n 
lookup = strings . split ( \'","\' ) \n 
variable = % varname \n 
for index , value in enumerate ( lookup ) : \n 
~~~ source = source . replace ( variable % index , \'"%s"\' % value ) \n 
~~ return source [ startpoint : ] \n 
~~ return source \n 
~~ class Unbaser ( object ) : \n 
ALPHABET = { \n 
62 : , \n 
def __init__ ( self , base ) : \n 
~~~ self . base = base \n 
if 2 <= base <= 36 : \n 
~~~ self . unbase = lambda string : int ( string , base ) \n 
~~~ self . dictionary = dict ( ( cipher , index ) for \n 
index , cipher in enumerate ( self . ALPHABET [ base ] ) ) \n 
~~ self . unbase = self . _dictunbaser \n 
~~ ~~ def __call__ ( self , string ) : \n 
~~~ return self . unbase ( string ) \n 
~~ def _dictunbaser ( self , string ) : \n 
ret = 0 \n 
for index , cipher in enumerate ( string [ : : - 1 ] ) : \n 
~~~ ret += ( self . base ** index ) * self . dictionary [ cipher ] \n 
~~ return ret \n 
~~ ~~ import os , sys \n 
parentdir = os . path . dirname ( __file__ ) \n 
sys . path . insert ( 0 , parentdir ) \n 
import executemechanize \n 
class redirection : \n 
~~~ def createarray ( self ) : \n 
~~~ setattr ( self , "redirection_list" , [ ] ) \n 
~~ def appendurl ( self , url ) : \n 
~~~ url = str ( url ) \n 
if not url . endswith ( ".js" ) or url . endswith ( ".json" ) : \n 
~~~ self . redirection_list . append ( url ) ; \n 
self . passarray ( ) \n 
~~ ~~ def passarray ( self ) : \n 
~~~ executemechanize . set_redirection_list ( self . redirection_list ) \n 
~~ ~~ SQL_PORT = 15000 \n 
ZMQ_RPC_PORT = 15598 \n 
HTTP_PORT = 15597 \n 
HTTPS_PORT = 443 \n 
ZMQ_PUBSUB_PORT = 15596 \n 
__maintainer__ = \n 
__email__ = \n 
from . . config import PATHS \n 
from . . entity import Entity \n 
class StrategyConcept ( Entity ) : \n 
collection = \n 
resource = \n 
_relations = { \n 
_pull = { \n 
: int , \n 
: Entity . _strpt , \n 
: Entity . _int_to_bool , \n 
_push = _pull . copy ( ) \n 
_push . update ( { \n 
_readonly = Entity . _readonly | { , } \n 
def __init__ ( self , session , properties = None , ** kwargs ) : \n 
~~~ super ( StrategyConcept , self ) . __init__ ( session , properties , ** kwargs ) \n 
~~ def remove ( self ) : \n 
url = . join ( [ self . collection , \n 
str ( self . id ) , \n 
self . _post ( PATHS [ ] , rest = url , data = { : self . version } ) \n 
for item in list ( self . properties . keys ( ) ) : \n 
~~~ del self . properties [ item ] \n 
~~ ~~ ~~ from __future__ import print_function \n 
import responses \n 
from . requests_patch import patched_extract_cookies_to_jar \n 
from terminalone import T1 \n 
mock_credentials = { \n 
API_BASE = \n 
requests . sessions . extract_cookies_to_jar = patched_extract_cookies_to_jar \n 
requests . adapters . extract_cookies_to_jar = patched_extract_cookies_to_jar \n 
class TestPermissions ( unittest . TestCase ) : \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . POST , , \n 
body = fixture , \n 
adding_headers = { \n 
content_type = ) \n 
self . t1 = T1 ( auth_method = , \n 
api_base = API_BASE , \n 
** mock_credentials ) \n 
~~ @ responses . activate \n 
def test_get_permissions ( self ) : \n 
~~~ self . setup ( ) \n 
~~ responses . add ( responses . GET , \n 
content_type = , \n 
match_querystring = True ) \n 
p = self . t1 . get ( , 10000 , child = ) \n 
assert p . _type == , . format ( p . _type ) \n 
assert p . parent_id == 10000 , . format ( p . parent_id ) \n 
def test_remove_advertiser ( self ) : \n 
remove_id = 6 \n 
assert remove_id in p . advertiser . keys ( ) , . format \n 
p . remove ( , 6 ) \n 
assert remove_id not in p . advertiser . keys ( ) , . format ( remove_id ) \n 
def test_it_should_remove_child_advertisers_when_removing_agency ( self ) : \n 
remove_ids = [ 6 , 7 ] \n 
for ad_id in remove_ids : \n 
~~~ assert ad_id in p . advertiser . keys ( ) , . format \n 
~~ p . remove ( , 3 ) \n 
~~~ assert ad_id not in p . advertiser . keys ( ) , . format ( ad_id ) \n 
~~ ~~ @ responses . activate \n 
def test_it_should_remove_child_agencies_and_advertisers_when_removing_organization ( self ) : \n 
remove_advertiser_ids = [ 8 , 9 , 10 ] \n 
remove_agency_ids = [ 4 , 5 ] \n 
for advertiser_id in remove_advertiser_ids : \n 
~~~ assert advertiser_id in p . advertiser . keys ( ) , ~~ for agency_id in remove_agency_ids : \n 
~~~ assert agency_id in p . agency . keys ( ) , . format ( agency_id \n 
~~ p . remove ( , 2 ) \n 
~~~ assert advertiser_id not in p . advertiser . keys ( ) , . format ( advertiser_id ) \n 
~~ for agency_id in remove_agency_ids : \n 
~~~ assert agency_id not in p . agency . keys ( ) , . format ( agency_id ) \n 
def test_it_should_add_entity_ids_on_save ( self ) : \n 
p . add ( , 10 ) \n 
data = p . _generate_save_data ( ) \n 
assert sorted ( data [ ] ) == [ 1 , 2 , 10 ] , data [ ] \n 
def test_it_should_add_access_to_empty_permissions ( self ) : \n 
assert sorted ( data [ ] ) == [ 10 ] , data [ ] \n 
~~ ~~ VERSION = ( 0 , 1 , 9 ) \n 
__version__ = "0.1.9" \n 
needs_sphinx = \n 
extensions = [ , , , ] \n 
import pkg_resources \n 
~~~ release = pkg_resources . get_distribution ( ) . version \n 
~~ except pkg_resources . DistributionNotFound : \n 
~~ del pkg_resources \n 
version = . join ( release . split ( ) [ : 2 ] ) \n 
html_use_smartypants = True \n 
import math \n 
from shutil import rmtree \n 
from mrec import load_sparse_matrix , save_recommender \n 
class ItemSimilarityRunner ( object ) : \n 
~~~ def run ( self , view , model , input_format , trainfile , num_engines , simsdir , overwrite , max_sims , simsfile , modelfile \n 
~~~ logging . info ( ) \n 
dataset = load_sparse_matrix ( input_format , trainfile ) \n 
num_users , num_items = dataset . shape \n 
del dataset \n 
logging . info ( , num_users , num_items ) \n 
logging . info ( . format ( simsdir ) ) \n 
subprocess . check_call ( [ , , simsdir ] ) \n 
done = [ ] \n 
if not overwrite : \n 
done . extend ( self . find_done ( simsdir ) ) \n 
if done : \n 
~~~ logging . info ( . format ( len ( done ) ) ) \n 
~~ ~~ logging . info ( ) \n 
tasks = self . create_tasks ( model , input_format , trainfile , simsdir , num_items , num_engines , max_sims \n 
if num_engines > 0 : \n 
~~~ logging . info ( \n 
, len ( tasks ) ) \n 
async_job = view . map_async ( process , tasks , retries = 2 ) \n 
results = async_job . get ( ) \n 
results = [ process ( task ) for task in tasks ] \n 
~~ logging . info ( ) \n 
done = self . find_done ( simsdir ) \n 
remaining = len ( tasks ) - len ( done ) \n 
if remaining == 0 : \n 
logging . info ( . format ( len ( done ) ) ) \n 
paths = [ os . path . join ( simsdir , . format ( start , end ) ) for start , end in done cmd = [ ] + paths \n 
subprocess . check_call ( cmd , stdout = open ( simsfile , ) ) \n 
logging . info ( ) \n 
rmtree ( simsdir ) \n 
logging . info ( , \n 
num_items , type ( model ) . __name__ , simsfile ) \n 
model . load_similarity_matrix ( simsfile , num_items ) \n 
save_recommender ( model , modelfile ) \n 
~~~ logging . error ( . format ( remaining , len logging . error ( ) \n 
~~ ~~ def find_done ( self , outdir ) : \n 
~~~ success_files = glob . glob ( os . path . join ( outdir , ) ) \n 
r = re . compile ( ) \n 
for path in success_files : \n 
~~~ m = r . match ( path ) \n 
start = int ( m . group ( 1 ) ) \n 
end = int ( m . group ( 2 ) ) \n 
done . append ( ( start , end ) ) \n 
~~ return done \n 
~~ def create_tasks ( self , model , input_format , trainfile , outdir , num_items , num_engines , max_similar_items ~~~ if num_engines == 0 : \n 
~~~ num_engines = 1 \n 
~~ items_per_engine = int ( math . ceil ( float ( num_items ) / num_engines ) ) \n 
tasks = [ ] \n 
for start in xrange ( 0 , num_items , items_per_engine ) : \n 
~~~ end = min ( num_items , start + items_per_engine ) \n 
if ( start , end ) not in done : \n 
~~~ tasks . append ( ( model , input_format , trainfile , outdir , start , end , max_similar_items ) ) \n 
~~ ~~ return tasks \n 
~~ ~~ def process ( task ) : \n 
from mrec import load_fast_sparse_matrix \n 
model , input_format , trainfile , outdir , start , end , max_similar_items = task \n 
dataset = load_fast_sparse_matrix ( input_format , trainfile ) \n 
if hasattr ( model , ) : \n 
~~~ model . similarity_matrix = None \n 
~~ outfile = os . path . join ( outdir , . format ( start , end ) ) \n 
out = open ( outfile , ) \n 
for j in xrange ( start , end ) : \n 
~~~ w = model . get_similar_items ( j , max_similar_items = max_similar_items , dataset = dataset ) \n 
for k , v in w : \n 
~~ ~~ out . close ( ) \n 
cmd = [ , os . path . join ( outdir , . format ( start , end ) ) ] \n 
subprocess . check_call ( cmd ) \n 
return start , end \n 
import inspect \n 
NO_DEFAULT = object ( ) \n 
def func ( function ) : \n 
~~~ def wrapper ( obj , * args , ** kwargs ) : \n 
~~~ if evaluator_is_first_arg : \n 
~~~ cache = obj . memoize_cache \n 
~~~ cache = args [ 0 ] . memoize_cache \n 
~~~ cache = obj . _evaluator . memoize_cache \n 
~~~ memo = cache [ function ] \n 
~~~ memo = { } \n 
cache [ function ] = memo \n 
~~ key = ( obj , args , frozenset ( kwargs . items ( ) ) ) \n 
if key in memo : \n 
~~~ return memo [ key ] \n 
~~~ if default is not NO_DEFAULT : \n 
~~~ memo [ key ] = default \n 
~~ rv = function ( obj , * args , ** kwargs ) \n 
if inspect . isgenerator ( rv ) : \n 
~~~ rv = list ( rv ) \n 
~~ memo [ key ] = rv \n 
return rv \n 
~~ ~~ return wrapper \n 
~~ return func \n 
~~ class CachedMetaClass ( type ) : \n 
@ memoize_default ( None , second_arg_is_evaluator = True ) \n 
~~~ return super ( CachedMetaClass , self ) . __call__ ( * args , ** kwargs ) \n 
import __main__ \n 
from jedi import Interpreter \n 
from jedi . api . helpers import completion_parts \n 
from jedi . parser . user_context import UserContext \n 
def setup_readline ( namespace_module = __main__ ) : \n 
class JediRL ( object ) : \n 
~~~ def complete ( self , text , state ) : \n 
if state == 0 : \n 
~~~ sys . path . insert ( 0 , os . getcwd ( ) ) \n 
~~~ interpreter = Interpreter ( text , [ namespace_module . __dict__ ] ) \n 
path = UserContext ( text , ( 1 , len ( text ) ) ) . get_path_until_cursor ( ) \n 
path , dot , like = completion_parts ( path ) \n 
before = text [ : len ( text ) - len ( like ) ] \n 
completions = interpreter . completions ( ) \n 
~~~ sys . path . pop ( 0 ) \n 
~~ self . matches = [ before + c . name_with_symbols for c in completions ] \n 
~~~ return self . matches [ state ] \n 
~~ except IndexError : \n 
~~~ import readline \n 
~~~ readline . set_completer ( JediRL ( ) . complete ) \n 
readline . set_completer_delims ( ) \n 
~~ ~~ def version_info ( ) : \n 
Version = namedtuple ( , ) \n 
from jedi import __version__ \n 
tupl = re . findall ( , __version__ ) \n 
return Version ( * [ x if i == 3 else int ( x ) for i , x in enumerate ( tupl ) ] ) \n 
~~ import collections \n 
from . Utils import _write_complex_object \n 
class ExceptionDetails ( object ) : \n 
_defaults = collections . OrderedDict ( [ \n 
( , None ) , \n 
( , True ) , \n 
( , [ ] ) \n 
self . _values = { \n 
self . _initialize ( ) \n 
def id ( self ) : \n 
if in self . _values : \n 
~~~ return self . _values [ ] \n 
~~ return self . _defaults [ ] \n 
~~ @ id . setter \n 
def id ( self , value ) : \n 
if value == self . _defaults [ ] and in self . _values : \n 
~~~ del self . _values [ ] \n 
~~~ self . _values [ ] = value \n 
def outer_id ( self ) : \n 
~~ @ outer_id . setter \n 
def outer_id ( self , value ) : \n 
def type_name ( self ) : \n 
return self . _values [ ] \n 
~~ @ type_name . setter \n 
def type_name ( self , value ) : \n 
self . _values [ ] = value \n 
def message ( self ) : \n 
~~ @ message . setter \n 
def message ( self , value ) : \n 
def has_full_stack ( self ) : \n 
~~ @ has_full_stack . setter \n 
def has_full_stack ( self , value ) : \n 
def stack ( self ) : \n 
~~ @ stack . setter \n 
def stack ( self , value ) : \n 
def parsed_stack ( self ) : \n 
~~ self . _values [ ] = copy . deepcopy ( self . _defaults [ ] ) \n 
~~ @ parsed_stack . setter \n 
def parsed_stack ( self , value ) : \n 
~~ ~~ def _initialize ( self ) : \n 
~~ def write ( self ) : \n 
return _write_complex_object ( self . _defaults , self . _values ) \n 
~~ ~~ import random \n 
~~~ import BaseHTTPServer as HTTPServer \n 
~~~ import http . server as HTTPServer \n 
~~ import sys , os , os . path \n 
rootDirectory = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , , ) \n 
if rootDirectory not in sys . path : \n 
~~~ sys . path . append ( rootDirectory ) \n 
~~ from applicationinsights import channel \n 
class TestSenderBase ( unittest . TestCase ) : \n 
~~~ def test_construct ( self ) : \n 
~~~ actual = channel . SenderBase ( ) \n 
self . assertIsNotNone ( actual ) \n 
self . assertEqual ( , actual . service_endpoint_uri ) \n 
self . assertIsNone ( actual . queue ) \n 
self . assertEqual ( 100 , actual . send_buffer_size ) \n 
~~ def test_service_endpoint_uri_works_as_expected ( self ) : \n 
actual . service_endpoint_uri = \n 
~~ def test_queue_works_as_expected ( self ) : \n 
expected = object ( ) \n 
actual . queue = expected \n 
self . assertEqual ( expected , actual . queue ) \n 
~~ def test_send_buffer_size_works_as_expected ( self ) : \n 
actual . send_buffer_size = 42 \n 
self . assertEqual ( 42 , actual . send_buffer_size ) \n 
actual . send_buffer_size = - 1 \n 
self . assertEqual ( 1 , actual . send_buffer_size ) \n 
~~ def test_send_works_as_expected ( self ) : \n 
~~~ port = random . randint ( 50000 , 60000 ) \n 
actual = channel . SenderBase ( "http://localhost:" + str ( port ) + "/track" ) \n 
actual . queue = channel . QueueBase ( None ) \n 
thread = WorkerThread ( actual ) \n 
thread . join ( ) \n 
if "failed" in dir ( self ) : \n 
~~~ self . fail ( self . failed ) \n 
~~ self . assertEqual ( None , actual . queue . get ( ) ) \n 
~~ ~~ class WorkerThread ( threading . Thread ) : \n 
~~~ def __init__ ( self , sender ) : \n 
self . sender = sender \n 
~~~ time . sleep ( 1 ) \n 
self . sender . send ( [ MockSerializable ( 42 ) , MockSerializable ( 13 ) ] ) \n 
~~ ~~ class MockSerializable ( object ) : \n 
~~~ self . _data = data \n 
~~~ return self . _data \n 
~~ ~~ class MockHTTPRequestHandler ( HTTPServer . BaseHTTPRequestHandler ) : \n 
~~~ ExpectedContent = None \n 
TestCase = None \n 
def do_POST ( self ) : \n 
~~~ contentLength = int ( self . headers [ ] ) \n 
content = self . rfile . read ( contentLength ) \n 
response = "" \n 
if isinstance ( content , bytes ) : \n 
~~~ content = content . decode ( "utf-8" ) \n 
response = b"" \n 
~~ if "POST" != self . command : \n 
~~~ MockHTTPRequestHandler . TestCase . failed = \'"\' + MockHTTPRequestHandler . ExpectedContent + \n 
~~ self . send_response ( 200 ) \n 
self . send_header ( "Content-Type" , "application/json" ) \n 
self . send_header ( "Content-Length" , "0" ) \n 
self . end_headers ( ) \n 
self . wfile . write ( response ) \n 
~~ ~~ def runHttpHandlerOnce ( server = HTTPServer . HTTPServer , handler = HTTPServer . BaseHTTPRequestHandler , port ~~~ serverAddress = ( , port ) \n 
httpd = server ( serverAddress , handler ) \n 
httpd . handle_request ( ) from . import TestEnable # \n 
import sys as _sys \n 
from operator import itemgetter as _itemgetter \n 
from keyword import iskeyword as _iskeyword \n 
################################################################################ \n 
class tagtuple ( tuple ) : \n 
__slots__ = ( ) \n 
def __new__ ( cls , * args ) : \n 
return super ( tagtuple , cls ) . __new__ ( cls , args ) \n 
return type ( self ) . __name__ + super ( tagtuple , self ) . __repr__ ( ) \n 
~~ def __getnewargs__ ( self ) : \n 
return tuple ( self ) \n 
~~~ return type ( self ) is type ( other ) and super ( tagtuple , self ) . __eq__ ( other ) \n 
~~ def __getslice__ ( self , i , j ) : \n 
~~~ return type ( self ) ( * super ( tagtuple , self ) . __getslice__ ( i , j ) ) \n 
~~ __add__ = property ( ) \n 
__mul__ = property ( ) \n 
__rmul__ = property ( ) \n 
count = property ( ) \n 
index = property ( ) \n 
~~ _class_template = \n 
_repr_template = \n 
_field_template = \n 
def rectuple ( typename , field_names , verbose = False , rename = False ) : \n 
if isinstance ( field_names , basestring ) : \n 
~~~ field_names = field_names . replace ( , ) . split ( ) \n 
~~ field_names = map ( str , field_names ) \n 
if rename : \n 
~~~ seen = set ( ) \n 
for index , name in enumerate ( field_names ) : \n 
~~~ if ( not all ( c . isalnum ( ) or c == for c in name ) \n 
or _iskeyword ( name ) \n 
or not name \n 
or name [ 0 ] . isdigit ( ) \n 
or name . startswith ( ) \n 
or name in seen ) : \n 
~~~ field_names [ index ] = % index \n 
~~ seen . add ( name ) \n 
~~ ~~ for name in [ typename ] + field_names : \n 
~~~ if not all ( c . isalnum ( ) or c == for c in name ) : \n 
~~~ raise ValueError ( \n 
% name ) \n 
~~ if _iskeyword ( name ) : \n 
~~ if name [ 0 ] . isdigit ( ) : \n 
~~ ~~ seen = set ( ) \n 
for name in field_names : \n 
~~~ if name . startswith ( ) and not rename : \n 
~~ if name in seen : \n 
~~~ raise ValueError ( % name ) \n 
~~ class_definition = _class_template . format ( \n 
typename = typename , \n 
field_names = tuple ( field_names ) , \n 
num_fields = len ( field_names ) , \n 
arg_list = repr ( tuple ( field_names ) ) . replace ( "\'" , "" ) [ 1 : - 1 ] , \n 
repr_fmt = . join ( _repr_template . format ( name = name ) \n 
for name in field_names ) , \n 
field_defs = . join ( _field_template . format ( index = index , name = name ) \n 
for index , name in enumerate ( field_names ) ) \n 
if verbose : \n 
~~~ print class_definition \n 
~~ namespace = dict ( _itemgetter = _itemgetter , __name__ = % typename , \n 
OrderedDict = OrderedDict , _property = property , _tuple = tuple ) \n 
~~~ exec class_definition in namespace \n 
~~ except SyntaxError as e : \n 
~~~ raise SyntaxError ( e . message + + class_definition ) \n 
~~ result = namespace [ typename ] \n 
~~~ result . __module__ = _sys . _getframe ( 1 ) . f_globals . get ( , ) \n 
~~ except ( AttributeError , ValueError ) : \n 
from itertools import chain , product \n 
class A ( tagtuple ) : \n 
~~~ __slots__ = ( ) \n 
~~ class B ( tagtuple ) : \n 
~~ a = A ( 1 , 2 , 3 ) \n 
b = B ( 1 , 2 , 3 ) \n 
t = ( 1 , 2 , 3 ) \n 
d = { } \n 
d [ a ] = 1 \n 
d [ b ] = 2 \n 
d [ t ] = 3 \n 
s = set ( ) \n 
s . add ( a ) \n 
s . add ( b ) \n 
s . add ( t ) \n 
a0 = pickle . loads ( pickle . dumps ( a , 0 ) ) \n 
a1 = pickle . loads ( pickle . dumps ( a , 1 ) ) \n 
a2 = pickle . loads ( pickle . dumps ( a , 2 ) ) \n 
A = rectuple ( , , verbose = True ) \n 
B = rectuple ( , , verbose = True ) \n 
a = A ( 1 , 2 ) \n 
b = B ( 1 , 2 ) \n 
t = ( 1 , 2 ) \n 
~~ import pandas \n 
import util \n 
import matplotlib . pyplot as plt \n 
import scipy as sp \n 
import scipy . stats \n 
cur_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) \n 
def from_custom_file ( data_file , learn_options ) : \n 
data = pandas . read_csv ( data_file ) \n 
mandatory_columns = [ , , , ] \n 
for col in mandatory_columns : \n 
~~ Xdf = pandas . DataFrame ( data ) \n 
Xdf [ ] = Xdf [ ] \n 
Xdf = Xdf . set_index ( [ , ] ) \n 
Xdf . index . names = [ , ] \n 
Xdf [ ] = [ % i for i in range ( Xdf . shape [ 0 ] ) ] \n 
Xdf = Xdf . set_index ( , append = True ) \n 
Y = None \n 
gene_position = Xdf [ [ , ] ] \n 
target_genes = np . unique ( Xdf . index . levels [ 1 ] ) \n 
learn_options = set_V2_target_names ( learn_options ) \n 
return Xdf , Y , gene_position , target_genes \n 
~~ def from_file ( data_file , learn_options , data_file2 = None , data_file3 = None ) : \n 
annotations , gene_position , target_genes , Xdf , Y = read_V1_data ( data_file , learn_options ) \n 
learn_options [ ] = \n 
~~~ Xdf , drugs_to_genes , target_genes , Y , gene_position = read_V2_data ( data_file , learn_options ) \n 
xx = Xdf [ ] . values \n 
yy = Y [ ] . values \n 
rr , pp = sp . stats . pearsonr ( xx , yy ) \n 
~~~ learn_options [ ] = \n 
learn_options [ ] = None \n 
Xdf , Y , gene_position , target_genes = mergeV1_V2 ( data_file , data_file2 , learn_options ) \n 
Xdf , Y , gene_position , target_genes = merge_all ( data_file , data_file2 , data_file3 , learn_options \n 
~~ elif learn_options [ ] == 5 : \n 
gene_position , target_genes , Xdf , Y = read_xu_et_al ( data_file3 ) \n 
~~ Xdf [ "30mer" ] = Xdf [ "30mer" ] . apply ( lambda x : x [ 0 : 30 ] ) \n 
~~ def set_V2_target_names ( learn_options ) : \n 
~~~ if not in learn_options . keys ( ) : \n 
~~ if not in learn_options . keys ( ) : \n 
~~ learn_options [ ] = \n 
return learn_options \n 
~~ def combine_organisms ( human_data , mouse_data ) : \n 
~~~ cd13 = human_data . xs ( , level = , drop_level = False ) \n 
X_CD13 , Y_CD13 = util . get_data ( cd13 , y_names = [ , ] ) \n 
cd33 = human_data . xs ( , level = , drop_level = False ) \n 
X_CD33 , Y_CD33 = util . get_data ( cd33 , y_names = [ , , ] ) \n 
cd15 = human_data . xs ( , level = , drop_level = False ) \n 
X_CD15 , Y_CD15 = util . get_data ( cd15 , y_names = [ ] ) \n 
mouse_X = pandas . DataFrame ( ) \n 
mouse_Y = pandas . DataFrame ( ) \n 
for k in mouse_data . index . levels [ 1 ] : \n 
mouse_Y = pandas . concat ( [ mouse_Y , Y ] , axis = 0 ) \n 
~~ X = pandas . concat ( [ X_CD13 , X_CD15 , X_CD33 , mouse_X ] , axis = 0 ) \n 
Y = pandas . concat ( [ Y_CD13 , Y_CD15 , Y_CD33 , mouse_Y ] , axis = 0 ) \n 
return X , Y \n 
~~ def read_V1_data ( data_file , learn_options , AML_file = cur_dir + "/data/V1_suppl_data.txt" ) : \n 
~~~ if data_file is None : \n 
~~~ data_file = cur_dir + "/data/V1_data.xlsx" \n 
~~ human_data = pandas . read_excel ( data_file , sheetname = 0 , index_col = [ 0 , 1 ] ) \n 
mouse_data = pandas . read_excel ( data_file , sheetname = 1 , index_col = [ 0 , 1 ] ) \n 
Xdf , Y = combine_organisms ( human_data , mouse_data ) \n 
annotations = pandas . read_csv ( AML_file , delimiter = , index_col = [ 0 , 4 ] ) \n 
annotations . index . names = Xdf . index . names \n 
gene_position = pandas . merge ( Xdf , annotations , how = "inner" , left_index = True , right_index = True ) \n 
gene_position = util . impute_gene_position ( gene_position ) \n 
gene_position = gene_position [ [ , , Y = Y . loc [ gene_position . index ] \n 
Xdf = Xdf . loc [ gene_position . index ] \n 
target_genes = Y [ ] . unique ( ) \n 
Y . index . names = [ , ] \n 
if learn_options is not None and learn_options [ "flipV1target" ] : \n 
~~~ print "************************************************************************" \n 
print "************************************************************************" \n 
import ipdb \n 
ipdb . set_trace ( ) \n 
~~ return annotations , gene_position , target_genes , Xdf , Y \n 
~~ def rank_transform ( x ) : \n 
~~~ return 1.0 - sp . stats . mstats . rankdata ( x ) / sp . stats . mstats . rankdata ( x ) . max ( ) \n 
~~ def read_xu_et_al ( data_file , learn_options = None , verbose = True , subsetting = ) : \n 
~~~ data_file = \n 
~~ datasets = [ , , ] \n 
aggregated = None \n 
for d in datasets : \n 
~~~ data_efficient = pandas . read_excel ( data_file , sheetname = % d , skiprows = 2 data_inefficient = pandas . read_excel ( data_file , sheetname = % d , skiprows \n 
data_efficient [ ] = 1. \n 
data_inefficient [ ] = 0. \n 
exp_data = pandas . concat ( ( data_efficient , data_inefficient ) ) \n 
exp_data [ ] = exp_data . groupby ( ) [ ] . transform ( exp_data [ ] = exp_data . groupby ( ) [ ] . transform ( \n 
if aggregated is None : \n 
~~~ aggregated = exp_data \n 
~~~ aggregated = pandas . concat ( ( aggregated , exp_data ) ) \n 
~~ ~~ if subsetting == : \n 
~~~ aggregated [ "sequence(target+3\'+5\')" ] = aggregated [ "sequence(target+3\'+5\')" ] . apply ( lambda x : ~~ else : \n 
~~~ aggregated [ "sequence(target+3\'+5\')" ] = aggregated [ "sequence(target+3\'+5\')" ] . apply ( lambda x : \n 
~~ aggregated [ "sequence(target+3\'+5\')" ] = aggregated [ "sequence(target+3\'+5\')" ] . apply ( lambda x : x . upper \n 
aggregated . rename ( columns = { "sequence(target+3\'+5\')" : , : , \n 
aggregated [ ] . loc [ aggregated [ ] == ] = \n 
aggregated [ ] = aggregated [ [ , ] ] . mean ( axis = 1 ) \n 
df = aggregated \n 
df = df . rename ( columns = { : , : } ) \n 
df [ ] = \n 
df [ ] = 1 \n 
df = df . set_index ( [ , , ] ) \n 
df [ ] = df . index . get_level_values ( 0 ) \n 
df [ ] = df . index . get_level_values ( 1 ) \n 
df [ ] = df [ ] \n 
df [ ] = 0 \n 
target_genes = np . unique ( df [ ] . values ) \n 
return df [ [ , , ] ] , target_genes \n 
~~ def read_V2_data ( data_file , learn_options = None , verbose = True ) : \n 
~~~ data_file = cur_dir + "/data/V2_data.xlsx" \n 
Xdf = pandas . DataFrame ( ) \n 
known_pairs = { : [ , , , ] , \n 
: [ , , , ] } \n 
drugs_to_genes = { : [ , , , ] , \n 
if learn_options is not None : \n 
if learn_options [ ] : \n 
~~~ drugs_to_genes [ ] . extend ( [ , , ] ) \n 
~~ elif learn_options [ ] : \n 
~~~ drugs_to_genes [ ] . extend ( [ , , , ] ) \n 
drugs_to_genes [ ] . extend ( [ , , , ] ) \n 
drugs_to_genes [ ] . extend ( [ , , , , , \n 
~~ ~~ count = 0 \n 
for drug in drugs_to_genes . keys ( ) : \n 
~~~ genes = drugs_to_genes [ drug ] \n 
for g in genes : \n 
~~~ Xtmp = data . copy ( ) . xs ( g , level = , drop_level = False ) \n 
Xtmp [ ] = drug \n 
if g in known_pairs [ drug ] : \n 
~~~ Xtmp [ ] = 1. \n 
~~~ Xtmp [ ] = 0. \n 
~~ count = count + Xtmp . shape [ 0 ] \n 
Xdf = pandas . concat ( [ Xdf , Xtmp ] , axis = 0 ) \n 
~~ ~~ ~~ Xdf = Xdf . set_index ( , append = True ) \n 
Y = pandas . DataFrame ( Xdf . pop ( "score" ) ) \n 
Y . columns . names = [ "score" ] \n 
test_gene = pandas . DataFrame ( Xdf . pop ( ) ) \n 
target = pandas . DataFrame ( Xdf . index . get_level_values ( ) . values , index = Y . index , columns Y = pandas . concat ( ( Y , target , test_gene ) , axis = 1 ) \n 
y_rank = pandas . DataFrame ( ) \n 
y_threshold = pandas . DataFrame ( ) \n 
y_quant = pandas . DataFrame ( ) \n 
~~~ gene_list = drugs_to_genes [ drug ] \n 
for gene in gene_list : \n 
y_rank = pandas . concat ( ( y_rank , y_ranktmp ) , axis = 0 ) \n 
y_threshold = pandas . concat ( ( y_threshold , y_thresholdtmp ) , axis = 0 ) \n 
y_quant = pandas . concat ( ( y_quant , y_quanttmp ) , axis = 0 ) \n 
~~ ~~ yall = pandas . concat ( ( y_rank , y_threshold , y_quant ) , axis = 1 ) \n 
Y = pandas . merge ( Y , yall , how = , left_index = True , right_index = True ) \n 
~~~ ytmp = pandas . DataFrame ( Y . xs ( drug , level = "drug" , drop_level = False ) [ ] ) \n 
~~ yall = pandas . concat ( ( y_rank , y_threshold , y_quant ) , axis = 1 ) \n 
PLOT = False \n 
if PLOT : \n 
~~~ labels = [ "score" , "score_drug_gene_rank" , "score_drug_rank" , "score_drug_gene_threshold" , "score_drug_threshold" \n 
for label in labels : \n 
~~~ plt . figure ( ) \n 
plt . plot ( Xdf [ ] . values , Y [ label ] . values , ) \n 
r , pearp = sp . stats . pearsonr ( Xdf [ ] . values . flatten ( ) , Y [ label ] . values . flatten plt . title ( label + % ( r , pearp ) ) \n 
plt . ylabel ( label ) \n 
~~ ~~ gene_position = util . impute_gene_position ( gene_position ) \n 
if learn_options is not None and learn_options [ "weighted" ] == "variance" : \n 
experiments = { } \n 
experiments [ ] = [ , , , ] \n 
variance = None \n 
~~~ data_tmp = data . iloc [ data . index . get_level_values ( ) . isin ( drugs_to_genes [ drug data_tmp [ "drug" ] = drug \n 
data_tmp = data_tmp . set_index ( , append = True ) \n 
data_tmp [ "variance" ] = np . var ( data_tmp . values , axis = 1 ) \n 
if variance is None : \n 
~~~ variance = data_tmp [ "variance" ] . copy ( ) \n 
~~~ variance = pandas . concat ( ( variance , data_tmp [ "variance" ] ) , axis = 0 ) \n 
~~ ~~ orig_index = Y . index . copy ( ) \n 
Y = pandas . merge ( Y , pandas . DataFrame ( variance ) , how = "inner" , left_index = True , right_index = True Y = Y . ix [ orig_index ] \n 
print "done." \n 
return Xdf , drugs_to_genes , target_genes , Y , gene_position \n 
~~ def merge_all ( data_file = None , data_file2 = None , data_file3 = None , learn_options = None ) : \n 
~~~ Xdf , Y , gene_position , target_genes = mergeV1_V2 ( data_file , data_file2 , learn_options ) \n 
gene_position_xu , target_genes_xu , Xdf_xu , Y_xu = read_xu_et_al ( data_file3 , learn_options ) \n 
Xdf = pandas . concat ( ( Xdf , Xdf_xu ) ) \n 
Y = pandas . concat ( ( Y , Y_xu ) ) \n 
gene_position = pandas . concat ( ( gene_position , gene_position_xu ) ) \n 
target_genes = np . concatenate ( ( target_genes , target_genes_xu ) ) \n 
~~ def mergeV1_V2 ( data_file , data_file2 , learn_options ) : \n 
annotations , gene_position1 , target_genes1 , Xdf1 , Y1 = read_V1_data ( data_file , learn_options ) \n 
Xdf2 , drugs_to_genes , target_genes2 , Y2 , gene_position2 = read_V2_data ( data_file2 ) \n 
Y1 [ "drug" ] = [ "nodrug" for x in range ( Y1 . shape [ 0 ] ) ] \n 
Y1 = Y1 . set_index ( , append = True ) \n 
Y1 . index . names = [ , , ] \n 
Y_cols_to_keep = np . unique ( [ , , , \n 
Y1 = Y1 [ Y_cols_to_keep ] \n 
Y2 = Y2 [ Y_cols_to_keep ] \n 
Xdf1 [ "drug" ] = [ "nodrug" for x in range ( Xdf1 . shape [ 0 ] ) ] \n 
Xdf1 = Xdf1 . set_index ( , append = True ) \n 
X_cols_to_keep = [ , ] \n 
Xdf1 = Xdf1 [ X_cols_to_keep ] \n 
Xdf2 = Xdf2 [ X_cols_to_keep ] \n 
gene_position1 [ "drug" ] = [ "nodrug" for x in range ( gene_position1 . shape [ 0 ] ) ] \n 
gene_position1 = gene_position1 . set_index ( , append = True ) \n 
gene_position1 . index . names = [ , , ] \n 
cols_to_keep = [ , ] \n 
gene_position1 = gene_position1 [ cols_to_keep ] \n 
gene_position2 = gene_position2 [ cols_to_keep ] \n 
Y = pandas . concat ( ( Y1 , Y2 ) , axis = 0 ) \n 
Xdf = pandas . concat ( ( Xdf1 , Xdf2 ) , axis = 0 ) \n 
gene_position = pandas . concat ( ( gene_position1 , gene_position2 ) ) \n 
target_genes = np . concatenate ( ( target_genes1 , target_genes2 ) ) \n 
save_to_file = False \n 
if save_to_file : \n 
~~~ Y . index . names = [ , , ] \n 
onedupind = np . where ( Y . index . duplicated ( ) ) [ 0 ] [ 0 ] \n 
alldupind = np . where ( Y . index . get_level_values ( 0 ) . values == Y . index [ onedupind ] [ 0 ] ) [ 0 ] \n 
newindex = Y . index . tolist ( ) \n 
newindex [ onedupind ] = ( newindex [ onedupind ] [ 0 ] , newindex [ onedupind ] [ 1 ] , "nodrug2" ) \n 
Y . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n 
Xdf . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n 
gene_position_tmp = gene_position . copy ( ) \n 
gene_position_tmp . index . names = [ , , ] \n 
gene_position_tmp . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n 
XandY = pandas . merge ( XandY , gene_position_tmp , how = "inner" , left_index = True , right_index = True \n 
XandY [ "30mer" ] = XandY [ "30mer" ] . apply ( lambda x : x [ 0 : 30 ] ) \n 
XandY . to_csv ( ) \n 
~~ return Xdf , Y , gene_position , target_genes \n 
~~ def get_V1_genes ( data_file = None ) : \n 
~~~ annotations , gene_position , target_genes , Xdf , Y = read_V1_data ( data_file , learn_options = None ) \n 
return target_genes \n 
~~ def get_V2_genes ( data_file = None ) : \n 
~~~ Xdf , drugs_to_genes , target_genes , Y , gene_position = read_V2_data ( data_file , verbose = False ) \n 
~~ def get_V3_genes ( data_fileV1 = None , data_fileV2 = None ) : \n 
~~~ target_genes = np . concatenate ( ( get_V1_genes ( data_fileV1 ) , get_V2_genes ( data_fileV2 ) ) ) \n 
~~ def get_xu_genes ( data_file = None ) : \n 
~~~ return read_xu_et_al ( data_file ) [ 1 ] \n 
~~ def get_mouse_genes ( data_file = None ) : \n 
return Xdf [ Xdf [ ] == ] [ ] . unique ( ) \n 
~~ def get_human_genes ( data_file = None ) : \n 
mouse_genes = Xdf [ Xdf [ ] == ] [ ] . unique ( ) \n 
all_genes = get_V3_genes ( None , None ) return np . setdiff1d ( all_genes , mouse_genes ) \n 
~~ from . detector import Detector \n 
from . lang_detect_exception import ErrorCode , LangDetectException \n 
from . utils . lang_profile import LangProfile \n 
class DetectorFactory ( object ) : \n 
seed = None \n 
~~~ self . word_lang_prob_map = { } \n 
self . langlist = [ ] \n 
~~ def load_profile ( self , profile_directory ) : \n 
~~~ list_files = os . listdir ( profile_directory ) \n 
if not list_files : \n 
~~~ raise LangDetectException ( ErrorCode . NeedLoadProfileError , + profile_directory \n 
~~ langsize , index = len ( list_files ) , 0 \n 
for filename in list_files : \n 
~~~ if filename . startswith ( ) : \n 
~~ filename = path . join ( profile_directory , filename ) \n 
if not path . isfile ( filename ) : \n 
~~ f = None \n 
~~~ if sys . version_info [ 0 ] < 3 : \n 
~~~ f = open ( filename , ) \n 
~~~ f = open ( filename , , encoding = ) \n 
~~ json_data = json . load ( f ) \n 
profile = LangProfile ( ** json_data ) \n 
self . add_profile ( profile , index , langsize ) \n 
~~~ if f : \n 
~~~ f . close ( ) \n 
~~ ~~ ~~ ~~ def load_json_profile ( self , json_profiles ) : \n 
~~~ langsize , index = len ( json_profiles ) , 0 \n 
if langsize < 2 : \n 
~~~ raise LangDetectException ( ErrorCode . NeedLoadProfileError , ) \n 
~~ for json_profile in json_profiles : \n 
~~~ json_data = json . loads ( json_profile ) \n 
~~~ raise LangDetectException ( ErrorCode . FormatError , ) \n 
~~ ~~ ~~ def add_profile ( self , profile , index , langsize ) : \n 
~~~ lang = profile . name \n 
if lang in self . langlist : \n 
~~~ raise LangDetectException ( ErrorCode . DuplicateLangError , ~~ self . langlist . append ( lang ) \n 
for word in profile . freq : \n 
~~~ if word not in self . word_lang_prob_map : \n 
~~~ self . word_lang_prob_map [ word ] = [ 0.0 ] * langsize \n 
~~ length = len ( word ) \n 
if 1 <= length <= 3 : \n 
~~~ prob = 1.0 * profile . freq . get ( word ) / profile . n_words [ length - 1 ] \n 
self . word_lang_prob_map [ word ] [ index ] = prob \n 
~~ ~~ ~~ def clear ( self ) : \n 
~~~ self . langlist = [ ] \n 
self . word_lang_prob_map = { } \n 
~~ def create ( self , alpha = None ) : \n 
detector = self . _create_detector ( ) \n 
if alpha is not None : \n 
~~~ detector . set_alpha ( alpha ) \n 
~~ return detector \n 
~~ def _create_detector ( self ) : \n 
~~~ if not self . langlist : \n 
~~ return Detector ( self ) \n 
~~ def set_seed ( self , seed ) : \n 
~~~ self . seed = seed \n 
~~ def get_lang_list ( self ) : \n 
~~~ return list ( self . langlist ) \n 
~~ ~~ PROFILES_DIRECTORY = path . join ( path . dirname ( __file__ ) , ) \n 
_factory = None \n 
def init_factory ( ) : \n 
~~~ global _factory \n 
if _factory is None : \n 
~~~ _factory = DetectorFactory ( ) \n 
_factory . load_profile ( PROFILES_DIRECTORY ) \n 
~~ ~~ def detect ( text ) : \n 
~~~ init_factory ( ) \n 
detector = _factory . create ( ) \n 
detector . append ( text ) \n 
return detector . detect ( ) \n 
~~ def detect_langs ( text ) : \n 
return detector . get_probabilities ( ) \n 
~~ import math \n 
def distance ( pa , pb ) : \n 
~~~ ax , ay = pa \n 
bx , by = pb \n 
return math . sqrt ( ( ax - bx ) ** 2 + ( ay - by ) ** 2 ) \n 
~~ def index_of_nearest ( p , hot_points , distance_f = distance ) : \n 
min_dist = None \n 
nearest_hp_i = None \n 
for i , hp in enumerate ( hot_points ) : \n 
~~~ dist = distance_f ( p , hp ) \n 
if min_dist is None or dist < min_dist : \n 
~~~ min_dist = dist \n 
nearest_hp_i = i \n 
~~ ~~ return nearest_hp_i \n 
~~ from pig_util import outputSchema \n 
@ outputSchema ( ) \n 
def reverse ( word ) : \n 
return word [ : : - 1 ] \n 
~~ @ outputSchema ( ) \n 
def num_chars ( word ) : \n 
~~ from fabric import main as fab_main \n 
from cloudferry import fabfile \n 
~~~ fab = fabfile . __file__ \n 
if fab . endswith ( ) : \n 
~~~ fab = fab [ : - 1 ] \n 
~~ fab_main . main ( [ fab ] ) \n 
~~ from cloudferry . lib . base . action import action \n 
DEFAULT = 0 \n 
PATH_ONE = 1 \n 
PATH_TWO = 2 \n 
class IsOption ( action . Action ) : \n 
~~~ def __init__ ( self , init , option_name ) : \n 
~~~ self . option_name = option_name \n 
super ( IsOption , self ) . __init__ ( init ) \n 
~~ def run ( self , ** kwargs ) : \n 
option_value = self . cfg . migrate [ self . option_name ] \n 
if option_value : \n 
~~~ self . set_next_path ( PATH_ONE ) \n 
~~~ self . set_next_path ( PATH_TWO ) \n 
~~ return { } \n 
~~ ~~ from cloudferry . lib . base . action import action \n 
from cloudferry . lib . utils import log \n 
from cloudferry . lib . utils import utils as utl \n 
LOG = log . getLogger ( __name__ ) \n 
class CheckConfigQuotaNeutron ( action . Action ) : \n 
def run ( self , ** kwargs ) : \n 
~~~ src_cloud = self . src_cloud \n 
dst_cloud = self . dst_cloud \n 
network_src = src_cloud . resources [ utl . NETWORK_RESOURCE ] \n 
identity_dst = dst_cloud . resources [ utl . IDENTITY_RESOURCE ] \n 
network_dst = dst_cloud . resources [ utl . NETWORK_RESOURCE ] \n 
search_opts_tenant = kwargs . get ( , { } ) \n 
tenants_src = self . get_src_tenants ( search_opts_tenant ) \n 
list_quotas = network_src . list_quotas ( ) \n 
tenants_without_quotas = self . get_tenants_without_quotas ( tenants_src , \n 
list_quotas ) \n 
if not tenants_without_quotas : \n 
quot = network_src . show_quota ( tenants_without_quotas [ 0 ] ) \n 
quot_default_dst = network_dst . show_quota ( dst_temp_tenant . id ) \n 
is_configs_different = False \n 
identity_dst . delete_tenant ( dst_temp_tenant ) \n 
for item_quot , val_quot in quot . iteritems ( ) : \n 
~~~ if val_quot != quot_default_dst [ item_quot ] : \n 
~~~ is_configs_different = True \n 
quot_default_dst [ item_quot ] ) \n 
~~ ~~ if not is_configs_different : \n 
def get_tenants_without_quotas ( tenants_src , list_quotas ) : \n 
~~~ tenants_ids = tenants_src . keys ( ) \n 
quotas_ids_tenants = [ quota [ "tenant_id" ] for quota in list_quotas ] \n 
return list ( set ( tenants_ids ) - set ( quotas_ids_tenants ) ) \n 
~~ def get_src_tenants ( self , search_opts ) : \n 
~~~ identity_src = self . src_cloud . resources [ utl . IDENTITY_RESOURCE ] \n 
if search_opts . get ( ) : \n 
~~~ filter_tenants_ids_list = search_opts [ ] \n 
tenants = [ identity_src . keystone_client . tenants . find ( id = tnt_id ) for \n 
tnt_id in filter_tenants_ids_list ] \n 
~~~ tenants = identity_src . get_tenants_list ( ) \n 
~~ tenants_dict = { tenant . id : tenant . name for tenant in tenants } \n 
return tenants_dict \n 
~~ ~~ import copy \n 
from oslo_config import cfg \n 
from cloudferry . lib . base . action import action \n 
from cloudferry . lib . utils import utils \n 
LOG = logging . getLogger ( __name__ ) \n 
class DetachVolumesCompute ( action . Action ) : \n 
~~~ def run ( self , info , ** kwargs ) : \n 
~~~ info = copy . deepcopy ( info ) \n 
compute_resource = self . cloud . resources [ utils . COMPUTE_RESOURCE ] \n 
storage_resource = self . cloud . resources [ utils . STORAGE_RESOURCE ] \n 
for instance in info [ utils . INSTANCES_TYPE ] . itervalues ( ) : \n 
instance [ ] [ ] , instance [ ] [ ] ) \n 
if not instance [ ] [ utils . VOLUMES_TYPE ] : \n 
~~ for vol in instance [ ] [ utils . VOLUMES_TYPE ] : \n 
~~~ volume_status = storage_resource . get_status ( vol [ ] ) \n 
vol [ ] , volume_status ) \n 
if volume_status == : \n 
~~~ compute_resource . detach_volume ( instance [ ] [ ] , \n 
vol [ ] ) \n 
timeout = CONF . migrate . storage_backend_timeout \n 
storage_resource . wait_for_status ( \n 
vol [ ] , storage_resource . get_status , , \n 
timeout = timeout ) \n 
~~ ~~ ~~ return { } \n 
from cloudferry . lib . utils . ssh_util import SshUtil \n 
class RemoteExecution ( action . Action ) : \n 
~~~ def __init__ ( self , cloud , host = None , int_host = None , config_migrate = None ) : \n 
~~~ self . cloud = cloud \n 
self . int_host = int_host \n 
self . config_migrate = config_migrate \n 
self . remote_exec_obj = SshUtil ( self . cloud , \n 
self . config_migrate , \n 
self . host ) \n 
super ( RemoteExecution , self ) . __init__ ( { } ) \n 
~~ def run ( self , command , ** kwargs ) : \n 
~~~ self . remote_exec_obj . execute ( command , self . int_host ) \n 
return { } \n 
from xml . etree import ElementTree \n 
nova_instances_path = "/var/lib/nova/instances/" \n 
def instance_path ( instance_id ) : \n 
~~~ return os . path . join ( nova_instances_path , instance_id ) \n 
~~ def instance_image_path ( instance_id ) : \n 
~~~ return os . path . join ( instance_path ( instance_id ) , "disk" ) \n 
~~ def _qemu_img_rebase ( src , dst ) : \n 
~~ class QemuBackingFileMover ( object ) : \n 
~~~ def __init__ ( self , runner , src , instance_id ) : \n 
~~~ self . runner = runner \n 
self . src = src \n 
self . dst = instance_image_path ( instance_id ) \n 
~~ def __enter__ ( self ) : \n 
~~~ cmd = _qemu_img_rebase ( self . src , self . dst ) \n 
self . runner . run ( cmd ) \n 
return self \n 
~~~ cmd = _qemu_img_rebase ( self . dst , self . src ) \n 
self . runner . run_ignoring_errors ( cmd ) \n 
~~ ~~ class DestNovaInstanceDestroyer ( object ) : \n 
def __init__ ( self , dest_libvirt , dest_nova , libvirt_name , nova_vm_id ) : \n 
~~~ self . dest_libvirt = dest_libvirt \n 
self . dest_nova = dest_nova \n 
self . libvirt_name = libvirt_name \n 
self . nova_vm_id = nova_vm_id \n 
~~~ self . do ( ) \n 
~~~ self . undo ( ) \n 
~~ def do ( self ) : \n 
~~~ self . dest_libvirt . destroy_vm ( self . libvirt_name ) \n 
~~ def undo ( self ) : \n 
self . dest_nova . reset_state ( self . nova_vm_id ) \n 
self . dest_nova . delete_vm_by_id ( self . nova_vm_id ) \n 
~~ except RuntimeError : \n 
~~ ~~ ~~ class Libvirt ( object ) : \n 
~~~ def __init__ ( self , remote_runner ) : \n 
self . runner = remote_runner \n 
~~ def get_backing_file ( self , instance_id ) : \n 
image_path = instance_image_path ( instance_id ) ) ) \n 
~~~ image_info = json . loads ( self . runner . run ( cmd ) ) \n 
return image_info [ ] \n 
~~ except ( ValueError , TypeError ) as e : \n 
instance_id ) \n 
~~ ~~ def get_xml ( self , libvirt_instance_name ) : \n 
inst_name = libvirt_instance_name ) ) \n 
return LibvirtXml ( self . runner . run ( cmd ) ) \n 
~~ def destroy_vm ( self , libvirt_instance_name ) : \n 
~~~ cmds = [ \n 
for cmd in cmds : \n 
~~~ self . runner . run ( cmd ) \n 
~~ ~~ def move_backing_file ( self , source_file , instance_id ) : \n 
~~~ cmd = _qemu_img_rebase ( src = source_file , \n 
dst = instance_image_path ( instance_id ) ) \n 
~~ def live_migrate ( self , libvirt_instance_name , dest_host , migration_xml ) : \n 
dst_host = dest_host , \n 
migration_xml = migration_xml ) ) \n 
~~ ~~ class LibvirtDeviceInterfaceHwAddress ( object ) : \n 
~~~ def __init__ ( self , element ) : \n 
~~~ self . type = element . get ( ) \n 
self . domain = element . get ( ) \n 
self . bus = element . get ( ) \n 
self . slot = element . get ( ) \n 
self . function = element . get ( ) \n 
~~~ return ( isinstance ( other , self . __class__ ) and \n 
self . type == other . type and \n 
self . domain == other . domain and \n 
self . bus == other . bus and \n 
self . slot == other . slot and \n 
self . function == other . function ) \n 
~~ ~~ class LibvirtDeviceInterface ( object ) : \n 
~~~ def __init__ ( self , interface ) : \n 
self . _xml_element = interface \n 
self . mac = interface . find ( ) . get ( ) \n 
self . source_iface = interface . find ( ) . get ( ) \n 
self . target_iface = interface . find ( ) . get ( ) \n 
self . hw_address = LibvirtDeviceInterfaceHwAddress ( \n 
interface . find ( ) ) \n 
self . source_iface == other . source_iface and \n 
self . target_iface == other . target_iface and \n 
self . hw_address == other . hw_address ) \n 
mac = self . mac , src = self . source_iface , dst = self . target_iface ) \n 
def _replace_attr ( cls , element , attr , value ) : \n 
~~~ if element . get ( attr ) != value : \n 
~~~ element . clear ( ) \n 
element . attrib = { attr : value } \n 
~~ ~~ def element ( self ) : \n 
~~~ source = self . _xml_element . find ( ) \n 
target = self . _xml_element . find ( ) \n 
self . _replace_attr ( source , , self . source_iface ) \n 
self . _replace_attr ( target , , self . target_iface ) \n 
return self . _xml_element \n 
~~ ~~ class LibvirtXml ( object ) : \n 
~~~ def __init__ ( self , contents ) : \n 
self . _xml = ElementTree . fromstring ( contents ) \n 
self . _interfaces = [ LibvirtDeviceInterface ( i ) \n 
for i in self . _xml . findall ( ) ] \n 
self . disk_file = self . _get ( , ) \n 
self . serial_file = self . _get ( , ) \n 
self . console_file = self . _get ( , ) \n 
~~ def _get ( self , element , attribute ) : \n 
~~~ el = self . _xml . find ( element ) \n 
if el is not None : \n 
~~~ return el . get ( attribute ) \n 
~~ ~~ def _set ( self , element , attribute , value ) : \n 
~~~ el . set ( attribute , value ) \n 
def interfaces ( self ) : \n 
~~~ return self . _interfaces \n 
~~ @ interfaces . setter \n 
def interfaces ( self , other ) : \n 
if len ( self . interfaces ) != len ( other ) : \n 
~~ for other_iface in other : \n 
~~~ for this_iface in self . interfaces : \n 
~~~ identical = ( this_iface . mac == other_iface . mac ) \n 
if identical : \n 
~~~ this_iface . source_iface = other_iface . source_iface \n 
this_iface . target_iface = other_iface . target_iface \n 
~~ ~~ ~~ ~~ def dump ( self ) : \n 
~~~ self . _set ( , , self . disk_file ) \n 
self . _set ( , , self . serial_file ) \n 
self . _set ( , , self . console_file ) \n 
xml_devices = self . _xml . find ( ) \n 
xml_interfaces = self . _xml . findall ( ) \n 
for iface in xml_interfaces : \n 
~~~ xml_devices . remove ( iface ) \n 
~~ for iface in self . _interfaces : \n 
~~~ xml_devices . append ( iface . element ( ) ) \n 
~~ return ElementTree . tostring ( self . _xml ) \n 
~~ ~~ import abc \n 
from cloudferry . lib . utils import files \n 
from cloudferry . lib . utils import remote_runner \n 
from cloudferry . lib . copy_engines import base \n 
class CopyFailed ( RuntimeError ) : \n 
~~ class CopyMechanism ( object ) : \n 
~~~ __metaclass__ = abc . ABCMeta \n 
@ abc . abstractmethod \n 
def copy ( self , context , source_object , destination_object ) : \n 
~~ ~~ class CopyObject ( object ) : \n 
~~~ def __init__ ( self , host = None , path = None ) : \n 
~~~ self . host = host \n 
~~~ return "{host}:{path}" . format ( host = self . host , path = self . path ) \n 
~~ ~~ class RemoteFileCopy ( CopyMechanism ) : \n 
~~~ data = { \n 
: source_object . host , \n 
: source_object . path , \n 
: destination_object . host , \n 
: destination_object . path \n 
~~~ copier = base . get_copier ( context . src_cloud , \n 
context . dst_cloud , \n 
data ) \n 
copier . transfer ( data ) \n 
~~ except ( base . FileCopyError , \n 
base . CopierCannotBeUsed , \n 
base . CopierNotFound ) as e : \n 
src_host = source_object . host , \n 
src_file = source_object . path , \n 
dst_host = destination_object . host , \n 
dst_file = destination_object . path , \n 
err = e . message ) \n 
raise CopyFailed ( msg ) \n 
~~ ~~ ~~ class CopyRegularFileToBlockDevice ( CopyMechanism ) : \n 
~~~ src_user = context . cfg . src . ssh_user \n 
dst_user = context . cfg . dst . ssh_user \n 
src_host = source_object . host \n 
dst_host = destination_object . host \n 
rr = remote_runner . RemoteRunner ( src_host , src_user ) \n 
ssh_opts = ( \n 
~~~ progress_view = "" \n 
if files . is_installed ( rr , "pv" ) : \n 
~~~ src_file_size = files . remote_file_size ( rr , source_object . path ) \n 
size = src_file_size ) \n 
rr . run ( copy . format ( src_file = source_object . path , \n 
dst_user = dst_user , \n 
dst_host = dst_host , \n 
ssh_opts = ssh_opts , \n 
dst_device = destination_object . path , \n 
progress_view = progress_view ) ) \n 
~~ except remote_runner . RemoteExecutionError as e : \n 
msg = msg . format ( src_object = source_object , \n 
dst_object = destination_object , \n 
error = e . message ) \n 
~~ ~~ ~~ import datetime \n 
from logging import config \n 
from logging import handlers \n 
from fabric import api \n 
import yaml \n 
from cloudferry . lib . utils import sizeof_format \n 
getLogger = logging . getLogger \n 
class StdoutLogger ( object ) : \n 
def __init__ ( self , name = None ) : \n 
~~~ self . log = logging . getLogger ( name or ) \n 
~~ def write ( self , message ) : \n 
~~~ message = message . strip ( ) \n 
if message : \n 
~~~ self . log . info ( message ) \n 
~~ ~~ def flush ( self ) : \n 
~~ ~~ def configure_logging ( log_config = None , debug = None , forward_stdout = None ) : \n 
if log_config is None : \n 
~~~ log_config = CONF . migrate . log_config \n 
~~ if debug is None : \n 
~~~ debug = CONF . migrate . debug \n 
~~ if forward_stdout is None : \n 
~~~ forward_stdout = CONF . migrate . forward_stdout \n 
~~ with open ( log_config , ) as f : \n 
~~~ config . dictConfig ( yaml . load ( f ) ) \n 
~~ if debug : \n 
~~~ logger = logging . getLogger ( ) \n 
for handler in logger . handlers : \n 
~~~ if handler . name == : \n 
~~~ handler . setLevel ( logging . DEBUG ) \n 
~~ ~~ ~~ if forward_stdout : \n 
~~~ sys . stdout = StdoutLogger ( ) \n 
~~ ~~ class RunRotatingFileHandler ( handlers . RotatingFileHandler ) : \n 
filename = , \n 
date_format = , \n 
** kwargs ) : \n 
~~~ self . date_format = date_format \n 
max_bytes = sizeof_format . parse_size ( kwargs . pop ( , 0 ) ) \n 
super ( RunRotatingFileHandler , self ) . __init__ ( \n 
filename = self . get_filename ( filename ) , \n 
maxBytes = max_bytes , \n 
** kwargs ) \n 
~~ def get_filename ( self , filename ) : \n 
if hasattr ( CONF , ) and hasattr ( CONF . migrate , ) : \n 
~~~ scenario_filename = os . path . basename ( CONF . migrate . scenario ) \n 
scenario = os . path . splitext ( scenario_filename ) [ 0 ] \n 
~~~ scenario = \n 
~~ dt = datetime . datetime . now ( ) . strftime ( self . date_format ) \n 
return filename % { \n 
: scenario , \n 
: dt \n 
~~ ~~ class CurrentTaskFilter ( logging . Filter ) : \n 
def __init__ ( self , name_format = , ** kwargs ) : \n 
~~~ super ( CurrentTaskFilter , self ) . __init__ ( ** kwargs ) \n 
self . name_format = name_format \n 
~~ def filter ( self , record ) : \n 
~~~ current_task = self . name_format % { \n 
: api . env . current_task or , \n 
record . current_task = current_task \n 
import cloudferry_devlab . tests . config as config \n 
from cloudferry_devlab . tests . data_collector import DataCollector \n 
from cloudferry_devlab . tests import functional_test \n 
import cloudferry_devlab . tests . utils as utils \n 
class RollbackVerification ( functional_test . FunctionalTest ) : \n 
~~~ data_collector = DataCollector ( config = config ) \n 
self . data_after = utils . convert ( data_collector . data_collector ( ) ) \n 
file_name = config . rollback_params [ ] [ ] \n 
pre_file_path = os . path . join ( self . cloudferry_dir , file_name ) \n 
with open ( pre_file_path , "r" ) as f : \n 
~~~ self . pre_data = yaml . load ( f ) \n 
~~ ~~ def test_verify_rollback ( self ) : \n 
self . maxDiff = None \n 
for cloud in self . data_after : \n 
~~~ for service in self . data_after [ cloud ] : \n 
~~~ for resource in self . data_after [ cloud ] [ service ] : \n 
~~~ print ( msg . format ( service . lower ( ) , resource . lower ( ) ) ) \n 
self . assertEqual ( self . data_after [ cloud ] [ service ] [ resource ] , \n 
self . pre_data [ cloud ] [ service ] [ resource ] ) \n 
~~ ~~ ~~ ~~ ~~ import mock \n 
from cloudferry . lib . os . actions import convert_volume_to_image \n 
from tests import test \n 
class ConverterVolumeToImageTest ( test . TestCase ) : \n 
~~~ super ( ConverterVolumeToImageTest , self ) . setUp ( ) \n 
self . fake_src_cloud = mock . Mock ( ) \n 
self . fake_storage = mock . Mock ( ) \n 
self . fake_storage . deploy = mock . Mock ( ) \n 
self . fake_storage . upload_volume_to_image . return_value = ( \n 
, ) \n 
self . fake_storage . get_backend . return_value = \n 
self . fake_image = mock . Mock ( ) \n 
self . fake_image . wait_for_status = mock . Mock ( ) \n 
self . fake_image . get_image_by_id_converted = mock . Mock ( ) \n 
self . fake_image . get_image_by_id_converted . return_value = { \n 
: { : , : { } } } } \n 
self . fake_image . patch_image = mock . Mock ( ) \n 
self . fake_src_cloud . resources = { : self . fake_storage , \n 
: self . fake_image } \n 
self . fake_volumes_info = { \n 
} } , \n 
self . fake_dst_cloud = mock . Mock ( ) \n 
self . fake_config = utils . ext_dict ( migrate = utils . ext_dict ( \n 
{ : , \n 
: } ) ) \n 
self . fake_init = { \n 
: self . fake_src_cloud , \n 
: self . fake_dst_cloud , \n 
: self . fake_config \n 
~~ def test_action ( self ) : \n 
~~~ fake_action = convert_volume_to_image . ConvertVolumeToImage ( \n 
self . fake_init , \n 
cloud = ) \n 
res = fake_action . run ( self . fake_volumes_info ) \n 
self . assertEqual ( , \n 
res [ ] [ ] [ ] [ ] ) \n 
res [ ] [ ] [ ] [ ] [ \n 
] [ ] ) \n 
~~ ~~ from cloudferry . lib . utils . cache import Memoized , Cached \n 
class MemoizationTestCase ( test . TestCase ) : \n 
~~~ def test_treats_self_as_separate_objects ( self ) : \n 
~~~ class C ( object ) : \n 
~~~ def __init__ ( self , i ) : \n 
~~~ self . i = i \n 
~~ @ Memoized \n 
def get_i ( self ) : \n 
~~~ return self . i \n 
~~ ~~ o1 = C ( 1 ) \n 
o2 = C ( 2 ) \n 
self . assertNotEqual ( o1 . get_i ( ) , o2 . get_i ( ) ) \n 
self . assertEqual ( o1 . get_i ( ) , 1 ) \n 
self . assertEqual ( o2 . get_i ( ) , 2 ) \n 
~~ def test_takes_value_from_cache ( self ) : \n 
~~ def set_i ( self , i ) : \n 
~~ ~~ original = 1 \n 
o = C ( original ) \n 
self . assertEqual ( o . get_i ( ) , original ) \n 
o . set_i ( 10 ) \n 
~~ ~~ class CacheTestCase ( test . TestCase ) : \n 
~~~ def test_resets_cache_when_modifier_called ( self ) : \n 
~~~ @ Cached ( getter = , modifier = ) \n 
class C ( object ) : \n 
~~ def get_i ( self ) : \n 
~~ ~~ o = C ( 1 ) \n 
self . assertEqual ( o . get_i ( ) , 1 ) \n 
o . set_i ( 100 ) \n 
self . assertEqual ( o . get_i ( ) , 100 ) \n 
~~ ~~ from django . http import HttpResponse , HttpResponseRedirect , HttpResponseNotFound \n 
from django . template import Context , loader \n 
from django . shortcuts import get_object_or_404 , render_to_response \n 
from django . core . exceptions import ObjectDoesNotExist \n 
from tagging . models import Tag , TaggedItem \n 
from django . views . decorators . csrf import csrf_exempt \n 
from django . contrib . auth . models import User \n 
from django . contrib . auth . decorators import login_required \n 
from django . contrib . auth import authenticate , login \n 
from django . core . mail import send_mail \n 
from django . http import Http404 \n 
from django . db . models import Q \n 
from openwatch . recordings . models import Recording \n 
from openwatch import recording_tags \n 
@ login_required \n 
def moderate ( request ) : \n 
response_values = { } \n 
org_tag = request . user . get_profile ( ) . org_tag \n 
if not request . user . is_superuser and ( not request . user . get_profile ( ) . can_moderate or org_tag == ~~~ raise Http404 \n 
~~ if recording_tags . ACLU_NJ in org_tag : \n 
~~~ location = { } \n 
location [ ] = 40.167274 \n 
location [ ] = - 74.616338 \n 
response_values [ ] = location \n 
~~ response_values [ ] = \n 
return render_to_response ( , response_values , context_instance = RequestContext ( request \n 
~~ def map ( request ) : \n 
~~~ total = "lots!" \n 
return render_to_response ( , { : total } , context_instance = RequestContext ( request ) \n 
~~ def size ( request ) : \n 
~~~ featureset = Recording . objects . filter ( ~ Q ( lat = None ) , ~ Q ( lon = None ) , ~ Q ( jtype = ) ) . exclude ( location__exact total = len ( featureset ) \n 
~~ def redir ( self ) : \n 
~~~ return HttpResponseRedirect ( ) \n 
~~ def map_json ( request ) : \n 
~~~ featureset = Recording . objects . all ( ) . order_by ( ) . filter ( ~ Q ( location = ) ) . exclude ( location__isnull resp = encode_queryset ( featureset ) \n 
return HttpResponse ( resp , mimetype = "application/json" ) \n 
~~ @ login_required \n 
def map_json_moderate ( request ) : \n 
~~~ org_tag = request . user . get_profile ( ) . org_tag \n 
if org_tag != : \n 
~~~ featureset = Recording . objects . filter ( org_approved = False , org_flagged = False , tags__contains = ~~ else : \n 
~~~ featureset = Recording . objects . all ( ) \n 
~~ featureset = featureset . order_by ( ) . filter ( ~ Q ( location = ) ) . exclude ( location__isnull = True ) resp = encode_queryset ( featureset ) \n 
~~ def map_location_json ( request , ne_lat = 0 , ne_lon = 0 , sw_lat = 0 , sw_lon = 0 ) : \n 
~~~ ne_lat = float ( ne_lat ) \n 
ne_lon = float ( ne_lon ) \n 
sw_lat = float ( sw_lat ) \n 
sw_lon = float ( sw_lon ) \n 
featureset = Recording . objects . filter ( lat__lt = ne_lat , lat__gt = sw_lat , lon__lt = ne_lon , lon__gt = sw_lon \n 
if len ( featureset ) < 1 : \n 
~~~ return HttpResponse ( "{\\"objects\\":[]}" , mimetype = "application/json" ) \n 
~~ resp = encode_queryset ( featureset ) \n 
~~ def encode_queryset ( featureset ) : \n 
~~~ resp = \'{"objects":[\' \n 
for obj in featureset : \n 
~~~ resp = resp + json . dumps ( obj . to_dict ( ) ) + \n 
~~ resp = resp [ : - 1 ] + \n 
~~ from django . template import loader , RequestContext \n 
from django . http import HttpResponse , Http404 \n 
from django . http import HttpResponseRedirect , HttpResponsePermanentRedirect \n 
from django . db . models . base import ModelBase \n 
from django . db . models . manager import Manager \n 
from django . db . models . query import QuerySet \n 
from django . core import urlresolvers \n 
from django . utils import six \n 
def render_to_easy_api_response ( * args , ** kwargs ) : \n 
httpresponse_kwargs = { : kwargs . pop ( , None ) } \n 
context = kwargs . pop ( ) \n 
processors = context . context_processors \n 
request = processors [ ] [ ] \n 
if request . GET . has_key ( ) : \n 
~~~ api_type = request . GET [ ] \n 
for arg in args : \n 
~~~ passed = arg \n 
~~ dump_me = { } \n 
for key in passed . keys ( ) : \n 
~~~ value = passed [ key ] \n 
dump_me [ key ] = dump_object ( value ) \n 
~~ if api_type == : \n 
~~~ def replace_spaces ( dump_me ) : \n 
~~~ new = { } \n 
for k , v in dump_me . iteritems ( ) : \n 
~~~ if isinstance ( v , dict ) : \n 
~~~ v = replace_spaces ( v ) \n 
~~ new [ k . replace ( , ) ] = v \n 
~~ return new \n 
~~ new = replace_spaces ( dump_me ) \n 
dump_me = dict2xml ( new ) \n 
pretty = dom . toprettyxml ( ) \n 
return HttpResponse ( pretty , content_type = ) \n 
~~~ yml = yaml . safe_dump ( dump_me ) \n 
return HttpResponse ( yml , content_type = ) \n 
return HttpResponse ( dump_me , content_type = ) \n 
~~ ~~ return HttpResponse ( loader . render_to_string ( * args , ** kwargs ) , ** httpresponse_kwargs ) \n 
~~ def render_to_response ( * args , ** kwargs ) : \n 
return render_to_easy_api_response ( * args , ** kwargs ) \n 
## \n 
~~ def dump_object ( queryset ) : \n 
~~~ d = DataDumper ( ) \n 
ret = d . dump ( queryset ) \n 
~~~ modelName = queryset [ 0 ] . __class__ . __name__ \n 
modelNameData = [ ] \n 
fields = get_fields ( queryset [ 0 ] ) \n 
for obj in queryset : \n 
~~~ temp_dict = dict ( ) \n 
for field in fields : \n 
~~~ attribute = getattr ( obj , str ( field ) ) \n 
temp_dict [ field ] = attribute \n 
~~ ~~ modelNameData . append ( temp_dict ) \n 
~~ dthandler = lambda obj : obj . isoformat ( ) if isinstance ( obj , datetime . datetime ) or isinstance return json . loads ( json . dumps ( modelNameData , default = dthandler ) ) \n 
~~ ~~ def get_fields ( model ) : \n 
~~~ if hasattr ( model , "easy_api_fields" ) : \n 
~~~ fields = model . easy_api_fields ( ) \n 
~~~ fields = model . to_dict ( ) . keys ( ) \n 
~~~ fields = model . _meta . get_all_field_names ( ) \n 
~~ ~~ return fields \n 
~~ ~~ class SimpleEngagementCalculator ( object ) : \n 
~~~ def calculate_user_engagement_score ( self , user , start_date , end_date ) : \n 
~~ ~~ ROOT_URLCONF = None \n 
DATABASE_ENGINE = \n 
DATABASE_NAME = \n 
DATABASE_SUPPORTS_TRANSACTIONS = False \n 
INSTALLED_APPS = [ \n 
TEMPLATE_CONTEXT_PROCESSORS = ( \n 
"django.core.context_processors.auth" , \n 
"django.core.context_processors.debug" , \n 
"django.core.context_processors.i18n" , \n 
"django.core.context_processors.media" , \n 
"django.core.context_processors.request" ) \n 
if __name__ == "__main__" : \n 
~~~ os . environ . setdefault ( "DJANGO_SETTINGS_MODULE" , "test_settings" ) \n 
from django . core . management import execute_from_command_line \n 
is_testing = in sys . argv \n 
if is_testing : \n 
~~~ import coverage \n 
cov = coverage . coverage ( include = "django_zappa/*" , omit = [ ] ) \n 
cov . erase ( ) \n 
cov . start ( ) \n 
~~ execute_from_command_line ( sys . argv ) \n 
~~~ cov . stop ( ) \n 
cov . save ( ) \n 
cov . report ( ) \n 
class CountdownManager ( object ) : \n 
~~~ def __init__ ( self , root_tk_app ) : \n 
~~~ self . start_time = time . time ( ) \n 
self . minutes = 0 \n 
self . seconds = 0 \n 
self . time_change_callbacks = [ ] \n 
self . count_down_total = datetime . timedelta ( days = - 1 , minutes = 0 , seconds = 0 ) \n 
self . root_tk_app = root_tk_app \n 
self . refresh_timer ( ) \n 
~~ def set_countdown_duration ( self , minutes , seconds ) : \n 
self . minutes = minutes \n 
self . seconds = seconds \n 
self . count_down_total = datetime . timedelta ( minutes = minutes , seconds = seconds ) \n 
self . fire_time_change_callbacks ( ) \n 
~~ def subscribe_to_time_changes ( self , time_change_callback ) : \n 
~~~ self . time_change_callbacks . append ( time_change_callback ) \n 
~~ def fire_time_change_callbacks ( self ) : \n 
~~~ end_time = time . time ( ) \n 
up_time = end_time - self . start_time \n 
remaining_time = self . count_down_total - datetime . timedelta ( seconds = ( int ( up_time ) ) ) \n 
for callback in self . time_change_callbacks : \n 
~~~ if callback : \n 
~~~ callback ( remaining_time . days , ( remaining_time . seconds // 60 ) % 60 , remaining_time . seconds \n 
~~ ~~ ~~ def refresh_timer ( self ) : \n 
~~~ self . fire_time_change_callbacks ( ) \n 
if self . root_tk_app : \n 
import simplexml , time , sys \n 
from protocol import * \n 
from client import PlugIn \n 
DefaultTimeout = 25 \n 
ID = 0 \n 
class Dispatcher ( PlugIn ) : \n 
~~~ PlugIn . __init__ ( self ) \n 
DBG_LINE = \n 
self . handlers = { } \n 
self . _expected = { } \n 
self . _defaultHandler = None \n 
self . _pendingExceptions = [ ] \n 
self . _eventHandler = None \n 
self . _cycleHandlers = [ ] \n 
self . _exported_methods = [ self . Process , self . RegisterHandler , self . RegisterDefaultHandler , self . RegisterEventHandler , self . UnregisterCycleHandler , self . RegisterCycleHandler , self . RegisterHandlerOnce , self . UnregisterHandler , self . RegisterProtocol , self . WaitForResponse , self . SendAndWaitForResponse , self . send , self . disconnect , self . SendAndCallForResponse , ] \n 
~~ def dumpHandlers ( self ) : \n 
return self . handlers \n 
~~ def restoreHandlers ( self , handlers ) : \n 
self . handlers = handlers \n 
~~ def _init ( self ) : \n 
self . RegisterNamespace ( ) \n 
self . RegisterNamespace ( NS_STREAMS ) \n 
self . RegisterNamespace ( self . _owner . defaultNamespace ) \n 
self . RegisterProtocol ( , Iq ) \n 
self . RegisterProtocol ( , Presence ) \n 
self . RegisterProtocol ( , Message ) \n 
self . RegisterDefaultHandler ( self . returnStanzaHandler ) \n 
self . RegisterHandler ( , self . streamErrorHandler , xmlns = NS_STREAMS ) \n 
~~ def plugin ( self , owner ) : \n 
for method in self . _old_owners_methods : \n 
~~~ if method . __name__ == : self . _owner_send = method ; break \n 
~~ self . _owner . lastErrNode = None \n 
self . _owner . lastErr = None \n 
self . _owner . lastErrCode = None \n 
self . StreamInit ( ) \n 
~~ def plugout ( self ) : \n 
self . Stream . dispatch = None \n 
self . Stream . DEBUG = None \n 
self . Stream . features = None \n 
self . Stream . destroy ( ) \n 
~~ def StreamInit ( self ) : \n 
self . Stream = simplexml . NodeBuilder ( ) \n 
self . Stream . _dispatch_depth = 2 \n 
self . Stream . dispatch = self . dispatch \n 
self . Stream . stream_header_received = self . _check_stream_start \n 
self . _owner . debug_flags . append ( simplexml . DBG_NODEBUILDER ) \n 
self . Stream . DEBUG = self . _owner . DEBUG \n 
self . _metastream = Node ( ) \n 
self . _metastream . setNamespace ( self . _owner . Namespace ) \n 
self . _metastream . setAttr ( , ) \n 
self . _metastream . setAttr ( , NS_STREAMS ) \n 
self . _metastream . setAttr ( , self . _owner . Server ) \n 
~~ def _check_stream_start ( self , ns , tag , attrs ) : \n 
~~~ if ns < > NS_STREAMS or tag < > : \n 
~~~ raise ValueError ( % ( tag , ns ) ) \n 
~~ ~~ def Process ( self , timeout = 0 ) : \n 
for handler in self . _cycleHandlers : handler ( self ) \n 
if len ( self . _pendingExceptions ) > 0 : \n 
~~~ _pendingException = self . _pendingExceptions . pop ( ) \n 
raise _pendingException [ 0 ] , _pendingException [ 1 ] , _pendingException [ 2 ] \n 
~~ if self . _owner . Connection . pending_data ( timeout ) : \n 
~~~ try : data = self . _owner . Connection . receive ( ) \n 
except IOError : return \n 
self . Stream . Parse ( data ) \n 
~~ if data : return len ( data ) \n 
~~ def RegisterNamespace ( self , xmlns , order = ) : \n 
self . handlers [ xmlns ] = { } \n 
self . RegisterProtocol ( , Protocol , xmlns = xmlns ) \n 
~~ def RegisterProtocol ( self , tag_name , Proto , xmlns = None , order = ) : \n 
if not xmlns : xmlns = self . _owner . defaultNamespace \n 
self . handlers [ xmlns ] [ tag_name ] = { type : Proto , : [ ] } \n 
~~ def RegisterNamespaceHandler ( self , xmlns , handler , typ = , ns = , makefirst = 0 , system = 0 ) : \n 
self . RegisterHandler ( , handler , typ , ns , xmlns , makefirst , system ) \n 
~~ def RegisterHandler ( self , name , handler , typ = , ns = , xmlns = None , makefirst = 0 , system = 0 ) : \n 
if not self . handlers . has_key ( xmlns ) : self . RegisterNamespace ( xmlns , ) \n 
if not self . handlers [ xmlns ] . has_key ( name ) : self . RegisterProtocol ( name , Protocol , xmlns , ) if not self . handlers [ xmlns ] [ name ] . has_key ( typ + ns ) : self . handlers [ xmlns ] [ name ] [ typ + ns ] = [ ] \n 
if makefirst : self . handlers [ xmlns ] [ name ] [ typ + ns ] . insert ( 0 , { : handler , : system } ) \n 
else : self . handlers [ xmlns ] [ name ] [ typ + ns ] . append ( { : handler , : system } ) \n 
~~ def RegisterHandlerOnce ( self , name , handler , typ = , ns = , xmlns = None , makefirst = 0 , system = 0 ) : \n 
self . RegisterHandler ( name , handler , typ , ns , xmlns , makefirst , system ) \n 
~~ def UnregisterHandler ( self , name , handler , typ = , ns = , xmlns = None ) : \n 
if not self . handlers . has_key ( xmlns ) : return \n 
if not typ and not ns : typ = \n 
for pack in self . handlers [ xmlns ] [ name ] [ typ + ns ] : \n 
~~~ if handler == pack [ ] : break \n 
~~ else : pack = None \n 
try : self . handlers [ xmlns ] [ name ] [ typ + ns ] . remove ( pack ) \n 
except ValueError : pass \n 
~~ def RegisterDefaultHandler ( self , handler ) : \n 
self . _defaultHandler = handler \n 
~~ def RegisterEventHandler ( self , handler ) : \n 
self . _eventHandler = handler \n 
~~ def returnStanzaHandler ( self , conn , stanza ) : \n 
if stanza . getType ( ) in [ , ] : \n 
~~~ conn . send ( Error ( stanza , ERR_FEATURE_NOT_IMPLEMENTED ) ) \n 
~~ ~~ def streamErrorHandler ( self , conn , error ) : \n 
~~~ name , text = , error . getData ( ) \n 
for tag in error . getChildren ( ) : \n 
~~~ if tag . getNamespace ( ) == NS_XMPP_STREAMS : \n 
~~~ if tag . getName ( ) == : text = tag . getData ( ) \n 
else : name = tag . getName ( ) \n 
~~ ~~ if name in stream_exceptions . keys ( ) : exc = stream_exceptions [ name ] \n 
else : exc = StreamError \n 
raise exc ( ( name , text ) ) \n 
~~ def RegisterCycleHandler ( self , handler ) : \n 
if handler not in self . _cycleHandlers : self . _cycleHandlers . append ( handler ) \n 
~~ def UnregisterCycleHandler ( self , handler ) : \n 
if handler in self . _cycleHandlers : self . _cycleHandlers . remove ( handler ) \n 
~~ def Event ( self , realm , event , data ) : \n 
if self . _eventHandler : self . _eventHandler ( realm , event , data ) \n 
~~ def dispatch ( self , stanza , session = None , direct = 0 ) : \n 
if not session : session = self \n 
session . Stream . _mini_dom = None \n 
name = stanza . getName ( ) \n 
if not direct and self . _owner . _route : \n 
~~~ if name == : \n 
~~~ if stanza . getAttr ( ) == None : \n 
~~~ if len ( stanza . getChildren ( ) ) == 1 : \n 
~~~ stanza = stanza . getChildren ( ) [ 0 ] \n 
~~~ for each in stanza . getChildren ( ) : \n 
~~~ self . dispatch ( each , session , direct = 1 ) \n 
~~ ~~ ~~ elif name == : \n 
~~ elif name in ( , ) : \n 
~~~ raise UnsupportedStanzaType ( name ) \n 
~~ ~~ if name == : session . Stream . features = stanza \n 
xmlns = stanza . getNamespace ( ) \n 
if not self . handlers . has_key ( xmlns ) : \n 
xmlns = \n 
~~ if not self . handlers [ xmlns ] . has_key ( name ) : \n 
name = \n 
~~ if stanza . __class__ . __name__ == : stanza = self . handlers [ xmlns ] [ name ] [ type ] ( node = stanza ) \n 
typ = stanza . getType ( ) \n 
if not typ : typ = \n 
stanza . props = stanza . getProperties ( ) \n 
ID = stanza . getID ( ) \n 
~~~ if self . handlers [ xmlns ] [ name ] . has_key ( prop ) : list . append ( prop ) \n 
~~ chain = self . handlers [ xmlns ] [ ] [ ] \n 
for key in list : \n 
~~~ if key : chain = chain + self . handlers [ xmlns ] [ name ] [ key ] \n 
~~ output = \n 
if session . _expected . has_key ( ID ) : \n 
~~~ user = 0 \n 
if type ( session . _expected [ ID ] ) == type ( ( ) ) : \n 
~~~ cb , args = session . _expected [ ID ] \n 
try : cb ( session , stanza , ** args ) \n 
except Exception , typ : \n 
~~~ if typ . __class__ . __name__ < > : raise \n 
session . _expected [ ID ] = stanza \n 
~~ ~~ else : user = 1 \n 
for handler in chain : \n 
~~~ if user or handler [ ] : \n 
~~~ handler [ ] ( session , stanza ) \n 
~~ except Exception , typ : \n 
~~~ if typ . __class__ . __name__ < > : \n 
~~~ self . _pendingExceptions . insert ( 0 , sys . exc_info ( ) ) \n 
~~ user = 0 \n 
~~ ~~ ~~ if user and self . _defaultHandler : self . _defaultHandler ( session , stanza ) \n 
~~ def WaitForResponse ( self , ID , timeout = DefaultTimeout ) : \n 
self . _expected [ ID ] = None \n 
has_timed_out = 0 \n 
abort_time = time . time ( ) + timeout \n 
while not self . _expected [ ID ] : \n 
~~~ if not self . Process ( 0.04 ) : \n 
~~~ self . _owner . lastErr = "Disconnect" \n 
~~ if time . time ( ) > abort_time : \n 
~~~ self . _owner . lastErr = "Timeout" \n 
~~ ~~ response = self . _expected [ ID ] \n 
del self . _expected [ ID ] \n 
if response . getErrorCode ( ) : \n 
~~~ self . _owner . lastErrNode = response \n 
self . _owner . lastErr = response . getError ( ) \n 
self . _owner . lastErrCode = response . getErrorCode ( ) \n 
~~ def SendAndWaitForResponse ( self , stanza , timeout = DefaultTimeout ) : \n 
return self . WaitForResponse ( self . send ( stanza ) , timeout ) \n 
~~ def SendAndCallForResponse ( self , stanza , func , args = { } ) : \n 
self . _expected [ self . send ( stanza ) ] = ( func , args ) \n 
~~ def send ( self , stanza ) : \n 
if type ( stanza ) in [ type ( ) , type ( ) ] : return self . _owner_send ( stanza ) \n 
if not isinstance ( stanza , Protocol ) : _ID = None \n 
elif not stanza . getID ( ) : \n 
~~~ global ID \n 
ID += 1 \n 
_ID = ` ID ` \n 
stanza . setID ( _ID ) \n 
~~ else : _ID = stanza . getID ( ) \n 
if self . _owner . _registered_name and not stanza . getAttr ( ) : stanza . setAttr ( , self . _owner if self . _owner . _route and stanza . getName ( ) != : \n 
~~~ to = self . _owner . Server \n 
if stanza . getTo ( ) and stanza . getTo ( ) . getDomain ( ) : \n 
~~~ to = stanza . getTo ( ) . getDomain ( ) \n 
~~ frm = stanza . getFrom ( ) \n 
if frm . getDomain ( ) : \n 
~~~ frm = frm . getDomain ( ) \n 
~~ route = Protocol ( , to = to , frm = frm , payload = [ stanza ] ) \n 
stanza = route \n 
~~ stanza . setNamespace ( self . _owner . Namespace ) \n 
stanza . setParent ( self . _metastream ) \n 
self . _owner_send ( stanza ) \n 
return _ID \n 
~~ def disconnect ( self ) : \n 
self . _owner_send ( ) \n 
while self . Process ( 1 ) : pass \n 
~~ ~~ from . gl_utils import * \n 
from . texture import VideoTexture \n 
from . widget import Widget , BGUI_DEFAULT , WeakMethod \n 
from . image import Image \n 
class Video ( Image ) : \n 
def __init__ ( self , parent , vid , name = None , play_audio = False , repeat = 0 , aspect = None , size = [ 1 , 1 ] , pos sub_theme = , options = BGUI_DEFAULT ) : \n 
Image . __init__ ( self , parent , name , None , aspect , size , pos , sub_theme = sub_theme , options = options ) \n 
self . _texture = VideoTexture ( vid , GL_LINEAR , repeat , play_audio ) \n 
self . _on_finish = None \n 
self . _on_finish_called = False \n 
~~ def play ( self , start , end , use_frames = True , fps = None ) : \n 
~~~ self . _texture . play ( start , end , use_frames , fps ) \n 
def on_finish ( self ) : \n 
return self . _on_finish \n 
~~ @ on_finish . setter \n 
def on_finish ( self , value ) : \n 
~~~ self . _on_finish = WeakMethod ( value ) \n 
~~ def _draw ( self ) : \n 
self . _texture . update ( ) \n 
Image . _draw ( self ) \n 
if self . _texture . video . status == 3 : \n 
~~~ if self . _on_finish and not self . _on_finish_called : \n 
~~~ self . on_finish ( self ) \n 
self . _on_finish_called = Truefrom django import template \n 
~~ ~~ ~~ ~~ from django . conf import settings \n 
class CheckGrappelli ( template . Node ) : \n 
~~~ def __init__ ( self , var_name ) : \n 
~~~ self . var_name = var_name \n 
~~~ context [ self . var_name ] = in settings . INSTALLED_APPS \n 
~~ ~~ def check_grappelli ( parser , token ) : \n 
bits = token . contents . split ( ) \n 
if len ( bits ) != 3 : \n 
~~ if bits [ 1 ] != : \n 
~~ varname = bits [ 2 ] \n 
return CheckGrappelli ( varname ) \n 
~~ register . tag ( check_grappelli ) \n 
__description__ = , \n 
__author__ = , \n 
__email__ = , \n 
sys . path . insert ( 0 , os . path . dirname ( __file__ ) ) \n 
REQUIRES = [ i . strip ( ) for i in open ( "requirements.txt" ) . readlines ( ) ] \n 
download_url = , \n 
license = __license__ , \n 
author = __author__ , \n 
author_email = __email__ , \n 
description = __description__ , \n 
install_requires = REQUIRES , \n 
packages = find_packages ( exclude = ( , , ) ) , \n 
setup_requires = [ , ] , \n 
from mongoengine . base import BaseField \n 
__all__ = ( ) \n 
class WtfBaseField ( BaseField ) : \n 
def __init__ ( self , validators = None , filters = None , ** kwargs ) : \n 
~~~ self . validators = self . _ensure_callable_or_list ( validators , ) \n 
self . filters = self . _ensure_callable_or_list ( filters , ) \n 
BaseField . __init__ ( self , ** kwargs ) \n 
~~ def _ensure_callable_or_list ( self , field , msg_flag ) : \n 
if field is not None : \n 
~~~ if callable ( field ) : \n 
~~~ field = [ field ] \n 
if not isinstance ( field , list ) : \n 
~~~ raise TypeError ( msg ) \n 
~~ ~~ ~~ return field \n 
~~ ~~ from bson import DBRef , SON \n 
from mongoengine . python_support import txt_type \n 
from base import ( \n 
BaseDict , BaseList , EmbeddedDocumentList , \n 
TopLevelDocumentMetaclass , get_document \n 
from fields import ( ReferenceField , ListField , DictField , MapField ) \n 
from connection import get_db \n 
from queryset import QuerySet \n 
from document import Document , EmbeddedDocument \n 
class DeReference ( object ) : \n 
~~~ def __call__ ( self , items , max_depth = 1 , instance = None , name = None ) : \n 
if items is None or isinstance ( items , basestring ) : \n 
~~~ return items \n 
~~ if isinstance ( items , QuerySet ) : \n 
~~~ items = [ i for i in items ] \n 
~~ self . max_depth = max_depth \n 
doc_type = None \n 
if instance and isinstance ( instance , ( Document , EmbeddedDocument , \n 
TopLevelDocumentMetaclass ) ) : \n 
~~~ doc_type = instance . _fields . get ( name ) \n 
while hasattr ( doc_type , ) : \n 
~~~ doc_type = doc_type . field \n 
~~ if isinstance ( doc_type , ReferenceField ) : \n 
~~~ field = doc_type \n 
doc_type = doc_type . document_type \n 
is_list = not hasattr ( items , ) \n 
if is_list and all ( [ i . __class__ == doc_type for i in items ] ) : \n 
~~ elif not is_list and all ( \n 
[ i . __class__ == doc_type for i in items . values ( ) ] ) : \n 
~~ elif not field . dbref : \n 
~~~ if not hasattr ( items , ) : \n 
~~~ def _get_items ( items ) : \n 
~~~ new_items = [ ] \n 
for v in items : \n 
~~~ if isinstance ( v , list ) : \n 
~~~ new_items . append ( _get_items ( v ) ) \n 
~~ elif not isinstance ( v , ( DBRef , Document ) ) : \n 
~~~ new_items . append ( field . to_python ( v ) ) \n 
~~~ new_items . append ( v ) \n 
~~ ~~ return new_items \n 
~~ items = _get_items ( items ) \n 
~~~ items = dict ( [ \n 
( k , field . to_python ( v ) ) \n 
if not isinstance ( v , ( DBRef , Document ) ) else ( k , v ) \n 
for k , v in items . iteritems ( ) ] \n 
~~ ~~ ~~ ~~ self . reference_map = self . _find_references ( items ) \n 
self . object_map = self . _fetch_objects ( doc_type = doc_type ) \n 
return self . _attach_objects ( items , 0 , instance , name ) \n 
~~ def _find_references ( self , items , depth = 0 ) : \n 
reference_map = { } \n 
if not items or depth >= self . max_depth : \n 
~~~ return reference_map \n 
~~ if not hasattr ( items , ) : \n 
~~~ iterator = enumerate ( items ) \n 
~~~ iterator = items . iteritems ( ) \n 
~~ depth += 1 \n 
for k , item in iterator : \n 
~~~ if isinstance ( item , ( Document , EmbeddedDocument ) ) : \n 
~~~ for field_name , field in item . _fields . iteritems ( ) : \n 
~~~ v = item . _data . get ( field_name , None ) \n 
if isinstance ( v , DBRef ) : \n 
~~~ reference_map . setdefault ( field . document_type , set ( ) ) . add ( v . id ) \n 
~~ elif isinstance ( v , ( dict , SON ) ) and in v : \n 
~~~ reference_map . setdefault ( get_document ( v [ ] ) , set ( ) ) . add ( v [ ] . id ) \n 
~~ elif isinstance ( v , ( dict , list , tuple ) ) and depth <= self . max_depth : \n 
~~~ field_cls = getattr ( getattr ( field , , None ) , , None ) \n 
references = self . _find_references ( v , depth ) \n 
for key , refs in references . iteritems ( ) : \n 
~~~ if isinstance ( field_cls , ( Document , TopLevelDocumentMetaclass ) ) : \n 
~~~ key = field_cls \n 
~~ reference_map . setdefault ( key , set ( ) ) . update ( refs ) \n 
~~ ~~ ~~ ~~ elif isinstance ( item , DBRef ) : \n 
~~~ reference_map . setdefault ( item . collection , set ( ) ) . add ( item . id ) \n 
~~ elif isinstance ( item , ( dict , SON ) ) and in item : \n 
~~~ reference_map . setdefault ( get_document ( item [ ] ) , set ( ) ) . add ( item [ ] . id ) \n 
~~ elif isinstance ( item , ( dict , list , tuple ) ) and depth - 1 <= self . max_depth : \n 
~~~ references = self . _find_references ( item , depth - 1 ) \n 
~~~ reference_map . setdefault ( key , set ( ) ) . update ( refs ) \n 
~~ ~~ ~~ return reference_map \n 
~~ def _fetch_objects ( self , doc_type = None ) : \n 
object_map = { } \n 
for collection , dbrefs in self . reference_map . iteritems ( ) : \n 
~~~ col_name = collection . _get_collection_name ( ) \n 
refs = [ dbref for dbref in dbrefs \n 
if ( col_name , dbref ) not in object_map ] \n 
references = collection . objects . in_bulk ( refs ) \n 
for key , doc in references . iteritems ( ) : \n 
~~~ object_map [ ( col_name , key ) ] = doc \n 
~~~ if isinstance ( doc_type , ( ListField , DictField , MapField , ) ) : \n 
~~ refs = [ dbref for dbref in dbrefs \n 
if ( collection , dbref ) not in object_map ] \n 
if doc_type : \n 
~~~ references = doc_type . _get_db ( ) [ collection ] . find ( { : { : refs } } ) \n 
for ref in references : \n 
~~~ doc = doc_type . _from_son ( ref ) \n 
object_map [ ( collection , doc . id ) ] = doc \n 
~~~ references = get_db ( ) [ collection ] . find ( { : { : refs } } ) \n 
~~~ if in ref : \n 
~~~ doc = get_document ( ref [ "_cls" ] ) . _from_son ( ref ) \n 
~~ elif doc_type is None : \n 
~~~ doc = get_document ( \n 
. join ( x . capitalize ( ) \n 
for x in collection . split ( ) ) ) . _from_son ( ref ) \n 
~~ object_map [ ( collection , doc . id ) ] = doc \n 
~~ ~~ ~~ ~~ return object_map \n 
~~ def _attach_objects ( self , items , depth = 0 , instance = None , name = None ) : \n 
if not items : \n 
~~~ if isinstance ( items , ( BaseDict , BaseList ) ) : \n 
~~ if instance : \n 
~~~ if isinstance ( items , dict ) : \n 
~~~ return BaseDict ( items , instance , name ) \n 
~~~ return BaseList ( items , instance , name ) \n 
~~ ~~ ~~ if isinstance ( items , ( dict , SON ) ) : \n 
~~~ if in items : \n 
~~~ return self . object_map . get ( \n 
( items [ ] . collection , items [ ] . id ) , items ) \n 
~~ elif in items : \n 
~~~ doc = get_document ( items [ ] ) . _from_son ( items ) \n 
_cls = doc . _data . pop ( , None ) \n 
del items [ ] \n 
doc . _data = self . _attach_objects ( doc . _data , depth , doc , None ) \n 
if _cls is not None : \n 
~~~ doc . _data [ ] = _cls \n 
~~ return doc \n 
~~ ~~ if not hasattr ( items , ) : \n 
~~~ is_list = True \n 
list_type = BaseList \n 
if isinstance ( items , EmbeddedDocumentList ) : \n 
~~~ list_type = EmbeddedDocumentList \n 
~~ as_tuple = isinstance ( items , tuple ) \n 
iterator = enumerate ( items ) \n 
data = [ ] \n 
~~~ is_list = False \n 
iterator = items . iteritems ( ) \n 
data = { } \n 
for k , v in iterator : \n 
~~~ if is_list : \n 
~~~ data . append ( v ) \n 
~~~ data [ k ] = v \n 
~~ if k in self . object_map and not is_list : \n 
~~~ data [ k ] = self . object_map [ k ] \n 
~~ elif isinstance ( v , ( Document , EmbeddedDocument ) ) : \n 
~~~ for field_name , field in v . _fields . iteritems ( ) : \n 
~~~ v = data [ k ] . _data . get ( field_name , None ) \n 
~~~ data [ k ] . _data [ field_name ] = self . object_map . get ( \n 
( v . collection , v . id ) , v ) \n 
( v [ ] . collection , v [ ] . id ) , v ) \n 
~~~ item_name = txt_type ( "{0}.{1}.{2}" ) . format ( name , k , field_name ) \n 
data [ k ] . _data [ field_name ] = self . _attach_objects ( v , depth , instance = instance ~~ ~~ ~~ elif isinstance ( v , ( dict , list , tuple ) ) and depth <= self . max_depth : \n 
~~~ item_name = % ( name , k ) if name else name \n 
data [ k ] = self . _attach_objects ( v , depth - 1 , instance = instance , name = item_name ) \n 
~~ elif hasattr ( v , ) : \n 
~~~ data [ k ] = self . object_map . get ( ( v . collection , v . id ) , v ) \n 
~~ ~~ if instance and name : \n 
~~~ return tuple ( data ) if as_tuple else list_type ( data , instance , name ) \n 
~~ return BaseDict ( data , instance , name ) \n 
return data \n 
sys . path [ 0 : 0 ] = [ "" ] \n 
from mongoengine import * \n 
from mongoengine . connection import get_db \n 
__all__ = ( "GeoFieldTest" , ) \n 
class GeoFieldTest ( unittest . TestCase ) : \n 
~~~ connect ( db = ) \n 
self . db = get_db ( ) \n 
~~ def _test_for_expected_error ( self , Cls , loc , expected ) : \n 
~~~ Cls ( loc = loc ) . validate ( ) \n 
self . fail ( . format ( loc ) ) \n 
~~ except ValidationError as e : \n 
~~~ self . assertEqual ( expected , e . to_dict ( ) [ ] ) \n 
~~ ~~ def test_geopoint_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = GeoPointField ( ) \n 
~~ invalid_coords = [ { "x" : 1 , "y" : 2 } , 5 , "a" ] \n 
for coord in invalid_coords : \n 
~~~ self . _test_for_expected_error ( Location , coord , expected ) \n 
~~ invalid_coords = [ [ ] , [ 1 ] , [ 1 , 2 , 3 ] ] \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
~~ invalid_coords = [ [ { } , { } ] , ( "a" , "b" ) ] \n 
~~ ~~ def test_point_validation ( self ) : \n 
~~~ loc = PointField ( ) \n 
~~ invalid_coords = { "x" : 1 , "y" : 2 } \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ ] } \n 
invalid_coords = { "type" : "Point" , "coordinates" : [ 1 , 2 , 3 ] } \n 
invalid_coords = [ 5 , "a" ] \n 
~~ Location ( loc = [ 1 , 2 ] ) . validate ( ) \n 
Location ( loc = { \n 
"type" : "Point" , \n 
"coordinates" : [ \n 
81.4471435546875 , \n 
23.61432859499169 \n 
] } ) . validate ( ) \n 
~~ def test_linestring_validation ( self ) : \n 
~~~ loc = LineStringField ( ) \n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ [ ] ] } \n 
invalid_coords = { "type" : "LineString" , "coordinates" : [ [ 1 , 2 , 3 ] ] } \n 
invalid_coords = [ [ 1 ] ] \n 
invalid_coords = [ [ 1 , 2 , 3 ] ] \n 
invalid_coords = [ [ [ { } , { } ] ] , [ ( "a" , "b" ) ] ] \n 
~~ Location ( loc = [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ) . validate ( ) \n 
~~ def test_polygon_validation ( self ) : \n 
~~~ loc = PolygonField ( ) \n 
invalid_coords = { "type" : "Polygon" , "coordinates" : [ [ [ 1 , 2 , 3 ] ] ] } \n 
invalid_coords = [ [ [ 5 , "a" ] ] ] \n 
invalid_coords = [ [ [ ] ] ] \n 
invalid_coords = [ [ [ 1 , 2 , 3 ] ] ] \n 
invalid_coords = [ [ [ 1 , 2 ] , [ 3 , 4 ] ] ] \n 
Location ( loc = [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ) . validate ( ) \n 
~~ def test_multipoint_validation ( self ) : \n 
~~~ loc = MultiPointField ( ) \n 
invalid_coords = { "type" : "MultiPoint" , "coordinates" : [ [ 1 , 2 , 3 ] ] } \n 
invalid_coords = [ [ ] ] \n 
invalid_coords = [ [ [ 1 ] ] , [ [ 1 , 2 , 3 ] ] ] \n 
~~ invalid_coords = [ [ [ { } , { } ] ] , [ ( "a" , "b" ) ] ] \n 
~~ Location ( loc = [ [ 1 , 2 ] ] ) . validate ( ) \n 
"type" : "MultiPoint" , \n 
[ 1 , 2 ] , \n 
[ 81.4471435546875 , 23.61432859499169 ] \n 
~~ def test_multilinestring_validation ( self ) : \n 
~~~ loc = MultiLineStringField ( ) \n 
expected = self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
invalid_coords = { "type" : "MultiLineString" , "coordinates" : [ [ [ 1 , 2 , 3 ] ] ] } \n 
invalid_coords = [ [ [ 1 ] ] ] \n 
invalid_coords = [ [ [ [ { } , { } ] ] ] , [ [ ( "a" , "b" ) ] ] ] \n 
~~ Location ( loc = [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ) . validate ( ) \n 
~~ def test_multipolygon_validation ( self ) : \n 
~~~ loc = MultiPolygonField ( ) \n 
invalid_coords = { "type" : "MultiPolygon" , "coordinates" : [ [ [ [ 1 , 2 , 3 ] ] ] ] } \n 
invalid_coords = [ [ [ [ 5 , "a" ] ] ] ] \n 
invalid_coords = [ [ [ [ ] ] ] ] \n 
invalid_coords = [ [ [ [ 1 , 2 , 3 ] ] ] ] \n 
invalid_coords = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] ] ] ] \n 
Location ( loc = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ] ) . validate ( ) \n 
~~ def test_indexes_geopoint ( self ) : \n 
class Event ( Document ) : \n 
~~~ title = StringField ( ) \n 
location = GeoPointField ( ) \n 
~~ geo_indicies = Event . _geo_indices ( ) \n 
self . assertEqual ( geo_indicies , [ { : [ ( , ) ] } ] ) \n 
~~ def test_geopoint_embedded_indexes ( self ) : \n 
class Venue ( EmbeddedDocument ) : \n 
~~~ location = GeoPointField ( ) \n 
name = StringField ( ) \n 
~~ class Event ( Document ) : \n 
venue = EmbeddedDocumentField ( Venue ) \n 
~~ def test_indexes_2dsphere ( self ) : \n 
point = PointField ( ) \n 
line = LineStringField ( ) \n 
polygon = PolygonField ( ) \n 
self . assertTrue ( { : [ ( , ) ] } in geo_indicies ) \n 
~~ def test_indexes_2dsphere_embedded ( self ) : \n 
~~~ name = StringField ( ) \n 
~~ def test_geo_indexes_recursion ( self ) : \n 
~~ class Parent ( Document ) : \n 
location = ReferenceField ( Location ) \n 
~~ Location . drop_collection ( ) \n 
Parent . drop_collection ( ) \n 
Parent ( name = ) . save ( ) \n 
info = Parent . _get_collection ( ) . index_information ( ) \n 
self . assertFalse ( in info ) \n 
info = Location . _get_collection ( ) . index_information ( ) \n 
self . assertTrue ( in info ) \n 
self . assertEqual ( len ( Parent . _geo_indices ( ) ) , 0 ) \n 
self . assertEqual ( len ( Location . _geo_indices ( ) ) , 1 ) \n 
~~ def test_geo_indexes_auto_index ( self ) : \n 
~~~ class Log ( Document ) : \n 
~~~ location = PointField ( auto_index = False ) \n 
datetime = DateTimeField ( ) \n 
meta = { \n 
: [ [ ( "location" , "2dsphere" ) , ( "datetime" , 1 ) ] ] \n 
~~ self . assertEqual ( [ ] , Log . _geo_indices ( ) ) \n 
Log . drop_collection ( ) \n 
Log . ensure_indexes ( ) \n 
info = Log . _get_collection ( ) . index_information ( ) \n 
self . assertEqual ( info [ "location_2dsphere_datetime_1" ] [ "key" ] , \n 
[ ( , ) , ( , 1 ) ] ) \n 
class Log ( Document ) : \n 
{ : [ ( "location" , "2dsphere" ) , ( "datetime" , 1 ) ] } \n 
~~ from south . db import db \n 
from django . db import models \n 
from django_lean . experiments . models import * \n 
class Migration : \n 
~~~ def forwards ( self , orm ) : \n 
~~~ db . create_table ( , ( \n 
( , orm [ ] ) , \n 
db . send_create_signal ( , [ ] ) \n 
db . create_table ( , ( \n 
db . create_unique ( , [ , ] ) \n 
~~ def backwards ( self , orm ) : \n 
~~~ db . delete_table ( ) \n 
db . delete_table ( ) \n 
db . delete_unique ( , [ , ] ) \n 
~~ models = { \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } : ( , [ ] , { : "orm[\'auth.Permission\']" } , \n 
: ( , [ ] , { : "orm[\'contenttypes.ContentType\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : "orm[\'auth.Group\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'auth.Permission\']" : ( , [ ] , { : , : } , \n 
: ( , [ ] , { } ) , \n 
: ( , [ ] , { : "orm[\'experiments.Experiment\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { } ) \n 
: ( , [ ] , { : , : } ) : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : } ) \n 
: ( , [ ] , { : , : ( , [ ] , { : "orm[\'experiments.Experiment\']" : ( , [ ] , { } ) , \n 
: ( , [ ] , { : "orm[\'auth.User\']" } ) \n 
complete_apps = [ ] \n 
~~ class SimpleEngagementCalculator ( object ) : \n 
from django . contrib . sites . models import Site \n 
from django . core import mail \n 
from django . db import transaction \n 
from django . utils . functional import LazyObject \n 
def get_current_site ( ) : \n 
~~~ if Site . _meta . installed : \n 
~~~ return Site . objects . get_current ( ) \n 
~~ def in_transaction ( test_ignore = True ) : \n 
~~~ result = transaction . is_managed ( ) \n 
if test_ignore : \n 
~~~ result = result and not hasattr ( mail , ) \n 
def patch ( namespace , name , function ) : \n 
if isinstance ( namespace , LazyObject ) : \n 
~~~ if namespace . _wrapped is None : \n 
~~~ namespace . _setup ( ) \n 
~~ namespace = namespace . _wrapped \n 
~~~ original = getattr ( namespace , name ) \n 
~~~ original = NotImplemented \n 
~~~ setattr ( namespace , name , function ) \n 
~~~ if original is NotImplemented : \n 
~~~ delattr ( namespace , name ) \n 
~~~ setattr ( namespace , name , original ) \n 
from construct import * \n 
from ipv4 import IpAddress \n 
echo_payload = Struct ( "echo_payload" , \n 
UBInt16 ( "identifier" ) , \n 
UBInt16 ( "sequence" ) , \n 
dest_unreachable_payload = Struct ( "dest_unreachable_payload" , \n 
Padding ( 2 ) , \n 
UBInt16 ( "next_hop_mtu" ) , \n 
IpAddress ( "host" ) , \n 
Bytes ( "echo" , 8 ) , \n 
dest_unreachable_code = Enum ( Byte ( "code" ) , \n 
Network_unreachable_error = 0 , \n 
Host_unreachable_error = 1 , \n 
Protocol_unreachable_error = 2 , \n 
Port_unreachable_error = 3 , \n 
The_datagram_is_too_big = 4 , \n 
Source_route_failed_error = 5 , \n 
Destination_network_unknown_error = 6 , \n 
Destination_host_unknown_error = 7 , \n 
Source_host_isolated_error = 8 , \n 
Desination_administratively_prohibited = 9 , \n 
Host_administratively_prohibited2 = 10 , \n 
Network_TOS_unreachable = 11 , \n 
Host_TOS_unreachable = 12 , \n 
icmp_header = Struct ( "icmp_header" , \n 
Enum ( Byte ( "type" ) , \n 
Echo_reply = 0 , \n 
Destination_unreachable = 3 , \n 
Source_quench = 4 , \n 
Redirect = 5 , \n 
Alternate_host_address = 6 , \n 
Echo_request = 8 , \n 
Router_advertisement = 9 , \n 
Router_solicitation = 10 , \n 
Time_exceeded = 11 , \n 
Parameter_problem = 12 , \n 
Timestamp_request = 13 , \n 
Timestamp_reply = 14 , \n 
Information_request = 15 , \n 
Information_reply = 16 , \n 
Address_mask_request = 17 , \n 
Address_mask_reply = 18 , \n 
_default_ = Pass , \n 
Switch ( "code" , lambda ctx : ctx . type , \n 
"Destination_unreachable" : dest_unreachable_code , \n 
default = Byte ( "code" ) , \n 
UBInt16 ( "crc" ) , \n 
Switch ( "payload" , lambda ctx : ctx . type , \n 
"Echo_reply" : echo_payload , \n 
"Echo_request" : echo_payload , \n 
"Destination_unreachable" : dest_unreachable_payload , \n 
default = Pass \n 
~~~ cap1 = ( "0800305c02001b006162636465666768696a6b6c6d6e6f70717273747576776162" \n 
"63646566676869" ) . decode ( "hex" ) \n 
cap2 = ( "0000385c02001b006162636465666768696a6b6c6d6e6f70717273747576776162" \n 
cap3 = ( "0301000000001122aabbccdd0102030405060708" ) . decode ( "hex" ) \n 
print icmp_header . parse ( cap1 ) \n 
print icmp_header . parse ( cap2 ) \n 
print icmp_header . parse ( cap3 ) \n 
~~ from construct . core import Container \n 
from construct . adapters import Adapter \n 
class AstNode ( Container ) : \n 
~~~ def __init__ ( self , nodetype , ** kw ) : \n 
~~~ Container . __init__ ( self ) \n 
self . nodetype = nodetype \n 
for k , v in sorted ( kw . iteritems ( ) ) : \n 
~~~ setattr ( self , k , v ) \n 
~~ ~~ def accept ( self , visitor ) : \n 
~~~ return getattr ( visitor , "visit_%s" % ( self . nodetype , ) ) ( self ) \n 
~~ ~~ class AstTransformator ( Adapter ) : \n 
~~~ def _decode ( self , obj , context ) : \n 
~~~ return self . to_ast ( obj , context ) \n 
~~ def _encode ( self , obj , context ) : \n 
~~~ return self . to_cst ( obj , context ) \n 
~~ ~~ def pytest_funcarg__setupopts ( request ) : \n 
~~~ return OptsSetup ( request ) \n 
~~ def pytest_addoption ( parser ) : \n 
~~~ parser . addoption ( "--uri-file" , dest = "urifile" , \n 
type = str , default = None , \n 
action = "store_true" , \n 
parser . addoption ( "--create-graph" , dest = "create_graph" , \n 
type = int , default = 0 , \n 
type = str , default = "2:00:00:00" , \n 
parser . addoption ( "--proc" , dest = "proc" , \n 
type = int , default = 8 , \n 
type = float , default = 16 , \n 
parser . addoption ( "--queue" , dest = "queue" , \n 
parser . addoption ( "--restart" , dest = "restart" , \n 
parser . addoption ( "--backup-dir" , dest = "backup_directory" , \n 
type = str , default = ".pipeline-backup" , \n 
~~ class OptsSetup ( ) : \n 
~~~ def __init__ ( self , request ) : \n 
~~~ self . config = request . config \n 
~~ def returnAllOptions ( self ) : \n 
~~~ return self . config . option \n 
~~ def getNumExecutors ( self ) : \n 
~~~ return self . config . option . num_exec \n 
~~ def getTime ( self ) : \n 
~~~ return self . config . option . time \n 
~~ def getProc ( self ) : \n 
~~~ return self . config . option . proc \n 
~~ def getMem ( self ) : \n 
~~~ return self . config . option . mem \n 
~~ def getQueue ( self ) : \n 
~~~ return self . config . option . queue \n 
~~ def getPpn ( self ) : \n 
~~~ return self . config . option . ppn \n 
~~ def getRestart ( self ) : \n 
~~~ return self . config . option . restart \n 
~~ def getBackupDir ( self ) : \n 
~~~ return self . config . option . backup_directory \n 
~~ def returnSampleArgs ( self ) : \n 
~~~ sampleArgArray = [ "TestProgName.py" , "img_A.mnc" , "img_B.mnc" ] \n 
return sampleArgArray \n 
from twisted . python import log \n 
from twisted . web . error import Error as WebError \n 
from opennsa import constants as cnt , config \n 
from opennsa . backends . common import genericbackend \n 
from opennsa . protocols . shared import httpclient \n 
#</service> \n 
LOG_SYSTEM = \n 
class NCSVPNTarget ( object ) : \n 
~~~ def __init__ ( self , router , interface , vlan = None ) : \n 
~~~ self . router = router \n 
self . interface = interface \n 
self . vlan = vlan \n 
~~~ if self . vlan : \n 
~~~ return % ( self . router , self . interface , self . vlan ) \n 
~~~ return % ( self . router , self . interface ) \n 
~~ ~~ ~~ def createVPNPayload ( service_name , source_target , dest_target ) : \n 
~~~ intps = { \n 
: service_name , \n 
: source_target . router , \n 
: source_target . interface , \n 
: dest_target . router , \n 
: dest_target . interface \n 
if source_target . vlan and dest_target . vlan : \n 
~~~ if source_target . vlan == dest_target . vlan : \n 
~~~ intps [ ] = source_target . vlan \n 
payload = ETHERNET_VLAN_VPN_PAYLOAD_BASE % intps \n 
intps [ ] = dest_target . vlan \n 
payload = ETHERNET_VLAN_REWRITE_VPN_PAYLOAD_BASE % intps \n 
~~~ payload = ETHERNET_VPN_PAYLOAD_BASE % intps \n 
~~ return payload \n 
~~ def _extractErrorMessage ( failure ) : \n 
~~~ if isinstance ( failure . value , WebError ) : \n 
~~~ return failure . value . response \n 
~~~ return failure . getErrorMessage ( ) \n 
~~ ~~ class NCSVPNConnectionManager : \n 
~~~ def __init__ ( self , ncs_services_url , user , password , port_map , log_system ) : \n 
~~~ self . ncs_services_url = ncs_services_url \n 
self . user = user \n 
self . password = password \n 
self . port_map = port_map \n 
self . log_system = log_system \n 
~~ def getResource ( self , port , label_type , label_value ) : \n 
~~~ assert label_type in ( None , cnt . ETHERNET_VLAN ) , \n 
~~ def getTarget ( self , port , label_type , label_value ) : \n 
if label_type == cnt . ETHERNET_VLAN : \n 
~~~ vlan = int ( label_value ) \n 
assert 1 <= vlan <= 4095 , % label_value \n 
~~ ri = self . port_map [ port ] \n 
router , interface = ri . split ( ) \n 
return NCSVPNTarget ( router , interface , vlan ) \n 
~~ def createConnectionId ( self , source_target , dest_target ) : \n 
~~~ return + str ( random . randint ( 100000 , 999999 ) ) \n 
~~ def canSwapLabel ( self , label_type ) : \n 
~~~ return label_type == cnt . ETHERNET_VLAN \n 
~~ def _createAuthzHeader ( self ) : \n 
~~~ return + base64 . b64encode ( self . user + + self . password ) \n 
~~ def _createHeaders ( self ) : \n 
~~~ headers = { } \n 
headers [ ] = \n 
headers [ ] = self . _createAuthzHeader ( ) \n 
return headers \n 
~~ def setupLink ( self , connection_id , source_target , dest_target , bandwidth ) : \n 
~~~ service_url = self . ncs_services_url + + NO_OUT_OF_SYNC_CHECK \n 
payload = createVPNPayload ( connection_id , source_target , dest_target ) \n 
headers = self . _createHeaders ( ) \n 
def linkUp ( _ ) : \n 
~~~ log . msg ( % ( source_target , dest_target ) , system = self . log_system ) \n 
~~ def error ( failure ) : \n 
~~~ log . msg ( % ( source_target , dest_target ) , system = self . log_system log . msg ( % _extractErrorMessage ( failure ) , system = self . log_system ) \n 
return failure \n 
~~ d = httpclient . httpRequest ( service_url , payload , headers , method = , timeout = NCS_TIMEOUT d . addCallbacks ( linkUp , error ) \n 
return d \n 
~~ def teardownLink ( self , connection_id , source_target , dest_target , bandwidth ) : \n 
~~~ service_url = self . ncs_services_url + + connection_id + + NO_OUT_OF_SYNC_CHECK \n 
def linkDown ( _ ) : \n 
~~~ log . msg ( % ( source_target , dest_target ) , system = self . log . msg ( % _extractErrorMessage ( failure ) , system = self . log_system ) \n 
~~ d = httpclient . httpRequest ( service_url , None , headers , method = , timeout = NCS_TIMEOUT ) d . addCallbacks ( linkDown , error ) \n 
~~ ~~ def NCSVPNBackend ( network_name , nrm_ports , parent_requester , cfg ) : \n 
~~~ name = % network_name \n 
user = cfg [ config . NCS_USER ] \n 
password = cfg [ config . NCS_PASSWORD ] \n 
cm = NCSVPNConnectionManager ( ncs_services_url , user , password , port_map , name ) \n 
return genericbackend . GenericBackend ( network_name , nrm_map , cm , parent_requester , name ) \n 
from twisted . python import log , failure \n 
from opennsa import nsa , error \n 
from opennsa . shared import xmlhelper \n 
from opennsa . protocols . shared import minisoap , soapresource \n 
from opennsa . protocols . nsi2 import helper , queryhelper \n 
from opennsa . protocols . nsi2 . bindings import actions , nsiconnection , p2pservices \n 
class ProviderService : \n 
~~~ def __init__ ( self , soap_resource , provider ) : \n 
~~~ self . provider = provider \n 
soap_resource . registerDecoder ( actions . RESERVE , self . reserve ) \n 
soap_resource . registerDecoder ( actions . RESERVE_COMMIT , self . reserveCommit ) \n 
soap_resource . registerDecoder ( actions . RESERVE_ABORT , self . reserveAbort ) \n 
soap_resource . registerDecoder ( actions . PROVISION , self . provision ) \n 
soap_resource . registerDecoder ( actions . RELEASE , self . release ) \n 
soap_resource . registerDecoder ( actions . TERMINATE , self . terminate ) \n 
soap_resource . registerDecoder ( actions . QUERY_SUMMARY , self . querySummary ) \n 
soap_resource . registerDecoder ( actions . QUERY_SUMMARY_SYNC , self . querySummarySync ) \n 
soap_resource . registerDecoder ( actions . QUERY_RECURSIVE , self . queryRecursive ) \n 
~~ def _createSOAPFault ( self , err , provider_nsa , connection_id = None , service_type = None ) : \n 
~~~ log . msg ( % err . getErrorMessage ( ) , system \n 
se = helper . createServiceException ( err , provider_nsa , connection_id ) \n 
ex_element = se . xml ( nsiconnection . serviceException ) \n 
soap_fault = soapresource . SOAPFault ( err . getErrorMessage ( ) , ex_element ) \n 
return soap_fault \n 
~~ def reserve ( self , soap_data , request_info ) : \n 
~~~ t_start = time . time ( ) \n 
header , reservation = helper . parseRequest ( soap_data ) \n 
criteria = reservation . criteria \n 
if type ( p2ps ) is not p2pservices . P2PServiceBaseType : \n 
~~~ err = failure . Failure ( error . PayloadError ( return self . _createSOAPFault ( err , header . provider_nsa , service_type = service_type ) \n 
~~ if p2ps . directionality in ( None , ) : \n 
~~~ err = failure . Failure ( error . MissingParameterError ( return self . _createSOAPFault ( err , header . provider_nsa ) \n 
~~ start_time = xmlhelper . parseXMLTimestamp ( criteria . schedule . startTime ) if criteria . schedule . startTime end_time = xmlhelper . parseXMLTimestamp ( criteria . schedule . endTime ) if criteria . schedule . endTime schedule = nsa . Schedule ( start_time , end_time ) \n 
src_stp = helper . createSTP ( p2ps . sourceSTP ) \n 
dst_stp = helper . createSTP ( p2ps . destSTP ) \n 
if p2ps . ero : \n 
~~~ err = failure . Failure ( error . PayloadError ( ) ) \n 
return self . _createSOAPFault ( err , header . provider_nsa ) \n 
~~ params = [ ( p . type_ , p . value ) for p in p2ps . parameter ] if p2ps . parameter else None \n 
symmetric = p2ps . symmetricPath or False sd = nsa . Point2PointService ( src_stp , dst_stp , p2ps . capacity , p2ps . directionality , symmetric , \n 
crt = nsa . Criteria ( criteria . version , schedule , sd ) \n 
t_delta = time . time ( ) - t_start \n 
log . msg ( % round ( t_delta , 3 ) , profile = True , system = \n 
d = self . provider . reserve ( header , reservation . connectionId , reservation . globalReservationId , \n 
def createReserveAcknowledgement ( connection_id ) : \n 
~~~ soap_header_element = helper . createProviderHeader ( header . requester_nsa , header . provider_nsa \n 
reserve_response = nsiconnection . ReserveResponseType ( connection_id ) \n 
reserve_response_element = reserve_response . xml ( nsiconnection . reserveResponse ) \n 
payload = minisoap . createSoapPayload ( reserve_response_element , soap_header_element ) \n 
return payload \n 
~~ d . addCallbacks ( createReserveAcknowledgement , self . _createSOAPFault , errbackArgs = ( header . provider_nsa return d \n 
~~ def reserveCommit ( self , soap_data , request_info ) : \n 
~~~ header , confirm = helper . parseRequest ( soap_data ) \n 
d = self . provider . reserveCommit ( header , confirm . connectionId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault return d \n 
~~ def reserveAbort ( self , soap_data , request_info ) : \n 
~~~ header , request = helper . parseRequest ( soap_data ) \n 
d = self . provider . reserveAbort ( header , request . connectionId , request_info ) \n 
~~ def provision ( self , soap_data , request_info ) : \n 
d = self . provider . provision ( header , request . connectionId , request_info ) \n 
~~ def release ( self , soap_data , request_info ) : \n 
d = self . provider . release ( header , request . connectionId , request_info ) \n 
~~ def terminate ( self , soap_data , request_info ) : \n 
d = self . provider . terminate ( header , request . connectionId , request_info ) \n 
~~ def querySummary ( self , soap_data , request_info ) : \n 
~~~ header , query = helper . parseRequest ( soap_data ) \n 
d = self . provider . querySummary ( header , query . connectionId , query . globalReservationId , request_info d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault return d \n 
~~ def querySummarySync ( self , soap_data , request_info ) : \n 
~~~ def gotReservations ( reservations , header ) : \n 
qs_reservations = queryhelper . buildQuerySummaryResultType ( reservations ) \n 
qsct = nsiconnection . QuerySummaryConfirmedType ( qs_reservations ) \n 
payload = minisoap . createSoapPayload ( qsct . xml ( nsiconnection . querySummarySyncConfirmed ) , return payload \n 
~~ header , query = helper . parseRequest ( soap_data ) \n 
d = self . provider . querySummarySync ( header , query . connectionId , query . globalReservationId , request_info d . addCallbacks ( gotReservations , self . _createSOAPFault , callbackArgs = ( header , ) , errbackArgs = ( return d \n 
~~ def queryRecursive ( self , soap_data , request_info ) : \n 
d = self . provider . queryRecursive ( header , query . connectionId , query . globalReservationId , request_info d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault return d \n 
~~ ~~ import os , datetime , json \n 
from twisted . trial import unittest \n 
from twisted . internet import defer , task \n 
from opennsa import config , nsa , database \n 
from opennsa . topology import nml \n 
from opennsa . backends import ncsvpn \n 
from . import common \n 
class NCSVPNBackendTest ( unittest . TestCase ) : \n 
~~~ self . clock = task . Clock ( ) \n 
tcf = os . path . expanduser ( ) \n 
tc = json . load ( open ( tcf ) ) \n 
ncs_config = { \n 
config . NCS_SERVICES_URL : tc [ ] , \n 
config . NCS_USER : tc [ ] , \n 
config . NCS_PASSWORD : tc [ ] \n 
self . requester = common . DUDRequester ( ) \n 
self . backend = ncsvpn . NCSVPNBackend ( , self . sr , self . requester , ncs_config ) \n 
self . backend . scheduler . clock = self . clock \n 
self . backend . startService ( ) \n 
database . setupDatabase ( tc [ ] , tc [ ] , tc [ ] ) \n 
self . requester_nsa = nsa . NetworkServiceAgent ( , self . provider_nsa = nsa . NetworkServiceAgent ( , \n 
source_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , dest_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , start_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 2 ) \n 
end_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 30 ) \n 
bandwidth = 200 \n 
self . service_params = nsa . ServiceParameters ( start_time , end_time , source_stp , dest_stp , bandwidth \n 
~~ @ defer . inlineCallbacks \n 
def tearDown ( self ) : \n 
~~~ from opennsa . backends . common import simplebackend \n 
yield simplebackend . Simplebackendconnection . deleteAll ( ) \n 
yield self . backend . stopService ( ) \n 
def testActivation ( self ) : \n 
~~~ _ , _ , cid , sp = yield self . reserve ( self . requester_nsa , self . provider_nsa , None , None , None , None yield self . backend . reserveCommit ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
yield self . backend . provision ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
self . clock . advance ( 3 ) \n 
connection_id , active , version_consistent , version , timestamp = yield d_up \n 
self . failUnlessEqual ( cid , connection_id ) \n 
self . failUnlessEqual ( active , True ) \n 
self . failUnlessEqual ( version_consistent , True ) \n 
yield self . backend . terminate ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
connection_id , active , version_consistent , version , timestamp = yield d_down \n 
self . failUnlessEqual ( active , False ) \n 
~~ testActivation . skip = \n 
~~ from __future__ import absolute_import \n 
from . classification import * \n 
from . generic import * \n 
from . job import ImageDatasetJob \n 
from . images import * \n 
from . job import InferenceJob \n 
from . caffe_train import CaffeTrainTask \n 
from . torch_train import TorchTrainTask \n 
from . train import TrainTask \n 
import flask \n 
from flask . ext . socketio import SocketIO \n 
from gevent import monkey ; monkey . patch_all ( ) \n 
from . config import config_value \n 
from digits import utils \n 
import digits . scheduler \n 
app = flask . Flask ( __name__ ) \n 
app . config [ ] = True \n 
app . config [ ] = False \n 
app . config [ ] = config_value ( ) \n 
app . url_map . redirect_defaults = False \n 
socketio = SocketIO ( app ) \n 
scheduler = digits . scheduler . Scheduler ( config_value ( ) , True ) \n 
app . jinja_env . globals [ ] = config_value ( ) \n 
app . jinja_env . globals [ ] = digits . __version__ \n 
app . jinja_env . filters [ ] = utils . time_filters . print_time \n 
app . jinja_env . filters [ ] = utils . time_filters . print_time_diff \n 
app . jinja_env . filters [ ] = utils . time_filters . print_time_since \n 
app . jinja_env . filters [ ] = utils . sizeof_fmt \n 
app . jinja_env . filters [ ] = utils . auth . has_permission \n 
app . jinja_env . trim_blocks = True \n 
app . jinja_env . lstrip_blocks = True \n 
import digits . views \n 
app . register_blueprint ( digits . views . blueprint ) \n 
import digits . dataset . views \n 
app . register_blueprint ( digits . dataset . views . blueprint , url_prefix = ) \n 
import digits . dataset . images . views \n 
app . register_blueprint ( digits . dataset . images . views . blueprint , url_prefix = ) \n 
import digits . dataset . images . classification . views \n 
app . register_blueprint ( digits . dataset . images . classification . views . blueprint , url_prefix = import digits . dataset . images . generic . views \n 
app . register_blueprint ( digits . dataset . images . generic . views . blueprint , url_prefix = import digits . model . views \n 
app . register_blueprint ( digits . model . views . blueprint , url_prefix = ) \n 
import digits . model . images . views \n 
app . register_blueprint ( digits . model . images . views . blueprint , url_prefix = ) \n 
import digits . model . images . classification . views \n 
app . register_blueprint ( digits . model . images . classification . views . blueprint , url_prefix = import digits . model . images . generic . views \n 
app . register_blueprint ( digits . model . images . generic . views . blueprint , url_prefix = \n 
def username_decorator ( f ) : \n 
~~~ from functools import wraps \n 
@ wraps ( f ) \n 
def decorated ( * args , ** kwargs ) : \n 
~~~ this_username = flask . request . cookies . get ( , None ) \n 
app . jinja_env . globals [ ] = this_username \n 
return f ( * args , ** kwargs ) \n 
~~ return decorated \n 
~~ for endpoint , function in app . view_functions . iteritems ( ) : \n 
~~~ app . view_functions [ endpoint ] = username_decorator ( function ) \n 
~~ scheduler . load_past_jobs ( ) \n 
import requests_cache \n 
from nytcampfin import NytCampfin , NytCampfinError , NytNotFoundError \n 
CURRENT_CYCLE = 2012 \n 
~~~ API_KEY = os . environ [ ] \n 
~~ class APITest ( unittest . TestCase ) : \n 
~~~ def check_response ( self , result , url , parse = lambda r : r [ ] ) : \n 
~~~ response = requests . get ( url ) \n 
if parse and callable ( parse ) : \n 
~~~ response = parse ( response . json ) \n 
~~ self . assertEqual ( result , response ) \n 
~~ ~~ def setUp ( self ) : \n 
~~~ self . finance = NytCampfin ( API_KEY ) \n 
~~ ~~ class FilingTest ( APITest ) : \n 
~~~ def test_todays_filings ( self ) : \n 
~~~ today = self . finance . filings . today ( offset = 20 ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/filings.json?api-key=%s&offset=20" self . check_response ( today , url ) \n 
~~ def test_filings_for_date ( self ) : \n 
~~~ july4th = self . finance . filings . date ( 2012 , 0 7 , 0 4 ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/filings/2012/07/04.json?api-key=%s" self . check_response ( july4th , url ) \n 
~~ def test_form_types ( self ) : \n 
~~~ form_types = self . finance . filings . form_types ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/filings/types.json?api-key=%s" self . check_response ( form_types , url ) \n 
~~ def test_filings_by_form_type ( self ) : \n 
~~~ f2s = self . finance . filings . by_type ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/filings/types/f2.json?api-key=%s" self . check_response ( f2s , url ) \n 
~~ def test_amended_filings ( self ) : \n 
~~~ amendments = self . finance . filings . amendments ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/filings/amendments.json?api-key=%s" self . check_response ( amendments , url ) \n 
~~ ~~ class IndependentExpenditureTest ( APITest ) : \n 
~~~ def test_latest ( self ) : \n 
~~~ latest = self . finance . indexp . latest ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/independent_expenditures.json?api-key=%s" self . check_response ( latest , url ) \n 
~~ def test_ies_for_date ( self ) : \n 
~~~ july3rd = self . finance . indexp . date ( 2012 , 0 7 , 0 3 ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/independent_expenditures/2012/07/03.json?api-key=%s" self . check_response ( july3rd , url ) \n 
~~ def test_committee_ies ( self ) : \n 
~~~ ies = self . finance . indexp . committee ( "C00490045" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/C00490045/independent_expenditures.json?api-key=%s" self . check_response ( ies , url ) \n 
~~ def test_race_totals ( self ) : \n 
~~~ races = self . finance . indexp . race_totals ( "president" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/independent_expenditures/race_totals/president.json?api-key=%s" self . check_response ( races , url ) \n 
~~ def test_candidate_ies ( self ) : \n 
~~~ ies = self . finance . indexp . candidate ( "P00003608" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/candidates/P00003608/independent_expenditures.json?api-key=%s" self . check_response ( ies , url ) \n 
~~ def test_president_ies ( self ) : \n 
~~~ ies = self . finance . indexp . president ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/president/independent_expenditures.json?api-key=%s" self . check_response ( ies , url ) \n 
~~ def test_superpacs ( self ) : \n 
~~~ superpacs = self . finance . indexp . superpacs ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/superpacs.json?api-key=%s" self . check_response ( superpacs , url ) \n 
~~ ~~ class CandidateTest ( APITest ) : \n 
~~~ latest = self . finance . candidates . latest ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/candidates/new.json?api-key=%s" self . check_response ( latest , url ) \n 
~~ def test_detail ( self ) : \n 
~~~ detail = self . finance . candidates . get ( "H4NY11138" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/candidates/H4NY11138.json?api-key=%s" response = requests . get ( url ) \n 
self . check_response ( detail , url , parse = lambda r : r [ ] [ 0 ] ) \n 
~~ def test_filter ( self ) : \n 
~~~ wilson = self . finance . candidates . filter ( "Wilson" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/candidates/search.json?api-key=%s&query=Wilson" self . check_response ( wilson , url ) \n 
~~ def test_leaders ( self ) : \n 
~~~ loans = self . finance . candidates . leaders ( "candidate-loan" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/candidates/leaders/candidate-loan.json?api-key=%s" self . check_response ( loans , url ) \n 
~~ def test_candidates_for_state ( self ) : \n 
~~~ candidates = self . finance . candidates . seats ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/seats/RI.json?api-key=%s" % self . check_response ( candidates , url ) \n 
~~ def test_candidates_for_state_and_chamber ( self ) : \n 
~~~ candidates = self . finance . candidates . seats ( , ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/seats/MD/senate.json?api-key=%s" self . check_response ( candidates , url ) \n 
~~ def test_candidates_for_state_and_chamber_and_district ( self ) : \n 
~~~ candidates = self . finance . candidates . seats ( , , 6 ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/seats/MD/house/6.json?api-key=%s" self . check_response ( candidates , url ) \n 
~~ def test_late_contributions ( self ) : \n 
~~~ late_contribs = self . finance . candidates . late_contributions ( "H0TN08246" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/candidates/H0TN08246/48hour.json?api-key=%s" self . check_response ( late_contribs , url ) \n 
~~ ~~ class CommitteeTest ( APITest ) : \n 
~~~ latest = self . finance . committees . latest ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/new.json?api-key=%s" self . check_response ( latest , url ) \n 
~~~ detail = self . finance . committees . get ( "C00490045" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/C00490045.json?api-key=%s" response = requests . get ( url ) \n 
~~~ hallmark = self . finance . committees . filter ( "Hallmark" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/search.json?api-key=%s&query=Hallmark" self . check_response ( hallmark , url ) \n 
~~ def test_contributions ( self ) : \n 
~~~ contributions = self . finance . committees . contributions ( "C00381277" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/C00381277/contributions.json?api-key=%s" self . check_response ( contributions , url ) \n 
~~ def test_contributions_to_candidate ( self ) : \n 
~~~ contributions = self . finance . committees . contributions_to_candidate ( "C00007450" , "H0PA12132" ) url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/C00007450/contributions/candidates/H0PA12132.json?api-key=%s" self . check_response ( contributions , url ) \n 
~~~ late_contribs = self . finance . committees . late_contributions ( "C00466854" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/C00466854/48hour.json?api-key=%s" self . check_response ( late_contribs , url ) \n 
~~ def test_filings ( self ) : \n 
~~~ filings = self . finance . committees . filings ( "C00490045" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/C00490045/filings.json?api-key=%s" self . check_response ( filings , url ) \n 
~~ def test_ie_totals ( self ) : \n 
~~~ ie_totals = self . finance . committees . ie_totals ( "C00490045" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/C00490045/independent_expenditures/races.json?api-key=%s" self . check_response ( ie_totals , url ) \n 
~~ def test_leadership ( self ) : \n 
~~~ leadership = self . finance . committees . leadership ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/leadership.json?api-key=%s" self . check_response ( leadership , url ) \n 
~~ ~~ class LateContributionTest ( APITest ) : \n 
~~~ late_contribs = self . finance . late_contribs . latest ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/contributions/48hour.json?api-key=%s" self . check_response ( late_contribs , url ) \n 
~~ def test_date ( self ) : \n 
~~~ late_contribs = self . finance . late_contribs . date ( 2012 , 3 , 23 ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/contributions/48hour/2012/3/23.json?api-key=%s" self . check_response ( late_contribs , url ) \n 
~~ ~~ class PresidentTest ( APITest ) : \n 
~~~ def test_candidates ( self ) : \n 
~~~ candidates = self . finance . president . candidates ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/president/totals.json?api-key=%s" self . check_response ( candidates , url ) \n 
~~ def test_detail_using_id ( self ) : \n 
~~~ candidate = self . finance . president . detail ( "C00431445" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/president/candidates/C00431445.json?api-key=%s" self . check_response ( candidate , url , parse = lambda r : r [ ] [ 0 ] ) \n 
~~ def test_detail_using_name ( self ) : \n 
~~~ candidate = self . finance . president . detail ( "obama" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/president/candidates/obama.json?api-key=%s" self . check_response ( candidate , url , parse = lambda r : r [ ] [ 0 ] ) \n 
~~ def test_state_total ( self ) : \n 
~~~ state = self . finance . president . state ( "AZ" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/president/states/AZ.json?api-key=%s" self . check_response ( state , url ) \n 
~~ def test_zip_total ( self ) : \n 
~~~ zipcode = self . finance . president . zipcode ( "33407" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/president/zips/33407.json?api-key=%s" self . check_response ( zipcode , url ) \n 
~~~ unittest . main ( ) from O365 . attachment import Attachment \n 
~~ from O365 . contact import Contact \n 
from O365 . group import Group \n 
logging . basicConfig ( filename = , level = logging . DEBUG ) \n 
att_url = \n 
send_url = \n 
draft_url = \n 
update_url = \n 
def __init__ ( self , json = None , auth = None ) : \n 
if json : \n 
~~~ self . json = json \n 
self . hasAttachments = json [ ] \n 
~~~ self . json = { : { : { } } , : { } } \n 
self . hasAttachments = False \n 
~~ self . auth = auth \n 
self . attachments = [ ] \n 
self . reciever = None \n 
~~ def fetchAttachments ( self ) : \n 
if not self . hasAttachments : \n 
~~~ log . debug ( ) \n 
~~ response = requests . get ( self . att_url . format ( self . json [ ] ) , auth = self . auth ) \n 
log . info ( , str ( response ) ) \n 
json = response . json ( ) \n 
for att in json [ ] : \n 
~~~ self . attachments . append ( Attachment ( att ) ) \n 
log . debug ( , self . auth [ 0 ] ) \n 
~~~ log . info ( , self . auth [ 0 ] ) \n 
~~ ~~ return len ( self . attachments ) \n 
~~ def sendMessage ( self ) : \n 
headers = { : , : } \n 
~~~ data = { : { : { } } } \n 
data [ ] [ ] = self . json [ ] \n 
data [ ] [ ] [ ] = self . json [ ] [ ] \n 
data [ ] [ ] = [ att . json for att in self . attachments ] \n 
data [ ] = "false" \n 
data = json . dumps ( data ) \n 
log . debug ( str ( data ) ) \n 
~~~ log . error ( str ( e ) ) \n 
~~ response = requests . post ( self . send_url , data , headers = headers , auth = self . auth ) \n 
log . debug ( + str ( response ) ) \n 
if response . status_code != 202 : \n 
~~ def markAsRead ( self ) : \n 
read = \'{"IsRead":true}\' \n 
~~~ response = requests . patch ( self . update_url . format ( self . json [ ] ) , read , headers = headers , auth = self . ~~ except : \n 
~~ def getSender ( self ) : \n 
return self . json [ ] \n 
~~ def getSenderEmail ( self ) : \n 
return self . json [ ] [ ] [ ] \n 
~~ def getSenderName ( self ) : \n 
~~~ return self . json [ ] [ ] [ ] \n 
~~ ~~ def getSubject ( self ) : \n 
~~ def getBody ( self ) : \n 
return self . json [ ] [ ] \n 
~~ def setRecipients ( self , val ) : \n 
self . json [ ] = [ ] \n 
if isinstance ( val , list ) : \n 
~~~ for con in val : \n 
~~~ if isinstance ( con , Contact ) : \n 
~~~ self . addRecipient ( con ) \n 
~~ elif isinstance ( con , str ) : \n 
~~~ if in con : \n 
~~ ~~ elif isinstance ( con , dict ) : \n 
~~~ self . json [ ] . append ( con ) \n 
~~ ~~ ~~ elif isinstance ( val , dict ) : \n 
~~~ self . json [ ] = [ val ] \n 
~~ elif isinstance ( val , str ) : \n 
~~~ if in val : \n 
~~~ self . addRecipient ( val ) \n 
~~ ~~ elif isinstance ( val , Contact ) : \n 
~~ elif isinstance ( val , Group ) : \n 
~~~ for person in val : \n 
~~~ self . addRecipient ( person ) \n 
~~ def addRecipient ( self , address , name = None ) : \n 
if isinstance ( address , Contact ) : \n 
~~~ self . json [ ] . append ( address . getFirstEmailAddress ( ) ) \n 
~~ elif isinstance ( address , Group ) : \n 
~~~ for con in address . contacts : \n 
~~~ if name is None : \n 
~~~ name = address [ : address . index ( ) ] \n 
~~ self . json [ ] . append ( { : { : address , : name } } ) \n 
~~ ~~ def setSubject ( self , val ) : \n 
self . json [ ] = val \n 
~~ def setBody ( self , val ) : \n 
cont = False \n 
while not cont : \n 
~~~ self . json [ ] [ ] = val \n 
self . json [ ] [ ] = \n 
cont = True \n 
~~~ self . json [ ] = { } \n 
~~ ~~ ~~ def setBodyHTML ( self , val = None ) : \n 
if val : \n 
from eventlet import wsgi \n 
from eventlet . green import time \n 
from urlparse import parse_qs \n 
from random import random , choice \n 
datas = [ , ] \n 
comparators = [ , , , ] \n 
def parse_response ( env , start_response ) : \n 
delay = random ( ) \n 
time . sleep ( delay / 10 ) \n 
~~~ params = parse_qs ( env [ ] ) \n 
row_index = int ( params [ ] [ 0 ] ) \n 
char_index = int ( params [ ] [ 0 ] ) - 1 \n 
test_char = int ( params [ ] [ 0 ] ) \n 
comparator = comparators . index ( params [ ] [ 0 ] ) - 1 \n 
~~~ sleep_int = float ( params [ ] . pop ( 0 ) ) \n 
~~~ sleep_int = 1 \n 
~~ current_character = datas [ row_index ] [ char_index ] \n 
truth = ( cmp ( ord ( current_character ) , test_char ) == comparator ) \n 
response = types [ env [ ] ] ( test_char , current_character , comparator , sleep_int , start_response \n 
return response \n 
~~~ start_response ( , [ ( , ) ] ) \n 
return [ ] \n 
~~ ~~ def time_based_blind ( test_char , current_character , comparator , sleep_int , start_response , truth ) : \n 
~~~ sleep_time = sleep_int * truth \n 
time . sleep ( sleep_time ) \n 
start_response ( , [ ( , ) ] ) \n 
~~ def boolean_based_error ( test_char , current_character , comparator , env , start_response , truth ) : \n 
~~~ if truth : \n 
~~ ~~ def boolean_based_size ( test_char , current_character , comparator , env , start_response , truth ) : \n 
~~ ~~ types = { : time_based_blind , : boolean_based_error , : boolean_based_size } \n 
~~~ print "\\n" \n 
print "\\n" \n 
from sys import argv \n 
CHARSET = [ chr ( x ) for x in xrange ( 32 , 127 ) ] \n 
rre = re . compile ( ) \n 
cre = re . compile ( ) \n 
rows = filter ( rre . match , argv ) \n 
cols = filter ( cre . match , argv ) \n 
if rows and cols : \n 
~~~ rows = rows [ 0 ] \n 
cols = cols [ 0 ] \n 
datas = [ ] \n 
for asdf in range ( 5 ) : \n 
~~~ datas . append ( "" ) \n 
for fdsa in range ( 100 ) : \n 
~~~ datas [ - 1 ] += choice ( CHARSET ) \n 
~~ ~~ ~~ wsgi . server ( eventlet . listen ( ( , 8090 ) ) , parse_response ) \n 
import xml . etree . ElementTree as ET \n 
import cPickle \n 
def parse_rec ( filename ) : \n 
tree = ET . parse ( filename ) \n 
objects = [ ] \n 
for obj in tree . findall ( ) : \n 
~~~ obj_struct = { } \n 
obj_struct [ ] = obj . find ( ) . text \n 
obj_struct [ ] = int ( obj . find ( ) . text ) \n 
bbox = obj . find ( ) \n 
obj_struct [ ] = [ int ( bbox . find ( ) . text ) , \n 
int ( bbox . find ( ) . text ) , \n 
int ( bbox . find ( ) . text ) ] \n 
objects . append ( obj_struct ) \n 
~~ return objects \n 
~~ def voc_ap ( rec , prec , use_07_metric = False ) : \n 
if use_07_metric : \n 
~~~ ap = 0. \n 
for t in np . arange ( 0. , 1.1 , 0.1 ) : \n 
~~~ if np . sum ( rec >= t ) == 0 : \n 
~~~ p = 0 \n 
~~~ p = np . max ( prec [ rec >= t ] ) \n 
~~ ap = ap + p / 11. \n 
~~~ mrec = np . concatenate ( ( [ 0. ] , rec , [ 1. ] ) ) \n 
mpre = np . concatenate ( ( [ 0. ] , prec , [ 0. ] ) ) \n 
for i in range ( mpre . size - 1 , 0 , - 1 ) : \n 
~~~ mpre [ i - 1 ] = np . maximum ( mpre [ i - 1 ] , mpre [ i ] ) \n 
~~ i = np . where ( mrec [ 1 : ] != mrec [ : - 1 ] ) [ 0 ] \n 
ap = np . sum ( ( mrec [ i + 1 ] - mrec [ i ] ) * mpre [ i + 1 ] ) \n 
~~ return ap \n 
~~ def voc_eval ( detpath , \n 
annopath , \n 
imagesetfile , \n 
classname , \n 
cachedir , \n 
ovthresh = 0.5 , \n 
use_07_metric = False ) : \n 
if not os . path . isdir ( cachedir ) : \n 
~~~ os . mkdir ( cachedir ) \n 
~~ cachefile = os . path . join ( cachedir , ) \n 
with open ( imagesetfile , ) as f : \n 
~~~ lines = f . readlines ( ) \n 
~~ imagenames = [ x . strip ( ) for x in lines ] \n 
if not os . path . isfile ( cachefile ) : \n 
~~~ recs = { } \n 
for i , imagename in enumerate ( imagenames ) : \n 
~~~ recs [ imagename ] = parse_rec ( annopath . format ( imagename ) ) \n 
if i % 100 == 0 : \n 
~~~ print . format ( \n 
i + 1 , len ( imagenames ) ) \n 
~~ ~~ print . format ( cachefile ) \n 
with open ( cachefile , ) as f : \n 
~~~ cPickle . dump ( recs , f ) \n 
~~~ with open ( cachefile , ) as f : \n 
~~~ recs = cPickle . load ( f ) \n 
~~ ~~ class_recs = { } \n 
npos = 0 \n 
for imagename in imagenames : \n 
~~~ R = [ obj for obj in recs [ imagename ] if obj [ ] == classname ] \n 
bbox = np . array ( [ x [ ] for x in R ] ) \n 
difficult = np . array ( [ x [ ] for x in R ] ) . astype ( np . bool ) \n 
det = [ False ] * len ( R ) \n 
npos = npos + sum ( ~ difficult ) \n 
class_recs [ imagename ] = { : bbox , \n 
: difficult , \n 
: det } \n 
~~ detfile = detpath . format ( classname ) \n 
with open ( detfile , ) as f : \n 
~~ splitlines = [ x . strip ( ) . split ( ) for x in lines ] \n 
image_ids = [ x [ 0 ] for x in splitlines ] \n 
confidence = np . array ( [ float ( x [ 1 ] ) for x in splitlines ] ) \n 
BB = np . array ( [ [ float ( z ) for z in x [ 2 : ] ] for x in splitlines ] ) \n 
sorted_ind = np . argsort ( - confidence ) \n 
BB = BB [ sorted_ind , : ] \n 
image_ids = [ image_ids [ x ] for x in sorted_ind ] \n 
nd = len ( image_ids ) \n 
tp = np . zeros ( nd ) \n 
fp = np . zeros ( nd ) \n 
for d in range ( nd ) : \n 
~~~ R = class_recs [ image_ids [ d ] ] \n 
bb = BB [ d , : ] . astype ( float ) \n 
ovmax = - np . inf \n 
BBGT = R [ ] . astype ( float ) \n 
if BBGT . size > 0 : \n 
~~~ ixmin = np . maximum ( BBGT [ : , 0 ] , bb [ 0 ] ) \n 
iymin = np . maximum ( BBGT [ : , 1 ] , bb [ 1 ] ) \n 
ixmax = np . minimum ( BBGT [ : , 2 ] , bb [ 2 ] ) \n 
iymax = np . minimum ( BBGT [ : , 3 ] , bb [ 3 ] ) \n 
iw = np . maximum ( ixmax - ixmin + 1. , 0. ) \n 
ih = np . maximum ( iymax - iymin + 1. , 0. ) \n 
inters = iw * ih \n 
uni = ( ( bb [ 2 ] - bb [ 0 ] + 1. ) * ( bb [ 3 ] - bb [ 1 ] + 1. ) + \n 
( BBGT [ : , 2 ] - BBGT [ : , 0 ] + 1. ) * \n 
( BBGT [ : , 3 ] - BBGT [ : , 1 ] + 1. ) - inters ) \n 
overlaps = inters / uni \n 
ovmax = np . max ( overlaps ) \n 
jmax = np . argmax ( overlaps ) \n 
~~ if ovmax > ovthresh : \n 
~~~ if not R [ ] [ jmax ] : \n 
~~~ tp [ d ] = 1. \n 
R [ ] [ jmax ] = 1 \n 
~~~ fp [ d ] = 1. \n 
~~ ~~ fp = np . cumsum ( fp ) \n 
tp = np . cumsum ( tp ) \n 
rec = tp / float ( npos ) \n 
prec = tp / ( tp + fp + 1e-10 ) \n 
ap = voc_ap ( rec , prec , use_07_metric ) \n 
return rec , prec , ap \n 
~~ import numpy as np \n 
from ipdb import set_trace \n 
from struct import pack , unpack \n 
def ceil_div ( x , y ) : \n 
~~~ return - ( - x // y ) \n 
~~ def out_dim ( S , X , padding , strides ) : \n 
~~~ return ceil_div ( X - S + 1 + 2 * padding , strides ) \n 
~~ def strip_mantissa ( val ) : \n 
~~~ i = unpack ( , pack ( , val ) ) [ 0 ] & 0x7f800000 \n 
f = unpack ( , pack ( , i ) ) [ 0 ] \n 
return f \n 
~~ def quantize ( ary , bits , sign = 1 ) : \n 
~~~ maxval = float ( np . max ( np . absolute ( ary ) ) ) \n 
scale = strip_mantissa ( maxval ) / float ( 1 << ( bits - sign - 1 ) ) \n 
ary = np . around ( ary * ( 1.0 / scale ) ) . astype ( np . int64 ) \n 
return ary , np . float64 ( scale ) \n 
~~ def fconv_slice ( q , S , X , padding , strides ) : \n 
~~~ f1 = 0 \n 
f2 = S - 1 \n 
x1 = q * strides - padding \n 
x2 = x1 + f2 \n 
if x1 < 0 : \n 
~~~ f1 = - x1 \n 
x1 = 0 \n 
~~ if x2 >= X : \n 
~~~ dif = x2 - X + 1 \n 
f2 -= dif \n 
x2 -= dif \n 
~~ return ( slice ( f1 , f2 + 1 ) , slice ( x1 , x2 + 1 ) , f2 - f1 + 1 ) \n 
~~ def bconv_slice ( x , S , Q , padding , strides ) : \n 
~~~ qs = x - ( S - padding - 1 ) \n 
firstF = None \n 
~~~ q = qs + s \n 
if q % strides == 0 : \n 
~~~ q strides \n 
if q >= 0 and q < Q : \n 
~~~ if firstF is None : \n 
~~~ firstF = s \n 
firstE = q \n 
~~ lastF = s \n 
lastE = q \n 
~~ ~~ ~~ return ( slice ( firstF , lastF + 1 , strides ) , slice ( firstE , lastE + 1 , strides ) , 0 ) \n 
~~ def xprop_direct ( I , F , O , padding , strides , backward = False ) : \n 
~~~ if all ( x == 1 for x in F . shape [ 1 : 3 ] ) : \n 
~~~ C = F . shape [ 0 ] \n 
K = F . shape [ 4 ] \n 
if backward : \n 
~~~ O [ : ] = np . dot ( F . reshape ( ( C , - 1 ) ) , I . reshape ( ( K , - 1 ) ) ) . reshape ( ( O . shape ) ) \n 
~~~ O [ : ] = np . dot ( F . reshape ( ( C , - 1 ) ) . T , I . reshape ( ( C , - 1 ) ) ) . reshape ( ( O . shape ) ) \n 
~~ if backward : \n 
~~~ F = np . transpose ( F [ : , : : - 1 , : : - 1 , : ] , ( 3 , 1 , 2 , 0 ) ) . copy ( ) \n 
xconv_slice = bconv_slice \n 
~~~ xconv_slice = fconv_slice \n 
~~ C , Y , X , N = I . shape \n 
C , R , S , K = F . shape \n 
K , P , Q , N = O . shape \n 
qSlice = [ xconv_slice ( q , S , X , padding [ 0 ] , strides [ 0 ] ) for q in range ( Q ) ] \n 
for p in range ( P ) : \n 
~~~ sliceR , sliceY , _ = xconv_slice ( p , R , Y , padding [ 1 ] , strides [ 1 ] ) \n 
for q in range ( Q ) : \n 
~~~ sliceS , sliceX , _ = qSlice [ q ] \n 
slicedF = F [ : , sliceR , sliceS , : ] . reshape ( ( - 1 , K ) ) \n 
slicedI = I [ : , sliceY , sliceX , : ] . reshape ( ( - 1 , N ) ) \n 
O [ : , p , q , : ] = np . dot ( slicedF . T , slicedI ) \n 
~~ ~~ ~~ def updat_direct ( I , E , U , padding , strides ) : \n 
~~~ C , Y , X , N = I . shape \n 
K , P , Q , N = E . shape \n 
C , R , S , K = U . shape \n 
if all ( x == 1 for x in ( R , S ) ) : \n 
~~~ U [ : ] = np . dot ( I . reshape ( ( C , - 1 ) ) , E . reshape ( ( K , - 1 ) ) . T ) . reshape ( ( U . shape ) ) \n 
~~ U . fill ( 0.0 ) \n 
qSlice = [ fconv_slice ( q , S , X , padding [ 0 ] , strides [ 0 ] ) for q in range ( Q ) ] \n 
~~~ sliceR , sliceY , rlen = fconv_slice ( p , R , Y , padding [ 1 ] , strides [ 1 ] ) \n 
~~~ sliceS , sliceX , slen = qSlice [ q ] \n 
slicedE = E [ : , p , q , : ] \n 
U [ : , sliceR , sliceS , : ] += np . dot ( slicedI , slicedE . T ) . reshape ( ( C , rlen , slen , K ) ) \n 
~~ ~~ ~~ I_4x4_3x3 = ( \n 
np . array ( [ \n 
[ 4.0 , 0.0 , - 5.0 , 0.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , - 4.0 , - 4.0 , 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 4.0 , - 4.0 , - 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , - 2.0 , - 1.0 , 2.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 2.0 , - 1.0 , - 2.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 4.0 , 0.0 , - 5.0 , 0.0 , 1.0 ] ] ) , \n 
[ 1.0 , 0.0 , - 5.0 / 4.0 , 0.0 , 1.0 / 4.0 , 0.0 ] , \n 
[ 0.0 , 2.0 / 3.0 , 2.0 / 3.0 , - 1.0 / 6.0 , - 1.0 / 6.0 , 0.0 ] , \n 
[ 0.0 , - 2.0 / 3.0 , 2.0 / 3.0 , 1.0 / 6.0 , - 1.0 / 6.0 , 0.0 ] , \n 
[ 0.0 , - 1.0 / 12. , - 1.0 / 24. , 1.0 / 12. , 1.0 / 24. , 0.0 ] , \n 
[ 0.0 , 1.0 / 12. , - 1.0 / 24. , - 1.0 / 12. , 1.0 / 24. , 0.0 ] , \n 
F_4x4_3x3 = ( \n 
[ 1.0 / 4.0 , 0.0 , 0.0 ] , \n 
[ - 1.0 / 6.0 , - 1.0 / 6.0 , - 1.0 / 6.0 ] , \n 
[ - 1.0 / 6.0 , 1.0 / 6.0 , - 1.0 / 6.0 ] , \n 
[ 1.0 / 24. , 1.0 / 12. , 1.0 / 6.0 ] , \n 
[ 1.0 / 24. , - 1.0 / 12. , 1.0 / 6.0 ] , \n 
[ 0.0 , 0.0 , 1.0 ] ] ) , \n 
[ 1.0 , 0.0 , 0.0 ] , \n 
[ 1.0 , 1.0 , 1.0 ] , \n 
[ 1.0 , - 1.0 , 1.0 ] , \n 
[ 1.0 , 2.0 , 4.0 ] , \n 
[ 1.0 , - 2.0 , 4.0 ] , \n 
O_4x4_3x3 = ( \n 
[ 1.0 , 1.0 , 1.0 , 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , - 1.0 , 2.0 , - 2.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , 1.0 , 4.0 , 4.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , - 1.0 , 8.0 , - 8.0 , 1.0 ] ] ) , \n 
[ 1.0 / 4.0 , - 1.0 / 6.0 , - 1.0 / 6.0 , 1.0 / 24. , 1.0 / 24. , 0.0 ] , \n 
[ 0.0 , - 1.0 / 6.0 , 1.0 / 6.0 , 1.0 / 12. , - 1.0 / 12. , 0.0 ] , \n 
[ 0.0 , - 1.0 / 6.0 , - 1.0 / 6.0 , 1.0 / 6.0 , 1.0 / 6.0 , 0.0 ] , \n 
[ 0.0 , - 1.0 / 6.0 , 1.0 / 6.0 , 1.0 / 3.0 , - 1.0 / 3.0 , 1.0 ] ] ) , \n 
rcp3 = 1.0 / 3.0 \n 
rcp4 = 1.0 / 4.0 \n 
rcp6 = 1.0 / 6.0 \n 
rcp12 = 1.0 / 12.0 \n 
rcp24 = 1.0 / 24.0 \n 
def trans_I_4x4_3x3 ( Iw , I , minimal = False , trans = False ) : \n 
~~~ if minimal : \n 
~~~ T0 = np . empty ( ( 6 , 6 ) ) \n 
T1 = np . empty ( ( 6 , 6 ) ) \n 
for O , I in ( ( T0 , I ) , ( T1 , T0 . T ) ) : \n 
~~~ t0 = ( I [ 2 , : ] * 4.0 - I [ 4 , : ] ) * rcp6 \n 
t1 = ( I [ 1 , : ] * 4.0 - I [ 3 , : ] ) * rcp6 \n 
t2 = ( I [ 4 , : ] - I [ 2 , : ] ) * rcp24 \n 
t3 = ( I [ 3 , : ] - I [ 1 , : ] ) * rcp12 \n 
O [ 0 , : ] = I [ 0 , : ] + ( I [ 2 , : ] * - 5.0 + I [ 4 , : ] ) * rcp4 \n 
O [ 1 , : ] = t0 + t1 \n 
O [ 2 , : ] = t0 - t1 \n 
O [ 3 , : ] = t2 + t3 \n 
O [ 4 , : ] = t2 - t3 \n 
O [ 5 , : ] = I [ 1 , : ] * 4.0 - I [ 3 , : ] * 5.0 + I [ 5 , : ] \n 
~~ Iw [ : ] = T1 . T \n 
~~~ Iw [ : ] = np . dot ( np . dot ( I_4x4_3x3 [ trans [ 0 ] ] , I ) , I_4x4_3x3 [ trans [ 1 ] ] . T ) \n 
~~ ~~ def trans_F_4x4_3x3 ( Fw , F , minimal = False , trans = False ) : \n 
~~~ T0 = np . empty ( ( 6 , 3 ) ) \n 
for O , I in ( ( T0 , F ) , ( T1 , T0 . T ) ) : \n 
~~~ t0 = I [ 0 , : ] + I [ 2 , : ] \n 
t1 = I [ 0 , : ] + I [ 2 , : ] * 4.0 \n 
O [ 0 , : ] = I [ 0 , : ] \n 
O [ 1 , : ] = t0 + I [ 1 , : ] \n 
O [ 2 , : ] = t0 - I [ 1 , : ] \n 
O [ 3 , : ] = t1 + I [ 1 , : ] * 2.0 \n 
O [ 4 , : ] = t1 - I [ 1 , : ] * 2.0 \n 
O [ 5 , : ] = I [ 2 , : ] \n 
~~ Fw [ : ] = T1 . T \n 
~~~ Fw [ : ] = np . dot ( np . dot ( F_4x4_3x3 [ trans [ 0 ] ] , F ) , F_4x4_3x3 [ trans [ 1 ] ] . T ) \n 
~~ ~~ def trans_O_4x4_3x3 ( Mw , minimal = False , trans = False ) : \n 
~~~ T0 = np . empty ( ( 4 , 6 ) ) \n 
T1 = np . empty ( ( 4 , 4 ) ) \n 
for O , I in ( ( T0 , Mw ) , ( T1 , T0 . T ) ) : \n 
~~~ t0 = I [ 1 , : ] + I [ 2 , : ] \n 
t1 = I [ 3 , : ] + I [ 4 , : ] \n 
t2 = I [ 1 , : ] - I [ 2 , : ] \n 
t3 = I [ 3 , : ] - I [ 4 , : ] \n 
O [ 0 , : ] = t0 + t1 + I [ 0 , : ] \n 
O [ 1 , : ] = t2 + t3 * 2.0 \n 
O [ 2 , : ] = t0 + t1 * 4.0 \n 
O [ 3 , : ] = t2 + t3 * 8.0 + I [ 5 , : ] \n 
~~ return T1 . T \n 
~~~ return np . dot ( np . dot ( O_4x4_3x3 [ trans [ 0 ] ] , Mw ) , O_4x4_3x3 [ trans [ 1 ] ] . T ) \n 
~~ ~~ def trans_F_3x3_4x4 ( Fw , F , minimal = False , trans = False ) : \n 
~~~ T0 = np . empty ( ( 6 , 4 ) ) \n 
t2 = I [ 1 , : ] + I [ 3 , : ] \n 
t3 = I [ 1 , : ] + I [ 3 , : ] * 4.0 \n 
O [ 1 , : ] = t0 + t2 \n 
O [ 2 , : ] = t0 - t2 \n 
O [ 3 , : ] = t1 + t3 * 2.0 \n 
O [ 4 , : ] = t1 - t3 * 2.0 \n 
O [ 5 , : ] = I [ 3 , : ] \n 
~~~ Fw [ : ] = np . dot ( np . dot ( O_4x4_3x3 [ trans [ 0 ] ] . T , F ) , O_4x4_3x3 [ trans [ 1 ] ] ) \n 
~~ ~~ def trans_O_3x3_4x4 ( Mw , minimal = False , trans = False ) : \n 
~~~ T0 = np . empty ( ( 3 , 6 ) ) \n 
T1 = np . empty ( ( 3 , 3 ) ) \n 
O [ 0 , : ] = I [ 0 , : ] + t0 + t1 \n 
O [ 1 , : ] = I [ 1 , : ] - I [ 2 , : ] + 2 * ( I [ 3 , : ] - I [ 4 , : ] ) \n 
O [ 2 , : ] = t0 + 4 * t1 + I [ 5 , : ] \n 
~~~ return np . dot ( np . dot ( F_4x4_3x3 [ trans [ 0 ] ] . T , Mw ) , F_4x4_3x3 [ trans [ 1 ] ] ) \n 
~~ ~~ def image_slice ( x , X , B , D , pad = 0 ) : \n 
~~~ start = x * B - pad \n 
stop = start + D \n 
pad = [ 0 , 0 ] \n 
if start < 0 : \n 
~~~ pad [ 0 ] = - start \n 
start = 0 \n 
~~ if stop - 1 >= X : \n 
~~~ pad [ 1 ] = stop - X \n 
~~ return start , stop , pad \n 
~~ def output_slice ( p , P , B ) : \n 
~~~ p0 = p * B \n 
p1 = p0 + B \n 
if p1 > P : \n 
~~~ p1 = P \n 
~~ return p0 , p1 , p1 - p0 \n 
~~ def xprop_winograd ( I , F , O , padding , minimal = False , trans = False , backward = False ) : \n 
~~~ if backward : \n 
padding = [ 2 - p for p in padding ] \n 
B = 4 \n 
D = B + 2 \n 
Yw = ceil_div ( P , B ) \n 
Xw = ceil_div ( Q , B ) \n 
Fw = np . empty ( ( D , D , C , K ) ) \n 
Iw = np . empty ( ( D , D , C , Yw , Xw , N ) ) \n 
for c in range ( C ) : \n 
~~~ for k in range ( K ) : \n 
~~~ trans_F_4x4_3x3 ( Fw [ : , : , c , k ] , F [ c , : , : , k ] , minimal , trans ) \n 
~~ ~~ for y in range ( Yw ) : \n 
~~~ start_y , stop_y , pad_y = image_slice ( y , Y , B , D , padding [ 0 ] ) \n 
for x in range ( Xw ) : \n 
~~~ start_x , stop_x , pad_x = image_slice ( x , X , B , D , padding [ 1 ] ) \n 
sliceI = I [ : , start_y : stop_y , start_x : stop_x , : ] \n 
if any ( pad_y ) or any ( pad_x ) : \n 
~~~ sliceI = np . pad ( sliceI , ( ( 0 , 0 ) , pad_y , pad_x , ( 0 , 0 ) ) , ) \n 
~~ for c in range ( C ) : \n 
~~~ for n in range ( N ) : \n 
~~~ trans_I_4x4_3x3 ( Iw [ : , : , c , y , x , n ] , sliceI [ c , : , : , n ] , minimal , trans ) \n 
~~ ~~ ~~ ~~ for s in range ( D ) : \n 
~~~ for t in range ( D ) : \n 
~~~ Mw [ s , t ] = np . dot ( Fw [ s , t ] . T , Iw [ s , t ] . reshape ( C , - 1 ) ) . reshape ( ( K , Yw , Xw , N ) ) \n 
~~~ p0 , p1 , plen = output_slice ( y , P , B ) \n 
~~~ q0 , q1 , qlen = output_slice ( x , Q , B ) \n 
for k in range ( K ) : \n 
~~~ Out = trans_O_4x4_3x3 ( Mw [ : , : , k , y , x , n ] , minimal , trans ) \n 
O [ k , p0 : p1 , q0 : q1 , n ] = Out [ 0 : plen , 0 : qlen ] \n 
~~ ~~ ~~ ~~ ~~ def updat_winograd ( I , E , U , padding , minimal = False , trans = False , inner = True ) : \n 
Iw = np . empty ( ( D , D , N , C ) ) \n 
Ew = np . empty ( ( D , D , N , K ) ) \n 
if inner : \n 
~~~ Mw = np . empty ( ( D , D , C , K ) ) \n 
U . fill ( 0.0 ) \n 
~~~ Mw = np . zeros ( ( D , D , C , K ) ) \n 
~~ for y in range ( Yw ) : \n 
start_p , stop_p , pad_p = image_slice ( y , P , B , B ) \n 
start_q , stop_q , pad_q = image_slice ( x , Q , B , B ) \n 
sliceE = E [ : , start_p : stop_p , start_q : stop_q , : ] \n 
~~ if any ( pad_p ) or any ( pad_q ) : \n 
~~~ sliceE = np . pad ( sliceE , ( ( 0 , 0 ) , pad_p , pad_q , ( 0 , 0 ) ) , ) \n 
~~~ trans_I_4x4_3x3 ( Iw [ : , : , n , c ] , sliceI [ c , : , : , n ] , minimal , trans ) \n 
~~ ~~ for k in range ( K ) : \n 
~~~ trans_F_3x3_4x4 ( Ew [ : , : , n , k ] , sliceE [ k , : , : , n ] , minimal , trans ) \n 
~~ ~~ for s in range ( D ) : \n 
~~~ if inner : \n 
~~~ Mw [ s , t ] = np . dot ( Iw [ s , t ] . T , Ew [ s , t ] ) \n 
~~~ Mw [ s , t ] += np . dot ( Iw [ s , t ] . T , Ew [ s , t ] ) \n 
~~ ~~ ~~ if inner : \n 
~~~ for c in range ( C ) : \n 
~~~ U [ c , : , : , k ] += trans_O_3x3_4x4 ( Mw [ : , : , c , k ] , minimal , trans ) \n 
~~ ~~ ~~ ~~ ~~ if not inner : \n 
~~~ U [ c , : , : , k ] = trans_O_3x3_4x4 ( Mw [ : , : , c , k ] , minimal , trans ) \n 
~~ ~~ ~~ ~~ np . set_printoptions ( threshold = 8192 * 4 , linewidth = 600 , formatter = { : lambda x : "%6.3f" % x } ) \n 
minimal = 1 \n 
trans = ( 2 , 2 ) \n 
ones = 0 \n 
N = 32 \n 
C , K = 32 , 32 \n 
Y , X = 6 , 6 \n 
P = out_dim ( R , Y , padding [ 0 ] , strides [ 0 ] ) \n 
Q = out_dim ( S , X , padding [ 1 ] , strides [ 1 ] ) \n 
print P , Q \n 
dimI = ( C , Y , X , N ) \n 
dimF = ( C , R , S , K ) \n 
dimO = ( K , P , Q , N ) \n 
if ones : \n 
~~~ I = np . zeros ( dimI ) \n 
F = np . ones ( dimF ) \n 
E = np . zeros ( dimO ) \n 
for p , q in np . ndindex ( ( Y , X ) ) : \n 
~~~ I [ : , p , q , : ] = np . identity ( N ) \n 
~~ for p , q in np . ndindex ( ( P , Q ) ) : \n 
~~~ E [ : , p , q , n ] = range ( K ) \n 
~~~ I = np . random . uniform ( - 1.0 , 1.0 , dimI ) \n 
F = np . random . uniform ( - 1.0 , 1.0 , dimF ) \n 
E = np . random . uniform ( - 1.0 , 1.0 , dimO ) \n 
~~ Od = np . empty ( dimO ) \n 
Bd = np . empty ( dimI ) \n 
Ud = np . empty ( dimF ) \n 
Uw = np . empty ( dimF ) \n 
xprop_direct ( I , F , Od , padding , strides ) \n 
xprop_winograd ( I , F , Ow , padding , minimal = minimal , trans = trans ) \n 
xprop_direct ( E , F , Bd , padding , strides , backward = True ) \n 
xprop_winograd ( E , F , Bw , padding , minimal = minimal , trans = trans , backward = True ) \n 
updat_direct ( I , E , Ud , padding , strides ) \n 
updat_winograd ( I , E , Uw , padding , minimal = minimal , trans = trans ) \n 
difO = Od - Ow \n 
difB = Bd - Bw \n 
difU = Ud - Uw \n 
print abs ( difO ) . max ( ) / Od . max ( ) \n 
print abs ( difB ) . max ( ) / Bd . max ( ) \n 
print abs ( difU ) . max ( ) / Ud . max ( ) \n 
from neon . layers . layer import ( Linear , Bias , Affine , Conv , Convolution , GeneralizedCost , Dropout , \n 
Pooling , Activation , DataTransform , BatchNorm , BatchNormAutodiff , \n 
Deconv , Deconvolution , GeneralizedCostMask , LookupTable , \n 
BranchNode , SkipNode , LRN , ColorNoise ) \n 
from neon . layers . recurrent import ( Recurrent , LSTM , GRU , RecurrentSum , RecurrentMean , RecurrentLast , BiRNN , BiLSTM , DeepBiRNN , DeepBiLSTM ) \n 
from neon . layers . container import ( Tree , Sequential , MergeMultistream , MergeBroadcast , Multicost , \n 
RoiPooling , MergeSum , SingleOutputTree ) \n 
from neon . util . argparser import NeonArgparser \n 
from neon . initializers import Constant , Gaussian \n 
from neon . layers import Conv , Dropout , Pooling , GeneralizedCost , Affine \n 
from neon . optimizers import GradientDescentMomentum , MultiOptimizer , Schedule \n 
from neon . transforms import Rectlin , Softmax , CrossEntropyMulti , TopKMisclassification \n 
from neon . models import Model \n 
from neon . data import ImageLoader \n 
from neon . callbacks . callbacks import Callbacks \n 
parser = NeonArgparser ( __doc__ ) \n 
img_set_options = dict ( repo_dir = args . data_dir , \n 
inner_size = 224 , \n 
subset_pct = 0.09990891117239205 ) \n 
train = ImageLoader ( set_name = , scale_range = ( 256 , 256 ) , shuffle = False , \n 
do_transforms = False , ** img_set_options ) \n 
test = ImageLoader ( set_name = , scale_range = ( 256 , 384 ) , shuffle = False , \n 
layers = [ Conv ( ( 11 , 11 , 64 ) , init = Gaussian ( scale = 0.01 ) , bias = Constant ( 0 ) , \n 
activation = Rectlin ( ) , padding = 3 , strides = 4 ) , \n 
Pooling ( 3 , strides = 2 ) , \n 
Conv ( ( 5 , 5 , 192 ) , init = Gaussian ( scale = 0.01 ) , bias = Constant ( 1 ) , \n 
activation = Rectlin ( ) , padding = 2 ) , \n 
Conv ( ( 3 , 3 , 384 ) , init = Gaussian ( scale = 0.03 ) , bias = Constant ( 0 ) , \n 
activation = Rectlin ( ) , padding = 1 ) , \n 
Conv ( ( 3 , 3 , 256 ) , init = Gaussian ( scale = 0.03 ) , bias = Constant ( 1 ) , \n 
Affine ( nout = 4096 , init = Gaussian ( scale = 0.01 ) , bias = Constant ( 1 ) , activation = Rectlin ( ) ) , \n 
Dropout ( keep = 1.0 ) , \n 
Affine ( nout = 1000 , init = Gaussian ( scale = 0.01 ) , bias = Constant ( - 7 ) , activation = Softmax ( ) ) ] \n 
model = Model ( layers = layers ) \n 
weight_sched = Schedule ( [ 22 , 44 , 65 ] , ( 1 / 250. ) ** ( 1 / 3. ) ) \n 
opt_gdm = GradientDescentMomentum ( 0.01 / 10 , 0.9 , wdecay = 0.0005 , schedule = weight_sched , \n 
stochastic_round = args . rounding ) \n 
opt_biases = GradientDescentMomentum ( 0.02 / 10 , 0.9 , schedule = Schedule ( [ 44 ] , 0.1 ) , \n 
opt = MultiOptimizer ( { : opt_gdm , : opt_biases } ) \n 
valmetric = TopKMisclassification ( k = 5 ) \n 
callbacks = Callbacks ( model , eval_set = test , metric = valmetric , ** args . callback_args ) \n 
cost = GeneralizedCost ( costfunc = CrossEntropyMulti ( ) ) \n 
model . fit ( train , optimizer = opt , num_epochs = args . epochs , cost = cost , callbacks = callbacks ) \n 
import itertools as itt \n 
from neon import NervanaObject \n 
from neon . layers . layer import Pooling \n 
from tests . utils import allclose_with_out \n 
def pytest_generate_tests ( metafunc ) : \n 
~~~ np . random . seed ( 1 ) \n 
if metafunc . config . option . all : \n 
~~~ bsz_rng = [ 32 , 64 ] \n 
~~~ bsz_rng = [ 128 ] \n 
~~ if in metafunc . fixturenames : \n 
~~~ fargs = [ ] \n 
~~~ fs_rng = [ 2 , 3 , 5 ] \n 
pad_rng = [ 0 , 1 ] \n 
nifm_rng = [ 16 , 32 ] \n 
in_sz_rng = [ 8 , 16 ] \n 
~~~ fs_rng = [ 2 , 4 ] \n 
nifm_rng = [ 8 ] \n 
in_sz_rng = [ 8 ] \n 
~~ fargs_ = [ ] \n 
for fs in fs_rng : \n 
~~~ stride_rng = set ( [ 1 , fs / 2 , fs ] ) \n 
fargs_ . append ( itt . product ( fs_rng , nifm_rng , pad_rng , stride_rng , in_sz_rng , bsz_rng ) ) \n 
~~ fargs = itt . chain ( * fargs_ ) \n 
metafunc . parametrize ( , fargs ) \n 
~~ ~~ def ref_pooling ( inp , inp_shape , fshape , padding , strides , be , ncheck = None ) : \n 
~~~ inp_lshape = list ( inp_shape ) \n 
bsz = inp . shape [ - 1 ] \n 
if ncheck is None : \n 
~~~ check_inds = np . arange ( bsz ) \n 
~~ elif type ( ncheck ) is int : \n 
~~~ check_inds = np . random . permutation ( bsz ) \n 
check_inds = check_inds [ 0 : ncheck ] \n 
~~~ check_inds = ncheck \n 
~~ check_inds = np . sort ( check_inds ) \n 
inp_lshape . append ( bsz ) \n 
inpa = inp . get ( ) . reshape ( inp_lshape ) \n 
outshape = ( inp_lshape [ 0 ] , \n 
be . output_dim ( inp_lshape [ 1 ] , fshape [ 0 ] , padding , strides [ 0 ] , pooling = True ) , \n 
be . output_dim ( inp_lshape [ 2 ] , fshape [ 1 ] , padding , strides [ 1 ] , pooling = True ) , \n 
len ( check_inds ) ) \n 
if padding > 0 : \n 
~~~ padded_shape = ( inp_lshape [ 0 ] , \n 
inp_lshape [ 1 ] + 2 * padding , \n 
inp_lshape [ 2 ] + 2 * padding , \n 
inp_lshape [ - 1 ] ) \n 
inp_pad = np . zeros ( padded_shape ) \n 
inp_pad [ : , padding : - padding , padding : - padding , : ] = inpa [ : , 0 : , 0 : , : ] \n 
~~~ inp_pad = inpa \n 
~~ out_exp = np . zeros ( outshape ) \n 
for indC in range ( outshape [ 0 ] ) : \n 
~~~ for indh in range ( outshape [ 1 ] ) : \n 
~~~ hrng = ( indh * strides [ 0 ] , indh * strides [ 0 ] + fshape [ 0 ] ) \n 
for indw in range ( outshape [ 2 ] ) : \n 
~~~ wrng = ( indw * strides [ 1 ] , indw * strides [ 1 ] + fshape [ 1 ] ) \n 
for cnt , indb in enumerate ( check_inds ) : \n 
~~~ inp_check = inp_pad [ indC , hrng [ 0 ] : hrng [ 1 ] , wrng [ 0 ] : wrng [ 1 ] , indb ] \n 
out_exp [ indC , indh , indw , cnt ] = np . max ( inp_check ) \n 
~~ ~~ ~~ ~~ return ( out_exp , check_inds ) \n 
~~ def test_padding ( backend_default , poolargs ) : \n 
~~~ fshape , nifm , padding , stride , in_sz , batch_size = poolargs \n 
NervanaObject . be . bsz = batch_size \n 
inshape = ( nifm , in_sz , in_sz ) \n 
insize = np . prod ( inshape ) \n 
neon_layer = Pooling ( fshape = fshape , strides = stride , padding = padding ) \n 
inp = neon_layer . be . array ( np . random . random ( ( insize , batch_size ) ) ) \n 
inp . lshape = inshape \n 
neon_layer . configure ( inshape ) \n 
neon_layer . prev_layer = True \n 
neon_layer . allocate ( ) \n 
neon_layer . set_deltas ( [ neon_layer . be . iobuf ( inshape ) ] ) \n 
out = neon_layer . fprop ( inp ) . get ( ) \n 
ncheck = [ 0 , batch_size / 2 , batch_size - 1 ] \n 
( out_exp , check_inds ) = ref_pooling ( inp , inp . lshape , \n 
( fshape , fshape ) , \n 
padding , \n 
( stride , stride ) , \n 
neon_layer . be , \n 
ncheck = ncheck ) \n 
out_shape = list ( out_exp . shape [ 0 : 3 ] ) \n 
out_shape . append ( batch_size ) \n 
outa = out . reshape ( out_shape ) \n 
assert allclose_with_out ( out_exp , outa [ : , : , : , check_inds ] , atol = 0.0 , rtol = 0.0 ) \n 
#-- \n 
revision = \n 
down_revision = \n 
from alembic import op \n 
import sqlalchemy as sa \n 
def upgrade ( ) : \n 
~~~ op . add_column ( , sa . Column ( , sa . Integer ) ) \n 
~~ def downgrade ( ) : \n 
~~~ op . drop_column ( , ) \n 
~~ from . import view \n 
from . comp import Card , NewCard \n 
from nagare . i18n import _ \n 
from nagare import presentation , ajax , security , component \n 
from . comp import Gallery , Asset , AssetCropper \n 
def render_image ( self , h , comp , size , randomize = False , ** kw ) : \n 
~~~ metadata = self . assets_manager . get_metadata ( self . filename ) \n 
src = self . assets_manager . get_image_url ( self . filename , size ) \n 
if randomize : \n 
~~~ src += + h . generate_id ( ) \n 
~~ return h . img ( title = metadata [ ] , alt = metadata [ ] , \n 
src = src , ** kw ) \n 
~~ def render_file ( self , h , comp , size , ** kw ) : \n 
~~~ kw [ ] += \n 
metadata = self . assets_manager . get_metadata ( self . filename ) \n 
res = [ h . img ( title = metadata [ ] , alt = metadata [ ] , \n 
src = "img/file-icon.jpg" , ** kw ) ] \n 
if size == : \n 
~~~ res . append ( h . span ( metadata [ ] ) ) \n 
~~ CONTENT_TYPES = { : render_image , \n 
: render_image , \n 
: render_image } \n 
@ presentation . render_for ( Gallery ) \n 
def render ( self , h , comp , * args ) : \n 
~~~ with h . div ( id = + self . comp_id ) : \n 
~~~ with h . div ( class_ = ) : \n 
~~~ h << comp . render ( h , model = ) \n 
~~ with h . div ( id = "card-gallery" ) : \n 
~~~ h << comp . render ( h , self . model ) \n 
~~ ~~ return h . root \n 
~~ @ presentation . render_for ( Gallery , ) \n 
def render_Gallery_view ( self , h , comp , model ) : \n 
~~~ model = if security . has_permissions ( , self ) else \n 
for asset in self . assets : \n 
~~~ h << asset . render ( h , model ) \n 
~~ return h . root \n 
def render_Gallery_crop ( self , h , comp , model ) : \n 
~~~ return self . cropper . on_answer ( self . action ) \n 
def render_cover ( self , h , comp , model ) : \n 
~~~ cover = self . get_cover ( ) \n 
if cover : \n 
~~~ h << h . p ( component . Component ( self . get_cover ( ) , model = ) , class_ = ) \n 
~~ @ presentation . render_for ( Gallery , "action" ) \n 
def render_download ( self , h , comp , * args ) : \n 
~~~ if security . has_permissions ( , self ) : \n 
~~~ submit_id = h . generate_id ( "attach_submit" ) \n 
input_id = h . generate_id ( "attach_input" ) \n 
h << h . label ( ( h . i ( class_ = ) , \n 
with h . form : \n 
~~~ h << h . script ( \n 
% \n 
: ajax . py2js ( self . assets_manager . max_size ) , \n 
: ajax . py2js ( input_id ) , \n 
: ajax . py2js ( submit_id ) , \n 
: ajax . py2js ( \n 
_ ( ) \n 
) . decode ( ) \n 
submit_action = ajax . Update ( \n 
render = lambda r : r . div ( comp . render ( r , model = None ) , r . script ( component_to_update = + self . comp_id , \n 
h << h . input ( id = input_id , class_ = , type = "file" , name = "file" , multiple = "multiple" h << h . input ( class_ = , id = submit_id , type = "submit" ) . action ( submit_action ) \n 
~~ @ presentation . render_for ( Gallery , model = ) \n 
def render_gallery_badge ( self , h , * args ) : \n 
if self . assets : \n 
~~~ with h . span ( class_ = ) : \n 
~~~ h << h . span ( h . i ( class_ = ) , , len ( self . assets ) , class_ = ) \n 
~~ @ presentation . render_for ( Asset ) \n 
@ presentation . render_for ( Asset , model = ) \n 
def render_asset ( self , h , comp , model , * args ) : \n 
~~~ res = [ ] \n 
kw = { : True } if model == else { } \n 
kw [ ] = model \n 
if self . is_cover : \n 
~~~ res . append ( h . span ( class_ = ) ) \n 
~~ meth = CONTENT_TYPES . get ( metadata [ ] , render_file ) \n 
res . append ( meth ( self , h , comp , model , ** kw ) ) \n 
return res \n 
~~ @ presentation . render_for ( Asset , model = ) \n 
def render_Asset_thumb ( self , h , comp , model , * args ) : \n 
~~~ action = h . a . action ( lambda : comp . answer ( ( , self ) ) ) . get ( ) \n 
onclick = _ ( ) \n 
with h . a ( class_ = , title = _ ( ) , href = , onclick = onclick ) : \n 
~~~ h << h . i ( class_ = ) \n 
~~ if self . is_image ( ) : \n 
~~~ with h . a ( class_ = , title = _ ( ) ) . action ( lambda : comp . answer ( ( ~~~ if self . is_cover : \n 
~~~ h << { : } \n 
~~ h << h . i ( class_ = ) \n 
~~ ~~ with h . a ( href = self . assets_manager . get_image_url ( self . filename ) , target = ) : \n 
~~~ h << comp . render ( h , ) \n 
~~ @ presentation . render_for ( Asset , model = "anonymous" ) \n 
def render_asset_anonymous ( self , h , comp , model , * args ) : \n 
~~~ with h . a ( href = self . assets_manager . get_image_url ( self . filename ) , target = ) : \n 
~~~ h << comp . render ( h , model = "thumb" ) \n 
~~ @ presentation . render_for ( AssetCropper ) \n 
def render_gallery_cropper ( self , h , comp , * args ) : \n 
~~~ h << h . p ( _ ( ) ) \n 
form_id = h . generate_id ( ) \n 
img_id = h . generate_id ( ) \n 
~~~ for crop_name in , , , : \n 
~~~ h << h . input ( type = , id = form_id + + crop_name ) . action ( getattr ( self , crop_name ~~ h << h . p ( render_image ( self . asset , h , comp , , id = img_id ) ) \n 
h << h . script ( \n 
"YAHOO.util.Event.onContentReady(%s," \n 
ajax . py2js ( img_id ) , \n 
ajax . py2js ( form_id ) , \n 
ajax . py2js ( self . crop_width ( ) ) , \n 
ajax . py2js ( self . crop_height ( ) ) \n 
with h . div ( class_ = ) : \n 
~~~ h << h . button ( _ ( ) , class_ = ) . action ( self . commit , comp ) \n 
if self . asset . is_cover : \n 
~~~ h << \n 
h << h . button ( _ ( ) , class_ = ) . action ( self . remove_cover , comp ~~ h << \n 
h << h . button ( _ ( ) , class_ = ) . action ( self . cancel , comp ) \n 
~~ class EventHandlerMixIn ( object ) : \n 
def emit_event ( self , comp , kind , data = None ) : \n 
~~~ event = kind ( data , source = [ self ] ) \n 
return comp . answer ( event ) \n 
~~ def handle_event ( self , comp , event ) : \n 
~~~ local_res = None \n 
local_handler = getattr ( self , , None ) \n 
if local_handler : \n 
~~~ local_res = local_handler ( comp , event ) \n 
~~ event . append ( self ) \n 
upper_res = comp . answer ( event ) \n 
return local_res or upper_res \n 
~~ ~~ class Event ( object ) : \n 
def __init__ ( self , data , source = [ ] ) : \n 
self . _source = source \n 
self . data = data \n 
def source ( self ) : \n 
~~~ return self . _source . copy ( ) \n 
def emitter ( self ) : \n 
~~~ return self . _source [ 0 ] \n 
def last_relay ( self ) : \n 
~~~ return self . _source [ - 1 ] \n 
~~ def is_ ( self , kind ) : \n 
~~~ return type ( self ) is kind \n 
~~ def is_kind_of ( self , kind ) : \n 
~~~ return isinstance ( self , kind ) \n 
~~ def append ( self , relay ) : \n 
~~~ self . _source . append ( relay ) \n 
~~ def cast_as ( self , sub_kind ) : \n 
~~~ return sub_kind ( self . data , self . _source ) \n 
~~ ~~ class ColumnDeleted ( Event ) : \n 
~~ class CardClicked ( Event ) : \n 
~~ class PopinClosed ( Event ) : \n 
~~ class CardEditorClosed ( PopinClosed ) : \n 
~~ class CardArchived ( Event ) : \n 
~~ class SearchIndexUpdated ( Event ) : \n 
~~ class CardDisplayed ( Event ) : \n 
~~ class BoardAccessChanged ( Event ) : \n 
~~ class BoardDeleted ( BoardAccessChanged ) : \n 
~~ class BoardArchived ( BoardAccessChanged ) : \n 
~~ class BoardRestored ( BoardAccessChanged ) : \n 
~~ class BoardLeft ( BoardAccessChanged ) : \n 
~~ class ParentTitleNeeded ( Event ) : \n 
~~ class NewTemplateRequested ( Event ) : \n 
~~ from . comp import EditableTitle \n 
from . import view \n 
#!/usr/bin/python2.7 \n 
####################################################################################################################### \n 
import genie2 . client . wrapper \n 
import genie2 . model . ClusterCriteria \n 
import genie2 . model . Job \n 
import genie2 . model . FileAttachment \n 
genie2 . client . wrapper . RetryPolicy ( \n 
tries = 8 , none_on_404 = True , no_retry_http_codes = range ( 400 , 500 ) ) \n 
job = genie2 . model . Job . Job ( ) \n 
job . name = "GenieDockerExamplePigJob2" \n 
job . user = "root" \n 
job . version = "0.14.0" \n 
job . clusterCriterias = list ( ) \n 
cluster_criteria = genie2 . model . ClusterCriteria . ClusterCriteria ( ) \n 
criteria = set ( ) \n 
criteria . add ( "sched:adhoc" ) \n 
criteria . add ( "type:yarn" ) \n 
cluster_criteria . tags = criteria \n 
job . clusterCriterias . append ( cluster_criteria ) \n 
command_criteria = set ( ) \n 
command_criteria . add ( "type:pig" ) \n 
job . commandCriteria = command_criteria \n 
job . fileDependencies = "file:///apps/genie/pig/0.14.0/tutorial/script2-hadoop.pig,file:///apps/genie/pig/0.14.0/tutorial/tutorial.jar" \n 
job . commandArgs = "script2-hadoop.pig" \n 
job = genie . submitJob ( job ) \n 
while job . status != "SUCCEEDED" and job . status != "KILLED" and job . status != "FAILED" : \n 
time . sleep ( 10 ) \n 
job = genie . getJob ( job . id ) \n 
from os import environ \n 
from aminator . config import conf_action \n 
from aminator . plugins . finalizer . tagging_base import TaggingBaseFinalizerPlugin \n 
from aminator . util . linux import sanitize_metadata \n 
__all__ = ( , ) \n 
class TaggingEBSFinalizerPlugin ( TaggingBaseFinalizerPlugin ) : \n 
~~~ _name = \n 
def add_plugin_args ( self ) : \n 
~~~ tagging = super ( TaggingEBSFinalizerPlugin , self ) . add_plugin_args ( ) \n 
context = self . _config . context \n 
tagging . add_argument ( , , dest = , action = conf_action ( context . ami ) , help = \n 
~~ def _set_metadata ( self ) : \n 
~~~ super ( TaggingEBSFinalizerPlugin , self ) . _set_metadata ( ) \n 
config = self . _config . plugins [ self . full_name ] \n 
metadata = context . package . attributes \n 
ami_name = context . ami . get ( , None ) \n 
if not ami_name : \n 
~~~ ami_name = config . name_format . format ( ** metadata ) \n 
~~ context . ami . name = sanitize_metadata ( . format ( ami_name ) ) \n 
~~ def _snapshot_volume ( self ) : \n 
~~~ log . info ( ) \n 
if not self . _cloud . snapshot_volume ( ) : \n 
~~ log . info ( ) \n 
~~ def _register_image ( self , block_device_map = None , root_device = None ) : \n 
if block_device_map is None : \n 
~~~ block_device_map = config . default_block_device_map \n 
~~ if root_device is None : \n 
~~~ root_device = config . default_root_device \n 
~~ if not self . _cloud . register_image ( block_device_map , root_device ) : \n 
~~ def finalize ( self ) : \n 
self . _set_metadata ( ) \n 
if not self . _snapshot_volume ( ) : \n 
~~~ log . critical ( ) \n 
~~ if not self . _register_image ( ) : \n 
~~ if not self . _add_tags ( [ , ] ) : \n 
self . _log_ami_metadata ( ) \n 
~~~ context = self . _config . context \n 
environ [ "AMINATOR_STORE_TYPE" ] = "ebs" \n 
if context . ami . get ( "name" , None ) : \n 
~~~ environ [ "AMINATOR_AMI_NAME" ] = context . ami . name \n 
~~ return super ( TaggingEBSFinalizerPlugin , self ) . __enter__ ( ) \n 
~~ ~~ OFF = 0 \n 
ON = 1 \n 
DISCONNECTED = 20 \n 
CONNECTED = 30 \n 
DEFAULT_EVENT_VERSION = 1 \n 
LOG_LEVEL = "DEBUG" \n 
LOG_FILE = "/var/log/security_monkey/security_monkey-deploy.log" \n 
SQLALCHEMY_DATABASE_URI = \n 
SQLALCHEMY_POOL_SIZE = 50 \n 
SQLALCHEMY_MAX_OVERFLOW = 15 \n 
ENVIRONMENT = \n 
USE_ROUTE53 = False \n 
FQDN = \n 
API_PORT = \n 
WEB_PORT = \n 
WEB_PATH = \n 
FRONTED_BY_NGINX = True \n 
NGINX_PORT = \n 
BASE_URL = . format ( FQDN ) \n 
SECRET_KEY = \n 
MAIL_DEFAULT_SENDER = \n 
SECURITY_REGISTERABLE = True \n 
SECURITY_CONFIRMABLE = False \n 
SECURITY_RECOVERABLE = False \n 
SECURITY_PASSWORD_HASH = \n 
SECURITY_PASSWORD_SALT = \n 
SECURITY_TRACKABLE = True \n 
SECURITY_POST_LOGIN_VIEW = BASE_URL \n 
SECURITY_POST_REGISTER_VIEW = BASE_URL \n 
SECURITY_POST_CONFIRM_VIEW = BASE_URL \n 
SECURITY_POST_RESET_VIEW = BASE_URL \n 
SECURITY_POST_CHANGE_VIEW = BASE_URL \n 
SECURITY_TEAM_EMAIL = [ ] \n 
SES_REGION = \n 
MAIL_SERVER = \n 
MAIL_PORT = 465 \n 
MAIL_USE_SSL = True \n 
MAIL_USERNAME = \n 
MAIL_PASSWORD = \n 
WTF_CSRF_ENABLED = True \n 
WTF_CSRF_METHODS = [ , , , ] \n 
SECURITYGROUP_INSTANCE_DETAIL = \n 
CORE_THREADS = 25 \n 
MAX_THREADS = 30 \n 
from security_monkey . auditor import Auditor \n 
from security_monkey . watchers . rds_security_group import RDSSecurityGroup \n 
from security_monkey . datastore import NetworkWhitelistEntry \n 
from security_monkey . auditors . security_group import _check_rfc_1918 \n 
import ipaddr \n 
class RDSSecurityGroupAuditor ( Auditor ) : \n 
~~~ index = RDSSecurityGroup . index \n 
i_am_singular = RDSSecurityGroup . i_am_singular \n 
i_am_plural = RDSSecurityGroup . i_am_plural \n 
network_whitelist = [ ] \n 
def __init__ ( self , accounts = None , debug = False ) : \n 
~~~ super ( RDSSecurityGroupAuditor , self ) . __init__ ( accounts = accounts , debug = debug ) \n 
~~ def prep_for_audit ( self ) : \n 
~~~ self . network_whitelist = NetworkWhitelistEntry . query . all ( ) \n 
~~ def _check_inclusion_in_network_whitelist ( self , cidr ) : \n 
~~~ for entry in self . network_whitelist : \n 
~~~ if ipaddr . IPNetwork ( cidr ) in ipaddr . IPNetwork ( str ( entry . cidr ) ) : \n 
~~ def check_rds_ec2_rfc1918 ( self , sg_item ) : \n 
severity = 8 \n 
if sg_item . config . get ( "vpc_id" , None ) : \n 
~~ for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
~~~ cidr = ipr . get ( "cidr_ip" , None ) \n 
if cidr and _check_rfc_1918 ( cidr ) : \n 
~~~ self . add_issue ( severity , tag , sg_item , notes = cidr ) \n 
~~ ~~ ~~ def check_securitygroup_large_subnet ( self , sg_item ) : \n 
severity = 3 \n 
for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
if cidr and not self . _check_inclusion_in_network_whitelist ( cidr ) : \n 
~~~ if in cidr and not cidr == "0.0.0.0/0" and not cidr == "10.0.0.0/8" : \n 
~~~ mask = int ( cidr . split ( ) [ 1 ] ) \n 
if mask < 24 and mask > 0 : \n 
~~ ~~ ~~ ~~ ~~ def check_securitygroup_zero_subnet ( self , sg_item ) : \n 
severity = 10 \n 
if cidr and in cidr and not cidr == "0.0.0.0/0" and not cidr == "10.0.0.0/8" : \n 
if mask == 0 : \n 
~~ ~~ ~~ ~~ def check_securitygroup_any ( self , sg_item ) : \n 
severity = 5 \n 
~~~ cidr = ipr . get ( "cidr_ip" ) \n 
if "0.0.0.0/0" == cidr : \n 
~~ ~~ ~~ def check_securitygroup_10net ( self , sg_item ) : \n 
if "10.0.0.0/8" == cidr : \n 
from security_monkey . datastore import NetworkWhitelistEntry , Account \n 
from security_monkey . tests import SecurityMonkeyTestCase \n 
from security_monkey import db \n 
from security_monkey . watchers . elasticsearch_service import ElasticSearchServiceItem \n 
CONFIG_ONE = { \n 
"name" : "es_test" , \n 
CONFIG_TWO = { \n 
"name" : "es_test_2" , \n 
CONFIG_THREE = { \n 
"name" : "es_test_3" , \n 
CONFIG_FOUR = { \n 
"name" : "es_test_4" , \n 
CONFIG_FIVE = { \n 
"name" : "es_test_5" , \n 
CONFIG_SIX = { \n 
"name" : "es_test_6" , \n 
CONFIG_SEVEN = { \n 
"name" : "es_test_7" , \n 
CONFIG_EIGHT = { \n 
"name" : "es_test_8" , \n 
CONFIG_NINE = { \n 
"name" : "es_test_9" , \n 
WHITELIST_CIDRS = [ \n 
class ElasticSearchServiceTestCase ( SecurityMonkeyTestCase ) : \n 
~~~ self . es_items = [ \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test" , config ElasticSearchServiceItem ( region = "us-west-2" , account = "TEST_ACCOUNT" , name = "es_test_2" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_3" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_4" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_5" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_6" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_7" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_8" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_9" , config ] \n 
test_account = Account ( ) \n 
test_account . name = "TEST_ACCOUNT" \n 
test_account . s3_name = "TEST_ACCOUNT" \n 
test_account . number = "012345678910" \n 
test_account . role_name = "TEST_ACCOUNT" \n 
db . session . add ( test_account ) \n 
db . session . commit ( ) \n 
~~~ test_account = Account . query . filter ( Account . number == "012345678910" ) . first ( ) \n 
if test_account is not None : \n 
~~~ db . session . delete ( test_account ) \n 
~~ ~~ def test_es_auditor ( self ) : \n 
~~~ from security_monkey . auditors . elasticsearch_service import ElasticSearchServiceAuditor \n 
es_auditor = ElasticSearchServiceAuditor ( accounts = [ "012345678910" ] ) \n 
es_auditor . network_whitelist = [ ] \n 
for cidr in WHITELIST_CIDRS : \n 
~~~ whitelist_cidr = NetworkWhitelistEntry ( ) \n 
whitelist_cidr . cidr = cidr [ 1 ] \n 
whitelist_cidr . name = cidr [ 0 ] \n 
es_auditor . network_whitelist . append ( whitelist_cidr ) \n 
~~ for es_domain in self . es_items : \n 
~~~ es_auditor . check_es_access_policy ( es_domain ) \n 
~~ self . assertEquals ( len ( self . es_items [ 0 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 0 ] . audit_issues [ 0 ] . score , 20 ) \n 
self . assertEquals ( len ( self . es_items [ 1 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 1 ] . audit_issues [ 0 ] . score , 20 ) \n 
self . assertEquals ( len ( self . es_items [ 2 ] . audit_issues ) , 2 ) \n 
self . assertEquals ( self . es_items [ 2 ] . audit_issues [ 0 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 2 ] . audit_issues [ 1 ] . score , 7 ) \n 
self . assertEquals ( len ( self . es_items [ 3 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 3 ] . audit_issues [ 0 ] . score , 20 ) \n 
self . assertEquals ( len ( self . es_items [ 4 ] . audit_issues ) , 0 ) \n 
self . assertEquals ( len ( self . es_items [ 5 ] . audit_issues ) , 0 ) \n 
self . assertEquals ( len ( self . es_items [ 6 ] . audit_issues ) , 3 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 0 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 1 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 2 ] . score , 7 ) \n 
self . assertEquals ( len ( self . es_items [ 7 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 7 ] . audit_issues [ 0 ] . score , 20 ) \n 
self . assertEquals ( len ( self . es_items [ 8 ] . audit_issues ) , 2 ) \n 
self . assertEquals ( self . es_items [ 8 ] . audit_issues [ 0 ] . score , 6 ) \n 
self . assertEquals ( self . es_items [ 8 ] . audit_issues [ 1 ] . score , 10 ) \n 
from security_monkey . watcher import Watcher \n 
from security_monkey . watcher import ChangeItem \n 
from security_monkey . constants import TROUBLE_REGIONS \n 
from security_monkey . exceptions import BotoConnectionIssue \n 
from security_monkey import app \n 
from boto . redshift import regions \n 
class Redshift ( Watcher ) : \n 
~~~ index = \n 
i_am_singular = \n 
i_am_plural = \n 
~~~ super ( Redshift , self ) . __init__ ( accounts = accounts , debug = debug ) \n 
~~ def slurp ( self ) : \n 
self . prep_for_slurp ( ) \n 
from security_monkey . common . sts_connect import connect \n 
item_list = [ ] \n 
exception_map = { } \n 
for account in self . accounts : \n 
~~~ for region in regions ( ) : \n 
~~~ redshift = connect ( account , , region = region ) \n 
all_clusters = [ ] \n 
marker = None \n 
~~~ response = self . wrap_aws_rate_limited_call ( \n 
redshift . describe_clusters , \n 
marker = marker \n 
all_clusters . extend ( response [ ] [ if response [ ] [ ] [ ] ~~~ marker = response [ ] [ ] [ ~~ else : \n 
~~~ if region . name not in TROUBLE_REGIONS : \n 
~~~ exc = BotoConnectionIssue ( str ( e ) , , account , region . name ) \n 
self . slurp_exception ( ( self . index , account , region . name ) , exc , exception_map ) ~~ continue \n 
for cluster in all_clusters : \n 
~~~ cluster_id = cluster [ ] \n 
if self . check_ignore_list ( cluster_id ) : \n 
~~ item = RedshiftCluster ( region = region . name , account = account , name = cluster_id , config item_list . append ( item ) \n 
~~ ~~ ~~ return item_list , exception_map \n 
~~ ~~ class RedshiftCluster ( ChangeItem ) : \n 
~~~ def __init__ ( self , region = None , account = None , name = None , config = { } ) : \n 
~~~ super ( RedshiftCluster , self ) . __init__ ( \n 
index = Redshift . index , \n 
region = region , \n 
account = account , \n 
new_config = config ) \n 
from __future__ import absolute_import , division , print_function \n 
from neo . core . container import Container \n 
class RecordingChannelGroup ( Container ) : \n 
_container_child_objects = ( , ) \n 
_data_child_objects = ( , ) \n 
_multi_child_objects = ( , ) \n 
_single_parent_objects = ( , ) \n 
_recommended_attrs = ( ( ( , np . ndarray , 1 , np . dtype ( ) ) , \n 
( , np . ndarray , 1 , np . dtype ( ) ) ) + \n 
Container . _recommended_attrs ) \n 
def __init__ ( self , channel_names = None , channel_indexes = None , name = None , \n 
description = None , file_origin = None , ** annotations ) : \n 
super ( RecordingChannelGroup , self ) . __init__ ( name = name , \n 
description = description , \n 
file_origin = file_origin , \n 
** annotations ) \n 
if channel_indexes is None : \n 
~~~ channel_indexes = np . array ( [ ] , dtype = np . int ) \n 
~~ if channel_names is None : \n 
~~~ channel_names = np . array ( [ ] , dtype = ) \n 
~~ self . channel_names = channel_names \n 
self . channel_indexes = channel_indexes \n 
import ctypes \n 
~~~ file \n 
~~~ import io \n 
file = io . BufferedReader \n 
import quantities as pq \n 
from neo . io . baseio import BaseIO \n 
from neo . core import Segment , AnalogSignal , SpikeTrain , EventArray \n 
class NeuroshareError ( Exception ) : \n 
~~~ def __init__ ( self , lib , errno ) : \n 
~~~ self . lib = lib \n 
self . errno = errno \n 
pszMsgBuffer = ctypes . create_string_buffer ( 256 ) \n 
self . lib . ns_GetLastErrorMsg ( pszMsgBuffer , ctypes . c_uint32 ( 256 ) ) \n 
errstr = . format ( errno , pszMsgBuffer . value ) \n 
Exception . __init__ ( self , errstr ) \n 
~~ ~~ class DllWithError ( ) : \n 
~~~ def __init__ ( self , lib ) : \n 
~~ def __getattr__ ( self , attr ) : \n 
~~~ f = getattr ( self . lib , attr ) \n 
return self . decorate_with_error ( f ) \n 
~~ def decorate_with_error ( self , f ) : \n 
~~~ def func_with_error ( * args ) : \n 
~~~ errno = f ( * args ) \n 
if errno != ns_OK : \n 
~~~ raise NeuroshareError ( self . lib , errno ) \n 
~~ return errno \n 
~~ return func_with_error \n 
~~ ~~ class NeurosharectypesIO ( BaseIO ) : \n 
is_readable = True \n 
is_writable = False \n 
supported_objects = [ Segment , AnalogSignal , EventArray , SpikeTrain ] \n 
readable_objects = [ Segment ] \n 
writeable_objects = [ ] \n 
has_header = False \n 
is_streameable = False \n 
read_params = { Segment : [ ] } \n 
write_params = None \n 
extensions = [ ] \n 
mode = \n 
def __init__ ( self , filename = , dllname = ) : \n 
BaseIO . __init__ ( self ) \n 
self . dllname = dllname \n 
self . filename = filename \n 
~~ def read_segment ( self , import_neuroshare_segment = True , \n 
lazy = False , cascade = True ) : \n 
seg = Segment ( file_origin = os . path . basename ( self . filename ) , ) \n 
if sys . platform . startswith ( ) : \n 
~~~ neuroshare = ctypes . windll . LoadLibrary ( self . dllname ) \n 
~~ elif sys . platform . startswith ( ) : \n 
~~~ neuroshare = ctypes . cdll . LoadLibrary ( self . dllname ) \n 
~~ neuroshare = DllWithError ( neuroshare ) \n 
info = ns_LIBRARYINFO ( ) \n 
neuroshare . ns_GetLibraryInfo ( ctypes . byref ( info ) , ctypes . sizeof ( info ) ) \n 
seg . annotate ( neuroshare_version = str ( info . dwAPIVersionMaj ) + + str ( info . dwAPIVersionMin ) ) \n 
if not cascade : \n 
~~~ return seg \n 
~~ hFile = ctypes . c_uint32 ( 0 ) \n 
neuroshare . ns_OpenFile ( ctypes . c_char_p ( self . filename ) , ctypes . byref ( hFile ) ) \n 
fileinfo = ns_FILEINFO ( ) \n 
neuroshare . ns_GetFileInfo ( hFile , ctypes . byref ( fileinfo ) , ctypes . sizeof ( fileinfo ) ) \n 
for dwEntityID in range ( fileinfo . dwEntityCount ) : \n 
~~~ entityInfo = ns_ENTITYINFO ( ) \n 
neuroshare . ns_GetEntityInfo ( hFile , dwEntityID , ctypes . byref ( entityInfo ) , ctypes . sizeof ( \n 
if entity_types [ entityInfo . dwEntityType ] == : \n 
~~~ pEventInfo = ns_EVENTINFO ( ) \n 
neuroshare . ns_GetEventInfo ( hFile , dwEntityID , ctypes . byref ( pEventInfo ) , ctypes . sizeof \n 
if pEventInfo . dwEventType == 0 : #TEXT \n 
~~~ pData = ctypes . create_string_buffer ( pEventInfo . dwMaxDataLength ) \n 
~~ elif pEventInfo . dwEventType == 1 : #CVS \n 
~~~ pData = ctypes . c_byte ( 0 ) \n 
~~~ pData = ctypes . c_int16 ( 0 ) \n 
~~~ pData = ctypes . c_int32 ( 0 ) \n 
~~ pdTimeStamp = ctypes . c_double ( 0. ) \n 
pdwDataRetSize = ctypes . c_uint32 ( 0 ) \n 
ea = EventArray ( name = str ( entityInfo . szEntityLabel ) , ) \n 
if not lazy : \n 
~~~ times = [ ] \n 
labels = [ ] \n 
for dwIndex in range ( entityInfo . dwItemCount ) : \n 
~~~ neuroshare . ns_GetEventData ( hFile , dwEntityID , dwIndex , \n 
ctypes . byref ( pdTimeStamp ) , ctypes . byref ( pData ) , \n 
ctypes . sizeof ( pData ) , ctypes . byref ( pdwDataRetSize ) ) \n 
times . append ( pdTimeStamp . value ) \n 
labels . append ( str ( pData . value ) ) \n 
~~ ea . times = times * pq . s \n 
ea . labels = np . array ( labels , dtype = ) \n 
~~~ ea . lazy_shape = entityInfo . dwItemCount \n 
~~ seg . eventarrays . append ( ea ) \n 
~~ if entity_types [ entityInfo . dwEntityType ] == : \n 
~~~ pAnalogInfo = ns_ANALOGINFO ( ) \n 
neuroshare . ns_GetAnalogInfo ( hFile , dwEntityID , ctypes . byref ( pAnalogInfo ) , ctypes . sizeof dwIndexCount = entityInfo . dwItemCount \n 
if lazy : \n 
~~~ signal = [ ] * pq . Quantity ( 1 , pAnalogInfo . szUnits ) \n 
~~~ pdwContCount = ctypes . c_uint32 ( 0 ) \n 
pData = np . zeros ( ( entityInfo . dwItemCount , ) , dtype = ) \n 
total_read = 0 \n 
while total_read < entityInfo . dwItemCount : \n 
~~~ dwStartIndex = ctypes . c_uint32 ( total_read ) \n 
dwStopIndex = ctypes . c_uint32 ( entityInfo . dwItemCount - total_read ) \n 
neuroshare . ns_GetAnalogData ( hFile , dwEntityID , dwStartIndex , \n 
dwStopIndex , ctypes . byref ( pdwContCount ) , pData [ total_read : ] . ctypes total_read += pdwContCount . value \n 
~~ signal = pq . Quantity ( pData , units = pAnalogInfo . szUnits , copy = False ) \n 
#t_start \n 
~~ dwIndex = 0 \n 
pdTime = ctypes . c_double ( 0 ) \n 
neuroshare . ns_GetTimeByIndex ( hFile , dwEntityID , dwIndex , ctypes . byref ( pdTime ) ) \n 
anaSig = AnalogSignal ( signal , \n 
sampling_rate = pAnalogInfo . dSampleRate * pq . Hz , \n 
t_start = pdTime . value * pq . s , \n 
name = str ( entityInfo . szEntityLabel ) , \n 
anaSig . annotate ( probe_info = str ( pAnalogInfo . szProbeInfo ) ) \n 
~~~ anaSig . lazy_shape = entityInfo . dwItemCount \n 
~~ seg . analogsignals . append ( anaSig ) \n 
#segment \n 
~~ if entity_types [ entityInfo . dwEntityType ] == and import_neuroshare_segment \n 
~~~ pdwSegmentInfo = ns_SEGMENTINFO ( ) \n 
if not str ( entityInfo . szEntityLabel ) . startswith ( ) : \n 
~~ neuroshare . ns_GetSegmentInfo ( hFile , dwEntityID , \n 
ctypes . byref ( pdwSegmentInfo ) , ctypes . sizeof ( pdwSegmentInfo nsource = pdwSegmentInfo . dwSourceCount \n 
neuroshare . ns_GetLastErrorMsg ( ctypes . byref ( pszMsgBuffer ) , 256 ) \n 
for dwSourceID in range ( pdwSegmentInfo . dwSourceCount ) : \n 
~~~ pSourceInfo = ns_SEGSOURCEINFO ( ) \n 
neuroshare . ns_GetSegmentSourceInfo ( hFile , dwEntityID , dwSourceID , \n 
ctypes . byref ( pSourceInfo ) , ctypes . sizeof ( pSourceInfo ) ) \n 
~~ if lazy : \n 
~~~ sptr = SpikeTrain ( times , name = str ( entityInfo . szEntityLabel ) , t_stop = 0. * pq . s ) sptr . lazy_shape = entityInfo . dwItemCount \n 
~~~ pdTimeStamp = ctypes . c_double ( 0. ) \n 
dwDataBufferSize = pdwSegmentInfo . dwMaxSampleCount * pdwSegmentInfo . dwSourceCount \n 
pData = np . zeros ( ( dwDataBufferSize ) , dtype = ) \n 
pdwSampleCount = ctypes . c_uint32 ( 0 ) \n 
pdwUnitID = ctypes . c_uint32 ( 0 ) \n 
nsample = int ( dwDataBufferSize ) \n 
times = np . empty ( ( entityInfo . dwItemCount ) , dtype = ) \n 
waveforms = np . empty ( ( entityInfo . dwItemCount , nsource , nsample ) , dtype = ) \n 
~~~ neuroshare . ns_GetSegmentData ( hFile , dwEntityID , dwIndex , \n 
ctypes . byref ( pdTimeStamp ) , pData . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double dwDataBufferSize * 8 , ctypes . byref ( pdwSampleCount ) , \n 
ctypes . byref ( pdwUnitID ) ) \n 
times [ dwIndex ] = pdTimeStamp . value \n 
waveforms [ dwIndex , : , : ] = pData [ : nsample * nsource ] . reshape ( nsample , nsource ) . \n 
~~ sptr = SpikeTrain ( times = pq . Quantity ( times , units = , copy = False ) , \n 
t_stop = times . max ( ) , \n 
waveforms = pq . Quantity ( waveforms , units = str ( pdwSegmentInfo left_sweep = nsample / 2. / float ( pdwSegmentInfo . dSampleRate ) * pq sampling_rate = float ( pdwSegmentInfo . dSampleRate ) * pq . Hz , \n 
~~ seg . spiketrains . append ( sptr ) \n 
~~~ pNeuralInfo = ns_NEURALINFO ( ) \n 
neuroshare . ns_GetNeuralInfo ( hFile , dwEntityID , \n 
ctypes . byref ( pNeuralInfo ) , ctypes . sizeof ( pNeuralInfo ) ) \n 
~~~ times = [ ] * pq . s \n 
t_stop = 0 * pq . s \n 
~~~ pData = np . zeros ( ( entityInfo . dwItemCount , ) , dtype = ) \n 
dwStartIndex = 0 \n 
dwIndexCount = entityInfo . dwItemCount \n 
neuroshare . ns_GetNeuralData ( hFile , dwEntityID , dwStartIndex , \n 
dwIndexCount , pData . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) ) \n 
times = pData * pq . s \n 
t_stop = times . max ( ) \n 
~~ sptr = SpikeTrain ( times , t_stop = t_stop , \n 
name = str ( entityInfo . szEntityLabel ) , ) \n 
~~~ sptr . lazy_shape = entityInfo . dwItemCount \n 
~~ ~~ neuroshare . ns_CloseFile ( hFile ) \n 
seg . create_many_to_one_relationship ( ) \n 
return seg \n 
~~ ~~ class ns_FILEDESC ( ctypes . Structure ) : \n 
~~~ _fields_ = [ ( , ctypes . c_char * 32 ) , \n 
( , ctypes . c_char * 8 ) , \n 
( , ctypes . c_char * 16 ) , \n 
~~ class ns_LIBRARYINFO ( ctypes . Structure ) : \n 
~~~ _fields_ = [ ( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_char * 64 ) , \n 
( , ns_FILEDESC * 16 ) , \n 
~~ class ns_FILEINFO ( ctypes . Structure ) : \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_char * 256 ) , \n 
~~ class ns_ENTITYINFO ( ctypes . Structure ) : \n 
~~ entity_types = { 0 : , \n 
1 : , \n 
2 : , \n 
3 : , \n 
4 : , \n 
class ns_EVENTINFO ( ctypes . Structure ) : \n 
~~~ _fields_ = [ \n 
( , ctypes . c_char * 128 ) , \n 
~~ class ns_ANALOGINFO ( ctypes . Structure ) : \n 
~~ class ns_SEGMENTINFO ( ctypes . Structure ) : \n 
( , ctypes . c_char * 32 ) , \n 
~~ class ns_SEGSOURCEINFO ( ctypes . Structure ) : \n 
~~ class ns_NEURALINFO ( ctypes . Structure ) : \n 
~~~ import unittest2 as unittest \n 
~~~ import unittest \n 
~~~ from IPython . lib . pretty import pretty \n 
~~ except ImportError as err : \n 
~~~ HAVE_IPYTHON = False \n 
~~~ HAVE_IPYTHON = True \n 
~~ from neo . core . segment import Segment \n 
from neo . core import ( AnalogSignalArray , Block , \n 
Epoch , EpochArray , \n 
RecordingChannelGroup , SpikeTrain , Unit ) \n 
from neo . core . container import filterdata \n 
from neo . test . tools import ( assert_neo_object_is_compliant , \n 
assert_same_sub_schema ) \n 
from neo . test . generate_datasets import ( fake_neo , get_fake_value , \n 
get_fake_values , get_annotations , \n 
clone_object , TEST_ANNOTATIONS ) \n 
class Test__generate_datasets ( unittest . TestCase ) : \n 
~~~ np . random . seed ( 0 ) \n 
self . annotations = dict ( [ ( str ( x ) , TEST_ANNOTATIONS [ x ] ) for x in \n 
range ( len ( TEST_ANNOTATIONS ) ) ] ) \n 
~~ def test__get_fake_values ( self ) : \n 
~~~ self . annotations [ ] = 0 \n 
file_datetime = get_fake_value ( , datetime , seed = 0 ) \n 
rec_datetime = get_fake_value ( , datetime , seed = 1 ) \n 
index = get_fake_value ( , int , seed = 2 ) \n 
name = get_fake_value ( , str , seed = 3 , obj = Segment ) \n 
description = get_fake_value ( , str , seed = 4 , obj = ) \n 
file_origin = get_fake_value ( , str ) \n 
attrs1 = { : file_datetime , \n 
: rec_datetime , \n 
: index , \n 
: name , \n 
: description , \n 
: file_origin } \n 
attrs2 = attrs1 . copy ( ) \n 
attrs2 . update ( self . annotations ) \n 
res11 = get_fake_values ( Segment , annotate = False , seed = 0 ) \n 
res12 = get_fake_values ( , annotate = False , seed = 0 ) \n 
res21 = get_fake_values ( Segment , annotate = True , seed = 0 ) \n 
res22 = get_fake_values ( , annotate = True , seed = 0 ) \n 
self . assertEqual ( res11 , attrs1 ) \n 
self . assertEqual ( res12 , attrs1 ) \n 
self . assertEqual ( res21 , attrs2 ) \n 
self . assertEqual ( res22 , attrs2 ) \n 
~~ def test__fake_neo__cascade ( self ) : \n 
~~~ self . annotations [ ] = None \n 
obj_type = Segment \n 
cascade = True \n 
res = fake_neo ( obj_type = obj_type , cascade = cascade ) \n 
self . assertTrue ( isinstance ( res , Segment ) ) \n 
assert_neo_object_is_compliant ( res ) \n 
self . assertEqual ( res . annotations , self . annotations ) \n 
self . assertEqual ( len ( res . analogsignalarrays ) , 1 ) \n 
self . assertEqual ( len ( res . analogsignals ) , 1 ) \n 
self . assertEqual ( len ( res . irregularlysampledsignals ) , 1 ) \n 
self . assertEqual ( len ( res . spiketrains ) , 1 ) \n 
self . assertEqual ( len ( res . spikes ) , 1 ) \n 
self . assertEqual ( len ( res . events ) , 1 ) \n 
self . assertEqual ( len ( res . epochs ) , 1 ) \n 
self . assertEqual ( len ( res . eventarrays ) , 1 ) \n 
self . assertEqual ( len ( res . epocharrays ) , 1 ) \n 
for child in res . children : \n 
~~~ del child . annotations [ ] \n 
del child . annotations [ ] \n 
~~ self . assertEqual ( res . analogsignalarrays [ 0 ] . annotations , \n 
self . annotations ) \n 
self . assertEqual ( res . analogsignals [ 0 ] . annotations , \n 
self . assertEqual ( res . irregularlysampledsignals [ 0 ] . annotations , \n 
self . assertEqual ( res . spiketrains [ 0 ] . annotations , \n 
self . assertEqual ( res . spikes [ 0 ] . annotations , \n 
self . assertEqual ( res . events [ 0 ] . annotations , \n 
self . assertEqual ( res . epochs [ 0 ] . annotations , \n 
self . assertEqual ( res . eventarrays [ 0 ] . annotations , \n 
self . assertEqual ( res . epocharrays [ 0 ] . annotations , \n 
~~ def test__fake_neo__nocascade ( self ) : \n 
cascade = False \n 
self . assertEqual ( len ( res . analogsignalarrays ) , 0 ) \n 
self . assertEqual ( len ( res . analogsignals ) , 0 ) \n 
self . assertEqual ( len ( res . irregularlysampledsignals ) , 0 ) \n 
self . assertEqual ( len ( res . spiketrains ) , 0 ) \n 
self . assertEqual ( len ( res . spikes ) , 0 ) \n 
self . assertEqual ( len ( res . events ) , 0 ) \n 
self . assertEqual ( len ( res . epochs ) , 0 ) \n 
self . assertEqual ( len ( res . eventarrays ) , 0 ) \n 
self . assertEqual ( len ( res . epocharrays ) , 0 ) \n 
~~ ~~ class TestSegment ( unittest . TestCase ) : \n 
~~~ self . nchildren = 2 \n 
blk = fake_neo ( Block , seed = 0 , n = self . nchildren ) \n 
self . unit1 , self . unit2 , self . unit3 , self . unit4 = blk . list_units \n 
self . seg1 , self . seg2 = blk . segments \n 
self . targobj = self . seg1 \n 
self . seed1 = self . seg1 . annotations [ ] \n 
self . seed2 = self . seg2 . annotations [ ] \n 
del self . seg1 . annotations [ ] \n 
del self . seg2 . annotations [ ] \n 
self . sigs1 = self . seg1 . analogsignals \n 
self . sigs2 = self . seg2 . analogsignals \n 
self . sigarrs1 = self . seg1 . analogsignalarrays \n 
self . sigarrs2 = self . seg2 . analogsignalarrays \n 
self . irsigs1 = self . seg1 . irregularlysampledsignals \n 
self . irsigs2 = self . seg2 . irregularlysampledsignals \n 
self . spikes1 = self . seg1 . spikes \n 
self . spikes2 = self . seg2 . spikes \n 
self . trains1 = self . seg1 . spiketrains \n 
self . trains2 = self . seg2 . spiketrains \n 
self . epcs1 = self . seg1 . epochs \n 
self . epcs2 = self . seg2 . epochs \n 
self . epcas1 = self . seg1 . epocharrays \n 
self . epcas2 = self . seg2 . epocharrays \n 
self . evts1 = self . seg1 . events \n 
self . evts2 = self . seg2 . events \n 
self . evtas1 = self . seg1 . eventarrays \n 
self . evtas2 = self . seg2 . eventarrays \n 
self . sigs1a = clone_object ( self . sigs1 ) \n 
self . sigarrs1a = clone_object ( self . sigarrs1 , n = 2 ) \n 
self . irsigs1a = clone_object ( self . irsigs1 ) \n 
self . spikes1a = clone_object ( self . spikes1 ) \n 
self . trains1a = clone_object ( self . trains1 ) \n 
self . epcs1a = clone_object ( self . epcs1 ) \n 
self . epcas1a = clone_object ( self . epcas1 ) \n 
self . evts1a = clone_object ( self . evts1 ) \n 
self . evtas1a = clone_object ( self . evtas1 ) \n 
for obj , obja in zip ( self . sigs1 + self . sigarrs1 , \n 
self . sigs1a + self . sigarrs1a ) : \n 
~~~ obja . channel_index = obj . channel_index \n 
~~ ~~ def test_init ( self ) : \n 
~~~ seg = Segment ( name = , index = 3 ) \n 
assert_neo_object_is_compliant ( seg ) \n 
self . assertEqual ( seg . name , ) \n 
self . assertEqual ( seg . file_origin , None ) \n 
self . assertEqual ( seg . index , 3 ) \n 
~~ def check_creation ( self , seg ) : \n 
~~~ assert_neo_object_is_compliant ( seg ) \n 
seed = seg . annotations [ ] \n 
targ0 = get_fake_value ( , datetime , seed = seed + 0 ) \n 
self . assertEqual ( seg . file_datetime , targ0 ) \n 
targ1 = get_fake_value ( , datetime , seed = seed + 1 ) \n 
self . assertEqual ( seg . rec_datetime , targ1 ) \n 
targ2 = get_fake_value ( , int , seed = seed + 2 ) \n 
self . assertEqual ( seg . index , targ2 ) \n 
targ3 = get_fake_value ( , str , seed = seed + 3 , obj = Segment ) \n 
self . assertEqual ( seg . name , targ3 ) \n 
targ4 = get_fake_value ( , str , \n 
seed = seed + 4 , obj = Segment ) \n 
self . assertEqual ( seg . description , targ4 ) \n 
targ5 = get_fake_value ( , str ) \n 
self . assertEqual ( seg . file_origin , targ5 ) \n 
targ6 = get_annotations ( ) \n 
targ6 [ ] = seed \n 
self . assertEqual ( seg . annotations , targ6 ) \n 
self . assertTrue ( hasattr ( seg , ) ) \n 
self . assertEqual ( len ( seg . analogsignals ) , self . nchildren ** 2 ) \n 
self . assertEqual ( len ( seg . analogsignalarrays ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . irregularlysampledsignals ) , self . nchildren ** 2 ) \n 
self . assertEqual ( len ( seg . epochs ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . epocharrays ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . events ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . eventarrays ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . spikes ) , self . nchildren ** 2 ) \n 
self . assertEqual ( len ( seg . spiketrains ) , self . nchildren ** 2 ) \n 
~~ def test__creation ( self ) : \n 
~~~ self . check_creation ( self . seg1 ) \n 
self . check_creation ( self . seg2 ) \n 
~~ def test__merge ( self ) : \n 
~~~ seg1a = fake_neo ( Block , seed = self . seed1 , n = self . nchildren ) . segments [ 0 ] \n 
assert_same_sub_schema ( self . seg1 , seg1a ) \n 
seg1a . spikes . append ( self . spikes2 [ 0 ] ) \n 
seg1a . epocharrays . append ( self . epcas2 [ 0 ] ) \n 
seg1a . annotate ( seed = self . seed2 ) \n 
seg1a . merge ( self . seg2 ) \n 
assert_same_sub_schema ( self . sigs1a + self . sigs2 , seg1a . analogsignals ) \n 
assert_same_sub_schema ( self . sigarrs1a + self . sigarrs2 , \n 
seg1a . analogsignalarrays ) \n 
assert_same_sub_schema ( self . irsigs1a + self . irsigs2 , \n 
seg1a . irregularlysampledsignals ) \n 
assert_same_sub_schema ( self . epcs1 + self . epcs2 , seg1a . epochs ) \n 
assert_same_sub_schema ( self . epcas1 + self . epcas2 , seg1a . epocharrays ) \n 
assert_same_sub_schema ( self . evts1 + self . evts2 , seg1a . events ) \n 
assert_same_sub_schema ( self . evtas1 + self . evtas2 , seg1a . eventarrays ) \n 
assert_same_sub_schema ( self . spikes1 + self . spikes2 , seg1a . spikes ) \n 
assert_same_sub_schema ( self . trains1 + self . trains2 , seg1a . spiketrains ) \n 
~~ def test__children ( self ) : \n 
~~~ blk = Block ( name = ) \n 
blk . segments = [ self . seg1 ] \n 
blk . create_many_to_one_relationship ( force = True ) \n 
assert_neo_object_is_compliant ( self . seg1 ) \n 
assert_neo_object_is_compliant ( blk ) \n 
childobjs = ( , , \n 
childconts = ( , , \n 
self . assertEqual ( self . seg1 . _container_child_objects , ( ) ) \n 
self . assertEqual ( self . seg1 . _data_child_objects , childobjs ) \n 
self . assertEqual ( self . seg1 . _single_parent_objects , ( , ) ) \n 
self . assertEqual ( self . seg1 . _multi_child_objects , ( ) ) \n 
self . assertEqual ( self . seg1 . _multi_parent_objects , ( ) ) \n 
self . assertEqual ( self . seg1 . _child_properties , ( ) ) \n 
self . assertEqual ( self . seg1 . _single_child_objects , childobjs ) \n 
self . assertEqual ( self . seg1 . _container_child_containers , ( ) ) \n 
self . assertEqual ( self . seg1 . _data_child_containers , childconts ) \n 
self . assertEqual ( self . seg1 . _single_child_containers , childconts ) \n 
self . assertEqual ( self . seg1 . _single_parent_containers , ( , ) ) \n 
self . assertEqual ( self . seg1 . _multi_child_containers , ( ) ) \n 
self . assertEqual ( self . seg1 . _multi_parent_containers , ( ) ) \n 
self . assertEqual ( self . seg1 . _child_objects , childobjs ) \n 
self . assertEqual ( self . seg1 . _child_containers , childconts ) \n 
self . assertEqual ( self . seg1 . _parent_objects , ( , ) ) \n 
self . assertEqual ( self . seg1 . _parent_containers , ( , ) ) \n 
self . assertEqual ( len ( self . seg1 . _single_children ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . data_children ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . children ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . data_children_recur ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . children_recur ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . _multi_children ) , 0 ) \n 
self . assertEqual ( len ( self . seg1 . container_children ) , 0 ) \n 
self . assertEqual ( len ( self . seg1 . container_children_recur ) , 0 ) \n 
children = ( self . sigs1a + self . sigarrs1a + \n 
self . epcs1a + self . epcas1a + \n 
self . evts1a + self . evtas1a + \n 
self . irsigs1a + \n 
self . spikes1a + self . trains1a ) \n 
assert_same_sub_schema ( list ( self . seg1 . _single_children ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . data_children ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . data_children_recur ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . children ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . children_recur ) , children ) \n 
self . assertEqual ( len ( self . seg1 . parents ) , 1 ) \n 
self . assertEqual ( self . seg1 . parents [ 0 ] . name , ) \n 
~~ def test__size ( self ) : \n 
~~~ targ1 = { "epochs" : self . nchildren , "events" : self . nchildren , \n 
"analogsignals" : self . nchildren ** 2 , \n 
"irregularlysampledsignals" : self . nchildren ** 2 , \n 
"spikes" : self . nchildren ** 2 , \n 
"spiketrains" : self . nchildren ** 2 , \n 
"epocharrays" : self . nchildren , "eventarrays" : self . nchildren , \n 
"analogsignalarrays" : self . nchildren } \n 
self . assertEqual ( self . targobj . size , targ1 ) \n 
~~ def test__filter_none ( self ) : \n 
~~~ targ = [ ] \n 
res0 = self . targobj . filter ( ) \n 
res1 = self . targobj . filter ( { } ) \n 
res2 = self . targobj . filter ( [ ] ) \n 
res3 = self . targobj . filter ( [ { } ] ) \n 
res4 = self . targobj . filter ( [ { } , { } ] ) \n 
res5 = self . targobj . filter ( [ { } , { } ] ) \n 
res6 = self . targobj . filter ( targdict = { } ) \n 
res7 = self . targobj . filter ( targdict = [ ] ) \n 
res8 = self . targobj . filter ( targdict = [ { } ] ) \n 
res9 = self . targobj . filter ( targdict = [ { } , { } ] ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
assert_same_sub_schema ( res1 , targ ) \n 
assert_same_sub_schema ( res2 , targ ) \n 
assert_same_sub_schema ( res3 , targ ) \n 
assert_same_sub_schema ( res4 , targ ) \n 
assert_same_sub_schema ( res5 , targ ) \n 
assert_same_sub_schema ( res6 , targ ) \n 
assert_same_sub_schema ( res7 , targ ) \n 
assert_same_sub_schema ( res8 , targ ) \n 
assert_same_sub_schema ( res9 , targ ) \n 
~~ def test__filter_annotation_single ( self ) : \n 
~~~ targ = ( self . sigs1a + self . sigarrs1a + \n 
[ self . epcs1a [ 0 ] , self . epcas1a [ 0 ] ] + \n 
[ self . evts1a [ 0 ] , self . evtas1a [ 0 ] ] + \n 
res0 = self . targobj . filter ( j = 0 ) \n 
res1 = self . targobj . filter ( { : 0 } ) \n 
res2 = self . targobj . filter ( targdict = { : 0 } ) \n 
res3 = self . targobj . filter ( [ { : 0 } ] ) \n 
res4 = self . targobj . filter ( targdict = [ { : 0 } ] ) \n 
~~ def test__filter_single_annotation_nores ( self ) : \n 
res0 = self . targobj . filter ( j = 5 ) \n 
res1 = self . targobj . filter ( { : 5 } ) \n 
res2 = self . targobj . filter ( targdict = { : 5 } ) \n 
res3 = self . targobj . filter ( [ { : 5 } ] ) \n 
res4 = self . targobj . filter ( targdict = [ { : 5 } ] ) \n 
~~ def test__filter_attribute_single ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] ] \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name ) \n 
res1 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name } ) \n 
res2 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name } ) \n 
~~ def test__filter_attribute_single_nores ( self ) : \n 
res0 = self . targobj . filter ( name = self . epcs2 [ 0 ] . name ) \n 
res1 = self . targobj . filter ( { : self . epcs2 [ 0 ] . name } ) \n 
res2 = self . targobj . filter ( targdict = { : self . epcs2 [ 0 ] . name } ) \n 
~~ def test__filter_multi ( self ) : \n 
self . spikes1a + self . trains1a + \n 
[ self . epcs1a [ 1 ] ] ) \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , j = 0 ) \n 
res1 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name , : 0 } ) \n 
res2 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name , \n 
: 0 } ) \n 
~~ def test__filter_multi_nores ( self ) : \n 
res0 = self . targobj . filter ( [ { : 0 } , { } ] ) \n 
res1 = self . targobj . filter ( { } , ttype = 0 ) \n 
res2 = self . targobj . filter ( [ { } ] , ttype = 0 ) \n 
res3 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name } , j = 0 ) \n 
res4 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name } , \n 
j = 0 ) \n 
res5 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
targdict = { : 0 } ) \n 
res6 = self . targobj . filter ( name = self . epcs2 [ 0 ] . name , j = 5 ) \n 
res7 = self . targobj . filter ( { : self . epcs2 [ 1 ] . name , : 5 } ) \n 
res8 = self . targobj . filter ( targdict = { : self . epcs2 [ 1 ] . name , \n 
: 5 } ) \n 
res9 = self . targobj . filter ( { : self . epcs2 [ 1 ] . name } , j = 5 ) \n 
res10 = self . targobj . filter ( targdict = { : self . epcs2 [ 1 ] . name } , \n 
j = 5 ) \n 
res11 = self . targobj . filter ( name = self . epcs2 [ 1 ] . name , \n 
targdict = { : 5 } ) \n 
res12 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name } , j = 5 ) \n 
res13 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name } , \n 
res14 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
assert_same_sub_schema ( res10 , targ ) \n 
assert_same_sub_schema ( res11 , targ ) \n 
assert_same_sub_schema ( res12 , targ ) \n 
assert_same_sub_schema ( res13 , targ ) \n 
assert_same_sub_schema ( res14 , targ ) \n 
~~ def test__filter_multi_partres ( self ) : \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , j = 5 ) \n 
res1 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name , : 5 } ) \n 
res3 = self . targobj . filter ( [ { : 1 } , { : 2 } ] ) \n 
res4 = self . targobj . filter ( { : 1 } , i = 2 ) \n 
res5 = self . targobj . filter ( [ { : 1 } ] , i = 2 ) \n 
~~ def test__filter_single_annotation_obj_single ( self ) : \n 
res0 = self . targobj . filter ( j = 1 , objects = ) \n 
res1 = self . targobj . filter ( j = 1 , objects = Epoch ) \n 
res2 = self . targobj . filter ( j = 1 , objects = [ ] ) \n 
res3 = self . targobj . filter ( j = 1 , objects = [ Epoch ] ) \n 
res4 = self . targobj . filter ( j = 1 , objects = [ Epoch , \n 
RecordingChannelGroup ] ) \n 
~~ def test__filter_single_annotation_obj_multi ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] , self . epcas1a [ 1 ] ] \n 
res0 = self . targobj . filter ( j = 1 , objects = [ , EpochArray ] ) \n 
~~ def test__filter_single_annotation_obj_none ( self ) : \n 
res0 = self . targobj . filter ( j = 1 , objects = RecordingChannelGroup ) \n 
res1 = self . targobj . filter ( j = 1 , objects = ) \n 
~~ def test__filter_single_annotation_norecur ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] , self . epcas1a [ 1 ] , \n 
self . evts1a [ 1 ] , self . evtas1a [ 1 ] ] \n 
res0 = self . targobj . filter ( j = 1 , \n 
recursive = False ) \n 
~~ def test__filter_single_attribute_norecur ( self ) : \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
~~ def test__filter_single_annotation_nodata ( self ) : \n 
res0 = self . targobj . filter ( j = 0 , \n 
data = False ) \n 
~~ def test__filter_single_attribute_nodata ( self ) : \n 
~~ def test__filter_single_annotation_nodata_norecur ( self ) : \n 
data = False , recursive = False ) \n 
~~ def test__filter_single_attribute_nodata_norecur ( self ) : \n 
~~ def test__filter_single_annotation_container ( self ) : \n 
container = True ) \n 
~~ def test__filter_single_attribute_container ( self ) : \n 
~~ def test__filter_single_annotation_container_norecur ( self ) : \n 
container = True , recursive = False ) \n 
~~ def test__filter_single_attribute_container_norecur ( self ) : \n 
~~ def test__filter_single_annotation_nodata_container ( self ) : \n 
data = False , container = True ) \n 
~~ def test__filter_single_attribute_nodata_container ( self ) : \n 
~~ def test__filter_single_annotation_nodata_container_norecur ( self ) : \n 
data = False , container = True , \n 
~~ def test__filter_single_attribute_nodata_container_norecur ( self ) : \n 
data = self . targobj . children_recur \n 
targ = ( self . sigs1a + self . sigarrs1a + \n 
res0 = filterdata ( data , name = self . epcs1a [ 1 ] . name , j = 0 ) \n 
res1 = filterdata ( data , { : self . epcs1a [ 1 ] . name , : 0 } ) \n 
res2 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name , : 0 } ) \n 
~~ def test__filterdata_multi_nores ( self ) : \n 
~~~ data = self . targobj . children_recur \n 
targ = [ ] \n 
res0 = filterdata ( data , [ { : 0 } , { } ] ) \n 
res1 = filterdata ( data , { } , ttype = 0 ) \n 
res2 = filterdata ( data , [ { } ] , ttype = 0 ) \n 
res3 = filterdata ( data , { : self . epcs1a [ 1 ] . name } , j = 0 ) \n 
res4 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name } , j = 0 ) \n 
res5 = filterdata ( data , name = self . epcs1a [ 1 ] . name , targdict = { : 0 } ) \n 
res6 = filterdata ( data , name = self . epcs2 [ 0 ] . name , j = 5 ) \n 
res7 = filterdata ( data , { : self . epcs2 [ 1 ] . name , : 5 } ) \n 
res8 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name , : 5 } ) \n 
res9 = filterdata ( data , { : self . epcs2 [ 1 ] . name } , j = 5 ) \n 
res10 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name } , j = 5 ) \n 
res11 = filterdata ( data , name = self . epcs2 [ 1 ] . name , targdict = { : 5 } ) \n 
res12 = filterdata ( data , { : self . epcs1a [ 1 ] . name } , j = 5 ) \n 
res13 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name } , j = 5 ) \n 
res14 = filterdata ( data , name = self . epcs1a [ 1 ] . name , targdict = { : 5 } ) \n 
~~ def test__filterdata_multi_partres ( self ) : \n 
targ = [ self . epcs1a [ 1 ] ] \n 
res0 = filterdata ( data , name = self . epcs1a [ 1 ] . name , j = 5 ) \n 
res1 = filterdata ( data , { : self . epcs1a [ 1 ] . name , : 5 } ) \n 
res2 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name , : 5 } ) \n 
res3 = filterdata ( data , [ { : 1 } , { : 2 } ] ) \n 
res4 = filterdata ( data , { : 1 } , i = 2 ) \n 
res5 = filterdata ( data , [ { : 1 } ] , i = 2 ) \n 
def test__pretty ( self ) : \n 
~~~ ann = get_annotations ( ) \n 
ann [ ] = self . seed1 \n 
ann = pretty ( ann ) . replace ( , ) \n 
res = pretty ( self . seg1 ) \n 
sig0 = pretty ( self . sigs1 [ 0 ] ) \n 
sig1 = pretty ( self . sigs1 [ 1 ] ) \n 
sig2 = pretty ( self . sigs1 [ 2 ] ) \n 
sig3 = pretty ( self . sigs1 [ 3 ] ) \n 
sig0 = sig0 . replace ( , ) \n 
sig1 = sig1 . replace ( , ) \n 
sig2 = sig2 . replace ( , ) \n 
sig3 = sig3 . replace ( , ) \n 
sigarr0 = pretty ( self . sigarrs1 [ 0 ] ) \n 
sigarr1 = pretty ( self . sigarrs1 [ 1 ] ) \n 
sigarr0 = sigarr0 . replace ( , ) \n 
sigarr1 = sigarr1 . replace ( , ) \n 
( len ( self . sigs1a ) , len ( self . sigarrs1a ) ) ) + \n 
( len ( self . epcs1a ) , len ( self . epcas1a ) ) ) + \n 
( len ( self . evts1a ) , len ( self . evtas1a ) ) ) + \n 
len ( self . irsigs1a ) ) + \n 
( len ( self . spikes1a ) , len ( self . trains1a ) ) ) + \n 
( self . seg1 . name , self . seg1 . description ) \n 
( % ( 0 , sig0 ) ) + \n 
( % ( 1 , sig1 ) ) + \n 
( % ( 2 , sig2 ) ) + \n 
( % ( 3 , sig3 ) ) + \n 
( % ( 0 , sigarr0 ) ) + \n 
( % ( 1 , sigarr1 ) ) ) \n 
self . assertEqual ( res , targ ) \n 
~~ def test__construct_subsegment_by_unit ( self ) : \n 
~~~ nb_seg = 3 \n 
nb_unit = 7 \n 
unit_with_sig = np . array ( [ 0 , 2 , 5 ] ) \n 
signal_types = [ , ] \n 
sig_len = 100 \n 
#recordingchannelgroups \n 
rcgs = [ RecordingChannelGroup ( name = , \n 
channel_indexes = unit_with_sig ) , \n 
RecordingChannelGroup ( name = , \n 
channel_indexes = unit_with_sig ) ] \n 
all_unit = [ ] \n 
for u in range ( nb_unit ) : \n 
~~~ un = Unit ( name = % u , channel_indexes = np . array ( [ u ] ) ) \n 
assert_neo_object_is_compliant ( un ) \n 
all_unit . append ( un ) \n 
~~ blk = Block ( ) \n 
blk . recordingchannelgroups = rcgs \n 
for s in range ( nb_seg ) : \n 
~~~ seg = Segment ( name = % s ) \n 
for j in range ( nb_unit ) : \n 
~~~ st = SpikeTrain ( [ 1 , 2 ] , units = , \n 
t_start = 0. , t_stop = 10 ) \n 
st . unit = all_unit [ j ] \n 
~~ for t in signal_types : \n 
~~~ anasigarr = AnalogSignalArray ( np . zeros ( ( sig_len , \n 
len ( unit_with_sig ) ) ) , \n 
units = , \n 
sampling_rate = 1000. * pq . Hz , \n 
channel_indexes = unit_with_sig ) \n 
seg . analogsignalarrays . append ( anasigarr ) \n 
~~ ~~ blk . create_many_to_one_relationship ( ) \n 
for unit in all_unit : \n 
~~~ assert_neo_object_is_compliant ( unit ) \n 
~~ for rcg in rcgs : \n 
~~~ assert_neo_object_is_compliant ( rcg ) \n 
~~ assert_neo_object_is_compliant ( blk ) \n 
newseg = seg . construct_subsegment_by_unit ( all_unit [ : 4 ] ) \n 
assert_neo_object_is_compliant ( newseg ) \n 
~~ def test_segment_take_spikes_by_unit ( self ) : \n 
~~~ result1 = self . seg1 . take_spikes_by_unit ( ) \n 
result21 = self . seg1 . take_spikes_by_unit ( [ self . unit1 ] ) \n 
result22 = self . seg1 . take_spikes_by_unit ( [ self . unit2 ] ) \n 
self . assertEqual ( result1 , [ ] ) \n 
assert_same_sub_schema ( result21 , [ self . spikes1a [ 0 ] ] ) \n 
assert_same_sub_schema ( result22 , [ self . spikes1a [ 1 ] ] ) \n 
~~ def test_segment_take_spiketrains_by_unit ( self ) : \n 
~~~ result1 = self . seg1 . take_spiketrains_by_unit ( ) \n 
result21 = self . seg1 . take_spiketrains_by_unit ( [ self . unit1 ] ) \n 
result22 = self . seg1 . take_spiketrains_by_unit ( [ self . unit2 ] ) \n 
assert_same_sub_schema ( result21 , [ self . trains1a [ 0 ] ] ) \n 
assert_same_sub_schema ( result22 , [ self . trains1a [ 1 ] ] ) \n 
~~ def test_segment_take_analogsignal_by_unit ( self ) : \n 
~~~ result1 = self . seg1 . take_analogsignal_by_unit ( ) \n 
result21 = self . seg1 . take_analogsignal_by_unit ( [ self . unit1 ] ) \n 
result22 = self . seg1 . take_analogsignal_by_unit ( [ self . unit2 ] ) \n 
assert_same_sub_schema ( result21 , [ self . sigs1a [ 0 ] ] ) \n 
assert_same_sub_schema ( result22 , [ self . sigs1a [ 1 ] ] ) \n 
~~ def test_segment_take_analogsignal_by_channelindex ( self ) : \n 
~~~ ind1 = self . unit1 . channel_indexes [ 0 ] \n 
ind2 = self . unit2 . channel_indexes [ 0 ] \n 
result1 = self . seg1 . take_analogsignal_by_channelindex ( ) \n 
result21 = self . seg1 . take_analogsignal_by_channelindex ( [ ind1 ] ) \n 
result22 = self . seg1 . take_analogsignal_by_channelindex ( [ ind2 ] ) \n 
~~ def test_seg_take_slice_of_analogsignalarray_by_unit ( self ) : \n 
~~~ seg = self . seg1 \n 
result1 = seg . take_slice_of_analogsignalarray_by_unit ( ) \n 
result21 = seg . take_slice_of_analogsignalarray_by_unit ( [ self . unit1 ] ) \n 
result23 = seg . take_slice_of_analogsignalarray_by_unit ( [ self . unit3 ] ) \n 
targ1 = [ self . sigarrs1a [ 0 ] [ : , np . array ( [ True ] ) ] , \n 
self . sigarrs1a [ 1 ] [ : , np . array ( [ False ] ) ] ] \n 
targ3 = [ self . sigarrs1a [ 0 ] [ : , np . array ( [ False ] ) ] , \n 
self . sigarrs1a [ 1 ] [ : , np . array ( [ True ] ) ] ] \n 
assert_same_sub_schema ( result21 , targ1 ) \n 
assert_same_sub_schema ( result23 , targ3 ) \n 
~~ def test_seg_take_slice_of_analogsignalarray_by_channelindex ( self ) : \n 
ind1 = self . unit1 . channel_indexes [ 0 ] \n 
ind3 = self . unit3 . channel_indexes [ 0 ] \n 
result1 = seg . take_slice_of_analogsignalarray_by_channelindex ( ) \n 
result21 = seg . take_slice_of_analogsignalarray_by_channelindex ( [ ind1 ] ) \n 
result23 = seg . take_slice_of_analogsignalarray_by_channelindex ( [ ind3 ] ) \n 
from __future__ import absolute_import , division \n 
~~ from neo . io import NeuroScopeIO \n 
from neo . test . iotest . common_io_test import BaseTestIO \n 
class TestNeuroScopeIO ( BaseTestIO , unittest . TestCase , ) : \n 
~~~ ioclass = NeuroScopeIO \n 
files_to_test = [ ] \n 
files_to_download = [ , \n 
~~ if __name__ == "__main__" : \n 
~~ from django . http import HttpResponseRedirect \n 
from neurovault . apps . statmaps . utils import HttpRedirectException \n 
class CollectionRedirectMiddleware : \n 
~~~ def process_exception ( self , request , exception ) : \n 
~~~ if isinstance ( exception , HttpRedirectException ) : \n 
~~~ return HttpResponseRedirect ( exception . args [ 0 ] ) \n 
~~ ~~ ~~ from __future__ import unicode_literals \n 
from django . db import models , migrations \n 
import json , os \n 
dir = os . path . abspath ( os . path . dirname ( __file__ ) ) \n 
def populate_cogatlas ( apps , schema_editor ) : \n 
~~~ CognitiveAtlasTask = apps . get_model ( "statmaps" , "CognitiveAtlasTask" ) \n 
CognitiveAtlasContrast = apps . get_model ( "statmaps" , "CognitiveAtlasContrast" ) \n 
json_content = open ( os . path . join ( dir , "cognitiveatlas_tasks.json" ) ) . read ( ) \n 
json_content = json_content . decode ( "utf-8" ) . replace ( , ) \n 
data = json . loads ( json_content ) \n 
for item in data : \n 
~~~ task = CognitiveAtlasTask ( name = item [ "name" ] , cog_atlas_id = item [ "id" ] ) \n 
task . save ( ) \n 
for contrast in item [ "contrasts" ] : \n 
~~~ contrast = CognitiveAtlasContrast ( name = contrast [ "conname" ] , cog_atlas_id = contrast [ "conid" contrast . save ( ) \n 
~~ ~~ ~~ class Migration ( migrations . Migration ) : \n 
migrations . RunPython ( populate_cogatlas ) , \n 
~~ from __future__ import unicode_literals \n 
( , ) \n 
from django . core . files . uploadedfile import SimpleUploadedFile \n 
from django . test import TestCase , Client \n 
from neurovault . apps . statmaps . forms import NIDMResultsForm \n 
from neurovault . apps . statmaps . models import Collection , StatisticMap , Comparison \n 
from neurovault . apps . statmaps . utils import count_processing_comparisons , count_existing_comparisons \n 
from . utils import clearDB \n 
class Test_Counter ( TestCase ) : \n 
self . test_path = os . path . abspath ( os . path . dirname ( __file__ ) ) \n 
self . user = User . objects . create ( username = ) \n 
self . client = Client ( ) \n 
self . client . login ( username = self . user ) \n 
self . Collection1 = Collection ( name = , owner = self . user , \n 
DOI = ) \n 
self . Collection1 . save ( ) \n 
self . Collection2 = Collection ( name = , owner = self . user , \n 
self . Collection2 . save ( ) \n 
self . Collection3 = Collection ( name = , owner = self . user , \n 
self . Collection3 . save ( ) \n 
~~~ clearDB ( ) \n 
~~ def test_statmaps_processing ( self ) : \n 
Image1 = StatisticMap ( name = , collection = self . Collection1 , file = , Image1 . file = SimpleUploadedFile ( , file ( os . path . join ( self . test_path , Image1 . save ( ) \n 
images_processing = count_processing_comparisons ( Image1 . pk ) \n 
self . assertEqual ( images_processing , 0 ) \n 
total_comparisons = count_existing_comparisons ( Image1 . pk ) \n 
self . assertEqual ( total_comparisons , 1 ) \n 
~~ def test_adding_nidm ( self ) : \n 
~~~ Image2 = StatisticMap ( name = , collection = self . Collection1 , file = , map_type Image2 . file = SimpleUploadedFile ( , file ( os . path . join ( self . test_path , Image2 . save ( ) \n 
zip_file = open ( os . path . join ( self . test_path , ) , ) \n 
post_dict = { \n 
: . format ( ) , \n 
: self . Collection2 . pk } \n 
fname = os . path . basename ( os . path . join ( self . test_path , ) ) file_dict = { : SimpleUploadedFile ( fname , zip_file . read ( ) ) } \n 
zip_file . close ( ) \n 
form = NIDMResultsForm ( post_dict , file_dict ) \n 
nidm = form . save ( ) \n 
total_comparisons = count_existing_comparisons ( Image2 . pk ) \n 
Image2ss = StatisticMap ( name = , collection = self . Collection3 , file = Image2ss . file = SimpleUploadedFile ( , file ( os . path . join ( self . test_path , Image2ss . save ( ) \n 
total_comparisons = count_existing_comparisons ( Image2ss . pk ) \n 
self . assertEqual ( total_comparisons , 0 ) \n 
number_comparisons = len ( Comparison . objects . all ( ) ) \n 
self . assertEqual ( number_comparisons > 0 , True ) \n 
from neurovault . settings import PRIVATE_MEDIA_ROOT \n 
import os . path \n 
from neurovault . apps . statmaps . models import * \n 
def delOldCollDir ( ) : \n 
~~~ collDir = os . path . join ( PRIVATE_MEDIA_ROOT , ) \n 
for folder in os . listdir ( collDir ) : \n 
~~~ if not Collection . objects . filter ( pk = folder ) : \n 
~~~ os . rmdir ( os . path . join ( collDir , folder ) ) \n 
~~ ~~ ~~ delOldCollDir ( ) import theano \n 
from theano import tensor as T \n 
from theano . sandbox . rng_mrg import MRG_RandomStreams as RandomStreams \n 
from load import mnist \n 
srng = RandomStreams ( ) \n 
def floatX ( X ) : \n 
~~~ return np . asarray ( X , dtype = theano . config . floatX ) \n 
~~ def init_weights ( shape ) : \n 
~~~ return theano . shared ( floatX ( np . random . randn ( * shape ) * 0.01 ) ) \n 
~~ def rectify ( X ) : \n 
~~~ return T . maximum ( X , 0. ) \n 
~~ def softmax ( X ) : \n 
~~~ e_x = T . exp ( X - X . max ( axis = 1 ) . dimshuffle ( 0 , ) ) \n 
return e_x / e_x . sum ( axis = 1 ) . dimshuffle ( 0 , ) \n 
~~ def RMSprop ( cost , params , lr = 0.001 , rho = 0.9 , epsilon = 1e-6 ) : \n 
~~~ grads = T . grad ( cost = cost , wrt = params ) \n 
updates = [ ] \n 
for p , g in zip ( params , grads ) : \n 
~~~ acc = theano . shared ( p . get_value ( ) * 0. ) \n 
acc_new = rho * acc + ( 1 - rho ) * g ** 2 \n 
gradient_scaling = T . sqrt ( acc_new + epsilon ) \n 
g = g / gradient_scaling \n 
updates . append ( ( acc , acc_new ) ) \n 
updates . append ( ( p , p - lr * g ) ) \n 
~~ return updates \n 
~~ def dropout ( X , p = 0. ) : \n 
~~~ if p > 0 : \n 
~~~ retain_prob = 1 - p \n 
X *= srng . binomial ( X . shape , p = retain_prob , dtype = theano . config . floatX ) \n 
X /= retain_prob \n 
~~ def model ( X , w_h , w_h2 , w_o , p_drop_input , p_drop_hidden ) : \n 
~~~ X = dropout ( X , p_drop_input ) \n 
h = rectify ( T . dot ( X , w_h ) ) \n 
h = dropout ( h , p_drop_hidden ) \n 
h2 = rectify ( T . dot ( h , w_h2 ) ) \n 
h2 = dropout ( h2 , p_drop_hidden ) \n 
py_x = softmax ( T . dot ( h2 , w_o ) ) \n 
return h , h2 , py_x \n 
~~ trX , teX , trY , teY = mnist ( onehot = True ) \n 
X = T . fmatrix ( ) \n 
Y = T . fmatrix ( ) \n 
w_h = init_weights ( ( 784 , 625 ) ) \n 
w_h2 = init_weights ( ( 625 , 625 ) ) \n 
w_o = init_weights ( ( 625 , 10 ) ) \n 
noise_h , noise_h2 , noise_py_x = model ( X , w_h , w_h2 , w_o , 0.2 , 0.5 ) \n 
h , h2 , py_x = model ( X , w_h , w_h2 , w_o , 0. , 0. ) \n 
y_x = T . argmax ( py_x , axis = 1 ) \n 
cost = T . mean ( T . nnet . categorical_crossentropy ( noise_py_x , Y ) ) \n 
params = [ w_h , w_h2 , w_o ] \n 
updates = RMSprop ( cost , params , lr = 0.001 ) \n 
train = theano . function ( inputs = [ X , Y ] , outputs = cost , updates = updates , allow_input_downcast = True ) \n 
predict = theano . function ( inputs = [ X ] , outputs = y_x , allow_input_downcast = True ) \n 
for i in range ( 100 ) : \n 
~~~ for start , end in zip ( range ( 0 , len ( trX ) , 128 ) , range ( 128 , len ( trX ) , 128 ) ) : \n 
~~~ cost = train ( trX [ start : end ] , trY [ start : end ] ) \n 
~~ print np . mean ( np . argmax ( teY , axis = 1 ) == predict ( teX ) ) \n 
from ndscheduler import settings \n 
from ndscheduler . core . datastore . providers import base \n 
class DatastorePostgresql ( base . DatastoreBase ) : \n 
def get_db_url ( cls ) : \n 
return % ( \n 
settings . DATABASE_CONFIG_DICT [ ] , \n 
settings . DATABASE_CONFIG_DICT [ ] ) \n 
from ndscheduler import job \n 
logger = logging . getLogger ( __name__ ) \n 
class CurlJob ( job . JobBase ) : \n 
~~~ TIMEOUT = 10 \n 
def meta_info ( cls ) : \n 
: % ( cls . __module__ , cls . __name__ ) , \n 
{ : , : } , \n 
{ : , : \n 
~~ def run ( self , url , request_type , * args , ** kwargs ) : \n 
~~~ print ( % ( url ) ) \n 
session = requests . Session ( ) \n 
result = session . request ( request_type , \n 
url , \n 
timeout = self . TIMEOUT , \n 
headers = None , \n 
data = None ) \n 
print ( result . text ) \n 
~~~ job = CurlJob . create_test_instance ( ) \n 
job . run ( ) \n 
from . mock import MagicMock , Mock \n 
from . util import TrelloElementMock , CommandMock , OperationMock \n 
from operations import * \n 
class BaseOperationTests ( unittest . TestCase ) : \n 
~~~ self . base_operation , self . trello_element = OperationMock . create ( BaseOperation ) \n 
self . class_mock , self . instance_mock = OperationMock . instance ( self . base_operation ) \n 
self . collection = TrelloElementMock . collection ( ) \n 
self . base_operation . collection = TrelloCollection ( self . collection ) \n 
~~ def test_items_sets_the_collection ( self ) : \n 
~~~ self . base_operation . set_collection = MagicMock ( ) \n 
self . base_operation . items ( ) \n 
self . base_operation . set_collection . assert_called_with ( ) \n 
~~ def test_items_returns_every_name_from_the_collection_with_the_added_options ( self ) : \n 
~~ def test_callback_uses_find_to_instantiate_the_operation_if_the_index_is_in_the_collection ( self ) ~~~ self . base_operation . callback ( 3 ) \n 
self . class_mock . assert_called_with ( self . collection [ 0 ] , self . base_operation ) \n 
~~ def test_callback_calls_execute_on_the_operation ( self ) : \n 
~~~ self . base_operation . callback ( 3 ) \n 
self . instance_mock . execute . assert_called_with ( self . base_operation . command ) \n 
~~ def test_callback_doesnt_call_find_if_the_index_is_bigger_than_the_collection_length ( self ) : \n 
~~~ big_index = 55 \n 
self . base_operation . callback ( big_index ) \n 
assert not self . class_mock . called \n 
~~ def test_callback_calls_execute_on_the_previous_operation_if_index_is_0 ( self ) : \n 
~~~ self . base_operation . callback ( 0 ) \n 
self . base_operation . previous_operation . execute . assert_called_with ( ) \n 
~~ def test_callback_calls_the_input_method_on_the_command_with_deferred_add_as_callback_if_index_is_1 ~~~ self . base_operation . command . input = MagicMock ( ) \n 
self . base_operation . callback ( 2 ) \n 
self . base_operation . command . input . assert_called_with ( "Name" , self . base_operation . deferred_add \n 
~~ def test_base_add_calls_add_with_the_text_and_cleans_the_cache_for_the_element ( self ) : \n 
~~~ text = "Text" \n 
self . base_operation . add = MagicMock ( ) \n 
self . base_operation . trello_element . reload = MagicMock ( ) \n 
self . base_operation . base_add ( text ) \n 
self . base_operation . add . assert_called_with ( text ) \n 
self . trello_element . reload . assert_called_with ( ) \n 
~~ def test_base_add_calls_add_and_execute_if_renavigate_is_true ( self ) : \n 
self . base_operation . command . renavigate = True \n 
self . base_operation . execute = MagicMock ( ) \n 
self . base_operation . execute . assert_called_with ( ) \n 
~~ ~~ class BoardOperationTests ( unittest . TestCase ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( BoardOperation ) \n 
self . operation . collection = TrelloCollection ( TrelloElementMock . collection ( ) ) \n 
~~ def test_items_returns_every_name_from_the_collection_without_goback ( self ) : \n 
~~~ self . operation . set_collection = MagicMock ( ) \n 
~~ def test_trello_element_property ( self ) : \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "boards" ) \n 
~~ def test_callback_calls_execute_command_with_the_index ( self ) : \n 
~~~ self . operation . execute_command = MagicMock ( ) \n 
self . operation . callback ( 5 ) \n 
self . operation . execute_command . assert_called_with ( 3 ) \n 
~~ def test_callback_calls_the_input_method_on_the_command_with_deferred_add_as_callback_if_index_is_1 ~~~ self . operation . command . input = MagicMock ( ) \n 
self . operation . callback ( 1 ) \n 
self . operation . command . input . assert_called_with ( "Name" , self . operation . deferred_add ) \n 
~~ def test_next_operation_class ( self ) : \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , ListOperation ) \n 
~~ def test_add_creates_a_board_with_the_text ( self ) : \n 
self . trello_element . add_board = MagicMock ( ) \n 
self . operation . add ( text ) \n 
self . trello_element . add_board . assert_called_with ( text ) \n 
~~ ~~ class ListOperationTests ( unittest . TestCase ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( ListOperation ) \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "lists" ) \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , CardOperation ) \n 
~~ def test_add_creates_a_list_with_the_text ( self ) : \n 
self . trello_element . add_list = MagicMock ( ) \n 
self . trello_element . add_list . assert_called_with ( text ) \n 
~~ ~~ class CardOperationTests ( unittest . TestCase ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( CardOperation ) \n 
~~ def test_items_returns_every_name_from_the_collection_with_custom_actions ( self ) : \n 
self . assertEqual ( self . operation . items ( ) , [ , , \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "cards" ) \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , CardOptions ) \n 
~~ def test_add_creates_a_card_with_the_text_and_description ( self ) : \n 
self . trello_element . add_card = MagicMock ( ) \n 
self . operation . add ( name , desc ) \n 
self . trello_element . add_card . assert_called_with ( name , desc ) \n 
~~ def test_split_card_contents_returns_the_name_and_description_splitted_by_new_lines ( self ) : \n 
~~~ content = "Name!!\\n\\nDescription\\nYeah!" \n 
name , desc = self . operation . split_card_contents ( content ) \n 
self . assertEqual ( name , "Name!!" ) \n 
self . assertEqual ( desc , "Description\\nYeah!" ) \n 
DEBUG = env . bool ( , default = True ) \n 
TEMPLATES [ 0 ] [ ] [ ] = DEBUG \n 
SECRET_KEY = env ( "DJANGO_SECRET_KEY" , default = ) \n 
EMAIL_HOST = \n 
EMAIL_PORT = 1025 \n 
EMAIL_BACKEND = env ( , \n 
CACHES = { \n 
MIDDLEWARE_CLASSES += ( , ) \n 
INSTALLED_APPS += ( , ) \n 
INTERNAL_IPS = ( , , ) \n 
DEBUG_TOOLBAR_CONFIG = { \n 
TEST_RUNNER = \n 
from django . contrib import messages \n 
from django . contrib . auth import logout , login , authenticate \n 
from django . http import HttpResponseBadRequest , Http404 \n 
from django . shortcuts import render , redirect , get_object_or_404 \n 
from reddit . forms import UserForm , ProfileForm \n 
from reddit . utils . helpers import post_only \n 
from users . models import RedditUser \n 
def user_profile ( request , username ) : \n 
~~~ user = get_object_or_404 ( User , username = username ) \n 
profile = RedditUser . objects . get ( user = user ) \n 
return render ( request , , { : profile } ) \n 
def edit_profile ( request ) : \n 
~~~ user = RedditUser . objects . get ( user = request . user ) \n 
if request . method == : \n 
~~~ profile_form = ProfileForm ( instance = user ) \n 
~~ elif request . method == : \n 
~~~ profile_form = ProfileForm ( request . POST , instance = user ) \n 
if profile_form . is_valid ( ) : \n 
~~~ profile = profile_form . save ( commit = False ) \n 
profile . update_profile_data ( ) \n 
profile . save ( ) \n 
~~~ raise Http404 \n 
~~ return render ( request , , { : profile_form } ) \n 
~~ def user_login ( request ) : \n 
if request . user . is_authenticated ( ) : \n 
return render ( request , ) \n 
~~ if request . method == "POST" : \n 
~~~ username = request . POST . get ( ) \n 
password = request . POST . get ( ) \n 
if not username or not password : \n 
~~~ return HttpResponseBadRequest ( ) \n 
~~ user = authenticate ( username = username , \n 
password = password ) \n 
~~~ if user . is_active : \n 
~~~ login ( request , user ) \n 
redirect_url = request . POST . get ( ) or \n 
return redirect ( redirect_url ) \n 
~~~ return render ( request , , \n 
~~ ~~ return render ( request , ) \n 
~~ @ post_only \n 
def user_logout ( request ) : \n 
~~~ redirect_page = request . POST . get ( , ) \n 
logout ( request ) \n 
messages . success ( request , ) \n 
return redirect ( redirect_page ) \n 
~~ return redirect ( ) \n 
~~ def register ( request ) : \n 
user_form = UserForm ( ) \n 
~~~ messages . warning ( request , \n 
return render ( request , , { : user_form } ) \n 
~~~ user_form = UserForm ( request . POST ) \n 
if user_form . is_valid ( ) : \n 
~~~ user = user_form . save ( ) \n 
user . set_password ( user . password ) \n 
user . save ( ) \n 
reddit_user = RedditUser ( ) \n 
reddit_user . user = user \n 
reddit_user . save ( ) \n 
user = authenticate ( username = request . POST [ ] , \n 
password = request . POST [ ] ) \n 
login ( request , user ) \n 
return redirect ( ) \n 
~~ ~~ return render ( request , , { : user_form } ) \n 
~~ import unittest2 \n 
from pymysql . tests import base \n 
from pymysql import util \n 
class TestNextset ( base . PyMySQLTestCase ) : \n 
~~~ super ( TestNextset , self ) . setUp ( ) \n 
self . con = self . connections [ 0 ] \n 
~~ def test_nextset ( self ) : \n 
~~~ cur = self . con . cursor ( ) \n 
self . assertEqual ( [ ( 1 , ) ] , list ( cur ) ) \n 
r = cur . nextset ( ) \n 
self . assertTrue ( r ) \n 
self . assertEqual ( [ ( 2 , ) ] , list ( cur ) ) \n 
self . assertIsNone ( cur . nextset ( ) ) \n 
~~ def test_skip_nextset ( self ) : \n 
self . assertEqual ( [ ( 42 , ) ] , list ( cur ) ) \n 
~~ def test_ok_and_next ( self ) : \n 
self . assertTrue ( cur . nextset ( ) ) \n 
self . assertFalse ( bool ( cur . nextset ( ) ) ) \n 
~~ @ unittest2 . expectedFailure \n 
def test_multi_cursor ( self ) : \n 
~~~ cur1 = self . con . cursor ( ) \n 
cur2 = self . con . cursor ( ) \n 
self . assertEqual ( [ ( 1 , ) ] , list ( cur1 ) ) \n 
self . assertEqual ( [ ( 42 , ) ] , list ( cur2 ) ) \n 
r = cur1 . nextset ( ) \n 
self . assertEqual ( [ ( 2 , ) ] , list ( cur1 ) ) \n 
self . assertIsNone ( cur1 . nextset ( ) ) \n 
~~ def test_multi_statement_warnings ( self ) : \n 
~~~ cursor = self . con . cursor ( ) \n 
~~~ cursor . execute ( \n 
~~~ self . fail ( ) \n 
from spyderlib . qt . QtGui import QVBoxLayout , QGroupBox , QLabel \n 
from spyderlib . qt . QtCore import SIGNAL , Qt \n 
from spyderlib . baseconfig import get_translation \n 
_ = get_translation ( "p_memoryprofiler" , dirname = "spyderplugins" ) \n 
from spyderlib . utils . qthelpers import get_icon , create_action \n 
from spyderlib . plugins import SpyderPluginMixin , PluginConfigPage , runconfig \n 
from spyderplugins . widgets . memoryprofilergui import ( \n 
MemoryProfilerWidget , is_memoryprofiler_installed ) \n 
class MemoryProfilerConfigPage ( PluginConfigPage ) : \n 
def setup_page ( self ) : \n 
~~~ settings_group = QGroupBox ( _ ( "Settings" ) ) \n 
use_color_box = self . create_checkbox ( \n 
, default = True ) \n 
results_group = QGroupBox ( _ ( "Results" ) ) \n 
results_label1 . setWordWrap ( True ) \n 
results_label2 = QLabel ( MemoryProfilerWidget . DATAPATH ) \n 
results_label2 . setTextInteractionFlags ( Qt . TextSelectableByMouse ) \n 
results_label2 . setWordWrap ( True ) \n 
settings_layout = QVBoxLayout ( ) \n 
settings_layout . addWidget ( use_color_box ) \n 
settings_group . setLayout ( settings_layout ) \n 
results_layout = QVBoxLayout ( ) \n 
results_layout . addWidget ( results_label1 ) \n 
results_layout . addWidget ( results_label2 ) \n 
results_group . setLayout ( results_layout ) \n 
vlayout = QVBoxLayout ( ) \n 
vlayout . addWidget ( settings_group ) \n 
vlayout . addWidget ( results_group ) \n 
vlayout . addStretch ( 1 ) \n 
self . setLayout ( vlayout ) \n 
~~ ~~ class MemoryProfiler ( MemoryProfilerWidget , SpyderPluginMixin ) : \n 
CONF_SECTION = \n 
CONFIGWIDGET_CLASS = MemoryProfilerConfigPage \n 
def __init__ ( self , parent = None ) : \n 
~~~ MemoryProfilerWidget . __init__ ( self , parent = parent ) \n 
SpyderPluginMixin . __init__ ( self , parent ) \n 
self . initialize_plugin ( ) \n 
~~ def get_plugin_title ( self ) : \n 
~~ def get_plugin_icon ( self ) : \n 
return get_icon ( ) \n 
~~ def get_focus_widget ( self ) : \n 
return self . datatree \n 
~~ def get_plugin_actions ( self ) : \n 
~~ def on_first_registration ( self ) : \n 
self . main . tabify_plugins ( self . main . inspector , self ) \n 
self . dockwidget . hide ( ) \n 
~~ def register_plugin ( self ) : \n 
self . connect ( self , SIGNAL ( "edit_goto(QString,int,QString)" ) , \n 
self . main . editor . load ) \n 
self . connect ( self , SIGNAL ( ) , \n 
self . main . redirect_internalshell_stdio ) \n 
self . main . add_dockwidget ( self ) \n 
icon = self . get_plugin_icon ( ) , \n 
triggered = self . run_memoryprofiler ) \n 
memoryprofiler_act . setEnabled ( is_memoryprofiler_installed ( ) ) \n 
self . main . run_menu_actions += [ memoryprofiler_act ] \n 
self . main . editor . pythonfile_dependent_actions += [ memoryprofiler_act ] \n 
~~ def refresh_plugin ( self ) : \n 
~~ def closing_plugin ( self , cancelable = False ) : \n 
~~ def apply_plugin_settings ( self , options ) : \n 
~~ def run_memoryprofiler ( self ) : \n 
self . analyze ( self . main . editor . get_current_filename ( ) ) \n 
~~ def analyze ( self , filename ) : \n 
if self . dockwidget and not self . ismaximized : \n 
~~~ self . dockwidget . setVisible ( True ) \n 
self . dockwidget . setFocus ( ) \n 
self . dockwidget . raise_ ( ) \n 
~~ pythonpath = self . main . get_spyder_pythonpath ( ) \n 
runconf = runconfig . get_run_configuration ( filename ) \n 
wdir , args = None , None \n 
if runconf is not None : \n 
~~~ if runconf . wdir_enabled : \n 
~~~ wdir = runconf . wdir \n 
~~ if runconf . args_enabled : \n 
~~~ args = runconf . args \n 
~~ ~~ MemoryProfilerWidget . analyze ( \n 
self , filename , wdir = wdir , args = args , pythonpath = pythonpath , \n 
use_colors = self . get_option ( , True ) ) \n 
#============================================================================== \n 
~~ ~~ PLUGIN_CLASS = MemoryProfiler \n 
from google . protobuf import descriptor as _descriptor \n 
from google . protobuf import message as _message \n 
from google . protobuf import reflection as _reflection \n 
from google . protobuf import descriptor_pb2 \n 
DESCRIPTOR = _descriptor . FileDescriptor ( \n 
package = , \n 
_PUSHNOTIFICATION = _descriptor . Descriptor ( \n 
full_name = , \n 
filename = None , \n 
file = DESCRIPTOR , \n 
containing_type = None , \n 
fields = [ \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 0 , \n 
number = 1 , type = 3 , cpp_type = 2 , label = 1 , \n 
has_default_value = False , default_value = 0 , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
name = , full_name = , index = 1 , \n 
number = 2 , type = 9 , cpp_type = 9 , label = 1 , \n 
has_default_value = False , default_value = unicode ( "" , "utf-8" ) , \n 
name = , full_name = , index = 2 , \n 
number = 3 , type = 9 , cpp_type = 9 , label = 1 , \n 
name = , full_name = , index = 3 , \n 
number = 4 , type = 9 , cpp_type = 9 , label = 1 , \n 
extensions = [ \n 
nested_types = [ ] , \n 
enum_types = [ \n 
options = None , \n 
is_extendable = False , \n 
extension_ranges = [ ] , \n 
serialized_start = 33 , \n 
serialized_end = 117 , \n 
_BATCHNOTIFICATIONREQUEST = _descriptor . Descriptor ( \n 
number = 1 , type = 11 , cpp_type = 10 , label = 3 , \n 
has_default_value = False , default_value = [ ] , \n 
serialized_start = 119 , \n 
serialized_end = 187 , \n 
_BATCHNOTIFICATIONREQUEST . fields_by_name [ ] . message_type = _PUSHNOTIFICATION \n 
DESCRIPTOR . message_types_by_name [ ] = _PUSHNOTIFICATION \n 
DESCRIPTOR . message_types_by_name [ ] = _BATCHNOTIFICATIONREQUEST \n 
class PushNotification ( _message . Message ) : \n 
~~~ __metaclass__ = _reflection . GeneratedProtocolMessageType \n 
DESCRIPTOR = _PUSHNOTIFICATION \n 
~~ class BatchNotificationRequest ( _message . Message ) : \n 
DESCRIPTOR = _BATCHNOTIFICATIONREQUEST \n 
~~ DESCRIPTOR . has_options = True \n 
from pushkin import pushkin_cli \n 
import tornado . web \n 
from pushkin import context \n 
from pushkin . database import database \n 
from pushkin . request . request_processor import RequestProcessor \n 
from pushkin . requesthandlers . events import JsonEventHandler \n 
from pushkin . requesthandlers . notifications import JsonNotificationHandler \n 
from pushkin import test_config_ini_path \n 
from pushkin import config \n 
def setup_database ( ) : \n 
~~~ database . create_database ( ) \n 
def mock_processor ( mocker ) : \n 
mocker . patch ( ) \n 
def app ( ) : \n 
~~~ pushkin_cli . CONFIGURATION_FILENAME = test_config_ini_path \n 
pushkin_cli . init ( ) \n 
return pushkin_cli . create_app ( ) \n 
def notification_batch_json ( ) : \n 
def post_notification_url ( base_url ) : \n 
~~~ return base_url + config . json_notification_handler_url \n 
def event_batch_json ( ) : \n 
def post_event_url ( base_url ) : \n 
~~~ return base_url + config . json_event_handler_url \n 
~~ @ pytest . mark . gen_test \n 
@ pytest . mark . parametrize ( "input" , [ \n 
( ) , \n 
def test_post_notification_empty_request ( setup_database , mock_processor , http_client , post_notification_url ~~~ request = tornado . httpclient . HTTPRequest ( post_notification_url , method = , body = input ) \n 
with pytest . raises ( tornado . httpclient . HTTPError ) : \n 
~~~ yield http_client . fetch ( request ) \n 
~~ assert not context . request_processor . submit . called \n 
def test_post_notification ( setup_database , mock_processor , http_client , post_notification_url , \n 
notification_batch_json ) : \n 
request = tornado . httpclient . HTTPRequest ( post_notification_url , method = , body = notification_batch_json response = yield http_client . fetch ( request ) \n 
assert response . code == 200 \n 
assert context . request_processor . submit . called \n 
def test_post_event_empty_request ( setup_database , mock_processor , http_client , post_event_url , input ~~~ \n 
request = tornado . httpclient . HTTPRequest ( post_event_url , method = , body = input ) \n 
def test_post_event ( setup_database , mock_processor , http_client , post_event_url , event_batch_json ) : \n 
context . request_processor . submit . return_value = True \n 
request = tornado . httpclient . HTTPRequest ( post_event_url , method = , body = event_batch_json ) \n 
response = yield http_client . fetch ( request ) \n 
def test_post_event_service_unavailable ( setup_database , mock_processor , http_client , post_event_url , app ) : \n 
context . request_processor . submit . return_value = False \n 
RequestProcessor . submit . return_value = False \n 
import tempfile \n 
from zipfile import ZipFile \n 
from nordicsemi . dfu . package import Package \n 
class TestPackage ( unittest . TestCase ) : \n 
~~~ self . work_directory = tempfile . mkdtemp ( prefix = "nrf_dfu_tests_" ) \n 
~~~ shutil . rmtree ( self . work_directory , ignore_errors = True ) \n 
~~ def test_generate_package_application ( self ) : \n 
~~~ self . p = Package ( \n 
dev_type = 1 , \n 
dev_rev = 2 , \n 
app_version = 100 , \n 
sd_req = [ 0x1000 , 0xfffe ] , \n 
app_fw = "firmwares/bar.hex" \n 
pkg_name = "mypackage.zip" \n 
self . p . generate_package ( pkg_name , preserve_work_directory = False ) \n 
expected_zip_content = [ "manifest.json" , "bar.bin" , "bar.dat" ] \n 
with ZipFile ( pkg_name , ) as pkg : \n 
~~~ infolist = pkg . infolist ( ) \n 
for file_information in infolist : \n 
~~~ self . assertTrue ( file_information . filename in expected_zip_content ) \n 
self . assertGreater ( file_information . file_size , 0 ) \n 
~~ pkg . extractall ( self . work_directory ) \n 
with open ( os . path . join ( self . work_directory , ) , ) as f : \n 
~~~ _json = json . load ( f ) \n 
self . assertEqual ( , _json [ ] [ ] [ ] ) \n 
self . assertTrue ( not in _json [ ] ) \n 
~~ ~~ ~~ def test_generate_package_sd_bl ( self ) : \n 
~~~ self . p = Package ( dev_type = 1 , \n 
softdevice_fw = "firmwares/foo.hex" , \n 
bootloader_fw = "firmwares/bar.hex" ) \n 
expected_zip_content = [ "manifest.json" , "sd_bl.bin" , "sd_bl.dat" ] \n 
self . assertEqual ( , _json [ ] [ ] [ self . assertEqual ( , _json [ ] [ ] [ \n 
~~ ~~ ~~ def test_unpack_package_a ( self ) : \n 
sd_req = [ 0x1000 , 0xffff ] , \n 
softdevice_fw = "firmwares/bar.hex" , \n 
dfu_ver = 0.6 ) \n 
pkg_name = os . path . join ( self . work_directory , "mypackage.zip" ) \n 
unpacked_dir = os . path . join ( self . work_directory , "unpacked" ) \n 
manifest = self . p . unpack_package ( os . path . join ( self . work_directory , pkg_name ) , unpacked_dir ) \n 
self . assertIsNotNone ( manifest ) \n 
self . assertEqual ( , manifest . softdevice . bin_file ) \n 
self . assertEqual ( 0 , manifest . softdevice . init_packet_data . ext_packet_id ) \n 
self . assertIsNotNone ( manifest . softdevice . init_packet_data . firmware_crc16 ) \n 
~~ def test_unpack_package_b ( self ) : \n 
dfu_ver = 0.7 ) \n 
self . assertEqual ( 1 , manifest . softdevice . init_packet_data . ext_packet_id ) \n 
self . assertIsNone ( manifest . softdevice . init_packet_data . firmware_crc16 ) \n 
self . assertIsNotNone ( manifest . softdevice . init_packet_data . firmware_hash ) \n 
~~ def test_unpack_package_c ( self ) : \n 
key_file = "key.pem" ) \n 
self . assertEqual ( 2 , manifest . softdevice . init_packet_data . ext_packet_id ) \n 
self . assertIsNotNone ( manifest . softdevice . init_packet_data . init_packet_ecds ) \n 
self . assertEqual ( manifest . dfu_version , 0.8 ) \n 
~~ from agamotto . utils import execute , grepc \n 
def installed ( package ) : \n 
~~ except Exception , _e : \n 
~~ ~~ def is_installed ( package ) : \n 
return installed ( package ) \n 
~~ __author__ = \n 
import wx \n 
from wx import AboutBox , AboutDialogInfo , ClientDC \n 
from wx . lib . wordwrap import wordwrap \n 
from odmtools . meta import data \n 
class frmAbout ( wx . Dialog ) : \n 
~~~ def __init__ ( self , parent ) : \n 
~~~ self . parent = parent \n 
info = AboutDialogInfo ( ) \n 
info . Name = data . app_name \n 
info . Version = data . version \n 
info . Copyright = data . copyright \n 
info . Description = wordwrap ( data . description , 350 , ClientDC ( parent ) ) \n 
info . WebSite = data . website \n 
info . Developers = data . developers \n 
info . License = wordwrap ( data . license , 500 , ClientDC ( parent ) ) \n 
AboutBox ( info ) \n 
#self.ShowModal()#Boa:FramePanel:pnlMethods \n 
~~ ~~ import wx \n 
import wx . grid \n 
import wx . richtext \n 
from odmtools . odmdata import Method \n 
[ wxID_PNLMETHOD , wxID_PNLMETHODSLISTCTRL1 , wxID_PNLMETHODSRBCREATENEW , \n 
wxID_PNLMETHODSRBGENERATE , wxID_PNLMETHODSRBSELECT , \n 
wxID_PNLMETHODSRICHTEXTCTRL1 , \n 
] = [ wx . NewId ( ) for _init_ctrls in range ( 6 ) ] \n 
from odmtools . common . logger import LoggerTool \n 
tool = LoggerTool ( ) \n 
logger = tool . setupLogger ( __name__ , __name__ + , , logging . DEBUG ) \n 
class pnlMethod ( wx . Panel ) : \n 
~~~ def _init_ctrls ( self , prnt ) : \n 
~~~ wx . Panel . __init__ ( self , id = wxID_PNLMETHOD , name = , \n 
parent = prnt , pos = wx . Point ( 135 , 307 ) , size = wx . Size ( 439 , 357 ) , \n 
style = wx . TAB_TRAVERSAL ) \n 
self . SetClientSize ( wx . Size ( 423 , 319 ) ) \n 
self . rbGenerate = wx . RadioButton ( id = wxID_PNLMETHODSRBGENERATE , \n 
label = , name = , \n 
parent = self , pos = wx . Point ( 16 , 8 ) , size = wx . Size ( 392 , 16 ) , style = 0 ) \n 
self . rbGenerate . SetValue ( True ) \n 
self . rbGenerate . Bind ( wx . EVT_RADIOBUTTON , self . OnRbGenerateRadiobutton , \n 
id = wxID_PNLMETHODSRBGENERATE ) \n 
self . rbSelect = wx . RadioButton ( id = wxID_PNLMETHODSRBSELECT , \n 
label = , name = , parent = self , \n 
pos = wx . Point ( 16 , 32 ) , size = wx . Size ( 392 , 13 ) , style = 0 ) \n 
self . rbSelect . SetValue ( False ) \n 
self . rbSelect . Bind ( wx . EVT_RADIOBUTTON , self . OnRbSelectRadiobutton , \n 
id = wxID_PNLMETHODSRBSELECT ) \n 
self . rbCreateNew = wx . RadioButton ( id = wxID_PNLMETHODSRBCREATENEW , \n 
pos = wx . Point ( 16 , 208 ) , size = wx . Size ( 392 , 13 ) , style = 0 ) \n 
self . rbCreateNew . SetValue ( False ) \n 
self . rbCreateNew . Bind ( wx . EVT_RADIOBUTTON , self . OnRbCreateNewRadiobutton , \n 
id = wxID_PNLMETHODSRBCREATENEW ) \n 
self . txtMethodDescrip = wx . richtext . RichTextCtrl ( id = wxID_PNLMETHODSRICHTEXTCTRL1 , \n 
parent = self , pos = wx . Point ( 16 , 224 ) , size = wx . Size ( 392 , 84 ) , \n 
style = wx . richtext . RE_MULTILINE , value = ) \n 
self . txtMethodDescrip . Enable ( False ) \n 
self . txtMethodDescrip . Bind ( wx . EVT_SET_FOCUS , self . OnTxtMethodDescripSetFocus ) \n 
self . txtMethodDescrip . Bind ( wx . EVT_KILL_FOCUS , self . OnTxtMethodDescripKillFocus ) \n 
self . lstMethods = wx . ListCtrl ( id = wxID_PNLMETHODSLISTCTRL1 , \n 
name = , parent = self , pos = wx . Point ( 16 , 48 ) , \n 
size = wx . Size ( 392 , 152 ) , style = wx . LC_REPORT | wx . LC_SINGLE_SEL ) \n 
self . lstMethods . InsertColumn ( 0 , ) \n 
self . lstMethods . InsertColumn ( 1 , ) \n 
self . lstMethods . InsertColumn ( 2 , ) \n 
self . lstMethods . SetColumnWidth ( 0 , 200 ) \n 
self . lstMethods . SetColumnWidth ( 1 , 153 ) \n 
self . lstMethods . SetColumnWidth ( 2 , 0 ) \n 
self . lstMethods . Enable ( False ) \n 
~~ def __init__ ( self , parent , id , pos , size , style , name , sm , method ) : \n 
~~~ self . series_service = sm . get_series_service ( ) \n 
self . prev_val = method \n 
self . _init_ctrls ( parent ) \n 
~~ def OnRbGenerateRadiobutton ( self , event ) : \n 
~~~ self . lstMethods . Enable ( False ) \n 
event . Skip ( ) \n 
~~ def OnRbSelectRadiobutton ( self , event ) : \n 
~~~ self . lstMethods . Enable ( True ) \n 
~~ def OnRbCreateNewRadiobutton ( self , event ) : \n 
self . txtMethodDescrip . Enable ( True ) \n 
~~ def OnTxtMethodDescripSetFocus ( self , event ) : \n 
~~~ self . txtMethodDescrip . SetValue ( "" ) \n 
~~ event . Skip ( ) \n 
~~ def OnTxtMethodDescripKillFocus ( self , event ) : \n 
~~~ if self . txtMethodDescrip . GetValue ( ) == "" : \n 
~~ def getMethod ( self ) : \n 
~~~ m = Method ( ) \n 
if self . rbGenerate . Value : \n 
~~~ m = self . series_service . get_method_by_description ( genmethod ) \n 
~~~ m . description = genmethod \n 
~~ ~~ elif self . rbSelect . Value : \n 
~~~ index = self . lstMethods . GetFirstSelected ( ) \n 
desc = self . lstMethods . GetItem ( index , 0 ) . GetText ( ) \n 
logger . debug ( desc ) \n 
m = self . series_service . get_method_by_description ( desc ) \n 
~~ elif self . rbCreateNew . Value : \n 
~~~ m . description = self . txtMethodDescrip . GetValue ( ) \n 
import wx . xrc \n 
import wx . lib . masked as masked \n 
class clsDataFilters ( wx . Dialog ) : \n 
self . SetSizeHintsSz ( wx . Size ( 358 , 452 ) , wx . DefaultSize ) \n 
bSizer1 = wx . BoxSizer ( wx . VERTICAL ) \n 
bSizer3 = wx . BoxSizer ( wx . VERTICAL ) \n 
bsValueThresh = wx . BoxSizer ( wx . HORIZONTAL ) \n 
self . rbThreshold = wx . RadioButton ( self , wx . ID_ANY , wx . EmptyString , wx . Point ( 10 , 8 ) , wx . DefaultSize self . rbThreshold . SetValue ( True ) \n 
bsValueThresh . Add ( self . rbThreshold , 0 , wx . ALL , 5 ) \n 
fgSizer1 = wx . FlexGridSizer ( 0 , 2 , 0 , 0 ) \n 
fgSizer1 . SetFlexibleDirection ( wx . HORIZONTAL ) \n 
fgSizer1 . SetNonFlexibleGrowMode ( wx . FLEX_GROWMODE_SPECIFIED ) \n 
fgSizer1 . Add ( self . lblChangegt , 0 , wx . ALL , 5 ) \n 
self . txtThreshValGT = wx . TextCtrl ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . Size fgSizer1 . Add ( self . txtThreshValGT , 1 , wx . ALL | wx . EXPAND , 5 ) \n 
fgSizer1 . Add ( self . lblChangelt , 0 , wx . ALL , 5 ) \n 
self . txtThreshValLT = wx . TextCtrl ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . Size fgSizer1 . Add ( self . txtThreshValLT , 1 , wx . ALL | wx . EXPAND , 5 ) \n 
sbThreshold . Add ( fgSizer1 , 0 , wx . EXPAND , 5 ) \n 
bsValueThresh . Add ( sbThreshold , 1 , 0 , 5 ) \n 
bSizer3 . Add ( bsValueThresh , 1 , wx . EXPAND , 5 ) \n 
bsGaps = wx . BoxSizer ( wx . HORIZONTAL ) \n 
self . rbDataGaps = wx . RadioButton ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . DefaultSize bsGaps . Add ( self . rbDataGaps , 1 , wx . ALL , 5 ) \n 
fgSizer2 = wx . FlexGridSizer ( 0 , 2 , 0 , 0 ) \n 
fgSizer2 . SetFlexibleDirection ( wx . HORIZONTAL ) \n 
fgSizer2 . SetNonFlexibleGrowMode ( wx . FLEX_GROWMODE_SPECIFIED ) \n 
self . lblGapsVal = wx . StaticText ( self , wx . ID_ANY , u"Value:" , wx . DefaultPosition , wx . DefaultSize self . lblGapsVal . Wrap ( - 1 ) \n 
fgSizer2 . Add ( self . lblGapsVal , 0 , wx . ALL , 5 ) \n 
self . txtGapsVal = wx . TextCtrl ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . Size ( 230 fgSizer2 . Add ( self . txtGapsVal , 0 , wx . ALL , 5 ) \n 
fgSizer2 . Add ( self . lblGapsTime , 0 , wx . ALL , 5 ) \n 
cbGapTimeChoices = [ u"second" , u"minute" , u"hour" , u"day" ] \n 
self . cbGapTime = wx . ComboBox ( self , wx . ID_ANY , u"second" , wx . DefaultPosition , wx . Size ( 230 , - 1 wx . CB_READONLY ) \n 
fgSizer2 . Add ( self . cbGapTime , 0 , wx . ALL , 5 ) \n 
sbGaps . Add ( fgSizer2 , 1 , wx . EXPAND , 5 ) \n 
bsGaps . Add ( sbGaps , 0 , 0 , 5 ) \n 
bSizer3 . Add ( bsGaps , 0 , 0 , 5 ) \n 
bsDate = wx . BoxSizer ( wx . HORIZONTAL ) \n 
self . rbDate = wx . RadioButton ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . DefaultSize bsDate . Add ( self . rbDate , 0 , wx . ALL , 5 ) \n 
sbDate = wx . StaticBoxSizer ( wx . StaticBox ( self , wx . ID_ANY , u"Date" ) , wx . VERTICAL ) \n 
fgSizer3 = wx . FlexGridSizer ( 0 , 4 , 0 , 0 ) \n 
fgSizer3 . SetFlexibleDirection ( wx . HORIZONTAL ) \n 
fgSizer3 . SetNonFlexibleGrowMode ( wx . FLEX_GROWMODE_SPECIFIED ) \n 
self . lblDateAfter = wx . StaticText ( self , wx . ID_ANY , u"Start:" , wx . DefaultPosition , wx . DefaultSize self . lblDateAfter . Wrap ( - 1 ) \n 
fgSizer3 . Add ( self . lblDateAfter , 0 , wx . ALL , 5 ) \n 
self . dpAfter = wx . DatePickerCtrl ( self , wx . ID_ANY , wx . DefaultDateTime , wx . DefaultPosition , wx wx . DP_DROPDOWN | wx . DP_SHOWCENTURY ) \n 
fgSizer3 . Add ( self . dpAfter , 0 , wx . ALL , 5 ) \n 
self . sbAfter = wx . SpinButton ( self , wx . ID_ANY , wx . DefaultPosition , wx . Size ( 15 , - 1 ) , 0 ) \n 
fmt24hr = True , spinButton = self . sbAfter , oob_color = "White" ) \n 
fgSizer3 . Add ( self . tpAfter , 0 , wx . ALL , 5 ) \n 
fgSizer3 . Add ( self . sbAfter , 0 , wx . ALL , 5 ) \n 
fgSizer3 . Add ( self . lblDateBefore , 0 , wx . ALL , 5 ) \n 
self . dpBefore = wx . DatePickerCtrl ( self , wx . ID_ANY , wx . DefaultDateTime , wx . DefaultPosition , wx wx . DP_DROPDOWN | wx . DP_SHOWCENTURY ) \n 
fgSizer3 . Add ( self . dpBefore , 0 , wx . ALL , 5 ) \n 
fmt24hr = True , spinButton = self . sbBefore , oob_color = ) \n 
fgSizer3 . Add ( self . tpBefore , 0 , wx . ALL , 5 ) \n 
fgSizer3 . Add ( self . sbBefore , 0 , wx . ALL , 5 ) \n 
sbDate . Add ( fgSizer3 , 1 , wx . EXPAND , 5 ) \n 
bsDate . Add ( sbDate , 1 , wx . EXPAND , 5 ) \n 
bSizer3 . Add ( bsDate , 0 , wx . EXPAND , 5 ) \n 
bsValChange = wx . BoxSizer ( wx . HORIZONTAL ) \n 
self . rbVChangeThresh = wx . RadioButton ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx bsValChange . Add ( self . rbVChangeThresh , 0 , wx . ALL , 5 ) \n 
fgSizer4 = wx . FlexGridSizer ( 0 , 2 , 0 , 0 ) \n 
fgSizer4 . SetFlexibleDirection ( wx . HORIZONTAL ) \n 
fgSizer4 . SetNonFlexibleGrowMode ( wx . FLEX_GROWMODE_SPECIFIED ) \n 
fgSizer4 . Add ( self . lblChangeGT , 0 , wx . ALL , 5 ) \n 
self . txtVChangeGT = wx . TextCtrl ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . Size fgSizer4 . Add ( self . txtVChangeGT , 0 , wx . ALL , 5 ) \n 
fgSizer4 . Add ( self . lblChangeLT , 0 , wx . ALL , 5 ) \n 
self . txtVChangeLT = wx . TextCtrl ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . Size fgSizer4 . Add ( self . txtVChangeLT , 0 , wx . ALL , 5 ) \n 
sbValChange . Add ( fgSizer4 , 1 , wx . EXPAND , 5 ) \n 
bsValChange . Add ( sbValChange , 1 , 0 , 5 ) \n 
bSizer3 . Add ( bsValChange , 1 , wx . EXPAND , 5 ) \n 
bSizer3 . Add ( self . chkToggleFilterSelection , 1 , wx . ALL | wx . EXPAND , 5 ) \n 
bsButtons = wx . BoxSizer ( wx . HORIZONTAL ) \n 
self . btnOK = wx . Button ( self , wx . ID_ANY , u"OK" , wx . DefaultPosition , wx . Size ( 64 , 23 ) , 0 ) \n 
bsButtons . Add ( self . btnOK , 1 , wx . ALL | wx . EXPAND , 5 ) \n 
self . btnCancel = wx . Button ( self , wx . ID_ANY , u"Cancel" , wx . DefaultPosition , wx . Size ( 64 , 23 ) , bsButtons . Add ( self . btnCancel , 1 , wx . ALL | wx . EXPAND , 5 ) \n 
self . btnApply = wx . Button ( self , wx . ID_ANY , u"Apply" , wx . DefaultPosition , wx . Size ( 64 , 23 ) , 0 ) bsButtons . Add ( self . btnApply , 1 , wx . ALL | wx . EXPAND , 5 ) \n 
bSizer3 . Add ( bsButtons , 1 , wx . EXPAND , 0 ) \n 
bSizer1 . Add ( bSizer3 , 1 , wx . EXPAND , 5 ) \n 
self . SetSizer ( bSizer1 ) \n 
self . Layout ( ) \n 
self . txtThreshValGT . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . txtThreshValLT . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . txtGapsVal . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . cbGapTime . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . dpAfter . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . tpAfter . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . sbAfter . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . dpBefore . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . tpBefore . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . sbBefore . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . txtVChangeGT . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . txtVChangeLT . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . chkToggleFilterSelection . Bind ( wx . EVT_CHECKBOX , self . onCheckBox ) \n 
self . btnClear . Bind ( wx . EVT_BUTTON , self . onBtnClearButton ) \n 
self . btnOK . Bind ( wx . EVT_BUTTON , self . onBtnOKButton ) \n 
self . btnCancel . Bind ( wx . EVT_BUTTON , self . onBtnCancelButton ) \n 
self . btnApply . Bind ( wx . EVT_BUTTON , self . onBtnApplyButton ) \n 
~~ def onSetFocus ( self , event ) : \n 
~~~ event . Skip ( ) \n 
~~ def onCheckBox ( self , event ) : \n 
~~ def onBtnClearButton ( self , event ) : \n 
~~ def onBtnOKButton ( self , event ) : \n 
~~ def onBtnCancelButton ( self , event ) : \n 
~~ def onBtnApplyButton ( self , event ) : \n 
~~ ~~ from odmtools . odmdata import SessionFactory \n 
class TestSessionFactory : \n 
~~~ self . connection_string = "sqlite:///:memory:" \n 
self . session_factory = SessionFactory ( self . connection_string , echo = True ) \n 
~~ def test_create_session_factory ( self ) : \n 
~~~ assert repr ( self . session_factory ) == "<SessionFactory(\'Engine(%s)\')>" % self . connection_string assert self . session_factory . Session != None \n 
~~ def test_get_session ( self ) : \n 
~~~ session = self . session_factory . get_session ( ) \n 
class DataTypeError ( TypeError ) : \n 
~~ class DimTypeError ( TypeError ) : \n 
~~ class ArityTypeError ( TypeError ) : \n 
~~ class IndexTypeError ( TypeError ) : \n 
~~ class NameTypeError ( TypeError ) : \n 
~~ class SetTypeError ( TypeError ) : \n 
~~ class SizeTypeError ( TypeError ) : \n 
~~ class SubsetIndexOutOfBounds ( TypeError ) : \n 
~~ class SparsityTypeError ( TypeError ) : \n 
~~ class MapTypeError ( TypeError ) : \n 
~~ class DataSetTypeError ( TypeError ) : \n 
~~ class MatTypeError ( TypeError ) : \n 
~~ class DatTypeError ( TypeError ) : \n 
~~ class KernelTypeError ( TypeError ) : \n 
~~ class DataValueError ( ValueError ) : \n 
~~ class IndexValueError ( ValueError ) : \n 
~~ class ModeValueError ( ValueError ) : \n 
~~ class IterateValueError ( ValueError ) : \n 
~~ class SetValueError ( ValueError ) : \n 
~~ class MapValueError ( ValueError ) : \n 
~~ class ConfigurationError ( RuntimeError ) : \n 
~~ class CompilationError ( RuntimeError ) : \n 
~~ import pytest \n 
from random import randrange \n 
from pyop2 import plan as _plan \n 
from pyop2 import op2 \n 
backends = [ , ] \n 
valuetype = numpy . float64 \n 
NUM_ELE = 12 \n 
NUM_NODES = 36 \n 
NUM_ENTRIES = 4 \n 
class TestColoring : \n 
def nodes ( cls ) : \n 
~~~ return op2 . Set ( NUM_NODES , "nodes" ) \n 
def elements ( cls ) : \n 
~~~ return op2 . Set ( NUM_ELE , "elements" ) \n 
def dnodes ( cls , nodes ) : \n 
~~~ return op2 . DataSet ( nodes , 1 , "dnodes" ) \n 
def elem_node_map ( cls ) : \n 
~~~ v = [ randrange ( NUM_ENTRIES ) for i in range ( NUM_ELE * 3 ) ] \n 
return numpy . asarray ( v , dtype = numpy . uint32 ) \n 
def elem_node ( cls , elements , nodes , elem_node_map ) : \n 
~~~ return op2 . Map ( elements , nodes , 3 , elem_node_map , "elem_node" ) \n 
def mat ( cls , elem_node , dnodes ) : \n 
~~~ sparsity = op2 . Sparsity ( ( dnodes , dnodes ) , ( elem_node , elem_node ) , "sparsity" ) \n 
return op2 . Mat ( sparsity , valuetype , "mat" ) \n 
def x ( cls , dnodes ) : \n 
~~~ return op2 . Dat ( dnodes , numpy . zeros ( NUM_NODES , dtype = numpy . uint32 ) , numpy . uint32 , "x" ) \n 
plan = _plan . Plan ( elements . all_part , \n 
mat ( op2 . INC , ( elem_node [ op2 . i [ 0 ] ] , \n 
elem_node [ op2 . i [ 1 ] ] ) ) , \n 
x ( op2 . WRITE , elem_node [ 0 ] ) , \n 
partition_size = NUM_ELE / 2 , \n 
matrix_coloring = True ) \n 
assert plan . nblocks == 2 \n 
eidx = 0 \n 
for p in range ( plan . nblocks ) : \n 
~~~ for thrcol in range ( plan . nthrcol [ p ] ) : \n 
~~~ counter = numpy . zeros ( NUM_NODES , dtype = numpy . uint32 ) \n 
for e in range ( eidx , eidx + plan . nelems [ p ] ) : \n 
~~~ if plan . thrcol [ e ] == thrcol : \n 
~~~ counter [ elem_node . values [ e ] [ 0 ] ] += 1 \n 
~~ ~~ assert ( counter < 2 ) . all ( ) \n 
~~ eidx += plan . nelems [ p ] \n 
from matplotlib import pyplot as plt \n 
import oscaar \n 
import astrometry \n 
import photometry \n 
import dataBank \n 
import systematics \n 
import IO \n 
import pyfits \n 
plt . ion ( ) \n 
data = dataBank . dataBank ( ) \n 
allStars = data . getDict ( ) \n 
outputPath = data . outputPath \n 
N_exposures = len ( data . getPaths ( ) ) \n 
meanDarkFrame = data . getMeanDarkFrame ( ) \n 
masterFlat = data . masterFlat \n 
plottingThings , statusBarFig , statusBarAx = IO . plottingSettings ( data . trackPlots , data . photPlots ) \n 
for expNumber in xrange ( N_exposures ) : \n 
~~~ if statusBarAx is not None and expNumber % 15 == 0 : \n 
~~~ plt . cla ( ) \n 
statusBarAx . set_title ( ) \n 
statusBarAx . set_xlim ( [ 0 , 100 ] ) \n 
statusBarAx . set_xlabel ( ) \n 
statusBarAx . get_yaxis ( ) . set_ticks ( [ ] ) \n 
statusBarAx . barh ( [ 0 ] , [ 100.0 * expNumber / len ( data . getPaths ( ) ) ] , \n 
[ 1 ] , color = ) \n 
~~ image = ( pyfits . getdata ( data . getPaths ( ) [ expNumber ] ) - meanDarkFrame ) / masterFlat \n 
data . storeTime ( expNumber ) \n 
for star in allStars : \n 
~~~ est_x , est_y = data . centroidInitialGuess ( expNumber , star ) \n 
x , y , radius , trackFlag = astrometry . trackSmooth ( image , est_x , est_y , \n 
data . smoothConst , \n 
plottingThings , \n 
zoom = data . trackingZoom , \n 
plots = data . trackPlots ) \n 
data . storeCentroid ( star , expNumber , x , y ) \n 
fluxes , errors , photFlags = photometry . multirad ( image , x , y , \n 
data . apertureRadii , \n 
ccdGain = data . ccdGain , \n 
plots = data . photPlots ) \n 
photFlag = any ( photFlags ) \n 
data . storeFluxes ( star , expNumber , fluxes , errors ) \n 
if trackFlag or photFlag and not data . getFlag ( ) : \n 
~~~ data . setFlag ( star , False ) \n 
~~ if data . trackPlots or data . photPlots : \n 
~~~ plt . draw ( ) \n 
~~ ~~ if statusBarAx is not None and expNumber % 15 == 0 : \n 
~~ ~~ plt . close ( ) \n 
data . scaleFluxes_multirad ( ) \n 
meanComparisonStars , meanComparisonStarErrors = data . calcMeanComparison_multirad ( ccdGain = data . ccdGain ) \n 
lightCurves , lightCurveErrors = data . computeLightCurve_multirad ( meanComparisonStars , \n 
meanComparisonStarErrors ) \n 
oscaar . IO . save ( data , outputPath ) \n 
data . plotLightCurve_multirad ( ) \n 
from RESTfulResource import RESTfulResource \n 
class Observers ( RESTfulResource ) : \n 
~~~ RESTfulResource . __init__ ( self ) \n 
self . __schemes = [ , , ] \n 
self . __observers = [ ] \n 
~~ def onUpdate ( self , resource ) : \n 
~~~ self . __onUpdate ( resource ) \n 
~~ def __onUpdate ( self , resource ) : \n 
~~~ for self . __observer in self . __observers : \n 
~~~ self . __notify ( self . __observer , resource ) \n 
~~ ~~ def __notify ( self , observer , resource ) : \n 
~~~ urlObject = urlparse ( observer ) \n 
if urlObject . scheme == : \n 
~~~ self . __httpNotify ( observer , resource ) \n 
~~ elif urlObject . scheme == : \n 
~~~ self . __coapNotify ( observer , resource ) \n 
~~~ self . __callbackNotify ( observer , resource ) \n 
~~~ observer ( resource ) \n 
~~ ~~ def __httpNotify ( self , targetURI , resource ) : \n 
~~ def __coapNotify ( self , targetURI , resource ) : \n 
~~ def __callbackNotify ( self , observer , resource ) : \n 
~~ def get ( self , targetURI = None ) : \n 
~~~ if targetURI != None : \n 
~~~ if targetURI in self . __observers : \n 
~~~ return targetURI \n 
~~ def set ( self , targetURI ) : \n 
~~~ self . create ( targetURI ) \n 
~~ def create ( self , targetURI ) : \n 
~~~ if urlparse ( targetURI ) . scheme not in self . __schemes : \n 
~~ if targetURI not in self . __observers : \n 
~~ return targetURI \n 
~~ def delete ( self , targetURI ) : \n 
~~~ self . __observers . remove ( targetURI ) \n 
return targetURI \n 
from pywechat . excepts import WechatError \n 
class Basic ( object ) : \n 
def __init__ ( self , app_id , app_secret ) : \n 
self . __app_id = app_id \n 
self . __app_secret = app_secret \n 
self . __access_token = self . access_token \n 
self . __token_expires_at = None \n 
def access_token ( self ) : \n 
if self . __access_token and self . __token_expires_at : \n 
~~~ if self . __token_expires_at - time . time ( ) > 60 : \n 
~~~ return self . __access_token \n 
~~ ~~ self . _grant_access_token ( ) \n 
return self . __access_token \n 
~~ def _send_request ( self , method , url , ** kwargs ) : \n 
if not kwargs . get ( ) : \n 
~~~ kwargs [ ] = { \n 
"access_token" : self . access_token \n 
~~ if kwargs . get ( ) : \n 
~~~ data = json . dumps ( kwargs [ ] ) . encode ( ) \n 
kwargs [ "data" ] = data \n 
~~ request = requests . request ( \n 
method = method , \n 
url = url , \n 
** kwargs \n 
request . raise_for_status ( ) \n 
json_data = request . json ( ) \n 
self . _check_wechat_error ( json_data ) \n 
return json_data \n 
def _check_wechat_error ( cls , json_data ) : \n 
errcode = json_data . get ( ) \n 
if errcode and errcode != 0 : \n 
~~~ raise WechatError ( errcode , json_data . get ( ) ) \n 
~~ ~~ def _grant_access_token ( self ) : \n 
"grant_type" : "client_credential" , \n 
"appid" : self . __app_id , \n 
"secret" : self . __app_secret \n 
json_data = self . _send_request ( , url , params = params ) \n 
self . __access_token = json_data . get ( ) \n 
self . __token_expires_at = int ( \n 
time . time ( ) ) + json_data . get ( ) \n 
~~ def _get_wechat_server_ips ( self ) : \n 
url = "https://api.weixin.qq.com/cgi-bin/getcallbackip" \n 
from fabric . api import * \n 
from . deployer . configuration import Configuration \n 
from . deployer . helpers import mkdir , rmdir \n 
from . deployer . standard_packages import package_list \n 
site_settings = { \n 
"settings_module" : , \n 
"settings_local" : , \n 
"application_name" : , \n 
"git_location" : "https://github.com/OfferTeam/OfferListing.git" , \n 
"git_branch" : "develop" , \n 
"static_dir" : "resources/static" , \n 
"media_dir" : "resources/media" , \n 
"requirements_file" : , \n 
def load_configuration ( ) : \n 
~~~ with open ( os . path . join ( os . path . dirname ( os . path . abspath ( __file__ ) ) , ) , ) as f ~~~ configuration_file = . join ( f . readlines ( ) ) \n 
conf = json . JSONDecoder ( ) . decode ( configuration_file ) \n 
env . hosts . append ( conf [ ] ) \n 
env . hosts_data = Configuration ( conf , site_settings ) \n 
~~ load_configuration ( ) \n 
@ task \n 
def install_requirements ( ) : \n 
~~~ sudo ( ) \n 
sudo ( ) \n 
sudo ( . format ( package_list ( ) ) ) \n 
def create_folders ( ) : \n 
~~~ mkdir ( env . hosts_data . base_path ( ) ) \n 
mkdir ( env . hosts_data . log_path ( ) ) \n 
def create_virtual_environment ( ) : \n 
~~~ with cd ( env . hosts_data . base_path ( ) ) : \n 
~~~ run ( . format ( env . hosts_data . virtualenv_path ( ) ) ) \n 
~~~ run ( ) \n 
run ( . format ( env . hosts_data . requirements_path ( ) ) ) \n 
if env . hosts_data . is_mysql ( ) : \n 
~~ ~~ ~~ @ task \n 
def create_local_settings ( ) : \n 
~~~ rmdir ( env . hosts_data . local_settings_path ( ) ) \n 
put ( StringIO ( env . hosts_data . local_settings ( ) ) , env . hosts_data . local_settings_path ( ) ) \n 
def create_gunicorn_config ( ) : \n 
~~~ rmdir ( env . hosts_data . gunicorn_config_path ( ) ) \n 
put ( StringIO ( env . hosts_data . gunicorn_config ( ) ) , env . hosts_data . gunicorn_config_path ( ) ) \n 
sudo ( . format ( env . hosts_data . gunicorn_config_path ( ) ) ) \n 
def create_demo_superuser ( ) : \n 
~~~ with cd ( env . hosts_data . app_path ( ) ) : \n 
~~~ commands = [ \n 
run ( . join ( commands ) ) \n 
def create_gunicorn_supervisor ( ) : \n 
~~~ rmdir ( env . hosts_data . gunicorn_supervisor_config_path ( ) ) \n 
put ( \n 
StringIO ( env . hosts_data . gunicorn_supervisor_config ( ) ) , \n 
env . hosts_data . gunicorn_supervisor_config_path ( ) , \n 
use_sudo = True \n 
def create_celery_supervisor ( ) : \n 
~~~ rmdir ( env . hosts_data . celery_supervisor_config_path ( ) ) \n 
StringIO ( env . hosts_data . celery_supervisor_config ( ) ) , \n 
env . hosts_data . celery_supervisor_config_path ( ) , \n 
def create_nginx_config ( ) : \n 
~~~ put ( StringIO ( env . hosts_data . nginx_config ( ) ) , env . hosts_data . nginx_available_path ( ) , use_sudo = True sudo ( . format ( env . hosts_data . nginx_available_path ( ) , env . hosts_data . nginx_enabled_path sudo ( ) \n 
def delete_nginx_config ( ) : \n 
~~~ rmdir ( env . hosts_data . nginx_enabled_path ( ) , sudo_access = True ) \n 
rmdir ( env . hosts_data . nginx_available_path ( ) , sudo_access = True ) \n 
def migrate_database ( ) : \n 
def collect_static ( ) : \n 
def delete_folders ( ) : \n 
~~~ rmdir ( env . hosts_data . base_path ( ) ) \n 
def delete_gunicorn_supervisor ( ) : \n 
~~~ server_stop ( ) \n 
rmdir ( env . hosts_data . gunicorn_supervisor_config_path ( ) , sudo_access = True ) \n 
def delete_celery_supervisor ( ) : \n 
rmdir ( env . hosts_data . celery_supervisor_config_path ( ) , sudo_access = True ) \n 
~~ @ task ( default = True ) \n 
def make_deploy ( ) : \n 
~~~ install_requirements ( ) \n 
create_folders ( ) \n 
run ( env . hosts_data . git_clone_command ( ) ) \n 
with cd ( env . hosts_data . app_path ( ) ) : \n 
~~~ run ( env . hosts_data . git_checkout_command ( ) ) \n 
~~ create_virtual_environment ( ) \n 
create_local_settings ( ) \n 
migrate_database ( ) \n 
create_demo_superuser ( ) \n 
collect_static ( ) \n 
create_gunicorn_config ( ) \n 
create_gunicorn_supervisor ( ) \n 
create_celery_supervisor ( ) \n 
create_nginx_config ( ) \n 
def server_status ( ) : \n 
~~~ sudo ( . format ( env . hosts_data . application_name ( ) ) ) \n 
def server_stop ( ) : \n 
sudo ( . format ( env . hosts_data . application_name ( ) ) ) \n 
def server_start ( ) : \n 
def server_restart ( ) : \n 
def destroy_deploy ( ) : \n 
~~~ delete_folders ( ) \n 
delete_gunicorn_supervisor ( ) \n 
delete_celery_supervisor ( ) \n 
delete_nginx_config ( ) \n 
def update_deploy ( ) : \n 
~~ ~~ migrate_database ( ) \n 
server_start ( ) \n 
~~ from django . conf . urls import patterns , url , include \n 
from haystack . views import search_view_factory , SearchView \n 
from offers . feeds import OfferFeed , OfferAtomFeed \n 
from offers . forms import OfferSearchForm \n 
from . import views as offer_views \n 
url ( , , name = "like" ) , \n 
url ( , search_view_factory ( \n 
view_class = SearchView , \n 
form_class = OfferSearchForm , \n 
template = , \n 
results_per_page = 8 , \n 
url ( , OfferFeed ( ) , name = ) , \n 
url ( , OfferAtomFeed ( ) , name = ) , \n 
url ( , , name = "admin_home" ) , \n 
url ( , , name = "admin_requests" ) , \n 
url ( , , name = "admin_request_new" ) , \n 
url ( , , name = "admin_request_edit" ) , \n 
url ( , , name = "admin_request_delete" url ( , , name = "admin_request_mark" url ( , , name = "admin_request_preview" \n 
url ( , , name = "admin_offers" ) , \n 
url ( , , name = "admin_offer" ) , \n 
url ( , , name = "admin_offer_mark" url ( \n 
name = "admin_offer_plan_mark" \n 
url ( , , name = "admin_offer_update" \n 
url ( , , name = "admin_locations" ) , \n 
url ( , , name = "admin_location_edit" url ( , , name = "admin_location_new" ) , \n 
__productname__ = \n 
__version__ = "0.025" \n 
__author_email__ = "nicolas.s-dev@laposte.net" \n 
__homepage__ = "http://github.com/OfflineIMAP/imapfw" \n 
from imapfw import runtime \n 
from . manager import Manager \n 
class FolderManager ( Manager ) : \n 
~~~ super ( FolderManager , self ) . __init__ ( ) \n 
self . rascal = runtime . rascal \n 
from . . model . hashes import Hashes \n 
from . . one_drive_object_base import OneDriveObjectBase \n 
class File ( OneDriveObjectBase ) : \n 
~~~ def __init__ ( self , prop_dict = { } ) : \n 
~~~ self . _prop_dict = prop_dict \n 
def hashes ( self ) : \n 
if "hashes" in self . _prop_dict : \n 
~~~ if isinstance ( self . _prop_dict [ "hashes" ] , OneDriveObjectBase ) : \n 
~~~ return self . _prop_dict [ "hashes" ] \n 
~~~ self . _prop_dict [ "hashes" ] = Hashes ( self . _prop_dict [ "hashes" ] ) \n 
return self . _prop_dict [ "hashes" ] \n 
~~ @ hashes . setter \n 
def hashes ( self , val ) : \n 
~~~ self . _prop_dict [ "hashes" ] = val \n 
def mime_type ( self ) : \n 
if "mimeType" in self . _prop_dict : \n 
~~~ return self . _prop_dict [ "mimeType" ] \n 
~~ ~~ @ mime_type . setter \n 
def mime_type ( self , val ) : \n 
~~~ self . _prop_dict [ "mimeType" ] = val \n 
class RequestBuilderBase ( object ) : \n 
~~~ def __init__ ( self , request_url , client ) : \n 
self . _request_url = request_url \n 
self . _client = client \n 
~~ def append_to_request_url ( self , url_segment ) : \n 
return self . _request_url + "/" + url_segment \n 
from . . collection_base import CollectionRequestBase , CollectionResponseBase , CollectionPageBase \n 
from . . request_builder_base import RequestBuilderBase \n 
from . . model . item import Item \n 
class SharedCollectionRequest ( CollectionRequestBase ) : \n 
~~~ def __init__ ( self , request_url , client , options ) : \n 
super ( SharedCollectionRequest , self ) . __init__ ( request_url , client , options ) \n 
~~ def get ( self ) : \n 
self . method = "GET" \n 
collection_response = SharedCollectionResponse ( json . loads ( self . send ( ) . content ) ) \n 
return self . _page_from_response ( collection_response ) \n 
~~ ~~ class SharedCollectionRequestBuilder ( RequestBuilderBase ) : \n 
~~~ def __getitem__ ( self , key ) : \n 
return ItemRequestBuilder ( self . append_to_request_url ( str ( key ) ) , self . _client ) \n 
~~ def request ( self , expand = None , select = None , top = None , order_by = None , options = None ) : \n 
req = SharedCollectionRequest ( self . _request_url , self . _client , options ) \n 
req . _set_query_options ( expand = expand , select = select , top = top , order_by = order_by ) \n 
return req \n 
return self . request ( ) . get ( ) \n 
~~ ~~ class SharedCollectionResponse ( CollectionResponseBase ) : \n 
~~~ @ property \n 
def collection_page ( self ) : \n 
if self . _collection_page : \n 
~~~ self . _collection_page . _prop_list = self . _prop_dict [ "value" ] \n 
~~~ self . _collection_page = SharedCollectionPage ( self . _prop_dict [ "value" ] ) \n 
~~ return self . _collection_page \n 
~~ ~~ class SharedCollectionPage ( CollectionPageBase ) : \n 
~~~ def __getitem__ ( self , index ) : \n 
return Item ( self . _prop_list [ index ] ) \n 
~~ def shared ( self ) : \n 
for item in self . _prop_list : \n 
~~~ yield Item ( item ) \n 
~~ ~~ def _init_next_page_request ( self , next_page_link , client , options ) : \n 
self . _next_page_request = SharedCollectionRequest ( next_page_link , client , options ) \n 
~~ ~~ from . . request . item_request_builder import ItemRequestBuilder \n 
from cffi import FFI \n 
from cffi . verifier import Verifier \n 
__all__ = [ "ffi" ] \n 
HEADERS = glob . glob ( \n 
os . path . join ( os . path . abspath ( os . path . dirname ( __file__ ) ) , "*.h" ) \n 
ffi = FFI ( ) \n 
for header in sorted ( HEADERS ) : \n 
~~~ with open ( header , "r" ) as hfile : \n 
~~~ ffi . cdef ( hfile . read ( ) ) \n 
~~ ~~ ffi . verifier = Verifier ( \n 
ffi , \n 
libraries = [ "sodium" ] , \n 
ext_package = "nacl._lib" , \n 
class Library ( object ) : \n 
~~~ def __init__ ( self , ffi ) : \n 
~~~ self . ffi = ffi \n 
self . _lib = None \n 
def _compile_module ( * args , ** kwargs ) : \n 
~~ self . ffi . verifier . compile_module = _compile_module \n 
~~~ if self . _lib is None : \n 
~~~ self . _lib = self . ffi . verifier . load_library ( ) \n 
~~ return getattr ( self . _lib , name ) \n 
~~ ~~ lib = Library ( ffi ) \n 
import sqlite3 \n 
def migrate ( database_path ) : \n 
conn = sqlite3 . connect ( database_path ) \n 
conn . text_factory = str \n 
cursor = conn . cursor ( ) \n 
cursor . execute ( ) \n 
notifications = cursor . fetchall ( ) \n 
for n in notifications : \n 
~~~ cursor . execute ( , ( n [ 0 ] , n [ 1 ] , n [ 2 ] , n [ 3 ] , n [ 4 ] , n [ 5 ] , n [ 6 ] , n [ 7 ] , \n 
~~ cursor . execute ( ) \n 
conn . commit ( ) \n 
conn . close ( ) \n 
DEBUG = 5 \n 
WARNING = 4 \n 
INFO = 3 \n 
ERROR = 2 \n 
CRITICAL = 1 \n 
levels = { "debug" : 5 , "warning" : 4 , "info" : 3 , "error" : 2 , "critical" : 1 } \n 
class FileLogObserver ( log . FileLogObserver ) : \n 
~~~ def __init__ ( self , f = None , level = "info" , default = DEBUG ) : \n 
~~~ log . FileLogObserver . __init__ ( self , f or sys . stdout ) \n 
self . level = levels [ level ] \n 
self . default = default \n 
~~ def emit ( self , eventDict ) : \n 
~~~ ll = eventDict . get ( , self . default ) \n 
if eventDict [ ] or in eventDict or self . level >= ll : \n 
~~~ log . FileLogObserver . emit ( self , eventDict ) \n 
~~ ~~ ~~ class Logger ( object ) : \n 
~~~ def __init__ ( self , ** kwargs ) : \n 
~~~ self . kwargs = kwargs \n 
~~ def msg ( self , message , ** kw ) : \n 
~~~ kw . update ( self . kwargs ) \n 
if in kw and not isinstance ( kw [ ] , str ) : \n 
~~~ kw [ ] = kw [ ] . __class__ . __name__ \n 
~~ log . msg ( message , ** kw ) \n 
~~ def info ( self , message , ** kw ) : \n 
~~~ kw [ ] = INFO \n 
~~ def debug ( self , message , ** kw ) : \n 
~~~ kw [ ] = DEBUG \n 
~~ def warning ( self , message , ** kw ) : \n 
~~~ kw [ ] = WARNING \n 
~~ def error ( self , message , ** kw ) : \n 
~~~ kw [ ] = ERROR \n 
~~ def critical ( self , message , ** kw ) : \n 
~~~ kw [ ] = CRITICAL \n 
~~~ theLogger \n 
~~~ theLogger = Logger ( ) \n 
msg = theLogger . msg \n 
info = theLogger . info \n 
debug = theLogger . debug \n 
warning = theLogger . warning \n 
error = theLogger . error \n 
critical = theLogger . critical \n 
~~ import sys \n 
_b = sys . version_info [ 0 ] < 3 and ( lambda x : x ) or ( lambda x : x . encode ( ) ) \n 
from google . protobuf import symbol_database as _symbol_database \n 
_sym_db = _symbol_database . Default ( ) \n 
_sym_db . RegisterFileDescriptor ( DESCRIPTOR ) \n 
_PEERSEEDS = _descriptor . Descriptor ( \n 
number = 1 , type = 12 , cpp_type = 9 , label = 3 , \n 
number = 2 , type = 12 , cpp_type = 9 , label = 2 , \n 
has_default_value = False , default_value = _b ( "" ) , \n 
oneofs = [ \n 
serialized_start = 15 , \n 
serialized_end = 69 , \n 
DESCRIPTOR . message_types_by_name [ ] = _PEERSEEDS \n 
PeerSeeds = _reflection . GeneratedProtocolMessageType ( , ( _message . Message , ) , dict ( \n 
DESCRIPTOR = _PEERSEEDS , \n 
__module__ = \n 
_sym_db . RegisterMessage ( PeerSeeds ) \n 
from kmip . core . enums import Tags \n 
from kmip . core . primitives import Struct \n 
from kmip . core . primitives import ByteString \n 
from kmip . core . utils import BytearrayStream \n 
class RawKey ( ByteString ) : \n 
~~~ def __init__ ( self , value = None ) : \n 
~~~ super ( RawKey , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n 
~~ ~~ class OpaqueKey ( ByteString ) : \n 
~~~ super ( OpaqueKey , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n 
~~ ~~ class PKCS1Key ( ByteString ) : \n 
~~~ super ( PKCS1Key , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n 
~~ ~~ class PKCS8Key ( ByteString ) : \n 
~~~ super ( PKCS8Key , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n 
~~ ~~ class X509Key ( ByteString ) : \n 
~~~ super ( X509Key , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n 
~~ ~~ class ECPrivateKey ( ByteString ) : \n 
~~~ super ( ECPrivateKey , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n 
~~ ~~ class TransparentSymmetricKey ( Struct ) : \n 
~~~ class Key ( ByteString ) : \n 
~~~ super ( TransparentSymmetricKey . Key , self ) . __init__ ( value , Tags . KEY ) \n 
~~ ~~ def __init__ ( self , key = None ) : \n 
~~~ super ( TransparentSymmetricKey , self ) . __init__ ( Tags . KEY_MATERIAL ) \n 
self . validate ( ) \n 
~~ def read ( self , istream ) : \n 
~~~ super ( TransparentSymmetricKey , self ) . read ( istream ) \n 
tstream = BytearrayStream ( istream . read ( self . length ) ) \n 
self . key = TransparentSymmetricKey . Key ( ) \n 
self . key . read ( tstream ) \n 
self . is_oversized ( tstream ) \n 
~~ def write ( self , ostream ) : \n 
~~~ tstream = BytearrayStream ( ) \n 
self . key . write ( tstream ) \n 
self . length = tstream . length ( ) \n 
super ( TransparentSymmetricKey , self ) . write ( ostream ) \n 
ostream . write ( tstream . buffer ) \n 
~~~ self . __validate ( ) \n 
~~ def __validate ( self ) : \n 
from kmip . core import enums \n 
from kmip . demos import utils \n 
from kmip . pie import client \n 
from kmip . pie import objects \n 
~~~ logger = utils . build_console_logger ( logging . INFO ) \n 
parser = utils . build_cli_parser ( ) \n 
opts , args = parser . parse_args ( sys . argv [ 1 : ] ) \n 
config = opts . config \n 
value = \n 
opaque_type = enums . OpaqueDataType . NONE \n 
obj = objects . OpaqueObject ( value , opaque_type , name ) \n 
with client . ProxyKmipClient ( config = config ) as client : \n 
~~~ uid = client . register ( obj ) \n 
"{0}" . format ( uid ) ) \n 
~~~ logger . error ( e ) \n 
from six . moves import configparser \n 
from kmip . core import exceptions \n 
class KmipServerConfig ( object ) : \n 
self . _logger = logging . getLogger ( ) \n 
self . settings = dict ( ) \n 
self . _expected_settings = [ \n 
~~ def set_setting ( self , setting , value ) : \n 
if setting not in self . _expected_settings : \n 
~~~ raise exceptions . ConfigurationError ( \n 
~~ if setting == : \n 
~~~ self . _set_hostname ( value ) \n 
~~ elif setting == : \n 
~~~ self . _set_port ( value ) \n 
~~~ self . _set_certificate_path ( value ) \n 
~~~ self . _set_key_path ( value ) \n 
~~~ self . _set_ca_path ( value ) \n 
~~~ self . _set_auth_suite ( value ) \n 
~~ ~~ def load_settings ( self , path ) : \n 
if not os . path . exists ( path ) : \n 
"located." . format ( path ) \n 
~~ self . _logger . info ( \n 
parser = configparser . SafeConfigParser ( ) \n 
parser . read ( path ) \n 
self . _parse_settings ( parser ) \n 
~~ def _parse_settings ( self , parser ) : \n 
~~~ if not parser . has_section ( ) : \n 
"section." \n 
~~ settings = [ x [ 0 ] for x in parser . items ( ) ] \n 
for setting in settings : \n 
~~~ if setting not in self . _expected_settings : \n 
~~ ~~ for setting in self . _expected_settings : \n 
~~~ if setting not in settings : \n 
"file." . format ( setting ) \n 
~~ ~~ if parser . has_option ( , ) : \n 
~~~ self . _set_hostname ( parser . get ( , ) ) \n 
~~ if parser . has_option ( , ) : \n 
~~~ self . _set_port ( parser . getint ( , ) ) \n 
~~~ self . _set_certificate_path ( parser . get ( \n 
~~~ self . _set_key_path ( parser . get ( , ) ) \n 
~~~ self . _set_ca_path ( parser . get ( , ) ) \n 
~~~ self . _set_auth_suite ( parser . get ( , ) ) \n 
~~ ~~ def _set_hostname ( self , value ) : \n 
~~~ if isinstance ( value , six . string_types ) : \n 
~~~ self . settings [ ] = value \n 
~~ ~~ def _set_port ( self , value ) : \n 
~~~ if isinstance ( value , six . integer_types ) : \n 
~~~ if 0 < value < 65535 : \n 
~~ ~~ def _set_certificate_path ( self , value ) : \n 
~~~ self . settings [ ] = None \n 
~~ elif isinstance ( value , six . string_types ) : \n 
~~~ if os . path . exists ( value ) : \n 
~~ ~~ def _set_key_path ( self , value ) : \n 
~~ ~~ def _set_ca_path ( self , value ) : \n 
~~ ~~ def _set_auth_suite ( self , value ) : \n 
~~~ auth_suites = [ , ] \n 
if value not in auth_suites : \n 
~~ ~~ ~~ from six . moves import xrange \n 
from testtools import TestCase \n 
from kmip . core import utils \n 
from kmip . core . messages . contents import ProtocolVersion \n 
from kmip . core . messages . payloads import discover_versions \n 
class TestDiscoverVersionsRequestPayload ( TestCase ) : \n 
~~~ super ( TestDiscoverVersionsRequestPayload , self ) . setUp ( ) \n 
self . protocol_versions_empty = list ( ) \n 
self . protocol_versions_one = list ( ) \n 
self . protocol_versions_one . append ( ProtocolVersion . create ( 1 , 0 ) ) \n 
self . protocol_versions_two = list ( ) \n 
self . protocol_versions_two . append ( ProtocolVersion . create ( 1 , 1 ) ) \n 
self . protocol_versions_two . append ( ProtocolVersion . create ( 1 , 0 ) ) \n 
self . encoding_empty = utils . BytearrayStream ( ( \n 
self . encoding_one = utils . BytearrayStream ( ( \n 
self . encoding_two = utils . BytearrayStream ( ( \n 
~~~ super ( TestDiscoverVersionsRequestPayload , self ) . tearDown ( ) \n 
~~ def test_init_with_none ( self ) : \n 
~~~ discover_versions . DiscoverVersionsRequestPayload ( ) \n 
~~ def test_init_with_args ( self ) : \n 
~~~ discover_versions . DiscoverVersionsRequestPayload ( \n 
self . protocol_versions_empty ) \n 
~~ def test_validate_with_invalid_protocol_versions ( self ) : \n 
~~~ kwargs = { : } \n 
self . assertRaisesRegexp ( \n 
discover_versions . DiscoverVersionsRequestPayload , ** kwargs ) \n 
~~ def test_validate_with_invalid_protocol_version ( self ) : \n 
~~~ kwargs = { : [ ] } \n 
~~ def _test_read ( self , stream , payload , protocol_versions ) : \n 
~~~ payload . read ( stream ) \n 
expected = len ( protocol_versions ) \n 
observed = len ( payload . protocol_versions ) \n 
expected , observed ) \n 
self . assertEqual ( expected , observed , msg ) \n 
for i in xrange ( len ( protocol_versions ) ) : \n 
~~~ expected = protocol_versions [ i ] \n 
observed = payload . protocol_versions [ i ] \n 
~~ ~~ def test_read_with_empty_protocol_list ( self ) : \n 
~~~ stream = self . encoding_empty \n 
payload = discover_versions . DiscoverVersionsRequestPayload ( ) \n 
protocol_versions = self . protocol_versions_empty \n 
self . _test_read ( stream , payload , protocol_versions ) \n 
~~ def test_read_with_one_protocol_version ( self ) : \n 
~~~ stream = self . encoding_one \n 
protocol_versions = self . protocol_versions_one \n 
~~ def test_read_with_two_protocol_versions ( self ) : \n 
~~~ stream = self . encoding_two \n 
protocol_versions = self . protocol_versions_two \n 
~~ def _test_write ( self , payload , expected ) : \n 
~~~ stream = utils . BytearrayStream ( ) \n 
payload . write ( stream ) \n 
length_expected = len ( expected ) \n 
length_received = len ( stream ) \n 
length_expected , length_received ) \n 
self . assertEqual ( length_expected , length_received , msg ) \n 
msg += ";\\nexpected:\\n{0}\\nreceived:\\n{1}" . format ( expected , stream ) \n 
self . assertEqual ( expected , stream , msg ) \n 
~~ def test_write_with_empty_protocol_list ( self ) : \n 
~~~ payload = discover_versions . DiscoverVersionsRequestPayload ( \n 
expected = self . encoding_empty \n 
self . _test_write ( payload , expected ) \n 
~~ def test_write_with_one_protocol_version ( self ) : \n 
self . protocol_versions_one ) \n 
expected = self . encoding_one \n 
~~ def test_write_with_two_protocol_versions ( self ) : \n 
self . protocol_versions_two ) \n 
expected = self . encoding_two \n 
~~ ~~ class TestDiscoverVersionsResponsePayload ( TestCase ) : \n 
~~~ super ( TestDiscoverVersionsResponsePayload , self ) . setUp ( ) \n 
~~~ super ( TestDiscoverVersionsResponsePayload , self ) . tearDown ( ) \n 
~~~ discover_versions . DiscoverVersionsResponsePayload ( ) \n 
~~~ discover_versions . DiscoverVersionsResponsePayload ( \n 
discover_versions . DiscoverVersionsResponsePayload , ** kwargs ) \n 
payload = discover_versions . DiscoverVersionsResponsePayload ( ) \n 
~~~ payload = discover_versions . DiscoverVersionsResponsePayload ( \n 
~~ ~~ import binascii \n 
import testtools \n 
from kmip . pie . objects import ManagedObject , OpaqueObject \n 
from kmip . pie import sqltypes \n 
from sqlalchemy import create_engine \n 
from sqlalchemy . orm import sessionmaker \n 
class TestOpaqueObject ( testtools . TestCase ) : \n 
~~~ super ( TestOpaqueObject , self ) . setUp ( ) \n 
self . bytes_a = ( \n 
self . bytes_b = ( \n 
self . engine = create_engine ( , echo = True ) \n 
sqltypes . Base . metadata . create_all ( self . engine ) \n 
~~~ super ( TestOpaqueObject , self ) . tearDown ( ) \n 
obj = OpaqueObject ( \n 
self . bytes_a , enums . OpaqueDataType . NONE ) \n 
self . assertEqual ( obj . value , self . bytes_a ) \n 
self . assertEqual ( obj . opaque_type , enums . OpaqueDataType . NONE ) \n 
self . assertEqual ( obj . names , [ ] ) \n 
self . bytes_a , \n 
enums . OpaqueDataType . NONE , \n 
name = ) \n 
~~ def test_get_object_type ( self ) : \n 
expected = enums . ObjectType . OPAQUE_DATA \n 
obj = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
observed = obj . object_type \n 
self . assertEqual ( expected , observed ) \n 
~~ def test_validate_on_invalid_value ( self ) : \n 
args = ( 0 , enums . OpaqueDataType . NONE ) \n 
self . assertRaises ( TypeError , OpaqueObject , * args ) \n 
~~ def test_validate_on_invalid_data_type ( self ) : \n 
args = ( self . bytes_a , ) \n 
~~ def test_validate_on_invalid_name ( self ) : \n 
args = ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
kwargs = { : 0 } \n 
self . assertRaises ( TypeError , OpaqueObject , * args , ** kwargs ) \n 
binascii . hexlify ( self . bytes_a ) , enums . OpaqueDataType . NONE ) \n 
expected = "OpaqueObject({0})" . format ( args ) \n 
observed = repr ( obj ) \n 
~~ def test_str ( self ) : \n 
expected = str ( binascii . hexlify ( self . bytes_a ) ) \n 
observed = str ( obj ) \n 
~~ def test_equal_on_equal ( self ) : \n 
a = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
b = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
self . assertTrue ( a == b ) \n 
self . assertTrue ( b == a ) \n 
~~ def test_equal_on_not_equal_value ( self ) : \n 
b = OpaqueObject ( self . bytes_b , enums . OpaqueDataType . NONE ) \n 
self . assertFalse ( a == b ) \n 
self . assertFalse ( b == a ) \n 
~~ def test_equal_on_not_equal_data_type ( self ) : \n 
b . opaque_type = "invalid" \n 
~~ def test_equal_on_type_mismatch ( self ) : \n 
b = "invalid" \n 
~~ def test_not_equal_on_equal ( self ) : \n 
self . assertFalse ( a != b ) \n 
self . assertFalse ( b != a ) \n 
~~ def test_not_equal_on_not_equal_value ( self ) : \n 
self . assertTrue ( a != b ) \n 
self . assertTrue ( b != a ) \n 
~~ def test_not_equal_on_not_equal_data_type ( self ) : \n 
~~ def test_not_equal_on_type_mismatch ( self ) : \n 
~~ def test_save ( self ) : \n 
Session = sessionmaker ( bind = self . engine ) \n 
session = Session ( ) \n 
session . add ( obj ) \n 
session . commit ( ) \n 
~~ def test_get ( self ) : \n 
test_name = \n 
self . bytes_a , enums . OpaqueDataType . NONE , name = test_name ) \n 
get_obj = session . query ( OpaqueObject ) . filter ( \n 
ManagedObject . unique_identifier == obj . unique_identifier \n 
) . one ( ) \n 
self . assertEqual ( 1 , len ( get_obj . names ) ) \n 
self . assertEqual ( [ test_name ] , get_obj . names ) \n 
self . assertEqual ( self . bytes_a , get_obj . value ) \n 
self . assertEqual ( enums . ObjectType . OPAQUE_DATA , get_obj . object_type ) \n 
self . assertEqual ( enums . OpaqueDataType . NONE , get_obj . opaque_type ) \n 
~~ def test_add_multiple_names ( self ) : \n 
expected_names = [ , , ] \n 
self . bytes_a , enums . OpaqueDataType . NONE , name = expected_names [ 0 ] ) \n 
obj . names . append ( expected_names [ 1 ] ) \n 
obj . names . append ( expected_names [ 2 ] ) \n 
self . assertEquals ( 3 , obj . name_index ) \n 
expected_mo_names = list ( ) \n 
for i , name in enumerate ( expected_names ) : \n 
~~~ expected_mo_names . append ( sqltypes . ManagedObjectName ( name , i ) ) \n 
~~ self . assertEquals ( expected_mo_names , obj . _names ) \n 
self . assertEquals ( expected_mo_names , get_obj . _names ) \n 
~~ def test_remove_name ( self ) : \n 
names = [ , , ] \n 
remove_index = 1 \n 
self . bytes_a , enums . OpaqueDataType . NONE , name = names [ 0 ] ) \n 
obj . names . append ( names [ 1 ] ) \n 
obj . names . append ( names [ 2 ] ) \n 
obj . names . pop ( remove_index ) \n 
expected_names = list ( ) \n 
for i , name in enumerate ( names ) : \n 
~~~ if i != remove_index : \n 
~~~ expected_names . append ( name ) \n 
expected_mo_names . append ( sqltypes . ManagedObjectName ( name , i ) ) \n 
~~ ~~ self . assertEquals ( expected_names , obj . names ) \n 
self . assertEquals ( expected_mo_names , obj . _names ) \n 
self . assertEquals ( expected_names , get_obj . names ) \n 
~~ def test_remove_and_add_name ( self ) : \n 
obj . names . pop ( ) \n 
obj . names . append ( ) \n 
self . assertEquals ( 4 , obj . name_index ) \n 
expected_names = [ , ] \n 
expected_mo_names . append ( sqltypes . ManagedObjectName ( expected_names [ 0 ] , \n 
0 ) ) \n 
expected_mo_names . append ( sqltypes . ManagedObjectName ( expected_names [ 1 ] , \n 
3 ) ) \n 
self . assertEquals ( expected_names , obj . names ) \n 
~~ def test_update_with_add_name ( self ) : \n 
first_name = \n 
self . bytes_a , enums . OpaqueDataType . NONE , name = first_name ) \n 
added_name = \n 
expected_names = [ first_name , added_name ] \n 
~~ session = Session ( ) \n 
update_obj = session . query ( OpaqueObject ) . filter ( \n 
update_obj . names . append ( added_name ) \n 
~~ def test_update_with_remove_name ( self ) : \n 
~~ ~~ session = Session ( ) \n 
update_obj . names . pop ( remove_index ) \n 
~~ def test_update_with_remove_and_add_name ( self ) : \n 
update_obj . names . pop ( ) \n 
update_obj . names . append ( ) \n 
~~ ~~ from setuptools import setup , find_packages \n 
kwargs = { : , \n 
: [ , \n 
: : True , \n 
: { : [ , \n 
] } , \n 
: { : } , \n 
: False } \n 
setup ( ** kwargs ) \n 
import setuptools \n 
from numpy . distutils . core import setup \n 
from numpy . distutils . misc_util import Configuration \n 
include_dirs = [ ] \n 
library_dirs = [ ] \n 
if sys . platform == : \n 
~~~ import types \n 
def _lib_dir_option ( self , dir ) : \n 
~~~ return \'/LIBPATH:"%s"\' % dir \n 
~~ from distutils . msvc9compiler import MSVCCompiler \n 
setattr ( MSVCCompiler , , \n 
types . MethodType ( _lib_dir_option , None , MSVCCompiler ) ) \n 
sdkdir = os . environ . get ( ) \n 
if sdkdir : \n 
~~~ include_dirs . append ( os . path . join ( sdkdir , ) ) \n 
library_dirs . append ( os . path . join ( sdkdir , ) ) \n 
path = os . environ [ ] . split ( ) \n 
path . append ( os . path . join ( sdkdir , ) ) \n 
os . environ [ ] = . join ( path ) \n 
~~ ~~ config = Configuration ( name = ) \n 
config . add_extension ( , \n 
sources = [ , \n 
include_dirs = include_dirs , \n 
library_dirs = library_dirs ) \n 
config . add_data_files ( , ) \n 
kwds = { : [ ] , \n 
: { : [ ] } , \n 
kwds . update ( config . todict ( ) ) \n 
setup ( ** kwds ) \n 
version = \n 
release = \n 
today_fmt = \n 
exclude_trees = [ ] \n 
html_style = \n 
html_last_updated_fmt = \n 
html_theme = "default" \n 
html_theme_options = { \n 
"headtextcolor" : "darkred" , \n 
"headbgcolor" : "gainsboro" , \n 
"headfont" : "Arial" , \n 
"relbarbgcolor" : "black" , \n 
"relbartextcolor" : "white" , \n 
"relbarlinkcolor" : "white" , \n 
"sidebarbgcolor" : "gainsboro" , \n 
"sidebartextcolor" : "darkred" , \n 
"sidebarlinkcolor" : "black" , \n 
"footerbgcolor" : "gainsboro" , \n 
"footertextcolor" : "darkred" , \n 
"textcolor" : "black" , \n 
"codebgcolor" : "#FFFFCC" , \n 
"linkcolor" : "darkred" , \n 
"codebgcolor" : "#ffffcc" , \n 
todo_include_todos = True \n 
intersphinx_mapping = { : None } \n 
autodoc_member_order = \n 
#!/usr/local/bin/python \n 
from dirwalk import includingWalk \n 
from os import system \n 
from subprocess import Popen , PIPE , STDOUT \n 
from compmodtimes import compmodtimes \n 
def resize_image ( fname , max_width = 620 ) : \n 
~~~ im = Image . open ( fname ) \n 
width , height = tuple ( im . getbbox ( ) [ 2 : ] ) \n 
print , height , , width \n 
if width > max_width : \n 
~~~ wrat = max_width / float ( width ) \n 
new_w = int ( width * wrat ) \n 
new_h = int ( height * wrat ) \n 
newim = im . transform ( ( new_w , new_h ) , Image . EXTENT , \n 
im . getbbox ( ) , Image . BICUBIC ) \n 
newim . save ( fname ) \n 
~~ ~~ for diafile in includingWalk ( ".." , [ "*.dia" ] ) : \n 
~~~ pth = os . path . split ( diafile ) \n 
dest = pth [ 1 ] . split ( ) [ 0 ] \n 
retcode = compmodtimes ( diafile , + dest + ) \n 
if retcode == - 1 or retcode == 0 : \n 
~~~ print + dest + \n 
~~~ cmd = + dest + + diafile \n 
system ( cmd ) \n 
resize_image ( os . path . abspath ( os . path . join ( , dest + ) ) ) \n 
from openmdao . main . api import Assembly , Component \n 
from openmdao . lib . datatypes . api import Float \n 
from openmdao . lib . drivers . api import CaseIteratorDriver \n 
from openmdao . lib . components . api import MultiFiMetaModel \n 
from openmdao . lib . surrogatemodels . api import MultiFiCoKrigingSurrogate , KrigingSurrogate \n 
class Model ( Component ) : \n 
~~~ x = Float ( 0 , iotype = "in" ) \n 
f_x = Float ( 0.0 , iotype = "out" ) \n 
def execute ( self ) : \n 
~~~ x = self . x \n 
self . f_x = ( ( 6 * x - 2 ) ** 2 ) * np . sin ( ( 6 * x - 2 ) * 2 ) \n 
~~ ~~ class LowFidelityModel ( Component ) : \n 
~~~ x = Float ( 0.0 , iotype = "in" ) \n 
self . f_x = 0.5 * ( ( 6 * x - 2 ) ** 2 ) * np . sin ( ( 6 * x - 2 ) * 2 ) + ( x - 0.5 ) * 10. - 5 \n 
~~ ~~ class HighFidelityModel ( Model ) : \n 
~~ class CasesBuilder ( Assembly ) : \n 
~~~ def __init__ ( self , model , cases ) : \n 
~~~ self . instance = model \n 
self . cases = cases \n 
super ( CasesBuilder , self ) . __init__ ( ) \n 
~~ def configure ( self ) : \n 
~~~ self . add ( "model" , self . instance ) \n 
self . add ( "driver" , CaseIteratorDriver ( ) ) \n 
self . driver . workflow . add ( ) \n 
self . driver . add_parameter ( "model.x" , low = 0 , high = 1 ) \n 
self . driver . add_response ( "model.f_x" ) \n 
self . driver . case_inputs . model . x = self . cases \n 
self . create_passthrough ( ) \n 
~~ ~~ class Simulation ( Assembly ) : \n 
~~~ def __init__ ( self , surrogate , nfi = 1 ) : \n 
~~~ self . surrogate = surrogate \n 
self . nfi = nfi \n 
super ( Simulation , self ) . __init__ ( ) \n 
~~~ doe_e = [ 0.0 , 0.4 , 0.6 , 1.0 ] \n 
doe_c = [ 0.1 , 0.2 , 0.3 , 0.5 , 0.7 , 0.8 , 0.9 ] + doe_e \n 
self . add ( , CasesBuilder ( HighFidelityModel ( ) , doe_e ) ) \n 
self . add ( , CasesBuilder ( LowFidelityModel ( ) , doe_c ) ) \n 
self . add ( "meta_model" , MultiFiMetaModel ( params = ( , ) , \n 
responses = ( , ) , nfi = self . nfi ) ) \n 
self . meta_model . default_surrogate = self . surrogate \n 
self . connect ( , ) \n 
if self . nfi > 1 : \n 
~~~ self . connect ( , ) \n 
~~ self . add ( , CaseIteratorDriver ( ) ) \n 
self . add ( , Model ( ) ) \n 
self . mm_checker . add_parameter ( "meta_model.x" , low = 0 , high = 1 ) \n 
self . mm_checker . add_parameter ( "model.x" , low = 0 , high = 1 ) \n 
self . mm_checker . add_response ( "model.f_x" ) \n 
self . mm_checker . add_response ( "meta_model.f_x" ) \n 
ngrid = 100 \n 
self . mm_checker . case_inputs . meta_model . x = np . linspace ( 0 , 1 , ngrid ) \n 
self . mm_checker . case_inputs . model . x = np . linspace ( 0 , 1 , ngrid ) \n 
~~~ self . driver . workflow . add ( ) \n 
~~ self . driver . workflow . add ( ) \n 
~~~ surrogate = MultiFiCoKrigingSurrogate ( ) \n 
sim_cok = Simulation ( surrogate , nfi = 2 ) \n 
sim_cok . run ( ) \n 
predicted_cok = np . array ( [ d . mu for d in sim_cok . mm_checker . case_outputs . meta_model . f_x ] ) \n 
sigma_cok = np . array ( [ d . sigma for d in sim_cok . mm_checker . case_outputs . meta_model . f_x ] ) \n 
sim_k = Simulation ( surrogate , nfi = 1 ) \n 
sim_k . run ( ) \n 
predicted_k = np . array ( [ d . mu for d in sim_k . mm_checker . case_outputs . meta_model . f_x ] ) \n 
sigma_k = np . array ( [ d . sigma for d in sim_k . mm_checker . case_outputs . meta_model . f_x ] ) \n 
actual = sim_k . mm_checker . case_outputs . model . f_x \n 
check = sim_k . mm_checker . case_inputs . meta_model . x \n 
import pylab as plt \n 
plt . figure ( 2 ) \n 
plt . ioff ( ) \n 
plt . plot ( check , actual , , label = ) \n 
plt . plot ( check , predicted_cok , , label = ) \n 
plt . plot ( check , predicted_cok + 2 * sigma_cok , , alpha = 0.5 , label = ) \n 
plt . plot ( check , predicted_cok - 2 * sigma_cok , , alpha = 0.5 ) \n 
plt . fill_between ( check , predicted_cok + 2 * sigma_cok , \n 
predicted_cok - 2 * sigma_cok , facecolor = , alpha = 0.2 ) \n 
plt . plot ( check , predicted_k , , label = ) \n 
plt . plot ( check , predicted_k + 2 * sigma_k , , alpha = 0.5 , label = ) \n 
plt . plot ( check , predicted_k - 2 * sigma_k , , alpha = 0.5 ) \n 
plt . fill_between ( check , predicted_k + 2 * sigma_k , \n 
predicted_k - 2 * sigma_k , facecolor = , alpha = 0.2 ) \n 
plt . legend ( loc = ) \n 
plt . show ( ) \n 
error = 0. \n 
for a , p in zip ( actual , predicted_cok ) : \n 
~~~ error += ( a - p ) ** 2 \n 
~~ error = ( error / len ( actual ) ) \n 
for a , p in zip ( actual , predicted_k ) : \n 
import urllib2 \n 
from optparse import OptionParser \n 
def has_setuptools ( ) : \n 
~~~ import setuptools \n 
~~ def make_new_setupfile ( setupfile ) : \n 
setupfile = os . path . abspath ( setupfile ) \n 
newsetupfile = os . path . join ( os . path . dirname ( setupfile ) , \n 
+ os . path . basename ( setupfile ) ) \n 
startdir = os . getcwd ( ) \n 
os . chdir ( os . path . dirname ( setupfile ) ) \n 
if not os . path . isfile ( ) : \n 
resp = urllib2 . urlopen ( ) \n 
with open ( , ) as easyf : \n 
~~~ shutil . copyfileobj ( resp . fp , easyf ) \n 
~~ print \n 
if not os . path . isfile ( setupfile ) : \n 
~~ setupf = open ( setupfile , ) \n 
setup_contents = setupf . read ( ) \n 
setupf . close ( ) \n 
with open ( newsetupfile , ) as newf : \n 
newf . write ( "use_setuptools(download_delay=0)\\n\\n" ) \n 
newf . write ( setup_contents ) \n 
~~ ~~ finally : \n 
~~~ os . chdir ( startdir ) \n 
~~ return newsetupfile \n 
~~ def build_dist ( srcdir , destdir = , build_type = ) : \n 
destdir = os . path . abspath ( os . path . expanduser ( destdir ) ) . replace ( , ) \n 
srcdir = os . path . abspath ( os . path . expanduser ( srcdir ) ) . replace ( , ) \n 
setupname = os . path . join ( srcdir , ) \n 
if not has_setuptools ( ) : \n 
~~~ setupname = make_new_setupfile ( setupname ) \n 
~~ dirfiles = set ( os . listdir ( destdir ) ) \n 
cmd = [ sys . executable . replace ( , ) , \n 
os . path . basename ( setupname ) , \n 
cmd . extend ( build_type . split ( ) ) \n 
cmd . extend ( [ , destdir ] ) \n 
os . chdir ( srcdir ) \n 
out = codecs . open ( , , \n 
encoding = , errors = ) \n 
print % . join ( cmd ) \n 
~~~ p = subprocess . Popen ( . join ( cmd ) , \n 
stdout = out , stderr = subprocess . STDOUT , \n 
shell = True ) \n 
~~~ out . close ( ) \n 
with open ( , ) as f : \n 
~~~ print f . read ( ) \n 
~~ os . chdir ( startdir ) \n 
~~ newfiles = set ( os . listdir ( destdir ) ) - dirfiles \n 
if len ( newfiles ) != 1 : \n 
list ( newfiles ) ) \n 
~~ if p . returncode != 0 : \n 
( srcdir , p . returncode ) ) \n 
~~ distfile = os . path . join ( destdir , newfiles . pop ( ) ) \n 
print % distfile \n 
return distfile \n 
parser . add_option ( "-s" , "--src" , action = "store" , type = , \n 
parser . add_option ( "-d" , "--dest" , action = "store" , type = , \n 
dest = , default = , \n 
parser . add_option ( "-b" , "--bldtype" , action = "store" , type = , \n 
( options , args ) = parser . parse_args ( sys . argv [ 1 : ] ) \n 
retcode = - 1 \n 
if not options . srcdir : \n 
parser . print_help ( ) \n 
sys . exit ( retcode ) \n 
~~ srcdir = os . path . abspath ( os . path . expanduser ( options . srcdir ) ) \n 
destdir = os . path . abspath ( os . path . expanduser ( options . destdir ) ) \n 
if not os . path . exists ( srcdir ) : \n 
~~~ distfile = build_dist ( srcdir , destdir , options . buildtype ) \n 
from openmdao . lib . casehandlers . caseset import CaseArray , CaseSet , caseiter_to_caseset \n 
from openmdao . lib . casehandlers . csvcase import CSVCaseIterator , CSVCaseRecorder \n 
from openmdao . lib . casehandlers . dbcase import DBCaseIterator , DBCaseRecorder , case_db_to_dict \n 
from openmdao . lib . casehandlers . dumpcase import DumpCaseRecorder \n 
from openmdao . lib . casehandlers . jsoncase import JSONCaseRecorder , BSONCaseRecorder , verify_json \n 
from openmdao . lib . casehandlers . listcase import ListCaseRecorder , ListCaseIterator \n 
from openmdao . lib . casehandlers . filters import SequenceCaseFilter , SliceCaseFilter , ExprCaseFilter \n 
from openmdao . lib . casehandlers . query import CaseDataset \n 
from openmdao . lib . casehandlers . csv_post_processor import caseset_query_to_csv \n 
from openmdao . lib . casehandlers . dump_post_processor import caseset_query_dump \n 
from openmdao . lib . casehandlers . html_post_processor import caseset_query_to_html \n 
~~~ from openmdao . lib . casehandlers . query_hdf5 import CaseDatasetHDF5 \n 
from openmdao . lib . casehandlers . hdf5case import HDF5CaseRecorder \n 
~~ from weakref import ref \n 
from openmdao . main . api import VariableTree \n 
from openmdao . lib . casehandlers . query import DictList , ListResult \n 
_GLOBAL_DICT = dict ( __builtins__ = None ) \n 
class CaseDatasetHDF5 ( object ) : \n 
def __init__ ( self , filename , format ) : \n 
~~~ format = format . lower ( ) \n 
~~~ self . _reader = _HDF5Reader ( filename ) \n 
~~ self . _query_id = self . _query_itername = self . _parent_id = self . _parent_itername = self . _driver_id self . _case_ids = self . _drivers = self . _case_iternames = None \n 
self . metadata_names = [ , , , , , , ] \n 
def data ( self ) : \n 
return QueryHDF5 ( self ) \n 
def drivers ( self ) : \n 
return self . _reader . drivers ( ) \n 
def simulation_info ( self ) : \n 
return self . _reader . simulation_info \n 
~~ def _fetch ( self , query ) : \n 
self . _setup ( query ) \n 
if query . vnames : \n 
~~~ tmp = [ ] \n 
for name in self . metadata_names : \n 
~~~ if name in query . vnames : \n 
~~~ tmp . append ( name ) \n 
~~ ~~ self . metadata_names = tmp \n 
names = query . vnames \n 
~~~ if query . driver_name : \n 
~~~ driver_info = self . _drivers [ self . _driver_name ] \n 
prefix = driver_info [ ] \n 
all_names = [ prefix + name \n 
for name in driver_info [ ] ] \n 
~~~ all_names = [ ] \n 
for driver_info in self . _drivers . values ( ) : \n 
~~~ prefix = driver_info [ ] \n 
all_names . extend ( [ prefix + name \n 
for name in driver_info [ ] ] ) \n 
~~ ~~ names = sorted ( all_names + self . metadata_names ) \n 
~~ if query . names : \n 
~~~ return names \n 
~~ nan = float ( ) \n 
rows = ListResult ( ) \n 
for case_data in self . _reader . cases ( ) : \n 
~~~ data = case_data [ ] \n 
metadata = case_data [ ] \n 
case_id = metadata [ ] \n 
case_driver_id = metadata [ ] \n 
case_driver_name = metadata [ ] \n 
case_itername = metadata [ ] \n 
prefix = self . _drivers [ case_driver_name ] [ ] \n 
if prefix : \n 
~~~ data = data . copy ( ) \n 
~~ state . update ( data ) \n 
if self . _driver_name is not None and case_driver_name != self . _driver_name : \n 
#continue \n 
~~ if self . _case_iternames is None or case_itername in self . _case_iternames : \n 
~~~ for name in self . metadata_names : \n 
~~~ data [ name ] = case_data [ ] [ name ] \n 
~~ row = DictList ( names ) \n 
for name in names : \n 
~~~ if query . local_only : \n 
~~~ if name in self . metadata_names : \n 
~~~ row . append ( data [ name ] ) \n 
~~~ driver = self . _drivers [ case_driver_name ] \n 
lnames = [ prefix + rec for rec in driver [ ] ] \n 
if name in lnames : \n 
~~~ row . append ( nan ) \n 
~~ ~~ ~~ elif name in state : \n 
~~~ row . append ( state [ name ] ) \n 
~~ elif name in data : \n 
~~ ~~ rows . append ( row ) \n 
~~ if case_itername == self . _query_itername or case_itername == self . _parent_itername : \n 
~~ ~~ if self . _query_id and not rows : \n 
~~~ raise ValueError ( % self . _query_id ) \n 
~~ if query . transpose : \n 
~~~ tmp = DictList ( names ) \n 
for i in range ( len ( rows [ 0 ] ) ) : \n 
~~~ tmp . append ( [ row [ i ] for row in rows ] ) \n 
~~ tmp . cds = self \n 
return tmp \n 
~~ rows . cds = self \n 
return rows \n 
~~ def _write ( self , query , out , format ) : \n 
~~ def _setup ( self , query ) : \n 
if query . vnames is not None : \n 
~~~ bad = [ ] \n 
metadata = self . simulation_info [ ] \n 
expressions = self . simulation_info [ ] \n 
for name in query . vnames : \n 
~~~ if name not in metadata and name not in [ e [ ] for e in expressions . values ~~~ bad . append ( name ) \n 
~~ ~~ if bad : \n 
~~~ raise RuntimeError ( % bad ) \n 
~~ ~~ self . _drivers = { } \n 
self . _driver_id = None \n 
self . _driver_name = None \n 
for driver_info in self . _reader . drivers ( ) : \n 
~~~ _id = driver_info [ ] \n 
name = driver_info [ ] \n 
prefix , _ , name = name . rpartition ( ) \n 
~~~ prefix += \n 
~~ driver_info [ ] = prefix \n 
self . _drivers [ driver_info [ ] ] = driver_info \n 
if ( driver_info [ ] ) == query . driver_name : \n 
~~~ self . _driver_name = query . driver_name \n 
~~ ~~ if query . driver_name : \n 
~~~ if self . _driver_name is None : \n 
~~~ raise ValueError ( % query . driver_name ) \n 
~~ ~~ self . _case_ids = None \n 
self . _query_id = None \n 
self . _parent_id = None \n 
if query . case_itername is not None : \n 
~~~ self . _query_itername = query . case_itername \n 
self . _case_iternames = set ( ( self . _query_itername , ) ) \n 
~~~ self . _parent_itername = query . parent_itername \n 
self . _case_iternames = set ( ( self . _parent_itername , ) ) \n 
parent_itername_parts = self . _parent_itername . split ( ) \n 
~~~ itername = case_data [ ] [ ] \n 
itername_parts = itername . split ( ) \n 
if len ( parent_itername_parts ) + 1 == len ( itername_parts ) and itername_parts [ : - 1 ] == ~~~ self . _case_iternames . add ( itername ) \n 
~~ ~~ ~~ ~~ def restore ( self , assembly , case_id ) : \n 
~~ def _set ( self , assembly , name , value ) : \n 
if isinstance ( value , dict ) : \n 
~~~ curr = assembly . get ( name ) \n 
if isinstance ( curr , VariableTree ) : \n 
~~~ for key , val in value . items ( ) : \n 
~~~ self . _set ( assembly , . join ( ( name , key ) ) , val ) \n 
~~ ~~ elif in name : \n 
~~~ if isinstance ( value , unicode ) : \n 
~~~ value = str ( value ) \n 
~~ exec ( % name , _GLOBAL_DICT , locals ( ) ) \n 
~~~ if isinstance ( val , unicode ) : \n 
~~~ value [ key ] = str ( val ) \n 
~~ ~~ assembly . set ( name , value ) \n 
~~ if in name : \n 
~~~ exec ( % name , _GLOBAL_DICT , locals ( ) ) \n 
~~~ assembly . set ( name , value ) \n 
~~ ~~ ~~ ~~ class QueryHDF5 ( object ) : \n 
~~~ self . _dataset = dataset \n 
self . driver_name = None \n 
self . case_id = None \n 
self . case_itername = None \n 
self . parent_id = None \n 
self . parent_itername = None \n 
self . vnames = None \n 
self . local_only = False \n 
self . names = False \n 
self . transpose = False \n 
~~ def fetch ( self ) : \n 
return self . _dataset . _fetch ( self ) \n 
~~ def write ( self , out , format = None ) : \n 
#else: \n 
~~ def driver ( self , driver_name ) : \n 
self . driver_name = driver_name \n 
~~ def case ( self , case_itername ) : \n 
self . case_itername = case_itername \n 
~~ def parent_case ( self , parent_case_id ) : \n 
self . parent_id = parent_case_id \n 
self . parent_itername = parent_case_id \n 
~~ def vars ( self , * args ) : \n 
self . vnames = [ ] \n 
~~~ if isinstance ( arg , basestring ) : \n 
~~~ self . vnames . append ( arg ) \n 
~~~ self . vnames . extend ( arg ) \n 
~~ ~~ return self \n 
~~ def local ( self ) : \n 
self . local_only = True \n 
~~ def by_case ( self ) : \n 
~~ def by_variable ( self ) : \n 
self . transpose = True \n 
~~ def var_names ( self ) : \n 
self . names = True \n 
~~ ~~ class _HDF5Reader ( object ) : \n 
def __init__ ( self , filename ) : \n 
self . _inp = h5py . File ( filename , ) \n 
self . _simulation_info = self . read_simulation_info ( ) \n 
self . _state = \n 
self . _info = None \n 
return self . _simulation_info \n 
~~ def read_iteration_case_from_hdf5 ( self , hdf5file , driver_name , iteration_case_name ) : \n 
driver_grp = self . _inp [ ] [ driver_name ] \n 
iteration_grp = self . _inp [ ] [ driver_name ] [ iteration_case_name ] \n 
info [ ] = self . read_from_hdf5 ( iteration_grp [ ] ) \n 
data_grp = iteration_grp [ ] \n 
info [ ] = { } \n 
float_names = driver_grp [ ] \n 
int_names = driver_grp [ ] \n 
str_names = driver_grp [ ] \n 
for i , name in enumerate ( float_names ) : \n 
~~~ info [ ] [ name ] = data_grp [ ] [ i ] \n 
~~ for i , name in enumerate ( str_names ) : \n 
~~ for i , name in enumerate ( int_names ) : \n 
~~ for name in data_grp . keys ( ) : \n 
~~~ if name not in [ , , ] : \n 
~~~ if in data_grp [ name ] . attrs : \n 
~~~ info [ ] [ name ] = { } \n 
for n , v in data_grp [ name ] . items ( ) : \n 
~~~ info [ ] [ name ] [ n ] = self . read_from_hdf5 ( data_grp [ name ] [ n ] ) \n 
~~ ~~ info [ ] [ name ] = self . read_from_hdf5 ( data_grp [ name ] ) \n 
~~ ~~ return info \n 
~~ def read_from_hdf5 ( self , value ) : \n 
if isinstance ( value , h5py . _hl . group . Group ) : \n 
~~~ d = { } \n 
group = value \n 
for name , value in group . attrs . items ( ) : \n 
~~~ d [ name ] = self . read_from_hdf5 ( value ) \n 
~~ for name , value in group . items ( ) : \n 
~~ return d \n 
for name in value . dtype . names : \n 
~~~ d [ name ] = value [ name ] [ 0 ] \n 
~~~ return value [ ( ) ] \n 
~~ ~~ def read_simulation_info ( self ) : \n 
sim_info = { } \n 
for name , value in sim_info_grp . attrs . items ( ) : \n 
~~~ sim_info [ name ] = self . read_from_hdf5 ( value ) \n 
~~ for name , value in sim_info_grp . items ( ) : \n 
~~ return sim_info \n 
~~ def drivers ( self ) : \n 
driver_info = [ ] \n 
for name in self . _inp . keys ( ) : \n 
~~~ if name . startswith ( ) : \n 
~~~ driver_info . append ( self . read_from_hdf5 ( self . _inp [ name ] ) ) \n 
~~ ~~ return driver_info \n 
~~ def cases ( self ) : \n 
iteration_cases_grp = self . _inp [ ] \n 
case_timestamps = { } \n 
for driver_name in iteration_cases_grp : \n 
~~~ for iteration_case_name in iteration_cases_grp [ driver_name ] : \n 
~~~ if iteration_case_name . startswith ( ) : \n 
~~~ timestamp = iteration_cases_grp [ driver_name ] [ iteration_case_name ] [ ] [ case_timestamps [ timestamp ] = ( driver_name , iteration_case_name ) \n 
~~ ~~ ~~ sorted_timestamps = sorted ( case_timestamps ) \n 
for timestamp in sorted_timestamps : \n 
~~~ driver_name , iteration_case_name = case_timestamps [ timestamp ] \n 
info = self . read_iteration_case_from_hdf5 ( self . _inp , driver_name , iteration_case_name ) yield info \n 
~~ ~~ def _next ( self ) : \n 
from openmdao . main . api import Component \n 
from openmdao . main . datatypes . api import Float \n 
class SleepComponent ( Component ) : \n 
sleep_time = Float ( 0.0 , iotype = , desc = ) \n 
~~~ time . sleep ( self . sleep_time ) \n 
from openmdao . lib . datatypes . domain . flow import FlowSolution \n 
from openmdao . lib . datatypes . domain . grid import GridCoordinates \n 
CARTESIAN = \n 
CYLINDRICAL = \n 
_COORD_SYSTEMS = ( CARTESIAN , CYLINDRICAL ) \n 
class Zone ( object ) : \n 
~~~ self . grid_coordinates = GridCoordinates ( ) \n 
self . flow_solution = FlowSolution ( ) \n 
self . reference_state = None \n 
self . _coordinate_system = CARTESIAN \n 
self . right_handed = True \n 
self . symmetry = None \n 
self . symmetry_axis = None \n 
self . symmetry_instances = 1 \n 
def shape ( self ) : \n 
return self . grid_coordinates . shape \n 
def extent ( self ) : \n 
return self . grid_coordinates . extent \n 
~~ def _get_coord_sys ( self ) : \n 
~~~ return self . _coordinate_system \n 
~~ def _set_coord_sys ( self , sys ) : \n 
~~~ if sys in _COORD_SYSTEMS : \n 
~~~ self . _coordinate_system = sys \n 
~~~ raise ValueError ( % sys ) \n 
~~ ~~ coordinate_system = property ( _get_coord_sys , _set_coord_sys , \n 
doc = ) \n 
return copy . deepcopy ( self ) \n 
~~ def is_equivalent ( self , other , logger , tolerance = 0. ) : \n 
if not isinstance ( other , Zone ) : \n 
~~ if self . coordinate_system != other . coordinate_system : \n 
~~ if self . right_handed != other . right_handed : \n 
~~ if self . symmetry != other . symmetry : \n 
~~ if self . symmetry_axis != other . symmetry_axis : \n 
~~ if self . symmetry_instances != other . symmetry_instances : \n 
~~ if not self . grid_coordinates . is_equivalent ( other . grid_coordinates , \n 
logger , tolerance ) : \n 
~~ if not self . flow_solution . is_equivalent ( other . flow_solution , logger , \n 
tolerance ) : \n 
~~ def extract ( self , imin , imax , jmin = None , jmax = None , kmin = None , kmax = None , \n 
grid_ghosts = None , flow_ghosts = None ) : \n 
zone = Zone ( ) \n 
zone . grid_coordinates = self . grid_coordinates . extract ( imin , imax , jmin , jmax , kmin , kmax , \n 
grid_ghosts ) \n 
zone . flow_solution = self . flow_solution . extract ( imin , imax , jmin , jmax , kmin , kmax , \n 
flow_ghosts ) \n 
if self . reference_state is not None : \n 
~~~ zone . reference_state = self . reference_state . copy ( ) \n 
~~ zone . coordinate_system = self . coordinate_system \n 
zone . right_handed = self . right_handed \n 
zone . symmetry = self . symmetry \n 
zone . symmetry_axis = self . symmetry_axis \n 
zone . symmetry_instances = self . symmetry_instances \n 
return zone \n 
~~ def extend ( self , axis , delta , grid_points , flow_points , normal = None ) : \n 
if grid_points > 0 : \n 
~~~ zone . grid_coordinates = self . grid_coordinates . extend ( axis , delta , grid_points , normal ) \n 
~~~ zone . grid_coordinates = self . grid_coordinates . copy ( ) \n 
~~ if flow_points > 0 : \n 
~~~ zone . flow_solution = self . flow_solution . extend ( axis , delta , flow_points ) \n 
~~~ zone . flow_solution = self . flow_solution . copy ( ) \n 
~~ if self . reference_state is not None : \n 
~~ def make_cartesian ( self , axis = ) : \n 
if self . coordinate_system != CARTESIAN : \n 
~~~ self . flow_solution . make_cartesian ( self . grid_coordinates , axis ) \n 
self . grid_coordinates . make_cartesian ( axis ) \n 
self . coordinate_system = CARTESIAN \n 
~~ ~~ def make_cylindrical ( self , axis = ) : \n 
if self . coordinate_system != CYLINDRICAL : \n 
~~~ self . grid_coordinates . make_cylindrical ( axis ) \n 
self . flow_solution . make_cylindrical ( self . grid_coordinates , axis ) \n 
self . coordinate_system = CYLINDRICAL \n 
~~ ~~ def make_left_handed ( self ) : \n 
if self . right_handed : \n 
~~~ self . grid_coordinates . flip_z ( ) \n 
self . flow_solution . flip_z ( ) \n 
self . right_handed = False \n 
~~ ~~ def make_right_handed ( self ) : \n 
if not self . right_handed : \n 
~~ ~~ def translate ( self , delta_x , delta_y , delta_z ) : \n 
if self . coordinate_system == CARTESIAN : \n 
~~~ self . grid_coordinates . translate ( delta_x , delta_y , delta_z ) \n 
~~~ raise RuntimeError ( ) \n 
~~ ~~ def rotate_about_x ( self , deg ) : \n 
~~~ self . grid_coordinates . rotate_about_x ( deg ) \n 
self . flow_solution . rotate_about_x ( deg ) \n 
~~ ~~ def rotate_about_y ( self , deg ) : \n 
~~~ self . grid_coordinates . rotate_about_y ( deg ) \n 
self . flow_solution . rotate_about_y ( deg ) \n 
~~ ~~ def rotate_about_z ( self , deg ) : \n 
~~~ self . grid_coordinates . rotate_about_z ( deg ) \n 
self . flow_solution . rotate_about_z ( deg ) \n 
~~ ~~ def promote ( self ) : \n 
self . grid_coordinates . promote ( ) \n 
self . flow_solution . promote ( ) \n 
~~ def demote ( self ) : \n 
self . grid_coordinates . demote ( ) \n 
self . flow_solution . demote ( ) \n 
from math import isnan \n 
from numpy import zeros , array \n 
from slsqp . slsqp import slsqp , closeunit , pyflush \n 
from openmdao . main . datatypes . api import Enum , Float , Int , Str \n 
from openmdao . main . driver_uses_derivatives import Driver \n 
from openmdao . main . hasparameters import HasParameters \n 
from openmdao . main . hasconstraints import HasConstraints \n 
from openmdao . main . hasobjective import HasObjective \n 
from openmdao . main . interfaces import IHasParameters , IHasConstraints , IHasObjective , implements , IOptimizer \n 
from openmdao . util . decorators import add_delegate \n 
@ add_delegate ( HasParameters , HasConstraints , HasObjective ) \n 
class SLSQPdriver ( Driver ) : \n 
implements ( IHasParameters , IHasConstraints , IHasObjective , IOptimizer ) \n 
accuracy = Float ( 1.0e-6 , iotype = , \n 
desc = ) \n 
maxiter = Int ( 50 , iotype = , \n 
iprint = Enum ( 0 , [ 0 , 1 , 2 , 3 ] , iotype = , \n 
iout = Int ( 6 , iotype = , \n 
output_filename = Str ( , iotype = , \n 
error_code = Int ( 0 , iotype = , \n 
~~~ super ( SLSQPdriver , self ) . __init__ ( ) \n 
self . error_messages = { \n 
self . x = zeros ( 0 , ) \n 
self . x_lower_bounds = zeros ( 0 , ) \n 
self . x_upper_bounds = zeros ( 0 , ) \n 
self . inputs = None \n 
self . obj = None \n 
self . con = None \n 
self . nparam = None \n 
self . ncon = None \n 
self . neqcon = None \n 
self . ff = 0 \n 
self . nfunc = 0 \n 
self . ngrad = 0 \n 
self . _continue = None \n 
~~ def start_iteration ( self ) : \n 
super ( SLSQPdriver , self ) . run_iteration ( ) \n 
self . inputs = self . list_param_group_targets ( ) \n 
self . obj = self . list_objective_targets ( ) \n 
self . con = self . list_constraint_targets ( ) \n 
self . nparam = self . total_parameters ( ) \n 
self . ncon = self . total_constraints ( ) \n 
self . neqcon = self . total_eq_constraints ( ) \n 
self . x = self . eval_parameters ( self . parent ) \n 
self . x_lower_bounds = self . get_lower_bounds ( ) \n 
self . x_upper_bounds = self . get_upper_bounds ( ) \n 
self . _continue = True \n 
~~ def run_iteration ( self ) : \n 
n = self . nparam \n 
m = self . ncon \n 
meq = self . neqcon \n 
la = max ( m , 1 ) \n 
gg = zeros ( [ la ] , ) \n 
df = zeros ( [ n + 1 ] , ) \n 
dg = zeros ( [ la , n + 1 ] , ) \n 
mineq = m - meq + 2 * ( n + 1 ) \n 
lsq = ( n + 1 ) * ( ( n + 1 ) + 1 ) + meq * ( ( n + 1 ) + 1 ) + mineq * ( ( n + 1 ) + 1 ) \n 
lsi = ( ( n + 1 ) - meq + 1 ) * ( mineq + 2 ) + 2 * mineq \n 
lsei = ( ( n + 1 ) + mineq ) * ( ( n + 1 ) - meq ) + 2 * meq + ( n + 1 ) \n 
slsqpb = ( n + 1 ) * ( n / 2 ) + 2 * m + 3 * n + 3 * ( n + 1 ) + 1 \n 
lw = lsq + lsi + lsei + slsqpb + n + m \n 
w = zeros ( [ lw ] , ) \n 
ljw = max ( mineq , ( n + 1 ) - meq ) \n 
jw = zeros ( [ ljw ] , ) \n 
~~~ dg , self . error_code , self . nfunc , self . ngrad = slsqp ( self . ncon , self . neqcon , la , self . nparam , \n 
self . x , self . x_lower_bounds , self . x_upper_bounds , \n 
self . ff , gg , df , dg , self . accuracy , self . maxiter , \n 
self . iprint - 1 , self . iout , self . output_filename , \n 
self . error_code , w , lw , jw , ljw , \n 
self . nfunc , self . ngrad , \n 
self . _func , self . _grad ) \n 
#slsqp(m,meq,la,n,xx,xl,xu,ff,gg,df,dg,acc,maxit,iprint, \n 
~~ except Exception as err : \n 
~~~ self . _logger . error ( str ( err ) ) \n 
~~ if self . iprint > 0 : \n 
~~~ closeunit ( self . iout ) \n 
~~ if self . error_code != 0 : \n 
~~~ self . _logger . warning ( self . error_messages [ self . error_code ] ) \n 
~~ self . _continue = False \n 
~~ def _func ( self , m , me , la , n , f , g , xnew ) : \n 
self . set_parameters ( xnew ) \n 
f = self . eval_objective ( ) \n 
if isnan ( f ) : \n 
self . raise_exception ( msg , RuntimeError ) \n 
~~ if self . ncon > 0 : \n 
~~~ g = - 1. * array ( self . eval_constraints ( self . parent ) ) \n 
~~~ pyflush ( self . iout ) \n 
~~ return f , g \n 
~~ def _grad ( self , m , me , la , n , f , g , df , dg , xnew ) : \n 
J = self . _calc_gradient ( self . inputs , self . obj + self . con ) \n 
df [ 0 : self . nparam ] = J [ 0 , : ] . ravel ( ) \n 
if self . ncon > 0 : \n 
~~~ dg [ 0 : self . ncon , 0 : self . nparam ] = - J [ 1 : 1 + self . ncon , : ] \n 
~~ return df , dg \n 
~~ def requires_derivs ( self ) : \n 
~~ ~~ from sellar import SellarProblem , SellarProblemWithDeriv \n 
from branin import BraninProblem \n 
from scalable import UnitScalableProblem \n 
import pprint \n 
copy . _copy_dispatch [ weakref . ref ] = copy . _copy_immutable \n 
copy . _deepcopy_dispatch [ weakref . ref ] = copy . _deepcopy_atomic \n 
copy . _deepcopy_dispatch [ weakref . KeyedRef ] = copy . _deepcopy_atomic \n 
from zope . interface import Interface , implements \n 
from numpy import ndarray \n 
from traits . api import HasTraits , Missing , Python , push_exception_handler , TraitType , CTrait \n 
from traits . has_traits import FunctionType , _clone_trait , MetaHasTraits \n 
from traits . trait_base import not_none \n 
from multiprocessing import connection \n 
from openmdao . main . datatypes . file import FileRef \n 
from openmdao . main . datatypes . list import List \n 
from openmdao . main . datatypes . slot import Slot \n 
from openmdao . main . datatypes . vtree import VarTree \n 
from openmdao . main . interfaces import ICaseIterator , IResourceAllocator , IContainer , IVariableTree , IContainerProxy , IOverrideSet \n 
from openmdao . main . mp_support import ObjectManager , is_instance , CLASSES_TO_PROXY , has_interface \n 
from openmdao . main . rbac import rbac \n 
from openmdao . main . variable import Variable , is_legal_name , _missing \n 
from openmdao . main . array_helpers import flattened_value , get_index \n 
from openmdao . util . log import Logger , logger \n 
from openmdao . util import eggloader , eggsaver , eggobserver \n 
from openmdao . util . eggsaver import SAVE_CPICKLE \n 
from openmdao . util . typegroups import int_types , complex_or_real_types \n 
_copydict = { \n 
: copy . deepcopy , \n 
: copy . copy \n 
_iodict = { : , : } \n 
__missing__ = object ( ) \n 
def get_closest_proxy ( obj , pathname ) : \n 
names = pathname . split ( ) \n 
~~~ if IContainerProxy . providedBy ( obj ) : \n 
~~~ return ( obj , . join ( names [ i : ] ) ) \n 
~~~ obj = getattr ( obj , name ) \n 
~~ i += 1 \n 
~~ return ( obj , . join ( names [ i : ] ) ) \n 
~~ def proxy_parent ( obj , pathname ) : \n 
for name in names [ : - 1 ] : \n 
~~ push_exception_handler ( handler = lambda o , t , ov , nv : None , \n 
reraise_exceptions = True , \n 
main = True , \n 
locked = True ) \n 
class _MetaSafe ( MetaHasTraits ) : \n 
def __new__ ( mcs , class_name , bases , class_dict ) : \n 
~~~ for name , obj in class_dict . items ( ) : \n 
~~~ if isinstance ( obj , Variable ) : \n 
~~~ for base in bases : \n 
~~~ if name in base . __dict__ : \n 
~~~ raise NameError ( \n 
% ( class_name , name , base . __name__ ) ) \n 
~~ ~~ ~~ ~~ return super ( _MetaSafe , mcs ) . __new__ ( mcs , class_name , bases , class_dict ) \n 
~~ ~~ class SafeHasTraits ( HasTraits ) : \n 
__metaclass__ = _MetaSafe \n 
~~ def _check_bad_default ( name , trait , obj = None ) : \n 
~~~ if trait . vartypename not in [ , ] and trait . required is True and not trait . assumed_default and trait . _illegal_default_ is True : \n 
if obj is None : \n 
~~~ raise RuntimeError ( msg ) \n 
~~~ obj . raise_exception ( msg , RuntimeError ) \n 
~~ ~~ ~~ class Container ( SafeHasTraits ) : \n 
implements ( IContainer ) \n 
super ( Container , self ) . __init__ ( ) \n 
self . _call_cpath_updated = True \n 
self . _call_configure = True \n 
self . _added_traits = { } \n 
self . _getcache = { } \n 
self . _setcache = { } \n 
self . _copycache = { } \n 
self . _cached_traits_ = None \n 
self . _repair_trait_info = None \n 
self . _trait_metadata = { } \n 
self . _logger = Logger ( ) \n 
for name , obj in self . items ( ) : \n 
~~~ if isinstance ( obj , FileRef ) : \n 
~~~ setattr ( self , name , obj . copy ( owner = self ) ) \n 
~~ ~~ for name , obj in self . __class__ . __dict__ [ ] . items ( ) : \n 
~~~ ttype = obj . trait_type \n 
if isinstance ( ttype , VarTree ) : \n 
~~~ variable_tree = getattr ( self , name ) \n 
if not obj . required : \n 
~~~ new_tree = variable_tree . copy ( ) \n 
setattr ( self , name , new_tree ) \n 
~~ ~~ if obj . required : \n 
~~~ _check_bad_default ( name , obj , self ) \n 
~~ ~~ ~~ @ property \n 
def parent ( self ) : \n 
return self . _parent \n 
~~ @ parent . setter \n 
def parent ( self , value ) : \n 
if self . _parent is not value : \n 
~~~ self . _parent = value \n 
self . _fix_loggers ( self , recurse = True ) \n 
self . _branch_moved ( ) \n 
~~ ~~ def _branch_moved ( self ) : \n 
~~~ self . _call_cpath_updated = True \n 
for n , cont in self . items ( ) : \n 
~~~ if is_instance ( cont , Container ) and cont is not self . _parent : \n 
~~~ cont . _branch_moved ( ) \n 
def name ( self ) : \n 
if self . _name is None : \n 
~~~ if self . parent : \n 
~~~ self . _name = find_name ( self . parent , self ) \n 
~~ elif self . _call_cpath_updated is False : \n 
~~~ self . _name = \n 
~~ ~~ return self . _name \n 
~~ @ name . setter \n 
def name ( self , name ) : \n 
if not is_legal_name ( name ) : \n 
~~ if self . _name != name : \n 
~~~ self . _name = name \n 
~~ ~~ def _fix_loggers ( self , container , recurse ) : \n 
container . _logger . rename ( container . get_pathname ( ) . replace ( , ) ) \n 
if recurse : \n 
~~~ for name in container . list_containers ( ) : \n 
~~~ obj = getattr ( container , name ) \n 
self . _fix_loggers ( obj , recurse ) \n 
~~ ~~ ~~ @ rbac ( ( , ) ) \n 
def get_pathname ( self , rel_to_scope = None ) : \n 
path = [ ] \n 
obj = self \n 
name = obj . name \n 
while obj is not rel_to_scope and name : \n 
~~~ path . append ( name ) \n 
obj = obj . parent \n 
~~ name = obj . name \n 
~~ return . join ( path [ : : - 1 ] ) \n 
~~ def get_trait ( self , name , copy = False ) : \n 
if self . _cached_traits_ is None : \n 
~~~ self . _cached_traits_ = self . traits ( ) \n 
self . _cached_traits_ . update ( self . _instance_traits ( ) ) \n 
~~ if copy : \n 
~~~ if self . _cached_traits_ . get ( name ) : \n 
~~~ return self . trait ( name , copy = copy ) \n 
~~~ return self . _cached_traits_ . get ( name ) \n 
~~ ~~ def __deepcopy__ ( self , memo ) : \n 
id_self = id ( self ) \n 
if id_self in memo : \n 
~~~ return memo [ id_self ] \n 
~~ memo [ ] = "deep" \n 
saved_p = self . _parent \n 
saved_c = self . _cached_traits_ \n 
saved_s = self . _setcache \n 
saved_g = self . _getcache \n 
self . _parent = None \n 
~~~ result = super ( Container , self ) . __deepcopy__ ( memo ) \n 
~~~ self . _parent = saved_p \n 
self . _cached_traits_ = saved_c \n 
self . _getcache = saved_g \n 
self . _setcache = saved_s \n 
~~ olditraits = self . _instance_traits ( ) \n 
for name , trait in olditraits . items ( ) : \n 
~~~ if trait . type is not and name in self . _added_traits : \n 
~~~ if isinstance ( trait . trait_type , VarTree ) : \n 
~~~ if name not in result . _added_traits : \n 
~~~ result . add_trait ( name , _clone_trait ( trait ) ) \n 
~~~ result . __dict__ [ name ] = copy . deepcopy ( self . __dict__ [ name ] ) \n 
state = super ( Container , self ) . __getstate__ ( ) \n 
dct = { } \n 
for name , trait in state [ ] . items ( ) : \n 
~~~ if trait . transient is not True : \n 
~~~ dct [ name ] = trait \n 
~~ ~~ state [ ] = dct \n 
state [ ] = None \n 
state [ ] = { } \n 
return state \n 
~~ def __setstate__ ( self , state ) : \n 
super ( Container , self ) . __setstate__ ( { } ) \n 
self . __dict__ . update ( state ) \n 
self . _repair_trait_info = { } \n 
traits = self . _alltraits ( ) \n 
for name , trait in self . _added_traits . items ( ) : \n 
~~~ if name not in traits : \n 
~~~ self . add_trait ( name , trait , refresh = False ) \n 
~~ ~~ fixups = [ ] \n 
for name , trait in traits . items ( ) : \n 
~~~ get = trait . trait_type . get \n 
~~ if get is not None : \n 
~~~ if name not in self . _added_traits : \n 
~~~ val = getattr ( self , name ) \n 
self . remove_trait ( name ) \n 
self . add_trait ( name , trait ) \n 
setattr ( self , name , val ) \n 
~~ except Exception as exc : \n 
~~~ self . _logger . warning ( , \n 
name , val , exc ) \n 
fixups . append ( ( name , trait ) ) \n 
~~ ~~ ~~ ~~ self . _repair_trait_info [ ] = fixups \n 
fixups = [ ] \n 
for name , val in self . __dict__ . items ( ) : \n 
~~~ if not name . startswith ( ) and not self . get_trait ( name ) : \n 
fixups . append ( ( name , val ) ) \n 
~~ ~~ ~~ self . _repair_trait_info [ ] = fixups \n 
for name , trait in self . _alltraits ( ) . items ( ) : \n 
~~~ if isinstance ( trait . trait_type , List ) : \n 
~~~ setattr ( self , name , getattr ( self , name ) ) \n 
fixups . append ( name ) \n 
~~ def _repair_traits ( self ) : \n 
if self . _repair_trait_info is None : \n 
~~ for name , trait in self . _repair_trait_info [ ] : \n 
~~ for name , val in self . _repair_trait_info [ ] : \n 
~~~ setattr ( self , name , val ) \n 
~~ for name in self . _repair_trait_info [ ] : \n 
~~ self . _repair_trait_info = None \n 
def add_class_trait ( cls , name , * trait ) : \n 
bases = [ cls ] \n 
bases . extend ( cls . __bases__ ) \n 
for base in bases : \n 
% ( name , base . __name__ ) ) \n 
~~ ~~ for t in trait : \n 
~~~ _check_bad_default ( name , t ) \n 
~~ if name in cls . _trait_metadata : \n 
~~ return super ( Container , cls ) . add_class_trait ( name , * trait ) \n 
~~ def add_trait ( self , name , trait , refresh = True ) : \n 
if name . endswith ( ) and trait . type == : \n 
~~~ super ( Container , self ) . add_trait ( name , trait ) \n 
~~ bases = [ self . __class__ ] \n 
bases . extend ( self . __class__ . __bases__ ) \n 
~~ ~~ _check_bad_default ( name , trait , self ) \n 
if name not in self . _added_traits : \n 
~~~ self . _added_traits [ name ] = trait \n 
~~ super ( Container , self ) . add_trait ( name , trait ) \n 
if self . _cached_traits_ is not None : \n 
~~~ self . _cached_traits_ [ name ] = self . trait ( name ) \n 
~~ if name in self . _trait_metadata : \n 
~~ if refresh : \n 
~~ ~~ def remove_trait ( self , name ) : \n 
~~~ del self . _added_traits [ name ] \n 
~~~ del self . _cached_traits_ [ name ] \n 
~~ except ( KeyError , TypeError ) : \n 
~~~ del self . _trait_metadata [ name ] \n 
~~ super ( Container , self ) . remove_trait ( name ) \n 
~~ @ rbac ( ( , ) ) \n 
def get_attr_w_copy ( self , path ) : \n 
obj = self . get ( path ) \n 
copy = self . _copycache . get ( path , _missing ) \n 
if copy is _missing : \n 
~~~ copy = self . get_metadata ( path . split ( , 1 ) [ 0 ] , ) \n 
self . _copycache [ path ] = copy \n 
~~~ if isinstance ( obj , Container ) : \n 
~~~ obj = obj . copy ( ) \n 
~~~ obj = _copydict [ copy ] ( obj ) \n 
~~ ~~ return obj \n 
~~ def _add_after_parent_set ( self , name , obj ) : \n 
~~ def _prep_for_add ( self , name , obj ) : \n 
if in name : \n 
~~~ self . raise_exception ( \n 
name , ValueError ) \n 
~~ elif not is_legal_name ( name ) : \n 
NameError ) \n 
~~ removed = False \n 
if has_interface ( obj , IContainer ) : \n 
~~~ if self . contains ( name ) and getattr ( self , name ) : \n 
~~~ self . remove ( name ) \n 
removed = True \n 
~~ ~~ if has_interface ( obj , IContainer ) : \n 
~~~ self . _check_recursion ( obj ) \n 
if IContainerProxy . providedBy ( obj ) : \n 
~~~ obj . parent = self . _get_proxy ( obj ) \n 
~~~ obj . parent = self \n 
~~ obj . name = name \n 
self . _add_after_parent_set ( name , obj ) \n 
if self . _call_cpath_updated is False : \n 
~~~ obj . cpath_updated ( ) \n 
~~ ~~ return removed \n 
~~ def _post_container_add ( self , name , obj , removed ) : \n 
~~ def add ( self , name , obj ) : \n 
removed = self . _prep_for_add ( name , obj ) \n 
~~~ setattr ( self , name , obj ) \n 
~~~ self . get_trait ( name ) \n 
~~ self . _post_container_add ( name , obj , removed ) \n 
~~ elif is_instance ( obj , TraitType ) : \n 
~~~ self . add_trait ( name , obj ) \n 
~~ return obj \n 
~~ def _check_recursion ( self , obj ) : \n 
ancestor = self \n 
while is_instance ( ancestor , Container ) : \n 
~~~ if obj is ancestor : \n 
~~~ self . raise_exception ( , \n 
ValueError ) \n 
~~ ancestor = ancestor . parent \n 
~~ ~~ def _get_proxy ( self , proxy ) : \n 
addr_type = connection . address_type ( proxy . _token . address ) \n 
addr = proxy . _token . address [ 0 ] if addr_type == else None \n 
key = ( addr_type , addr , proxy . _authkey ) \n 
~~~ manager = self . _managers [ key ] \n 
~~~ if addr_type == : \n 
~~~ ip_addr = socket . gethostbyname ( socket . gethostname ( ) ) \n 
address = ( ip_addr , 0 ) \n 
allowed_hosts = [ addr ] \n 
if addr == ip_addr : \n 
~~~ allowed_hosts . append ( ) \n 
~~~ address = None \n 
allowed_hosts = None \n 
~~ name = self . name or \n 
access = addr if addr_type == else addr_type \n 
name = % ( name , access ) \n 
manager = ObjectManager ( self , address , authkey = proxy . _authkey , \n 
name = name , allowed_hosts = allowed_hosts ) \n 
self . _managers [ key ] = manager \n 
~~ return manager . proxy \n 
~~ def _check_rename ( self , oldname , newname ) : \n 
~~~ if in oldname or in newname : \n 
( oldname , newname ) , RuntimeError ) \n 
~~ if not self . contains ( oldname ) : \n 
( oldname , newname , oldname ) , RuntimeError ) \n 
~~ if self . contains ( newname ) : \n 
( oldname , newname , newname ) , RuntimeError ) \n 
~~ ~~ def rename ( self , oldname , newname ) : \n 
self . _check_rename ( oldname , newname ) \n 
obj = self . remove ( oldname ) \n 
self . add ( newname , obj ) \n 
~~ def remove ( self , name ) : \n 
name , NameError ) \n 
~~~ obj = getattr ( self , name ) \n 
~~ trait = self . get_trait ( name ) \n 
if trait is None : \n 
~~~ delattr ( self , name ) \n 
~~~ if trait . is_trait_type ( Slot ) : \n 
~~~ setattr ( self , name , None ) \n 
~~ except TypeError as err : \n 
~~~ self . raise_exception ( str ( err ) , RuntimeError ) \n 
~~~ self . remove_trait ( name ) \n 
def configure ( self ) : \n 
cp = copy . deepcopy ( self ) \n 
cp . _relink ( ) \n 
return cp \n 
~~ def _relink ( self ) : \n 
for name in self . list_containers ( ) : \n 
~~~ container = getattr ( self , name ) \n 
if container is not self . _parent : \n 
~~~ container . _parent = self \n 
container . _relink ( ) \n 
def cpath_updated ( self ) : \n 
self . _fix_loggers ( self , recurse = False ) \n 
self . _call_cpath_updated = False \n 
for cont in self . list_containers ( ) : \n 
~~~ cont = getattr ( self , cont ) \n 
if cont is not self . _parent : \n 
~~~ cont . cpath_updated ( ) \n 
~~ ~~ ~~ def revert_to_defaults ( self , recurse = True ) : \n 
self . reset_traits ( iotype = ) \n 
~~~ for cname in self . list_containers ( ) : \n 
~~~ getattr ( self , cname ) . revert_to_defaults ( recurse ) \n 
~~ ~~ ~~ def _items ( self , visited , recurse = False , ** metadata ) : \n 
if id ( self ) not in visited : \n 
~~~ visited . add ( id ( self ) ) \n 
match_dict = self . _alltraits ( ** metadata ) \n 
~~~ for name in self . list_containers ( ) : \n 
if name in match_dict and id ( obj ) not in visited : \n 
~~~ yield ( name , obj ) \n 
~~ if obj : \n 
~~~ for chname , child in obj . _items ( visited , recurse , \n 
** metadata ) : \n 
~~~ yield ( . join ( ( name , chname ) ) , child ) \n 
~~ ~~ ~~ ~~ for name , trait in match_dict . items ( ) : \n 
~~~ obj = getattr ( self , name , Missing ) \n 
if obj is not Missing : \n 
~~~ if is_instance ( obj , ( Container , VarTree ) ) and id ( obj ) not in visited : \n 
~~~ if not recurse : \n 
~~ ~~ elif trait . iotype is not None : \n 
~~ ~~ ~~ ~~ ~~ def items ( self , recurse = False , ** metadata ) : \n 
return self . _items ( set ( [ id ( self . parent ) ] ) , recurse , ** metadata ) \n 
~~ def list_containers ( self ) : \n 
return [ n for n , v in self . items ( ) if is_instance ( v , Container ) ] \n 
~~ def list_vars ( self ) : \n 
return [ k for k , v in self . items ( iotype = not_none ) ] \n 
def _alltraits ( self , traits = None , events = False , ** metadata ) : \n 
if traits is None : \n 
~~~ if self . _cached_traits_ : \n 
~~~ traits = self . _cached_traits_ \n 
~~~ traits = self . traits ( ) \n 
traits . update ( self . _instance_traits ( ) ) \n 
self . _cached_traits_ = traits \n 
~~ ~~ result = { } \n 
~~~ if not events and trait . type is : \n 
~~ for meta_name , meta_eval in metadata . items ( ) : \n 
~~~ if type ( meta_eval ) is FunctionType : \n 
~~~ if not meta_eval ( getattr ( trait , meta_name ) ) : \n 
~~ ~~ elif meta_eval != getattr ( trait , meta_name ) : \n 
~~~ result [ name ] = trait \n 
def contains ( self , path ) : \n 
childname , _ , restofpath = path . partition ( ) \n 
if restofpath : \n 
~~~ obj = getattr ( self , childname , Missing ) \n 
if obj is Missing : \n 
~~ elif is_instance ( obj , Container ) : \n 
~~~ return obj . contains ( restofpath ) \n 
~~~ return hasattr ( obj , restofpath ) \n 
~~ ~~ return hasattr ( self , path ) \n 
~~ def _get_metadata_failed ( self , traitpath , metaname ) : \n 
AttributeError ) \n 
def get_metadata ( self , traitpath , metaname = None ) : \n 
childname , _ , restofpath = traitpath . partition ( ) \n 
~~~ return self . _get_metadata_failed ( traitpath , metaname ) \n 
~~ elif hasattr ( obj , ) : \n 
~~~ return obj . get_metadata ( restofpath , metaname ) \n 
~~~ t = self . get_trait ( childname ) \n 
if t is not None and t . iotype and metaname == : \n 
~~~ return t . iotype \n 
~~~ self . _get_metadata_failed ( traitpath , metaname ) \n 
~~ ~~ ~~ varname , _ , _ = traitpath . partition ( ) \n 
~~~ mdict = self . _trait_metadata [ varname ] \n 
~~~ t = self . get_trait ( varname ) \n 
if t : \n 
~~~ t = t . trait_type \n 
mdict = t . _metadata . copy ( ) \n 
mdict . setdefault ( , t . __class__ . __name__ ) \n 
~~~ mdict = self . _get_metadata_failed ( traitpath , None ) \n 
~~ self . _trait_metadata [ varname ] = mdict \n 
~~ if metaname is None : \n 
~~~ return mdict \n 
~~~ return mdict . get ( metaname , None ) \n 
~~ ~~ @ rbac ( ( , ) ) \n 
def set_metadata ( self , traitpath , metaname , value ) : \n 
if metaname in ( , ) : \n 
% ( metaname , traitpath ) , TypeError ) \n 
~~ self . get_metadata ( traitpath ) [ metaname ] = value \n 
~~ @ rbac ( ( , ) , proxy_types = [ FileRef ] ) \n 
def get ( self , path ) : \n 
expr = self . _getcache . get ( path ) \n 
if expr is not None : \n 
~~~ return eval ( expr , self . __dict__ ) \n 
~~ obj , restofpath = get_closest_proxy ( self , path ) \n 
if restofpath and IContainerProxy . providedBy ( obj ) : \n 
~~~ return obj . get ( restofpath ) \n 
~~ expr = compile ( path , path , mode = ) \n 
~~~ val = eval ( expr , self . __dict__ ) \n 
~~ except ( AttributeError , NameError ) as err : \n 
~~ self . raise_exception ( str ( err ) , AttributeError ) \n 
~~~ self . _getcache [ path ] = expr \n 
return val \n 
~~ ~~ @ rbac ( ( , ) , proxy_types = [ FileRef ] ) \n 
def get_flattened_value ( self , path ) : \n 
return flattened_value ( path , self . get ( path ) ) \n 
def set_flattened_value ( self , path , value ) : \n 
~~~ obj , restofpath = proxy_parent ( self , path ) \n 
~~~ obj . set_flattened_value ( restofpath , value ) \n 
~~ val = self . get ( path ) \n 
if not isinstance ( val , int_types ) and isinstance ( val , complex_or_real_types ) : \n 
~~~ self . set ( path , value [ 0 ] ) \n 
~~ elif hasattr ( val , ) : \n 
~~~ val . set_flattened_value ( value ) \n 
~~ elif isinstance ( val , ndarray ) : \n 
~~~ newshape = value . shape \n 
self . set ( path , value . reshape ( val . shape ) ) \n 
% ( self . get_pathname ( ) , path , val . shape , newshape ) , \n 
sys . exc_info ( ) ) \n 
~~ val = self . get ( path . split ( , 1 ) [ 0 ] ) \n 
idx = get_index ( path ) \n 
if isinstance ( val , int_types ) : \n 
~~ elif hasattr ( val , ) and idx is not None : \n 
~~~ if isinstance ( val [ idx ] , complex_or_real_types ) : \n 
~~~ val [ idx ] = value [ 0 ] \n 
~~~ val [ idx ] = value \n 
~~ elif IVariableTree . providedBy ( val ) : \n 
~~ def get_iotype ( self , name ) : \n 
~~~ return self . get_trait ( name ) . iotype \n 
def set ( self , path , value ) : \n 
_local_setter_ = value \n 
expr = self . _setcache . get ( path ) \n 
~~~ exec ( expr ) \n 
~~ obj , restofpath = proxy_parent ( self , path ) \n 
if IOverrideSet . providedBy ( obj ) or ( restofpath and IContainerProxy . providedBy ( obj ) ) : \n 
~~~ obj . set ( restofpath , value ) \n 
~~ assign = "self.%s=_local_setter_" % path \n 
expr = compile ( assign , assign , mode = ) \n 
~~~ self . raise_exception ( str ( err ) , err . __class__ ) \n 
~~~ self . _setcache [ path ] = expr \n 
~~ ~~ def save_to_egg ( self , name , version , py_dir = None , src_dir = None , \n 
src_files = None , child_objs = None , dst_dir = None , \n 
observer = None , need_requirements = True ) : \n 
assert name and isinstance ( name , basestring ) \n 
assert version and isinstance ( version , basestring ) \n 
if not version . endswith ( ) : \n 
~~~ version += \n 
tstamp = % ( now . year , now . month , now . day , now . hour , now . minute ) \n 
version += tstamp \n 
observer = eggobserver . EggObserver ( observer , self . _logger ) \n 
entry_pts = [ ( self , name , _get_entry_group ( self ) ) ] \n 
if child_objs is not None : \n 
~~~ root_pathname = self . get_pathname ( ) \n 
root_start = root_pathname . rfind ( ) \n 
root_start = root_start + 1 if root_start >= 0 else 0 \n 
root_pathname += \n 
for child in child_objs : \n 
~~~ pathname = child . get_pathname ( ) \n 
if not pathname . startswith ( root_pathname ) : \n 
~~~ msg = % ( pathname , root_pathname ) \n 
observer . exception ( msg ) \n 
~~ entry_pts . append ( ( child , pathname [ root_start : ] , \n 
_get_entry_group ( child ) ) ) \n 
~~ ~~ parent = self . parent \n 
self . parent = None \n 
~~~ return eggsaver . save_to_egg ( entry_pts , version , py_dir , \n 
src_dir , src_files , dst_dir , \n 
self . _logger , observer . observer , \n 
need_requirements ) \n 
~~ ~~ def save ( self , outstream , fmt = SAVE_CPICKLE , proto = - 1 ) : \n 
parent = self . parent \n 
~~~ eggsaver . save ( self , outstream , fmt , proto , self . _logger ) \n 
def load_from_eggfile ( filename , observer = None , log = None ) : \n 
entry_group = \n 
entry_name = \n 
log = log or logger \n 
return eggloader . load_from_eggfile ( filename , entry_group , entry_name , \n 
log , observer ) \n 
def load_from_eggpkg ( package , entry_name = None , instance_name = None , \n 
observer = None ) : \n 
if not entry_name : \n 
~~ return eggloader . load_from_eggpkg ( package , entry_group , entry_name , \n 
instance_name , logger , observer ) \n 
def load ( instream , fmt = SAVE_CPICKLE , package = None , call_post_load = True , \n 
top = eggloader . load ( instream , fmt , package , logger ) \n 
top . cpath_updated ( ) \n 
if name : \n 
~~~ top . name = name \n 
~~ if call_post_load : \n 
~~~ top . parent = None \n 
top . post_load ( ) \n 
~~ return top \n 
~~ def post_load ( self ) : \n 
self . _repair_traits ( ) \n 
~~~ getattr ( self , name ) . post_load ( ) \n 
~~ ~~ @ rbac ( ) \n 
def pre_delete ( self ) : \n 
~~~ getattr ( self , name ) . pre_delete ( ) \n 
~~ ~~ @ rbac ( ( , ) , proxy_types = [ CTrait ] ) \n 
def get_dyn_trait ( self , pathname , iotype = None , trait = None ) : \n 
if pathname . startswith ( ) : \n 
~~ cname , _ , restofpath = pathname . partition ( ) \n 
~~~ child = getattr ( self , cname ) \n 
if is_instance ( child , Container ) : \n 
~~~ return child . get_dyn_trait ( restofpath , iotype , trait ) \n 
~~~ if deep_hasattr ( child , restofpath ) : \n 
~~~ trait = self . get_trait ( cname ) \n 
if trait is not None : \n 
~~~ if iotype is not None : \n 
~~~ obj = getattr ( self , cname ) \n 
t_iotype = getattr ( obj , , None ) \n 
~~~ t_iotype = self . get_iotype ( cname ) \n 
~~ if ( iotype == and t_iotype not in ( , ) ) or ( iotype == and t_iotype not in ( , , , ) ) : \n 
( pathname , _iodict [ iotype ] ) , \n 
RuntimeError ) \n 
~~ ~~ return trait \n 
~~ elif trait is None and self . contains ( cname ) : \n 
pathname , AttributeError ) \n 
def get_trait_typenames ( self , pathname , iotype = None ) : \n 
if not pathname : \n 
~~~ obj = self \n 
~~~ trait = self . get_dyn_trait ( pathname , iotype = iotype ) \n 
~~ trait = trait . trait_type or trait . trait or trait \n 
~~~ trait = self . get_dyn_trait ( trait . target ) \n 
~~~ ttype = trait . trait_type \n 
~~~ if ttype is not None : \n 
~~~ trait = ttype \n 
~~~ obj = self . get ( pathname ) \n 
~~~ obj = trait \n 
~~ ~~ names = [ ] \n 
Container . _bases ( type ( obj ) , names ) \n 
return names \n 
def _bases ( cls , names ) : \n 
names . append ( % ( cls . __module__ , cls . __name__ ) ) \n 
for base in cls . __bases__ : \n 
~~~ Container . _bases ( base , names ) \n 
~~ ~~ def raise_exception ( self , msg , exception_class = Exception ) : \n 
coords = \n 
while obj is not None : \n 
~~~ coords = obj . get_itername ( ) \n 
~~~ obj = obj . parent \n 
~~ ~~ if coords : \n 
~~~ full_msg = % ( self . get_pathname ( ) , coords , msg ) \n 
~~~ full_msg = % ( self . get_pathname ( ) , msg ) \n 
#self._logger.error(msg) \n 
~~ raise exception_class ( full_msg ) \n 
~~ def reraise_exception ( self , msg = , info = None ) : \n 
if info is None : \n 
~~~ exc_type , exc_value , exc_traceback = sys . exc_info ( ) \n 
~~~ exc_type , exc_value , exc_traceback = info \n 
~~ if msg : \n 
~~~ msg = % ( msg , exc_value ) \n 
~~~ msg = % exc_value \n 
~~ prefix = % self . get_pathname ( ) \n 
if not msg . startswith ( prefix ) : \n 
~~~ msg = prefix + msg \n 
~~ new_exc = exc_type ( msg ) \n 
raise exc_type , new_exc , exc_traceback \n 
~~ def build_trait ( self , ref_name , iotype = None , trait = None ) : \n 
self . raise_exception ( , NotImplementedError ) \n 
~~ ~~ CLASSES_TO_PROXY . append ( Container ) \n 
CLASSES_TO_PROXY . append ( FileRef ) \n 
def _get_entry_group ( obj ) : \n 
if _get_entry_group . group_map is None : \n 
~~~ from openmdao . main . component import Component \n 
from openmdao . main . driver import Driver \n 
_get_entry_group . group_map = [ \n 
( Variable , ) , \n 
( Driver , ) , \n 
( ICaseIterator , ) , \n 
( IResourceAllocator , ) , \n 
( Component , ) , \n 
( Container , ) , \n 
~~ for cls , group in _get_entry_group . group_map : \n 
~~~ if issubclass ( cls , Interface ) : \n 
~~~ if cls . providedBy ( obj ) : \n 
~~~ return group \n 
~~~ if isinstance ( obj , cls ) : \n 
def dump ( cont , recurse = False , stream = None , ** metadata ) : \n 
pprint . pprint ( dict ( [ ( n , str ( v ) ) \n 
for n , v in cont . items ( recurse = recurse , \n 
** metadata ) ] ) , \n 
stream ) \n 
~~ def find_name ( parent , obj ) : \n 
for name , val in parent . __dict__ . items ( ) : \n 
~~~ if val is obj : \n 
~~ def get_default_name ( obj , scope ) : \n 
classname = obj . __class__ . __name__ . lower ( ) \n 
if scope is None : \n 
~~~ sdict = { } \n 
~~~ sdict = scope . __dict__ \n 
~~ ver = 1 \n 
while % ( classname , ver ) in sdict : \n 
~~~ ver += 1 \n 
~~ return % ( classname , ver ) \n 
~~ def find_trait_and_value ( obj , pathname ) : \n 
~~ if is_instance ( obj , Container ) : \n 
~~~ objtrait = obj . get_trait ( names [ - 1 ] ) \n 
~~ elif isinstance ( obj , HasTraits ) : \n 
~~~ objtrait = obj . trait ( names [ - 1 ] ) \n 
~~~ objtrait = None \n 
~~ return ( objtrait , getattr ( obj , names [ - 1 ] ) ) \n 
~~ def create_io_traits ( cont , obj_info , iotype = ) : \n 
if isinstance ( obj_info , ( basestring , tuple ) ) : \n 
~~~ it = [ obj_info ] \n 
~~~ it = obj_info \n 
~~ for entry in it : \n 
~~~ iostat = iotype \n 
trait = None \n 
if isinstance ( entry , basestring ) : \n 
~~~ ref_name = entry \n 
name = entry . replace ( , ) \n 
~~ elif isinstance ( entry , tuple ) : \n 
~~~ cont . raise_exception ( % entry , \n 
~~ newtrait = cont . get_trait ( name ) \n 
if newtrait is not None : \n 
~~~ cont . raise_exception ( \n 
~~ if not cont . contains ( ref_name ) : \n 
~~ cont . add_trait ( name , cont . build_trait ( ref_name , iostat , trait ) ) \n 
__all__ = [ "VarTree" ] \n 
from traits . api import Instance \n 
from openmdao . main . variable import Variable , gui_excludes \n 
class VarTree ( Variable ) : \n 
def __init__ ( self , default_value , allow_none = True , ** metadata ) : \n 
if isinstance ( default_value , VariableTree ) : \n 
~~~ klass = default_value . __class__ \n 
if in metadata : \n 
~~~ default_value . _iotype = metadata [ ] \n 
~~~ metadata [ ] = default_value . iotype \n 
~~ metadata . setdefault ( , ) \n 
self . _allow_none = allow_none \n 
self . klass = klass \n 
self . _instance = Instance ( klass = klass , allow_none = False , factory = None , \n 
args = None , kw = None , ** metadata ) \n 
self . _instance . default_value = default_value \n 
super ( VarTree , self ) . __init__ ( default_value , ** metadata ) \n 
~~ def validate ( self , obj , name , value ) : \n 
~~~ if self . _allow_none : \n 
~~ self . validate_failed ( obj , name , value ) \n 
~~~ value = self . _instance . validate ( obj , name , value ) \n 
~~~ obj . raise_exception ( % \n 
( name , self . _instance . klass . __module__ , \n 
self . _instance . klass . __name__ , type ( value ) ) , \n 
TypeError ) \n 
~~ def post_setattr ( self , obj , name , value ) : \n 
if value . parent is not obj : \n 
~~~ value . parent = obj \n 
value . name = name \n 
~~ value . _iotype = self . iotype \n 
~~ def get_attribute ( self , name , value , trait , meta ) : \n 
io_attr = { } \n 
io_attr [ ] = name \n 
io_attr [ ] = trait . trait_type . klass . __name__ \n 
io_attr [ ] = \n 
for field in meta : \n 
~~~ if field not in gui_excludes : \n 
~~~ io_attr [ field ] = meta [ field ] \n 
~~ ~~ return io_attr , None \n 
def _redirect_streams ( to_fd ) : \n 
original_stdout_fd = sys . stdout . fileno ( ) \n 
original_stderr_fd = sys . stderr . fileno ( ) \n 
sys . stdout . close ( ) \n 
sys . stderr . close ( ) \n 
os . dup2 ( to_fd , original_stdout_fd ) \n 
os . dup2 ( to_fd , original_stderr_fd ) \n 
sys . stderr = os . fdopen ( original_stderr_fd , , 0 ) \n 
~~ def use_proc_files ( ) : \n 
~~~ if MPI is not None : \n 
~~~ rank = MPI . COMM_WORLD . rank \n 
sname = "%s.out" % rank \n 
ofile = open ( sname , ) \n 
_redirect_streams ( ofile . fileno ( ) ) \n 
~~ ~~ def under_mpirun ( ) : \n 
for name in os . environ . keys ( ) : \n 
~~~ if name . startswith ( ) or name . startswith ( ) : \n 
~~ class PETSc ( object ) : \n 
~~~ self . needs_ksp = False \n 
self . _PETSc = None \n 
def installed ( self ) : \n 
~~~ if self . _PETSc is None : \n 
~~~ PETSc = _import_petsc ( ) \n 
del sys . modules [ ] \n 
self . _PETSc = PETSc \n 
~~~ self . _PETSc = None \n 
~~ ~~ def __getattr__ ( self , name ) : \n 
~~~ if self . installed : \n 
~~~ return getattr ( self . _PETSc , name ) \n 
~~ raise AttributeError ( name ) \n 
~~ ~~ def create_petsc_vec ( comm , arr ) : \n 
~~~ if under_mpirun ( ) or PETSc . needs_ksp : \n 
~~~ if PETSc . installed and ( MPI is None or comm != MPI . COMM_NULL ) : \n 
~~~ return PETSc . Vec ( ) . createWithArray ( arr , comm = comm ) \n 
~~ def _import_petsc ( ) : \n 
~~~ import petsc4py \n 
from petsc4py import PETSc \n 
return PETSc \n 
~~ if under_mpirun ( ) : \n 
~~~ from mpi4py import MPI \n 
PETSc = _import_petsc ( ) \n 
PETSc . installed = True \n 
COMM_NULL = MPI . COMM_NULL \n 
~~~ MPI = None \n 
COMM_NULL = None \n 
PETSc = PETSc ( ) \n 
~~ class MPI_info ( object ) : \n 
~~~ self . requested_cpus = ( 1 , 1 ) \n 
self . comm = COMM_NULL \n 
def size ( self ) : \n 
~~~ if MPI and self . comm != COMM_NULL : \n 
~~~ return self . comm . size \n 
def rank ( self ) : \n 
~~~ if MPI : \n 
~~~ if self . comm != COMM_NULL : \n 
~~~ return self . comm . rank \n 
~~ ~~ def get_norm ( vec , order = None ) : \n 
if MPI : \n 
~~~ vec . petsc_vec . assemble ( ) \n 
return vec . petsc_vec . norm ( ) \n 
~~~ return numpy . linalg . norm ( vec . array , ord = order ) \n 
~~ ~~ idx_arr_type = PETSc . IntType if MPI else \n 
def make_idx_array ( start , end ) : \n 
return numpy . arange ( start , end , dtype = idx_arr_type ) \n 
~~ def to_idx_array ( idxs ) : \n 
return numpy . array ( idxs , dtype = idx_arr_type ) \n 
~~ def evenly_distrib_idxs ( num_divisions , arr_size ) : \n 
base = arr_size / num_divisions \n 
leftover = arr_size % num_divisions \n 
sizes = numpy . ones ( num_divisions , dtype = "int" ) * base \n 
sizes [ : leftover ] += 1 \n 
offsets = numpy . zeros ( num_divisions , dtype = "int" ) \n 
offsets [ 1 : ] = numpy . cumsum ( sizes ) [ : - 1 ] \n 
return sizes , offsets \n 
def MPIContext ( ) : \n 
~~~ exc_type , exc_val , exc_tb = sys . exc_info ( ) \n 
if exc_val is not None : \n 
~~~ fail = True \n 
~~~ fail = False \n 
~~ fails = MPI . COMM_WORLD . allgather ( fail ) \n 
if fail or not any ( fails ) : \n 
~~~ raise exc_type , exc_val , exc_tb \n 
~~~ for i , f in enumerate ( fails ) : \n 
~~ ~~ ~~ ~~ ~~ if os . environ . get ( ) : \n 
~~~ use_proc_files ( ) \n 
from openmdao . main . datatypes . api import Float , Array \n 
from openmdao . lib . drivers . api import CONMINdriver , BroydenSolver , SensitivityDriver , FixedPointIterator \n 
from openmdao . lib . optproblems import sellar \n 
class Dis12Linear ( Component ) : \n 
z1 = Float ( 0. , iotype = ) \n 
z2 = Float ( 0. , iotype = ) \n 
z_store = Array ( [ 0. , 0. ] , iotype = ) \n 
ssa_F = Array ( [ 0.0 ] , iotype = ) \n 
ssa_G = Array ( [ 0.0 , 0.0 ] , iotype = ) \n 
ssa_dF = Array ( [ 0.0 , 0.0 ] , iotype = ) \n 
ssa_dG = Array ( [ [ 0.0 , 0.0 ] , [ 0.0 , 0.0 ] ] , iotype = ) \n 
obj = Float ( 0.0 , iotype = ) \n 
con1 = Float ( 0.0 , iotype = ) \n 
con2 = Float ( 0.0 , iotype = ) \n 
~~~ self . obj = self . ssa_F [ 0 ] + self . ssa_dF [ 0 ] * ( self . z_store [ 0 ] - self . z1 ) + self . ssa_dF [ 1 ] * ( self . z_store [ 1 ] - self . z2 ) \n 
self . con1 = self . ssa_G [ 0 ] + self . ssa_dG [ 0 ] [ 0 ] * ( self . z_store [ 0 ] - self . z1 ) + self . ssa_dG [ 0 ] [ 1 ] * ( self . z_store [ 1 ] - self . z2 ) \n 
self . con2 = self . ssa_G [ 1 ] + self . ssa_dG [ 1 ] [ 0 ] * ( self . z_store [ 0 ] - self . z1 ) + self . ssa_dG [ 1 ] [ 1 ] * ( self . z_store [ 1 ] - self . z2 ) \n 
~~ ~~ class SellarBLISS ( Assembly ) : \n 
~~~ z_store = Array ( [ 0. , 0. ] , dtype = Float , iotype = ) \n 
self . add ( , Dis12Linear ( ) ) \n 
self . add ( , CONMINdriver ( ) ) \n 
self . sysopt . add_parameter ( , low = - 10.0 , high = 10.0 , start = 5.0 ) \n 
self . sysopt . add_objective ( ) \n 
self . driver . workflow . add ( [ ] ) \n 
self . sysopt . workflow . add ( [ ] ) \n 
~~ ~~ class BndryFullSubTestCase ( unittest . TestCase ) : \n 
~~~ def test_MDF ( self ) : \n 
~~~ prob = SellarBLISS ( ) \n 
prob . name = \n 
prob . run ( ) \n 
from openmdao . main . api import Assembly , Component , Driver , set_as_top \n 
from openmdao . main . hasconstraints import HasConstraints , HasEqConstraints , HasIneqConstraints , Constraint , Has2SidedConstraints \n 
from openmdao . main . interfaces import IHas2SidedConstraints , implements \n 
from openmdao . main . pseudocomp import SimpleEQConPComp , SimpleEQ0PComp \n 
from openmdao . main . test . simpledriver import SimpleDriver \n 
from openmdao . test . execcomp import ExecComp \n 
from openmdao . units . units import PhysicalQuantity \n 
@ add_delegate ( HasConstraints ) \n 
class MyDriver ( Driver ) : \n 
~~ @ add_delegate ( HasEqConstraints ) \n 
class MyEqDriver ( Driver ) : \n 
~~ @ add_delegate ( HasIneqConstraints ) \n 
class MyInEqDriver ( Driver ) : \n 
~~ @ add_delegate ( HasConstraints , Has2SidedConstraints ) \n 
class My2SDriver ( Driver ) : \n 
~~~ implements ( IHas2SidedConstraints ) \n 
~~ class SimpleUnits ( Component ) : \n 
~~~ a = Float ( iotype = , units = ) \n 
b = Float ( iotype = , units = ) \n 
c = Float ( iotype = , units = ) \n 
d = Float ( iotype = , units = ) \n 
arr = Array ( [ 1. , 2. , 3. ] , iotype = , units = ) \n 
arr_out = Array ( [ 1. , 2. , 3. ] , iotype = , units = ) \n 
~~~ super ( SimpleUnits , self ) . __init__ ( ) \n 
self . a = 1 \n 
self . b = 2 \n 
self . c = 3 \n 
self . d = - 1 \n 
~~~ self . c = PhysicalQuantity ( self . a + self . b , ) . in_units_of ( ) . value \n 
self . d = PhysicalQuantity ( self . a - self . b , ) . in_units_of ( ) . value \n 
~~ ~~ class Simple ( Component ) : \n 
~~~ a = Float ( iotype = ) \n 
b = Float ( iotype = ) \n 
c = Float ( iotype = ) \n 
d = Float ( iotype = ) \n 
~~~ super ( Simple , self ) . __init__ ( ) \n 
~~~ self . c = self . a + self . b \n 
self . d = self . a - self . b \n 
~~ def list_deriv_vars ( self ) : \n 
~~~ return ( , ) , ( , ) \n 
~~ def provideJ ( self ) : \n 
~~~ der = 1.0 \n 
return np . array ( [ [ der , der ] , [ der , der ] ] ) \n 
~~ ~~ class HasConstraintsTestCase ( unittest . TestCase ) : \n 
~~~ self . asm = set_as_top ( Assembly ( ) ) \n 
self . asm . add ( , Simple ( ) ) \n 
self . asm . add ( , SimpleUnits ( ) ) \n 
~~ def test_list_constraints ( self ) : \n 
~~~ drv = self . asm . add ( , MyDriver ( ) ) \n 
self . asm . run ( ) \n 
drv . add_constraint ( ) \n 
self . assertEqual ( drv . list_constraints ( ) , \n 
[ , ] ) \n 
~~ def test_list_eq_constraints ( self ) : \n 
~~~ drv = self . asm . add ( , MyEqDriver ( ) ) \n 
~~ def test_list_ineq_constraints ( self ) : \n 
~~ def _check_ineq_add_constraint ( self , drv ) : \n 
~~~ self . asm . add ( , drv ) \n 
~~~ drv . add_constraint ( ) \n 
~~ self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , 0 ) \n 
~~~ self . assertEqual ( str ( err ) , \n 
~~ self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , 1 ) \n 
drv . remove_constraint ( ) \n 
self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , 0 ) \n 
~~~ drv . remove_constraint ( ) \n 
~~ drv . add_constraint ( ) \n 
self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , 1 ) \n 
drv . add_constraint ( , name = ) \n 
self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , 2 ) \n 
~~~ drv . add_constraint ( , name = ) \n 
~~ self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , 2 ) \n 
drv . clear_constraints ( ) \n 
~~ except ValueError as err : \n 
~~ ~~ def _check_eq_add_constraint ( self , drv ) : \n 
self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , 0 ) \n 
self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , 1 ) \n 
~~ drv . remove_constraint ( ) \n 
~~ self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , 0 ) \n 
self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , 2 ) \n 
~~ ~~ def _check_eq_eval_constraints ( self , drv ) : \n 
vals = drv . eval_eq_constraints ( ) \n 
self . assertEqual ( len ( vals ) , 0 ) \n 
self . asm . comp1 . a = 4 \n 
self . asm . comp1 . b = 5 \n 
self . asm . comp1 . c = 9 \n 
self . asm . comp1 . d = - 1 \n 
self . assertEqual ( len ( vals ) , 1 ) \n 
self . assertEqual ( vals [ 0 ] , 10. ) \n 
vals = drv . get_eq_constraints ( ) \n 
self . assertTrue ( isinstance ( vals [ ] , Constraint ) ) \n 
~~ def _check_ineq_eval_constraints ( self , drv ) : \n 
vals = drv . eval_ineq_constraints ( ) \n 
self . assertEqual ( vals [ 0 ] , 1 ) \n 
vals = drv . get_ineq_constraints ( ) \n 
~~ def test_constraint_scaler_adder ( self ) : \n 
self . asm . comp1 . a = 3000 \n 
self . asm . comp1 . b = 5000 \n 
result = drv . eval_ineq_constraints ( ) \n 
self . assertEqual ( result [ 0 ] , - 5001.0 ) \n 
self . assertEqual ( result , [ ] ) \n 
#self.assertEqual(str(err), \n 
~~ def test_add_constraint_eq_eq ( self ) : \n 
~~~ drv = MyDriver ( ) \n 
self . asm . add ( , drv ) \n 
~~ ~~ def test_add_constraint ( self ) : \n 
self . _check_eq_add_constraint ( drv ) \n 
self . _check_ineq_add_constraint ( drv ) \n 
~~ def test_add_eq_constraint ( self ) : \n 
~~~ self . _check_eq_add_constraint ( MyEqDriver ( ) ) \n 
~~ def test_add_ineq_constraint ( self ) : \n 
~~~ self . _check_ineq_add_constraint ( MyInEqDriver ( ) ) \n 
~~ def test_implicit_constraint ( self ) : \n 
~~ except ValueError , err : \n 
~~ ~~ def test_eval_constraint ( self ) : \n 
~~~ self . _check_eq_eval_constraints ( MyDriver ( ) ) \n 
self . _check_ineq_eval_constraints ( MyDriver ( ) ) \n 
~~ def test_eval_eq_constraint ( self ) : \n 
~~~ self . _check_eq_eval_constraints ( MyEqDriver ( ) ) \n 
~~ def test_eval_ineq_constraint ( self ) : \n 
~~~ self . _check_ineq_eval_constraints ( MyInEqDriver ( ) ) \n 
~~ def test_pseudocomps ( self ) : \n 
~~~ self . asm . add ( , MyDriver ( ) ) \n 
self . asm . driver . workflow . add ( [ , ] ) \n 
self . asm . _setup ( ) \n 
self . assertEqual ( self . asm . _depgraph . list_connections ( ) , \n 
[ ] ) \n 
self . asm . driver . add_constraint ( ) \n 
self . assertEqual ( self . asm . _pseudo_0 . _orig_expr , ) \n 
self . assertEqual ( set ( self . asm . _depgraph . list_connections ( drivers = False ) ) , \n 
set ( [ ( , ) , ( , ) ] ) ) \n 
self . asm . driver . remove_constraint ( ) \n 
self . assertEqual ( self . asm . _depgraph . list_connections ( drivers = False ) , [ ] ) \n 
set ( [ ( , ) ] ) ) \n 
self . assertEqual ( self . asm . _pseudo_1 . _orig_expr , ) \n 
self . assertEqual ( self . asm . _pseudo_2 . _orig_expr , ) \n 
self . assertEqual ( self . asm . _pseudo_3 . _orig_expr , ) \n 
self . assertEqual ( self . asm . _pseudo_4 . _orig_expr , ) \n 
self . asm . driver . clear_constraints ( ) \n 
self . asm . comp1 . a = 2 \n 
self . asm . comp1 . b = 1 \n 
self . asm . comp2 . a = 4 \n 
self . asm . comp2 . b = 2 \n 
self . assertEqual ( self . asm . _pseudo_5 . out0 , 1.0 ) \n 
self . assertEqual ( self . asm . _pseudo_6 . out0 , - 1.0 ) \n 
self . assertEqual ( self . asm . _pseudo_7 . out0 , 2.0 ) \n 
~~ def test_custom_pseudocomp_creation ( self ) : \n 
arg = { } \n 
self . assertEqual ( self . asm . _pseudo_0 . __class__ , SimpleEQ0PComp ) \n 
arg [ ] = np . array ( [ 3.3 ] ) \n 
result [ ] = np . array ( [ 0.0 ] ) \n 
self . asm . _pseudo_0 . apply_deriv ( arg , result ) \n 
self . assertEqual ( result [ ] [ 0 ] , 3.3 ) \n 
self . assertEqual ( self . asm . _pseudo_1 . __class__ , SimpleEQ0PComp ) \n 
self . asm . _pseudo_1 . apply_deriv ( arg , result ) \n 
self . assertEqual ( self . asm . _pseudo_2 . __class__ , SimpleEQConPComp ) \n 
arg [ ] = np . array ( [ 7.2 ] ) \n 
arg [ ] = np . array ( [ 3.1 ] ) \n 
self . asm . _pseudo_2 . apply_deriv ( arg , result ) \n 
self . assertEqual ( result [ ] [ 0 ] , 4.1 ) \n 
self . assertEqual ( self . asm . _pseudo_3 . __class__ , SimpleEQConPComp ) \n 
self . asm . _pseudo_3 . apply_deriv ( arg , result ) \n 
self . assertEqual ( self . asm . _pseudo_4 . __class__ , SimpleEQConPComp ) \n 
self . asm . _pseudo_4 . apply_deriv ( arg , result ) \n 
~~ def test_custom_jacobian ( self ) : \n 
~~~ class AComp ( Component ) : \n 
~~~ x = Array ( [ [ 1.0 , 3.0 ] , [ - 2.0 , 4.0 ] ] , iotype = ) \n 
y = Array ( np . zeros ( ( 2 , 2 ) ) , iotype = ) \n 
~~~ super ( AComp , self ) . __init__ ( ) \n 
self . J = np . array ( [ [ 3.5 , - 2.5 , 1.5 , 4.0 ] , \n 
[ 4.0 , 2.0 , - 1.1 , 3.4 ] , \n 
[ 7.7 , 6.6 , 4.4 , 1.1 ] , \n 
[ 0.1 , 3.3 , 6.8 , - 5.5 ] ] ) \n 
y = self . J . dot ( self . x . flatten ( ) ) \n 
self . y = y . reshape ( ( 2 , 2 ) ) \n 
input_keys = ( , ) \n 
output_keys = ( , ) \n 
return input_keys , output_keys \n 
return self . J \n 
~~ ~~ def fake_jac ( ) : \n 
jacs = { } \n 
jacs [ ] = np . array ( [ [ 100.0 , 101 , 102 , 103 ] , \n 
[ 104 , 105 , 106 , 107 ] , \n 
[ 108 , 109 , 110 , 111 ] , \n 
[ 112 , 113 , 114 , 115 ] ] ) \n 
return jacs \n 
~~ top = set_as_top ( Assembly ( ) ) \n 
top . add ( , SimpleDriver ( ) ) \n 
top . add ( , AComp ( ) ) \n 
top . driver . workflow . add ( ) \n 
top . driver . add_parameter ( , low = 10 , high = 10 ) \n 
top . driver . add_constraint ( , jacs = fake_jac ) \n 
top . _setup ( ) \n 
top . run ( ) \n 
J = top . driver . calc_gradient ( mode = , return_format = ) \n 
J = J [ ] [ ] \n 
diff = np . abs ( J - top . comp . J ) \n 
assert_rel_error ( self , diff . max ( ) , 0.0 , 1e-4 ) \n 
diff = np . abs ( J - fake_jac ( ) [ ] ) \n 
top . driver . clear_constraints ( ) \n 
top . _pseudo_count = 0 \n 
top . driver . gradient_options . lin_solver = \n 
def fake_jac2 ( ) : \n 
~~ top . driver . clear_constraints ( ) \n 
top . driver . add_constraint ( , jacs = fake_jac2 ) \n 
J_abs = np . abs ( J ) \n 
assert_rel_error ( self , J_abs . max ( ) , 0.0 , 1e-4 ) \n 
~~ ~~ class Has2SidedConstraintsTestCase ( unittest . TestCase ) : \n 
~~ def test_unsupported ( self ) : \n 
~~ except AttributeError as err : \n 
~~ ~~ def test_get_2sided_constraints ( self ) : \n 
~~~ drv = self . asm . add ( , My2SDriver ( ) ) \n 
cons = drv . get_2sided_constraints ( ) \n 
self . assertTrue ( len ( cons ) == 2 ) \n 
con1 = cons [ ] \n 
self . assertEqual ( self . asm . comp1 . a , con1 . evaluate ( self . asm ) [ 0 ] ) \n 
self . assertEqual ( con1 . low , - 44.1 ) \n 
self . assertEqual ( con1 . high , 13.0 ) \n 
self . assertEqual ( self . asm . comp1 . c , con1 . evaluate ( self . asm ) [ 0 ] ) \n 
self . assertEqual ( con1 . low , 77.0 ) \n 
self . assertEqual ( con1 . high , 79.0 ) \n 
cons = drv . get_constraints ( ) \n 
self . assertTrue ( len ( cons ) == 0 ) \n 
cons = drv . list_constraints ( ) \n 
self . assertTrue ( in cons ) \n 
~~ def test_gradient ( self ) : \n 
J = drv . calc_gradient ( inputs = [ ] ) \n 
assert_rel_error ( self , J [ 0 ] [ 0 ] , 2.5 , 1e-5 ) \n 
assert_rel_error ( self , J [ 1 ] [ 0 ] , 1.0 , 1e-5 ) \n 
assert_rel_error ( self , J [ 2 ] [ 0 ] , 1.0 , 1e-5 ) \n 
assert_rel_error ( self , J [ 3 ] [ 0 ] , 3.0 , 1e-5 ) \n 
~~ def test_replace ( self ) : \n 
self . asm . replace ( , My2SDriver ( ) ) \n 
from openmdao . main . api import Component , Assembly \n 
from openmdao . main . datatypes . api import Float , List , Array \n 
from numpy import array , zeros \n 
class MyDefComp ( Component ) : \n 
~~~ f_in = Float ( 3.14 , iotype = ) \n 
f_out = Float ( iotype = ) \n 
arr_in = Array ( [ 1. , 2. , 3. ] , iotype = ) \n 
list_in = List ( value = [ , , ] , iotype = ) \n 
~~~ self . f_out = self . f_in + 1. \n 
~~ ~~ class MyNoDefComp ( Component ) : \n 
~~~ f_in = Float ( iotype = ) \n 
arr_in = Array ( iotype = ) \n 
list_in = List ( iotype = ) \n 
~~ ~~ class SetDefaultsTestCase ( unittest . TestCase ) : \n 
~~~ def test_set_to_unset_default ( self ) : \n 
~~~ comp = MyNoDefComp ( ) \n 
self . assertEqual ( 0. , comp . f_in ) \n 
comp . f_in = 42. \n 
comp . arr_in = array ( [ 88. , 32. ] ) \n 
comp . list_in = [ 1 , 2 , 3 ] \n 
comp . run ( ) \n 
comp . revert_to_defaults ( ) \n 
self . assertTrue ( all ( zeros ( 0 , ) == comp . arr_in ) ) \n 
self . assertEqual ( [ ] , comp . list_in ) \n 
~~ def test_set_to_default ( self ) : \n 
~~~ comp = MyDefComp ( ) \n 
self . assertEqual ( 3.14 , comp . f_in ) \n 
self . assertFalse ( array ( [ 1. , 2. , 3. ] ) == comp . arr_in ) \n 
self . assertTrue ( all ( array ( [ 1. , 2. , 3. ] ) == comp . arr_in ) ) \n 
~~ def test_set_recursive ( self ) : \n 
~~~ asm = Assembly ( ) \n 
asm . add ( , MyDefComp ( ) ) \n 
asm . add ( , MyNoDefComp ( ) ) \n 
self . assertEqual ( 0. , asm . nodefcomp . f_in ) \n 
self . assertEqual ( 3.14 , asm . defcomp . f_in ) \n 
asm . nodefcomp . f_in = 99 \n 
asm . defcomp . f_in = 99 \n 
asm . revert_to_defaults ( ) \n 
description = __doc__ , \n 
packages = [ ] , \n 
from pyparsing import CaselessLiteral , Combine , OneOrMore , Optional , TokenConverter , Word , nums , oneOf , printables , ParserElement , alphanums \n 
from numpy import append , array , zeros \n 
def _getformat ( val ) : \n 
~~~ if int ( val ) == val : \n 
~~~ return "%.1f" \n 
~~~ return "%.16g" \n 
~~ ~~ class _SubHelper ( object ) : \n 
~~~ self . newtext = "" \n 
self . replace_location = 0 \n 
self . current_location = 0 \n 
self . counter = 0 \n 
self . start_location = 0 \n 
self . end_location = 0 \n 
~~ def set ( self , newtext , location ) : \n 
self . newtext = newtext \n 
self . replace_location = location \n 
~~ def set_array ( self , newtext , start_location , end_location ) : \n 
self . start_location = start_location \n 
self . end_location = end_location \n 
~~ def replace ( self , text ) : \n 
self . current_location += 1 \n 
if self . current_location == self . replace_location : \n 
~~~ if isinstance ( self . newtext , float ) : \n 
~~~ return _getformat ( self . newtext ) % self . newtext \n 
~~~ return str ( self . newtext ) \n 
~~~ return text . group ( ) \n 
~~ ~~ def replace_array ( self , text ) : \n 
end = len ( self . newtext ) \n 
if self . current_location >= self . start_location and self . current_location <= self . end_location and self . counter < end : \n 
~~~ if isinstance ( self . newtext [ self . counter ] , float ) : \n 
~~~ val = self . newtext [ self . counter ] \n 
newval = _getformat ( val ) % val \n 
~~~ newval = str ( self . newtext [ self . counter ] ) \n 
~~ self . counter += 1 \n 
return newval \n 
~~ ~~ ~~ class ToInteger ( TokenConverter ) : \n 
def postParse ( self , instring , loc , tokenlist ) : \n 
return int ( tokenlist [ 0 ] ) \n 
~~ ~~ class ToFloat ( TokenConverter ) : \n 
return float ( tokenlist [ 0 ] . replace ( , ) ) \n 
~~ ~~ class ToNan ( TokenConverter ) : \n 
return float ( ) \n 
~~ ~~ class ToInf ( TokenConverter ) : \n 
~~ ~~ class InputFileGenerator ( object ) : \n 
~~~ self . template_filename = [ ] \n 
self . output_filename = [ ] \n 
self . reg = re . compile ( ) \n 
self . data = [ ] \n 
self . current_row = 0 \n 
self . anchored = False \n 
~~ def set_template_file ( self , filename ) : \n 
self . template_filename = filename \n 
templatefile = open ( filename , ) \n 
self . data = templatefile . readlines ( ) \n 
templatefile . close ( ) \n 
~~ def set_generated_file ( self , filename ) : \n 
self . output_filename = filename \n 
~~ def set_delimiters ( self , delimiter ) : \n 
self . delimiter = delimiter \n 
self . reg = re . compile ( + delimiter + ) \n 
~~ def mark_anchor ( self , anchor , occurrence = 1 ) : \n 
if not isinstance ( occurrence , int ) : \n 
~~ instance = 0 \n 
if occurrence > 0 : \n 
~~~ count = 0 \n 
max_lines = len ( self . data ) \n 
for index in xrange ( self . current_row , max_lines ) : \n 
~~~ line = self . data [ index ] \n 
if count == 0 and self . anchored : \n 
~~~ line = line . split ( anchor ) [ - 1 ] \n 
~~ if line . find ( anchor ) > - 1 : \n 
~~~ instance += 1 \n 
if instance == occurrence : \n 
~~~ self . current_row += count \n 
self . anchored = True \n 
~~ ~~ count += 1 \n 
~~ ~~ elif occurrence < 0 : \n 
~~~ max_lines = len ( self . data ) - 1 \n 
count = max_lines \n 
for index in xrange ( max_lines , - 1 , - 1 ) : \n 
if count == max_lines and self . anchored : \n 
~~~ line = line . split ( anchor ) [ 0 ] \n 
~~~ instance += - 1 \n 
~~~ self . current_row = count \n 
~~ ~~ count -= 1 \n 
~~ def reset_anchor ( self ) : \n 
~~ def transfer_var ( self , value , row , field ) : \n 
j = self . current_row + row \n 
line = self . data [ j ] \n 
sub = _SubHelper ( ) \n 
sub . set ( value , field ) \n 
newline = re . sub ( self . reg , sub . replace , line ) \n 
self . data [ j ] = newline \n 
~~ def transfer_array ( self , value , row_start , field_start , field_end , \n 
if row_end == None : \n 
~~~ row_end = row_start \n 
~~ sub = _SubHelper ( ) \n 
for row in range ( row_start , row_end + 1 ) : \n 
~~~ j = self . current_row + row \n 
if row == row_end : \n 
~~~ f_end = field_end \n 
~~~ f_end = 99999 \n 
~~ sub . set_array ( value , field_start , f_end ) \n 
field_start = 0 \n 
newline = re . sub ( self . reg , sub . replace_array , line ) \n 
~~ if sub . counter < len ( value ) : \n 
~~~ for val in value [ sub . counter : ] : \n 
~~~ newline = newline . rstrip ( ) + sep + str ( val ) \n 
~~ self . data [ j ] = newline \n 
~~ elif sub . counter > len ( value ) : \n 
~~ self . data [ j ] += "\\n" \n 
~~ def transfer_2Darray ( self , value , row_start , row_end , field_start , \n 
sub . set_array ( value [ i , : ] , field_start , field_end ) \n 
sub . current_location = 0 \n 
sub . counter = 0 \n 
~~ ~~ def clearline ( self , row ) : \n 
self . data [ self . current_row + row ] = "\\n" \n 
~~ def generate ( self ) : \n 
infile = open ( self . output_filename , ) \n 
infile . writelines ( self . data ) \n 
infile . close ( ) \n 
~~ ~~ class FileParser ( object ) : \n 
def __init__ ( self , end_of_line_comment_char = None , full_line_comment_char = None ) : \n 
~~~ self . filename = [ ] \n 
self . end_of_line_comment_char = end_of_line_comment_char \n 
self . full_line_comment_char = full_line_comment_char \n 
self . set_delimiters ( self . delimiter ) \n 
~~ def set_file ( self , filename ) : \n 
inputfile = open ( filename , ) \n 
if not self . end_of_line_comment_char and not self . full_line_comment_char : \n 
~~~ self . data = inputfile . readlines ( ) \n 
~~~ self . data = [ ] \n 
for line in inputfile : \n 
~~~ if line [ 0 ] == self . full_line_comment_char : \n 
~~ self . data . append ( line . split ( self . end_of_line_comment_char ) [ 0 ] ) \n 
~~ ~~ inputfile . close ( ) \n 
if delimiter != "columns" : \n 
~~~ ParserElement . setDefaultWhitespaceChars ( str ( delimiter ) ) \n 
~~ self . _reset_tokens ( ) \n 
~~ if anchor in line : \n 
~~ def transfer_line ( self , row ) : \n 
return self . data [ self . current_row + row ] . rstrip ( ) \n 
~~ def transfer_var ( self , row , field , fieldend = None ) : \n 
if self . delimiter == "columns" : \n 
~~~ if not fieldend : \n 
~~~ line = line [ ( field - 1 ) : ] \n 
~~~ line = line [ ( field - 1 ) : ( fieldend ) ] \n 
~~ data = self . _parse_line ( ) . parseString ( line ) \n 
if len ( data ) > 1 : \n 
~~~ return line \n 
~~~ return data [ 0 ] \n 
~~~ data = self . _parse_line ( ) . parseString ( line ) \n 
return data [ field - 1 ] \n 
~~ ~~ def transfer_keyvar ( self , key , field , occurrence = 1 , rowoffset = 0 ) : \n 
if not isinstance ( occurrence , int ) or occurrence == 0 : \n 
raise ValueError ( msg ) \n 
~~~ row = 0 \n 
for line in self . data [ self . current_row : ] : \n 
~~~ if line . find ( key ) > - 1 : \n 
~~ ~~ row += 1 \n 
~~~ row = - 1 \n 
for line in reversed ( self . data [ self . current_row : ] ) : \n 
~~ ~~ row -= 1 \n 
~~ ~~ j = self . current_row + row + rowoffset \n 
fields = self . _parse_line ( ) . parseString ( line . replace ( key , "KeyField" ) ) \n 
return fields [ field ] \n 
~~ def transfer_array ( self , rowstart , fieldstart , rowend = None , fieldend = None ) : \n 
j1 = self . current_row + rowstart \n 
if rowend is None : \n 
~~~ j2 = j1 + 1 \n 
~~~ j2 = self . current_row + rowend + 1 \n 
~~ if not fieldend : \n 
~~ lines = self . data [ j1 : j2 ] \n 
data = zeros ( shape = ( 0 , 0 ) ) \n 
for i , line in enumerate ( lines ) : \n 
~~~ if self . delimiter == "columns" : \n 
~~~ line = line [ ( fieldstart - 1 ) : fieldend ] \n 
line = line . strip ( ) \n 
parsed = self . _parse_line ( ) . parseString ( line ) \n 
newdata = array ( parsed [ : ] ) \n 
if in str ( newdata . dtype ) : \n 
~~~ newdata = array ( line ) \n 
~~ data = append ( data , newdata ) \n 
~~~ parsed = self . _parse_line ( ) . parseString ( line ) \n 
if i == j2 - j1 - 1 : \n 
~~~ data = append ( data , array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) ) \n 
~~~ data = append ( data , array ( parsed [ ( fieldstart - 1 ) : ] ) ) \n 
~~ fieldstart = 1 \n 
~~ ~~ return data \n 
~~ def transfer_2Darray ( self , rowstart , fieldstart , rowend , fieldend = None ) : \n 
if fieldend and ( fieldstart > fieldend ) : \n 
~~ if rowstart > rowend : \n 
~~ j1 = self . current_row + rowstart \n 
j2 = self . current_row + rowend + 1 \n 
lines = list ( self . data [ j1 : j2 ] ) \n 
~~~ if fieldend : \n 
~~~ line = lines [ 0 ] [ ( fieldstart - 1 ) : fieldend ] \n 
~~~ line = lines [ 0 ] [ ( fieldstart - 1 ) : ] \n 
~~ parsed = self . _parse_line ( ) . parseString ( line ) \n 
row = array ( parsed [ : ] ) \n 
data = zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n 
data [ 0 , : ] = row \n 
for i , line in enumerate ( list ( lines [ 1 : ] ) ) : \n 
~~~ line = line [ ( fieldstart - 1 ) : ] \n 
data [ i + 1 , : ] = array ( parsed [ : ] ) \n 
~~~ parsed = self . _parse_line ( ) . parseString ( lines [ 0 ] ) \n 
if fieldend : \n 
~~~ row = array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) \n 
~~~ row = array ( parsed [ ( fieldstart - 1 ) : ] ) \n 
~~ data = zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n 
~~~ data [ i + 1 , : ] = array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) \n 
~~~ print data \n 
~~~ data [ i + 1 , : ] = array ( parsed [ ( fieldstart - 1 ) : ] ) \n 
~~ ~~ ~~ return data \n 
~~ def _parse_line ( self ) : \n 
return self . line_parse_token \n 
~~ def _reset_tokens ( self ) : \n 
if self . delimiter . isspace ( ) : \n 
~~~ textchars = printables \n 
~~~ textchars = alphanums \n 
symbols = [ , , , , , , , , , , \n 
, , , , , , , , , , \n 
, , , , , , ] \n 
for symbol in symbols : \n 
~~~ if symbol not in self . delimiter : \n 
~~~ textchars = textchars + symbol \n 
~~ ~~ ~~ digits = Word ( nums ) \n 
dot = "." \n 
ee = CaselessLiteral ( ) | CaselessLiteral ( ) \n 
num_int = ToInteger ( Combine ( Optional ( sign ) + digits ) ) \n 
num_float = ToFloat ( Combine ( Optional ( sign ) + \n 
( ( digits + dot + Optional ( digits ) ) | \n 
( dot + digits ) ) + \n 
Optional ( ee + Optional ( sign ) + digits ) \n 
mixed_exp = ToFloat ( Combine ( digits + ee + Optional ( sign ) + digits ) ) \n 
string_text = Word ( textchars ) \n 
self . line_parse_token = ( OneOrMore ( ( nan | num_float | mixed_exp | num_int | \n 
string_text ) ) ) \n 
from openmdao . api import Component , Group , Problem , Newton , ScipyGMRES \n 
class Line ( Component ) : \n 
~~~ super ( Line , self ) . __init__ ( ) \n 
self . add_param ( , 1.0 ) \n 
self . add_output ( , 0.0 ) \n 
self . slope = - 2.0 \n 
self . intercept = 4.0 \n 
~~ def solve_nonlinear ( self , params , unknowns , resids ) : \n 
x = params [ ] \n 
m = self . slope \n 
b = self . intercept \n 
unknowns [ ] = m * x + b \n 
~~ def linearize ( self , params , unknowns , resids ) : \n 
J = { } \n 
J [ , ] = m \n 
return J \n 
~~ ~~ class Parabola ( Component ) : \n 
~~~ super ( Parabola , self ) . __init__ ( ) \n 
self . a = 3.0 \n 
self . b = 0.0 \n 
self . c = - 5.0 \n 
a = self . a \n 
b = self . b \n 
c = self . c \n 
unknowns [ ] = a * x ** 2 + b * x + c \n 
J [ , ] = 2.0 * a * x + b \n 
~~ ~~ class Balance ( Component ) : \n 
~~~ super ( Balance , self ) . __init__ ( ) \n 
self . add_param ( , 0.0 ) \n 
self . add_state ( , 5.0 ) \n 
~~ def apply_nonlinear ( self , params , unknowns , resids ) : \n 
y1 = params [ ] \n 
y2 = params [ ] \n 
resids [ ] = y1 - y2 \n 
J [ , ] = 1.0 \n 
J [ , ] = - 1.0 \n 
~~~ top = Problem ( ) \n 
root = top . root = Group ( ) \n 
root . add ( , Line ( ) ) \n 
root . add ( , Parabola ( ) ) \n 
root . add ( , Balance ( ) ) \n 
root . connect ( , ) \n 
root . nl_solver = Newton ( ) \n 
root . ln_solver = ScipyGMRES ( ) \n 
top . setup ( ) \n 
top [ ] = 7.0 \n 
root . list_states ( ) \n 
print ( % ( top [ ] , top [ ] , top [ \n 
top [ ] = - 7.0 \n 
from openmdao . components . exec_comp import ExecComp \n 
class ConstraintComp ( ExecComp ) : \n 
def __init__ ( self , expr , out = ) : \n 
~~~ warnings . simplefilter ( , DeprecationWarning ) \n 
DeprecationWarning , stacklevel = 2 ) \n 
warnings . simplefilter ( , DeprecationWarning ) \n 
newexpr = _combined_expr ( expr ) \n 
~~ ~~ def _combined_expr ( expr ) : \n 
lhs , op , rhs = _parse_constraint ( expr ) \n 
first , second = ( rhs , lhs ) if op . startswith ( ) else ( lhs , rhs ) \n 
~~~ if float ( first ) == 0 : \n 
~~~ return "-(%s)" % second \n 
~~~ if float ( second ) == 0. : \n 
~~~ return first \n 
~~ return % ( first , second ) \n 
~~ def _parse_constraint ( expr_string ) : \n 
for comparator in [ , , , , , ] : \n 
~~~ parts = expr_string . split ( comparator ) \n 
if len ( parts ) == 2 : \n 
~~~ if comparator == : \n 
~~ return ( parts [ 0 ] . strip ( ) , comparator , parts [ 1 ] . strip ( ) ) \n 
from itertools import chain \n 
from six import iteritems , itervalues \n 
from six . moves import cStringIO \n 
import networkx as nx \n 
from openmdao . core . system import System \n 
from openmdao . core . group import Group \n 
from openmdao . core . component import Component \n 
from openmdao . core . parallel_group import ParallelGroup \n 
from openmdao . core . parallel_fd_group import ParallelFDGroup \n 
from openmdao . core . basic_impl import BasicImpl \n 
from openmdao . core . _checks import check_connections , _both_names \n 
from openmdao . core . driver import Driver \n 
from openmdao . core . mpi_wrap import MPI , under_mpirun , debug \n 
from openmdao . core . relevance import Relevance \n 
from openmdao . components . indep_var_comp import IndepVarComp \n 
from openmdao . solvers . scipy_gmres import ScipyGMRES \n 
from openmdao . solvers . ln_direct import DirectSolver \n 
from openmdao . solvers . ln_gauss_seidel import LinearGaussSeidel \n 
from openmdao . units . units import get_conversion_tuple \n 
from openmdao . util . string_util import get_common_ancestor , nearest_child , name_relative_to \n 
from openmdao . util . graph import plain_bfs \n 
from openmdao . util . options import OptionsDictionary \n 
force_check = os . environ . get ( ) \n 
trace = os . environ . get ( ) \n 
class _ProbData ( object ) : \n 
~~~ self . top_lin_gs = False \n 
self . in_complex_step = False \n 
~~ ~~ class Problem ( object ) : \n 
def __init__ ( self , root = None , driver = None , impl = None , comm = None ) : \n 
~~~ super ( Problem , self ) . __init__ ( ) \n 
self . root = root \n 
self . _probdata = _ProbData ( ) \n 
~~~ from openmdao . core . petsc_impl import PetscImpl \n 
if impl != PetscImpl : \n 
~~ ~~ if impl is None : \n 
~~~ self . _impl = BasicImpl \n 
~~~ self . _impl = impl \n 
~~ self . comm = comm \n 
if driver is None : \n 
~~~ self . driver = Driver ( ) \n 
~~~ self . driver = driver \n 
~~ self . pathname = \n 
~~ def __getitem__ ( self , name ) : \n 
if name in self . root . unknowns : \n 
~~~ return self . root . unknowns [ name ] \n 
~~ elif name in self . root . params : \n 
~~~ return self . root . params [ name ] \n 
~~ elif name in self . root . _sysdata . to_abs_pnames : \n 
~~~ for p in self . root . _sysdata . to_abs_pnames [ name ] : \n 
~~~ return self . _rec_get_param ( p ) \n 
~~ ~~ elif name in self . _dangling : \n 
~~~ for p in self . _dangling [ name ] : \n 
~~ ~~ def _rec_get_param ( self , absname ) : \n 
~~~ parts = absname . rsplit ( , 1 ) \n 
if len ( parts ) == 1 : \n 
~~~ return self . root . params [ absname ] \n 
~~~ grp = self . root . _subsystem ( parts [ 0 ] ) \n 
return grp . params [ parts [ 1 ] ] \n 
~~ ~~ def __setitem__ ( self , name , val ) : \n 
~~~ self . root . unknowns [ name ] = val \n 
~~ elif name in self . _dangling : \n 
~~~ parts = p . rsplit ( , 1 ) \n 
~~~ self . root . params [ p ] = val \n 
grp . params [ parts [ 1 ] ] = val \n 
~~ ~~ def _setup_connections ( self , params_dict , unknowns_dict ) : \n 
to_prom_name = self . _probdata . to_prom_name \n 
connections = self . root . _get_explicit_connections ( ) \n 
prom_noconns = self . _add_implicit_connections ( connections ) \n 
input_graph = nx . DiGraph ( ) \n 
self . _dangling = { } \n 
to_abs_pnames = self . root . _sysdata . to_abs_pnames \n 
usrcs = set ( ) \n 
for tgt , srcs in iteritems ( connections ) : \n 
~~~ for src , idxs in srcs : \n 
~~~ input_graph . add_edge ( src , tgt , idxs = idxs ) \n 
if src in unknowns_dict : \n 
~~~ usrcs . add ( src ) \n 
~~ ~~ ~~ for prom , plist in iteritems ( to_abs_pnames ) : \n 
~~~ input_graph . add_nodes_from ( plist ) \n 
if prom in prom_noconns : \n 
~~~ start = plist [ 0 ] \n 
input_graph . add_edges_from ( ( ( start , p ) for p in plist [ 1 : ] ) , \n 
idxs = None ) \n 
~~ ~~ newconns = { } \n 
for src in usrcs : \n 
~~~ newconns [ src ] = None \n 
src_idxs = { src : None } \n 
for s , t in nx . dfs_edges ( input_graph , src ) : \n 
~~~ tidxs = input_graph [ s ] [ t ] [ ] \n 
sidxs = src_idxs [ s ] \n 
if tidxs is None : \n 
~~~ tidxs = sidxs \n 
~~ elif sidxs is not None : \n 
~~~ tidxs = np . array ( sidxs ) [ tidxs ] \n 
~~ src_idxs [ t ] = tidxs \n 
if t in newconns : \n 
~~~ newconns [ t ] . append ( ( src , tidxs ) ) \n 
~~~ newconns [ t ] = [ ( src , tidxs ) ] \n 
~~ ~~ ~~ self . _input_inputs = { } \n 
for node in input_graph . nodes_iter ( ) : \n 
~~~ if node not in newconns and len ( input_graph . pred [ node ] ) == 0 : \n 
~~~ nosrc = [ node ] \n 
for s , t in nx . dfs_edges ( input_graph , node ) : \n 
~~~ src = newconns [ t ] [ 0 ] [ 0 ] \n 
for n in nosrc : \n 
~~~ newconns [ n ] = [ ( src , None ) ] \n 
~~ break \n 
~~~ nosrc . append ( t ) \n 
~~~ set_nosrc = set ( nosrc ) \n 
~~~ self . _dangling [ to_prom_name [ n ] ] = set_nosrc \n 
self . _input_inputs [ n ] = nosrc \n 
~~ ~~ ~~ ~~ connections = OrderedDict ( ) \n 
for tgt , srcs in sorted ( newconns . items ( ) ) : \n 
~~~ if srcs is not None : \n 
~~~ if len ( srcs ) > 1 : \n 
~~~ src_names = ( n for n , idx in srcs ) \n 
( tgt , sorted ( src_names ) ) ) \n 
~~ connections [ tgt ] = srcs [ 0 ] \n 
~~ ~~ return connections \n 
~~ def _check_input_diffs ( self , connections , params_dict , unknowns_dict ) : \n 
for tgt , connected_inputs in iteritems ( self . _input_inputs ) : \n 
~~~ tgt_idx = connected_inputs . index ( tgt ) \n 
units = [ params_dict [ n ] . get ( ) for n in connected_inputs ] \n 
vals = [ params_dict [ n ] [ ] for n in connected_inputs ] \n 
diff_units = [ ] \n 
for i , u in enumerate ( units ) : \n 
~~~ if i != tgt_idx and u != units [ tgt_idx ] : \n 
~~~ if units [ tgt_idx ] is None : \n 
~~~ sname , s = connected_inputs [ i ] , u \n 
tname , t = connected_inputs [ tgt_idx ] , units [ tgt_idx ] \n 
~~~ sname , s = connected_inputs [ tgt_idx ] , units [ tgt_idx ] \n 
tname , t = connected_inputs [ i ] , u \n 
~~ diff_units . append ( ( connected_inputs [ i ] , u ) ) \n 
~~ ~~ if isinstance ( vals [ tgt_idx ] , np . ndarray ) : \n 
~~~ diff_vals = [ ( connected_inputs [ i ] , v ) for i , v in \n 
enumerate ( vals ) if not \n 
( isinstance ( v , np . ndarray ) and \n 
v . shape == vals [ tgt_idx ] . shape and \n 
( v == vals [ tgt_idx ] ) . all ( ) ) ] \n 
~~~ vtype = type ( vals [ tgt_idx ] ) \n 
diff_vals = [ ( connected_inputs [ i ] , v ) for i , v in \n 
enumerate ( vals ) if vtype != type ( v ) or \n 
v != vals [ tgt_idx ] ] \n 
~~ if diff_units : \n 
~~~ filt = set ( [ u for n , u in diff_units ] ) \n 
if None in filt : \n 
~~~ filt . remove ( None ) \n 
~~ if filt : \n 
~~~ proms = set ( [ params_dict [ item ] [ ] for item in connected_inputs ] ) \n 
if len ( proms ) == 1 : \n 
correct_src = params_dict [ connected_inputs [ 0 ] ] [ ] \n 
self . _setup_errors . append ( msg ) \n 
~~ ~~ if diff_vals : \n 
( sorted ( [ ( tgt , params_dict [ tgt ] [ ] ) ] + \n 
diff_vals ) ) ) \n 
~~ ~~ for promname , absnames in iteritems ( self . root . _sysdata . to_abs_pnames ) : \n 
~~~ if len ( absnames ) > 1 : \n 
~~~ step_sizes , step_types , forms = { } , { } , { } \n 
for name in absnames : \n 
~~~ meta = self . root . _params_dict [ name ] \n 
ss = meta . get ( ) \n 
if ss is not None : \n 
~~~ step_sizes [ ss ] = name \n 
~~ st = meta . get ( ) \n 
if st is not None : \n 
~~~ step_types [ st ] = name \n 
~~ f = meta . get ( ) \n 
if f is not None : \n 
~~~ forms [ f ] = name \n 
~~ ~~ if len ( step_sizes ) > 1 : \n 
sorted ( [ ( v , k ) for k , v in step_sizes . items ( ) ] ) ) ) \n 
~~ if len ( step_types ) > 1 : \n 
sorted ( [ ( v , k ) for k , v in step_types . items ( ) ] ) ) ) \n 
~~ if len ( forms ) > 1 : \n 
sorted ( [ ( v , k ) for k , v in forms . items ( ) ] ) ) ) \n 
~~ ~~ ~~ ~~ def _get_ubc_vars ( self , connections ) : \n 
full_order = { s . pathname : i for i , s in \n 
enumerate ( self . root . subsystems ( recurse = True ) ) } \n 
ubcs = [ ] \n 
~~~ tsys = tgt . rsplit ( , 1 ) [ 0 ] \n 
ssys = srcs [ 0 ] . rsplit ( , 1 ) [ 0 ] \n 
if full_order [ ssys ] > full_order [ tsys ] : \n 
~~~ ubcs . append ( tgt ) \n 
~~ ~~ return ubcs \n 
~~ def setup ( self , check = True , out_stream = sys . stdout ) : \n 
self . _setup_errors = [ ] \n 
tree_changed = False \n 
meta_changed = False \n 
if isinstance ( self . root . ln_solver , LinearGaussSeidel ) : \n 
~~~ self . _probdata . top_lin_gs = True \n 
~~ self . driver . root = self . root \n 
self . root . _init_sys_data ( self . pathname , self . _probdata ) \n 
self . _setup_communicators ( ) \n 
params_dict , unknowns_dict = self . root . _setup_variables ( ) \n 
self . _probdata . params_dict = params_dict \n 
self . _probdata . unknowns_dict = unknowns_dict \n 
self . _probdata . to_prom_name = self . root . _sysdata . to_prom_name \n 
connections = self . _setup_connections ( params_dict , unknowns_dict ) \n 
self . _probdata . connections = connections \n 
for tgt , ( src , idxs ) in iteritems ( connections ) : \n 
~~~ tmeta = params_dict [ tgt ] \n 
if not in tmeta or not tmeta [ ] : \n 
~~~ if tmeta [ ] == ( ) : \n 
~~~ smeta = unknowns_dict [ src ] \n 
if idxs is not None : \n 
~~~ size = len ( idxs ) \n 
tmeta [ ] = ( size , ) \n 
tmeta [ ] = size \n 
tmeta [ ] = smeta [ ] [ np . array ( idxs ) ] \n 
~~~ tmeta [ ] = smeta [ ] \n 
tmeta [ ] = smeta [ ] \n 
~~ ~~ if idxs is not None : \n 
~~~ if isinstance ( idxs , np . ndarray ) : \n 
~~~ tmeta [ ] = idxs \n 
~~~ tmeta [ ] = np . array ( idxs , \n 
dtype = self . _impl . idx_arr_type ) \n 
~~ ~~ ~~ ~~ if MPI : \n 
~~~ for s in self . root . components ( recurse = True ) : \n 
~~~ if hasattr ( s , ) or ( \n 
hasattr ( s , ) and ( s . setup_distrib \n 
is not Component . setup_distrib ) ) : \n 
~~~ meta_changed = True \n 
~~ ~~ ~~ if tree_changed : \n 
~~~ return self . setup ( check = check , out_stream = out_stream ) \n 
~~ elif meta_changed : \n 
~~~ params_dict , unknowns_dict = self . root . _setup_variables ( compute_indices = True ) \n 
~~ self . _setup_errors . extend ( check_connections ( connections , params_dict , \n 
unknowns_dict , \n 
self . root . _sysdata . to_prom_name ) ) \n 
self . _setup_units ( connections , params_dict , unknowns_dict ) \n 
to_prom_name = self . root . _sysdata . to_prom_name \n 
self . _probdata . to_prom_name = to_prom_name \n 
for path , meta in iteritems ( params_dict ) : \n 
~~~ meta [ ] = to_prom_name [ path ] \n 
if path not in connections : \n 
~~~ if not in meta or not meta [ ] : \n 
~~~ if meta [ ] == ( ) : \n 
~~ ~~ ~~ ~~ for path , meta in iteritems ( unknowns_dict ) : \n 
~~ param_owners = _assign_parameters ( connections ) \n 
pois = self . driver . desvars_of_interest ( ) \n 
oois = self . driver . outputs_of_interest ( ) \n 
self . _driver_vois = set ( ) \n 
for tup in chain ( pois , oois ) : \n 
~~~ self . _driver_vois . update ( tup ) \n 
~~ promoted_unknowns = self . root . _sysdata . to_abs_uname \n 
parallel_p = False \n 
for vnames in pois : \n 
~~~ if len ( vnames ) > 1 : \n 
~~~ parallel_p = True \n 
~~ for v in vnames : \n 
~~~ if v not in promoted_unknowns : \n 
~~ ~~ ~~ parallel_u = False \n 
for vnames in oois : \n 
~~~ parallel_u = True \n 
~~ ~~ ~~ mode = self . _check_for_parallel_derivs ( pois , oois , parallel_u , parallel_p ) \n 
self . _probdata . relevance = Relevance ( self . root , params_dict , \n 
unknowns_dict , connections , \n 
pois , oois , mode ) \n 
for s in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if not s . _order_set : \n 
~~~ order = None \n 
broken_edges = None \n 
if self . comm . rank == 0 : \n 
~~~ order , broken_edges = s . list_auto_order ( ) \n 
~~ if MPI : \n 
~~~ if trace : \n 
~~ order , broken_edges = self . comm . bcast ( ( order , broken_edges ) , root = 0 ) \n 
if trace : \n 
~~ ~~ s . set_order ( order ) \n 
for edge in broken_edges : \n 
~~~ cname = edge [ 1 ] \n 
head_sys = self . root \n 
for name in cname . split ( ) : \n 
~~~ head_sys = getattr ( head_sys , name ) \n 
~~ head_sys . _run_apply = True \n 
~~ ~~ ~~ self . _check_input_diffs ( connections , params_dict , unknowns_dict ) \n 
alloc_derivs = not self . root . fd_options [ ] \n 
for sub in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ alloc_derivs = alloc_derivs or sub . nl_solver . supports [ ] \n 
~~ self . root . _setup_vectors ( param_owners , impl = self . _impl , alloc_derivs = alloc_derivs ) \n 
self . driver . _setup ( ) \n 
self . _poi_indices , self . _qoi_indices = self . driver . _map_voi_indices ( ) \n 
~~~ sub . nl_solver . setup ( sub ) \n 
sub . ln_solver . setup ( sub ) \n 
~~ self . _check_solvers ( ) \n 
self . _start_recorders ( ) \n 
if self . _setup_errors : \n 
~~~ stream = cStringIO ( ) \n 
for err in self . _setup_errors : \n 
~~~ stream . write ( "%s\\n" % err ) \n 
~~ raise RuntimeError ( stream . getvalue ( ) ) \n 
~~ OptionsDictionary . locked = True \n 
if check or force_check : \n 
~~~ return self . check_setup ( out_stream ) \n 
~~ def cleanup ( self ) : \n 
self . driver . cleanup ( ) \n 
self . root . cleanup ( ) \n 
~~ def _check_solvers ( self ) : \n 
iterated_states = set ( ) \n 
group_states = [ ] \n 
has_iter_solver = { } \n 
for group in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ has_iter_solver [ group . pathname ] = ( group . ln_solver . options [ ] > 1 ) \n 
~~~ if isinstance ( group . ln_solver , DirectSolver ) : \n 
~~~ has_iter_solver [ group . pathname ] = ( True ) \n 
~~ ~~ opt = group . fd_options \n 
if opt [ ] == True and opt [ ] == : \n 
~~~ if group . name != : \n 
~~ for sub in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if hasattr ( sub . nl_solver , ) : \n 
self . _setup_errors . append ( msg . format ( sub . name ) ) \n 
~~ ~~ ~~ parts = group . pathname . split ( ) \n 
for i in range ( len ( parts ) ) : \n 
~~~ if has_iter_solver [ . join ( parts [ : i ] ) ] : \n 
~~~ is_iterated_somewhere = True \n 
~~~ is_iterated_somewhere = False \n 
~~ if is_iterated_somewhere : \n 
~~ if isinstance ( group . ln_solver , LinearGaussSeidel ) and group . ln_solver . options [ ] == 1 : \n 
~~~ graph = group . _get_sys_graph ( ) \n 
strong = [ sorted ( s ) for s in nx . strongly_connected_components ( graph ) \n 
if len ( s ) > 1 ] \n 
if strong : \n 
"recommended)." \n 
% ( group . pathname , strong ) ) \n 
~~ ~~ states = [ n for n , m in iteritems ( group . _unknowns_dict ) if m . get ( ) ] \n 
if states : \n 
~~~ group_states . append ( ( group , states ) ) \n 
if isinstance ( group . ln_solver , DirectSolver ) or group . ln_solver . options [ ] > 1 : \n 
~~~ iterated_states . update ( states ) \n 
~~~ for s in states : \n 
~~~ if s not in iterated_states : \n 
~~~ cname = s . rsplit ( , 1 ) [ 0 ] \n 
comp = self . root \n 
~~~ comp = getattr ( comp , name ) \n 
~~ if not _needs_iteration ( comp ) : \n 
~~~ iterated_states . add ( s ) \n 
~~ ~~ ~~ ~~ ~~ ~~ for group , states in group_states : \n 
~~~ uniterated_states = [ s for s in states if s not in iterated_states ] \n 
if uniterated_states : \n 
( group . pathname , uniterated_states ) ) \n 
~~ ~~ ~~ def _check_dangling_params ( self , out_stream = sys . stdout ) : \n 
dangling_params = sorted ( set ( [ \n 
to_prom_name [ p ] for p , m in iteritems ( self . root . _params_dict ) \n 
if p not in self . root . connections \n 
if dangling_params : \n 
file = out_stream ) \n 
for d in dangling_params : \n 
~~~ print ( d , file = out_stream ) \n 
~~ ~~ return dangling_params \n 
~~ def _check_mode ( self , out_stream = sys . stdout ) : \n 
if self . _calculated_mode != self . root . _probdata . relevance . mode : \n 
self . _p_length , \n 
self . _u_length ) , \n 
~~ return ( self . root . _probdata . relevance . mode , self . _calculated_mode ) \n 
~~ def _check_no_unknown_comps ( self , out_stream = sys . stdout ) : \n 
nocomps = sorted ( [ c . pathname for c in self . root . components ( recurse = True , \n 
local = True ) \n 
if len ( c . unknowns ) == 0 ] ) \n 
if nocomps : \n 
for n in nocomps : \n 
~~~ print ( n , file = out_stream ) \n 
~~ ~~ return nocomps \n 
~~ def _check_no_recorders ( self , out_stream = sys . stdout ) : \n 
recorders = [ ] \n 
recorders . extend ( self . driver . recorders ) \n 
for grp in self . root . subgroups ( recurse = True , local = True , \n 
include_self = True ) : \n 
~~~ recorders . extend ( grp . nl_solver . recorders ) \n 
recorders . extend ( grp . ln_solver . recorders ) \n 
~~ if not recorders : \n 
~~ return recorders \n 
~~ def _check_no_connect_comps ( self , out_stream = sys . stdout ) : \n 
conn_comps = set ( [ t . rsplit ( , 1 ) [ 0 ] \n 
for t in self . root . connections ] ) \n 
conn_comps . update ( [ s . rsplit ( , 1 ) [ 0 ] \n 
for s , i in itervalues ( self . root . connections ) ] ) \n 
noconn_comps = sorted ( [ c . pathname \n 
for c in self . root . components ( recurse = True , local = True ) \n 
if c . pathname not in conn_comps ] ) \n 
if noconn_comps : \n 
for comp in noconn_comps : \n 
~~~ print ( comp , file = out_stream ) \n 
~~ ~~ return noconn_comps \n 
~~ def _check_mpi ( self , out_stream = sys . stdout ) : \n 
if under_mpirun ( ) : \n 
~~~ parr = True \n 
~~~ for grp in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if ( isinstance ( grp , ParallelGroup ) or \n 
isinstance ( grp , ParallelFDGroup ) ) : \n 
~~~ parr = False \n 
~~ mincpu , maxcpu = self . root . get_req_procs ( ) \n 
if maxcpu is not None and self . comm . size > maxcpu : \n 
( self . comm . size , maxcpu ) ) \n 
~~ return ( self . comm . size , maxcpu , parr ) \n 
~~~ pargrps = [ ] \n 
for grp in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if isinstance ( grp , ParallelGroup ) : \n 
grp . pathname , file = out_stream ) \n 
pargrps . append ( grp . pathname ) \n 
~~ ~~ return sorted ( pargrps ) \n 
~~ ~~ def _check_graph ( self , out_stream = sys . stdout ) : \n 
cycles = [ ] \n 
ooo = [ ] \n 
~~~ graph = grp . _get_sys_graph ( ) \n 
strong = [ s for s in nx . strongly_connected_components ( graph ) \n 
~~~ relstrong = [ ] \n 
for slist in strong : \n 
~~~ relstrong . append ( [ ] ) \n 
for s in slist : \n 
~~~ relstrong [ - 1 ] . append ( nearest_child ( grp . pathname , s ) ) \n 
subs = [ s for s in grp . _subsystems ] \n 
tups = sorted ( [ ( subs . index ( s ) , s ) for s in relstrong [ - 1 ] ] ) \n 
relstrong [ - 1 ] = [ t [ 1 ] for t in tups ] \n 
( grp . pathname , relstrong ) , file = out_stream ) \n 
cycles . append ( relstrong ) \n 
~~ graph , _ = grp . _break_cycles ( grp . list_order ( ) , graph ) \n 
visited = set ( ) \n 
out_of_order = { } \n 
for sub in itervalues ( grp . _subsystems ) : \n 
~~~ visited . add ( sub . pathname ) \n 
for u , v in nx . dfs_edges ( graph , sub . pathname ) : \n 
~~~ if v in visited : \n 
~~~ out_of_order . setdefault ( nearest_child ( grp . pathname , v ) , \n 
set ( ) ) . add ( sub . pathname ) \n 
~~ ~~ ~~ if out_of_order : \n 
~~~ for name in out_of_order : \n 
~~~ out_of_order [ name ] = sorted ( [ \n 
nearest_child ( grp . pathname , n ) for n in out_of_order [ name ] \n 
for n , subs in iteritems ( out_of_order ) : \n 
~~ ooo . append ( ( grp . pathname , list ( iteritems ( out_of_order ) ) ) ) \n 
~~ ~~ return ( cycles , sorted ( ooo ) ) \n 
~~ def _check_gmres_under_mpi ( self , out_stream = sys . stdout ) : \n 
~~~ has_parallel = False \n 
~~~ if isinstance ( s , ParallelGroup ) : \n 
~~~ has_parallel = True \n 
~~ ~~ if has_parallel and isinstance ( self . root . ln_solver , ScipyGMRES ) : \n 
~~ ~~ ~~ def _check_ubcs ( self , out_stream = sys . stdout ) : \n 
~~~ ubcs = self . _get_ubc_vars ( self . root . connections ) \n 
if ubcs : \n 
~~ return ubcs \n 
~~ def _check_unmarked_pbos ( self , out_stream = sys . stdout ) : \n 
~~~ pbos = [ ] \n 
for comp in self . root . components ( recurse = True , include_self = True ) : \n 
~~~ if comp . _pbo_warns : \n 
~~~ pbos . append ( ( comp . pathname , comp . _pbo_warns ) ) \n 
~~ ~~ if pbos : \n 
for cname , pbo_warns in sorted ( pbos , key = lambda x : x [ 0 ] ) : \n 
~~~ for vname , val in pbo_warns : \n 
type ( val ) . __name__ ) , file = out_stream ) \n 
~~ ~~ ~~ return pbos \n 
~~ def _check_relevant_pbos ( self , out_stream = sys . stdout ) : \n 
if self . driver . __class__ is Driver or self . driver . supports [ ] is False or self . root . fd_options [ ] : \n 
~~ vec = self . root . unknowns \n 
pbos = [ var for var in vec if vec . metadata ( var ) . get ( ) ] \n 
rels = set ( ) \n 
for key , rel in iteritems ( self . _probdata . relevance . relevant ) : \n 
~~~ rels . update ( rel ) \n 
~~ rel_pbos = rels . intersection ( pbos ) \n 
if rel_pbos : \n 
~~~ rel_conns = [ ] \n 
for src in rel_pbos : \n 
~~~ for tgt , src_tuple in iteritems ( self . root . connections ) : \n 
~~~ if src_tuple [ 0 ] == src and tgt in rels : \n 
~~~ rel_conns . append ( ( src , tgt ) ) \n 
~~ ~~ ~~ if rel_conns : \n 
for src , tgt in rel_conns : \n 
~~~ val = vec [ src ] \n 
~~ return list ( rel_pbos ) \n 
~~ def check_setup ( self , out_stream = sys . stdout ) : \n 
print ( "##############################################" , file = out_stream ) \n 
results [ ] = self . _check_no_recorders ( out_stream ) \n 
results [ ] = self . _check_mpi ( out_stream ) \n 
results [ ] = self . _check_dangling_params ( out_stream ) \n 
results [ ] = self . _check_mode ( out_stream ) \n 
results [ ] = self . _check_no_unknown_comps ( out_stream ) \n 
results [ ] = self . _check_no_connect_comps ( out_stream ) \n 
results [ ] , results [ ] = self . _check_graph ( out_stream ) \n 
results [ ] = self . _check_ubcs ( out_stream ) \n 
results [ ] = self . _check_gmres_under_mpi ( out_stream ) \n 
results [ ] = self . _check_unmarked_pbos ( out_stream ) \n 
results [ ] = self . _check_relevant_pbos ( out_stream ) \n 
for s in self . root . subsystems ( recurse = True , local = True , include_self = True ) : \n 
s . check_setup ( out_stream = stream ) \n 
content = stream . getvalue ( ) \n 
if content : \n 
~~~ print ( "%s:\\n%s\\n" % ( s . pathname , content ) , file = out_stream ) \n 
results [ "@%s" % s . pathname ] = content \n 
print ( "##############################################\\n" , file = out_stream ) \n 
~~ def pre_run_check ( self ) : \n 
if not self . root . fd_options . locked : \n 
raise RuntimeError ( msg ) \n 
~~ ~~ def run ( self ) : \n 
self . pre_run_check ( ) \n 
if self . root . is_active ( ) : \n 
~~~ self . driver . run ( self ) \n 
self . root . comm . barrier ( ) \n 
~~ ~~ ~~ def run_once ( self ) : \n 
root = self . root \n 
driver = self . driver \n 
if root . is_active ( ) : \n 
~~~ driver . run_once ( self ) \n 
with root . _dircontext : \n 
~~~ root . apply_nonlinear ( root . params , root . unknowns , root . resids , \n 
metadata = driver . metadata ) \n 
root . comm . barrier ( ) \n 
~~ ~~ ~~ def _mode ( self , mode , indep_list , unknown_list ) : \n 
self . _p_length = 0 \n 
self . _u_length = 0 \n 
uset = set ( ) \n 
for unames in unknown_list : \n 
~~~ if isinstance ( unames , tuple ) : \n 
~~~ uset . update ( unames ) \n 
~~~ uset . add ( unames ) \n 
~~ ~~ pset = set ( ) \n 
for pnames in indep_list : \n 
~~~ if isinstance ( pnames , tuple ) : \n 
~~~ pset . update ( pnames ) \n 
~~~ pset . add ( pnames ) \n 
~~ ~~ to_prom_name = self . root . _sysdata . to_prom_name \n 
for path , meta in chain ( iteritems ( self . root . _unknowns_dict ) , \n 
iteritems ( self . root . _params_dict ) ) : \n 
~~~ prom_name = to_prom_name [ path ] \n 
if prom_name in uset : \n 
~~~ self . _u_length += meta [ ] \n 
uset . remove ( prom_name ) \n 
~~ if prom_name in pset : \n 
~~~ self . _p_length += meta [ ] \n 
pset . remove ( prom_name ) \n 
~~ ~~ if uset : \n 
~~ if pset : \n 
~~ if self . _p_length > self . _u_length : \n 
~~~ self . _calculated_mode = \n 
~~ if mode == : \n 
~~~ mode = self . root . ln_solver . options [ ] \n 
if mode == : \n 
~~~ mode = self . _calculated_mode \n 
~~ ~~ return mode \n 
~~ def calc_gradient ( self , indep_list , unknown_list , mode = , \n 
return_format = , dv_scale = None , cn_scale = None , \n 
sparsity = None ) : \n 
if mode not in [ , , , ] : \n 
~~ if return_format not in [ , ] : \n 
~~ with self . root . _dircontext : \n 
~~~ if mode == or self . root . fd_options [ ] : \n 
~~~ return self . _calc_gradient_fd ( indep_list , unknown_list , \n 
return_format , dv_scale = dv_scale , \n 
cn_scale = cn_scale , sparsity = sparsity ) \n 
~~~ return self . _calc_gradient_ln_solver ( indep_list , unknown_list , \n 
return_format , mode , \n 
dv_scale = dv_scale , \n 
cn_scale = cn_scale , \n 
sparsity = sparsity ) \n 
~~ ~~ ~~ def _calc_gradient_fd ( self , indep_list , unknown_list , return_format , \n 
dv_scale = None , cn_scale = None , sparsity = None ) : \n 
unknowns = root . unknowns \n 
params = root . params \n 
to_prom_name = root . _sysdata . to_prom_name \n 
to_abs_pnames = root . _sysdata . to_abs_pnames \n 
to_abs_uname = root . _sysdata . to_abs_uname \n 
if dv_scale is None : \n 
~~~ dv_scale = { } \n 
~~ if cn_scale is None : \n 
~~~ cn_scale = { } \n 
~~ abs_params = [ ] \n 
fd_unknowns = [ var for var in unknown_list if var not in indep_list ] \n 
pass_unknowns = [ var for var in unknown_list if var in indep_list ] \n 
for name in indep_list : \n 
~~~ if name in unknowns : \n 
~~~ name = to_abs_uname [ name ] \n 
~~ for tgt , ( src , idxs ) in iteritems ( root . connections ) : \n 
~~~ if name == src : \n 
~~~ name = tgt \n 
~~ ~~ abs_params . append ( name ) \n 
~~ Jfd = root . fd_jacobian ( params , unknowns , root . resids , total_derivs = True , \n 
fd_params = abs_params , fd_unknowns = fd_unknowns , \n 
pass_unknowns = pass_unknowns , \n 
poi_indices = self . _poi_indices , \n 
qoi_indices = self . _qoi_indices ) \n 
def get_fd_ikey ( ikey ) : \n 
~~~ if isinstance ( ikey , tuple ) : \n 
~~~ ikey = ikey [ 0 ] \n 
~~ fd_ikey = ikey \n 
if fd_ikey not in params : \n 
~~~ for tgt , ( src , idxs ) in iteritems ( root . connections ) : \n 
~~~ if src == ikey : \n 
~~~ fd_ikey = tgt \n 
~~ ~~ if fd_ikey not in params : \n 
~~~ for key , meta in iteritems ( params ) : \n 
~~~ if to_prom_name [ key ] == fd_ikey : \n 
~~~ fd_ikey = meta [ ] \n 
~~ ~~ ~~ ~~ return fd_ikey \n 
~~ if return_format == : \n 
~~~ J = OrderedDict ( ) \n 
for okey in unknown_list : \n 
~~~ J [ okey ] = OrderedDict ( ) \n 
for j , ikey in enumerate ( indep_list ) : \n 
~~~ if sparsity is not None : \n 
~~~ if ikey not in sparsity [ okey ] : \n 
~~ ~~ abs_ikey = abs_params [ j ] \n 
fd_ikey = get_fd_ikey ( abs_ikey ) \n 
if ( okey , fd_ikey ) not in Jfd : \n 
~~~ fd_ikey = to_abs_pnames [ fd_ikey ] [ 0 ] \n 
~~ J [ okey ] [ ikey ] = Jfd [ ( okey , fd_ikey ) ] \n 
if ikey in dv_scale : \n 
~~~ J [ okey ] [ ikey ] *= dv_scale [ ikey ] \n 
~~ if okey in cn_scale : \n 
~~~ J [ okey ] [ ikey ] *= cn_scale [ okey ] \n 
~~~ usize = 0 \n 
psize = 0 \n 
for u in unknown_list : \n 
~~~ if u in self . _qoi_indices : \n 
~~~ idx = self . _qoi_indices [ u ] \n 
usize += len ( idx ) \n 
~~~ usize += self . root . unknowns . metadata ( u ) [ ] \n 
~~ ~~ for p in indep_list : \n 
~~~ if p in self . _poi_indices : \n 
~~~ idx = self . _poi_indices [ p ] \n 
psize += len ( idx ) \n 
~~~ psize += self . root . unknowns . metadata ( p ) [ ] \n 
~~ ~~ J = np . zeros ( ( usize , psize ) ) \n 
ui = 0 \n 
~~~ pi = 0 \n 
for j , p in enumerate ( indep_list ) : \n 
~~~ abs_ikey = abs_params [ j ] \n 
if ( u , fd_ikey ) not in Jfd : \n 
~~ pd = Jfd [ u , fd_ikey ] \n 
rows , cols = pd . shape \n 
for row in range ( 0 , rows ) : \n 
~~~ for col in range ( 0 , cols ) : \n 
~~~ J [ ui + row ] [ pi + col ] = pd [ row ] [ col ] \n 
if p in dv_scale : \n 
~~~ J [ ui + row ] [ pi + col ] *= dv_scale [ p ] \n 
~~ if u in cn_scale : \n 
~~~ J [ ui + row ] [ pi + col ] *= cn_scale [ u ] \n 
~~ ~~ ~~ pi += cols \n 
~~ ui += rows \n 
~~ ~~ return J \n 
~~ def _calc_gradient_ln_solver ( self , indep_list , unknown_list , return_format , mode , \n 
relevance = root . _probdata . relevance \n 
unknowns_dict = root . _unknowns_dict \n 
comm = root . comm \n 
iproc = comm . rank \n 
nproc = comm . size \n 
owned = root . _owning_ranks \n 
~~ mode = self . _mode ( mode , indep_list , unknown_list ) \n 
fwd = mode == \n 
root . clear_dparams ( ) \n 
for names in root . _probdata . relevance . vars_of_interest ( mode ) : \n 
~~~ for name in names : \n 
~~~ if name in root . dumat : \n 
~~~ root . dumat [ name ] . vec [ : ] = 0.0 \n 
root . drmat [ name ] . vec [ : ] = 0.0 \n 
~~ ~~ ~~ root . dumat [ None ] . vec [ : ] = 0.0 \n 
root . drmat [ None ] . vec [ : ] = 0.0 \n 
root . _sys_linearize ( root . params , unknowns , root . resids ) \n 
if return_format == : \n 
for okeys in unknown_list : \n 
~~~ if isinstance ( okeys , str ) : \n 
~~~ okeys = ( okeys , ) \n 
~~ for okey in okeys : \n 
for ikeys in indep_list : \n 
~~~ if isinstance ( ikeys , str ) : \n 
~~~ ikeys = ( ikeys , ) \n 
~~ for ikey in ikeys : \n 
~~ ~~ J [ okey ] [ ikey ] = None \n 
~~ ~~ ~~ ~~ ~~ else : \n 
Jslices = OrderedDict ( ) \n 
~~~ start = usize \n 
if u in self . _qoi_indices : \n 
~~ Jslices [ u ] = slice ( start , usize ) \n 
~~ for p in indep_list : \n 
~~~ start = psize \n 
if p in self . _poi_indices : \n 
~~~ psize += unknowns . metadata ( p ) [ ] \n 
~~ Jslices [ p ] = slice ( start , psize ) \n 
~~ J = np . zeros ( ( usize , psize ) ) \n 
~~ if fwd : \n 
~~~ input_list , output_list = indep_list , unknown_list \n 
poi_indices , qoi_indices = self . _poi_indices , self . _qoi_indices \n 
in_scale , un_scale = dv_scale , cn_scale \n 
~~~ input_list , output_list = unknown_list , indep_list \n 
qoi_indices , poi_indices = self . _poi_indices , self . _qoi_indices \n 
in_scale , un_scale = cn_scale , dv_scale \n 
~~ all_vois = self . root . _probdata . relevance . vars_of_interest ( mode ) \n 
input_set = set ( ) \n 
for inp in input_list : \n 
~~~ if isinstance ( inp , str ) : \n 
~~~ input_set . add ( inp ) \n 
~~~ input_set . update ( inp ) \n 
~~ ~~ voi_sets = [ ] \n 
for voi_set in all_vois : \n 
~~~ for voi in voi_set : \n 
~~~ if voi in input_set : \n 
~~~ voi_sets . append ( voi_set ) \n 
~~ ~~ ~~ flat_voi = [ item for sublist in all_vois for item in sublist ] \n 
for items in input_list : \n 
~~~ if isinstance ( items , str ) : \n 
~~~ items = ( items , ) \n 
~~~ if item not in flat_voi : \n 
~~~ voi_sets . append ( ( item , ) ) \n 
~~ ~~ ~~ voi_srcs = { } \n 
for params in voi_sets : \n 
~~~ rhs = OrderedDict ( ) \n 
voi_idxs = { } \n 
old_size = None \n 
for voi in params : \n 
~~~ vkey = self . _get_voi_key ( voi , params ) \n 
duvec = self . root . dumat [ vkey ] \n 
rhs [ vkey ] = np . empty ( ( len ( duvec . vec ) , ) ) \n 
voi_srcs [ vkey ] = voi \n 
if voi in duvec : \n 
~~~ in_idxs = duvec . _get_local_idxs ( voi , poi_indices ) \n 
~~~ in_idxs = [ ] \n 
~~ if len ( in_idxs ) == 0 : \n 
~~~ if voi in poi_indices : \n 
~~~ in_idxs = duvec . to_idx_array ( poi_indices [ voi ] ) \n 
~~~ in_idxs = np . arange ( 0 , unknowns_dict [ to_abs_uname [ voi ] ] [ ] , dtype = int ) \n 
~~ ~~ if old_size is None : \n 
~~~ old_size = len ( in_idxs ) \n 
~~ elif old_size != len ( in_idxs ) : \n 
~~ for i in range ( len ( in_idxs ) ) : \n 
~~~ for voi in params : \n 
rhs [ vkey ] [ : ] = 0.0 \n 
if self . root . _owning_ranks [ voi_srcs [ vkey ] ] == iproc : \n 
~~~ rhs [ vkey ] [ voi_idxs [ vkey ] [ i ] ] = - 1.0 \n 
~~ ~~ dx_mat = root . ln_solver . solve ( rhs , root , mode ) \n 
for param , dx in iteritems ( dx_mat ) : \n 
~~~ vkey = self . _get_voi_key ( param , params ) \n 
if param is None : \n 
~~~ param = params [ 0 ] \n 
~~ for item in output_list : \n 
~~~ if fwd and param not in sparsity [ item ] : \n 
~~ elif not fwd and item not in sparsity [ param ] : \n 
~~ ~~ if relevance . is_relevant ( vkey , item ) : \n 
~~~ if fwd or owned [ item ] == iproc : \n 
~~~ out_idxs = self . root . dumat [ vkey ] . _get_local_idxs ( item , \n 
qoi_indices , \n 
get_slice = True ) \n 
dxval = dx [ out_idxs ] \n 
if dxval . size == 0 : \n 
~~~ dxval = None \n 
~~ if nproc > 1 : \n 
~~ dxval = comm . bcast ( dxval , root = owned [ item ] ) \n 
~~~ if item in qoi_indices : \n 
~~~ zsize = len ( qoi_indices [ item ] ) \n 
~~~ zsize = unknowns . metadata ( item ) [ ] \n 
~~ dxval = np . zeros ( zsize ) \n 
~~ if dxval is not None : \n 
~~~ nk = len ( dxval ) \n 
~~~ if fwd : \n 
~~~ if J [ item ] [ param ] is None : \n 
~~~ J [ item ] [ param ] = np . zeros ( ( nk , len ( in_idxs ) ) ) \n 
~~ J [ item ] [ param ] [ : , i ] = dxval \n 
if param in in_scale : \n 
~~~ J [ item ] [ param ] [ : , i ] *= in_scale [ param ] \n 
~~ if item in un_scale : \n 
~~~ J [ item ] [ param ] [ : , i ] *= un_scale [ item ] \n 
~~~ if J [ param ] [ item ] is None : \n 
~~~ J [ param ] [ item ] = np . zeros ( ( len ( in_idxs ) , nk ) ) \n 
~~ J [ param ] [ item ] [ i , : ] = dxval \n 
~~~ J [ param ] [ item ] [ i , : ] *= in_scale [ param ] \n 
~~~ J [ param ] [ item ] [ i , : ] *= un_scale [ item ] \n 
~~~ J [ Jslices [ item ] , Jslices [ param ] . start + i ] = dxval \n 
~~~ J [ Jslices [ item ] , Jslices [ param ] . start + i ] *= in_scale [ param ] \n 
~~~ J [ Jslices [ item ] , Jslices [ param ] . start + i ] *= un_scale [ item ] \n 
~~~ J [ Jslices [ param ] . start + i , Jslices [ item ] ] = dxval \n 
~~~ J [ Jslices [ param ] . start + i , Jslices [ item ] ] *= in_scale [ param ] \n 
~~~ J [ Jslices [ param ] . start + i , Jslices [ item ] ] *= un_scale [ item ] \n 
~~ ~~ ~~ ~~ ~~ ~~ ~~ ~~ root . clear_dparams ( ) \n 
~~ def _get_voi_key ( self , voi , grp ) : \n 
if ( voi in self . _driver_vois and \n 
isinstance ( self . root . ln_solver , LinearGaussSeidel ) ) : \n 
~~~ if ( len ( grp ) > 1 or \n 
self . root . ln_solver . options [ ] ) : \n 
~~~ return voi \n 
~~ def check_partial_derivatives ( self , out_stream = sys . stdout , comps = None , \n 
compact_print = False ) : \n 
if self . driver . iter_count < 1 : \n 
~~~ out_stream . write ( ) \n 
self . run_once ( ) \n 
~~ root . _sys_linearize ( root . params , root . unknowns , root . resids ) \n 
if out_stream is not None : \n 
~~ data = { } \n 
voi = None \n 
allcomps = root . components ( recurse = True ) \n 
if comps is None : \n 
~~~ comps = allcomps \n 
~~~ allcompnames = set ( [ c . pathname for c in allcomps ] ) \n 
requested = set ( comps ) \n 
diff = requested . difference ( allcompnames ) \n 
if diff : \n 
~~~ sorted_diff = list ( diff ) \n 
sorted_diff . sort ( ) \n 
msg += str ( sorted_diff ) \n 
~~ comps = [ root . _subsystem ( c_name ) for c_name in comps ] \n 
~~ for comp in comps : \n 
~~~ cname = comp . pathname \n 
opt = comp . fd_options \n 
fwd_rev = True \n 
if opt [ ] : \n 
~~~ f_d_2 = True \n 
fd_desc = opt [ ] \n 
fd_desc2 = opt [ ] \n 
~~~ f_d_2 = False \n 
fd_desc = None \n 
fd_desc2 = None \n 
~~ if opt [ ] : \n 
~~~ if not f_d_2 : \n 
~~ fwd_rev = False \n 
~~ if isinstance ( comp , IndepVarComp ) : \n 
~~ data [ cname ] = { } \n 
jac_fwd = OrderedDict ( ) \n 
jac_rev = OrderedDict ( ) \n 
jac_fd = OrderedDict ( ) \n 
jac_fd2 = OrderedDict ( ) \n 
params = comp . params \n 
unknowns = comp . unknowns \n 
resids = comp . resids \n 
dparams = comp . dpmat [ voi ] \n 
dunknowns = comp . dumat [ voi ] \n 
dresids = comp . drmat [ voi ] \n 
states = comp . states \n 
if len ( dparams ) == 0 : \n 
~~ param_list = [ item for item in dparams if not dparams . metadata ( item ) . get ( ) ] \n 
param_list . extend ( states ) \n 
unkn_list = [ item for item in dunknowns if not dunknowns . metadata ( item ) . get ( ) ] \n 
~~~ out_stream . write ( * ( len ( cname ) + 15 ) + ) \n 
out_stream . write ( * ( len ( cname ) + 15 ) + ) \n 
~~ for p_name in param_list : \n 
~~~ if not fwd_rev : \n 
~~ dinputs = dunknowns if p_name in states else dparams \n 
p_size = np . size ( dinputs [ p_name ] ) \n 
for u_name in unkn_list : \n 
~~~ u_size = np . size ( dunknowns [ u_name ] ) \n 
if comp . _jacobian_cache : \n 
~~~ if ( u_name , p_name ) in comp . _jacobian_cache : \n 
~~~ user = comp . _jacobian_cache [ ( u_name , p_name ) ] . shape \n 
if len ( user ) < 2 : \n 
~~~ user = ( user [ 0 ] , 1 ) \n 
~~ if user [ 0 ] != u_size or user [ 1 ] != p_size : \n 
msg = msg . format ( cname , u_name , p_name , ( u_size , p_size ) , user ) \n 
~~ ~~ ~~ jac_fwd [ ( u_name , p_name ) ] = np . zeros ( ( u_size , p_size ) ) \n 
jac_rev [ ( u_name , p_name ) ] = np . zeros ( ( u_size , p_size ) ) \n 
~~ ~~ if fwd_rev : \n 
~~~ for u_name in unkn_list : \n 
for idx in range ( u_size ) : \n 
~~~ dresids . vec [ : ] = 0.0 \n 
dunknowns . vec [ : ] = 0.0 \n 
dresids . _dat [ u_name ] . val [ idx ] = 1.0 \n 
~~~ comp . apply_linear ( params , unknowns , dparams , \n 
dunknowns , dresids , ) \n 
~~~ dparams . _apply_unit_derivatives ( ) \n 
~~~ dinputs = dunknowns if p_name in states else dparams \n 
jac_rev [ ( u_name , p_name ) ] [ idx , : ] = dinputs . _dat [ p_name ] . val \n 
~~ ~~ ~~ ~~ if fwd_rev : \n 
~~~ for p_name in param_list : \n 
for idx in range ( p_size ) : \n 
dinputs . _dat [ p_name ] . val [ idx ] = 1.0 \n 
dparams . _apply_unit_derivatives ( ) \n 
comp . apply_linear ( params , unknowns , dparams , \n 
for u_name , u_val in dresids . vec_val_iter ( ) : \n 
~~~ jac_fwd [ ( u_name , p_name ) ] [ : , idx ] = u_val \n 
~~ ~~ ~~ ~~ dresids . vec [ : ] = 0.0 \n 
if opt [ ] == : \n 
~~~ fd_func = comp . complex_step_jacobian \n 
~~~ fd_func = comp . fd_jacobian \n 
~~ jac_fd = fd_func ( params , unknowns , resids ) \n 
if f_d_2 : \n 
~~ save_form = opt [ ] \n 
OptionsDictionary . locked = False \n 
opt [ ] = opt [ ] \n 
jac_fd2 = fd_func ( params , unknowns , resids ) \n 
opt [ ] = save_form \n 
OptionsDictionary . locked = True \n 
~~ _assemble_deriv_data ( chain ( dparams , states ) , resids , data [ cname ] , \n 
jac_fwd , jac_rev , jac_fd , out_stream , \n 
c_name = cname , jac_fd2 = jac_fd2 , fd_desc = fd_desc , \n 
fd_desc2 = fd_desc2 , compact_print = compact_print ) \n 
~~ return data \n 
~~ def check_total_derivatives ( self , out_stream = sys . stdout ) : \n 
if driver . iter_count < 1 : \n 
~~ if out_stream is not None : \n 
~~ if len ( driver . _desvars ) > 0 : \n 
~~~ param_srcs = list ( driver . _desvars . keys ( ) ) \n 
to_abs_name = root . _sysdata . to_abs_uname \n 
indep_list = [ p for p in param_srcs if not root . _unknowns_dict [ to_abs_name [ p ] ] . get ( ) ] \n 
~~~ abs_indep_list = root . _get_fd_params ( ) \n 
param_srcs = [ root . connections [ p ] for p in abs_indep_list if not root . _params_dict [ p ] . get ( ) ] \n 
indep_list = [ \n 
to_prom_name [ p ] for p , idxs in param_srcs \n 
~~ if len ( driver . _objs ) > 0 or len ( driver . _cons ) > 0 : \n 
~~~ unknown_list = list ( driver . _objs . keys ( ) ) \n 
unknown_list . extend ( list ( driver . _cons . keys ( ) ) ) \n 
unknown_list = [ item for item in unknown_list if not root . unknowns . metadata ( item ) . get ( ) ] \n 
~~~ unknown_list = root . _get_fd_unknowns ( ) \n 
~~ if root . ln_solver . options . get ( ) : \n 
~~~ mode = self . _mode ( , indep_list , unknown_list ) \n 
~~~ fwd , rev = True , False \n 
Jrev = None \n 
out_stream . write ( ) \n 
~~~ fwd , rev = False , True \n 
Jfor = None \n 
~~~ fwd = rev = True \n 
~~~ Jfor = self . calc_gradient ( indep_list , unknown_list , mode = , \n 
return_format = ) \n 
Jfor = _jac_to_flat_dict ( Jfor ) \n 
~~ if rev : \n 
~~~ Jrev = self . calc_gradient ( indep_list , unknown_list , mode = , \n 
Jrev = _jac_to_flat_dict ( Jrev ) \n 
~~ Jfd = self . calc_gradient ( indep_list , unknown_list , mode = , \n 
Jfd = _jac_to_flat_dict ( Jfd ) \n 
_assemble_deriv_data ( indep_list , unknown_list , data , \n 
Jfor , Jrev , Jfd , out_stream ) \n 
~~ def _start_recorders ( self ) : \n 
self . driver . recorders . startup ( self . root ) \n 
self . driver . recorders . record_metadata ( self . root ) \n 
~~~ for solver in ( group . nl_solver , group . ln_solver ) : \n 
~~~ solver . recorders . startup ( group ) \n 
solver . recorders . record_metadata ( self . root ) \n 
~~ ~~ ~~ def _check_for_parallel_derivs ( self , params , unknowns , par_u , par_p ) : \n 
mode = self . _mode ( , params , unknowns ) \n 
~~~ has_parallel_derivs = par_p \n 
~~~ has_parallel_derivs = par_u \n 
~~ if ( isinstance ( self . root . ln_solver , LinearGaussSeidel ) and \n 
self . root . ln_solver . options [ ] ) and has_parallel_derivs : \n 
~~~ for sub in self . root . subgroups ( recurse = True ) : \n 
~~~ sub_mode = sub . ln_solver . options [ ] \n 
if isinstance ( sub . ln_solver , LinearGaussSeidel ) and sub_mode not in ( mode , ) : \n 
msg = msg . format ( name = sub . name , submode = sub_mode , rootmode = mode ) \n 
~~ ~~ ~~ return mode \n 
~~ def _json_system_tree ( self ) : \n 
def _tree_dict ( system ) : \n 
~~~ dct = OrderedDict ( ) \n 
for s in system . subsystems ( recurse = True ) : \n 
~~~ if isinstance ( s , Group ) : \n 
~~~ dct [ s . name ] = _tree_dict ( s ) \n 
~~~ dct [ s . name ] = OrderedDict ( ) \n 
for vname , meta in iteritems ( s . unknowns ) : \n 
~~~ dct [ s . name ] [ vname ] = m = meta . copy ( ) \n 
for mname in m : \n 
~~~ if isinstance ( m [ mname ] , np . ndarray ) : \n 
~~~ m [ mname ] = m [ mname ] . tolist ( ) \n 
~~ ~~ ~~ ~~ ~~ return dct \n 
~~ tree = OrderedDict ( ) \n 
tree [ ] = _tree_dict ( self . root ) \n 
return json . dumps ( tree ) \n 
~~ def _setup_communicators ( self ) : \n 
~~~ if self . comm is None : \n 
~~~ self . comm = self . _impl . world_comm ( ) \n 
~~ minproc , maxproc = self . driver . get_req_procs ( ) \n 
~~~ if not ( maxproc is None or maxproc >= self . comm . size ) : \n 
( self . comm . size , minproc , maxproc ) ) \n 
~~ elif self . comm . size < minproc : \n 
~~~ if maxproc is None : \n 
~~~ maxproc = \n 
~~ ~~ self . driver . _setup_communicators ( self . comm , os . getcwd ( ) ) \n 
~~ def _setup_units ( self , connections , params_dict , unknowns_dict ) : \n 
for target , ( source , idxs ) in iteritems ( connections ) : \n 
~~~ tmeta = params_dict [ target ] \n 
smeta = unknowns_dict [ source ] \n 
if not in tmeta or not in smeta : \n 
~~ src_unit = smeta [ ] \n 
tgt_unit = tmeta [ ] \n 
~~~ scale , offset = get_conversion_tuple ( src_unit , tgt_unit ) \n 
_both_names ( smeta , to_prom_name ) , \n 
tgt_unit , \n 
_both_names ( tmeta , to_prom_name ) ) \n 
~~ ~~ if scale != 1.0 or offset != 0.0 : \n 
~~~ tmeta [ ] = ( scale , offset ) \n 
~~ ~~ ~~ def _add_implicit_connections ( self , connections ) : \n 
dangling = set ( ) \n 
abs_unames = self . root . _sysdata . to_abs_uname \n 
for prom_name , pabs_list in iteritems ( self . root . _sysdata . to_abs_pnames ) : \n 
~~~ for pabs in pabs_list : \n 
~~~ connections . setdefault ( pabs , [ ] ) . append ( ( abs_unames [ prom_name ] , None ) ) \n 
~~~ dangling . add ( prom_name ) \n 
~~ ~~ return dangling \n 
~~ def print_all_convergence ( self ) : \n 
root . ln_solver . print_all_convergence ( ) \n 
root . nl_solver . print_all_convergence ( ) \n 
for grp in root . subgroups ( recurse = True ) : \n 
~~~ grp . ln_solver . print_all_convergence ( ) \n 
grp . nl_solver . print_all_convergence ( ) \n 
~~ ~~ ~~ def _assign_parameters ( connections ) : \n 
param_owners = { } \n 
for par , ( unk , idxs ) in iteritems ( connections ) : \n 
~~~ param_owners . setdefault ( get_common_ancestor ( par , unk ) , set ( ) ) . add ( par ) \n 
~~ return param_owners \n 
~~ def _jac_to_flat_dict ( jac ) : \n 
new_jac = OrderedDict ( ) \n 
for key1 , val1 in iteritems ( jac ) : \n 
~~~ for key2 , val2 in iteritems ( val1 ) : \n 
~~~ new_jac [ ( key1 , key2 ) ] = val2 \n 
~~ ~~ return new_jac \n 
~~ def _pad_name ( name , pad_num = 13 , quotes = True ) : \n 
l_name = len ( name ) \n 
if l_name < pad_num : \n 
~~~ pad = pad_num - l_name \n 
if quotes : \n 
~~~ pad_str = "\'{name}\'{sep:<{pad}}" \n 
~~~ pad_str = "{name}{sep:<{pad}}" \n 
~~ pad_name = pad_str . format ( name = name , sep = , pad = pad ) \n 
return pad_name \n 
~~~ return . format ( name ) \n 
~~ ~~ def _assemble_deriv_data ( params , resids , cdata , jac_fwd , jac_rev , jac_fd , \n 
out_stream , c_name = , jac_fd2 = None , fd_desc = None , \n 
fd_desc2 = None , compact_print = False ) : \n 
started = False \n 
for p_name in params : \n 
~~~ for u_name in resids : \n 
~~~ key = ( u_name , p_name ) \n 
if key not in jac_fd : \n 
~~ ldata = cdata [ key ] = { } \n 
Jsub_fd = jac_fd [ key ] \n 
ldata [ ] = Jsub_fd \n 
magfd = np . linalg . norm ( Jsub_fd ) \n 
if jac_fwd : \n 
~~~ Jsub_for = jac_fwd [ key ] \n 
ldata [ ] = Jsub_for \n 
magfor = np . linalg . norm ( Jsub_for ) \n 
~~~ magfor = None \n 
~~ if jac_rev : \n 
~~~ Jsub_rev = jac_rev [ key ] \n 
ldata [ ] = Jsub_rev \n 
magrev = np . linalg . norm ( Jsub_rev ) \n 
~~~ magrev = None \n 
~~ if jac_fd2 : \n 
~~~ Jsub_fd2 = jac_fd2 [ key ] \n 
ldata [ ] = Jsub_fd2 \n 
magfd2 = np . linalg . norm ( Jsub_fd2 ) \n 
~~~ magfd2 = None \n 
~~ ldata [ ] = ( magfor , magrev , magfd ) \n 
~~~ abs1 = np . linalg . norm ( Jsub_for - Jsub_fd ) \n 
~~~ abs1 = None \n 
~~~ abs2 = np . linalg . norm ( Jsub_rev - Jsub_fd ) \n 
~~~ abs2 = None \n 
~~ if jac_fwd and jac_rev : \n 
~~~ abs3 = np . linalg . norm ( Jsub_for - Jsub_rev ) \n 
~~~ abs3 = None \n 
~~~ abs4 = np . linalg . norm ( Jsub_fd2 - Jsub_fd ) \n 
~~~ abs4 = None \n 
~~ ldata [ ] = ( abs1 , abs2 , abs3 ) \n 
if magfd == 0.0 : \n 
~~~ rel1 = rel2 = rel3 = rel4 = float ( ) \n 
~~~ if jac_fwd : \n 
~~~ rel1 = np . linalg . norm ( Jsub_for - Jsub_fd ) / magfd \n 
~~~ rel1 = None \n 
~~~ rel2 = np . linalg . norm ( Jsub_rev - Jsub_fd ) / magfd \n 
~~~ rel2 = None \n 
~~~ rel3 = np . linalg . norm ( Jsub_for - Jsub_rev ) / magfd \n 
~~~ rel3 = None \n 
~~~ rel4 = np . linalg . norm ( Jsub_fd2 - Jsub_fd ) / magfd \n 
~~~ rel4 = None \n 
~~ ~~ ldata [ ] = ( rel1 , rel2 , rel3 ) \n 
if out_stream is None : \n 
~~ if compact_print : \n 
~~~ if jac_fwd and jac_rev : \n 
~~~ if not started : \n 
out_str = tmp1 . format ( _pad_name ( ) , _pad_name ( ) , \n 
_pad_name ( , 10 , quotes = False ) , \n 
_pad_name ( , 10 , quotes = False ) \n 
out_stream . write ( out_str ) \n 
out_stream . write ( * len ( out_str ) + ) \n 
started = True \n 
magfor , magrev , magfd , abs1 , abs2 , \n 
rel1 , rel2 ) ) \n 
~~ elif jac_fd and jac_fd2 : \n 
_pad_name ( , 13 , quotes = False ) , \n 
_pad_name ( , 12 , quotes = False ) , \n 
_pad_name ( , 12 , quotes = False ) \n 
out_stream . write ( tmp1 . format ( _pad_name ( u_name ) , _pad_name ( p_name ) , \n 
magfd , magfd2 , abs4 , rel4 ) ) \n 
~~~ if started : \n 
~~~ out_stream . write ( * 30 + ) \n 
~~~ started = True \n 
~~~ out_stream . write ( % magfor ) \n 
~~~ out_stream . write ( % magrev ) \n 
~~ if not jac_fwd and not jac_rev : \n 
~~~ out_stream . write ( ~~ if jac_fd : \n 
~~~ out_stream . write ( % magfd ) \n 
if fd_desc : \n 
~~~ out_stream . write ( % fd_desc ) \n 
~~ out_stream . write ( ) \n 
~~~ out_stream . write ( % magfd2 ) \n 
if fd_desc2 : \n 
~~~ out_stream . write ( % fd_desc2 ) \n 
~~~ out_stream . write ( % abs1 ) \n 
~~~ out_stream . write ( % abs2 ) \n 
~~~ out_stream . write ( % abs3 ) \n 
~~~ out_stream . write ( % abs4 ) \n 
~~~ out_stream . write ( % rel1 ) \n 
~~~ out_stream . write ( % rel2 ) \n 
~~~ out_stream . write ( % rel3 ) \n 
~~~ out_stream . write ( % rel4 ) \n 
out_stream . write ( str ( Jsub_for ) ) \n 
out_stream . write ( str ( Jsub_rev ) ) \n 
out_stream . write ( str ( Jsub_fd ) ) \n 
if jac_fd2 : \n 
out_stream . write ( str ( Jsub_fd2 ) ) \n 
~~ ~~ ~~ ~~ ~~ def _needs_iteration ( comp ) : \n 
if isinstance ( comp , Component ) and comp . is_active ( ) and comp . states : \n 
~~~ for klass in comp . __class__ . __mro__ : \n 
~~~ if klass is Component : \n 
~~ if in klass . __dict__ : \n 
~~ def _get_gmres_name ( ) : \n 
from sqlitedict import SqliteDict \n 
from openmdao . recorders . base_recorder import BaseRecorder \n 
from openmdao . util . record_util import format_iteration_coordinate \n 
from openmdao . core . mpi_wrap import MPI \n 
class SqliteRecorder ( BaseRecorder ) : \n 
def __init__ ( self , out , ** sqlite_dict_args ) : \n 
~~~ super ( SqliteRecorder , self ) . __init__ ( ) \n 
if MPI and MPI . COMM_WORLD . rank > 0 : \n 
~~~ self . _open_close_sqlitedict = False \n 
~~~ self . _open_close_sqlitedict = True \n 
~~ if self . _open_close_sqlitedict : \n 
~~~ sqlite_dict_args . setdefault ( , True ) \n 
sqlite_dict_args . setdefault ( , ) \n 
self . out = SqliteDict ( filename = out , flag = , ** sqlite_dict_args ) \n 
~~~ self . out = None \n 
~~ ~~ def record_metadata ( self , group ) : \n 
params = group . params . iteritems ( ) \n 
resids = group . resids . iteritems ( ) \n 
unknowns = group . unknowns . iteritems ( ) \n 
data = OrderedDict ( [ ( , dict ( params ) ) , \n 
( , dict ( unknowns ) ) , \n 
self . out [ ] = data \n 
~~ def record_iteration ( self , params , unknowns , resids , metadata ) : \n 
data = OrderedDict ( ) \n 
iteration_coordinate = metadata [ ] \n 
timestamp = metadata [ ] \n 
group_name = format_iteration_coordinate ( iteration_coordinate ) \n 
data [ ] = timestamp \n 
data [ ] = metadata [ ] \n 
if self . options [ ] : \n 
~~~ data [ ] = self . _filter_vector ( params , , iteration_coordinate ) \n 
~~ if self . options [ ] : \n 
~~~ data [ ] = self . _filter_vector ( unknowns , , iteration_coordinate ) \n 
~~~ data [ ] = self . _filter_vector ( resids , , iteration_coordinate ) \n 
~~ self . out [ group_name ] = data \n 
~~ def record_derivatives ( self , derivs , metadata ) : \n 
group_name = % group_name \n 
data [ ] = derivs \n 
self . out [ group_name ] = data \n 
if self . _open_close_sqlitedict : \n 
~~~ if self . out is not None : \n 
~~~ self . out . close ( ) \n 
self . out = None \n 
from numpy import atleast_2d as array2d \n 
from scipy import linalg \n 
from scipy . optimize import minimize \n 
from scipy . spatial . distance import squareform \n 
from openmdao . surrogate_models . surrogate_model import MultiFiSurrogateModel \n 
_logger = logging . getLogger ( ) \n 
THETA0_DEFAULT = 0.5 \n 
THETAL_DEFAULT = 1e-5 \n 
THETAU_DEFAULT = 50 \n 
if hasattr ( linalg , ) : \n 
~~~ solve_triangular = linalg . solve_triangular \n 
~~~ def solve_triangular ( x , y , lower = True ) : \n 
~~~ return linalg . solve ( x , y ) \n 
~~ ~~ def constant_regression ( x ) : \n 
x = np . asarray ( x , dtype = np . float ) \n 
n_eval = x . shape [ 0 ] \n 
f = np . ones ( [ n_eval , 1 ] ) \n 
~~ def linear_regression ( x ) : \n 
f = np . hstack ( [ np . ones ( [ n_eval , 1 ] ) , x ] ) \n 
~~ def squared_exponential_correlation ( theta , d ) : \n 
theta = np . asarray ( theta , dtype = np . float ) \n 
d = np . asarray ( d , dtype = np . float ) \n 
if d . ndim > 1 : \n 
~~~ n_features = d . shape [ 1 ] \n 
~~~ n_features = 1 \n 
~~ if theta . size == 1 : \n 
~~~ return np . exp ( - theta [ 0 ] * np . sum ( d ** 2 , axis = 1 ) ) \n 
~~ elif theta . size != n_features : \n 
~~~ return np . exp ( - np . sum ( theta . reshape ( 1 , n_features ) * d ** 2 , axis = 1 ) ) \n 
~~ ~~ def l1_cross_distances ( X , Y = None ) : \n 
if Y is None : \n 
~~~ X = array2d ( X ) \n 
n_samples , n_features = X . shape \n 
n_nonzero_cross_dist = n_samples * ( n_samples - 1 ) // 2 \n 
D = np . zeros ( ( n_nonzero_cross_dist , n_features ) ) \n 
ll_1 = 0 \n 
for k in range ( n_samples - 1 ) : \n 
~~~ ll_0 = ll_1 \n 
ll_1 = ll_0 + n_samples - k - 1 \n 
D [ ll_0 : ll_1 ] = np . abs ( X [ k ] - X [ ( k + 1 ) : ] ) \n 
~~ return D \n 
Y = array2d ( Y ) \n 
n_samples_X , n_features_X = X . shape \n 
n_samples_Y , n_features_Y = Y . shape \n 
if n_features_X != n_features_Y : \n 
~~ n_features = n_features_X \n 
n_nonzero_cross_dist = n_samples_X * n_samples_Y \n 
for k in range ( n_samples_X ) : \n 
D [ ll_0 : ll_1 ] = np . abs ( X [ k ] - Y ) \n 
~~ ~~ class MultiFiCoKriging ( object ) : \n 
_regression_types = { \n 
: constant_regression , \n 
: linear_regression } \n 
def __init__ ( self , regr = , rho_regr = , \n 
theta = None , theta0 = None , thetaL = None , thetaU = None ) : \n 
~~~ self . corr = squared_exponential_correlation \n 
self . regr = regr \n 
self . rho_regr = rho_regr \n 
self . theta = theta \n 
self . theta0 = theta0 \n 
self . thetaL = thetaL \n 
self . thetaU = thetaU \n 
self . _nfev = 0 \n 
~~ def _build_R ( self , lvl , theta ) : \n 
D = self . D [ lvl ] \n 
n_samples = self . n_samples [ lvl ] \n 
R = np . eye ( n_samples ) * ( 1. + NUGGET ) \n 
corr = squareform ( self . corr ( theta , D ) ) \n 
R = R + corr \n 
return R \n 
~~ def fit ( self , X , y , \n 
initial_range = INITIAL_RANGE_DEFAULT , tol = TOLERANCE_DEFAULT ) : \n 
self . _check_list_structure ( X , y ) \n 
self . _check_params ( ) \n 
X = self . X \n 
y = self . y \n 
nlevel = self . nlevel \n 
n_samples = self . n_samples \n 
self . beta = nlevel * [ 0 ] \n 
self . beta_rho = nlevel * [ None ] \n 
self . beta_regr = nlevel * [ None ] \n 
self . C = nlevel * [ 0 ] \n 
self . D = nlevel * [ 0 ] \n 
self . F = nlevel * [ 0 ] \n 
self . p = nlevel * [ 0 ] \n 
self . q = nlevel * [ 0 ] \n 
self . G = nlevel * [ 0 ] \n 
self . sigma2 = nlevel * [ 0 ] \n 
self . _R_adj = nlevel * [ None ] \n 
y_best = y [ nlevel - 1 ] \n 
for i in range ( nlevel - 1 ) [ : : - 1 ] : \n 
~~~ y_best = np . concatenate ( ( y [ i ] [ : - n_samples [ i + 1 ] ] , y_best ) ) \n 
~~ self . y_best = y_best \n 
self . y_mean = np . zeros ( 1 ) \n 
self . y_std = np . ones ( 1 ) \n 
self . X_mean = np . zeros ( 1 ) \n 
self . X_std = np . ones ( 1 ) \n 
for lvl in range ( nlevel ) : \n 
~~~ self . D [ lvl ] = l1_cross_distances ( X [ lvl ] ) \n 
if ( np . min ( np . sum ( self . D [ lvl ] , axis = 1 ) ) == 0. ) : \n 
~~ self . F [ lvl ] = self . regr ( X [ lvl ] ) \n 
self . p [ lvl ] = self . F [ lvl ] . shape [ 1 ] \n 
if lvl > 0 : \n 
~~~ F_rho = self . rho_regr ( X [ lvl ] ) \n 
self . q [ lvl ] = F_rho . shape [ 1 ] \n 
self . F [ lvl ] = np . hstack ( ( F_rho * np . dot ( ( self . y [ lvl - 1 ] ) [ - n_samples [ lvl ] : ] , \n 
np . ones ( ( 1 , self . q [ lvl ] ) ) ) , self . F [ lvl ] ) ) \n 
~~~ self . q [ lvl ] = 0 \n 
~~ n_samples_F_i = self . F [ lvl ] . shape [ 0 ] \n 
if n_samples_F_i != n_samples [ lvl ] : \n 
~~ if int ( self . p [ lvl ] + self . q [ lvl ] ) >= n_samples_F_i : \n 
% ( n_samples [ i ] , self . p [ lvl ] + self . q [ lvl ] ) ) \n 
~~ ~~ self . X = X \n 
self . y = y \n 
self . rlf_value = np . zeros ( nlevel ) \n 
~~~ if self . theta [ lvl ] is None : \n 
~~~ sol = self . _max_rlf ( lvl = lvl , initial_range = initial_range , tol = tol ) \n 
self . theta [ lvl ] = sol [ ] \n 
self . rlf_value [ lvl ] = sol [ ] \n 
if np . isinf ( self . rlf_value [ lvl ] ) : \n 
~~~ self . rlf_value [ lvl ] = self . rlf ( lvl = lvl ) \n 
~~ ~~ ~~ return \n 
~~ def rlf ( self , lvl , theta = None ) : \n 
if theta is None : \n 
~~~ theta = self . theta [ lvl ] \n 
~~ rlf_value = 1e20 \n 
y = self . y [ lvl ] \n 
F = self . F [ lvl ] \n 
p = self . p [ lvl ] \n 
q = self . q [ lvl ] \n 
R = self . _build_R ( lvl , theta ) \n 
~~~ C = linalg . cholesky ( R , lower = True ) \n 
~~ except linalg . LinAlgError : \n 
~~~ _logger . warning ( ( % lvl ) + \n 
+ str ( theta ) ) \n 
return rlf_value \n 
~~ Ft = solve_triangular ( C , F , lower = True ) \n 
Yt = solve_triangular ( C , y , lower = True ) \n 
~~~ Q , G = linalg . qr ( Ft , econ = True ) \n 
~~~ Q , G = linalg . qr ( Ft , mode = ) \n 
~~ beta = solve_triangular ( G , np . dot ( Q . T , Yt ) ) \n 
err = Yt - np . dot ( Ft , beta ) \n 
err2 = np . dot ( err . T , err ) [ 0 , 0 ] \n 
self . _err = err \n 
sigma2 = err2 / ( n_samples - p - q ) \n 
detR = ( ( np . diag ( C ) ) ** ( 2. / n_samples ) ) . prod ( ) \n 
rlf_value = ( n_samples - p - q ) * np . log10 ( sigma2 ) + n_samples * np . log10 ( detR ) \n 
self . beta_rho [ lvl ] = beta [ : q ] \n 
self . beta_regr [ lvl ] = beta [ q : ] \n 
self . beta [ lvl ] = beta \n 
self . sigma2 [ lvl ] = sigma2 \n 
self . C [ lvl ] = C \n 
self . G [ lvl ] = G \n 
~~ def _max_rlf ( self , lvl , initial_range , tol ) : \n 
thetaL = self . thetaL [ lvl ] \n 
thetaU = self . thetaU [ lvl ] \n 
def rlf_transform ( x ) : \n 
~~~ return self . rlf ( theta = 10. ** x , lvl = lvl ) \n 
~~ theta0 = self . theta0 [ lvl ] \n 
x0 = np . log10 ( theta0 [ 0 ] ) \n 
constraints = [ ] \n 
for i in range ( theta0 . size ) : \n 
~~~ constraints . append ( { : , : lambda log10t , i = i : \n 
log10t [ i ] - np . log10 ( thetaL [ 0 ] [ i ] ) } ) \n 
constraints . append ( { : , : lambda log10t , i = i : \n 
np . log10 ( thetaU [ 0 ] [ i ] ) - log10t [ i ] } ) \n 
~~ constraints = tuple ( constraints ) \n 
sol = minimize ( rlf_transform , x0 , method = , \n 
constraints = constraints , \n 
options = { : initial_range , \n 
: tol , : 0 } ) \n 
log10_optimal_x = sol [ ] \n 
optimal_rlf_value = sol [ ] \n 
self . _nfev += sol [ ] \n 
optimal_theta = 10. ** log10_optimal_x \n 
res = { } \n 
res [ ] = optimal_theta \n 
res [ ] = optimal_rlf_value \n 
~~ def predict ( self , X , eval_MSE = True ) : \n 
X = array2d ( X ) \n 
n_eval , n_features_X = X . shape \n 
mu = np . zeros ( ( n_eval , nlevel ) ) \n 
f = self . regr ( X ) \n 
f0 = self . regr ( X ) \n 
dx = l1_cross_distances ( X , Y = self . X [ 0 ] ) \n 
F = self . F [ 0 ] \n 
C = self . C [ 0 ] \n 
beta = self . beta [ 0 ] \n 
Ft = solve_triangular ( C , F , lower = True ) \n 
yt = solve_triangular ( C , self . y [ 0 ] , lower = True ) \n 
r_ = self . corr ( self . theta [ 0 ] , dx ) . reshape ( n_eval , self . n_samples [ 0 ] ) \n 
gamma = solve_triangular ( C . T , yt - np . dot ( Ft , beta ) , lower = False ) \n 
mu [ : , 0 ] = ( np . dot ( f , beta ) + np . dot ( r_ , gamma ) ) . ravel ( ) \n 
if eval_MSE : \n 
~~~ self . sigma2_rho = nlevel * [ None ] \n 
MSE = np . zeros ( ( n_eval , nlevel ) ) \n 
r_t = solve_triangular ( C , r_ . T , lower = True ) \n 
G = self . G [ 0 ] \n 
u_ = solve_triangular ( G . T , f . T - np . dot ( Ft . T , r_t ) , lower = True ) \n 
MSE [ : , 0 ] = self . sigma2 [ 0 ] * ( 1 - ( r_t ** 2 ) . sum ( axis = 0 ) + ( u_ ** 2 ) . sum ( axis = 0 ) ) \n 
~~ for i in range ( 1 , nlevel ) : \n 
~~~ C = self . C [ i ] \n 
F = self . F [ i ] \n 
g = self . rho_regr ( X ) \n 
dx = l1_cross_distances ( X , Y = self . X [ i ] ) \n 
r_ = self . corr ( self . theta [ i ] , dx ) . reshape ( n_eval , self . n_samples [ i ] ) \n 
f = np . vstack ( ( g . T * mu [ : , i - 1 ] , f0 . T ) ) \n 
yt = solve_triangular ( C , self . y [ i ] , lower = True ) \n 
G = self . G [ i ] \n 
beta = self . beta [ i ] \n 
mu [ : , i ] = ( np . dot ( f . T , beta ) + np . dot ( r_t . T , yt - np . dot ( Ft , beta ) ) ) . ravel ( ) \n 
~~~ Q_ = ( np . dot ( ( yt - np . dot ( Ft , beta ) ) . T , yt - np . dot ( Ft , beta ) ) ) [ 0 , 0 ] \n 
u_ = solve_triangular ( G . T , f - np . dot ( Ft . T , r_t ) , lower = True ) \n 
sigma2_rho = np . dot ( g , self . sigma2 [ i ] * linalg . inv ( np . dot ( G . T , G ) ) [ : self . q [ i ] , : self . q [ i ] ] + np . dot ( beta [ : self . q [ i ] ] , beta [ : self . q [ i ] ] . T ) ) \n 
sigma2_rho = ( sigma2_rho * g ) . sum ( axis = 1 ) \n 
MSE [ : , i ] = sigma2_rho * MSE [ : , i - 1 ] + Q_ / ( 2 * ( self . n_samples [ i ] - self . p [ i ] - self . q [ i ] ) ) * ( 1 - ( r_t ** 2 ) . sum ( axis = 0 ) ) + self . sigma2 [ i ] * ( u_ ** 2 ) . sum ( axis = 0 ) \n 
~~~ mu [ : , i ] = self . y_mean + self . y_std * mu [ : , i ] \n 
~~~ MSE [ : , i ] = self . y_std ** 2 * MSE [ : , i ] \n 
~~ ~~ if eval_MSE : \n 
~~~ return mu [ : , - 1 ] . reshape ( ( n_eval , 1 ) ) , MSE [ : , - 1 ] . reshape ( ( n_eval , 1 ) ) \n 
~~~ return mu [ : , - 1 ] . reshape ( ( n_eval , 1 ) ) \n 
~~ ~~ def _check_list_structure ( self , X , y ) : \n 
~~~ if type ( X ) is not list : \n 
~~~ nlevel = 1 \n 
X = [ X ] \n 
~~~ nlevel = len ( X ) \n 
~~ if type ( y ) is not list : \n 
~~~ y = [ y ] \n 
~~ if len ( X ) != len ( y ) : \n 
~~ n_samples = np . zeros ( nlevel , dtype = int ) \n 
n_features = np . zeros ( nlevel , dtype = int ) \n 
n_samples_y = np . zeros ( nlevel , dtype = int ) \n 
for i in range ( nlevel ) : \n 
~~~ n_samples [ i ] , n_features [ i ] = X [ i ] . shape \n 
if i > 1 and n_features [ i ] != n_features [ i - 1 ] : \n 
~~ y [ i ] = np . asarray ( y [ i ] ) . ravel ( ) [ : , np . newaxis ] \n 
n_samples_y [ i ] = y [ i ] . shape [ 0 ] \n 
if n_samples [ i ] != n_samples_y [ i ] : \n 
~~ ~~ self . n_features = n_features [ 0 ] \n 
if type ( self . theta ) is not list : \n 
~~~ self . theta = nlevel * [ self . theta ] \n 
~~ elif len ( self . theta ) != nlevel : \n 
~~ if type ( self . theta0 ) is not list : \n 
~~~ self . theta0 = nlevel * [ self . theta0 ] \n 
~~ elif len ( self . theta0 ) != nlevel : \n 
~~ if type ( self . thetaL ) is not list : \n 
~~~ self . thetaL = nlevel * [ self . thetaL ] \n 
~~ elif len ( self . thetaL ) != nlevel : \n 
~~ if type ( self . thetaU ) is not list : \n 
~~~ self . thetaU = nlevel * [ self . thetaU ] \n 
~~ elif len ( self . thetaU ) != nlevel : \n 
~~ self . nlevel = nlevel \n 
self . X = X [ : ] \n 
self . y = y [ : ] \n 
self . n_samples = n_samples \n 
~~ def _check_params ( self ) : \n 
~~~ if not callable ( self . regr ) : \n 
~~~ if self . regr in self . _regression_types : \n 
~~~ self . regr = self . _regression_types [ self . regr ] \n 
% ( self . _regression_types . keys ( ) , self . regr ) ) \n 
~~ ~~ if not callable ( self . rho_regr ) : \n 
~~~ if self . rho_regr in self . _regression_types : \n 
~~~ self . rho_regr = self . _regression_types [ self . rho_regr ] \n 
% ( self . _regression_types . keys ( ) , self . rho_regr ) ) \n 
~~ ~~ for i in range ( self . nlevel ) : \n 
~~~ if self . theta [ i ] is not None : \n 
~~~ self . theta [ i ] = array2d ( self . theta [ i ] ) \n 
if np . any ( self . theta [ i ] <= 0 ) : \n 
~~ ~~ if self . theta0 [ i ] is not None : \n 
~~~ self . theta0 [ i ] = array2d ( self . theta0 [ i ] ) \n 
if np . any ( self . theta0 [ i ] <= 0 ) : \n 
~~~ self . theta0 [ i ] = array2d ( self . n_features * [ THETA0_DEFAULT ] ) \n 
~~ lth = self . theta0 [ i ] . size \n 
if self . thetaL [ i ] is not None : \n 
~~~ self . thetaL [ i ] = array2d ( self . thetaL [ i ] ) \n 
if self . thetaL [ i ] . size != lth : \n 
~~~ self . thetaL [ i ] = array2d ( self . n_features * [ THETAL_DEFAULT ] ) \n 
~~ if self . thetaU [ i ] is not None : \n 
~~~ self . thetaU [ i ] = array2d ( self . thetaU [ i ] ) \n 
if self . thetaU [ i ] . size != lth : \n 
~~~ self . thetaU [ i ] = array2d ( self . n_features * [ THETAU_DEFAULT ] ) \n 
~~ if np . any ( self . thetaL [ i ] <= 0 ) or np . any ( self . thetaU [ i ] < self . thetaL [ i ] ) : \n 
"thetaU." ) \n 
~~ ~~ class MultiFiCoKrigingSurrogate ( MultiFiSurrogateModel ) : \n 
theta = None , theta0 = None , thetaL = None , thetaU = None , \n 
tolerance = TOLERANCE_DEFAULT , initial_range = INITIAL_RANGE_DEFAULT ) : \n 
~~~ super ( MultiFiCoKrigingSurrogate , self ) . __init__ ( ) \n 
self . tolerance = tolerance \n 
self . initial_range = initial_range \n 
self . model = MultiFiCoKriging ( regr = regr , rho_regr = rho_regr , theta = theta , \n 
theta0 = theta0 , thetaL = thetaL , thetaU = thetaU ) \n 
~~ def predict ( self , new_x ) : \n 
Y_pred , MSE = self . model . predict ( [ new_x ] ) \n 
return Y_pred , np . sqrt ( np . abs ( MSE ) ) \n 
~~ def train_multifi ( self , X , Y ) : \n 
X , Y = self . _fit_adapter ( X , Y ) \n 
self . model . fit ( X , Y , tol = self . tolerance , initial_range = self . initial_range ) \n 
~~ def _fit_adapter ( self , X , Y ) : \n 
~~~ if len ( np . shape ( np . array ( X [ 0 ] ) ) ) == 1 : \n 
~~~ X = [ X ] \n 
Y = [ Y ] \n 
~~ X = [ np . array ( x ) for x in reversed ( X ) ] \n 
Y = [ np . array ( y ) for y in reversed ( Y ) ] \n 
return ( X , Y ) \n 
~~ ~~ class FloatMultiFiCoKrigingSurrogate ( MultiFiCoKrigingSurrogate ) : \n 
def predict ( self , new_x ) : \n 
~~~ dist = super ( FloatMultiFiCoKrigingSurrogate , self ) . predict ( new_x ) \n 
return dist . mu \n 
~~~ import doctest \n 
doctest . testmod ( ) \n 
from six . moves import range \n 
for index in range ( self . current_row , max_lines ) : \n 
for index in range ( max_lines , - 1 , - 1 ) : \n 
if row_end is None : \n 
field_end ) : \n 
data = np . zeros ( shape = ( 0 , 0 ) ) \n 
newdata = np . array ( parsed [ : ] ) \n 
if newdata . dtype . type is np . str_ : \n 
~~~ newdata = np . array ( line ) \n 
~~ data = np . append ( data , newdata ) \n 
~~~ data = np . append ( data , np . array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) ) \n 
~~~ data = np . append ( data , np . array ( parsed [ ( fieldstart - 1 ) : ] ) ) \n 
row = np . array ( parsed [ : ] ) \n 
data = np . zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n 
data [ i + 1 , : ] = np . array ( parsed [ : ] ) \n 
~~~ row = np . array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) \n 
~~~ row = np . array ( parsed [ ( fieldstart - 1 ) : ] ) \n 
~~ data = np . zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n 
~~~ data [ i + 1 , : ] = np . array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) \n 
~~~ print ( data ) \n 
~~~ data [ i + 1 , : ] = np . array ( parsed [ ( fieldstart - 1 ) : ] ) \n 
~~ ~~ from django import template \n 
@ register . filter ( name = ) \n 
def get_item ( dictionary , key ) : \n 
~~~ return getattr ( dictionary , key ) \n 
if sys . version_info < ( 2 , 6 , 0 ) : \n 
~~~ print ( \n 
~~ elif sys . version_info >= ( 3 , 0 , 0 ) : \n 
~~~ sys . stderr . write ( \n 
~~ sys . path . insert ( 0 , ) \n 
sys . path . insert ( 0 , os . getcwd ( ) ) \n 
import optparse \n 
~~~ import instances_creator_conf as icc \n 
import psycopg2 \n 
sys . exit ( 3 ) \n 
~~ os . environ [ ] = icc . DB_PASSWORD \n 
class AskbotInstance ( ) : \n 
if not os . environ [ ] == : \n 
~~~ self . abort ( ) \n 
~~ sys . path . insert ( 0 , icc . DEFAULT_INSTANCE_DIR ) \n 
#""" \n 
#file. \n 
~~ def _populate_file ( self , original_file , values ) : \n 
f = open ( original_file , ) \n 
file_content = f . read ( ) \n 
populated_settings = file_content . format ( ** values ) \n 
f . write ( populated_settings ) \n 
~~ def create_instance ( self , instance_name , instance_db_name ) : \n 
INSTANCE_DIR = os . path . join ( icc . DEFAULT_INSTANCE_DIR , instance_name ) \n 
~~~ shutil . copytree ( icc . SKEL_DIR , INSTANCE_DIR ) \n 
os . chdir ( INSTANCE_DIR ) \n 
template = os . path . join ( INSTANCE_DIR , ) \n 
os . symlink ( \n 
os . path . join ( , , ) , \n 
os . path . join ( INSTANCE_DIR , ) ) \n 
values = { \n 
: instance_name , \n 
: instance_db_name , \n 
: icc . DB_HOST , \n 
: icc . BASE_URL \n 
self . _populate_file ( template , values ) \n 
print ( . format ( instance_name ) ) \n 
~~~ self . abort ( \n 
~~ ~~ def create_db ( self , instance_db_name ) : \n 
icc . DB_USER ) , shell = True ) \n 
~~~ psycopg2 . connect ( \n 
database = instance_db_name , \n 
user = icc . DB_USER , \n 
password = icc . DB_PASSWORD , \n 
host = icc . DB_HOST \n 
print ( \n 
. format ( instance_db_name ) ) \n 
~~ ~~ def syncdb_and_migrate ( self , instance_name ) : \n 
working_dir = os . path . join ( icc . DEFAULT_INSTANCE_DIR , instance_name ) \n 
os . chdir ( working_dir ) \n 
syncdb = subprocess . Popen ( ( \n 
) , shell = True ) \n 
syncdb . wait ( ) \n 
~~ def collect_static ( seld , instance_name ) : \n 
collectstatic = subprocess . Popen ( ( \n 
collectstatic . wait ( ) \n 
~~ def add_instance_to_supervisor ( self , instance_name ) : \n 
~~~ template = os . path . join ( INSTANCE_DIR , ) \n 
: INSTANCE_DIR \n 
os . symlink ( template , os . path . join ( \n 
% instance_name ) ) \n 
~~ ~~ def add_instance_to_nginx ( self , instance_name ) : \n 
values = { : instance_name } \n 
~~ ~~ def restart_server ( self ) : \n 
~~~ supervisord = subprocess . Popen ( ( \n 
supervisord . wait ( ) \n 
nginx = subprocess . Popen ( ( \n 
nginx . wait ( ) \n 
time . sleep ( 5 ) \n 
~~ def update_entries_metadata ( self ) : \n 
~~~ update_metadata = subprocess . Popen ( ( \n 
url = ( \n 
) % icc . META_REFRESH_KEY \n 
requests . get ( url ) \n 
~~ ~~ def disable_instance ( self , instance_name ) : \n 
~~~ if not os . path . isdir ( icc . DEFAULT_DISABLED_INSTANCES_DIR ) : \n 
~~~ os . makedirs ( icc . DEFAULT_DISABLED_INSTANCES_DIR ) \n 
~~ shutil . copy ( INSTANCE_DIR , icc . DEFAULT_DISABLED_INSTANCES_DIR ) \n 
shutil . rmtree ( INSTANCE_DIR ) \n 
~~ ~~ def destroy_instance ( self , instance_name ) : \n 
sys . path . insert ( 0 , INSTANCE_DIR ) \n 
~~~ import instance_settings \n 
~~~ instance_db_name = instance_settings . DATABASE_NAME \n 
instance_db_name , shell = True ) \n 
dropdb . wait ( ) \n 
os . remove ( os . path . join ( , , \n 
~~ ~~ def abort ( self , msg , status = 1 ) : \n 
~~~ sys . stderr . write ( msg ) \n 
sys . exit ( status ) \n 
~~ ~~ parser = optparse . OptionParser ( \n 
description = ( \n 
parser . add_option ( \n 
nargs = 2 \n 
dest = \n 
( opts , args ) = parser . parse_args ( ) \n 
inst = AskbotInstance ( ) \n 
if opts . instance_data : \n 
~~~ INSTANCE_NAME = opts . instance_data [ 0 ] \n 
INSTANCE_DB_NAME = opts . instance_data [ 1 ] \n 
inst . create_instance ( INSTANCE_NAME , INSTANCE_DB_NAME ) \n 
inst . add_instance_to_supervisor ( INSTANCE_NAME ) \n 
inst . add_instance_to_nginx ( INSTANCE_NAME ) \n 
inst . create_db ( INSTANCE_DB_NAME ) \n 
inst . syncdb_and_migrate ( INSTANCE_NAME ) \n 
inst . collect_static ( INSTANCE_NAME ) \n 
inst . restart_server ( ) \n 
if not opts . no_metadata : \n 
~~~ inst . update_entries_metadata ( ) \n 
~~ ~~ elif opts . disable_instance_name : \n 
~~~ INSTANCE_NAME = opts . disable_instance_name \n 
inst . disable_instance ( INSTANCE_NAME ) \n 
~~ elif opts . destroy_instance_name : \n 
~~~ INSTANCE_NAME = opts . destroy_instance_name \n 
inst . destroy_instance ( INSTANCE_NAME ) \n 
~~ default_app_config = \n 
from . . utils . access_permissions import BaseAccessPermissions \n 
class MediafileAccessPermissions ( BaseAccessPermissions ) : \n 
def can_retrieve ( self , user ) : \n 
return user . has_perm ( ) \n 
~~ def get_serializer_class ( self , user = None ) : \n 
from . serializers import MediafileSerializer \n 
return MediafileSerializer \n 
~~ ~~ from __future__ import unicode_literals \n 
import openslides . utils . models \n 
~~~ initial = True \n 
dependencies = [ \n 
migrations . CreateModel ( \n 
( , models . AutoField ( auto_created = True , primary_key = True , serialize = False , verbose_name ( , models . CharField ( max_length = 128 , verbose_name = ) ) , \n 
( , models . DateTimeField ( blank = True , null = True , verbose_name = ( , models . BooleanField ( \n 
help_text = verbose_name = ) ) , \n 
( , models . CharField ( blank = True , max_length = 255 , unique = True ) ) , \n 
( , models . CharField ( blank = True , max_length = 255 ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 255 ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 50 ) ) , \n 
( , models . TextField ( blank = True , default = ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 100 ) ) , \n 
( , models . BooleanField ( default = True ) ) , \n 
( , models . BooleanField ( default = False ) ) , \n 
( , models . ManyToManyField ( \n 
blank = True , \n 
help_text = related_name = , \n 
related_query_name = , \n 
to = , \n 
verbose_name = ) ) , \n 
help_text = , \n 
related_name = , \n 
options = { \n 
: ( \n 
( , ) ( , ) ) , \n 
: ( ) , \n 
: ( , , ) , \n 
bases = ( openslides . utils . models . RESTModelMixin , models . Model ) , \n 
from django . dispatch import receiver \n 
from rest_framework import status \n 
from rest_framework . test import APIClient \n 
from openslides import __version__ as version \n 
from openslides . core . config import ConfigVariable , config \n 
from openslides . core . models import CustomSlide , Projector \n 
from openslides . core . signals import config_signal \n 
from openslides . utils . rest_api import ValidationError \n 
from openslides . utils . test import TestCase \n 
class ProjectorAPI ( TestCase ) : \n 
def test_slide_on_default_projector ( self ) : \n 
~~~ self . client . login ( username = , password = ) \n 
customslide = CustomSlide . objects . create ( title = , text = default_projector = Projector . objects . get ( pk = 1 ) \n 
default_projector . config = { \n 
: { : , : customslide . id } } \n 
default_projector . save ( ) \n 
response = self . client . get ( reverse ( , args = [ ] ) ) \n 
self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n 
self . assertEqual ( json . loads ( response . content . decode ( ) ) , { \n 
{ : customslide . id , \n 
: } } , \n 
: 0 , \n 
~~ def test_invalid_slide_on_default_projector ( self ) : \n 
default_projector = Projector . objects . get ( pk = 1 ) \n 
: { : } } \n 
~~ ~~ class VersionView ( TestCase ) : \n 
def test_get ( self ) : \n 
response = self . client . get ( reverse ( ) ) \n 
: version , \n 
: } ] } ) \n 
~~ ~~ class ConfigViewSet ( TestCase ) : \n 
def test_retrieve ( self ) : \n 
config [ ] = \n 
response = self . client . get ( reverse ( , args = [ ] ) ) self . assertEqual ( \n 
response . data , \n 
: } ) \n 
~~ def test_update ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( config [ ] , ) \n 
~~ def test_update_wrong_datatype ( self ) : \n 
self . assertEqual ( response . status_code , status . HTTP_400_BAD_REQUEST ) \n 
~~ def test_update_wrong_datatype_that_can_be_converted ( self ) : \n 
self . client = APIClient ( ) \n 
self . assertEqual ( response . status_code , 200 ) \n 
~~ def test_update_good_choice ( self ) : \n 
~~ def test_update_bad_choice ( self ) : \n 
self . assertEqual ( response . data , { : } ) \n 
~~ def test_update_validator_ok ( self ) : \n 
~~ def test_update_validator_invalid ( self ) : \n 
~~ def test_update_only_with_key ( self ) : \n 
reverse ( , args = [ ] ) ) \n 
~~ def test_metadata_with_hidden ( self ) : \n 
response = self . client . options ( reverse ( ) ) \n 
filter_obj = filter ( \n 
lambda item : item [ ] == , \n 
response . data [ ] [ 0 ] [ ] [ 0 ] [ ] ) \n 
self . assertEqual ( len ( list ( filter_obj ) ) , 0 ) \n 
~~ ~~ def validator_for_testing ( value ) : \n 
if value == : \n 
~~~ raise ValidationError ( { : } ) \n 
~~ ~~ @ receiver ( config_signal , dispatch_uid = ) \n 
def set_simple_config_view_integration_config_test ( sender , ** kwargs ) : \n 
yield ConfigVariable ( \n 
default_value = None , \n 
label = ) \n 
default_value = ) \n 
default_value = 0 , \n 
input_type = ) \n 
default_value = , \n 
input_type = , \n 
choices = ( \n 
{ : , : } , { : , : } ) \n 
validators = ( validator_for_testing , ) ) \n 
label = , \n 
hidden = True ) \n 
~~ from unittest import TestCase \n 
from unittest . mock import MagicMock , patch \n 
from openslides . users . serializers import UserFullSerializer \n 
class UserCreateUpdateSerializerTest ( TestCase ) : \n 
~~~ def test_validate_no_data ( self ) : \n 
serializer = UserFullSerializer ( ) \n 
~~~ serializer . validate ( data ) \n 
~~ ~~ @ patch ( ) \n 
def test_validate_no_username ( self , generate_username ) : \n 
generate_username . return_value = \n 
data = { : } \n 
new_data = serializer . validate ( data ) \n 
self . assertEqual ( new_data [ ] , ) \n 
~~ def test_validate_no_username_in_patch_request ( self ) : \n 
view = MagicMock ( action = ) \n 
serializer = UserFullSerializer ( context = { : view } ) \n 
self . assertIsNone ( new_data . get ( ) ) \n 
#domain... #localhost... \n 
, re . IGNORECASE ) \n 
USERNAME_REGEX = re . compile ( , re . I ) \n 
FULLNAME_REGEX = re . compile ( , re . U ) \n 
EMAIL_REGEX = re . compile ( , re . IGNORECASE ) \n 
class CheckValue ( object ) : \n 
~~ def length ( self , data , minimum = - 1 , maximum = - 1 ) : \n 
len_input = len ( data ) \n 
if len_input < minimum or maximum != - 1 and len_input > maximum : \n 
~~ def regexp ( self , data , regex , flags = 0 ) : \n 
regex = re . compile ( regex , flags ) \n 
if regex . match ( data ) : \n 
~~ def username ( self , data ) : \n 
if USERNAME_REGEX . match ( data ) : \n 
~~ def full_name ( self , data ) : \n 
if FULLNAME_REGEX . match ( data ) : \n 
~~ def email ( self , data ) : \n 
if EMAIL_REGEX . match ( data ) : \n 
~~ def url ( self , data ) : \n 
if URL_REGEX . match ( data ) : \n 
~~ def url_two ( self , data ) : \n 
regex = re . compile ( , re . IGNORECASE ) \n 
~~ def is_integer ( self , data ) : \n 
~~~ tmp = int ( data ) \n 
~~ ~~ def float ( self , data ) : \n 
~~~ tmp = float ( data ) \n 
from bagpipe . bgp . common import utils \n 
from bagpipe . bgp . common import logDecorator \n 
from bagpipe . bgp . vpn . vpn_instance import VPNInstance \n 
from bagpipe . bgp . engine import RouteEvent \n 
from bagpipe . bgp . vpn . dataplane_drivers import DummyDataplaneDriver as _DummyDataplaneDriver \n 
from bagpipe . bgp . common . looking_glass import LookingGlass , LGMap \n 
from bagpipe . exabgp . structure . vpn import RouteDistinguisher , VPNLabelledPrefix \n 
from bagpipe . exabgp . structure . mpls import LabelStackEntry \n 
from bagpipe . exabgp . structure . address import AFI , SAFI \n 
from bagpipe . exabgp . structure . ip import Inet , Prefix \n 
from bagpipe . exabgp . message . update . route import Route \n 
from bagpipe . exabgp . message . update . attribute . nexthop import NextHop \n 
from bagpipe . exabgp . message . update . attribute . communities import ECommunities \n 
class DummyDataplaneDriver ( _DummyDataplaneDriver ) : \n 
~~ class VRF ( VPNInstance , LookingGlass ) : \n 
~~~ type = "ipvpn" \n 
afi = AFI ( AFI . ipv4 ) \n 
safi = SAFI ( SAFI . mpls_vpn ) \n 
@ logDecorator . log \n 
~~~ VPNInstance . __init__ ( self , * args , ** kwargs ) \n 
self . readvertised = set ( ) \n 
~~ def _routeFrom ( self , prefix , label , rd ) : \n 
~~~ return Route ( VPNLabelledPrefix ( self . afi , self . safi , prefix , rd , \n 
[ LabelStackEntry ( label , True ) ] \n 
~~ def generateVifBGPRoute ( self , macAdress , ipPrefix , prefixLen , label ) : \n 
~~~ route = self . _routeFrom ( Prefix ( self . afi , ipPrefix , prefixLen ) , label , \n 
RouteDistinguisher ( \n 
RouteDistinguisher . TYPE_IP_LOC , None , \n 
self . bgpManager . getLocalAddress ( ) , \n 
self . instanceId ) \n 
return self . _newRouteEntry ( self . afi , self . safi , self . exportRTs , \n 
route . nlri , route . attributes ) \n 
~~ def _getLocalLabels ( self ) : \n 
~~~ for portData in self . macAddress2LocalPortData . itervalues ( ) : \n 
~~~ yield portData [ ] \n 
~~ ~~ def _getRDFromLabel ( self , label ) : \n 
~~~ return RouteDistinguisher ( RouteDistinguisher . TYPE_IP_LOC , None , \n 
10000 + label ) \n 
~~ def _routeForReAdvertisement ( self , prefix , label ) : \n 
~~~ route = self . _routeFrom ( prefix , label , \n 
self . _getRDFromLabel ( label ) ) \n 
nh = Inet ( 1 , socket . inet_pton ( socket . AF_INET , \n 
self . dataplane . driver . getLocalAddress ( ) ) ) \n 
route . attributes . add ( NextHop ( nh ) ) \n 
route . attributes . add ( ECommunities ( self . readvertiseToRTs ) ) \n 
routeEntry = self . _newRouteEntry ( self . afi , self . safi , \n 
self . readvertiseToRTs , \n 
return routeEntry \n 
~~ @ logDecorator . log \n 
def _readvertise ( self , nlri ) : \n 
for label in self . _getLocalLabels ( ) : \n 
nlri . prefix , label ) \n 
routeEntry = self . _routeForReAdvertisement ( nlri . prefix , label ) \n 
self . _pushEvent ( RouteEvent ( RouteEvent . ADVERTISE , routeEntry ) ) \n 
~~ self . readvertised . add ( nlri . prefix ) \n 
def _readvertiseStop ( self , nlri ) : \n 
self . _pushEvent ( RouteEvent ( RouteEvent . WITHDRAW , routeEntry ) ) \n 
~~ self . readvertised . remove ( nlri . prefix ) \n 
~~ def vifPlugged ( self , macAddress , ipAddressPrefix , localPort , \n 
advertiseSubnet ) : \n 
~~~ VPNInstance . vifPlugged ( self , macAddress , ipAddressPrefix , localPort , \n 
advertiseSubnet ) \n 
label = self . macAddress2LocalPortData [ macAddress ] [ ] \n 
for prefix in self . readvertised : \n 
prefix ) \n 
routeEntry = self . _routeForReAdvertisement ( prefix , label ) \n 
~~ ~~ def vifUnplugged ( self , macAddress , ipAddressPrefix , advertiseSubnet ) : \n 
~~~ label = self . macAddress2LocalPortData [ macAddress ] [ ] \n 
~~ VPNInstance . vifUnplugged ( self , macAddress , ipAddressPrefix , \n 
~~ def _route2trackedEntry ( self , route ) : \n 
~~~ if isinstance ( route . nlri , VPNLabelledPrefix ) : \n 
~~~ return route . nlri . prefix \n 
type ( route . nlri ) ) \n 
~~ ~~ def _toReadvertise ( self , route ) : \n 
~~~ return ( len ( set ( route . routeTargets ) . intersection ( \n 
set ( self . readvertiseFromRTs ) ) ) > 0 ) \n 
~~ def _imported ( self , route ) : \n 
set ( self . importRTs ) ) ) > 0 ) \n 
~~ @ utils . synchronized \n 
def _newBestRoute ( self , entry , newRoute ) : \n 
~~~ prefix = entry \n 
if self . readvertise : \n 
if self . _toReadvertise ( newRoute ) : \n 
self . _readvertise ( newRoute . nlri ) \n 
if not self . _imported ( newRoute ) : \n 
~~ ~~ ~~ encaps = self . _checkEncaps ( newRoute ) \n 
if not encaps : \n 
~~ self . dataplane . setupDataplaneForRemoteEndpoint ( \n 
prefix , newRoute . attributes . get ( NextHop . ID ) . next_hop , \n 
newRoute . nlri . labelStack [ 0 ] . labelValue , newRoute . nlri , encaps ) \n 
def _bestRouteRemoved ( self , entry , oldRoute , last ) : \n 
if self . readvertise and last : \n 
~~~ if self . _toReadvertise ( oldRoute ) : \n 
self . _readvertiseStop ( oldRoute . nlri ) \n 
if not self . _imported ( oldRoute ) : \n 
~~ ~~ ~~ if self . _skipRouteRemoval ( last ) : \n 
~~ self . dataplane . removeDataplaneForRemoteEndpoint ( \n 
prefix , oldRoute . attributes . get ( NextHop . ID ) . next_hop , \n 
oldRoute . nlri . labelStack [ 0 ] . labelValue , oldRoute . nlri ) \n 
~~ def getLGMap ( self ) : \n 
"readvertised" : ( LGMap . VALUE , [ repr ( prefix ) for prefix in \n 
self . readvertised ] ) \n 
from bagpipe . exabgp . message . update . attribute import AttributeID , Flag , Attribute \n 
class Origin ( Attribute ) : \n 
~~~ ID = AttributeID . ORIGIN \n 
FLAG = Flag . TRANSITIVE \n 
MULTIPLE = False \n 
IGP = 0x00 \n 
EGP = 0x01 \n 
INCOMPLETE = 0x02 \n 
def __init__ ( self , origin ) : \n 
~~~ self . origin = origin \n 
~~ def pack ( self ) : \n 
~~~ return self . _attribute ( chr ( self . origin ) ) \n 
~~~ return len ( self . pack ( ) ) \n 
~~~ if self . origin == 0x00 : return \n 
if self . origin == 0x01 : return \n 
if self . origin == 0x02 : return \n 
~~~ return str ( self ) \n 
~~ def __cmp__ ( self , other ) : \n 
~~~ if ( not isinstance ( other , Origin ) \n 
or ( self . origin != other . origin ) \n 
~~ ~~ ~~ import setuptools \n 
setuptools . setup ( \n 
setup_requires = [ ] , \n 
pbr = True ) \n 
if __name__ != : raise ImportError ( ) \n 
DIRECTORY = os . path . abspath ( os . path . dirname ( __file__ ) ) \n 
sys . path . insert ( 0 , os . path . abspath ( os . path . join ( DIRECTORY , , , ) ) ) \n 
unforkedPid = os . getpid ( ) \n 
childProcessPid = 0 \n 
from twisted . internet import protocol , defer \n 
from twisted . python . log import FileLogObserver , textFromEventDict \n 
from twisted . python . util import untilConcludes \n 
import signal \n 
DEFAULT_REACTORS = { \n 
def set_reactor ( ) : \n 
~~~ import platform \n 
REACTORNAME = DEFAULT_REACTORS . get ( platform . system ( ) , ) \n 
if REACTORNAME == : \n 
~~~ from twisted . internet import kqreactor \n 
kqreactor . install ( ) \n 
~~ elif REACTORNAME == : \n 
~~~ from twisted . internet import epollreactor \n 
epollreactor . install ( ) \n 
~~~ from twisted . internet import pollreactor \n 
pollreactor . install ( ) \n 
~~~ from twisted . internet import selectreactor \n 
selectreactor . install ( ) \n 
~~ from twisted . internet import reactor \n 
set_reactor = lambda : reactor \n 
return reactor \n 
~~ class ManagedLogger ( FileLogObserver ) : \n 
def emit ( self , eventDict ) : \n 
text = textFromEventDict ( eventDict ) \n 
if text is None : return \n 
untilConcludes ( self . write , text ) \n 
~~ ~~ class DaemonProtocol ( protocol . ProcessProtocol ) : \n 
def __init__ ( self , name , label , r , deferred , ** kwargs ) : \n 
self . reactor = r \n 
out = { \n 
: % ( name , label ) \n 
err = { \n 
self . label = label \n 
import droned . logging \n 
self . log_stdout = droned . logging . logWithContext ( ** out ) \n 
self . log_stderr = droned . logging . logWithContext ( ** err ) \n 
~~ def inConnectionLost ( self ) : \n 
~~ def errReceived ( self , data ) : \n 
self . log_stderr ( str ( data ) ) \n 
~~ def outReceived ( self , data ) : \n 
self . log_stdout ( str ( data ) ) \n 
~~ def outConnectionLost ( self ) : \n 
~~ def errConnectionLost ( self ) : \n 
~~ def connectionMade ( self ) : \n 
global closestdin \n 
if closestdin : \n 
~~ global childProcessPid \n 
global unforkedPid \n 
x = unforkedPid \n 
unforkedPid = 0 \n 
if x : self . reactor . callLater ( 2.0 , os . kill , x , signal . SIGTERM ) \n 
childProcessPid = self . transport . pid \n 
sys . stdout . write ( % ( self . name , self . label , childProcessPid ) ) \n 
~~ def processExited ( self , reason ) : \n 
sys . stdout . write ( % ( self . name , ) ) \n 
if not self . deferred . called : \n 
~~~ self . deferred . errback ( reason ) \n 
~~ global unforkedPid \n 
global childProcessPid \n 
if unforkedPid : os . kill ( unforkedPid , signal . SIGTERM ) \n 
~~ processEnded = processExited \n 
~~ class DaemonWrapper ( object ) : \n 
SIGNALS = dict ( ( k , v ) for v , k in signal . __dict__ . iteritems ( ) if v . startswith ( ) and not v . startswith ( ) ) \n 
def __init__ ( self , r , name , label , cmd , args , env ) : \n 
~~~ self . reactor = r \n 
self . fqcmd = cmd \n 
self . args = args \n 
self . env = env \n 
self . exitCode = 0 \n 
self . deferred = defer . succeed ( None ) \n 
self . log = droned . logging . logWithContext ( type = ) \n 
~~ def routeSignal ( self , signum , frame ) : \n 
if signum == signal . SIGTERM : \n 
~~~ signal . signal ( signal . SIGTERM , signal . SIG_IGN ) \n 
self . reactor . callLater ( 120 , self . reactor . stop ) \n 
if childProcessPid : \n 
~~~ self . log ( % ( self . SIGNALS [ signum ] , childProcessPid ) ) \n 
try : os . kill ( childProcessPid , signum ) \n 
except : \n 
~~~ droned . logging . err ( % ( self . SIGNALS [ signum ] , childProcessPid ) ) \n 
~~ ~~ ~~ def processResult ( self , result ) : \n 
self . reactor . callLater ( 3.0 , self . reactor . stop ) \n 
~~ def running ( self ) : \n 
global masksignals \n 
if masksignals : \n 
~~~ for signum , signame in self . SIGNALS . items ( ) : \n 
~~~ if signame in ( , ) : continue \n 
try : signal . signal ( signum , self . routeSignal ) \n 
~~ ~~ from droned . clients import command \n 
self . log ( % ( self . name , self . label ) ) \n 
pargs = ( self . name , self . label , self . reactor ) \n 
pkwargs = { } \n 
global usetty \n 
global path \n 
self . deferred = command ( self . fqcmd , self . args , self . env , \n 
path , usetty , { } , DaemonProtocol , \n 
* pargs , ** pkwargs \n 
self . deferred . addBoth ( self . processResult ) \n 
return self . deferred \n 
logdir = env . pop ( , os . path . join ( os . path . sep , ) ) \n 
masksignals = bool ( env . pop ( , True ) ) \n 
closestdin = bool ( env . pop ( , True ) ) \n 
name = env . pop ( , ) \n 
label = env . pop ( , ) \n 
usetty = bool ( env . pop ( , ) ) \n 
path = env . pop ( , os . path . sep ) \n 
if name not in logdir : \n 
~~~ t = os . path . join ( logdir , name , label ) \n 
~~~ if not os . path . exists ( t ) : \n 
~~~ os . makedirs ( t , mode = 0 755 ) \n 
~~ logdir = t \n 
~~ if args and os . path . exists ( args [ 0 ] ) : \n 
if os . fork ( ) == 0 : \n 
sys . stdout . write ( % ( os . getpid ( ) , ) ) \n 
sys . stderr . flush ( ) \n 
sys . stdout = droned . logging . StdioKabob ( 0 ) \n 
sys . stderr = droned . logging . StdioKabob ( 1 ) \n 
maxfd = resource . getrlimit ( resource . RLIMIT_NOFILE ) [ 1 ] \n 
if ( maxfd == resource . RLIM_INFINITY ) : \n 
~~ ~~ except : pass \n 
for fd in range ( 0 , maxfd ) : \n 
~~~ try : os . close ( fd ) \n 
except OSError : pass #ignore \n 
~~ os . open ( \n 
hasattr ( os , "devnull" ) and os . devnull or "/dev/null" , \n 
os . O_RDWR \n 
os . dup2 ( 0 , 1 ) \n 
os . dup2 ( 0 , 2 ) \n 
loggers = [ \n 
% ( name , label ) , \n 
droned . logging . logToDir ( directory = logdir ) \n 
reactor = set_reactor ( ) \n 
droned . logging . logToDir ( \n 
directory = logdir , \n 
LOG_TYPE = tuple ( loggers ) , \n 
OBSERVER = ManagedLogger \n 
dmx = DaemonWrapper ( reactor , name , label , args [ 0 ] , args [ 1 : ] , env ) \n 
def killGroup ( ) : \n 
dmx . log ( ) \n 
signal . signal ( signal . SIGTERM , signal . SIG_IGN ) \n 
os . kill ( - os . getpgid ( os . getpid ( ) ) , signal . SIGTERM ) \n 
~~ reactor . addSystemEventTrigger ( , , killGroup ) \n 
reactor . callWhenRunning ( dmx . running ) \n 
sys . exit ( dmx . exitCode ) \n 
~~~ reactor = set_reactor ( ) \n 
reactor . callLater ( 120 , sys . exit , 1 ) \n 
reactor . run ( ) \n 
~~ ~~ sys . exit ( 255 ) \n 
from droned . models . team import Team , SupportAgent \n 
from droned . models . issue import Issue \n 
from droned . responders import responder \n 
def teams ( conversation ) : \n 
conversation . say ( + teamlist , useHTML = False ) \n 
~~~ if not Team . exists ( name ) : \n 
~~~ team = Team ( name ) \n 
if team . agents : \n 
~~~ members = . join ( agent . jid for agent in team . agents ) \n 
conversation . say ( heading + members , useHTML = False ) \n 
team . addMember ( conversation . buddy ) \n 
team . removeMember ( conversation . buddy ) \n 
~~~ agent = SupportAgent ( conversation . buddy ) \n 
agent . ready = False \n 
agent . ready = True \n 
told = 0 \n 
for team in agent . teams : \n 
~~~ for otherAgent in team . agents : \n 
~~~ if otherAgent is agent : continue \n 
otherAgent . tell ( message ) \n 
told += 1 \n 
~~ ~~ if told : \n 
~~~ agent . tell ( % told ) \n 
~~~ agent . tell ( ) \n 
~~ ~~ @ responder ( pattern = "^issues$" , form = , help = ) \n 
def issues ( conversation ) : \n 
~~~ issues = sorted ( [ i for i in Issue . objects if not i . resolved ] , key = lambda i : i . id ) \n 
summaries = [ ] \n 
for issue in issues : \n 
if issue . context [ ] : \n 
~~ elif isinstance ( issue . context [ ] , SupportAgent ) : \n 
~~ elif issue . context [ ] is None : \n 
~~ summaries . append ( summary ) \n 
~~ heading = % len ( issues ) \n 
listing = . join ( summaries ) \n 
conversation . say ( heading + listing , useHTML = False ) \n 
~~~ id = int ( id ) \n 
issue = Issue . byID ( id ) \n 
if not issue : \n 
~~ elif not issue . hasSOP : \n 
conversation . say ( issue . description ) \n 
conversation . say ( contextSummary , useHTML = False ) \n 
~~~ existingAgent = issue . context [ ] \n 
if existingAgent : \n 
~~~ if existingAgent . conversation is conversation : \n 
existingAgent . conversation . nevermind ( ) \n 
agent = SupportAgent ( conversation . buddy ) \n 
agent . currentIssue = None \n 
agent . conversation . nevermind ( ) \n 
agent . engage ( issue ) \n 
~~ ~~ @ responder ( pattern = , form = , \n 
def resolve ( conversation , id , resolution ) : \n 
~~ elif issue . hasSOP : \n 
issue . resolve ( resolution ) \n 
~~ ~~ from kitt . interfaces import moduleProvides , IDroneDService \n 
moduleProvides ( IDroneDService ) #requirement \n 
from kitt . util import dictwrapper \n 
SERVICENAME = \n 
SERVICECONFIG = dictwrapper ( { \n 
config . LOG_DIR : [ \n 
( , int ( 7 * len ( config . AUTOSTART_SERVICES ) ) ) , \n 
import os , re , time \n 
from twisted . application . service import Service \n 
from droned . logging import logWithContext \n 
from kitt . decorators import synchronizedDeferred , deferredAsThread \n 
log = logWithContext ( type = SERVICENAME ) \n 
def ageCompare ( f1 , f2 ) : \n 
~~~ t1 = os . path . getmtime ( f1 ) \n 
t2 = os . path . getmtime ( f2 ) \n 
if t1 > t2 : return 1 \n 
if t2 == t2 : return 0 \n 
if t2 < t2 : return - 1 \n 
~~ class Janitizer ( Service ) : \n 
~~~ minute = property ( lambda foo : 60 ) \n 
hour = property ( lambda foo : 3600 ) \n 
day = property ( lambda foo : 86400 ) \n 
week = property ( lambda f : 604800 ) \n 
oldfiles = { } \n 
watchDict = property ( lambda s : SERVICECONFIG . wrapped . get ( , { } ) ) \n 
busy = defer . DeferredLock ( ) \n 
def update ( self , watchDict ) : \n 
tmp = copy . deepcopy ( self . watchDict ) \n 
SERVICECONFIG . JANITIZE = tmp \n 
~~ @ synchronizedDeferred ( busy ) \n 
@ deferredAsThread \n 
def garbageCheck ( self ) : \n 
for directory , garbageList in watchDict . iteritems ( ) : \n 
~~~ if not os . path . exists ( directory ) : continue \n 
for pattern , limit in garbageList : \n 
~~~ self . cleanupLinks ( directory ) \n 
files = [ os . path . join ( directory , f ) for f in os . listdir ( directory ) if re . search ( pattern , f ) ] \n 
files = sorted ( files ) \n 
if len ( files ) > int ( limit ) : \n 
~~~ log ( % . join ( files ) ) \n 
~~ while len ( files ) > int ( limit ) : \n 
~~~ oldfile = files . pop ( 0 ) \n 
log ( % oldfile ) \n 
if os . path . islink ( oldfile ) : continue \n 
if os . path . isdir ( oldfile ) : \n 
~~~ for base , dirs , myfiles in os . walk ( oldfile , topdown = False ) : \n 
~~~ for name in myfiles : \n 
~~~ os . remove ( os . path . join ( base , name ) ) \n 
~~ for name in dirs : \n 
~~~ os . rmdir ( os . path . join ( base , name ) ) \n 
~~ ~~ os . rmdir ( oldfile ) \n 
~~ else : os . unlink ( oldfile ) \n 
~~ ~~ self . cleanupLinks ( directory ) \n 
~~ ~~ def cleanupLinks ( self , directory ) : \n 
files = [ os . path . join ( directory , f ) for f in os . listdir ( directory ) ] \n 
for f in files [ : ] : \n 
~~~ if not os . path . exists ( f ) : \n 
~~~ log ( % f ) \n 
os . unlink ( f ) \n 
files . remove ( f ) \n 
~~ ~~ return files \n 
~~ def clean_old_files ( self , directory , age , recurse = True ) : \n 
self . oldfiles [ directory ] = ( age , recurse ) \n 
def clean_elderly ( self ) : \n 
for directory in self . oldfiles : \n 
~~~ self . recursive_clean ( directory , * self . oldfiles [ directory ] ) \n 
~~ ~~ def recursive_clean ( self , directory , age , recurse ) : \n 
try : data = map ( lambda n : os . path . join ( directory , n ) , os . listdir ( directory ) ) \n 
~~~ log ( % directory ) \n 
~~ for node in data : \n 
~~~ if os . path . isdir ( node ) and recurse : \n 
~~~ empty = self . recursive_clean ( node , age , recurse ) \n 
if empty : \n 
~~~ try : os . rmdir ( node ) \n 
except : log ( % node ) \n 
if ( time . time ( ) - os . stat ( node ) . st_mtime ) > age : \n 
~~~ try : os . remove ( node ) \n 
~~ ~~ return bool ( os . listdir ( directory ) ) \n 
~~ def startService ( self ) : \n 
self . GARBAGE_CHECK = task . LoopingCall ( self . garbageCheck ) \n 
self . ELDERLY_CHECK = task . LoopingCall ( self . clean_elderly ) \n 
Service . startService ( self ) \n 
self . GARBAGE_CHECK . start ( self . minute * 20 ) \n 
self . ELDERLY_CHECK . start ( self . minute ) \n 
~~ def stopService ( self ) : \n 
~~~ if self . GARBAGE_CHECK . running : \n 
~~~ self . GARBAGE_CHECK . stop ( ) \n 
~~ if self . ELDERLY_CHECK . running : \n 
~~~ self . ELDERLY_CHECK . stop ( ) \n 
Service . stopService ( self ) \n 
~~ ~~ parentService = None \n 
service = None \n 
def update ( watchDict ) : \n 
~~~ global service \n 
if not running ( ) : \n 
~~~ raise AssertionError ( ) \n 
~~ return service . update ( watchDict ) \n 
~~ def install ( _parentService ) : \n 
~~~ global parentService \n 
parentService = _parentService \n 
~~ def start ( ) : \n 
~~~ service = Janitizer ( ) \n 
service . setName ( SERVICENAME ) \n 
service . setServiceParent ( parentService ) \n 
~~ ~~ def stop ( ) : \n 
if running ( ) : \n 
~~~ service . disownServiceParent ( ) \n 
service . stopService ( ) \n 
~~ ~~ def running ( ) : \n 
~~~ return bool ( service ) and service . running \n 
~~ __all__ = [ , , , ] \n 
from OpenPNM . Geometry import models as gm \n 
from OpenPNM . Geometry import GenericGeometry \n 
class SGL10 ( GenericGeometry ) : \n 
~~~ super ( ) . __init__ ( ** kwargs ) \n 
self . _generate ( ) \n 
~~ def _generate ( self ) : \n 
~~~ self . models . add ( propname = , \n 
model = gm . pore_misc . random , \n 
num_range = [ 0 , 0.8834 ] , \n 
regen_mode = ) \n 
self . models . add ( propname = , \n 
model = gm . throat_misc . neighbor , \n 
pore_prop = , \n 
mode = ) \n 
model = gm . pore_diameter . sphere , \n 
psd_name = , \n 
psd_shape = 3.07 , \n 
psd_loc = 1.97e-6 , \n 
psd_scale = 1.6e-5 , \n 
psd_offset = 18e-6 ) \n 
model = gm . pore_area . spherical ) \n 
model = gm . pore_volume . sphere ) \n 
model = gm . throat_diameter . cylinder , \n 
tsd_name = , \n 
tsd_shape = 3.07 , \n 
tsd_loc = 1.97e-6 , \n 
tsd_scale = 1.6e-5 , \n 
tsd_offset = 18e-6 ) \n 
model = gm . throat_length . straight ) \n 
model = gm . throat_volume . cylinder ) \n 
model = gm . throat_area . cylinder ) \n 
model = gm . throat_surface_area . cylinder ) \n 
import scipy as _sp \n 
def pore_to_pore ( geometry , ** kwargs ) : \n 
network = geometry . _net \n 
throats = network . throats ( geometry . name ) \n 
pores = network . find_connected_pores ( throats , flatten = False ) \n 
C0 = network [ ] [ pores , 0 ] \n 
C1 = network [ ] [ pores , 1 ] \n 
V = C1 - C0 \n 
L = _sp . array ( _sp . sqrt ( _sp . sum ( V [ : , : ] ** 2 , axis = 1 ) ) , ndmin = 1 ) \n 
value = V / _sp . array ( L , ndmin = 2 ) . T \n 
def standard ( phase , \n 
pore_MW = , \n 
pore_density = , \n 
MW = phase [ pore_MW ] \n 
rho = phase [ pore_density ] \n 
value = rho / MW \n 
~~ def ideal_gas ( phase , \n 
pore_pressure = , \n 
pore_temperature = , \n 
R = 8.31447 \n 
P = phase [ pore_pressure ] \n 
T = phase [ pore_temperature ] \n 
value = P / ( R * T ) \n 
~~ def vanderwaals ( phase , \n 
pore_P = , \n 
pore_T = , \n 
pore_Pc = , \n 
pore_Tc = , \n 
P = phase [ pore_P ] / 100000 \n 
T = phase [ pore_T ] \n 
Tc = phase [ pore_Tc ] \n 
R = 83.1447 \n 
a = 27 * ( R ** 2 ) * ( Tc ** 2 ) / ( 64 * Pc ) \n 
b = R * Tc / ( 8 * Pc ) \n 
a1 = - 1 / b \n 
a2 = ( R * T + b * P ) / ( a * b ) \n 
a3 = - P / ( a * b ) \n 
a0 = sp . ones ( sp . shape ( a1 ) ) \n 
coeffs = sp . vstack ( ( a0 , a1 , a2 , a3 ) ) . T \n 
density = sp . array ( [ sp . roots ( C ) for C in coeffs ] ) \n 
from distutils . util import convert_path \n 
~~~ from distutils . core import setup \n 
~~ sys . path . append ( os . getcwd ( ) ) \n 
main_ = { } \n 
ver_path = convert_path ( ) \n 
with open ( ver_path ) as f : \n 
~~~ for line in f : \n 
~~~ if line . startswith ( ) : \n 
~~~ exec ( line , main_ ) \n 
~~ ~~ ~~ setup ( \n 
description = version = main_ [ ] , \n 
packages = [ \n 
class PoreCentroidTest : \n 
~~~ def test_voronoi ( self ) : \n 
import OpenPNM \n 
class GenericPhaseTest : \n 
~~~ def setup_class ( self ) : \n 
~~~ self . net = OpenPNM . Network . Cubic ( shape = [ 5 , 5 , 5 ] ) \n 
~~ def test_init_w_no_network ( self ) : \n 
~~~ OpenPNM . Phases . GenericPhase ( ) \n 
~~ def test_init_w_components ( self ) : \n 
~~~ comp1 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
comp2 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
OpenPNM . Phases . GenericPhase ( network = self . net , \n 
components = [ comp1 , comp2 ] ) \n 
~~ def test_set_component_add ( self ) : \n 
phase = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
phase . set_component ( comp1 ) \n 
phase . set_component ( comp2 ) \n 
~~ def test_set_component_add_twice ( self ) : \n 
with pytest . raises ( Exception ) : \n 
~~~ phase . set_components ( comp1 ) \n 
~~ ~~ def test_set_component_remove ( self ) : \n 
phase = OpenPNM . Phases . GenericPhase ( network = self . net , \n 
phase . set_component ( comp1 , mode = ) \n 
phase . set_component ( comp2 , mode = ) \n 
~~ def test_set_component_remove_twice ( self ) : \n 
~~~ phase . set_component ( comp1 , mode = ) \n 
~~ ~~ ~~ import pdb \n 
import os , sys \n 
from cPickle import * \n 
from collections import defaultdict , namedtuple \n 
from pbtools . pbtranscript . Utils import check_ids_unique \n 
import pbtools . pbtranscript . tofu_wrap as tofu_wrap \n 
import pbtools . pbtranscript . BioReaders as BioReaders \n 
import pbtools . pbtranscript . branch . branch_simple2 as branch_simple2 \n 
import pbtools . pbtranscript . counting . compare_junctions as compare_junctions \n 
from pbtools . pbtranscript . io . SeqReaders import LazyFastaReader , LazyFastqReader \n 
from pbcore . io . FastaIO import FastaWriter \n 
from pbcore . io . FastqIO import FastqWriter \n 
from bx . intervals . cluster import ClusterTree \n 
if is_fq : \n 
~~~ fd = LazyFastqReader ( fa_fq_filename ) \n 
fout = FastqWriter ( output_filename ) \n 
~~~ fd = LazyFastaReader ( fa_fq_filename ) \n 
fout = FastaWriter ( output_filename ) \n 
~~ rep_info = { } \n 
id_to_rep = { } \n 
for line in open ( group_filename ) : \n 
~~~ pb_id , members = line . strip ( ) . split ( ) \n 
best_id = None \n 
best_seq = None \n 
best_qual = None \n 
best_err = 9999999 \n 
err = 9999999 \n 
max_len = 0 \n 
for x in members . split ( ) : \n 
~~~ if is_fq and pick_least_err_instead : \n 
~~~ err = sum ( i ** - ( i / 10. ) for i in fd [ x ] . quality ) \n 
~~ if ( is_fq and pick_least_err_instead and err < best_err ) or ( ( not is_fq or not pick_least_err_instead ~~~ best_id = x \n 
best_seq = fd [ x ] . sequence \n 
~~~ best_qual = fd [ x ] . quality \n 
best_err = err \n 
~~ max_len = len ( fd [ x ] . sequence ) \n 
~~ ~~ rep_info [ pb_id ] = ( best_id , best_seq , best_qual ) \n 
id_to_rep [ best_id ] = pb_id \n 
~~ f_gff = open ( gff_filename , ) \n 
coords = { } \n 
~~~ if r . qID in id_to_rep : \n 
~~~ pb_id = id_to_rep [ r . qID ] \n 
best_id , best_seq , best_qual = rep_info [ pb_id ] \n 
if r . qID not in coords : \n 
~~~ coords [ r . qID ] = "{0}:{1}-{2}({3})" . format ( r . sID , r . sStart , r . sEnd , r . flag . strand ) \n 
isoform_index = 1 \n 
record_storage [ pb_id ] = r \n 
~~~ coords [ r . qID ] += "+{0}:{1}-{2}({3})" . format ( r . sID , r . sStart , r . sEnd , r . flag . strand ) \n 
old_r = record_storage [ pb_id ] \n 
for pb_id in rep_info : \n 
~~~ best_id , best_seq , best_qual = rep_info [ pb_id ] \n 
_id_ = "{0}|{1}|{2}" . format ( pb_id , coords [ best_id ] , best_id ) \n 
_seq_ = best_seq \n 
~~~ fout . writeRecord ( _id_ , _seq_ , best_qual ) \n 
~~~ fout . writeRecord ( _id_ , _seq_ ) \n 
~~ ~~ ~~ def sep_by_strand ( records ) : \n 
~~~ output = { : [ ] , : [ ] } \n 
for r in records : \n 
~~~ output [ r . flag . strand ] . append ( r ) \n 
~~ return output \n 
~~ def is_fusion_compatible ( r1 , r2 , max_fusion_point_dist , max_exon_end_dist , allow_extra_5_exons ) : \n 
assert r1 . flag . strand == r2 . flag . strand \n 
if r1 . qStart <= .5 * r1 . qLen : \n 
~~~ if r2 . qStart > .5 * r2 . qLen : \n 
~~ in_5_portion = True \n 
~~~ if r2 . qStart <= .5 * r2 . qLen : \n 
~~ in_5_portion = False \n 
~~ plus_is_5end = ( r1 . flag . strand == ) \n 
type = compare_junctions . compare_junctions ( r1 , r2 ) \n 
if type == : \n 
~~~ if len ( r1 . segments ) == 1 : \n 
~~~ if len ( r2 . segments ) == 1 : \n 
~~~ if in_5_portion and plus_is_5end : dist = abs ( r1 . sStart - r2 . sStart ) \n 
else : dist = abs ( r1 . sEnd - r2 . sEnd ) \n 
return dist <= max_fusion_point_dist \n 
~~ ~~ elif type == or type == : \n 
~~~ if allow_extra_5_exons : \n 
~~~ if in_5_portion and plus_is_5end : \n 
~~~ if abs ( r1 . segments [ - 1 ] . start - r2 . segments [ - 1 ] . start ) > max_exon_end_dist : return False if abs ( r1 . segments [ - 1 ] . end - r2 . segments [ - 1 ] . end ) > max_fusion_point_dist : return False return True \n 
~~ elif in_5_portion and ( not plus_is_5end ) : \n 
~~~ if abs ( r1 . segments [ 0 ] . end - r2 . segments [ 0 ] . end ) > max_exon_end_dist : return False \n 
if abs ( r1 . segments [ 0 ] . start - r2 . segments [ 0 ] . start ) > max_fusion_point_dist : return return True \n 
~~ ~~ def merge_fusion_exons ( records , max_fusion_point_dist , max_exon_end_dist , allow_extra_5_exons ) : \n 
output = [ [ records [ 0 ] ] ] \n 
for r1 in records [ 1 : ] : \n 
~~~ merged = False \n 
for i , r2s in enumerate ( output ) : \n 
~~~ if all ( is_fusion_compatible ( r1 , r2 , max_fusion_point_dist , max_exon_end_dist , allow_extra_5_exons ~~~ output [ i ] . append ( r1 ) \n 
merged = True \n 
~~ ~~ if not merged : \n 
~~~ output . append ( [ r1 ] ) \n 
~~ ~~ return output \n 
~~ def iter_gmap_sam_for_fusion ( gmap_sam_filename , fusion_candidates , transfrag_len_dict ) : \n 
records = [ ] \n 
iter = BioReaders . GMAPSAMReader ( gmap_sam_filename , True , query_len_dict = transfrag_len_dict ) \n 
for r in iter : \n 
~~~ if r . qID in fusion_candidates : \n 
~~~ records = [ r ] \n 
~~ ~~ for r in iter : \n 
~~~ if len ( records ) >= 1 and ( r . sID == records [ - 1 ] . sID and r . sStart < records [ - 1 ] . sStart ) : \n 
~~ if len ( records ) >= 1 and ( r . sID != records [ 0 ] . sID or r . sStart > records [ - 1 ] . sEnd ) : \n 
~~~ yield ( sep_by_strand ( records ) ) \n 
~~ if r . qID in fusion_candidates : \n 
~~~ records . append ( r ) \n 
~~ ~~ if len ( records ) > 0 : \n 
TmpRec = namedtuple ( , [ , , , , , , ] ) \n 
def total_coverage ( tmprecs ) : \n 
~~~ tree = ClusterTree ( 0 , 0 ) \n 
for r in tmprecs : tree . insert ( r . qStart , r . qEnd , - 1 ) \n 
return sum ( reg [ 1 ] - reg [ 0 ] for reg in tree . getregions ( ) ) \n 
~~ d = defaultdict ( lambda : [ ] ) \n 
reader = BioReaders . GMAPSAMReader ( sam_filename , True , query_len_dict = query_len_dict ) \n 
for r in reader : \n 
~~~ if r . sID == : continue \n 
if r . flag . strand == : \n 
~~~ d [ r . qID ] . append ( TmpRec ( qCov = r . qCoverage , qLen = r . qLen , qStart = r . qStart , qEnd = r . qEnd , sStart ~~ else : \n 
~~~ d [ r . qID ] . append ( TmpRec ( qCov = r . qCoverage , qLen = r . qLen , qStart = r . qLen - r . qEnd , qEnd = r . qLen - ~~ ~~ fusion_candidates = [ ] \n 
for k , data in d . iteritems ( ) : \n 
~~~ if len ( data ) > 1 and all ( a . iden >= .95 for a in data ) and all ( a . qCov >= min_locus_coverage for a in data ) and all ( a . qCov * a . qLen >= min_locus_coverage_bp for a in data ) and total_coverage ( data ) * 1. / data [ 0 ] . qLen >= min_total_coverage and all ( max ( a . sStart , b . sStart ) - min ( a . sEnd , b . sEnd ) >= min_dist_between_loci for a , b in itertools . combinations ( data , 2 ) ) : \n 
~~~ fusion_candidates . append ( k ) \n 
~~ ~~ return fusion_candidates \n 
compressed_records_pointer_dict = defaultdict ( lambda : [ ] ) \n 
merged_exons = [ ] \n 
merged_i = 0 \n 
check_ids_unique ( fa_or_fq_filename , is_fq = is_fq ) \n 
bs = branch_simple2 . BranchSimple ( fa_or_fq_filename , is_fq = is_fq ) \n 
fusion_candidates = find_fusion_candidates ( sam_filename , bs . transfrag_len_dict , min_locus_coverage \n 
for recs in iter_gmap_sam_for_fusion ( sam_filename , fusion_candidates , bs . transfrag_len_dict ) : \n 
~~~ for v in recs . itervalues ( ) : \n 
~~~ if len ( v ) > 0 : \n 
~~~ o = merge_fusion_exons ( v , max_fusion_point_dist = 100 , max_exon_end_dist = 0 , allow_extra_5_exons for group in o : \n 
~~~ merged_exons . append ( group ) \n 
for r in group : compressed_records_pointer_dict [ r . qID ] . append ( merged_i ) \n 
merged_i += 1 \n 
~~ ~~ ~~ ~~ f_group = open ( , ) \n 
gene_index = 1 \n 
already_seen = set ( ) \n 
for qid , indices in compressed_records_pointer_dict . iteritems ( ) : \n 
~~~ combo = tuple ( indices ) \n 
if combo in already_seen : \n 
#raw_input("") \n 
~~ already_seen . add ( combo ) \n 
for isoform_index , i in enumerate ( indices ) : \n 
records = merged_exons [ i ] \n 
~~ gene_index += 1 \n 
~~ f_group . close ( ) \n 
f_group = open ( output_prefix + , ) \n 
~~~ line = f . readline ( ) . strip ( ) \n 
if len ( line ) == 0 : break \n 
pbid1 , groups1 = line . strip ( ) . split ( ) \n 
pbid2 , groups2 = f . readline ( ) . strip ( ) . split ( ) \n 
assert pbid1 . split ( ) [ 1 ] == pbid2 . split ( ) [ 1 ] \n 
group = set ( groups1 . split ( ) ) . intersection ( groups2 . split ( ) ) \n 
f_group . write ( "{0}\\t{1}\\n" . format ( pbid1 [ : pbid1 . rfind ( ) ] , "," . join ( group ) ) ) \n 
group_info [ pbid1 [ : pbid1 . rfind ( ) ] ] = list ( group ) \n 
count += 1 \n 
~~ ~~ f_group . close ( ) \n 
gff_filename = output_prefix + \n 
group_filename = output_prefix + \n 
~~~ output_filename = output_prefix + \n 
~~ pick_rep ( fa_or_fq_filename , sam_filename , gff_filename , group_filename , output_filename , is_fq = is_fq \n 
if prefix_dict_pickle_filename is not None : \n 
~~~ with open ( prefix_dict_pickle_filename ) as f : \n 
~~~ d = load ( f ) \n 
d1 = d [ ] \n 
d1 . update ( d [ ] ) \n 
~~ tofu_wrap . get_abundance ( output_prefix , d1 , output_prefix ) \n 
~~~ from argparse import ArgumentParser \n 
parser = ArgumentParser ( ) \n 
fusion_main ( args . input , args . sam , args . prefix , \n 
is_fq = args . fq , allow_extra_5_exons = args . allow_extra_5exon , \n 
skip_5_exon_alt = False , prefix_dict_pickle_filename = args . prefix_dict_pickle_filename , min_locus_coverage = args . min_locus_coverage , min_locus_coverage_bp = args . min_locus_coverage_bp min_total_coverage = args . min_total_coverage , \n 
min_dist_between_loci = args . min_dist_between_loci ) \n 
import os , sys , subprocess \n 
from pbtools . pbtranscript . io . BLASRRecord import BLASRRecord \n 
from pbtools . pbtranscript . ice . IceUtils import HitItem , eval_blasr_alignment , alignment_has_large_nonmatch from pbtools . pbtranscript . ice . c_IceAlign import get_ece_arr_from_alignment \n 
class LAshowAlignReader : \n 
def __init__ ( self , las_out_filename ) : \n 
~~~ self . las_out_filename = las_out_filename \n 
self . f = open ( las_out_filename ) \n 
~~ def next ( self ) : \n 
raw = self . f . readline ( ) . strip ( ) . split ( ) \n 
~~~ raise StopIteration \n 
score = int ( raw [ 2 ] ) \n 
iden = float ( raw [ 3 ] ) \n 
qStrand = int ( raw [ 4 ] ) \n 
qEnd = int ( raw [ 6 ] ) \n 
qLen = int ( raw [ 7 ] ) \n 
sStrand = int ( raw [ 8 ] ) \n 
sStart = int ( raw [ 9 ] ) \n 
sEnd = int ( raw [ 10 ] ) \n 
sLen = int ( raw [ 11 ] ) \n 
_qStart , qAln = self . f . readline ( ) . strip ( ) . split ( ) \n 
assert ( qStrand == 0 and int ( _qStart ) - 1 == qStart ) or ( qStrand == 1 and int ( _qStart ) - 1 == qLen alnStr = self . f . readline ( ) . strip ( ) \n 
_sStart , sAln = self . f . readline ( ) . strip ( ) . split ( ) [ : 2 ] \n 
assert ( sStrand == 0 and int ( _sStart ) - 1 == sStart ) or ( sStrand == 1 and int ( _sStart ) - 1 == sLen return BLASRRecord ( qID , qLen , qStart , qEnd , qStrand , sID , sLen , sStart , sEnd , sStrand , score qAln = qAln , alnStr = alnStr , sAln = sAln , identity = iden , strand = if qStrand == sStrand \n 
~~ ~~ def dalign_against_ref ( dazz_query_obj , dazz_db_obj , las_out_filename , is_FL , sID_starts_with_c , \n 
qver_get_func , qvmean_get_func , qv_prob_threshold = .03 , \n 
ece_penalty = 1 , ece_min_len = 20 , same_strand_only = True , no_qv_or_aln_checking = False max_missed_start = 200 , max_missed_end = 50 ) : \n 
for r in LAshowAlignReader ( las_out_filename ) : \n 
~~~ missed_q = r . qStart + r . qLength - r . qEnd \n 
missed_t = r . sStart + r . sLength - r . sEnd \n 
r . qID = dazz_query_obj [ r . qID ] \n 
r . sID = dazz_db_obj [ r . sID ] \n 
if sID_starts_with_c : \n 
~~~ assert r . sID . startswith ( ) \n 
if r . sID . find ( ) > 0 : \n 
~~~ r . sID = r . sID . split ( ) [ 0 ] \n 
~~ if r . sID . endswith ( ) : \n 
~~~ cID = int ( r . sID [ 1 : - 4 ] ) \n 
~~~ cID = int ( r . sID [ 1 : ] ) \n 
~~~ cID = r . sID \n 
~~ if ( cID == r . qID or \n 
( r . strand == and same_strand_only ) ) : \n 
~~~ yield HitItem ( qID = r . qID , cID = cID ) \n 
~~ if no_qv_or_aln_checking : \n 
~~~ yield HitItem ( qID = r . qID , cID = cID , \n 
qStart = r . qStart , qEnd = r . qEnd , \n 
missed_q = missed_q * 1. / r . qLength , \n 
missed_t = missed_t * 1. / r . sLength , \n 
fakecigar = 1 , \n 
ece_arr = 1 ) \n 
~~ if ( is_FL and ( r . sStart > max_missed_start or r . qStart > max_missed_start or \n 
( r . sLength - r . sEnd > max_missed_end ) or \n 
( r . qLength - r . qEnd > max_missed_end ) ) ) : \n 
~~~ cigar_str , ece_arr = eval_blasr_alignment ( \n 
record = r , \n 
qver_get_func = qver_get_func , \n 
sID_starts_with_c = sID_starts_with_c , \n 
qv_prob_threshold = qv_prob_threshold , \n 
qvmean_get_func = qvmean_get_func ) \n 
if alignment_has_large_nonmatch ( ece_arr , \n 
ece_penalty , ece_min_len ) : \n 
fakecigar = cigar_str , \n 
ece_arr = ece_arr ) \n 
class TestInitICE ( unittest . TestCase ) : \n 
self . rootDir = op . dirname ( op . dirname ( op . abspath ( __file__ ) ) ) \n 
self . testDir = op . join ( self . rootDir , "" ) \n 
import copy as _copy \n 
import os as _os \n 
import re as _re \n 
import textwrap as _textwrap \n 
from gettext import gettext as _ \n 
~~~ set \n 
~~~ from sets import Set as set \n 
~~~ basestring \n 
~~~ sorted \n 
~~~ def sorted ( iterable , reverse = False ) : \n 
~~~ result = list ( iterable ) \n 
result . sort ( ) \n 
if reverse : \n 
~~~ result . reverse ( ) \n 
~~ ~~ def _callable ( obj ) : \n 
~~~ return hasattr ( obj , ) or hasattr ( obj , ) \n 
~~ SUPPRESS = \n 
OPTIONAL = \n 
ZERO_OR_MORE = \n 
ONE_OR_MORE = \n 
PARSER = \n 
REMAINDER = \n 
_UNRECOGNIZED_ARGS_ATTR = \n 
class _AttributeHolder ( object ) : \n 
~~~ type_name = type ( self ) . __name__ \n 
arg_strings = [ ] \n 
for arg in self . _get_args ( ) : \n 
~~~ arg_strings . append ( repr ( arg ) ) \n 
~~ for name , value in self . _get_kwargs ( ) : \n 
~~~ arg_strings . append ( % ( name , value ) ) \n 
~~ return % ( type_name , . join ( arg_strings ) ) \n 
~~ def _get_kwargs ( self ) : \n 
~~~ return sorted ( self . __dict__ . items ( ) ) \n 
~~ def _get_args ( self ) : \n 
~~ ~~ def _ensure_value ( namespace , name , value ) : \n 
~~~ if getattr ( namespace , name , None ) is None : \n 
~~~ setattr ( namespace , name , value ) \n 
~~ return getattr ( namespace , name ) \n 
~~ class HelpFormatter ( object ) : \n 
prog , \n 
indent_increment = 2 , \n 
max_help_position = 24 , \n 
width = None ) : \n 
~~~ if width is None : \n 
~~~ width = int ( _os . environ [ ] ) \n 
~~ except ( KeyError , ValueError ) : \n 
~~~ width = 80 \n 
~~ width -= 2 \n 
~~ self . _prog = prog \n 
self . _indent_increment = indent_increment \n 
self . _max_help_position = max_help_position \n 
self . _width = width \n 
self . _current_indent = 0 \n 
self . _level = 0 \n 
self . _action_max_length = 0 \n 
self . _root_section = self . _Section ( self , None ) \n 
self . _current_section = self . _root_section \n 
self . _whitespace_matcher = _re . compile ( ) \n 
self . _long_break_matcher = _re . compile ( ) \n 
~~ def _indent ( self ) : \n 
~~~ self . _current_indent += self . _indent_increment \n 
self . _level += 1 \n 
~~ def _dedent ( self ) : \n 
~~~ self . _current_indent -= self . _indent_increment \n 
assert self . _current_indent >= 0 , \n 
self . _level -= 1 \n 
~~ class _Section ( object ) : \n 
~~~ def __init__ ( self , formatter , parent , heading = None ) : \n 
~~~ self . formatter = formatter \n 
self . parent = parent \n 
self . heading = heading \n 
~~ def format_help ( self ) : \n 
~~~ if self . parent is not None : \n 
~~~ self . formatter . _indent ( ) \n 
~~ join = self . formatter . _join_parts \n 
for func , args in self . items : \n 
~~~ func ( * args ) \n 
~~ item_help = join ( [ func ( * args ) for func , args in self . items ] ) \n 
if self . parent is not None : \n 
~~~ self . formatter . _dedent ( ) \n 
~~ if not item_help : \n 
~~ if self . heading is not SUPPRESS and self . heading is not None : \n 
~~~ current_indent = self . formatter . _current_indent \n 
heading = % ( current_indent , , self . heading ) \n 
~~~ heading = \n 
~~ return join ( [ , heading , item_help , ] ) \n 
~~ ~~ def _add_item ( self , func , args ) : \n 
~~~ self . _current_section . items . append ( ( func , args ) ) \n 
~~ def start_section ( self , heading ) : \n 
~~~ self . _indent ( ) \n 
section = self . _Section ( self , self . _current_section , heading ) \n 
self . _add_item ( section . format_help , [ ] ) \n 
self . _current_section = section \n 
~~ def end_section ( self ) : \n 
~~~ self . _current_section = self . _current_section . parent \n 
self . _dedent ( ) \n 
~~ def add_text ( self , text ) : \n 
~~~ if text is not SUPPRESS and text is not None : \n 
~~~ self . _add_item ( self . _format_text , [ text ] ) \n 
~~ ~~ def add_usage ( self , usage , actions , groups , prefix = None ) : \n 
~~~ if usage is not SUPPRESS : \n 
~~~ args = usage , actions , groups , prefix \n 
self . _add_item ( self . _format_usage , args ) \n 
~~ ~~ def add_argument ( self , action ) : \n 
~~~ if action . help is not SUPPRESS : \n 
~~~ get_invocation = self . _format_action_invocation \n 
invocations = [ get_invocation ( action ) ] \n 
for subaction in self . _iter_indented_subactions ( action ) : \n 
~~~ invocations . append ( get_invocation ( subaction ) ) \n 
~~ invocation_length = max ( [ len ( s ) for s in invocations ] ) \n 
action_length = invocation_length + self . _current_indent \n 
self . _action_max_length = max ( self . _action_max_length , \n 
action_length ) \n 
self . _add_item ( self . _format_action , [ action ] ) \n 
~~ ~~ def add_arguments ( self , actions ) : \n 
~~~ for action in actions : \n 
~~~ self . add_argument ( action ) \n 
~~ ~~ def format_help ( self ) : \n 
~~~ help = self . _root_section . format_help ( ) \n 
if help : \n 
~~~ help = self . _long_break_matcher . sub ( , help ) \n 
help = help . strip ( ) + \n 
~~ return help \n 
~~ def _join_parts ( self , part_strings ) : \n 
~~~ return . join ( [ part \n 
for part in part_strings \n 
if part and part is not SUPPRESS ] ) \n 
~~ def _format_usage ( self , usage , actions , groups , prefix ) : \n 
~~~ if prefix is None : \n 
~~~ prefix = _ ( ) \n 
~~ if usage is not None : \n 
~~~ usage = usage % dict ( prog = self . _prog ) \n 
~~ elif usage is None and not actions : \n 
~~~ usage = % dict ( prog = self . _prog ) \n 
~~ elif usage is None : \n 
~~~ prog = % dict ( prog = self . _prog ) \n 
optionals = [ ] \n 
positionals = [ ] \n 
for action in actions : \n 
~~~ if action . option_strings : \n 
~~~ optionals . append ( action ) \n 
~~~ positionals . append ( action ) \n 
~~ ~~ format = self . _format_actions_usage \n 
action_usage = format ( optionals + positionals , groups ) \n 
usage = . join ( [ s for s in [ prog , action_usage ] if s ] ) \n 
text_width = self . _width - self . _current_indent \n 
if len ( prefix ) + len ( usage ) > text_width : \n 
~~~ part_regexp = \n 
opt_usage = format ( optionals , groups ) \n 
pos_usage = format ( positionals , groups ) \n 
opt_parts = _re . findall ( part_regexp , opt_usage ) \n 
pos_parts = _re . findall ( part_regexp , pos_usage ) \n 
assert . join ( opt_parts ) == opt_usage \n 
assert . join ( pos_parts ) == pos_usage \n 
def get_lines ( parts , indent , prefix = None ) : \n 
~~~ lines = [ ] \n 
line = [ ] \n 
if prefix is not None : \n 
~~~ line_len = len ( prefix ) - 1 \n 
~~~ line_len = len ( indent ) - 1 \n 
~~ for part in parts : \n 
~~~ if line_len + 1 + len ( part ) > text_width : \n 
~~~ lines . append ( indent + . join ( line ) ) \n 
line_len = len ( indent ) - 1 \n 
~~ line . append ( part ) \n 
line_len += len ( part ) + 1 \n 
~~ if line : \n 
~~ if prefix is not None : \n 
~~~ lines [ 0 ] = lines [ 0 ] [ len ( indent ) : ] \n 
~~ return lines \n 
~~ if len ( prefix ) + len ( prog ) <= 0.75 * text_width : \n 
~~~ indent = * ( len ( prefix ) + len ( prog ) + 1 ) \n 
if opt_parts : \n 
~~~ lines = get_lines ( [ prog ] + opt_parts , indent , prefix ) \n 
lines . extend ( get_lines ( pos_parts , indent ) ) \n 
~~ elif pos_parts : \n 
~~~ lines = get_lines ( [ prog ] + pos_parts , indent , prefix ) \n 
~~~ lines = [ prog ] \n 
~~~ indent = * len ( prefix ) \n 
parts = opt_parts + pos_parts \n 
lines = get_lines ( parts , indent ) \n 
if len ( lines ) > 1 : \n 
lines . extend ( get_lines ( opt_parts , indent ) ) \n 
~~ lines = [ prog ] + lines \n 
~~ usage = . join ( lines ) \n 
~~ ~~ return % ( prefix , usage ) \n 
~~ def _format_actions_usage ( self , actions , groups ) : \n 
~~~ group_actions = set ( ) \n 
inserts = { } \n 
for group in groups : \n 
~~~ start = actions . index ( group . _group_actions [ 0 ] ) \n 
~~~ end = start + len ( group . _group_actions ) \n 
if actions [ start : end ] == group . _group_actions : \n 
~~~ for action in group . _group_actions : \n 
~~~ group_actions . add ( action ) \n 
~~ if not group . required : \n 
~~~ if start in inserts : \n 
~~~ inserts [ start ] += \n 
~~~ inserts [ start ] = \n 
~~ inserts [ end ] = \n 
~~ for i in range ( start + 1 , end ) : \n 
~~~ inserts [ i ] = \n 
~~ ~~ ~~ ~~ parts = [ ] \n 
for i , action in enumerate ( actions ) : \n 
~~~ if action . help is SUPPRESS : \n 
~~~ parts . append ( None ) \n 
if inserts . get ( i ) == : \n 
~~~ inserts . pop ( i ) \n 
~~ elif inserts . get ( i + 1 ) == : \n 
~~~ inserts . pop ( i + 1 ) \n 
~~ ~~ elif not action . option_strings : \n 
~~~ part = self . _format_args ( action , action . dest ) \n 
if action in group_actions : \n 
~~~ if part [ 0 ] == and part [ - 1 ] == : \n 
~~~ part = part [ 1 : - 1 ] \n 
~~ ~~ parts . append ( part ) \n 
~~~ option_string = action . option_strings [ 0 ] \n 
if action . nargs == 0 : \n 
~~~ part = % option_string \n 
~~~ default = action . dest . upper ( ) \n 
args_string = self . _format_args ( action , default ) \n 
part = % ( option_string , args_string ) \n 
~~ if not action . required and action not in group_actions : \n 
~~~ part = % part \n 
~~ parts . append ( part ) \n 
~~ ~~ for i in sorted ( inserts , reverse = True ) : \n 
~~~ parts [ i : i ] = [ inserts [ i ] ] \n 
~~ text = . join ( [ item for item in parts if item is not None ] ) \n 
open = \n 
close = \n 
text = _re . sub ( % open , , text ) \n 
text = _re . sub ( % close , , text ) \n 
text = _re . sub ( % ( open , close ) , , text ) \n 
text = _re . sub ( , , text ) \n 
text = text . strip ( ) \n 
return text \n 
~~ def _format_text ( self , text ) : \n 
~~~ if in text : \n 
~~~ text = text % dict ( prog = self . _prog ) \n 
~~ text_width = self . _width - self . _current_indent \n 
indent = * self . _current_indent \n 
return self . _fill_text ( text , text_width , indent ) + \n 
~~ def _format_action ( self , action ) : \n 
~~~ help_position = min ( self . _action_max_length + 2 , \n 
self . _max_help_position ) \n 
help_width = self . _width - help_position \n 
action_width = help_position - self . _current_indent - 2 \n 
action_header = self . _format_action_invocation ( action ) \n 
if not action . help : \n 
~~~ tup = self . _current_indent , , action_header \n 
action_header = % tup \n 
~~ elif len ( action_header ) <= action_width : \n 
~~~ tup = self . _current_indent , , action_width , action_header \n 
indent_first = 0 \n 
indent_first = help_position \n 
~~ parts = [ action_header ] \n 
if action . help : \n 
~~~ help_text = self . _expand_help ( action ) \n 
help_lines = self . _split_lines ( help_text , help_width ) \n 
parts . append ( % ( indent_first , , help_lines [ 0 ] ) ) \n 
for line in help_lines [ 1 : ] : \n 
~~~ parts . append ( % ( help_position , , line ) ) \n 
~~ ~~ elif not action_header . endswith ( ) : \n 
~~~ parts . append ( ) \n 
~~ for subaction in self . _iter_indented_subactions ( action ) : \n 
~~~ parts . append ( self . _format_action ( subaction ) ) \n 
~~ return self . _join_parts ( parts ) \n 
~~ def _format_action_invocation ( self , action ) : \n 
~~~ if not action . option_strings : \n 
~~~ metavar , = self . _metavar_formatter ( action , action . dest ) ( 1 ) \n 
return metavar \n 
~~~ parts = [ ] \n 
~~~ parts . extend ( action . option_strings ) \n 
for option_string in action . option_strings : \n 
~~~ parts . append ( % ( option_string , args_string ) ) \n 
~~ ~~ return . join ( parts ) \n 
~~ ~~ def _metavar_formatter ( self , action , default_metavar ) : \n 
~~~ if action . metavar is not None : \n 
~~~ result = action . metavar \n 
~~ elif action . choices is not None : \n 
~~~ choice_strs = [ str ( choice ) for choice in action . choices ] \n 
result = % . join ( choice_strs ) \n 
~~~ result = default_metavar \n 
~~ def format ( tuple_size ) : \n 
~~~ if isinstance ( result , tuple ) : \n 
~~~ return ( result , ) * tuple_size \n 
~~ ~~ return format \n 
~~ def _format_args ( self , action , default_metavar ) : \n 
~~~ get_metavar = self . _metavar_formatter ( action , default_metavar ) \n 
if action . nargs is None : \n 
~~~ result = % get_metavar ( 1 ) \n 
~~ elif action . nargs == OPTIONAL : \n 
~~ elif action . nargs == ZERO_OR_MORE : \n 
~~~ result = % get_metavar ( 2 ) \n 
~~ elif action . nargs == ONE_OR_MORE : \n 
~~ elif action . nargs == REMAINDER : \n 
~~~ result = \n 
~~ elif action . nargs == PARSER : \n 
~~~ formats = [ for _ in range ( action . nargs ) ] \n 
result = . join ( formats ) % get_metavar ( action . nargs ) \n 
~~ def _expand_help ( self , action ) : \n 
~~~ params = dict ( vars ( action ) , prog = self . _prog ) \n 
for name in list ( params ) : \n 
~~~ if params [ name ] is SUPPRESS : \n 
~~~ del params [ name ] \n 
~~ ~~ for name in list ( params ) : \n 
~~~ if hasattr ( params [ name ] , ) : \n 
~~~ params [ name ] = params [ name ] . __name__ \n 
~~ ~~ if params . get ( ) is not None : \n 
~~~ choices_str = . join ( [ str ( c ) for c in params [ ] ] ) \n 
params [ ] = choices_str \n 
~~ return self . _get_help_string ( action ) % params \n 
~~ def _iter_indented_subactions ( self , action ) : \n 
~~~ get_subactions = action . _get_subactions \n 
for subaction in get_subactions ( ) : \n 
~~~ yield subaction \n 
~~ self . _dedent ( ) \n 
~~ ~~ def _split_lines ( self , text , width ) : \n 
~~~ text = self . _whitespace_matcher . sub ( , text ) . strip ( ) \n 
return _textwrap . wrap ( text , width ) \n 
~~ def _fill_text ( self , text , width , indent ) : \n 
return _textwrap . fill ( text , width , initial_indent = indent , \n 
subsequent_indent = indent ) \n 
~~ def _get_help_string ( self , action ) : \n 
~~~ return action . help \n 
~~ ~~ class RawDescriptionHelpFormatter ( HelpFormatter ) : \n 
def _fill_text ( self , text , width , indent ) : \n 
~~~ return . join ( [ indent + line for line in text . splitlines ( True ) ] ) \n 
~~ ~~ class RawTextHelpFormatter ( RawDescriptionHelpFormatter ) : \n 
def _split_lines ( self , text , width ) : \n 
~~~ return text . splitlines ( ) \n 
~~ ~~ class ArgumentDefaultsHelpFormatter ( HelpFormatter ) : \n 
def _get_help_string ( self , action ) : \n 
~~~ help = action . help \n 
if not in action . help : \n 
~~~ if action . default is not SUPPRESS : \n 
~~~ defaulting_nargs = [ OPTIONAL , ZERO_OR_MORE ] \n 
if action . option_strings or action . nargs in defaulting_nargs : \n 
~~~ help += \n 
~~ ~~ ~~ return help \n 
~~ ~~ def _get_action_name ( argument ) : \n 
~~~ if argument is None : \n 
~~ elif argument . option_strings : \n 
~~~ return . join ( argument . option_strings ) \n 
~~ elif argument . metavar not in ( None , SUPPRESS ) : \n 
~~~ return argument . metavar \n 
~~ elif argument . dest not in ( None , SUPPRESS ) : \n 
~~~ return argument . dest \n 
~~ ~~ class ArgumentError ( Exception ) : \n 
def __init__ ( self , argument , message ) : \n 
~~~ self . argument_name = _get_action_name ( argument ) \n 
self . message = message \n 
~~~ if self . argument_name is None : \n 
~~~ format = \n 
~~ return format % dict ( message = self . message , \n 
argument_name = self . argument_name ) \n 
~~ ~~ class ArgumentTypeError ( Exception ) : \n 
~~ class Action ( _AttributeHolder ) : \n 
option_strings , \n 
dest , \n 
nargs = None , \n 
const = None , \n 
default = None , \n 
type = None , \n 
choices = None , \n 
help = None , \n 
metavar = None ) : \n 
~~~ self . option_strings = option_strings \n 
self . dest = dest \n 
self . nargs = nargs \n 
self . const = const \n 
self . type = type \n 
self . choices = choices \n 
self . required = required \n 
self . help = help \n 
self . metavar = metavar \n 
~~~ names = [ \n 
return [ ( name , getattr ( self , name ) ) for name in names ] \n 
~~ def __call__ ( self , parser , namespace , values , option_string = None ) : \n 
~~~ raise NotImplementedError ( _ ( ) ) \n 
~~ ~~ class _StoreAction ( Action ) : \n 
~~~ def __init__ ( self , \n 
~~~ if nargs == 0 : \n 
~~ if const is not None and nargs != OPTIONAL : \n 
~~~ raise ValueError ( % OPTIONAL ) \n 
~~ super ( _StoreAction , self ) . __init__ ( \n 
option_strings = option_strings , \n 
dest = dest , \n 
nargs = nargs , \n 
const = const , \n 
default = default , \n 
type = type , \n 
choices = choices , \n 
required = required , \n 
help = help , \n 
metavar = metavar ) \n 
~~~ setattr ( namespace , self . dest , values ) \n 
~~ ~~ class _StoreConstAction ( Action ) : \n 
const , \n 
~~~ super ( _StoreConstAction , self ) . __init__ ( \n 
nargs = 0 , \n 
help = help ) \n 
~~~ setattr ( namespace , self . dest , self . const ) \n 
~~ ~~ class _StoreTrueAction ( _StoreConstAction ) : \n 
help = None ) : \n 
~~~ super ( _StoreTrueAction , self ) . __init__ ( \n 
const = True , \n 
~~ ~~ class _StoreFalseAction ( _StoreConstAction ) : \n 
default = True , \n 
~~~ super ( _StoreFalseAction , self ) . __init__ ( \n 
const = False , \n 
~~ ~~ class _AppendAction ( Action ) : \n 
~~ super ( _AppendAction , self ) . __init__ ( \n 
~~~ items = _copy . copy ( _ensure_value ( namespace , self . dest , [ ] ) ) \n 
items . append ( values ) \n 
setattr ( namespace , self . dest , items ) \n 
~~ ~~ class _AppendConstAction ( Action ) : \n 
~~~ super ( _AppendConstAction , self ) . __init__ ( \n 
items . append ( self . const ) \n 
~~ ~~ class _CountAction ( Action ) : \n 
~~~ super ( _CountAction , self ) . __init__ ( \n 
~~~ new_count = _ensure_value ( namespace , self . dest , 0 ) + 1 \n 
setattr ( namespace , self . dest , new_count ) \n 
~~ ~~ class _HelpAction ( Action ) : \n 
dest = SUPPRESS , \n 
default = SUPPRESS , \n 
~~~ super ( _HelpAction , self ) . __init__ ( \n 
~~~ parser . print_help ( ) \n 
parser . exit ( ) \n 
~~ ~~ class _VersionAction ( Action ) : \n 
version = None , \n 
~~~ super ( _VersionAction , self ) . __init__ ( \n 
self . version = version \n 
~~~ version = self . version \n 
if version is None : \n 
~~~ version = parser . version \n 
~~ formatter = parser . _get_formatter ( ) \n 
formatter . add_text ( version ) \n 
parser . exit ( message = formatter . format_help ( ) ) \n 
~~ ~~ class _SubParsersAction ( Action ) : \n 
~~~ class _ChoicesPseudoAction ( Action ) : \n 
~~~ def __init__ ( self , name , help ) : \n 
~~~ sup = super ( _SubParsersAction . _ChoicesPseudoAction , self ) \n 
sup . __init__ ( option_strings = [ ] , dest = name , help = help ) \n 
~~ ~~ def __init__ ( self , \n 
parser_class , \n 
~~~ self . _prog_prefix = prog \n 
self . _parser_class = parser_class \n 
self . _name_parser_map = { } \n 
self . _choices_actions = [ ] \n 
super ( _SubParsersAction , self ) . __init__ ( \n 
nargs = PARSER , \n 
choices = self . _name_parser_map , \n 
~~ def add_parser ( self , name , ** kwargs ) : \n 
~~~ if kwargs . get ( ) is None : \n 
~~~ kwargs [ ] = % ( self . _prog_prefix , name ) \n 
~~ if in kwargs : \n 
~~~ help = kwargs . pop ( ) \n 
choice_action = self . _ChoicesPseudoAction ( name , help ) \n 
self . _choices_actions . append ( choice_action ) \n 
~~ parser = self . _parser_class ( ** kwargs ) \n 
self . _name_parser_map [ name ] = parser \n 
return parser \n 
~~ def _get_subactions ( self ) : \n 
~~~ return self . _choices_actions \n 
~~~ parser_name = values [ 0 ] \n 
arg_strings = values [ 1 : ] \n 
if self . dest is not SUPPRESS : \n 
~~~ setattr ( namespace , self . dest , parser_name ) \n 
~~~ parser = self . _name_parser_map [ parser_name ] \n 
~~~ tup = parser_name , . join ( self . _name_parser_map ) \n 
msg = _ ( % tup ) \n 
raise ArgumentError ( self , msg ) \n 
~~ namespace , arg_strings = parser . parse_known_args ( arg_strings , namespace ) \n 
if arg_strings : \n 
~~~ vars ( namespace ) . setdefault ( _UNRECOGNIZED_ARGS_ATTR , [ ] ) \n 
getattr ( namespace , _UNRECOGNIZED_ARGS_ATTR ) . extend ( arg_strings ) \n 
~~ ~~ ~~ class FileType ( object ) : \n 
def __init__ ( self , mode = , bufsize = None ) : \n 
~~~ self . _mode = mode \n 
self . _bufsize = bufsize \n 
~~ def __call__ ( self , string ) : \n 
~~~ if string == : \n 
~~~ if in self . _mode : \n 
~~~ return _sys . stdin \n 
~~ elif in self . _mode : \n 
~~~ return _sys . stdout \n 
~~ ~~ if self . _bufsize : \n 
~~~ return open ( string , self . _mode , self . _bufsize ) \n 
~~~ return open ( string , self . _mode ) \n 
~~~ args = [ self . _mode , self . _bufsize ] \n 
args_str = . join ( [ repr ( arg ) for arg in args if arg is not None ] ) \n 
return % ( type ( self ) . __name__ , args_str ) \n 
~~ ~~ class Namespace ( _AttributeHolder ) : \n 
~~~ for name in kwargs : \n 
~~~ setattr ( self , name , kwargs [ name ] ) \n 
~~ ~~ __hash__ = None \n 
def __eq__ ( self , other ) : \n 
~~~ return vars ( self ) == vars ( other ) \n 
~~~ return not ( self == other ) \n 
~~~ return key in self . __dict__ \n 
~~ ~~ class _ActionsContainer ( object ) : \n 
prefix_chars , \n 
argument_default , \n 
conflict_handler ) : \n 
~~~ super ( _ActionsContainer , self ) . __init__ ( ) \n 
self . description = description \n 
self . argument_default = argument_default \n 
self . prefix_chars = prefix_chars \n 
self . conflict_handler = conflict_handler \n 
self . _registries = { } \n 
self . register ( , None , _StoreAction ) \n 
self . register ( , , _StoreAction ) \n 
self . register ( , , _StoreConstAction ) \n 
self . register ( , , _StoreTrueAction ) \n 
self . register ( , , _StoreFalseAction ) \n 
self . register ( , , _AppendAction ) \n 
self . register ( , , _AppendConstAction ) \n 
self . register ( , , _CountAction ) \n 
self . register ( , , _HelpAction ) \n 
self . register ( , , _VersionAction ) \n 
self . register ( , , _SubParsersAction ) \n 
self . _get_handler ( ) \n 
self . _actions = [ ] \n 
self . _option_string_actions = { } \n 
self . _action_groups = [ ] \n 
self . _mutually_exclusive_groups = [ ] \n 
self . _defaults = { } \n 
self . _negative_number_matcher = _re . compile ( ) \n 
self . _has_negative_number_optionals = [ ] \n 
~~ def register ( self , registry_name , value , object ) : \n 
~~~ registry = self . _registries . setdefault ( registry_name , { } ) \n 
registry [ value ] = object \n 
~~ def _registry_get ( self , registry_name , value , default = None ) : \n 
~~~ return self . _registries [ registry_name ] . get ( value , default ) \n 
~~ def set_defaults ( self , ** kwargs ) : \n 
~~~ self . _defaults . update ( kwargs ) \n 
for action in self . _actions : \n 
~~~ if action . dest in kwargs : \n 
~~~ action . default = kwargs [ action . dest ] \n 
~~ ~~ ~~ def get_default ( self , dest ) : \n 
~~~ for action in self . _actions : \n 
~~~ if action . dest == dest and action . default is not None : \n 
~~~ return action . default \n 
~~ ~~ return self . _defaults . get ( dest , None ) \n 
~~ def add_argument ( self , * args , ** kwargs ) : \n 
chars = self . prefix_chars \n 
if not args or len ( args ) == 1 and args [ 0 ] [ 0 ] not in chars : \n 
~~~ if args and in kwargs : \n 
~~ kwargs = self . _get_positional_kwargs ( * args , ** kwargs ) \n 
~~~ kwargs = self . _get_optional_kwargs ( * args , ** kwargs ) \n 
~~ if not in kwargs : \n 
~~~ dest = kwargs [ ] \n 
if dest in self . _defaults : \n 
~~~ kwargs [ ] = self . _defaults [ dest ] \n 
~~ elif self . argument_default is not None : \n 
~~~ kwargs [ ] = self . argument_default \n 
~~ ~~ action_class = self . _pop_action_class ( kwargs ) \n 
if not _callable ( action_class ) : \n 
~~ action = action_class ( ** kwargs ) \n 
type_func = self . _registry_get ( , action . type , action . type ) \n 
if not _callable ( type_func ) : \n 
~~~ raise ValueError ( % type_func ) \n 
~~ return self . _add_action ( action ) \n 
~~ def add_argument_group ( self , * args , ** kwargs ) : \n 
~~~ group = _ArgumentGroup ( self , * args , ** kwargs ) \n 
self . _action_groups . append ( group ) \n 
return group \n 
~~ def add_mutually_exclusive_group ( self , ** kwargs ) : \n 
~~~ group = _MutuallyExclusiveGroup ( self , ** kwargs ) \n 
self . _mutually_exclusive_groups . append ( group ) \n 
~~ def _add_action ( self , action ) : \n 
~~~ self . _check_conflict ( action ) \n 
self . _actions . append ( action ) \n 
action . container = self \n 
~~~ self . _option_string_actions [ option_string ] = action \n 
~~ for option_string in action . option_strings : \n 
~~~ if self . _negative_number_matcher . match ( option_string ) : \n 
~~~ if not self . _has_negative_number_optionals : \n 
~~~ self . _has_negative_number_optionals . append ( True ) \n 
~~ ~~ ~~ return action \n 
~~ def _remove_action ( self , action ) : \n 
~~~ self . _actions . remove ( action ) \n 
~~ def _add_container_actions ( self , container ) : \n 
~~~ title_group_map = { } \n 
for group in self . _action_groups : \n 
~~~ if group . title in title_group_map : \n 
~~~ msg = _ ( ) \n 
raise ValueError ( msg % ( group . title ) ) \n 
~~ title_group_map [ group . title ] = group \n 
~~ group_map = { } \n 
for group in container . _action_groups : \n 
~~~ if group . title not in title_group_map : \n 
~~~ title_group_map [ group . title ] = self . add_argument_group ( \n 
title = group . title , \n 
description = group . description , \n 
conflict_handler = group . conflict_handler ) \n 
~~ for action in group . _group_actions : \n 
~~~ group_map [ action ] = title_group_map [ group . title ] \n 
~~ ~~ for group in container . _mutually_exclusive_groups : \n 
~~~ mutex_group = self . add_mutually_exclusive_group ( \n 
required = group . required ) \n 
for action in group . _group_actions : \n 
~~~ group_map [ action ] = mutex_group \n 
~~ ~~ for action in container . _actions : \n 
~~~ group_map . get ( action , self ) . _add_action ( action ) \n 
~~ ~~ def _get_positional_kwargs ( self , dest , ** kwargs ) : \n 
raise TypeError ( msg ) \n 
~~ if kwargs . get ( ) not in [ OPTIONAL , ZERO_OR_MORE ] : \n 
~~~ kwargs [ ] = True \n 
~~ if kwargs . get ( ) == ZERO_OR_MORE and not in kwargs : \n 
~~ return dict ( kwargs , dest = dest , option_strings = [ ] ) \n 
~~ def _get_optional_kwargs ( self , * args , ** kwargs ) : \n 
~~~ option_strings = [ ] \n 
long_option_strings = [ ] \n 
for option_string in args : \n 
~~~ if not option_string [ 0 ] in self . prefix_chars : \n 
~~~ msg = _ ( \n 
tup = option_string , self . prefix_chars \n 
raise ValueError ( msg % tup ) \n 
~~ option_strings . append ( option_string ) \n 
if option_string [ 0 ] in self . prefix_chars : \n 
~~~ if len ( option_string ) > 1 : \n 
~~~ if option_string [ 1 ] in self . prefix_chars : \n 
~~~ long_option_strings . append ( option_string ) \n 
~~ ~~ ~~ ~~ dest = kwargs . pop ( , None ) \n 
if dest is None : \n 
~~~ if long_option_strings : \n 
~~~ dest_option_string = long_option_strings [ 0 ] \n 
~~~ dest_option_string = option_strings [ 0 ] \n 
~~ dest = dest_option_string . lstrip ( self . prefix_chars ) \n 
if not dest : \n 
raise ValueError ( msg % option_string ) \n 
~~ dest = dest . replace ( , ) \n 
~~ return dict ( kwargs , dest = dest , option_strings = option_strings ) \n 
~~ def _pop_action_class ( self , kwargs , default = None ) : \n 
~~~ action = kwargs . pop ( , default ) \n 
return self . _registry_get ( , action , action ) \n 
~~ def _get_handler ( self ) : \n 
~~~ handler_func_name = % self . conflict_handler \n 
~~~ return getattr ( self , handler_func_name ) \n 
raise ValueError ( msg % self . conflict_handler ) \n 
~~ ~~ def _check_conflict ( self , action ) : \n 
~~~ confl_optionals = [ ] \n 
~~~ if option_string in self . _option_string_actions : \n 
~~~ confl_optional = self . _option_string_actions [ option_string ] \n 
confl_optionals . append ( ( option_string , confl_optional ) ) \n 
~~ ~~ if confl_optionals : \n 
~~~ conflict_handler = self . _get_handler ( ) \n 
conflict_handler ( action , confl_optionals ) \n 
~~ ~~ def _handle_conflict_error ( self , action , conflicting_actions ) : \n 
~~~ message = _ ( ) \n 
conflict_string = . join ( [ option_string \n 
for option_string , action \n 
in conflicting_actions ] ) \n 
raise ArgumentError ( action , message % conflict_string ) \n 
~~ def _handle_conflict_resolve ( self , action , conflicting_actions ) : \n 
~~~ for option_string , action in conflicting_actions : \n 
~~~ action . option_strings . remove ( option_string ) \n 
self . _option_string_actions . pop ( option_string , None ) \n 
if not action . option_strings : \n 
~~~ action . container . _remove_action ( action ) \n 
~~ ~~ ~~ ~~ class _ArgumentGroup ( _ActionsContainer ) : \n 
~~~ def __init__ ( self , container , title = None , description = None , ** kwargs ) : \n 
~~~ update = kwargs . setdefault \n 
update ( , container . conflict_handler ) \n 
update ( , container . prefix_chars ) \n 
update ( , container . argument_default ) \n 
super_init = super ( _ArgumentGroup , self ) . __init__ \n 
super_init ( description = description , ** kwargs ) \n 
self . _group_actions = [ ] \n 
self . _registries = container . _registries \n 
self . _actions = container . _actions \n 
self . _option_string_actions = container . _option_string_actions \n 
self . _defaults = container . _defaults \n 
self . _has_negative_number_optionals = container . _has_negative_number_optionals \n 
~~~ action = super ( _ArgumentGroup , self ) . _add_action ( action ) \n 
self . _group_actions . append ( action ) \n 
return action \n 
~~~ super ( _ArgumentGroup , self ) . _remove_action ( action ) \n 
self . _group_actions . remove ( action ) \n 
~~ ~~ class _MutuallyExclusiveGroup ( _ArgumentGroup ) : \n 
~~~ def __init__ ( self , container , required = False ) : \n 
~~~ super ( _MutuallyExclusiveGroup , self ) . __init__ ( container ) \n 
self . _container = container \n 
~~~ if action . required : \n 
~~ action = self . _container . _add_action ( action ) \n 
~~~ self . _container . _remove_action ( action ) \n 
~~ ~~ class ArgumentParser ( _AttributeHolder , _ActionsContainer ) : \n 
prog = None , \n 
usage = None , \n 
epilog = None , \n 
parents = [ ] , \n 
formatter_class = HelpFormatter , \n 
prefix_chars = , \n 
fromfile_prefix_chars = None , \n 
argument_default = None , \n 
conflict_handler = , \n 
add_help = True ) : \n 
~~~ if version is not None : \n 
~~~ import warnings \n 
warnings . warn ( \n 
"""instead""" , DeprecationWarning ) \n 
~~ superinit = super ( ArgumentParser , self ) . __init__ \n 
superinit ( description = description , \n 
prefix_chars = prefix_chars , \n 
argument_default = argument_default , \n 
conflict_handler = conflict_handler ) \n 
if prog is None : \n 
~~~ prog = _os . path . basename ( _sys . argv [ 0 ] ) \n 
~~ self . prog = prog \n 
self . usage = usage \n 
self . epilog = epilog \n 
self . formatter_class = formatter_class \n 
self . fromfile_prefix_chars = fromfile_prefix_chars \n 
self . add_help = add_help \n 
add_group = self . add_argument_group \n 
self . _positionals = add_group ( _ ( ) ) \n 
self . _optionals = add_group ( _ ( ) ) \n 
self . _subparsers = None \n 
def identity ( string ) : \n 
~~~ return string \n 
~~ self . register ( , None , identity ) \n 
if in prefix_chars : \n 
~~~ default_prefix = \n 
~~~ default_prefix = prefix_chars [ 0 ] \n 
~~ if self . add_help : \n 
~~~ self . add_argument ( \n 
default_prefix + , default_prefix * 2 + , \n 
action = , default = SUPPRESS , \n 
help = _ ( ) ) \n 
~~ if self . version : \n 
version = self . version , \n 
~~ for parent in parents : \n 
~~~ self . _add_container_actions ( parent ) \n 
~~~ defaults = parent . _defaults \n 
~~~ self . _defaults . update ( defaults ) \n 
~~ ~~ ~~ def _get_kwargs ( self ) : \n 
~~ def add_subparsers ( self , ** kwargs ) : \n 
~~~ if self . _subparsers is not None : \n 
~~~ self . error ( _ ( ) ) \n 
~~ kwargs . setdefault ( , type ( self ) ) \n 
if in kwargs or in kwargs : \n 
~~~ title = _ ( kwargs . pop ( , ) ) \n 
description = _ ( kwargs . pop ( , None ) ) \n 
self . _subparsers = self . add_argument_group ( title , description ) \n 
~~~ self . _subparsers = self . _positionals \n 
~~ if kwargs . get ( ) is None : \n 
~~~ formatter = self . _get_formatter ( ) \n 
positionals = self . _get_positional_actions ( ) \n 
groups = self . _mutually_exclusive_groups \n 
formatter . add_usage ( self . usage , positionals , groups , ) \n 
kwargs [ ] = formatter . format_help ( ) . strip ( ) \n 
~~ parsers_class = self . _pop_action_class ( kwargs , ) \n 
action = parsers_class ( option_strings = [ ] , ** kwargs ) \n 
self . _subparsers . _add_action ( action ) \n 
~~~ self . _optionals . _add_action ( action ) \n 
~~~ self . _positionals . _add_action ( action ) \n 
~~ return action \n 
~~ def _get_optional_actions ( self ) : \n 
~~~ return [ action \n 
for action in self . _actions \n 
if action . option_strings ] \n 
~~ def _get_positional_actions ( self ) : \n 
if not action . option_strings ] \n 
~~ def parse_args ( self , args = None , namespace = None ) : \n 
~~~ args , argv = self . parse_known_args ( args , namespace ) \n 
if argv : \n 
self . error ( msg % . join ( argv ) ) \n 
~~ return args \n 
~~ def parse_known_args ( self , args = None , namespace = None ) : \n 
~~~ if args is None : \n 
~~~ args = _sys . argv [ 1 : ] \n 
~~ if namespace is None : \n 
~~~ namespace = Namespace ( ) \n 
~~ for action in self . _actions : \n 
~~~ if action . dest is not SUPPRESS : \n 
~~~ if not hasattr ( namespace , action . dest ) : \n 
~~~ default = action . default \n 
if isinstance ( action . default , basestring ) : \n 
~~~ default = self . _get_value ( action , default ) \n 
~~ setattr ( namespace , action . dest , default ) \n 
~~ ~~ ~~ ~~ for dest in self . _defaults : \n 
~~~ if not hasattr ( namespace , dest ) : \n 
~~~ setattr ( namespace , dest , self . _defaults [ dest ] ) \n 
~~~ namespace , args = self . _parse_known_args ( args , namespace ) \n 
if hasattr ( namespace , _UNRECOGNIZED_ARGS_ATTR ) : \n 
~~~ args . extend ( getattr ( namespace , _UNRECOGNIZED_ARGS_ATTR ) ) \n 
delattr ( namespace , _UNRECOGNIZED_ARGS_ATTR ) \n 
~~ return namespace , args \n 
~~ except ArgumentError : \n 
~~~ err = _sys . exc_info ( ) [ 1 ] \n 
self . error ( str ( err ) ) \n 
~~ ~~ def _parse_known_args ( self , arg_strings , namespace ) : \n 
~~~ if self . fromfile_prefix_chars is not None : \n 
~~~ arg_strings = self . _read_args_from_files ( arg_strings ) \n 
~~ action_conflicts = { } \n 
for mutex_group in self . _mutually_exclusive_groups : \n 
~~~ group_actions = mutex_group . _group_actions \n 
for i , mutex_action in enumerate ( mutex_group . _group_actions ) : \n 
~~~ conflicts = action_conflicts . setdefault ( mutex_action , [ ] ) \n 
conflicts . extend ( group_actions [ : i ] ) \n 
conflicts . extend ( group_actions [ i + 1 : ] ) \n 
~~ ~~ option_string_indices = { } \n 
arg_string_pattern_parts = [ ] \n 
arg_strings_iter = iter ( arg_strings ) \n 
for i , arg_string in enumerate ( arg_strings_iter ) : \n 
~~~ if arg_string == : \n 
~~~ arg_string_pattern_parts . append ( ) \n 
for arg_string in arg_strings_iter : \n 
~~~ option_tuple = self . _parse_optional ( arg_string ) \n 
if option_tuple is None : \n 
~~~ pattern = \n 
~~~ option_string_indices [ i ] = option_tuple \n 
pattern = \n 
~~ arg_string_pattern_parts . append ( pattern ) \n 
~~ ~~ arg_strings_pattern = . join ( arg_string_pattern_parts ) \n 
seen_actions = set ( ) \n 
seen_non_default_actions = set ( ) \n 
def take_action ( action , argument_strings , option_string = None ) : \n 
~~~ seen_actions . add ( action ) \n 
argument_values = self . _get_values ( action , argument_strings ) \n 
if argument_values is not action . default : \n 
~~~ seen_non_default_actions . add ( action ) \n 
for conflict_action in action_conflicts . get ( action , [ ] ) : \n 
~~~ if conflict_action in seen_non_default_actions : \n 
action_name = _get_action_name ( conflict_action ) \n 
raise ArgumentError ( action , msg % action_name ) \n 
~~ ~~ ~~ if argument_values is not SUPPRESS : \n 
~~~ action ( self , namespace , argument_values , option_string ) \n 
~~ ~~ def consume_optional ( start_index ) : \n 
~~~ option_tuple = option_string_indices [ start_index ] \n 
action , option_string , explicit_arg = option_tuple \n 
match_argument = self . _match_argument \n 
action_tuples = [ ] \n 
~~~ if action is None : \n 
~~~ extras . append ( arg_strings [ start_index ] ) \n 
return start_index + 1 \n 
~~ if explicit_arg is not None : \n 
~~~ arg_count = match_argument ( action , ) \n 
if arg_count == 0 and option_string [ 1 ] not in chars : \n 
~~~ action_tuples . append ( ( action , [ ] , option_string ) ) \n 
char = option_string [ 0 ] \n 
option_string = char + explicit_arg [ 0 ] \n 
new_explicit_arg = explicit_arg [ 1 : ] or None \n 
optionals_map = self . _option_string_actions \n 
if option_string in optionals_map : \n 
~~~ action = optionals_map [ option_string ] \n 
explicit_arg = new_explicit_arg \n 
raise ArgumentError ( action , msg % explicit_arg ) \n 
~~ ~~ elif arg_count == 1 : \n 
~~~ stop = start_index + 1 \n 
args = [ explicit_arg ] \n 
action_tuples . append ( ( action , args , option_string ) ) \n 
~~~ start = start_index + 1 \n 
selected_patterns = arg_strings_pattern [ start : ] \n 
arg_count = match_argument ( action , selected_patterns ) \n 
stop = start + arg_count \n 
args = arg_strings [ start : stop ] \n 
~~ ~~ assert action_tuples \n 
for action , args , option_string in action_tuples : \n 
~~~ take_action ( action , args , option_string ) \n 
~~ return stop \n 
~~ positionals = self . _get_positional_actions ( ) \n 
def consume_positionals ( start_index ) : \n 
~~~ match_partial = self . _match_arguments_partial \n 
selected_pattern = arg_strings_pattern [ start_index : ] \n 
arg_counts = match_partial ( positionals , selected_pattern ) \n 
for action , arg_count in zip ( positionals , arg_counts ) : \n 
~~~ args = arg_strings [ start_index : start_index + arg_count ] \n 
start_index += arg_count \n 
take_action ( action , args ) \n 
~~ positionals [ : ] = positionals [ len ( arg_counts ) : ] \n 
return start_index \n 
~~ extras = [ ] \n 
start_index = 0 \n 
if option_string_indices : \n 
~~~ max_option_string_index = max ( option_string_indices ) \n 
~~~ max_option_string_index = - 1 \n 
~~ while start_index <= max_option_string_index : \n 
~~~ next_option_string_index = min ( [ \n 
index \n 
for index in option_string_indices \n 
if index >= start_index ] ) \n 
if start_index != next_option_string_index : \n 
~~~ positionals_end_index = consume_positionals ( start_index ) \n 
if positionals_end_index > start_index : \n 
~~~ start_index = positionals_end_index \n 
~~ ~~ if start_index not in option_string_indices : \n 
~~~ strings = arg_strings [ start_index : next_option_string_index ] \n 
extras . extend ( strings ) \n 
start_index = next_option_string_index \n 
~~ start_index = consume_optional ( start_index ) \n 
~~ stop_index = consume_positionals ( start_index ) \n 
extras . extend ( arg_strings [ stop_index : ] ) \n 
if positionals : \n 
~~~ if action not in seen_actions : \n 
~~~ name = _get_action_name ( action ) \n 
self . error ( _ ( ) % name ) \n 
~~ ~~ ~~ for group in self . _mutually_exclusive_groups : \n 
~~~ if group . required : \n 
~~~ if action in seen_non_default_actions : \n 
~~~ names = [ _get_action_name ( action ) \n 
for action in group . _group_actions \n 
if action . help is not SUPPRESS ] \n 
msg = _ ( ) \n 
self . error ( msg % . join ( names ) ) \n 
~~ ~~ ~~ return namespace , extras \n 
~~ def _read_args_from_files ( self , arg_strings ) : \n 
~~~ new_arg_strings = [ ] \n 
for arg_string in arg_strings : \n 
~~~ if arg_string [ 0 ] not in self . fromfile_prefix_chars : \n 
~~~ new_arg_strings . append ( arg_string ) \n 
~~~ args_file = open ( arg_string [ 1 : ] ) \n 
~~~ arg_strings = [ ] \n 
for arg_line in args_file . read ( ) . splitlines ( ) : \n 
~~~ for arg in self . convert_arg_line_to_args ( arg_line ) : \n 
~~~ arg_strings . append ( arg ) \n 
~~ ~~ arg_strings = self . _read_args_from_files ( arg_strings ) \n 
new_arg_strings . extend ( arg_strings ) \n 
~~~ args_file . close ( ) \n 
~~ ~~ except IOError : \n 
~~ ~~ ~~ return new_arg_strings \n 
~~ def convert_arg_line_to_args ( self , arg_line ) : \n 
~~~ return [ arg_line ] \n 
~~ def _match_argument ( self , action , arg_strings_pattern ) : \n 
~~~ nargs_pattern = self . _get_nargs_pattern ( action ) \n 
match = _re . match ( nargs_pattern , arg_strings_pattern ) \n 
if match is None : \n 
~~~ nargs_errors = { \n 
None : _ ( ) , \n 
OPTIONAL : _ ( ) , \n 
ONE_OR_MORE : _ ( ) , \n 
default = _ ( ) % action . nargs \n 
msg = nargs_errors . get ( action . nargs , default ) \n 
raise ArgumentError ( action , msg ) \n 
~~ return len ( match . group ( 1 ) ) \n 
~~ def _match_arguments_partial ( self , actions , arg_strings_pattern ) : \n 
for i in range ( len ( actions ) , 0 , - 1 ) : \n 
~~~ actions_slice = actions [ : i ] \n 
pattern = . join ( [ self . _get_nargs_pattern ( action ) \n 
for action in actions_slice ] ) \n 
match = _re . match ( pattern , arg_strings_pattern ) \n 
if match is not None : \n 
~~~ result . extend ( [ len ( string ) for string in match . groups ( ) ] ) \n 
~~ def _parse_optional ( self , arg_string ) : \n 
~~~ if not arg_string : \n 
~~ if not arg_string [ 0 ] in self . prefix_chars : \n 
~~ if arg_string in self . _option_string_actions : \n 
~~~ action = self . _option_string_actions [ arg_string ] \n 
return action , arg_string , None \n 
~~ if len ( arg_string ) == 1 : \n 
~~ if in arg_string : \n 
~~~ option_string , explicit_arg = arg_string . split ( , 1 ) \n 
if option_string in self . _option_string_actions : \n 
~~~ action = self . _option_string_actions [ option_string ] \n 
return action , option_string , explicit_arg \n 
~~ ~~ option_tuples = self . _get_option_tuples ( arg_string ) \n 
if len ( option_tuples ) > 1 : \n 
~~~ options = . join ( [ option_string \n 
for action , option_string , explicit_arg in option_tuples ] ) \n 
tup = arg_string , options \n 
self . error ( _ ( ) % tup ) \n 
~~ elif len ( option_tuples ) == 1 : \n 
~~~ option_tuple , = option_tuples \n 
return option_tuple \n 
~~ if self . _negative_number_matcher . match ( arg_string ) : \n 
~~ ~~ if in arg_string : \n 
~~ return None , arg_string , None \n 
~~ def _get_option_tuples ( self , option_string ) : \n 
if option_string [ 0 ] in chars and option_string [ 1 ] in chars : \n 
~~~ if in option_string : \n 
~~~ option_prefix , explicit_arg = option_string . split ( , 1 ) \n 
~~~ option_prefix = option_string \n 
explicit_arg = None \n 
~~ for option_string in self . _option_string_actions : \n 
~~~ if option_string . startswith ( option_prefix ) : \n 
tup = action , option_string , explicit_arg \n 
result . append ( tup ) \n 
~~ ~~ ~~ elif option_string [ 0 ] in chars and option_string [ 1 ] not in chars : \n 
short_option_prefix = option_string [ : 2 ] \n 
short_explicit_arg = option_string [ 2 : ] \n 
for option_string in self . _option_string_actions : \n 
~~~ if option_string == short_option_prefix : \n 
tup = action , option_string , short_explicit_arg \n 
~~ elif option_string . startswith ( option_prefix ) : \n 
~~~ self . error ( _ ( ) % option_string ) \n 
~~ def _get_nargs_pattern ( self , action ) : \n 
~~~ nargs = action . nargs \n 
if nargs is None : \n 
~~~ nargs_pattern = \n 
~~ elif nargs == OPTIONAL : \n 
~~ elif nargs == ZERO_OR_MORE : \n 
~~ elif nargs == ONE_OR_MORE : \n 
~~ elif nargs == REMAINDER : \n 
~~ elif nargs == PARSER : \n 
~~~ nargs_pattern = % . join ( * nargs ) \n 
~~ if action . option_strings : \n 
~~~ nargs_pattern = nargs_pattern . replace ( , ) \n 
nargs_pattern = nargs_pattern . replace ( , ) \n 
~~ return nargs_pattern \n 
~~ def _get_values ( self , action , arg_strings ) : \n 
~~~ if action . nargs not in [ PARSER , REMAINDER ] : \n 
~~~ arg_strings = [ s for s in arg_strings if s != ] \n 
~~ if not arg_strings and action . nargs == OPTIONAL : \n 
~~~ value = action . const \n 
~~~ value = action . default \n 
~~ if isinstance ( value , basestring ) : \n 
~~~ value = self . _get_value ( action , value ) \n 
self . _check_value ( action , value ) \n 
~~ ~~ elif ( not arg_strings and action . nargs == ZERO_OR_MORE and \n 
not action . option_strings ) : \n 
~~~ if action . default is not None : \n 
~~~ value = arg_strings \n 
~~ self . _check_value ( action , value ) \n 
~~ elif len ( arg_strings ) == 1 and action . nargs in [ None , OPTIONAL ] : \n 
~~~ arg_string , = arg_strings \n 
value = self . _get_value ( action , arg_string ) \n 
~~~ value = [ self . _get_value ( action , v ) for v in arg_strings ] \n 
self . _check_value ( action , value [ 0 ] ) \n 
for v in value : \n 
~~~ self . _check_value ( action , v ) \n 
~~ def _get_value ( self , action , arg_string ) : \n 
~~~ type_func = self . _registry_get ( , action . type , action . type ) \n 
raise ArgumentError ( action , msg % type_func ) \n 
~~~ result = type_func ( arg_string ) \n 
~~ except ArgumentTypeError : \n 
~~~ name = getattr ( action . type , , repr ( action . type ) ) \n 
msg = str ( _sys . exc_info ( ) [ 1 ] ) \n 
~~ except ( TypeError , ValueError ) : \n 
raise ArgumentError ( action , msg % ( name , arg_string ) ) \n 
~~ def _check_value ( self , action , value ) : \n 
~~~ if action . choices is not None and value not in action . choices : \n 
~~~ tup = value , . join ( map ( repr , action . choices ) ) \n 
msg = _ ( ) % tup \n 
~~ ~~ def format_usage ( self ) : \n 
formatter . add_usage ( self . usage , self . _actions , \n 
self . _mutually_exclusive_groups ) \n 
return formatter . format_help ( ) \n 
formatter . add_text ( self . description ) \n 
for action_group in self . _action_groups : \n 
~~~ formatter . start_section ( action_group . title ) \n 
formatter . add_text ( action_group . description ) \n 
formatter . add_arguments ( action_group . _group_actions ) \n 
formatter . end_section ( ) \n 
~~ formatter . add_text ( self . epilog ) \n 
~~ def format_version ( self ) : \n 
DeprecationWarning ) \n 
formatter = self . _get_formatter ( ) \n 
formatter . add_text ( self . version ) \n 
~~ def _get_formatter ( self ) : \n 
~~~ return self . formatter_class ( prog = self . prog ) \n 
~~ def print_usage ( self , file = None ) : \n 
~~~ if file is None : \n 
~~~ file = _sys . stdout \n 
~~ self . _print_message ( self . format_usage ( ) , file ) \n 
~~ def print_help ( self , file = None ) : \n 
~~ self . _print_message ( self . format_help ( ) , file ) \n 
~~ def print_version ( self , file = None ) : \n 
self . _print_message ( self . format_version ( ) , file ) \n 
~~ def _print_message ( self , message , file = None ) : \n 
~~~ if message : \n 
~~~ file = _sys . stderr \n 
~~ file . write ( message ) \n 
~~ ~~ def exit ( self , status = 0 , message = None ) : \n 
~~~ self . _print_message ( message , _sys . stderr ) \n 
~~ _sys . exit ( status ) \n 
~~ def error ( self , message ) : \n 
self . print_usage ( _sys . stderr ) \n 
self . exit ( 2 , _ ( ) % ( self . prog , message ) ) \n 
######################################################### \n 
libpath = os . path . dirname ( os . path . abspath ( __file__ ) ) \n 
sys . path [ : 0 ] = [ os . path . join ( libpath , ) ] \n 
import common \n 
import environment \n 
import xmltodict \n 
logger = common . logging . getLogger ( ) . getChild ( ) \n 
~~ except ImportError as e : \n 
~~ libpath = os . path . dirname ( os . path . abspath ( __file__ ) ) \n 
sys . path [ : 0 ] = [ os . path . join ( libpath , , , ) ] \n 
sys . path [ : 0 ] = [ os . path . join ( libpath , , ) ] \n 
~~~ import pandevice . base \n 
import pan . xapi \n 
~~ from common import log \n 
def usage ( ) : \n 
~~ def parse_apps ( apps_xml ) : \n 
~~~ obj = xmltodict . parse ( apps_xml ) \n 
~~~ apps = obj [ ] [ ] [ ] [ ] \n 
~~ except KeyError as e : \n 
raise e \n 
~~ csv_apps = [ ] \n 
for app in apps : \n 
~~~ a = OrderedDict ( ) \n 
~~~ a [ ] = app [ ] \n 
a [ ] = app . get ( , "" ) \n 
a [ ] = app [ ] \n 
if a [ ] != u"yes" and a [ ] != ~~~ a [ ] = a [ ] [ ] \n 
~~ a [ ] = app [ ] \n 
a [ ] = app . get ( , "no" ) \n 
a [ ] = "" \n 
~~~ default = app [ ] \n 
if isinstance ( default , list ) : \n 
~~~ for d in default : \n 
~~~ a [ ] = d [ ] [ ] \n 
~~~ a [ ] = default [ ] [ ] \n 
~~ ~~ except KeyError : \n 
~~~ if not isinstance ( a [ ] , basestring ) : \n 
~~~ a [ ] = "|" . join ( a [ ] ) \n 
logger . error ( traceback . format_exc ( ) ) \n 
common . exit_with_error ( str ( e ) ) \n 
~~ for key in a : \n 
~~~ a [ key ] = str ( a [ key ] ) \n 
~~ csv_apps . append ( a ) \n 
return csv_apps \n 
~~ def parse_threats ( threats_xml ) : \n 
~~~ obj = xmltodict . parse ( threats_xml ) \n 
~~~ phone_home = obj [ ] [ ] [ ] [ ] [ ] \n 
vulnerability = obj [ ] [ ] [ ] [ ] [ ] \n 
threats = phone_home + vulnerability \n 
~~ csv_threats = [ ] \n 
for threat in threats : \n 
~~~ a [ ] = threat [ ] \n 
a [ ] = threat [ ] \n 
a [ ] = threat . get ( , None ) \n 
if a [ ] is not None : \n 
~~~ a [ ] = threat [ ] [ ] \n 
if not isinstance ( a [ ] , basestring ) : \n 
~~~ a [ ] = "" \n 
~~ ~~ except KeyError as e : \n 
~~ csv_threats . append ( a ) \n 
return csv_threats \n 
~~~ args , kwargs = splunk . Intersplunk . getKeywordsAndOptions ( ) \n 
debug = common . check_debug ( kwargs ) \n 
if len ( args ) < 2 : \n 
usage ( ) \n 
~~ if args [ 1 ] == "apps" : \n 
~~~ usage ( ) \n 
~~ results , unused1 , settings = splunk . Intersplunk . getOrganizedResults ( ) \n 
sessionKey = settings [ ] \n 
apikey = common . apikey ( sessionKey , args [ 0 ] , debug ) \n 
device = pandevice . base . PanDevice ( args [ 0 ] , api_key = apikey ) \n 
~~~ if args [ 1 ] == "apps" : \n 
~~~ device . xapi . get ( "/config/predefined/application" ) \n 
app_xml = device . xapi . xml_document \n 
csv = parse_apps ( app_xml ) \n 
~~~ device . xapi . get ( "/config/predefined/threats" ) \n 
threat_xml = device . xapi . xml_document \n 
csv = parse_threats ( threat_xml ) \n 
~~ ~~ except pan . xapi . PanXapiError as e : \n 
~~~ common . exit_with_error ( str ( e ) ) \n 
~~ splunk . Intersplunk . outputResults ( csv ) \n 
~~ from __future__ import division \n 
__all__ = [ "BalancedConsumer" ] \n 
from kazoo . client import KazooClient \n 
from kazoo . handlers . gevent import SequentialGeventHandler \n 
from kazoo . exceptions import NoNodeException , NodeExistsError \n 
from kazoo . recipe . watchers import ChildrenWatch \n 
from . common import OffsetType \n 
from . exceptions import KafkaException , PartitionOwnedError , ConsumerStoppedException \n 
from . handlers import GEventHandler \n 
from . simpleconsumer import SimpleConsumer \n 
from . utils . compat import range , get_bytes , itervalues , iteritems , get_string \n 
~~~ from . import rdkafka \n 
~~~ rdkafka = False \n 
~~ log = logging . getLogger ( __name__ ) \n 
def _catch_thread_exception ( fn ) : \n 
def wrapped ( self , * args , ** kwargs ) : \n 
~~~ ret = fn ( self , * args , ** kwargs ) \n 
~~~ self . _worker_exception = sys . exc_info ( ) \n 
~~ ~~ return wrapped \n 
~~ class BalancedConsumer ( object ) : \n 
topic , \n 
cluster , \n 
consumer_group , \n 
fetch_message_max_bytes = 1024 * 1024 , \n 
num_consumer_fetchers = 1 , \n 
auto_commit_enable = False , \n 
auto_commit_interval_ms = 60 * 1000 , \n 
queued_max_messages = 2000 , \n 
fetch_min_bytes = 1 , \n 
fetch_wait_max_ms = 100 , \n 
offsets_channel_backoff_ms = 1000 , \n 
offsets_commit_max_retries = 5 , \n 
auto_offset_reset = OffsetType . EARLIEST , \n 
consumer_timeout_ms = - 1 , \n 
rebalance_max_retries = 5 , \n 
rebalance_backoff_ms = 2 * 1000 , \n 
zookeeper_connection_timeout_ms = 6 * 1000 , \n 
zookeeper_connect = , \n 
zookeeper = None , \n 
auto_start = True , \n 
reset_offset_on_start = False , \n 
post_rebalance_callback = None , \n 
use_rdkafka = False , \n 
compacted_topic = False ) : \n 
self . _cluster = cluster \n 
if not isinstance ( consumer_group , bytes ) : \n 
~~ self . _consumer_group = consumer_group \n 
self . _topic = topic \n 
self . _auto_commit_enable = auto_commit_enable \n 
self . _auto_commit_interval_ms = auto_commit_interval_ms \n 
self . _fetch_message_max_bytes = fetch_message_max_bytes \n 
self . _fetch_min_bytes = fetch_min_bytes \n 
self . _rebalance_max_retries = rebalance_max_retries \n 
self . _num_consumer_fetchers = num_consumer_fetchers \n 
self . _queued_max_messages = queued_max_messages \n 
self . _fetch_wait_max_ms = fetch_wait_max_ms \n 
self . _rebalance_backoff_ms = rebalance_backoff_ms \n 
self . _consumer_timeout_ms = consumer_timeout_ms \n 
self . _offsets_channel_backoff_ms = offsets_channel_backoff_ms \n 
self . _offsets_commit_max_retries = offsets_commit_max_retries \n 
self . _auto_offset_reset = auto_offset_reset \n 
self . _zookeeper_connect = zookeeper_connect \n 
self . _zookeeper_connection_timeout_ms = zookeeper_connection_timeout_ms \n 
self . _reset_offset_on_start = reset_offset_on_start \n 
self . _post_rebalance_callback = post_rebalance_callback \n 
self . _generation_id = - 1 \n 
self . _running = False \n 
self . _worker_exception = None \n 
self . _worker_trace_logged = False \n 
self . _is_compacted_topic = compacted_topic \n 
if not rdkafka and use_rdkafka : \n 
~~ if isinstance ( self . _cluster . handler , GEventHandler ) and use_rdkafka : \n 
~~ self . _use_rdkafka = rdkafka and use_rdkafka \n 
self . _rebalancing_lock = cluster . handler . Lock ( ) \n 
self . _consumer = None \n 
self . _consumer_id = get_bytes ( "{hostname}:{uuid}" . format ( \n 
hostname = socket . gethostname ( ) , \n 
uuid = uuid4 ( ) \n 
self . _setting_watches = True \n 
self . _topic_path = . format ( \n 
group = self . _consumer_group , \n 
topic = self . _topic . name ) \n 
self . _consumer_id_path = . format ( \n 
group = self . _consumer_group ) \n 
self . _zookeeper = None \n 
self . _owns_zookeeper = zookeeper is None \n 
if zookeeper is not None : \n 
~~~ self . _zookeeper = zookeeper \n 
~~ if auto_start is True : \n 
~~~ self . start ( ) \n 
~~ ~~ def __del__ ( self ) : \n 
if self . _running : \n 
~~~ self . stop ( ) \n 
module = self . __class__ . __module__ , \n 
name = self . __class__ . __name__ , \n 
id_ = hex ( id ( self ) ) , \n 
group = self . _consumer_group \n 
~~ def _raise_worker_exceptions ( self ) : \n 
if self . _worker_exception is not None : \n 
~~~ _ , ex , tb = self . _worker_exception \n 
if not self . _worker_trace_logged : \n 
~~~ self . _worker_trace_logged = True \n 
"" . join ( traceback . format_tb ( tb ) ) ) \n 
~~ raise ex \n 
def topic ( self ) : \n 
return self . _topic \n 
def partitions ( self ) : \n 
return self . _consumer . partitions if self . _consumer else dict ( ) \n 
def _partitions ( self ) : \n 
return set ( \n 
[ ] if self . partitions is None else itervalues ( self . partitions ) ) \n 
def held_offsets ( self ) : \n 
if not self . _consumer : \n 
~~ return self . _consumer . held_offsets \n 
~~ def start ( self ) : \n 
~~~ if self . _zookeeper is None : \n 
~~~ self . _setup_zookeeper ( self . _zookeeper_connect , \n 
self . _zookeeper_connection_timeout_ms ) \n 
~~ self . _zookeeper . ensure_path ( self . _topic_path ) \n 
self . _add_self ( ) \n 
self . _running = True \n 
self . _set_watches ( ) \n 
self . _rebalance ( ) \n 
self . stop ( ) \n 
~~ ~~ def stop ( self ) : \n 
with self . _rebalancing_lock : \n 
~~~ self . _running = False \n 
~~ if self . _consumer is not None : \n 
~~~ self . _consumer . stop ( ) \n 
~~ if self . _owns_zookeeper : \n 
~~~ self . _zookeeper . stop ( ) \n 
~~~ self . _remove_partitions ( self . _get_held_partitions ( ) ) \n 
~~~ self . _zookeeper . delete ( self . _path_self ) \n 
~~ except NoNodeException : \n 
~~ ~~ ~~ def _setup_zookeeper ( self , zookeeper_connect , timeout ) : \n 
kazoo_kwargs = { : timeout / 1000 } \n 
if isinstance ( self . _cluster . handler , GEventHandler ) : \n 
~~~ kazoo_kwargs [ ] = SequentialGeventHandler ( ) \n 
~~ self . _zookeeper = KazooClient ( zookeeper_connect , ** kazoo_kwargs ) \n 
self . _zookeeper . start ( ) \n 
~~ def _setup_internal_consumer ( self , partitions = None , start = True ) : \n 
if partitions is None : \n 
~~~ partitions = [ ] \n 
~~ if partitions != self . _partitions : \n 
~~~ cns = self . _get_internal_consumer ( partitions = list ( partitions ) , start = start ) \n 
if self . _post_rebalance_callback is not None : \n 
~~~ old_offsets = ( self . _consumer . held_offsets \n 
if self . _consumer else dict ( ) ) \n 
new_offsets = cns . held_offsets \n 
~~~ reset_offsets = self . _post_rebalance_callback ( \n 
self , old_offsets , new_offsets ) \n 
self . _worker_exception = sys . exc_info ( ) \n 
~~ if reset_offsets : \n 
~~~ cns . reset_offsets ( partition_offsets = [ \n 
( cns . partitions [ id_ ] , offset ) for \n 
( id_ , offset ) in iteritems ( reset_offsets ) ] ) \n 
~~ ~~ self . _consumer = cns \n 
~~ def _get_internal_consumer ( self , partitions = None , start = True ) : \n 
~~ reset_offset_on_start = self . _reset_offset_on_start \n 
if self . _consumer is not None : \n 
reset_offset_on_start = False \n 
~~ Cls = ( rdkafka . RdKafkaSimpleConsumer \n 
if self . _use_rdkafka else SimpleConsumer ) \n 
return Cls ( \n 
self . _topic , \n 
self . _cluster , \n 
consumer_group = self . _consumer_group , \n 
partitions = partitions , \n 
auto_commit_enable = self . _auto_commit_enable , \n 
auto_commit_interval_ms = self . _auto_commit_interval_ms , \n 
fetch_message_max_bytes = self . _fetch_message_max_bytes , \n 
fetch_min_bytes = self . _fetch_min_bytes , \n 
num_consumer_fetchers = self . _num_consumer_fetchers , \n 
queued_max_messages = self . _queued_max_messages , \n 
fetch_wait_max_ms = self . _fetch_wait_max_ms , \n 
consumer_timeout_ms = self . _consumer_timeout_ms , \n 
offsets_channel_backoff_ms = self . _offsets_channel_backoff_ms , \n 
offsets_commit_max_retries = self . _offsets_commit_max_retries , \n 
auto_offset_reset = self . _auto_offset_reset , \n 
reset_offset_on_start = reset_offset_on_start , \n 
auto_start = start , \n 
compacted_topic = self . _is_compacted_topic , \n 
generation_id = self . _generation_id , \n 
consumer_id = self . _consumer_id \n 
~~ def _decide_partitions ( self , participants , consumer_id = None ) : \n 
p_to_str = lambda p : . join ( [ str ( p . topic . name ) , str ( p . leader . id ) , str ( p . id ) ] ) \n 
all_parts = self . _topic . partitions . values ( ) \n 
all_parts = sorted ( all_parts , key = p_to_str ) \n 
participants = sorted ( participants ) \n 
idx = participants . index ( consumer_id or self . _consumer_id ) \n 
parts_per_consumer = len ( all_parts ) // len ( participants ) \n 
remainder_ppc = len ( all_parts ) % len ( participants ) \n 
start = parts_per_consumer * idx + min ( idx , remainder_ppc ) \n 
num_parts = parts_per_consumer + ( 0 if ( idx + 1 > remainder_ppc ) else 1 ) \n 
new_partitions = itertools . islice ( all_parts , start , start + num_parts ) \n 
new_partitions = set ( new_partitions ) \n 
log . info ( , \n 
self . _consumer_id , len ( participants ) , len ( all_parts ) , \n 
len ( new_partitions ) ) \n 
log . debug ( , [ p_to_str ( p ) for p in new_partitions ] ) \n 
return new_partitions \n 
~~ def _get_participants ( self ) : \n 
~~~ consumer_ids = self . _zookeeper . get_children ( self . _consumer_id_path ) \n 
~~ participants = [ ] \n 
for id_ in consumer_ids : \n 
~~~ topic , stat = self . _zookeeper . get ( "%s/%s" % ( self . _consumer_id_path , id_ ) ) \n 
if topic == self . _topic . name : \n 
~~~ participants . append ( get_bytes ( id_ ) ) \n 
~~ ~~ except NoNodeException : \n 
~~ ~~ participants = sorted ( participants ) \n 
return participants \n 
~~ def _build_watch_callback ( self , fn , proxy ) : \n 
def _callback ( children ) : \n 
~~~ proxy . __repr__ ( ) \n 
~~ except ReferenceError : \n 
~~ return fn ( proxy , children ) \n 
~~ return _callback \n 
~~ def _set_watches ( self ) : \n 
proxy = weakref . proxy ( self ) \n 
_brokers_changed = self . _build_watch_callback ( BalancedConsumer . _brokers_changed , proxy ) \n 
_topics_changed = self . _build_watch_callback ( BalancedConsumer . _topics_changed , proxy ) \n 
_consumers_changed = self . _build_watch_callback ( BalancedConsumer . _consumers_changed , proxy ) \n 
broker_path = \n 
~~~ self . _broker_watcher = ChildrenWatch ( \n 
self . _zookeeper , broker_path , \n 
_brokers_changed \n 
% broker_path ) \n 
~~ self . _topics_watcher = ChildrenWatch ( \n 
self . _zookeeper , \n 
_topics_changed \n 
self . _consumer_watcher = ChildrenWatch ( \n 
self . _zookeeper , self . _consumer_id_path , \n 
_consumers_changed \n 
self . _setting_watches = False \n 
~~ def _add_self ( self ) : \n 
participants = self . _get_participants ( ) \n 
if len ( self . _topic . partitions ) <= len ( participants ) : \n 
~~ self . _zookeeper . create ( \n 
self . _path_self , self . _topic . name , ephemeral = True , makepath = True ) \n 
def _path_self ( self ) : \n 
return . format ( \n 
path = self . _consumer_id_path , \n 
id_ = get_string ( self . _consumer_id ) \n 
~~ def _update_member_assignment ( self ) : \n 
for i in range ( self . _rebalance_max_retries ) : \n 
~~~ participants = self . _get_participants ( ) \n 
if self . _consumer_id not in participants : \n 
~~~ self . _add_self ( ) \n 
participants . append ( self . _consumer_id ) \n 
~~ new_partitions = self . _decide_partitions ( participants ) \n 
if not new_partitions : \n 
self . _consumer_id ) \n 
~~ current_zk_parts = self . _get_held_partitions ( ) \n 
self . _remove_partitions ( current_zk_parts - new_partitions ) \n 
self . _add_partitions ( new_partitions - current_zk_parts ) \n 
if self . _setup_internal_consumer ( new_partitions ) : \n 
~~ except PartitionOwnedError as ex : \n 
~~~ if i == self . _rebalance_max_retries - 1 : \n 
~~~ log . warning ( , \n 
ex . partition , i ) \n 
~~ log . info ( , ex . partition ) \n 
self . _cluster . handler . sleep ( i * ( self . _rebalance_backoff_ms / 1000 ) ) \n 
~~ ~~ ~~ def _rebalance ( self ) : \n 
~~~ self . commit_offsets ( ) \n 
~~ with self . _rebalancing_lock : \n 
~~~ if not self . _running : \n 
~~~ raise ConsumerStoppedException \n 
self . _consumer_id , self . _topic . name ) ) \n 
self . _update_member_assignment ( ) \n 
~~ ~~ def _path_from_partition ( self , p ) : \n 
return "%s/%s-%s" % ( self . _topic_path , p . leader . id , p . id ) \n 
~~ def _remove_partitions ( self , partitions ) : \n 
for p in partitions : \n 
~~~ self . _zookeeper . delete ( self . _path_from_partition ( p ) ) \n 
~~ ~~ def _add_partitions ( self , partitions ) : \n 
~~~ self . _zookeeper . create ( \n 
self . _path_from_partition ( p ) , \n 
value = get_bytes ( self . _consumer_id ) , \n 
ephemeral = True \n 
~~ except NodeExistsError : \n 
~~~ raise PartitionOwnedError ( p ) \n 
~~ ~~ ~~ def _get_held_partitions ( self ) : \n 
zk_partition_ids = set ( ) \n 
all_partitions = self . _zookeeper . get_children ( self . _topic_path ) \n 
for partition_slug in all_partitions : \n 
~~~ owner_id , stat = self . _zookeeper . get ( \n 
. format ( \n 
path = self . _topic_path , slug = partition_slug ) ) \n 
if owner_id == get_bytes ( self . _consumer_id ) : \n 
~~~ zk_partition_ids . add ( int ( partition_slug . split ( ) [ 1 ] ) ) \n 
~~ ~~ return set ( self . _topic . partitions [ _id ] for _id in zk_partition_ids ) \n 
~~ @ _catch_thread_exception \n 
def _brokers_changed ( self , brokers ) : \n 
~~ if self . _setting_watches : \n 
self . _consumer_id ) ) \n 
def _consumers_changed ( self , consumers ) : \n 
def _topics_changed ( self , topics ) : \n 
~~ def reset_offsets ( self , partition_offsets = None ) : \n 
self . _raise_worker_exceptions ( ) \n 
~~ self . _consumer . reset_offsets ( partition_offsets = partition_offsets ) \n 
~~ def consume ( self , block = True ) : \n 
def consumer_timed_out ( ) : \n 
if self . _consumer_timeout_ms == - 1 : \n 
~~ disp = ( time . time ( ) - self . _last_message_time ) * 1000.0 \n 
return disp > self . _consumer_timeout_ms \n 
~~ message = None \n 
self . _last_message_time = time . time ( ) \n 
while message is None and not consumer_timed_out ( ) : \n 
~~~ self . _raise_worker_exceptions ( ) \n 
~~~ message = self . _consumer . consume ( block = block ) \n 
~~ except ( ConsumerStoppedException , AttributeError ) : \n 
~~ if message : \n 
~~~ self . _last_message_time = time . time ( ) \n 
~~ if not block : \n 
~~~ return message \n 
~~ ~~ return message \n 
~~~ message = self . consume ( block = True ) \n 
if not message : \n 
~~ yield message \n 
~~ ~~ def commit_offsets ( self ) : \n 
~~ return self . _consumer . commit_offsets ( ) \n 
from itertools import cycle \n 
from streamparse import Spout \n 
class WordSpout ( Spout ) : \n 
~~~ outputs = [ ] \n 
def initialize ( self , stormconf , context ) : \n 
~~~ self . words = cycle ( [ , , , ] ) \n 
~~ def next_tuple ( self ) : \n 
~~~ word = next ( self . words ) \n 
self . emit ( [ word ] ) \n 
from . stream import Grouping , Stream \n 
from . topology import Topology \n 
from . constants import eStart , eError , eItsMe \n 
HZ_cls = ( \n 
HZ_st = ( \n 
HZCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 ) \n 
HZSMModel = { : HZ_cls , \n 
: 6 , \n 
: HZ_st , \n 
: HZCharLenTable , \n 
: "HZ-GB-2312" } \n 
ISO2022CN_cls = ( \n 
ISO2022CN_st = ( \n 
ISO2022CNCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ) \n 
ISO2022CNSMModel = { : ISO2022CN_cls , \n 
: 9 , \n 
: ISO2022CN_st , \n 
: ISO2022CNCharLenTable , \n 
: "ISO-2022-CN" } \n 
ISO2022JP_cls = ( \n 
ISO2022JP_st = ( \n 
ISO2022JPCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ) \n 
ISO2022JPSMModel = { : ISO2022JP_cls , \n 
: ISO2022JP_st , \n 
: ISO2022JPCharLenTable , \n 
: "ISO-2022-JP" } \n 
ISO2022KR_cls = ( \n 
ISO2022KR_st = ( \n 
ISO2022KRCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 ) \n 
ISO2022KRSMModel = { : ISO2022KR_cls , \n 
: ISO2022KR_st , \n 
: ISO2022KRCharLenTable , \n 
: "ISO-2022-KR" } \n 
from multiprocessing import Process \n 
import tasa \n 
from tasa . worker import BaseWorker \n 
logging . basicConfig ( level = logging . INFO ) \n 
def signal_handler ( signal , frame ) : \n 
~~~ sys . exit ( 0 ) \n 
~~ def _get_argparser ( ) : \n 
~~~ parser = argparse . ArgumentParser ( ) \n 
parser . add_argument ( \n 
, , action = , \n 
version = % ( \n 
tasa . __version__ , sys . version ) ) \n 
~~ def run ( ) : \n 
~~~ sys . path . insert ( 0 , ) \n 
parser = _get_argparser ( ) \n 
parser . description = \n 
parser . add_argument ( , \n 
type = lambda w : w . partition ( ) [ : : 2 ] , \n 
help = \n 
worker_class_name = args . worker [ 1 ] or \n 
worker_module = __import__ ( args . worker [ 0 ] , globals ( ) , locals ( ) , \n 
[ worker_class_name ] ) \n 
~~~ WorkerClass = getattr ( worker_module , worker_class_name ) \n 
potential_workers = inspect . getmembers ( \n 
worker_module , \n 
lambda x : type ( x ) == type and issubclass ( x , BaseWorker ) ) \n 
if potential_workers : \n 
for name , value in potential_workers : \n 
~~~ print . join ( [ args . worker [ 0 ] , name ] ) \n 
~~ ~~ exit ( 1 ) \n 
~~ worker = WorkerClass ( ) \n 
print % ( args . worker [ 0 ] , \n 
worker . __class__ . __name__ ) \n 
~~~ for job in worker : \n 
~~~ if job : \n 
worker . __class__ . __name__ , \n 
str ( job ) [ : 50 ] ) \n 
~~~ time . sleep ( .3 ) \n 
~~ ~~ def runm ( ) : \n 
signal . signal ( signal . SIGINT , signal_handler ) \n 
count = int ( sys . argv . pop ( 1 ) ) \n 
processes = [ Process ( target = run , args = ( ) ) for x in range ( count ) ] \n 
~~~ for p in processes : \n 
~~~ p . start ( ) \n 
~~~ p . join ( ) \n 
~~ ~~ ~~ def log ( ) : \n 
~~~ parser = _get_argparser ( ) \n 
raise NotImplemented ( ) \n 
~~~ cmd = if len ( sys . argv ) < 2 else sys . argv . pop ( 1 ) \n 
if cmd == : \n 
~~ elif cmd == : \n 
~~~ log ( ) \n 
~~ ~~ from datetime import date , timedelta \n 
import phonenumbers \n 
from django . core . validators import RegexValidator \n 
from django . utils import timezone \n 
from . behaviors import TimeStampedModel \n 
from . import helpers , managers \n 
class Tag ( models . Model ) : \n 
name = models . CharField ( max_length = 64 , unique = True , help_text = ) \n 
color = models . CharField ( max_length = 6 , validators = [ color_regex ] , help_text = \n 
description = models . CharField ( max_length = 64 , blank = True , help_text = \n 
~~~ ordering = [ ] \n 
~~~ return self . name \n 
~~ def save ( self , * args , ** kwargs ) : \n 
super ( Tag , self ) . save ( * args , ** kwargs ) \n 
~~ ~~ class Person ( models . Model ) : \n 
DEVELOPER_ROLE = \n 
QUALITY_ASSURANCE_ROLE = \n 
OPERATIONS_ROLE = \n 
MANAGER_ROLE = \n 
SECURITY_OFFICER_ROLE = \n 
SECURITY_CHAMPION_ROLE = \n 
ROLE_CHOICES = ( \n 
( DEVELOPER_ROLE , ) , \n 
( QUALITY_ASSURANCE_ROLE , ) , \n 
( OPERATIONS_ROLE , ) , \n 
( MANAGER_ROLE , ) , \n 
( SECURITY_OFFICER_ROLE , ) , \n 
( SECURITY_CHAMPION_ROLE , ) , \n 
first_name = models . CharField ( max_length = 64 ) \n 
last_name = models . CharField ( max_length = 64 ) \n 
email = models . EmailField ( max_length = 128 , unique = True ) \n 
role = models . CharField ( max_length = 17 , choices = ROLE_CHOICES ) \n 
phone_work = models . CharField ( max_length = 15 , validators = [ phone_regex ] , blank = True ) \n 
phone_mobile = models . CharField ( max_length = 15 , validators = [ phone_regex ] , blank = True ) \n 
job_title = models . CharField ( max_length = 128 , blank = True ) \n 
verbose_name_plural = \n 
~~~ return self . first_name + + self . last_name \n 
~~~ if self . phone_work : \n 
~~~ self . phone_work = phonenumbers . format_number ( phonenumbers . parse ( self . phone_work , ) , ~~ if self . phone_mobile : \n 
~~~ self . phone_mobile = phonenumbers . format_number ( phonenumbers . parse ( self . phone_mobile , ~~ self . email = self . email . lower ( ) \n 
super ( Person , self ) . save ( * args , ** kwargs ) \n 
~~ ~~ class Organization ( models . Model ) : \n 
name = models . CharField ( max_length = 32 , unique = True , help_text = description = models . TextField ( blank = True , help_text = \n 
people = models . ManyToManyField ( Person , blank = True ) \n 
~~ ~~ class DataElement ( models . Model ) : \n 
GLOBAL_CATEGORY = \n 
PERSONAL_CATEGORY = \n 
COMPANY_CATEGORY = \n 
STUDENT_CATEGORY = \n 
GOVERNMENT_CATEGORY = \n 
PCI_CATEGORY = \n 
MEDICAL_CATEGORY = \n 
CATEGORY_CHOICES = ( \n 
( GLOBAL_CATEGORY , ) , \n 
( PERSONAL_CATEGORY , ) , \n 
( COMPANY_CATEGORY , ) , \n 
( STUDENT_CATEGORY , ) , \n 
( GOVERNMENT_CATEGORY , ) , \n 
( PCI_CATEGORY , ) , \n 
( MEDICAL_CATEGORY , ) , \n 
name = models . CharField ( max_length = 128 , unique = True ) \n 
description = models . TextField ( blank = True ) \n 
category = models . CharField ( max_length = 10 , choices = CATEGORY_CHOICES ) \n 
weight = models . PositiveIntegerField ( ) \n 
~~ ~~ class Technology ( models . Model ) : \n 
PROGRAMMING_LANGUAGE_CATEGORY = \n 
OPERATING_SYSTEM_CATEGORY = \n 
DATA_STORE_CATEGORY = \n 
FRAMEWORK_CATEGORY = \n 
THIRD_PARTY_COMPONENT = \n 
WEB_SERVER_CATEGORY = \n 
APPLICATION_SERVER_CATEGORY = \n 
HOSTING_PROVIDER_CATEGORY = \n 
DENIAL_OF_SERVICE_CATEGORY = \n 
FIREWALL_CATEGORY = \n 
( PROGRAMMING_LANGUAGE_CATEGORY , ) , \n 
( OPERATING_SYSTEM_CATEGORY , ) , \n 
( DATA_STORE_CATEGORY , ) , \n 
( FRAMEWORK_CATEGORY , ) , \n 
( THIRD_PARTY_COMPONENT , ) , \n 
( APPLICATION_SERVER_CATEGORY , ) , \n 
( WEB_SERVER_CATEGORY , ) , \n 
( HOSTING_PROVIDER_CATEGORY , ) , \n 
( DENIAL_OF_SERVICE_CATEGORY , ) , \n 
( FIREWALL_CATEGORY , ) , \n 
name = models . CharField ( max_length = 64 , help_text = ) \n 
category = models . CharField ( max_length = 21 , choices = CATEGORY_CHOICES , help_text = description = models . CharField ( max_length = 256 , blank = True , help_text = reference = models . URLField ( blank = True , help_text = ) \n 
~~~ ordering = [ , ] \n 
~~~ return self . get_category_display ( ) + + self . name \n 
~~ ~~ class Regulation ( models . Model ) : \n 
PRIVACY_CATEGORY = \n 
FINANCE_CATEGORY = \n 
EDUCATION_CATEGORY = \n 
OTHER_CATEGORY = \n 
( PRIVACY_CATEGORY , ) , \n 
( FINANCE_CATEGORY , ) , \n 
( EDUCATION_CATEGORY , ) , \n 
( OTHER_CATEGORY , ) , \n 
name = models . CharField ( max_length = 128 , help_text = ) \n 
acronym = models . CharField ( max_length = 20 , unique = True , help_text = category = models . CharField ( max_length = 9 , choices = CATEGORY_CHOICES , help_text = jurisdiction = models . CharField ( max_length = 64 , help_text = description = models . TextField ( blank = True , help_text = reference = models . URLField ( blank = True , help_text = ) \n 
~~~ ordering = [ , , ] \n 
~~~ return self . acronym + + self . jurisdiction + \n 
~~ ~~ class ServiceLevelAgreement ( models . Model ) : \n 
description = models . CharField ( max_length = 256 , blank = True , help_text = \n 
~~ ~~ class ThreadFix ( models . Model ) : \n 
name = models . CharField ( max_length = 32 , unique = True , help_text = host = models . URLField ( help_text = api_key = models . CharField ( max_length = 50 , help_text = verify_ssl = models . BooleanField ( default = True , help_text = \n 
~~~ verbose_name = \n 
~~~ return self . name + + self . host \n 
~~ ~~ class Application ( TimeStampedModel , models . Model ) : \n 
WEB_PLATFORM = \n 
DESKTOP_PLATFORM = \n 
MOBILE_PLATFORM = \n 
WEB_SERVICE_PLATFORM = \n 
PLATFORM_CHOICES = ( \n 
( WEB_PLATFORM , ) , \n 
( DESKTOP_PLATFORM , ) , \n 
( MOBILE_PLATFORM , ) , \n 
( WEB_SERVICE_PLATFORM , ) , \n 
IDEA_LIFECYCLE = \n 
EXPLORE_LIFECYCLE = \n 
VALIDATE_LIFECYCLE = \n 
GROW_LIFECYCLE = \n 
SUSTAIN_LIFECYCLE = \n 
RETIRE_LIFECYCLE = \n 
LIFECYCLE_CHOICES = ( \n 
( IDEA_LIFECYCLE , ) , \n 
( EXPLORE_LIFECYCLE , ) , \n 
( VALIDATE_LIFECYCLE , ) , \n 
( GROW_LIFECYCLE , ) , \n 
( SUSTAIN_LIFECYCLE , ) , \n 
( RETIRE_LIFECYCLE , ) , \n 
THIRD_PARTY_LIBRARY_ORIGIN = \n 
PURCHASED_ORIGIN = \n 
CONTRACTOR_ORIGIN = \n 
INTERNALLY_DEVELOPED_ORIGIN = \n 
OPEN_SOURCE_ORIGIN = \n 
OUTSOURCED_ORIGIN = \n 
ORIGIN_CHOICES = ( \n 
( THIRD_PARTY_LIBRARY_ORIGIN , ) , \n 
( PURCHASED_ORIGIN , ) , \n 
( CONTRACTOR_ORIGIN , ) , \n 
( INTERNALLY_DEVELOPED_ORIGIN , ) , \n 
( OPEN_SOURCE_ORIGIN , ) , \n 
( OUTSOURCED_ORIGIN , ) , \n 
VERY_HIGH_CRITICALITY = \n 
HIGH_CRITICALITY = \n 
MEDIUM_CRITICALITY = \n 
LOW_CRITICALITY = \n 
VERY_LOW_CRITICALITY = \n 
NONE_CRITICALITY = \n 
BUSINESS_CRITICALITY_CHOICES = ( \n 
( VERY_HIGH_CRITICALITY , ) , \n 
( HIGH_CRITICALITY , ) , \n 
( MEDIUM_CRITICALITY , ) , \n 
( LOW_CRITICALITY , ) , \n 
( VERY_LOW_CRITICALITY , ) , \n 
( NONE_CRITICALITY , ) , \n 
DCL_1 = 1 \n 
DCL_2 = 2 \n 
DCL_3 = 3 \n 
DCL_4 = 4 \n 
DATA_CLASSIFICATION_CHOICES = ( \n 
( None , ) , \n 
( DCL_1 , ) , \n 
( DCL_2 , ) , \n 
( DCL_3 , ) , \n 
( DCL_4 , ) , \n 
ASVS_0 = 0 \n 
ASVS_1 = 1 \n 
ASVS_2 = 2 \n 
ASVS_3 = 3 \n 
ASVS_CHOICES = ( \n 
( ASVS_0 , ) , \n 
( ASVS_1 , ) , \n 
( ASVS_2 , ) , \n 
( ASVS_3 , ) , \n 
name = models . CharField ( max_length = 128 , unique = True , help_text = description = models . TextField ( blank = True , help_text = \n 
business_criticality = models . CharField ( max_length = 9 , choices = BUSINESS_CRITICALITY_CHOICES , blank platform = models . CharField ( max_length = 11 , choices = PLATFORM_CHOICES , blank = True , null = True ) \n 
lifecycle = models . CharField ( max_length = 8 , choices = LIFECYCLE_CHOICES , blank = True , null = True ) \n 
origin = models . CharField ( max_length = 19 , choices = ORIGIN_CHOICES , blank = True , null = True ) \n 
user_records = models . PositiveIntegerField ( blank = True , null = True , help_text = revenue = models . DecimalField ( max_digits = 15 , decimal_places = 2 , blank = True , null = True , help_text = external_audience = models . BooleanField ( default = False , help_text = internet_accessible = models . BooleanField ( default = False , help_text = requestable = models . NullBooleanField ( default = True , help_text = _ ( \n 
technologies = models . ManyToManyField ( Technology , blank = True ) \n 
regulations = models . ManyToManyField ( Regulation , blank = True ) \n 
service_level_agreements = models . ManyToManyField ( ServiceLevelAgreement , blank = True ) \n 
data_elements = models . ManyToManyField ( DataElement , blank = True ) \n 
override_dcl = models . IntegerField ( choices = DATA_CLASSIFICATION_CHOICES , blank = True , null = True , help_text override_reason = models . TextField ( blank = True , help_text = \n 
threadfix = models . ForeignKey ( ThreadFix , blank = True , null = True , help_text = threadfix_team_id = models . PositiveIntegerField ( blank = True , null = True , help_text = threadfix_application_id = models . PositiveIntegerField ( blank = True , null = True , help_text = \n 
asvs_level = models . IntegerField ( choices = ASVS_CHOICES , blank = True , null = True , help_text = asvs_level_percent_achieved = models . PositiveIntegerField ( blank = True , null = True , help_text = asvs_doc_url = models . URLField ( blank = True , help_text = ) \n 
asvs_level_target = models . IntegerField ( choices = ASVS_CHOICES , blank = True , null = True , help_text = \n 
organization = models . ForeignKey ( Organization , help_text = people = models . ManyToManyField ( Person , through = , blank = True ) \n 
tags = models . ManyToManyField ( Tag , blank = True ) \n 
objects = managers . ApplicationManager . from_queryset ( managers . ApplicationQuerySet ) ( ) \n 
~~~ get_latest_by = \n 
ordering = [ ] \n 
~~ def data_classification_level ( self ) : \n 
return helpers . data_classification_level ( self . data_sensitivity_value ( ) ) \n 
~~ def data_sensitivity_value ( self ) : \n 
return helpers . data_sensitivity_value ( self . data_elements . all ( ) ) \n 
~~ def is_new ( self ) : \n 
delta = self . created_date - timezone . now ( ) \n 
return delta >= timedelta ( days = - 7 ) \n 
~~ ~~ class ThreadFixMetrics ( TimeStampedModel , models . Model ) : \n 
critical_count = models . PositiveIntegerField ( default = 0 ) \n 
high_count = models . PositiveIntegerField ( default = 0 ) \n 
medium_count = models . PositiveIntegerField ( default = 0 ) \n 
low_count = models . PositiveIntegerField ( default = 0 ) \n 
informational_count = models . PositiveIntegerField ( default = 0 ) \n 
application = models . ForeignKey ( Application ) \n 
verbose_name = \n 
~~ def total ( self ) : \n 
~~~ return self . critical_count + self . high_count + self . medium_count + self . low_count + self . informational_count \n 
~~ ~~ class Relation ( models . Model ) : \n 
owner = models . BooleanField ( default = False , help_text = emergency = models . BooleanField ( default = False , help_text = notes = models . TextField ( blank = True , help_text = \n 
person = models . ForeignKey ( Person , help_text = ) \n 
~~~ unique_together = ( , ) \n 
~~~ return self . person . first_name + + self . person . last_name + + self . application . name \n 
~~ ~~ class Environment ( models . Model ) : \n 
DEVELOPMENT_ENVIRONMENT = \n 
INTEGRATION_ENVIRONMENT = \n 
QUALITY_ASSURANCE_ENVIRONMENT = \n 
PRE_PRODUCTION_ENVIRONMENT = \n 
CUSTOMER_ACCEPTANCE_ENVIRONMENT = \n 
PRODUCTION_ENVIRONMENT = \n 
ENVIRONMENT_CHOICES = ( \n 
( DEVELOPMENT_ENVIRONMENT , ) , \n 
( INTEGRATION_ENVIRONMENT , ) , \n 
( QUALITY_ASSURANCE_ENVIRONMENT , ) , \n 
( PRE_PRODUCTION_ENVIRONMENT , ) , \n 
( CUSTOMER_ACCEPTANCE_ENVIRONMENT , ) , \n 
( PRODUCTION_ENVIRONMENT , ) , \n 
environment_type = models . CharField ( max_length = 4 , choices = ENVIRONMENT_CHOICES , help_text = description = models . TextField ( blank = True , help_text = testing_approved = models . BooleanField ( default = False , help_text = \n 
~~~ return self . application . name + + dict ( Environment . ENVIRONMENT_CHOICES ) [ self . environment_type \n 
~~ ~~ class EnvironmentLocation ( models . Model ) : \n 
location = models . URLField ( help_text = notes = models . TextField ( blank = True , help_text = \n 
environment = models . ForeignKey ( Environment ) \n 
~~~ return self . location \n 
~~ ~~ class EnvironmentCredentials ( TimeStampedModel , models . Model ) : \n 
username = models . CharField ( max_length = 128 , blank = True ) \n 
password = models . CharField ( max_length = 128 , blank = True ) \n 
role_description = models . CharField ( max_length = 128 , blank = True , help_text = notes = models . TextField ( blank = True , help_text = \n 
~~~ verbose_name_plural = \n 
ordering = [ , ] \n 
~~ ~~ class Engagement ( TimeStampedModel , models . Model ) : \n 
PENDING_STATUS = \n 
OPEN_STATUS = \n 
CLOSED_STATUS = \n 
STATUS_CHOICES = ( \n 
( PENDING_STATUS , ) , \n 
( OPEN_STATUS , ) , \n 
( CLOSED_STATUS , ) \n 
status = models . CharField ( max_length = 7 , choices = STATUS_CHOICES , default = PENDING_STATUS ) \n 
start_date = models . DateField ( help_text = ) \n 
end_date = models . DateField ( help_text = ) \n 
open_date = models . DateTimeField ( blank = True , null = True , help_text = close_date = models . DateTimeField ( blank = True , null = True , help_text = duration = models . DurationField ( blank = True , null = True ) \n 
requestor = models . ForeignKey ( Person , blank = True , null = True , help_text = application = models . ForeignKey ( Application ) \n 
objects = managers . EngagementManager . from_queryset ( managers . EngagementQuerySet ) ( ) \n 
metrics = managers . EngagementMetrics . from_queryset ( managers . EngagementQuerySet ) ( ) \n 
if self . pk is not None : \n 
~~~ engagement = Engagement . objects . get ( pk = self . pk ) \n 
now = timezone . now ( ) \n 
if engagement . status != self . status : \n 
~~~ if self . status == Engagement . PENDING_STATUS : \n 
~~~ self . open_date = None \n 
self . close_date = None \n 
~~ elif self . status == Engagement . OPEN_STATUS : \n 
~~~ self . open_date = now \n 
~~ elif self . status == Engagement . CLOSED_STATUS : \n 
~~~ if self . open_date is None : \n 
~~ self . close_date = now \n 
~~ ~~ ~~ if self . open_date is not None and self . close_date is not None : \n 
~~~ self . duration = self . close_date - self . open_date \n 
~~ super ( Engagement , self ) . save ( * args , ** kwargs ) \n 
~~ def is_pending ( self ) : \n 
~~~ return self . status == Engagement . PENDING_STATUS \n 
~~ def is_open ( self ) : \n 
~~~ return self . status == Engagement . OPEN_STATUS \n 
~~ def is_closed ( self ) : \n 
~~~ return self . status == Engagement . CLOSED_STATUS \n 
~~ def is_ready_for_work ( self ) : \n 
if self . status == Engagement . PENDING_STATUS : \n 
~~~ if date . today ( ) >= self . start_date : \n 
~~ def is_past_due ( self ) : \n 
if self . status == Engagement . PENDING_STATUS or self . status == Engagement . OPEN_STATUS : \n 
~~~ if date . today ( ) > self . end_date : \n 
~~ ~~ class ActivityType ( TimeStampedModel , models . Model ) : \n 
name = models . CharField ( max_length = 128 , unique = True , help_text = _ ( documentation = models . TextField ( blank = True , help_text = _ ( \n 
objects = managers . ActivityTypeManager . from_queryset ( managers . ActivityTypeQuerySet ) ( ) \n 
metrics = managers . ActivityTypeMetrics . from_queryset ( managers . ActivityTypeQuerySet ) ( ) \n 
~~ ~~ class Activity ( models . Model ) : \n 
activity_type = models . ForeignKey ( ActivityType ) \n 
engagement = models . ForeignKey ( Engagement ) \n 
users = models . ManyToManyField ( settings . AUTH_USER_MODEL , blank = True ) \n 
objects = managers . ActivityManager . from_queryset ( managers . ActivityQuerySet ) ( ) \n 
~~~ return self . activity_type . name \n 
~~~ activity = Activity . objects . get ( pk = self . pk ) \n 
~~~ now = timezone . now ( ) \n 
if self . status == Activity . PENDING_STATUS : \n 
~~ elif self . status == Activity . OPEN_STATUS : \n 
if self . engagement . status is not Engagement . OPEN_STATUS : \n 
~~~ self . engagement . status = Engagement . OPEN_STATUS \n 
self . engagement . save ( ) \n 
~~ ~~ elif self . status == Activity . CLOSED_STATUS : \n 
for current_activity in self . engagement . activity_set . exclude ( id = self . id ) : \n 
~~~ if current_activity . status != Activity . CLOSED_STATUS : \n 
~~~ close = False \n 
~~ ~~ if close : \n 
~~~ self . engagement . status = Engagement . CLOSED_STATUS \n 
~~ ~~ ~~ ~~ if self . open_date is not None and self . close_date is not None : \n 
~~ super ( Activity , self ) . save ( * args , ** kwargs ) \n 
~~~ return self . status == Activity . PENDING_STATUS \n 
~~~ return self . status == Activity . OPEN_STATUS \n 
~~~ return self . status == Activity . CLOSED_STATUS \n 
~~~ if date . today ( ) >= self . engagement . start_date : \n 
if self . status == Activity . PENDING_STATUS or self . status == Activity . OPEN_STATUS : \n 
~~~ if date . today ( ) > self . engagement . end_date : \n 
~~ ~~ class Comment ( TimeStampedModel , models . Model ) : \n 
message = models . TextField ( ) \n 
user = models . ForeignKey ( settings . AUTH_USER_MODEL ) \n 
~~~ return self . message \n 
~~ class Meta : \n 
~~ ~~ class EngagementComment ( Comment ) : \n 
~~ class ActivityComment ( Comment ) : \n 
activity = models . ForeignKey ( Activity ) \n 
~~ class ExternalRequest ( TimeStampedModel , models . Model ) : \n 
token = models . UUIDField ( default = uuid . uuid4 , editable = False ) \n 
requestor = models . ForeignKey ( Person ) \n 
application = models . ForeignKey ( Application , blank = True ) \n 
activities = models . ManyToManyField ( ActivityType , limit_choices_to = { : True } ) \n 
~~ class FileUpload ( TimeStampedModel , models . Model ) : \n 
file = models . FileField ( ) \n 
~~ ~~ class ApplicationFileUpload ( FileUpload ) : \n 
REPORT_FILE_TYPE = \n 
DOCUMENTATION_FILE_TYPE = \n 
FILE_TYPE_CHOICES = ( \n 
( REPORT_FILE_TYPE , ) , \n 
( DOCUMENTATION_FILE_TYPE , ) , \n 
file_type = models . CharField ( max_length = 13 , choices = FILE_TYPE_CHOICES ) \n 
from time import sleep \n 
from flask import Flask \n 
from flask_tut . models import ( \n 
db , \n 
User , \n 
Address , \n 
app = Flask ( __name__ ) \n 
with app . app_context ( ) : \n 
~~~ db . create_all ( ) \n 
~~ i = 0 \n 
while i < 30 : \n 
~~~ address = Address ( description = + str ( i ) . rjust ( 2 , "0" ) ) \n 
db . session . add ( address ) \n 
user = User ( name = + str ( i ) . rjust ( 2 , "0" ) ) \n 
user . address = address \n 
db . session . add ( user ) \n 
sleep ( 1 ) \n 
~~ db . session . commit ( ) \n 
import google \n 
import pyprind \n 
class Crawler ( object ) : \n 
~~~ version = "1.2.3" \n 
outputDir = "output" \n 
languageDir = "languages" \n 
basicString = "/get.php?username=%s&password=%s&type=m3u&output=mpegts" \n 
def __init__ ( self , language = "it" ) : \n 
self . language = language . lower ( ) \n 
self . parsedUrls = [ ] \n 
self . foundedAccounts = 0 \n 
~~ def change_language ( self , language = "it" ) : \n 
if os . path . isfile ( self . languageDir + "/" + language + ".txt" ) : \n 
~~ ~~ def search_links ( self ) : \n 
for url in google . search ( self . searchString , num = 30 , stop = 1 ) : \n 
~~~ parsed = urlparse ( url ) \n 
self . parsedUrls . append ( parsed . scheme + "://" + parsed . netloc ) \n 
~~ ~~ def search_accounts ( self , url = None ) : \n 
if not self . parsedUrls : \n 
~~~ if not url : \n 
~~~ url = random . choice ( self . parsedUrls ) \n 
~~ fileName = self . languageDir + "/" + self . language + ".txt" \n 
fileLength = self . file_length ( fileName ) \n 
with open ( fileName ) as f : \n 
~~~ rows = f . readlines ( ) \n 
~~ for row in rows : \n 
~~~ opener = urllib2 . build_opener ( ) \n 
opener . addheaders = [ ( , ) ] \n 
response = opener . open ( url + self . basicString % ( row . rstrip ( ) . lstrip ( ) , row . rstrip ( ) fetched = response . read ( ) \n 
fileLength = fileLength - 1 \n 
progressBar . update ( ) \n 
if len ( fetched ) > 0 : \n 
~~~ newPath = self . outputDir + "/" + url . replace ( "http://" , "" ) \n 
self . create_file ( row , newPath , fetched ) \n 
~~ ~~ self . parsedUrls . remove ( url ) \n 
if self . foundedAccounts != 0 : \n 
~~ except urllib2 . HTTPError , e : \n 
~~ except urllib2 . URLError , e : \n 
~~ ~~ def create_file ( self , row , newPath , fetched ) : \n 
if os . path . exists ( newPath ) is False : \n 
~~~ os . makedirs ( newPath ) \n 
~~ outputFile = open ( str ( newPath ) + "/tv_channels_%s.m3u" % row . rstrip ( ) . lstrip ( ) , "w" ) \n 
outputFile . write ( fetched ) \n 
self . foundedAccounts = self . foundedAccounts + 1 \n 
outputFile . close ( ) \n 
~~ def file_length ( self , fileName ) : \n 
~~~ for i , l in enumerate ( f ) : \n 
~~ ~~ return i + 1 \n 
~~ ~~ from cStringIO import StringIO \n 
from binascii import b2a_hex \n 
from urllib import quote \n 
import Connecter \n 
~~~ True \n 
~~~ True = 1 \n 
False = 0 \n 
~~ DEBUG = False \n 
protocol_name = \n 
option_pattern = chr ( 0 ) * 8 \n 
def toint ( s ) : \n 
~~~ return long ( b2a_hex ( s ) , 16 ) \n 
~~ def tohex ( s ) : \n 
~~~ return b2a_hex ( s ) . upper ( ) \n 
~~ def make_readable ( s ) : \n 
~~~ if not s : \n 
~~ if quote ( s ) . find ( ) >= 0 : \n 
~~~ return tohex ( s ) \n 
~~ return \'"\' + s + \'"\' \n 
~~ streamno = 0 \n 
class StreamCheck : \n 
~~~ global streamno \n 
self . no = streamno \n 
streamno += 1 \n 
self . buffer = StringIO ( ) \n 
self . next_len , self . next_func = 1 , self . read_header_len \n 
~~ def read_header_len ( self , s ) : \n 
~~~ if ord ( s ) != len ( protocol_name ) : \n 
~~~ print self . no , \n 
~~ return len ( protocol_name ) , self . read_header \n 
~~ def read_header ( self , s ) : \n 
~~~ if s != protocol_name : \n 
~~ return 8 , self . read_reserved \n 
~~ def read_reserved ( self , s ) : \n 
~~~ return 20 , self . read_download_id \n 
~~ def read_download_id ( self , s ) : \n 
~~~ if DEBUG : \n 
~~~ print self . no , + tohex ( s ) \n 
~~ return 20 , self . read_peer_id \n 
~~ def read_peer_id ( self , s ) : \n 
~~~ print self . no , + make_readable ( s ) \n 
~~ return 4 , self . read_len \n 
~~ def read_len ( self , s ) : \n 
~~~ l = toint ( s ) \n 
if l > 2 ** 23 : \n 
~~~ print self . no , + str ( l ) + + s + \n 
~~ return l , self . read_message \n 
~~ def read_message ( self , s ) : \n 
~~~ return 4 , self . read_len \n 
~~ m = s [ 0 ] \n 
if ord ( m ) > 8 : \n 
~~~ print self . no , + str ( ord ( m ) ) \n 
~~ if m == Connecter . REQUEST : \n 
~~~ if len ( s ) != 13 : \n 
~~~ print self . no , + str ( len ( s ) ) \n 
return 4 , self . read_len \n 
~~ index = toint ( s [ 1 : 5 ] ) \n 
begin = toint ( s [ 5 : 9 ] ) \n 
length = toint ( s [ 9 : ] ) \n 
print self . no , + str ( index ) + + str ( begin ) + + str ( begin ) + + str ( length ) \n 
~~ elif m == Connecter . CANCEL : \n 
~~ elif m == Connecter . PIECE : \n 
~~~ index = toint ( s [ 1 : 5 ] ) \n 
length = len ( s ) - 9 \n 
~~~ print self . no , + str ( ord ( m ) ) + + str ( len ( s ) ) + \n 
~~ def write ( self , s ) : \n 
~~~ while 1 : \n 
~~~ i = self . next_len - self . buffer . tell ( ) \n 
if i > len ( s ) : \n 
~~~ self . buffer . write ( s ) \n 
~~ self . buffer . write ( s [ : i ] ) \n 
s = s [ i : ] \n 
m = self . buffer . getvalue ( ) \n 
self . buffer . reset ( ) \n 
self . buffer . truncate ( ) \n 
x = self . next_func ( m ) \n 
self . next_len , self . next_func = x \n 
~~ ~~ ~~ from types import * \n 
from cStringIO import StringIO \n 
def splitLine ( line , COLS = 80 , indent = 10 ) : \n 
width = COLS - ( len ( indent ) + 1 ) \n 
if indent and width < 15 : \n 
~~~ width = COLS - 2 \n 
~~ s = StringIO ( ) \n 
for word in line . split ( ) : \n 
~~~ if i == 0 : \n 
~~~ s . write ( indent + word ) \n 
i = len ( word ) \n 
~~ if i + len ( word ) >= width : \n 
~~~ s . write ( + indent + word ) \n 
~~ s . write ( + word ) \n 
i += len ( word ) + 1 \n 
~~ return s . getvalue ( ) \n 
~~ def formatDefinitions ( options , COLS , presets = { } ) : \n 
~~~ s = StringIO ( ) \n 
for ( longname , default , doc ) in options : \n 
~~~ s . write ( + longname + ) \n 
default = presets . get ( longname , default ) \n 
if type ( default ) in ( IntType , LongType ) : \n 
~~~ default = int ( default ) \n 
~~ ~~ if default is not None : \n 
~~~ doc += + repr ( default ) + \n 
~~ s . write ( splitLine ( doc , COLS , 10 ) ) \n 
s . write ( ) \n 
~~ def usage ( string ) : \n 
~~~ raise ValueError ( string ) \n 
~~ def defaultargs ( options ) : \n 
~~~ l = { } \n 
~~~ if default is not None : \n 
~~~ l [ longname ] = default \n 
~~ ~~ return l \n 
~~ def parseargs ( argv , options , minargs = None , maxargs = None , presets = { } ) : \n 
~~~ config = { } \n 
longkeyed = { } \n 
~~~ longname , default , doc = option \n 
longkeyed [ longname ] = option \n 
config [ longname ] = default \n 
~~~ config [ longname ] = presets [ longname ] \n 
~~ options = [ ] \n 
args = [ ] \n 
while pos < len ( argv ) : \n 
~~~ if argv [ pos ] [ : 2 ] != : \n 
~~~ args . append ( argv [ pos ] ) \n 
pos += 1 \n 
~~~ if pos == len ( argv ) - 1 : \n 
~~ key , value = argv [ pos ] [ 2 : ] , argv [ pos + 1 ] \n 
pos += 2 \n 
if not longkeyed . has_key ( key ) : \n 
~~~ usage ( + key ) \n 
~~ longname , default , doc = longkeyed [ key ] \n 
~~~ t = type ( config [ longname ] ) \n 
if t is NoneType or t is StringType : \n 
~~~ config [ longname ] = value \n 
~~ elif t in ( IntType , LongType ) : \n 
~~~ config [ longname ] = long ( value ) \n 
~~ elif t is FloatType : \n 
~~~ config [ longname ] = float ( value ) \n 
~~~ assert 0 \n 
~~ ~~ except ValueError , e : \n 
~~~ usage ( % ( key , str ( e ) ) ) \n 
~~ ~~ ~~ for key , value in config . items ( ) : \n 
~~ ~~ if minargs is not None and len ( args ) < minargs : \n 
~~ if maxargs is not None and len ( args ) > maxargs : \n 
~~ return ( config , args ) \n 
~~ def test_parseargs ( ) : \n 
~~~ assert parseargs ( ( , , , , , , , , ) , ( ( , , ) , ( , assert parseargs ( [ ] , [ ( , , ) ] ) == ( { : } , [ ] ) \n 
assert parseargs ( [ , , , ] , [ ( , , ) ] ) == ( { : } , [ ] ) \n 
~~~ parseargs ( [ ] , [ ( , , ) ] ) \n 
~~~ parseargs ( [ , ] , [ ] ) \n 
~~~ parseargs ( [ ] , [ ] , 1 , 2 ) \n 
~~ assert parseargs ( [ ] , [ ] , 1 , 2 ) == ( { } , [ ] ) \n 
assert parseargs ( [ , ] , [ ] , 1 , 2 ) == ( { } , [ , ] ) \n 
~~~ parseargs ( [ , , ] , [ ] , 1 , 2 ) \n 
~~~ parseargs ( [ , ] , [ ( , 3 , ) ] ) \n 
~~~ parseargs ( [ , ] , [ ( , 2.1 , ) ] ) \n 
from south . db import db \n 
from south . v2 import SchemaMigration \n 
class Migration ( SchemaMigration ) : \n 
~~~ db . add_column ( , , \n 
self . gf ( ) ( to = orm [ keep_default = False ) \n 
~~~ db . delete_column ( , ) \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } , \n 
: ( , [ ] , { : "\'taggit_taggeditem_tagged_items\'" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "\'taggit_taggeditem_items\'" } , \n 
: ( , [ ] , { : , : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : , } , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'videoportal.Video\']" } , \n 
: ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : "orm[\'videoportal.Video\']" } , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : "orm[\'auth.User\']" , : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : } \n 
from stat import S_IFDIR , S_IFREG , S_IFLNK \n 
from pygit2 import ( clone_repository , Signature , GIT_SORT_TOPOLOGICAL , \n 
GIT_FILEMODE_TREE , GIT_STATUS_CURRENT , \n 
GIT_FILEMODE_LINK , GIT_FILEMODE_BLOB , GIT_BRANCH_REMOTE , \n 
GIT_BRANCH_LOCAL , GIT_FILEMODE_BLOB_EXECUTABLE ) \n 
from six import iteritems \n 
from gitfs . cache import CommitCache \n 
from gitfs . log import log \n 
from gitfs . utils . path import split_path_into_components \n 
from gitfs . utils . commits import CommitsList \n 
DivergeCommits = namedtuple ( "DivergeCommits" , [ "common_parent" , \n 
"first_commits" , "second_commits" ] ) \n 
class Repository ( object ) : \n 
~~~ def __init__ ( self , repository , commits = None ) : \n 
~~~ self . _repo = repository \n 
self . commits = commits or CommitCache ( self ) \n 
self . behind = False \n 
~~ def __getitem__ ( self , item ) : \n 
return self . _repo [ item ] \n 
if attr not in self . __dict__ : \n 
~~~ return getattr ( self . _repo , attr ) \n 
~~~ return self . __dict__ [ attr ] \n 
~~ ~~ def ahead ( self , upstream , branch ) : \n 
~~~ ahead , _ = self . diverge ( upstream , branch ) \n 
return ahead \n 
~~ def diverge ( self , upstream , branch ) : \n 
~~~ reference = "{}/{}" . format ( upstream , branch ) \n 
remote_branch = self . lookup_branch ( reference , GIT_BRANCH_REMOTE ) \n 
local_branch = self . lookup_branch ( branch , GIT_BRANCH_LOCAL ) \n 
if remote_branch . target == local_branch . target : \n 
~~~ return False , False \n 
~~ diverge_commits = self . find_diverge_commits ( local_branch , \n 
remote_branch ) \n 
behind = len ( diverge_commits . second_commits ) > 0 \n 
ahead = len ( diverge_commits . first_commits ) > 0 \n 
return ahead , behind \n 
~~ def checkout ( self , ref , * args , ** kwargs ) : \n 
~~~ result = self . _repo . checkout ( ref , * args , ** kwargs ) \n 
self . ignore . update ( ) \n 
status = self . _repo . status ( ) \n 
for path , status in iteritems ( status ) : \n 
~~~ if status == GIT_STATUS_CURRENT : \n 
~~ full_path = self . _full_path ( path ) \n 
if path not in self . _repo . index : \n 
~~~ if path not in self . ignore : \n 
~~~ os . unlink ( full_path ) \n 
~~~ rmtree ( \n 
full_path , \n 
onerror = lambda function , fpath , excinfo : log . info ( \n 
~~ ~~ continue \n 
~~ stats = self . get_git_object_default_stats ( ref , path ) \n 
current_stat = os . lstat ( full_path ) \n 
if stats [ ] != current_stat . st_mode : \n 
~~~ os . chmod ( full_path , current_stat . st_mode ) \n 
self . _repo . index . add ( self . _sanitize ( path ) ) \n 
~~ def _sanitize ( self , path ) : \n 
~~~ if path is not None and path . startswith ( "/" ) : \n 
~~~ path = path [ 1 : ] \n 
~~ return path \n 
~~ def push ( self , upstream , branch , credentials ) : \n 
remote = self . get_remote ( upstream ) \n 
remote . push ( [ "refs/heads/%s" % ( branch ) ] , callbacks = credentials ) \n 
~~ def fetch ( self , upstream , branch_name , credentials ) : \n 
remote . fetch ( callbacks = credentials ) \n 
_ , behind = self . diverge ( upstream , branch_name ) \n 
self . behind = behind \n 
return behind \n 
~~ def commit ( self , message , author , commiter , parents = None , ref = "HEAD" ) : \n 
if status == { } : \n 
~~ author = Signature ( author [ 0 ] , author [ 1 ] ) \n 
commiter = Signature ( commiter [ 0 ] , commiter [ 1 ] ) \n 
tree = self . _repo . index . write_tree ( ) \n 
self . _repo . index . write ( ) \n 
if parents is None : \n 
~~~ parents = [ self . _repo . revparse_single ( ref ) . id ] \n 
~~ return self . _repo . create_commit ( ref , author , commiter , message , \n 
tree , parents ) \n 
def clone ( cls , remote_url , path , branch = None , credentials = None ) : \n 
repo = clone_repository ( remote_url , path , checkout_branch = branch , \n 
callbacks = credentials ) \n 
repo . checkout_head ( ) \n 
return cls ( repo ) \n 
~~ def _is_searched_entry ( self , entry_name , searched_entry , path_components ) : \n 
return ( entry_name == searched_entry and \n 
len ( path_components ) == 1 and \n 
entry_name == path_components [ 0 ] ) \n 
~~ def _get_git_object ( self , tree , obj_name , path_components , modifier ) : \n 
git_obj = None \n 
for entry in tree : \n 
~~~ if self . _is_searched_entry ( entry . name , obj_name , path_components ) : \n 
~~~ return modifier ( entry ) \n 
~~ elif entry . filemode == GIT_FILEMODE_TREE : \n 
~~~ git_obj = self . _get_git_object ( self . _repo [ entry . id ] , obj_name , \n 
path_components [ 1 : ] , modifier ) \n 
if git_obj : \n 
~~~ return git_obj \n 
~~ ~~ ~~ return git_obj \n 
~~ def get_git_object_type ( self , tree , path ) : \n 
path_components = split_path_into_components ( path ) \n 
~~~ return self . _get_git_object ( tree , path_components [ - 1 ] , \n 
path_components , \n 
lambda entry : entry . filemode ) \n 
~~~ return GIT_FILEMODE_TREE \n 
~~ ~~ def get_git_object ( self , tree , path ) : \n 
return self . _get_git_object ( tree , path_components [ - 1 ] , path_components , \n 
lambda entry : self . _repo [ entry . id ] ) \n 
~~ def get_git_object_default_stats ( self , ref , path ) : \n 
~~~ types = { \n 
GIT_FILEMODE_LINK : { \n 
: S_IFLNK | 0o444 , \n 
} , GIT_FILEMODE_TREE : { \n 
: S_IFDIR | 0o555 , \n 
: 2 \n 
} , GIT_FILEMODE_BLOB : { \n 
: S_IFREG | 0o444 , \n 
} , GIT_FILEMODE_BLOB_EXECUTABLE : { \n 
: S_IFREG | 0o555 , \n 
if path == "/" : \n 
~~~ return types [ GIT_FILEMODE_TREE ] \n 
~~ obj_type = self . get_git_object_type ( ref , path ) \n 
if obj_type is None : \n 
~~~ return obj_type \n 
~~ stats = types [ obj_type ] \n 
if obj_type in [ GIT_FILEMODE_BLOB , GIT_FILEMODE_BLOB_EXECUTABLE ] : \n 
~~~ stats [ ] = self . get_blob_size ( ref , path ) \n 
~~ return stats \n 
~~ def get_blob_size ( self , tree , path ) : \n 
return self . get_git_object ( tree , path ) . size \n 
~~ def get_blob_data ( self , tree , path ) : \n 
return self . get_git_object ( tree , path ) . data \n 
~~ def get_commit_dates ( self ) : \n 
return list ( self . commits . keys ( ) ) \n 
~~ def get_commits_by_date ( self , date ) : \n 
return list ( map ( str , self . commits [ date ] ) ) \n 
~~ def walk_branches ( self , sort , * branches ) : \n 
iterators = [ self . _repo . walk ( branch . target , sort ) \n 
for branch in branches ] \n 
stop_iteration = [ False for branch in branches ] \n 
commits = [ ] \n 
for iterator in iterators : \n 
~~~ commit = next ( iterator ) \n 
~~~ commit = None \n 
~~ commits . append ( commit ) \n 
~~ yield ( commit for commit in commits ) \n 
while not all ( stop_iteration ) : \n 
~~~ for index , iterator in enumerate ( iterators ) : \n 
commits [ index ] = commit \n 
~~~ stop_iteration [ index ] = True \n 
~~ ~~ if not all ( stop_iteration ) : \n 
~~~ yield ( commit for commit in commits ) \n 
~~ ~~ ~~ def remote_head ( self , upstream , branch ) : \n 
~~~ ref = "%s/%s" % ( upstream , branch ) \n 
remote = self . _repo . lookup_branch ( ref , GIT_BRANCH_REMOTE ) \n 
return remote . get_object ( ) \n 
~~ def get_remote ( self , name ) : \n 
remote = [ remote for remote in self . _repo . remotes \n 
if remote . name == name ] \n 
if not remote : \n 
~~ return remote [ 0 ] \n 
~~ def _full_path ( self , partial ) : \n 
~~~ if partial . startswith ( "/" ) : \n 
~~~ partial = partial [ 1 : ] \n 
~~ return os . path . join ( self . _repo . workdir , partial ) \n 
~~ def find_diverge_commits ( self , first_branch , second_branch ) : \n 
common_parent = None \n 
first_commits = CommitsList ( ) \n 
second_commits = CommitsList ( ) \n 
walker = self . walk_branches ( GIT_SORT_TOPOLOGICAL , \n 
first_branch , second_branch ) \n 
for first_commit , second_commit in walker : \n 
~~~ if ( first_commit in second_commits or \n 
second_commit in first_commits ) : \n 
~~ if first_commit not in first_commits : \n 
~~~ first_commits . append ( first_commit ) \n 
~~ if second_commit not in second_commits : \n 
~~~ second_commits . append ( second_commit ) \n 
~~ if second_commit . hex == first_commit . hex : \n 
~~~ index = second_commits . index ( first_commit ) \n 
~~~ second_commits = second_commits [ : index ] \n 
common_parent = first_commit \n 
~~~ index = first_commits . index ( second_commit ) \n 
~~~ first_commits = first_commits [ : index ] \n 
common_parent = second_commit \n 
~~ return DivergeCommits ( common_parent , first_commits , second_commits ) \n 
~~ ~~ from datetime import datetime \n 
from mock import MagicMock , call \n 
from pygit2 import GIT_SORT_TIME \n 
from gitfs . cache . commits import Commit , CommitCache \n 
class TestCommit ( object ) : \n 
~~~ def test_commit ( self ) : \n 
~~~ commit = Commit ( 1 , 1 , 1 ) \n 
new_commit = Commit ( 2 , 2 , "21111111111" ) \n 
assert new_commit > commit \n 
assert repr ( new_commit ) == "2-2111111111" \n 
~~ ~~ class TestCommitCache ( object ) : \n 
~~~ def test_cache ( self ) : \n 
~~~ mocked_repo = MagicMock ( ) \n 
mocked_commit = MagicMock ( ) \n 
mocked_repo . lookup_reference ( ) . resolve ( ) . target = "head" \n 
mocked_repo . walk . return_value = [ mocked_commit ] \n 
mocked_commit . commit_time = 1411135000 \n 
mocked_commit . hex = \n 
cache = CommitCache ( mocked_repo ) \n 
cache . update ( ) \n 
cache [ ] = Commit ( 1 , 1 , "1111111111" ) \n 
assert sorted ( cache . keys ( ) ) == [ , ] \n 
asserted_time = datetime . fromtimestamp ( mocked_commit . commit_time ) \n 
asserted_time = "{}-{}-{}" . format ( asserted_time . hour , asserted_time . minute , \n 
asserted_time . second ) \n 
assert repr ( cache [ ] ) == % asserted_time \n 
del cache [ ] \n 
for commit_date in cache : \n 
~~~ assert commit_date == \n 
~~ mocked_repo . lookup_reference . has_calls ( [ call ( "HEAD" ) ] ) \n 
mocked_repo . walk . assert_called_once_with ( "head" , GIT_SORT_TIME ) \n 
assert mocked_repo . lookup_reference ( ) . resolve . call_count == 2 \n 
import datetime as dt \n 
from mock import MagicMock \n 
from gitfs . utils . strptime import TimeParser \n 
from gitfs . utils import strptime \n 
class TestDateTimeUtils ( object ) : \n 
~~~ def test_strptime ( self ) : \n 
~~~ date = dt . date ( 2014 , 8 , 21 ) \n 
datetime = dt . datetime ( 2014 , 8 , 21 , 1 , 2 , 3 ) \n 
to_datetime = True ) == datetime \n 
date = dt . date ( 2014 , 8 , 30 ) \n 
datetime = dt . datetime ( 2014 , 8 , 30 , 1 , 2 , 3 ) \n 
date = dt . date ( 1970 , 1 , 1 ) \n 
datetime = dt . datetime ( 1970 , 1 , 1 , 13 , 30 ) \n 
~~ ~~ def test_time_parser_match_with_value_error ( self ) : \n 
~~~ mocked_pattern = MagicMock ( ) \n 
mocked_pattern . match . return_value = False \n 
parser . pattern = mocked_pattern \n 
~~~ parser . match ( "daytime" ) \n 
~~ mocked_pattern . match . assert_called_once_with ( "daytime" ) \n 
from django . db import migrations \n 
import jsonfield . fields \n 
field = jsonfield . fields . JSONField ( null = True , blank = True ) , \n 
~~ from zipa import api_github_com as github \n 
repos = github . orgs . django . repos \n 
for repo in repos [ { : , : } ] : \n 
~~~ print repo . name \n 
~~ import supybot . conf as conf \n 
import supybot . registry as registry \n 
from supybot . i18n import PluginInternationalization , internationalizeDocstring \n 
_ = PluginInternationalization ( ) \n 
def configure ( advanced ) : \n 
~~~ from supybot . questions import expect , anything , something , yn \n 
conf . registerPlugin ( , True ) \n 
~~ Alias = conf . registerPlugin ( ) \n 
conf . registerGroup ( Alias , ) \n 
conf . registerGlobalValue ( Alias , , \n 
import supybot . conf as conf \n 
~~~ from supybot . i18n import PluginInternationalization \n 
from supybot . i18n import internationalizeDocstring \n 
~~~ _ = lambda x : x \n 
internationalizeDocstring = lambda x : x \n 
~~ def configure ( advanced ) : \n 
~~ Conditional = conf . registerPlugin ( ) \n 
Filter = conf . registerPlugin ( ) \n 
conf . registerGroup ( Filter , ) \n 
conf . registerGlobalValue ( Filter . spellit , \n 
conf . registerChannelValue ( Filter . shrink , , \n 
~~ Karma = conf . registerPlugin ( ) \n 
conf . registerChannelValue ( Karma , , \n 
import supybot \n 
import supybot . world as world \n 
__version__ = "0.1" \n 
__author__ = supybot . authors . strike \n 
__contributors__ = { } \n 
from . import config \n 
from . import plugin \n 
from imp import reload \n 
reload ( plugin ) \n 
if world . testing : \n 
~~~ from . import test \n 
~~ Class = plugin . Class \n 
configure = config . configure \n 
__version__ = "%%VERSION%%" \n 
__author__ = supybot . authors . jemfinch \n 
__url__ = \n 
import errno \n 
import select \n 
from . . import ( conf , drivers , log , utils , world ) \n 
from . . utils import minisix \n 
from . . utils . str import decode_raw_line \n 
SSLError = ssl . SSLError \n 
~~~ drivers . log . debug ( \n 
class SSLError ( Exception ) : \n 
~~ ~~ class SocketDriver ( drivers . IrcDriver , drivers . ServersMixin ) : \n 
~~~ _instances = [ ] \n 
def __init__ ( self , irc ) : \n 
~~~ self . _instances . append ( self ) \n 
assert irc is not None \n 
self . irc = irc \n 
drivers . IrcDriver . __init__ ( self , irc ) \n 
drivers . ServersMixin . __init__ ( self , irc ) \n 
self . conn = None \n 
self . _attempt = - 1 \n 
self . servers = ( ) \n 
self . eagains = 0 \n 
self . inbuffer = \n 
self . outbuffer = \n 
self . zombie = False \n 
self . connected = False \n 
self . writeCheckTime = None \n 
self . nextReconnectTime = None \n 
self . resetDelay ( ) \n 
if self . networkGroup . get ( ) . value and not in globals ( ) : \n 
~~~ drivers . log . error ( \n 
self . ssl = False \n 
~~~ self . ssl = self . networkGroup . get ( ) . value \n 
self . connect ( ) \n 
~~ ~~ def getDelay ( self ) : \n 
~~~ ret = self . currentDelay \n 
self . currentDelay = min ( self . currentDelay * 2 , \n 
conf . supybot . drivers . maxReconnectWait ( ) ) \n 
~~ def resetDelay ( self ) : \n 
~~~ self . currentDelay = 10.0 \n 
~~ def _getNextServer ( self ) : \n 
~~~ oldServer = getattr ( self , , None ) \n 
server = drivers . ServersMixin . _getNextServer ( self ) \n 
if self . currentServer != oldServer : \n 
~~~ self . resetDelay ( ) \n 
~~ return server \n 
~~ def _handleSocketError ( self , e ) : \n 
~~~ if e . args [ 0 ] != 11 or self . eagains > 120 : \n 
~~~ drivers . log . disconnect ( self . currentServer , e ) \n 
if self in self . _instances : \n 
~~~ self . _instances . remove ( self ) \n 
~~~ self . conn . close ( ) \n 
~~ self . connected = False \n 
self . scheduleReconnect ( ) \n 
~~~ log . debug ( , self . eagains ) \n 
self . eagains += 1 \n 
~~ ~~ def _sendIfMsgs ( self ) : \n 
~~~ if not self . connected : \n 
~~ if not self . zombie : \n 
~~~ msgs = [ self . irc . takeMsg ( ) ] \n 
while msgs [ - 1 ] is not None : \n 
~~~ msgs . append ( self . irc . takeMsg ( ) ) \n 
~~ del msgs [ - 1 ] \n 
self . outbuffer += . join ( map ( str , msgs ) ) \n 
~~ if self . outbuffer : \n 
~~~ if minisix . PY2 : \n 
~~~ sent = self . conn . send ( self . outbuffer ) \n 
~~~ sent = self . conn . send ( self . outbuffer . encode ( ) ) \n 
~~ self . outbuffer = self . outbuffer [ sent : ] \n 
~~ except socket . error as e : \n 
~~~ self . _handleSocketError ( e ) \n 
~~ ~~ if self . zombie and not self . outbuffer : \n 
~~~ self . _reallyDie ( ) \n 
def _select ( cls ) : \n 
~~~ if cls . _selecting [ 0 ] : \n 
~~~ cls . _selecting [ 0 ] = True \n 
for inst in cls . _instances : \n 
~~~ if not inst . connected or ( minisix . PY3 and inst . conn . _closed ) or ( minisix . PY2 and \n 
inst . conn . _sock . __class__ is socket . _closedsocket ) : \n 
~~~ cls . _instances . remove ( inst ) \n 
~~ elif inst . conn . fileno ( ) == - 1 : \n 
~~~ inst . reconnect ( ) \n 
~~ ~~ if not cls . _instances : \n 
~~ rlist , wlist , xlist = select . select ( [ x . conn for x in cls . _instances ] , \n 
[ ] , [ ] , conf . supybot . drivers . poll ( ) ) \n 
for instance in cls . _instances : \n 
~~~ if instance . conn in rlist : \n 
~~~ instance . _read ( ) \n 
~~ ~~ ~~ except select . error as e : \n 
~~~ if e . args [ 0 ] != errno . EINTR : \n 
~~~ cls . _selecting [ 0 ] = False \n 
~~ for instance in cls . _instances : \n 
~~~ if instance . irc and not instance . irc . zombie : \n 
~~~ instance . _sendIfMsgs ( ) \n 
~~ ~~ ~~ def run ( self ) : \n 
~~~ now = time . time ( ) \n 
if self . nextReconnectTime is not None and now > self . nextReconnectTime : \n 
~~~ self . reconnect ( ) \n 
~~ elif self . writeCheckTime is not None and now > self . writeCheckTime : \n 
~~~ self . _checkAndWriteOrReconnect ( ) \n 
~~ if not self . connected : \n 
~~~ time . sleep ( conf . supybot . drivers . poll ( ) ) \n 
~~ self . _sendIfMsgs ( ) \n 
self . _select ( ) \n 
~~ def _read ( self ) : \n 
~~~ self . inbuffer += self . conn . recv ( 1024 ) \n 
lines = self . inbuffer . split ( ) \n 
self . inbuffer = lines . pop ( ) \n 
for line in lines : \n 
~~~ line = decode_raw_line ( line ) \n 
msg = drivers . parseMsg ( line ) \n 
if msg is not None and self . irc is not None : \n 
~~~ self . irc . feedMsg ( msg ) \n 
~~ ~~ ~~ except socket . timeout : \n 
~~ except SSLError as e : \n 
~~~ if e . args [ 0 ] == : \n 
~~ ~~ except socket . error as e : \n 
~~ if self . irc and not self . irc . zombie : \n 
~~~ self . _sendIfMsgs ( ) \n 
~~ ~~ def connect ( self , ** kwargs ) : \n 
~~~ self . reconnect ( reset = False , ** kwargs ) \n 
~~ def reconnect ( self , wait = False , reset = True ) : \n 
~~~ self . _attempt += 1 \n 
if self . connected : \n 
~~~ drivers . log . reconnect ( self . irc . network ) \n 
~~~ self . conn . shutdown ( socket . SHUT_RDWR ) \n 
~~ self . conn . close ( ) \n 
~~ if reset : \n 
~~~ drivers . log . debug ( , self . irc ) \n 
self . irc . reset ( ) \n 
~~ if wait : \n 
~~~ self . scheduleReconnect ( ) \n 
~~ self . server = self . _getNextServer ( ) \n 
network_config = getattr ( conf . supybot . networks , self . irc . network ) \n 
socks_proxy = network_config . socksproxy ( ) \n 
~~~ if socks_proxy : \n 
~~~ import socks \n 
~~ ~~ except ImportError : \n 
~~~ log . error ( \n 
socks_proxy = \n 
~~ if socks_proxy : \n 
~~~ address = self . server [ 0 ] \n 
~~~ address = utils . net . getAddressFromHostname ( self . server [ 0 ] , \n 
attempt = self . _attempt ) \n 
~~ except ( socket . gaierror , socket . error ) as e : \n 
~~~ drivers . log . connectError ( self . currentServer , e ) \n 
~~ ~~ port = self . server [ 1 ] \n 
drivers . log . connect ( self . currentServer ) \n 
~~~ self . conn = utils . net . getSocket ( address , port = port , \n 
socks_proxy = socks_proxy , \n 
vhost = conf . supybot . protocols . irc . vhost ( ) , \n 
vhostv6 = conf . supybot . protocols . irc . vhostv6 ( ) , \n 
~~ self . conn . settimeout ( max ( 10 , conf . supybot . drivers . poll ( ) * 10 ) ) \n 
~~~ self . conn . connect ( ( address , port ) ) \n 
if network_config . ssl ( ) : \n 
~~~ self . starttls ( ) \n 
~~ elif not network_config . requireStarttls ( ) : \n 
~~~ drivers . log . warning ( ( \n 
% self . irc . network ) \n 
~~ def setTimeout ( ) : \n 
~~~ self . conn . settimeout ( conf . supybot . drivers . poll ( ) ) \n 
~~ conf . supybot . drivers . poll . addCallback ( setTimeout ) \n 
setTimeout ( ) \n 
self . connected = True \n 
~~~ if e . args [ 0 ] == 115 : \n 
when = now + 60 \n 
whenS = log . timestamp ( when ) \n 
drivers . log . debug ( \n 
, whenS ) \n 
self . writeCheckTime = when \n 
~~ self . _instances . append ( self ) \n 
~~ def _checkAndWriteOrReconnect ( self ) : \n 
~~~ self . writeCheckTime = None \n 
drivers . log . debug ( ) \n 
( _ , w , _ ) = select . select ( [ ] , [ self . conn ] , [ ] , 0 ) \n 
if w : \n 
~~~ drivers . log . debug ( ) \n 
~~~ drivers . log . connectError ( self . currentServer , ) \n 
self . reconnect ( ) \n 
~~ ~~ def scheduleReconnect ( self ) : \n 
~~~ when = time . time ( ) + self . getDelay ( ) \n 
if not world . dying : \n 
~~~ drivers . log . reconnect ( self . irc . network , when ) \n 
~~ if self . nextReconnectTime : \n 
~~ self . nextReconnectTime = when \n 
~~ def die ( self ) : \n 
~~~ if self in self . _instances : \n 
~~ self . zombie = True \n 
if self . nextReconnectTime is not None : \n 
~~~ self . nextReconnectTime = None \n 
~~ if self . writeCheckTime is not None : \n 
~~ drivers . log . die ( self . irc ) \n 
~~ def _reallyDie ( self ) : \n 
~~~ if self . conn is not None : \n 
~~ drivers . IrcDriver . die ( self ) \n 
~~~ return % ( self . __class__ . __name__ , self . irc ) \n 
~~ def starttls ( self ) : \n 
~~~ assert in globals ( ) \n 
certfile = network_config . certfile ( ) \n 
if not certfile : \n 
~~~ certfile = conf . supybot . protocols . irc . certfile ( ) \n 
~~ if not certfile : \n 
~~~ certfile = None \n 
~~ elif not os . path . isfile ( certfile ) : \n 
~~~ drivers . log . warning ( % \n 
certfile ) \n 
certfile = None \n 
~~ verifyCertificates = conf . supybot . protocols . ssl . verifyCertificates ( ) \n 
if not verifyCertificates : \n 
~~~ drivers . log . warning ( \n 
~~~ self . conn = utils . net . ssl_wrap_socket ( self . conn , \n 
logger = drivers . log , hostname = self . server [ 0 ] , \n 
certfile = certfile , \n 
verify = verifyCertificates , \n 
trusted_fingerprints = network_config . ssl . serverFingerprints ( ) , \n 
ca_file = network_config . ssl . authorityCertificate ( ) , \n 
~~ except getattr ( ssl , , None ) as e : \n 
~~~ drivers . log . error ( ( \n 
% ( self . irc . network , e . args [ 0 ] ) ) \n 
raise ssl . SSLError ( \n 
~~ except ssl . SSLError as e : \n 
% ( self . irc . network , e . args [ 1 ] ) ) \n 
~~ ~~ ~~ Driver = SocketDriver \n 
import fnmatch \n 
def universalImport ( * names ) : \n 
f = sys . _getframe ( 1 ) \n 
~~~ ret = __import__ ( name , f . f_globals ) \n 
~~~ if in name : \n 
~~~ parts = name . split ( ) [ 1 : ] \n 
while parts : \n 
~~~ ret = getattr ( ret , parts [ 0 ] ) \n 
del parts [ 0 ] \n 
~~ ~~ raise ImportError ( . join ( names ) ) \n 
~~ def changeFunctionName ( f , name , doc = None ) : \n 
~~~ if doc is None : \n 
~~~ doc = f . __doc__ \n 
~~ if hasattr ( f , ) : \n 
~~~ closure = f . __closure__ \n 
~~~ closure = f . func_closure \n 
~~ newf = types . FunctionType ( f . __code__ , f . __globals__ , name , \n 
f . __defaults__ , closure ) \n 
newf . __doc__ = doc \n 
return newf \n 
~~ class Object ( object ) : \n 
~~~ def __ne__ ( self , other ) : \n 
~~~ return not self == other \n 
~~ ~~ class MetaSynchronized ( type ) : \n 
~~~ METHODS = \n 
LOCK = \n 
def __new__ ( cls , name , bases , dict ) : \n 
~~~ sync = set ( ) \n 
~~~ if hasattr ( base , MetaSynchronized . METHODS ) : \n 
~~~ sync . update ( getattr ( base , MetaSynchronized . METHODS ) ) \n 
~~ ~~ if MetaSynchronized . METHODS in dict : \n 
~~~ sync . update ( dict [ MetaSynchronized . METHODS ] ) \n 
~~ if sync : \n 
~~~ def synchronized ( f ) : \n 
~~~ def g ( self , * args , ** kwargs ) : \n 
~~~ lock = getattr ( self , MetaSynchronized . LOCK ) \n 
lock . acquire ( ) \n 
~~~ f ( self , * args , ** kwargs ) \n 
~~~ lock . release ( ) \n 
~~ ~~ return changeFunctionName ( g , f . __name__ , f . __doc__ ) \n 
~~ for attr in sync : \n 
~~~ if attr in dict : \n 
~~~ dict [ attr ] = synchronized ( dict [ attr ] ) \n 
~~ ~~ original__init__ = dict . get ( ) \n 
~~~ if not hasattr ( self , MetaSynchronized . LOCK ) : \n 
~~~ setattr ( self , MetaSynchronized . LOCK , threading . RLock ( ) ) \n 
~~ if original__init__ : \n 
~~~ original__init__ ( self , * args , ** kwargs ) \n 
~~~ super ( newclass , self ) . __init__ ( * args , ** kwargs ) \n 
~~ ~~ dict [ ] = __init__ \n 
~~ newclass = super ( MetaSynchronized , cls ) . __new__ ( cls , name , bases , dict ) \n 
return newclass \n 
~~ ~~ Synchronized = MetaSynchronized ( , ( ) , { } ) \n 
def glob2re ( g ) : \n 
~~~ return fnmatch . translate ( g ) [ : - 7 ] \n 
~~ _debug_software_name = \n 
_debug_software_version = None \n 
def collect_extra_debug_data ( ) : \n 
data = \n 
~~~ tb = sys . exc_info ( ) [ 2 ] \n 
stack = [ ] \n 
while tb : \n 
~~~ stack . append ( tb . tb_frame ) \n 
tb = tb . tb_next \n 
~~~ del tb \n 
~~ if _debug_software_version : \n 
~~~ data += % ( _debug_software_name , _debug_software_version ) \n 
~~~ data += % _debug_software_name \n 
~~ data += \n 
for frame in stack : \n 
~~~ data += \n 
data += ( % ( frame . f_code . co_name , \n 
frame . f_code . co_filename , \n 
frame . f_lineno ) ) \n 
frame_locals = frame . f_locals \n 
for inspected in ( , ) : \n 
~~~ if inspected in frame_locals : \n 
~~~ if hasattr ( frame_locals [ inspected ] , ) and frame_locals [ inspected ] . __dict__ : \n 
~~~ for ( key , value ) in frame_locals [ inspected ] . __dict__ . items ( ) : \n 
~~~ frame_locals [ % ( inspected , key ) ] = value \n 
~~ ~~ ~~ ~~ for key , value in frame_locals . items ( ) : \n 
~~~ if key == : \n 
~~ data += ( % key ) \n 
~~~ data += repr ( value ) + \n 
~~ ~~ ~~ data += \n 
data += \n 
~~ import sys , os \n 
~~~ from django . conf . urls import patterns , url \n 
~~ urlpatterns = patterns ( , \n 
url ( , , name = ) , ) \n 
from django . http import * \n 
from django . core import serializers \n 
from django . core . exceptions import ValidationError , SuspiciousOperation , ObjectDoesNotExist \n 
from django . db import IntegrityError , connection , transaction \n 
from django . core . context_processors import csrf \n 
from django . contrib . comments . models import Comment \n 
from django . contrib . comments . forms import CommentForm \n 
from django . contrib . contenttypes . models import ContentType \n 
from django . contrib . auth . decorators import login_required , user_passes_test \n 
from django . contrib . sessions . models import Session \n 
from django . contrib . sessions . backends . db import SessionStore \n 
from django . contrib . gis . geos . collections import MultiPolygon \n 
from django . contrib . gis . geos import GEOSGeometry \n 
from django . contrib . gis . gdal import * \n 
from django . contrib . gis . gdal . libgdal import lgdal \n 
from django . contrib import humanize \n 
from django . template import loader , Context as DjangoContext , RequestContext \n 
from django . utils import simplejson as json , translation \n 
from django . utils . translation import ugettext as _ , ungettext as _n \n 
from django . template . defaultfilters import slugify , force_escape \n 
from tagging . utils import parse_tag_input \n 
from datetime import datetime , time , timedelta \n 
from decimal import * \n 
from functools import wraps \n 
from redistricting . calculators import * \n 
from redistricting . models import * \n 
from redistricting . tasks import * \n 
import random , string , math , types , copy , time , threading , traceback , os \n 
import commands , sys , tempfile , csv , hashlib , inflect , logging \n 
import ModestMaps \n 
from PIL import Image , ImageChops , ImageMath \n 
import urllib , urllib2 \n 
from xhtml2pdf . pisa import CreatePDF \n 
import StringIO \n 
UNASSIGNED_DISTRICT_ID = 0 \n 
def using_unique_session ( u ) : \n 
if u . is_anonymous ( ) or u . is_superuser : \n 
~~ sessions = Session . objects . all ( ) \n 
for session in sessions : \n 
~~~ decoded = session . get_decoded ( ) \n 
if in decoded and decoded [ ] == u . id : \n 
~~~ if in decoded and decoded [ ] < datetime . now ( ) : \n 
~~~ Session . objects . filter ( session_key = session . session_key ) . delete ( ) \n 
~~~ count += 1 \n 
~~ ~~ ~~ except SuspiciousOperation : \n 
~~ ~~ for session in sessions : \n 
~~~ websession = SessionStore ( session_key = session . session_key ) \n 
websession [ ] = count \n 
websession . save ( ) \n 
~~ ~~ except SuspiciousOperation : \n 
~~ ~~ return ( count <= 1 ) \n 
~~ def unique_session_or_json_redirect ( function ) : \n 
def decorator ( request , * args , ** kwargs ) : \n 
~~~ def return_nonunique_session_result ( ) : \n 
~~~ status = { : False } \n 
status [ ] = _ ( \n 
status [ ] = \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
~~ if not using_unique_session ( request . user ) : \n 
~~~ return return_nonunique_session_result ( ) \n 
~~~ return function ( request , * args , ** kwargs ) \n 
~~ ~~ return wraps ( function ) ( decorator ) \n 
~~ def is_session_available ( req ) : \n 
if req . user . is_superuser or req . user . is_staff : \n 
~~ sessions = Session . objects . filter ( expire_date__gt = datetime . now ( ) ) \n 
if ( not req . user . is_anonymous ( ) ) and in decoded and decoded [ ~~~ count += 1 \n 
~~ ~~ avail = count < settings . CONCURRENT_SESSIONS \n 
req . session [ ] = avail \n 
return avail \n 
~~ def note_session_activity ( req ) : \n 
window = timedelta ( 0 , 0 , 0 , 0 , settings . SESSION_TIMEOUT ) \n 
req . session [ ] = datetime . now ( ) + window \n 
def unloadplan ( request , planid ) : \n 
note_session_activity ( request ) \n 
status = { : False } \n 
ps = Plan . objects . filter ( pk = planid ) \n 
if len ( ps ) > 0 : \n 
~~~ p = ps [ 0 ] \n 
if not can_copy ( request . user , p ) : \n 
~~ if settings . MAX_UNDOS_AFTER_EDIT > 0 : \n 
~~~ p . purge_beyond_nth_step ( settings . MAX_UNDOS_AFTER_EDIT ) \n 
~~ ~~ status [ ] = True \n 
@ unique_session_or_json_redirect \n 
def copyplan ( request , planid ) : \n 
if not is_plan_ready ( planid ) : \n 
~~ status = { : False } \n 
p = Plan . objects . get ( pk = planid ) \n 
if ( request . method == "POST" ) : \n 
~~~ newname = request . POST [ "name" ] [ 0 : 200 ] \n 
shared = request . POST . get ( "shared" , False ) \n 
~~ plan_copy = Plan . objects . filter ( name = newname , owner = request . user , legislative_body = p . legislative_body \n 
if len ( plan_copy ) > 0 : \n 
~~ plan_copy = Plan ( name = newname , owner = request . user , is_shared = shared , legislative_body = p . legislative_body plan_copy . create_unassigned = False \n 
plan_copy . save ( ) \n 
districts = p . get_districts_at_version ( p . version , include_geom = True ) \n 
for district in districts : \n 
~~~ district_copy = copy . copy ( district ) \n 
district_copy . id = None \n 
district_copy . version = 0 \n 
district_copy . is_locked = False \n 
district_copy . plan = plan_copy \n 
~~~ district_copy . save ( ) \n 
~~ except Exception as inst : \n 
status [ "exception" ] = inst . message \n 
~~ district_copy . clone_relations_from ( district ) \n 
~~ data = serializers . serialize ( "json" , [ plan_copy ] ) \n 
return HttpResponse ( data , mimetype = ) \n 
def scoreplan ( request , planid ) : \n 
plan = Plan . objects . get ( pk = planid ) \n 
criterion = ValidationCriteria . objects . filter ( legislative_body = plan . legislative_body ) \n 
status [ ] = True \n 
for criteria in criterion : \n 
~~~ score = ComputedPlanScore . compute ( criteria . function , plan ) \n 
~~~ logger . debug ( traceback . format_exc ( ) ) \n 
~~ if not score or not score [ ] : \n 
~~~ status [ ] = False \n 
status [ ] = % ( criteria . get_short_label ( ) , criteria . get_long_description break \n 
~~ ~~ if status [ ] : \n 
~~~ status [ ] = True \n 
plan . is_valid = True \n 
plan . save ( ) \n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
~~ def get_user_info ( user ) : \n 
if user . is_anonymous ( ) : \n 
~~ profile = user . get_profile ( ) \n 
: user . username , \n 
: user . email , \n 
: profile . pass_hint , \n 
: user . first_name , \n 
: user . last_name , \n 
: profile . organization , \n 
: user . id \n 
~~ def commonplan ( request , planid ) : \n 
plan = Plan . objects . filter ( id = planid ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
plan . edited = getutc ( plan . edited ) \n 
levels = plan . legislative_body . get_geolevels ( ) \n 
districts = plan . get_districts_at_version ( plan . version , include_geom = False ) \n 
editable = can_edit ( request . user , plan ) \n 
default_demo = plan . legislative_body . get_default_subject ( ) \n 
max_dists = plan . legislative_body . max_districts \n 
body_member_short_label = plan . legislative_body . get_short_label ( ) \n 
body_member_long_label = plan . legislative_body . get_label ( ) \n 
body_members = plan . legislative_body . get_members_label ( ) \n 
reporting_template = % plan . legislative_body . name if not plan . is_community ( ) \n 
index = body_member_short_label . find ( ) \n 
if index >= 0 : \n 
~~~ body_member_short_label = body_member_short_label [ 0 : index ] \n 
~~ index = body_member_long_label . find ( ) \n 
~~~ body_member_long_label = body_member_long_label [ 0 : index ] \n 
~~ if not editable and not can_view ( request . user , plan ) : \n 
~~~ plan = { } \n 
tags = [ ] \n 
calculator_reports = [ ] \n 
~~~ tags = Tag . objects . filter ( name__startswith = ) . order_by ( ) . values_list ( , flat tags = map ( lambda x : x [ 5 : ] , tags ) \n 
if settings . REPORTS_ENABLED == : \n 
~~~ report_displays = ScoreDisplay . objects . filter ( name = "%s_reports" % plan . legislative_body if len ( report_displays ) > 0 : \n 
~~~ calculator_reports = map ( lambda p : { \n 
: p . __unicode__ ( ) , \n 
: map ( lambda f : { \n 
: f . get_label ( ) , \n 
: f . id \n 
} , p . score_functions . all ( ) . filter ( selectable_bodies = plan . legislative_body } , report_displays [ 0 ] . scorepanel_set . all ( ) . order_by ( ) ) \n 
levels = list ( ) \n 
districts = { } \n 
editable = False \n 
default_demo = None \n 
max_dists = 0 \n 
body_member_short_label = \n 
body_member_long_label = _ ( ) + \n 
body_members = _n ( , , 2 ) \n 
reporting_template = None \n 
~~ demos = Subject . objects . all ( ) . order_by ( ) [ 0 : 3 ] \n 
layers = [ ] \n 
snaplayers = [ ] \n 
if len ( levels ) > 0 : \n 
~~~ study_area_extent = list ( levels [ 0 ] . geounit_set . extent ( field_name = ) ) \n 
~~~ for lb in LegislativeBody . objects . all ( ) : \n 
~~~ biglevel = lb . get_geolevels ( ) [ 0 ] \n 
if biglevel . geounit_set . count ( ) > 0 : \n 
~~~ study_area_extent = biglevel . geounit_set . extent ( field_name = ) \n 
~~ ~~ ~~ for level in levels : \n 
~~~ snaplayers . append ( { \n 
: level . id , \n 
: level . name , \n 
: + level . name , \n 
: level . get_long_description ( ) , \n 
: level . min_zoom \n 
~~ default_selected = False \n 
for demo in demos : \n 
~~~ isdefault = str ( ( not default_demo is None ) and ( demo . id == default_demo . id ) ) . lower ( ) \n 
if isdefault == : \n 
~~~ default_selected = True \n 
~~ layers . append ( { \n 
: demo . id , \n 
: demo . get_short_label ( ) , \n 
: demo . name , \n 
: isdefault , \n 
: str ( demo . is_displayed ) . lower ( ) \n 
~~ if default_demo and not default_selected : \n 
~~~ layers . insert ( 0 , { \n 
: default_demo . id , \n 
: default_demo . get_short_label ( ) , \n 
: default_demo . name , \n 
: str ( True ) . lower ( ) , \n 
: str ( default_demo . is_displayed ) . lower ( ) \n 
~~ if in settings . __members__ : \n 
~~~ mapserver_protocol = settings . MAP_SERVER_PROTOCOL \n 
~~~ mapserver_protocol = \n 
~~ short_label = body_member_short_label . strip ( ) . lower ( ) \n 
long_label = body_member_long_label . strip ( ) . lower ( ) \n 
has_regions = Region . objects . all ( ) . count ( ) > 1 \n 
bodies = LegislativeBody . objects . all ( ) . order_by ( , ) \n 
l_bodies = [ b for b in bodies if b in [ sd . legislative_body for sd in ScoreDisplay . objects . filter \n 
~~~ loader . get_template ( reporting_template ) \n 
~~~ reporting_template = None \n 
~~ return RequestContext ( request , { \n 
: bodies , \n 
: has_regions , \n 
: l_bodies , \n 
: plan , \n 
: districts , \n 
: settings . MAP_SERVER , \n 
: mapserver_protocol , \n 
: settings . BASE_MAPS , \n 
: settings . MAP_SERVER_NS , \n 
: settings . MAP_SERVER_NSHREF , \n 
: settings . FEATURE_LIMIT , \n 
: settings . ADJACENCY , \n 
: settings . CONVEX_CHOROPLETH , \n 
: layers , \n 
: snaplayers , \n 
: UNASSIGNED_DISTRICT_ID , \n 
: request . user . username != and request . user . username != , \n 
: settings . DEBUG and request . user . is_staff , \n 
: get_user_info ( request . user ) , \n 
: editable , \n 
: max_dists + 1 , \n 
: settings . GA_ACCOUNT , \n 
: settings . GA_DOMAIN , \n 
: short_label , \n 
: long_label , \n 
: body_members , \n 
: reporting_template , \n 
: study_area_extent , \n 
: len ( ScoreDisplay . objects . filter ( is_page = True ) ) > 0 , \n 
: json . dumps ( calculator_reports ) , \n 
: ( in settings . __members__ ) , \n 
: tags , \n 
: Site . objects . get_current ( ) , \n 
: translation . get_language ( ) , \n 
~~ def is_plan_ready ( planid ) : \n 
planid = int ( planid ) \n 
return planid == 0 or len ( Plan . objects . filter ( id = planid , processing_state = ProcessingState . READY ) \n 
~~ @ user_passes_test ( using_unique_session ) \n 
def viewplan ( request , planid ) : \n 
if not is_session_available ( request ) or not is_plan_ready ( planid ) : \n 
~~ if not request . user . is_anonymous ( ) and ( int ( planid ) == 0 ) and ( settings . MAX_UNDOS_AFTER_EDIT > 0 ~~~ for p in Plan . objects . filter ( owner = request . user ) : \n 
~~ ~~ return render_to_response ( , commonplan ( request , planid ) ) \n 
def editplan ( request , planid ) : \n 
if request . user . is_anonymous ( ) or not is_session_available ( request ) or not is_plan_ready ( planid ) ~~~ return HttpResponseRedirect ( ) \n 
~~ cfg = commonplan ( request , planid ) \n 
if cfg [ ] == False : \n 
~~~ return HttpResponseRedirect ( % planid ) \n 
~~ plan = Plan . objects . get ( id = planid , owner = request . user ) \n 
cfg [ ] = len ( cfg [ ] ) > plan . legislative_body . max_districts \n 
cfg [ ] = plan . get_available_districts ( ) \n 
if settings . MAX_UNDOS_AFTER_EDIT > 0 : \n 
~~~ plan . purge_beyond_nth_step ( settings . MAX_UNDOS_AFTER_EDIT ) \n 
~~ return render_to_response ( , cfg ) \n 
def printplan ( request , planid ) : \n 
if not is_session_available ( request ) : \n 
sha = hashlib . sha1 ( ) \n 
sha . update ( str ( planid ) + str ( datetime . now ( ) ) ) \n 
cfg [ ] = % sha . hexdigest ( ) \n 
cfg [ ] = % request . META [ ] \n 
~~~ if not in request . REQUEST or not in request . REQUEST or not in request . REQUEST or not in request . REQUEST or not in request . REQUEST : \n 
~~ height = 500 * 2 \n 
if in request . REQUEST : \n 
~~~ height = int ( request . REQUEST [ ] ) * 2 \n 
~~ width = 1024 * 2 \n 
~~~ width = int ( request . REQUEST [ ] ) * 2 \n 
~~ opacity = 0.8 \n 
~~~ opacity = float ( request . REQUEST [ ] ) \n 
~~ full_legend = json . loads ( request . REQUEST [ ] ) \n 
cfg [ ] = request . REQUEST [ ] \n 
cfg [ ] = full_legend [ ] \n 
cfg [ ] = Plan . objects . get ( id = int ( request . REQUEST [ ] ) ) \n 
cfg [ ] = datetime . now ( ) \n 
bbox = request . REQUEST [ ] . split ( ) \n 
pt1 = Point ( float ( bbox [ 0 ] ) , float ( bbox [ 1 ] ) , srid = 3785 ) \n 
pt1 . transform ( SpatialReference ( ) ) \n 
ll = ModestMaps . Geo . Location ( pt1 . y , pt1 . x ) \n 
pt2 = Point ( float ( bbox [ 2 ] ) , float ( bbox [ 3 ] ) , srid = 3785 ) \n 
pt2 . transform ( SpatialReference ( ) ) \n 
ur = ModestMaps . Geo . Location ( pt2 . y , pt2 . x ) \n 
dims = ModestMaps . Core . Point ( width , height ) \n 
provider = ModestMaps . OpenStreetMap . Provider ( ) \n 
basemap = ModestMaps . mapByExtent ( provider , ll , ur , dims ) \n 
fullImg = basemap . draw ( ) \n 
provider = ModestMaps . WMS . Provider ( cfg [ ] , { \n 
: cfg [ ] , \n 
: 512 , \n 
: 512 \n 
overlayImg = ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) \n 
maskImg = ImageChops . invert ( overlayImg ) \n 
: request . REQUEST [ ] , \n 
overlayImg = Image . blend ( overlayImg , ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) , \n 
fullImg = Image . composite ( fullImg , Image . blend ( fullImg , overlayImg , opacity ) , maskImg ) \n 
fullImg . save ( settings . WEB_TEMP + ( % sha . hexdigest ( ) ) , , quality = 100 ) \n 
t = loader . get_template ( ) \n 
page = t . render ( DjangoContext ( cfg ) ) \n 
result = StringIO . StringIO ( ) \n 
CreatePDF ( page , result , show_error_as_pdf = True ) \n 
response = HttpResponse ( result . getvalue ( ) , mimetype = ) \n 
response [ ] = \n 
~~ ~~ @ login_required \n 
def createplan ( request ) : \n 
if request . method == "POST" : \n 
~~~ name = request . POST [ ] [ 0 : 200 ] \n 
body = LegislativeBody . objects . get ( id = int ( request . POST [ ] ) ) \n 
plan = Plan ( name = name , owner = request . user , legislative_body = body , processing_state = ProcessingState try : \n 
~~~ plan . save ( ) \n 
status = serializers . serialize ( "json" , [ plan ] ) \n 
~~ ~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
~~ @ unique_session_or_json_redirect \n 
def uploadfile ( request ) : \n 
if request . user . is_anonymous ( ) : \n 
~~ status = commonplan ( request , 0 ) \n 
index_file = request . FILES . get ( , False ) \n 
if not index_file : \n 
return render_to_response ( , status ) \n 
~~~ filename = index_file . name \n 
~~ if index_file . size > settings . MAX_UPLOAD_SIZE : \n 
~~~ logger . error ( ) \n 
status [ ] = False \n 
~~ if not filename . endswith ( ( , ) ) : \n 
~~ elif request . POST [ ] == : \n 
~~~ dest = tempfile . NamedTemporaryFile ( mode = , delete = False ) \n 
for chunk in request . FILES [ ] . chunks ( ) : \n 
~~~ dest . write ( chunk ) \n 
~~ dest . close ( ) \n 
if request . FILES [ ] . name . endswith ( ) : \n 
~~~ os . rename ( dest . name , % ( dest . name , ) ) \n 
filename = % ( dest . name , ) \n 
~~~ filename = dest . name \n 
~~ ~~ except Exception as ex : \n 
logger . error ( , ex ) \n 
~~ DistrictIndexFile . index2plan . delay ( request . POST [ ] , request . POST [ \n 
~~ return render_to_response ( , status ) \n 
~~ def generate_report_hash ( qdict ) : \n 
params = qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) \n 
sha . update ( params ) \n 
return sha . hexdigest ( ) \n 
def getreport ( request , planid ) : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~~ status [ ] = _ ( ) \n 
~~ if not can_view ( request . user , plan ) : \n 
~~ if not settings . REPORTS_ENABLED is None : \n 
~~ if request . method != : \n 
~~ stamp = request . POST . get ( , generate_report_hash ( request . POST ) ) \n 
rptstatus = PlanReport . checkreport ( planid , stamp ) \n 
if rptstatus == : \n 
~~~ status = { \n 
: PlanReport . getreport ( planid , stamp ) , \n 
: _ ( ) , \n 
: stamp \n 
~~ elif rptstatus == : \n 
: reverse ( getreport , args = [ planid ] ) , \n 
req = { \n 
: request . POST . get ( , ) , \n 
: request . POST . getlist ( ) , \n 
: request . POST . get ( , ) \n 
PlanReport . markpending ( planid , stamp ) \n 
PlanReport . createreport . delay ( planid , stamp , req , language = translation . get_language ( ) ) \n 
~~~ status [ ] = _ ( \n 
def getcalculatorreport ( request , planid ) : \n 
~~ function_ids = request . POST . get ( , ) \n 
sha . update ( function_ids ) \n 
stamp = request . POST . get ( , sha . hexdigest ( ) ) \n 
rptstatus = CalculatorReport . checkreport ( planid , stamp ) \n 
: CalculatorReport . getreport ( planid , stamp ) , \n 
: reverse ( getcalculatorreport , args = [ planid ] ) , \n 
: 5 , \n 
req = { : function_ids } \n 
CalculatorReport . markpending ( planid , stamp ) \n 
CalculatorReport . createcalculatorreport . delay ( planid , stamp , req , language = translation . get_language ~~ else : \n 
def newdistrict ( request , planid ) : \n 
if len ( request . REQUEST . items ( ) ) >= 3 : \n 
~~~ plan = Plan . objects . get ( pk = planid , owner = request . user ) \n 
~~~ geolevel = request . REQUEST [ ] \n 
~~~ geolevel = None \n 
~~ if in request . REQUEST : \n 
~~~ geounit_ids = string . split ( request . REQUEST [ ] , ) \n 
~~~ geounit_ids = None \n 
~~~ district_id = int ( request . REQUEST [ ] ) \n 
~~~ district_id = None \n 
~~~ district_short = request . REQUEST [ ] [ 0 : 10 ] \n 
~~ elif not district_id is None : \n 
~~~ district_short = plan . legislative_body . get_short_label ( ) % { : district_id } \n 
~~~ district_short = None \n 
~~~ district_long = request . REQUEST [ ] [ 0 : 256 ] \n 
~~~ district_long = plan . legislative_body . get_label ( ) % { : district_id } \n 
~~~ district_long = None \n 
~~~ version = request . REQUEST [ ] \n 
~~~ version = plan . version \n 
~~ if geolevel and geounit_ids and district_id : \n 
~~~ fixed = plan . add_geounits ( ( district_id , district_short , district_long , ) , geounit_ids \n 
district = plan . district_set . filter ( district_id = district_id , short_label = district_short if plan . legislative_body . multi_members_allowed : \n 
~~~ district . num_members = plan . legislative_body . min_multi_district_members \n 
district . save ( ) \n 
~~ ct = ContentType . objects . get ( app_label = , model = ) \n 
if in request . POST and request . POST [ ] != : \n 
~~~ comment = Comment ( \n 
object_pk = district . id , \n 
content_type = ct , \n 
site_id = Site . objects . get_current ( ) . id , \n 
user_name = request . user . username , \n 
user_email = request . user . email , \n 
comment = request . POST [ ] ) \n 
comment . save ( ) \n 
~~ if len ( request . REQUEST . getlist ( ) ) > 0 : \n 
~~~ strtags = request . REQUEST . getlist ( ) \n 
for strtag in strtags : \n 
~~~ if strtag == : \n 
~~ if strtag . count ( ) > 0 : \n 
~~~ strtag = \'"type=%s"\' % strtag \n 
~~~ strtag = % strtag \n 
~~ Tag . objects . add_tag ( district , strtag ) \n 
status [ ] = _ ( ) \n 
plan = Plan . objects . get ( pk = planid , owner = request . user ) \n 
status [ ] = getutc ( plan . edited ) . isoformat ( ) \n 
status [ ] = district_id \n 
status [ ] = plan . version \n 
~~ except ValidationError : \n 
~~ except Exception , ex : \n 
~~~ logger . warn ( ) \n 
logger . debug ( , ex ) \n 
@ transaction . commit_manually \n 
def add_districts_to_plan ( request , planid ) : \n 
~~ if not can_edit ( request . user , plan ) : \n 
~~ district_list = request . POST . getlist ( ) \n 
if len ( district_list ) == 0 : \n 
~~~ districts = District . objects . filter ( id__in = district_list ) \n 
version = int ( request . POST . get ( , None ) ) \n 
status [ ] = _ ( ) % { : len ( districts ) } \n 
~~ allowed_districts = plan . get_available_districts ( version = version ) \n 
if len ( districts ) > allowed_districts : \n 
~~~ status [ ] = _ ( ) % { : allowed_districts } \n 
~~~ results = plan . paste_districts ( districts , version = version ) \n 
transaction . commit ( ) \n 
status [ ] = _ ( ) % { status [ ] = plan . version \n 
~~ except Exception as ex : \n 
~~~ transaction . rollback ( ) \n 
status [ ] = str ( ex ) \n 
status [ ] = traceback . format_exc ( ) \n 
def assign_district_members ( request , planid ) : \n 
~~ leg_bod = plan . legislative_body \n 
if ( not leg_bod . multi_members_allowed ) : \n 
~~ districts = request . POST . getlist ( ) \n 
counts = request . POST . getlist ( ) \n 
~~~ changed = 0 \n 
for i in range ( 0 , len ( districts ) ) : \n 
~~~ id = int ( districts [ i ] ) \n 
count = int ( counts [ i ] ) \n 
district = District . objects . filter ( plan = plan , district_id = id , version__lte = version ) . order_by \n 
if district . num_members != count : \n 
~~~ if ( changed == 0 ) : \n 
~~~ if version != plan . version : \n 
~~~ plan . purge ( after = version ) \n 
~~ plan . version = plan . version + 1 \n 
~~ plan . update_num_members ( district , count ) \n 
changed += 1 \n 
~~ ~~ transaction . commit ( ) \n 
status [ ] = changed \n 
) % { : changed } \n 
logger . warn ( ) \n 
def combine_districts ( request , planid ) : \n 
~~ version = int ( request . POST . get ( , plan . version ) ) \n 
from_id = int ( request . POST . get ( , - 1 ) ) \n 
to_id = int ( request . POST . get ( , None ) ) \n 
~~~ all_districts = plan . get_districts_at_version ( version , include_geom = True ) \n 
from_districts = filter ( lambda d : True if d . district_id == from_id else False , all_districts to_district = filter ( lambda d : True if d . district_id == to_id else False , all_districts ) [ 0 ] \n 
locked = to_district . is_locked \n 
for district in from_districts : \n 
~~~ if district . is_locked : \n 
~~~ locked = True \n 
~~ ~~ if locked : \n 
~~ result = plan . combine_districts ( to_district , from_districts , version = version ) \n 
if result [ 0 ] == True : \n 
status [ ] = result [ 1 ] \n 
~~ ~~ except Exception , ex : \n 
def fix_unassigned ( request , planid ) : \n 
~~~ version = int ( request . POST . get ( , plan . version ) ) \n 
result = plan . fix_unassigned ( version ) \n 
status [ ] = result [ 0 ] \n 
def get_splits ( request , planid , otherid , othertype ) : \n 
otherid = int ( otherid ) \n 
~~ version = int ( request . REQUEST [ ] if in request . REQUEST else plan . version ) \n 
~~~ if othertype == : \n 
~~~ otherplan = Plan . objects . get ( pk = otherid ) \n 
~~ if not can_view ( request . user , otherplan ) : \n 
~~ otherversion = int ( request . REQUEST [ ] if in request . REQUEST splits = plan . find_plan_splits ( otherplan , version , otherversion ) \n 
~~ elif othertype == : \n 
~~~ splits = plan . find_geolevel_splits ( otherid , version ) \n 
~~~ status [ ] = _ ( ) % { : othertype } \n 
~~ split_word = _ ( ) if len ( splits ) == 1 else inflect . engine ( ) . plural ( _ ( ) ) \n 
status [ ] = _ ( ) % { : len ( splits ) , : split_word } \n 
status [ ] = splits \n 
status [ ] = list ( set ( [ i [ 0 ] for i in splits ] ) ) \n 
status [ ] = list ( set ( [ i [ 1 ] for i in splits ] ) ) \n 
~~ def get_processing_status ( request ) : \n 
plan_ids = request . REQUEST . getlist ( ) \n 
if len ( plan_ids ) == 0 : \n 
~~~ statuses = { } \n 
for p in Plan . objects . filter ( id__in = plan_ids ) : \n 
~~~ statuses [ str ( p . id ) ] = p . get_processing_state_display ( ) \n 
~~ status [ ] = True \n 
status [ ] = statuses \n 
~~ def get_splits_report ( request , planid ) : \n 
~~~ return HttpResponse ( _ ( ) , mimetype = ) \n 
~~ if not using_unique_session ( request . user ) or not can_view ( request . user , plan ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
inverse = request . REQUEST [ ] == if in request . REQUEST else False \n 
extended = request . REQUEST [ ] == if in request . REQUEST else False \n 
layers = request . REQUEST . getlist ( ) \n 
if len ( layers ) == 0 : \n 
~~~ report = loader . get_template ( ) \n 
html = \n 
for layer in layers : \n 
~~~ my_context = { : extended } \n 
my_context . update ( plan . compute_splits ( layer , version = version , inverse = inverse , extended last_item = layer is layers [ - 1 ] \n 
community_info = plan . get_community_type_info ( layer , version = version , inverse = inverse if community_info is not None : \n 
~~~ my_context . update ( community_info ) \n 
~~ calc_context = DjangoContext ( my_context ) \n 
html += report . render ( calc_context ) \n 
if not last_item : \n 
~~~ html += \n 
~~ ~~ return HttpResponse ( html , mimetype = ) \n 
return HttpResponse ( str ( ex ) , mimetype = ) \n 
def addtodistrict ( request , planid , districtid ) : \n 
if len ( request . REQUEST . items ( ) ) >= 2 : \n 
~~~ geolevel = request . REQUEST [ "geolevel" ] \n 
geounit_ids = string . split ( request . REQUEST [ "geounits" ] , "|" ) \n 
~~~ status [ ] = traceback . format_exc ( ) \n 
~~~ fixed = plan . add_geounits ( districtid , geounit_ids , geolevel , version ) \n 
status [ ] = True ; \n 
status [ ] = _ ( ) % { : fixed } \n 
status [ ] = fixed \n 
def setdistrictlock ( request , planid , district_id ) : \n 
if request . method != : \n 
~~ lock = request . POST . get ( ) . lower ( ) == \n 
version = request . POST . get ( ) \n 
if lock == None : \n 
~~ elif version == None : \n 
district = plan . district_set . filter ( district_id = district_id , version__lte = version ) . order_by ( ~~ except ObjectDoesNotExist : \n 
~~ if plan . owner != request . user : \n 
~~ district . is_locked = lock \n 
status [ ] = _ ( ) % { : _ ( ) if lock else _ ( ) } \n 
def getdistricts ( request , planid ) : \n 
~~~ version = int ( request . REQUEST [ ] ) \n 
~~ districts = plan . get_districts_at_version ( version , include_geom = False ) \n 
status [ ] = [ ] \n 
status [ ] = plan . legislative_body . max_districts - len ( districts ) + 1 \n 
max_version = max ( [ d . version for d in districts ] ) \n 
can_undo = max_version > plan . min_version \n 
~~~ status [ ] . append ( { \n 
: district . district_id , \n 
: . join ( map ( _ , district . short_label . split ( ) ) ) , \n 
: . join ( map ( _ , district . long_label . split ( ) ) ) , \n 
: district . version \n 
~~ status [ ] = can_undo \n 
~~ def simple_district_versioned ( request , planid , district_ids = None ) : \n 
status = { : } \n 
~~ subject_id = None \n 
~~~ subject_id = request . REQUEST [ ] \n 
~~ elif plan . legislative_body . get_default_subject ( ) : \n 
~~~ subject_id = plan . legislative_body . get_default_subject ( ) . id \n 
~~ geolevel = plan . legislative_body . get_geolevels ( ) [ 0 ] . id \n 
~~~ geolevel = int ( request . REQUEST [ ] ) \n 
~~~ district_ids = request . REQUEST [ ] \n 
if len ( district_ids ) > 0 : \n 
~~~ district_ids = district_ids . split ( ) \n 
~~~ district_ids = [ ] \n 
~~ ~~ if subject_id : \n 
~~~ bbox = None \n 
~~~ bbox = request . REQUEST [ ] \n 
bbox = tuple ( map ( lambda x : float ( x ) , bbox . split ( ) ) ) \n 
~~~ bbox = plan . district_set . all ( ) . extent ( field_name = ) \n 
~~ status [ ] = plan . get_wfs_districts ( version , subject_id , bbox , geolevel , district_ids ~~ else : \n 
~~~ status [ ] = [ ] \n 
~~ def get_unlocked_simple_geometries ( request , planid ) : \n 
version = request . POST . get ( , plan . version ) \n 
geolevel = request . POST . get ( , plan . legislative_body . get_geolevels ( ) [ 0 ] . id ) \n 
geom = request . POST . get ( , None ) \n 
if geom is not None : \n 
~~~ wkt = request . POST . get ( , None ) \n 
geom = GEOSGeometry ( wkt ) \n 
~~ except GEOSException : \n 
~~~ wkt = request . REQUEST [ ] . replace ( , ) \n 
wkt = wkt . replace ( , ) . replace ( , ) \n 
~~~ geom = GEOSGeometry ( wkt ) \n 
~~~ geom = None \n 
~~ ~~ selection = Q ( geom__intersects = geom ) \n 
districts = [ d . id for d in plan . get_districts_at_version ( version , include_geom = True ) if locked = District . objects . filter ( id__in = districts ) . collect ( ) \n 
locked_buffered = locked . simplify ( 100 , True ) . buffer ( 100 ) if locked else None \n 
filtered = Geolevel . objects . get ( id = geolevel ) . geounit_set . filter ( selection ) \n 
for feature in filtered : \n 
~~~ geom = feature . simple \n 
if locked and geom . intersects ( locked_buffered ) : \n 
~~~ if feature . geom . within ( locked ) : \n 
~~ if feature . geom . overlaps ( locked ) : \n 
~~ ~~ features . append ( { \n 
: % feature . id , \n 
: json . loads ( geom . json ) , \n 
: feature . name , \n 
: geolevel , \n 
: feature . id \n 
~~ status [ ] = features \n 
def get_statistics ( request , planid ) : \n 
~~~ note_session_activity ( request ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = , status = 500 ) \n 
~~~ display = ScoreDisplay . objects . get ( legislative_body = plan . legislative_body , name = "%s_sidebar_demo" ~~ except : \n 
~~~ display = ScoreDisplay . objects . get ( pk = request . POST [ ] ) \n 
logger . warn ( str ( request . POST ) ) \n 
~~~ html = display . render ( plan , request , version = version ) \n 
return HttpResponse ( html , mimetype = ) \n 
~~ ~~ def getutc ( t ) : \n 
t_tuple = t . timetuple ( ) \n 
t_seconds = time . mktime ( t_tuple ) \n 
return t . utcfromtimestamp ( t_seconds ) \n 
def getdistrictfilestatus ( request , planid ) : \n 
if not can_copy ( request . user , plan ) : \n 
~~~ is_shape = in request . REQUEST and request . REQUEST [ ] == \n 
file_status = DistrictFile . get_file_status ( plan , shape = is_shape ) \n 
status [ ] = file_status \n 
status [ ] = ex \n 
def getdistrictfile ( request , planid ) : \n 
~~ is_shape = in request . REQUEST and request . REQUEST [ ] == \n 
if file_status == : \n 
~~~ if is_shape : \n 
~~~ archive = DistrictShapeFile . plan2shape ( plan ) \n 
~~~ archive = DistrictIndexFile . plan2index ( plan ) \n 
~~ response = HttpResponse ( open ( archive . name ) . read ( ) , content_type = ) \n 
~~~ DistrictShapeFile . plan2shape . delay ( plan ) \n 
~~~ DistrictIndexFile . plan2index . delay ( plan ) \n 
~~ response = HttpResponse ( _ ( \n 
def emaildistrictindexfile ( request , planid ) : \n 
~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ DistrictIndexFile . emailfile . delay ( plan , request . user , request . POST , translation . get_language ( ) ) \n 
return HttpResponse ( json . dumps ( { \n 
: _ ( ) } ) , \n 
mimetype = ) \n 
~~ def getvalidplans ( leg_body , owner = None ) : \n 
pfilter = Q ( legislative_body = leg_body ) & Q ( is_valid = True ) \n 
if owner is not None : \n 
~~~ pfilter = pfilter & Q ( owner = owner ) \n 
~~ return list ( Plan . objects . filter ( pfilter ) ) \n 
~~ def getleaderboarddisplay ( leg_body , owner_filter ) : \n 
~~~ return ScoreDisplay . objects . get ( name = "%s_leader_%s" % ( leg_body . name , owner_filter ) ) \n 
~~ ~~ def getleaderboard ( request ) : \n 
if not using_unique_session ( request . user ) : \n 
~~ owner_filter = request . REQUEST [ ] \n 
body_pk = int ( request . REQUEST [ ] ) ; \n 
leg_body = LegislativeBody . objects . get ( pk = body_pk ) \n 
display = getleaderboarddisplay ( leg_body , owner_filter ) \n 
if display is None : \n 
~~ plans = getvalidplans ( leg_body , request . user if owner_filter == else None ) \n 
~~~ html = display . render ( plans , request ) \n 
~~ ~~ def getleaderboardcsv ( request ) : \n 
plans = getvalidplans ( leg_body , request . user if owner_filter == else None ) \n 
panels = display . scorepanel_set . all ( ) . order_by ( ) \n 
~~~ response = HttpResponse ( mimetype = ) \n 
writer = csv . writer ( response ) \n 
writer . writerow ( [ , , ] + [ p . __unicode__ ( ) for p in panels ] ) \n 
for plan in plans : \n 
~~~ row = [ plan . id , plan . name , plan . owner . username ] \n 
for panel in panels : \n 
~~~ function = panel . score_functions . all ( ) [ 0 ] \n 
score = ComputedPlanScore . compute ( function , plan ) \n 
row . append ( score [ ] ) \n 
~~ writer . writerow ( row ) \n 
~~ ~~ def getplans ( request ) : \n 
~~ if request . method == : \n 
~~~ page = int ( request . POST . get ( , 1 ) ) \n 
rows = int ( request . POST . get ( , 10 ) ) \n 
sidx = request . POST . get ( , ) \n 
sord = request . POST . get ( , ) \n 
owner_filter = request . POST . get ( ) ; \n 
body_pk = request . POST . get ( ) ; \n 
body_pk = int ( body_pk ) if body_pk else body_pk ; \n 
search = request . POST . get ( , False ) ; \n 
search_string = request . POST . get ( , ) ; \n 
is_community = request . POST . get ( , False ) == ; \n 
~~ end = page * rows \n 
start = end - rows \n 
if owner_filter == : \n 
~~~ available = Q ( is_template = True ) \n 
~~ elif owner_filter == : \n 
~~~ available = Q ( is_shared = True ) \n 
~~~ available = Q ( owner__exact = request . user ) \n 
~~ ~~ elif owner_filter == : \n 
~~~ available = Q ( is_template = True ) | Q ( is_shared = True ) \n 
if not request . user . is_anonymous ( ) : \n 
~~~ available = available | Q ( owner__exact = request . user ) \n 
~~ not_creating = ~ Q ( processing_state = ProcessingState . CREATING ) & ~ Q ( processing_state = ProcessingState \n 
if sidx . startswith ( ) : \n 
~~~ sidx = sidx [ len ( ) : ] \n 
~~ if sidx == : \n 
~~~ sidx = \n 
~~ if sord == : \n 
~~~ sidx = + sidx \n 
~~ if search : \n 
~~~ search_filter = Q ( name__icontains = search_string ) | Q ( description__icontains = search_string ~~ else : \n 
~~~ search_filter = None \n 
~~ if body_pk : \n 
~~~ body_filter = Q ( legislative_body = body_pk ) \n 
all_plans = Plan . objects . filter ( available , not_creating , body_filter , search_filter ) . order_by ~~ else : \n 
~~~ community_filter = Q ( legislative_body__is_community = is_community ) \n 
all_plans = Plan . objects . filter ( available , not_creating , search_filter , community_filter ) . order_by \n 
~~ if all_plans . count ( ) > 0 : \n 
~~~ total_pages = math . ceil ( all_plans . count ( ) / float ( rows ) ) \n 
~~~ total_pages = 1 \n 
~~ plans = all_plans [ start : end ] \n 
plans_list = list ( ) \n 
~~~ plans_list . append ( { \n 
: plan . id , \n 
: plan . name , \n 
: plan . description , \n 
: time . mktime ( plan . edited . timetuple ( ) ) , \n 
: plan . is_template , \n 
: plan . is_shared , \n 
: plan . owner . username , \n 
: can_edit ( request . user , plan ) , \n 
: plan . legislative_body . get_long_description ( ) , \n 
: plan . get_processing_state_display ( ) \n 
~~ def get_shared_districts ( request , planid ) : \n 
~~ all_districts = plan . get_districts_at_version ( plan . version , include_geom = False ) \n 
~~~ plan = None \n 
all_districts = ( ) \n 
~~ if len ( all_districts ) > 0 : \n 
~~~ total_pages = math . ceil ( len ( all_districts ) / float ( rows ) ) \n 
~~ districts = all_districts [ start : end ] \n 
districts_list = list ( ) \n 
~~~ if not district . is_unassigned : \n 
~~~ districts_list . append ( { \n 
: district . id , \n 
: district . short_label , \n 
: district . long_label , \n 
def editplanattributes ( request , planid ) : \n 
~~~ return HttpResponseNotAllowed ( [ ] ) \n 
~~ new_name = request . POST . get ( , None ) \n 
new_description = request . POST . get ( , ) \n 
if not planid or not ( new_name or new_description ) : \n 
~~~ return HttpResponseBadRequest ( \n 
_ ( ) ) \n 
~~ plan = Plan . objects . filter ( pk = planid , owner = request . user ) \n 
if not new_name is None : \n 
~~~ plan . name = new_name \n 
~~ plan . description = new_description \n 
def deleteplan ( request , planid ) : \n 
~~ if not planid : \n 
~~~ return HttpResponseBadRequest ( _ ( ) ) \n 
~~~ plan . delete ( ) \n 
def reaggregateplan ( request , planid ) : \n 
~~~ reaggregate_plan . delay ( plan . id ) \n 
plan . processing_state = ProcessingState . REAGGREGATING \n 
~~ def get_health ( request ) : \n 
~~~ def num_users ( minutes ) : \n 
~~~ users = 0 \n 
for session in Session . objects . all ( ) : \n 
~~~ session . delete ( ) \n 
~~ if in decoded : \n 
~~~ activity_delta = decoded [ ] - timedelta ( 0 , 0 , 0 , 0 , settings . SESSION_TIMEOUT if activity_delta > ( datetime . now ( ) - timedelta ( 0 , 0 , 0 , 0 , minutes ) ) : \n 
~~~ users += 1 \n 
~~ ~~ ~~ return users \n 
~~~ result = _ ( ) % { : datetime . now ( ) } \n 
result += _ ( ) % { : Plan . objects . all ( ) . count ( ) } \n 
result += _ ( ) % { : Session . objects . all ( ) . count ( ) , \n 
: settings . CONCURRENT_SESSIONS } \n 
result += _ ( ) % { : num_users ( 10 ) } \n 
space = os . statvfs ( ) \n 
result += _ ( ) % { : ( ( space . f_bsize * space . f_bavail ) / ( 1024 * 1024 ) ) } \n 
result += _ ( ) % { : commands . getoutput ( ) } \n 
return HttpResponse ( result , mimetype = ) \n 
~~ ~~ def statistics_sets ( request , planid ) : \n 
~~~ result = { : False } \n 
if plan . count ( ) == 0 : \n 
~~~ result [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( result ) , mimetype = ) \n 
~~~ sets = [ ] \n 
scorefunctions = [ ] \n 
user_functions = ScoreFunction . objects . filter ( selectable_bodies = plan . legislative_body ) . order_by for f in user_functions : \n 
~~~ if not in f . name . lower ( ) and not in f . name . lower ( ) : \n 
~~~ scorefunctions . append ( { : f . id , : force_escape ( f . get_label ( ) ) } ) \n 
~~ ~~ result [ ] = scorefunctions \n 
admin_display_names = [ \n 
"%s_sidebar_demo" % plan . legislative_body . name , \n 
if plan . legislative_body . is_community : \n 
~~~ admin_display_names . append ( "%s_sidebar_comments" % \n 
plan . legislative_body . name ) \n 
~~~ admin_display_names . append ( "%s_sidebar_basic" % \n 
~~ admin_displays = ScoreDisplay . objects . filter ( \n 
owner__is_superuser = True , \n 
legislative_body = plan . legislative_body , \n 
name__in = admin_display_names \n 
for admin_display in admin_displays : \n 
~~~ sets . append ( { \n 
: admin_display . id , \n 
: force_escape ( admin_display . get_label ( ) ) , \n 
: False \n 
~~~ user_displays = ScoreDisplay . objects . filter ( \n 
owner = request . user , \n 
is_page = False ) . order_by ( ) \n 
result [ ] = len ( user_displays ) \n 
for display in user_displays : \n 
~~~ functions = [ ] \n 
for panel in display . scorepanel_set . all ( ) : \n 
~~~ if panel . type == : \n 
~~~ functions = map ( lambda x : x . id , panel . score_functions . all ( ) ) \n 
if len ( functions ) == 0 : \n 
~~ ~~ ~~ sets . append ( { : display . id , : force_escape ( display . __unicode__ ( ) ) , ~~ ~~ except Exception , ex : \n 
~~~ result [ ] = _ ( ) % { : request . user } \n 
~~ result [ ] = sets \n 
result [ ] = True \n 
~~ elif request . method == and in request . POST : \n 
~~~ display = ScoreDisplay . objects . get ( pk = request . REQUEST . get ( , - 1 ) ) \n 
result [ ] = { : force_escape ( display . __unicode__ ( ) ) , : display . id } \n 
qset = display . scorepanel_set . all ( ) \n 
for panel in qset : \n 
~~~ if panel . displays . count ( ) == 1 : \n 
~~~ panel . delete ( ) \n 
~~ ~~ display . delete ( ) \n 
result [ ] = traceback . format_exc ( ) \n 
~~ ~~ elif request . method == : \n 
~~~ def validate_num ( user , limit = 3 ) : \n 
~~~ return ScoreDisplay . objects . filter ( owner = user , legislative_body = plan . legislative_body , is_page \n 
~~ if in request . POST : \n 
~~~ functions = request . POST . getlist ( ) \n 
functions = map ( lambda x : int ( x ) , functions ) \n 
~~~ display = ScoreDisplay . objects . get ( title = request . POST . get ( ) , owner = request . user display = display . copy_from ( display = display , functions = functions ) \n 
~~~ limit = 3 \n 
if validate_num ( request . user , limit ) : \n 
~~~ demo = ScoreDisplay . objects . filter ( \n 
is_page = False , \n 
title = "Demographics" \n 
for disp in demo : \n 
~~~ has_comments = False \n 
for pnl in disp . scorepanel_set . all ( ) : \n 
~~~ for fn in pnl . score_functions . all ( ) : \n 
~~~ has_comments = has_comments or fn . calculator . endswith ( ) \n 
~~ ~~ if not has_comments : \n 
~~~ demo = disp \n 
~~ ~~ display = ScoreDisplay ( ) \n 
display = display . copy_from ( display = demo , title = request . POST . get ( ) , owner = result [ ] = True \n 
~~~ result [ ] = _ ( \n 
) % { : limit } \n 
result [ ] = \n 
~~ ~~ result [ ] = { : force_escape ( display . __unicode__ ( ) ) , : display . id , result [ ] = True \n 
~~ ~~ return HttpResponse ( json . dumps ( result ) , mimetype = ) \n 
~~ def purge_plan_clear_cache ( district , version ) : \n 
district . plan . purge ( after = version ) \n 
district . plan . version = version \n 
district . plan . save ( ) \n 
cache = district . computeddistrictscore_set . filter ( function__calculator__endswith = ) \n 
cache . delete ( ) \n 
def district_info ( request , planid , district_id ) : \n 
version = plan . version \n 
version = min ( plan . version , int ( version ) ) \n 
~~ ~~ district_id = int ( district_id ) \n 
district = plan . get_districts_at_version ( version , include_geom = False ) \n 
district = filter ( lambda d : d . district_id == district_id , district ) \n 
~~~ district = plan . district_set . get ( id = request . POST [ ] ) \n 
district . short_label = request . POST [ ] [ 0 : 10 ] \n 
district . long_label = request . POST [ ] [ 0 : 256 ] \n 
if district . version < version : \n 
district_copy . version = version \n 
district_copy . save ( ) \n 
district_copy . clone_relations_from ( district ) \n 
district = district_copy \n 
~~~ district . save ( ) \n 
~~ has_comment = in request . POST and request . POST [ ] != \n 
if has_comment : \n 
~~~ ct = ContentType . objects . get ( app_label = , model = ) \n 
Comment . objects . filter ( object_pk = district . id , content_type = ct ) . delete ( ) \n 
comment = Comment ( \n 
~~ tset = Tag . objects . get_for_object ( district ) . filter ( name__startswith = ) \n 
TaggedItem . objects . filter ( tag__in = tset , object_id = district . id ) . delete ( ) \n 
purge_plan_clear_cache ( district , version ) \n 
if len ( request . REQUEST . getlist ( ) ) > 0 : \n 
~~ ~~ status [ ] = version \n 
~~ def plan_feed ( request ) : \n 
~~~ feed = loader . get_template ( ) \n 
plans = Plan . objects . all ( ) . order_by ( ) [ 0 : 10 ] \n 
geolevel = plans [ 0 ] . legislative_body . get_geolevels ( ) [ 0 ] \n 
extent = geolevel . geounit_set . collect ( ) . extent \n 
if extent [ 2 ] - extent [ 0 ] > extent [ 3 ] - extent [ 1 ] : \n 
~~~ width = 500 \n 
height = int ( 500 * ( extent [ 3 ] - extent [ 1 ] ) / ( extent [ 2 ] - extent [ 0 ] ) ) \n 
~~~ width = int ( 500 * ( extent [ 2 ] - extent [ 0 ] ) / ( extent [ 3 ] - extent [ 1 ] ) ) \n 
height = 500 \n 
~~ mapserver = settings . MAP_SERVER if settings . MAP_SERVER != else request . META [ ] \n 
context = { \n 
: plans , \n 
: mapserver , \n 
: extent , \n 
: width , \n 
: height \n 
xml = feed . render ( DjangoContext ( context ) ) \n 
return HttpResponse ( xml , mimetype = ) \n 
~~ def share_feed ( request ) : \n 
plans = Plan . objects . filter ( is_shared = True ) . order_by ( ) [ 0 : 10 ] \n 
if plans . count ( ) < 0 : \n 
~~~ geolevel = plans [ 0 ] . legislative_body . get_geolevels ( ) [ 0 ] \n 
~~~ extent = ( 0 , 0 , 0 , 0 , ) \n 
width = 1 \n 
height = 1 \n 
~~ url = \n 
cmp_to_key ) \n 
from functools import ( partial , reduce , wraps , \n 
a = 1 \n 
#------------------------------------------------------------------------------- \n 
~~ DSIZE = 4 \n 
a_offset = 1 * 1024 * 1024 \n 
b_offset = 2 * 1024 * 1024 \n 
iochannel = CoramIoChannel ( idx = 0 , datawidth = 32 ) \n 
channel = CoramChannel ( idx = 0 , datawidth = 8 * DSIZE ) \n 
def st_set_mesh_size ( mesh_size ) : \n 
~~~ channel . write ( mesh_size ) \n 
~~ def st_step ( mesh_size , read_start , write_start ) : \n 
~~~ read_page = 3 \n 
write_page = 0 \n 
read_addr = read_start \n 
mem0 . write ( 0 , read_addr , mesh_size ) \n 
read_addr += mesh_size * DSIZE \n 
mem1 . write ( 0 , read_addr , mesh_size ) \n 
mem2 . write ( 0 , read_addr , mesh_size ) \n 
write_addr = write_start + mesh_size * DSIZE + DSIZE \n 
for i in range ( mesh_size - 2 ) : \n 
~~~ hot_spot = 1 if i == 0 else 0 \n 
pos = ( ( hot_spot << 6 ) | \n 
( ( 0x1 << write_page ) << 4 ) | \n 
( 0x1 << read_page ) ) \n 
mem0 . wait ( ) \n 
mem1 . wait ( ) \n 
mem2 . wait ( ) \n 
mem3 . wait ( ) \n 
channel . write ( pos ) \n 
if read_page == 0 : \n 
~~~ mem0 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 1 : \n 
~~~ mem1 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 2 : \n 
~~~ mem2 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 3 : \n 
~~~ mem3 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ read_page = 0 if read_page == 3 else read_page + 1 \n 
channel . read ( ) \n 
mem_d0 . wait ( ) \n 
mem_d1 . wait ( ) \n 
if write_page == 0 : \n 
~~~ mem_d0 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
~~ elif write_page == 1 : \n 
~~~ mem_d1 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
~~ write_addr += mesh_size * DSIZE \n 
write_page = 0 if write_page == 1 else write_page + 1 \n 
~~ mem_d0 . wait ( ) \n 
~~ def st_computation ( num_iter , mesh_size ) : \n 
~~~ for i in range ( num_iter / 2 ) : \n 
~~~ st_step ( mesh_size , a_offset , b_offset ) \n 
st_step ( mesh_size , b_offset , a_offset ) \n 
~~ ~~ def st_sum ( mesh_size ) : \n 
~~~ check_sum = 0 \n 
read_addr = a_offset \n 
for i in range ( mesh_size ) : \n 
~~~ mem0 . write ( 0 , read_addr , mesh_size ) \n 
init_sum = 1 if i == 0 else 0 \n 
calc_sum = 1 \n 
pos = ( init_sum << 8 ) | ( calc_sum << 7 ) \n 
check_sum = channel . read ( ) \n 
return check_sum \n 
~~ def st_main ( ) : \n 
~~~ global a_offset \n 
global b_offset \n 
mesh_size = iochannel . read ( ) \n 
num_iter = iochannel . read ( ) \n 
a_offset = iochannel . read ( ) \n 
b_offset = iochannel . read ( ) \n 
st_set_mesh_size ( mesh_size ) \n 
st_computation ( num_iter , mesh_size ) \n 
check_sum = st_sum ( mesh_size ) \n 
iochannel . write ( check_sum ) \n 
~~~ st_main ( ) \n 
~~~ read_page = 0 \n 
pos = hot_spot \n 
~~ read_page = 0 if read_page == 2 else read_page + 1 \n 
mem_d0 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
write_addr += mesh_size * DSIZE \n 
~~ ~~ def st_computation ( num_iter , mesh_size ) : \n 
pos = ( init_sum << 2 ) | ( calc_sum << 1 ) \n 
def getRamId ( oid , sid ) : \n 
~~~ if 0 <= sid and sid <= 31 : \n 
~~ if 32 <= sid and sid <= 63 : \n 
~~ if 64 <= sid and sid <= 95 : \n 
~~~ return 2 \n 
~~ if 96 <= sid and sid <= 127 : \n 
~~~ return 3 \n 
~~ ~~ def getRamSubId ( oid , sid ) : \n 
~~~ return sid \n 
~~~ return sid - 32 \n 
~~~ return sid - 64 \n 
~~~ return sid - 96 \n 
~~ ~~ def getChannelId ( oid , sid ) : \n 
~~~ return oid \n 
~~ def getChannelSubId ( oid , sid ) : \n 
~~ def getRegisterId ( oid , sid ) : \n 
~~ def getRegisterSubId ( oid , sid ) : \n 
~~~ f = open ( sys . argv [ 1 ] , ) \n 
lines = f . readlines ( ) \n 
output = [ ] \n 
p_thread = re . compile ( ) \n 
p_thread_id = re . compile ( ) \n 
p_object_id = re . compile ( ) \n 
p_width = re . compile ( ) \n 
p_depth = re . compile ( ) \n 
p_indexwidth = re . compile ( ) \n 
p_logdepth = re . compile ( ) \n 
p_sub_id = re . compile ( ) \n 
module_name = None \n 
thread_name = None \n 
thread_id = None \n 
object_id = None \n 
sub_id = None \n 
width = None \n 
indexwidth = None \n 
depth = None \n 
mode = False \n 
sub_id_num = None \n 
sub_id_base = None \n 
buffer = [ ] \n 
~~~ if not mode : \n 
~~~ m = p_thread . match ( line ) \n 
~~~ thread_name = re . match ( \'.*(".*").*\' , m . group ( 2 ) ) . group ( 1 ) \n 
module_name = re . search ( , line ) . group ( 1 ) \n 
mode = True \n 
buffer . append ( line ) \n 
~~~ m = p_thread_id . match ( line ) \n 
~~~ tid_str = m . group ( 2 ) [ 1 : - 1 ] \n 
thread_id = re . match ( , tid_str ) . group ( 2 ) \n 
~~ m = p_object_id . match ( line ) \n 
~~~ oid_str = m . group ( 2 ) [ 1 : - 1 ] \n 
object_id = re . match ( , oid_str ) . group ( 2 ) \n 
~~ m = p_width . match ( line ) \n 
~~~ width_str = m . group ( 2 ) \n 
width = re . match ( , width_str ) . group ( 1 ) \n 
~~ m = p_depth . match ( line ) \n 
~~~ depth_str = m . group ( 2 ) \n 
depth = re . match ( , depth_str ) . group ( 1 ) \n 
~~ m = p_indexwidth . match ( line ) \n 
~~~ indexwidth_str = m . group ( 2 ) \n 
indexwidth = re . match ( , indexwidth_str ) . group ( 1 ) \n 
~~ m = p_logdepth . match ( line ) \n 
~~~ logdepth_str = m . group ( 2 ) \n 
logdepth = re . match ( , logdepth_str ) . group ( 1 ) \n 
~~ m = p_sub_id . match ( line ) \n 
~~~ sid_str = m . group ( 2 ) \n 
sub_id_m = re . search ( , sid_str ) \n 
sub_id = sub_id_m . group ( 0 ) \n 
sub_id_num = sub_id_m . group ( 2 ) \n 
sub_id_base = ( 10 if sub_id_m . group ( 1 ) . count ( "\'d" ) > 0 else \n 
16 if sub_id_m . group ( 1 ) . count ( "\'h" ) > 0 else \n 
2 if sub_id_m . group ( 1 ) . count ( "\'b" ) > 0 else \n 
10 ) \n 
~~ ~~ if mode : \n 
if module_name . count ( ) > 0 : \n 
~~ if module_name . count ( ) > 0 : \n 
print ( . join ( buffer [ 1 : ] ) ) \n 
~~ mode = False \n 
print ( line , end = ) \n 
~~ ~~ main ( ) \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) \n 
import pyverilog . utils . version \n 
from pyverilog . dataflow . dataflow_analyzer import VerilogDataflowAnalyzer \n 
VERSION = pyverilog . utils . version . VERSION \n 
def showVersion ( ) : \n 
~~~ print ( INFO ) \n 
print ( VERSION ) \n 
print ( USAGE ) \n 
sys . exit ( ) \n 
~~ optparser = OptionParser ( ) \n 
optparser . add_option ( "-v" , "--version" , action = "store_true" , dest = "showversion" , \n 
optparser . add_option ( "-I" , "--include" , dest = "include" , action = "append" , \n 
optparser . add_option ( "-D" , dest = "define" , action = "append" , \n 
optparser . add_option ( "-t" , "--top" , dest = "topmodule" , \n 
optparser . add_option ( "--nobind" , action = "store_true" , dest = "nobind" , \n 
optparser . add_option ( "--noreorder" , action = "store_true" , dest = "noreorder" , \n 
( options , args ) = optparser . parse_args ( ) \n 
filelist = args \n 
if options . showversion : \n 
~~~ showVersion ( ) \n 
~~ for f in filelist : \n 
~~ if len ( filelist ) == 0 : \n 
~~ analyzer = VerilogDataflowAnalyzer ( filelist , options . topmodule , \n 
noreorder = options . noreorder , \n 
nobind = options . nobind , \n 
preprocess_include = options . include , \n 
preprocess_define = options . define ) \n 
analyzer . generate ( ) \n 
directives = analyzer . get_directives ( ) \n 
for dr in sorted ( directives , key = lambda x : str ( x ) ) : \n 
~~~ print ( dr ) \n 
~~ instances = analyzer . getInstances ( ) \n 
for module , instname in sorted ( instances , key = lambda x : str ( x [ 1 ] ) ) : \n 
~~~ print ( ( module , instname ) ) \n 
~~ if options . nobind : \n 
signals = analyzer . getSignals ( ) \n 
for sig in signals : \n 
~~~ print ( sig ) \n 
consts = analyzer . getConsts ( ) \n 
for con in consts : \n 
~~~ print ( con ) \n 
~~~ terms = analyzer . getTerms ( ) \n 
for tk , tv in sorted ( terms . items ( ) , key = lambda x : str ( x [ 0 ] ) ) : \n 
~~~ print ( tv . tostr ( ) ) \n 
~~ binddict = analyzer . getBinddict ( ) \n 
for bk , bv in sorted ( binddict . items ( ) , key = lambda x : str ( x [ 0 ] ) ) : \n 
~~~ for bvi in bv : \n 
~~~ print ( bvi . tostr ( ) ) \n 
~~ ~~ ~~ ~~ if __name__ == : \n 
from pyverilog . dataflow . dataflow import * \n 
def replaceUndefined ( tree , termname ) : \n 
~~~ if tree is None : return DFTerminal ( termname ) \n 
if isinstance ( tree , DFUndefined ) : return DFTerminal ( termname ) \n 
if isinstance ( tree , DFConstant ) : return tree \n 
if isinstance ( tree , DFEvalValue ) : return tree \n 
if isinstance ( tree , DFTerminal ) : return tree \n 
if isinstance ( tree , DFBranch ) : \n 
~~~ condnode = replaceUndefined ( tree . condnode , termname ) \n 
truenode = replaceUndefined ( tree . truenode , termname ) \n 
falsenode = replaceUndefined ( tree . falsenode , termname ) \n 
return DFBranch ( condnode , truenode , falsenode ) \n 
~~ if isinstance ( tree , DFOperator ) : \n 
~~~ nextnodes = [ ] \n 
for n in tree . nextnodes : \n 
~~~ nextnodes . append ( replaceUndefined ( n , termname ) ) \n 
~~ return DFOperator ( tuple ( nextnodes ) , tree . operator ) \n 
~~ if isinstance ( tree , DFPartselect ) : \n 
~~~ msb = replaceUndefined ( tree . msb , termname ) \n 
lsb = replaceUndefined ( tree . lsb , termname ) \n 
var = replaceUndefined ( tree . var , termname ) \n 
return DFPartselect ( var , msb , lsb ) \n 
~~ if isinstance ( tree , DFPointer ) : \n 
~~~ ptr = replaceUndefined ( tree . ptr , termname ) \n 
return DFPointer ( var , ptr ) \n 
~~ if isinstance ( tree , DFConcat ) : \n 
~~ return DFConcat ( tuple ( nextnodes ) ) \n 
~~ raise DefinitionError ( % ( str ( type ( tree ) ) , str ( tree ) ) ) \n 
from pyverilog . dataflow . optimizer import VerilogDataflowOptimizer \n 
from pyverilog . controlflow . controlflow_analyzer import VerilogControlflowAnalyzer \n 
codedir = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) + \n 
def test ( ) : \n 
~~~ filelist = [ codedir + ] \n 
topmodule = \n 
noreorder = False \n 
nobind = False \n 
include = None \n 
define = None \n 
analyzer = VerilogDataflowAnalyzer ( filelist , topmodule , \n 
noreorder = noreorder , \n 
nobind = nobind , \n 
preprocess_include = include , \n 
preprocess_define = define ) \n 
instances = analyzer . getInstances ( ) \n 
terms = analyzer . getTerms ( ) \n 
binddict = analyzer . getBinddict ( ) \n 
optimizer = VerilogDataflowOptimizer ( terms , binddict ) \n 
optimizer . resolveConstant ( ) \n 
c_analyzer = VerilogControlflowAnalyzer ( topmodule , terms , \n 
binddict , \n 
resolved_terms = optimizer . getResolvedTerms ( ) , \n 
resolved_binddict = optimizer . getResolvedBinddict ( ) , \n 
constlist = optimizer . getConstlist ( ) \n 
for tk in sorted ( c_analyzer . resolved_terms . keys ( ) , key = lambda x : str ( x ) ) : \n 
~~~ tree = c_analyzer . makeTree ( tk ) \n 
output . append ( str ( tk ) + + tree . tocode ( ) ) \n 
~~ rslt = . join ( output ) + \n 
print ( rslt ) \n 
assert ( expected == rslt ) \n 
~~~ test ( ) \n 
import dataflow_example \n 
~~~ test_module = dataflow_example . mkTest ( ) \n 
code = test_module . to_verilog ( ) \n 
from pyverilog . vparser . parser import VerilogParser \n 
from pyverilog . ast_code_generator . codegen import ASTCodeGenerator \n 
parser = VerilogParser ( ) \n 
expected_ast = parser . parse ( expected_verilog ) \n 
codegen = ASTCodeGenerator ( ) \n 
expected_code = codegen . visit ( expected_ast ) \n 
assert ( expected_code == code ) \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) ) \n 
from veriloggen import * \n 
def mkLed ( ) : \n 
~~~ m = Module ( ) \n 
interval = m . Parameter ( , 16 ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
led = m . OutputReg ( , 8 , initval = 0 ) \n 
count = m . Reg ( , 32 , initval = 0 ) \n 
seq = Seq ( m , , clk , rst ) \n 
seq . add ( Systask ( , , led , count ) ) \n 
seq . add ( count ( count + 1 ) , cond = count < interval - 1 ) \n 
seq . add ( count ( 0 ) , cond = count == interval - 1 ) \n 
seq . add ( led ( led + 1 ) , cond = count == interval - 1 ) \n 
seq . make_always ( ) \n 
~~ def mkTest ( ) : \n 
led = mkLed ( ) \n 
params = m . copy_params ( led ) \n 
ports = m . copy_sim_ports ( led ) \n 
clk = ports [ ] \n 
rst = ports [ ] \n 
uut = m . Instance ( led , , \n 
params = m . connect_params ( led ) , \n 
ports = m . connect_ports ( led ) ) \n 
simulation . setup_clock ( m , clk , hperiod = 5 ) \n 
init = simulation . setup_reset ( m , rst , m . make_reset ( ) , period = 100 ) \n 
init . add ( \n 
Delay ( 1000 ) , \n 
Systask ( ) , \n 
~~~ test = mkTest ( ) \n 
verilog = test . to_verilog ( ) \n 
print ( verilog ) \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ \n 
width = m . Parameter ( , 8 ) \n 
led = m . OutputReg ( , width ) \n 
count = m . Reg ( , 32 ) \n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( rst ) ( \n 
count ( 0 ) \n 
) . Else ( \n 
count ( Cond ( count == 1023 , 0 , count + 1 ) ) \n 
led ( 0 ) \n 
led ( Cond ( count == 1024 - 1 , led + 1 , led ) ) \n 
~~~ led = mkLed ( ) \n 
verilog = led . to_verilog ( ) \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os \n 
def mkSub ( ) : \n 
count = m . OutputReg ( , 32 ) \n 
If ( count == 1023 ) ( \n 
count ( count + 1 ) \n 
~~ def mkLed ( ) : \n 
count = m . Wire ( , 32 ) \n 
sub = mkSub ( ) \n 
m . Instance ( sub , , m . connect_params ( sub ) , m . connect_ports ( sub ) ) \n 
led ( led + 1 ) \n 
inst_sub = m . Reg ( , 32 ) \n 
~~~ print ( e . args [ 0 ] ) \n 
#print(verilog) \n 
If ( count == 1024 - 1 ) ( \n 
led ( led + 1 ) , \n 
SingleStatement ( SystemTask ( , , led ) ) \n 
import veriloggen . dataflow as dataflow \n 
def mkMain ( ) : \n 
~~~ x = dataflow . Variable ( , valid = , ready = , point = 8 ) \n 
y = dataflow . Variable ( , valid = , ready = , point = 4 ) \n 
z = x * y \n 
z . output ( , valid = , ready = ) \n 
df = dataflow . Dataflow ( z ) \n 
m = df . to_module ( ) \n 
main = mkMain ( ) \n 
params = m . copy_params ( main ) \n 
ports = m . copy_sim_ports ( main ) \n 
xdata = ports [ ] \n 
xvalid = ports [ ] \n 
xready = ports [ ] \n 
ydata = ports [ ] \n 
yvalid = ports [ ] \n 
yready = ports [ ] \n 
zdata = ports [ ] \n 
zvalid = ports [ ] \n 
zready = ports [ ] \n 
xdata_orig = m . RegLike ( ports [ ] , name = , initval = 0 ) \n 
ydata_orig = m . RegLike ( ports [ ] , name = , initval = 0 ) \n 
zdata_orig = m . WireLike ( ports [ ] , name = ) \n 
m . Always ( ) ( xdata ( fixed . to_fixed ( xdata_orig , 8 ) ) ) \n 
m . Always ( ) ( ydata ( fixed . to_fixed ( ydata_orig , 4 ) ) ) \n 
m . Assign ( zdata_orig ( fixed . fixed_to_int ( zdata , 8 ) ) ) \n 
uut = m . Instance ( main , , \n 
params = m . connect_params ( main ) , \n 
ports = m . connect_ports ( main ) ) \n 
reset_done = m . Reg ( , initval = 0 ) \n 
reset_stmt = [ ] \n 
reset_stmt . append ( reset_done ( 0 ) ) \n 
reset_stmt . append ( xdata ( 0 ) ) \n 
reset_stmt . append ( xvalid ( 0 ) ) \n 
reset_stmt . append ( ydata ( 0 ) ) \n 
reset_stmt . append ( yvalid ( 0 ) ) \n 
reset_stmt . append ( zready ( 0 ) ) \n 
reset_stmt . append ( xdata_orig ( 0 ) ) \n 
reset_stmt . append ( ydata_orig ( 0 ) ) \n 
simulation . setup_waveform ( m , uut , xdata_orig , ydata_orig , zdata_orig ) \n 
init = simulation . setup_reset ( m , rst , reset_stmt , period = 100 ) \n 
nclk = simulation . next_clock \n 
reset_done ( 1 ) , \n 
nclk ( clk ) , \n 
Delay ( 10000 ) , \n 
def send ( name , data , valid , ready , step = 1 , waitnum = 10 ) : \n 
~~~ fsm = FSM ( m , name + , clk , rst ) \n 
count = m . TmpReg ( 32 , initval = 0 ) \n 
fsm . add ( valid ( 0 ) ) \n 
fsm . goto_next ( cond = reset_done ) \n 
for _ in range ( waitnum ) : \n 
~~~ fsm . goto_next ( ) \n 
~~ fsm . add ( valid ( 1 ) ) \n 
fsm . goto_next ( ) \n 
fsm . add ( data ( data + step ) , cond = ready ) \n 
fsm . add ( count . inc ( ) , cond = ready ) \n 
fsm . add ( valid ( 0 ) , cond = AndList ( count == 5 , ready ) ) \n 
fsm . goto_next ( cond = AndList ( count == 5 , ready ) ) \n 
fsm . add ( valid ( 0 ) , cond = AndList ( count == 10 , ready ) ) \n 
fsm . goto_next ( cond = AndList ( count == 10 , ready ) ) \n 
fsm . make_always ( ) \n 
~~ def receive ( name , data , valid , ready , waitnum = 10 ) : \n 
fsm . add ( ready ( 0 ) ) \n 
yinit = fsm . current ( ) \n 
fsm . add ( ready ( 1 ) , cond = valid ) \n 
fsm . goto_next ( cond = valid ) \n 
for i in range ( waitnum ) : \n 
~~~ fsm . add ( ready ( 0 ) ) \n 
~~ fsm . goto ( yinit ) \n 
~~ send ( , xdata_orig , xvalid , xready , step = 1 , waitnum = 10 ) \n 
send ( , ydata_orig , yvalid , yready , step = 1 , waitnum = 20 ) \n 
receive ( , zdata , zvalid , zready , waitnum = 50 ) \n 
If ( reset_done ) ( \n 
If ( AndList ( xvalid , xready ) ) ( \n 
Systask ( , , xdata_orig ) \n 
If ( AndList ( yvalid , yready ) ) ( \n 
Systask ( , , ydata_orig ) \n 
If ( AndList ( zvalid , zready ) ) ( \n 
Systask ( , , zdata_orig ) \n 
sim = simulation . Simulator ( test ) \n 
#sim.view_waveform(background=True) \n 
import dataflow_mul \n 
~~~ test_module = dataflow_mul . mkTest ( ) \n 
valid = m . OutputReg ( , initval = 0 ) \n 
count = m . Reg ( , width = 32 , initval = 0 ) \n 
up = m . Wire ( ) \n 
down = m . Wire ( ) \n 
m . Assign ( up ( 1 ) ) \n 
m . Assign ( down ( 0 ) ) \n 
fsm = FSM ( m , , clk , rst ) \n 
~~ c = count >= 16 \n 
fsm . add ( valid ( up ) , cond = c , keep = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . add ( valid ( down ) , cond = c , delay = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . goto_next ( cond = c ) \n 
for i in range ( 8 ) : \n 
~~ c = count >= 32 \n 
~~~ fsm . add ( valid ( up ) , cond = c , delay = 1 , keep = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . add ( valid ( down ) , cond = c , delay = 4 , eager_val = True , lazy_cond = True ) \n 
~~ fsm . make_always ( reset = [ count . reset ( ) ] , body = [ count ( count + 1 ) ] ) \n 
clk = m . Reg ( ) \n 
rst = m . Reg ( ) \n 
valid = m . Wire ( ) \n 
uut = m . Instance ( mkLed ( ) , , \n 
ports = ( ( , clk ) , ( , rst ) , ( , valid ) ) ) \n 
simulation . setup_waveform ( m , uut ) \n 
init = simulation . setup_reset ( m , rst , period = 100 ) \n 
import pipeline_draw_graph \n 
~~~ test_module = pipeline_draw_graph . mkTest ( ) \n 
import read_verilog_module_str \n 
~~~ test_module = read_verilog_module_str . mkTop ( ) \n 
import veriloggen . core . vtypes as vtypes \n 
import veriloggen . core . module as module \n 
def mkMultiplierCore ( index , lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
~~~ retwidth = lwidth + rwidth \n 
m = module . Module ( % index ) \n 
update = m . Input ( ) \n 
a = m . Input ( , lwidth ) \n 
b = m . Input ( , rwidth ) \n 
c = m . Output ( , retwidth ) \n 
_a = m . Reg ( , lwidth , signed = lsigned ) \n 
_b = m . Reg ( , rwidth , signed = rsigned ) \n 
tmpval = [ m . Reg ( % i , retwidth , signed = True ) for i in range ( depth - 1 ) ] \n 
rslt = m . Wire ( , retwidth , signed = True ) \n 
__a = _a \n 
__b = _b \n 
if not lsigned : \n 
~~~ __a = vtypes . SystemTask ( , vtypes . Cat ( vtypes . Int ( 0 , width = 1 ) , _a ) ) \n 
~~ if not rsigned : \n 
~~~ __b = vtypes . SystemTask ( , vtypes . Cat ( vtypes . Int ( 0 , width = 1 ) , _b ) ) \n 
~~ m . Assign ( rslt ( __a * __b ) ) \n 
m . Assign ( c ( tmpval [ depth - 2 ] ) ) \n 
m . Always ( vtypes . Posedge ( clk ) ) ( \n 
vtypes . If ( update ) ( \n 
_a ( a ) , \n 
_b ( b ) , \n 
tmpval [ 0 ] ( rslt ) , \n 
[ tmpval [ i ] ( tmpval [ i - 1 ] ) for i in range ( 1 , depth - 1 ) ] \n 
~~ def mkMultiplier ( index , lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
retwidth = lwidth + rwidth \n 
mult = mkMultiplierCore ( index , lwidth , rwidth , lsigned , rsigned , depth ) \n 
enable = m . Input ( ) \n 
valid = m . Output ( ) \n 
valid_reg = [ m . Reg ( % i ) for i in range ( depth ) ] \n 
m . Assign ( valid ( valid_reg [ depth - 1 ] ) ) \n 
vtypes . If ( rst ) ( \n 
[ valid_reg [ i ] ( 0 ) for i in range ( depth ) ] \n 
valid_reg [ 0 ] ( enable ) , \n 
[ valid_reg [ i ] ( valid_reg [ i - 1 ] ) for i in range ( 1 , depth ) ] \n 
ports = [ ( , clk ) , ( , update ) , ( , a ) , ( , b ) , ( , c ) ] \n 
m . Instance ( mult , , ports = ports ) \n 
~~ index_count = 0 \n 
def get_mul ( lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
~~~ global index_count \n 
mul = mkMultiplier ( index_count , lwidth , rwidth , lsigned , rsigned , depth ) \n 
index_count += 1 \n 
return mul \n 
index_count = 0 \n 
~~ from pymysql import OperationalError , Warning \n 
__all__ = [ "TestLoadLocal" ] \n 
class TestLoadLocal ( base . PyMySQLTestCase ) : \n 
~~~ def test_no_file ( self ) : \n 
conn = self . connections [ 0 ] \n 
c = conn . cursor ( ) \n 
~~~ self . assertRaises ( \n 
OperationalError , \n 
c . execute , \n 
c . close ( ) \n 
~~ ~~ def test_load_file ( self ) : \n 
filename = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , \n 
~~~ c . execute ( \n 
self . assertEqual ( 22749 , c . fetchone ( ) [ 0 ] ) \n 
~~ ~~ def test_load_warnings ( self ) : \n 
~~~ with warnings . catch_warnings ( record = True ) as w : \n 
~~~ warnings . simplefilter ( ) \n 
c . execute ( \n 
self . assertEqual ( w [ 0 ] . category , Warning ) \n 
unittest . main ( ) \n 
MROW = 1000 * 1000. \n 
COLDCACHE = 5 \n 
WARMCACHE = 5 \n 
rdm_cod = [ , ] \n 
def get_nrows ( nrows_str ) : \n 
~~~ if nrows_str . endswith ( "k" ) : \n 
~~~ return int ( float ( nrows_str [ : - 1 ] ) * 1000 ) \n 
~~ elif nrows_str . endswith ( "m" ) : \n 
~~~ return int ( float ( nrows_str [ : - 1 ] ) * 1000 * 1000 ) \n 
~~ elif nrows_str . endswith ( "g" ) : \n 
~~~ return int ( float ( nrows_str [ : - 1 ] ) * 1000 * 1000 * 1000 ) \n 
~~ ~~ class DB ( object ) : \n 
~~~ def __init__ ( self , nrows , rng , userandom ) : \n 
~~~ global step , scale \n 
self . step = STEP \n 
self . scale = SCALE \n 
self . rng = rng \n 
self . userandom = userandom \n 
self . filename = . join ( [ rdm_cod [ userandom ] , nrows ] ) \n 
self . nrows = get_nrows ( nrows ) \n 
~~ def get_db_size ( self ) : \n 
stdout = subprocess . PIPE ) . stdout \n 
line = [ l for l in sout ] [ 0 ] \n 
return int ( line . split ( ) [ 0 ] ) \n 
~~ def print_mtime ( self , t1 , explain ) : \n 
~~~ mtime = time ( ) - t1 \n 
print ( "%s:" % explain , round ( mtime , 6 ) ) \n 
print ( "Krows/s:" , round ( ( self . nrows / 1000. ) / mtime , 6 ) ) \n 
~~ def print_qtime ( self , colname , ltimes ) : \n 
print ( "Mrows/s:" , round ( ( self . nrows / ( MROW ) ) / qtime1 , 6 ) ) \n 
~~ def norm_times ( self , ltimes ) : \n 
lmean = ltimes . mean ( ) \n 
lstd = ltimes . std ( ) \n 
ntimes = ltimes [ ltimes < lmean + lstd ] \n 
nmean = ntimes . mean ( ) \n 
nstd = ntimes . std ( ) \n 
return nmean , nstd \n 
~~ def print_qtime_idx ( self , colname , ltimes , repeated , verbose ) : \n 
~~~ if repeated : \n 
~~ ltimes = numpy . array ( ltimes ) \n 
ntimes = len ( ltimes ) \n 
ctimes = ltimes [ 1 : COLDCACHE ] \n 
cmean , cstd = self . norm_times ( ctimes ) \n 
wtimes = ltimes [ WARMCACHE : ] \n 
wmean , wstd = self . norm_times ( wtimes ) \n 
numpy . histogram ( wtimes ) ) \n 
round ( qtime1 , prec ) ) \n 
round ( cmean , prec ) , "+-" , round ( cstd , prec ) ) \n 
round ( wmean , prec ) , "+-" , round ( wstd , prec ) ) \n 
~~ def print_db_sizes ( self , init , filled , indexed ) : \n 
~~~ table_size = ( filled - init ) / 1024. \n 
indexes_size = ( indexed - filled ) / 1024. \n 
~~ def fill_arrays ( self , start , stop ) : \n 
~~~ arr_f8 = numpy . arange ( start , stop , dtype = ) \n 
arr_i4 = numpy . arange ( start , stop , dtype = ) \n 
if self . userandom : \n 
~~~ arr_f8 += numpy . random . normal ( 0 , stop * self . scale , \n 
size = stop - start ) \n 
arr_i4 = numpy . array ( arr_f8 , dtype = ) \n 
~~ return arr_i4 , arr_f8 \n 
~~ def create_db ( self , dtype , kind , optlevel , verbose ) : \n 
~~~ self . con = self . open_db ( remove = 1 ) \n 
self . create_table ( self . con ) \n 
init_size = self . get_db_size ( ) \n 
t1 = time ( ) \n 
self . fill_table ( self . con ) \n 
table_size = self . get_db_size ( ) \n 
self . print_mtime ( t1 , ) \n 
self . index_db ( dtype , kind , optlevel , verbose ) \n 
indexes_size = self . get_db_size ( ) \n 
self . print_db_sizes ( init_size , table_size , indexes_size ) \n 
self . close_db ( self . con ) \n 
~~ def index_db ( self , dtype , kind , optlevel , verbose ) : \n 
~~~ if dtype == "int" : \n 
~~~ idx_cols = [ ] \n 
~~ elif dtype == "float" : \n 
~~~ idx_cols = [ , ] \n 
~~ for colname in idx_cols : \n 
~~~ t1 = time ( ) \n 
self . index_col ( self . con , colname , kind , optlevel , verbose ) \n 
self . print_mtime ( t1 , % colname ) \n 
~~ ~~ def query_db ( self , niter , dtype , onlyidxquery , onlynonidxquery , \n 
avoidfscache , verbose , inkernel ) : \n 
~~~ self . con = self . open_db ( ) \n 
if dtype == "int" : \n 
~~~ reg_cols = [ ] \n 
idx_cols = [ ] \n 
~~~ reg_cols = [ , ] \n 
idx_cols = [ , ] \n 
~~ if avoidfscache : \n 
~~~ rseed = int ( numpy . random . randint ( self . nrows ) ) \n 
~~~ rseed = 19 \n 
~~ numpy . random . seed ( rseed ) \n 
base = numpy . random . randint ( self . nrows ) \n 
if not onlyidxquery : \n 
~~~ for colname in reg_cols : \n 
~~~ ltimes = [ ] \n 
random . seed ( rseed ) \n 
for i in range ( NI_NTIMES ) : \n 
results = self . do_query ( self . con , colname , base , inkernel ) \n 
ltimes . append ( time ( ) - t1 ) \n 
~~ if verbose : \n 
~~ self . print_qtime ( colname , ltimes ) \n 
~~ self . close_db ( self . con ) \n 
self . con = self . open_db ( ) \n 
~~ if not onlynonidxquery : \n 
~~~ for colname in idx_cols : \n 
numpy . random . seed ( rseed ) \n 
rndbase = numpy . random . randint ( self . nrows , size = niter ) \n 
for i in range ( niter ) : \n 
~~~ base = rndbase [ i ] \n 
~~ self . print_qtime_idx ( colname , ltimes , False , verbose ) \n 
ltimes = [ ] \n 
~~ ~~ self . close_db ( self . con ) \n 
~~ def close_db ( self , con ) : \n 
~~~ con . close ( ) \n 
import getopt \n 
~~~ import psyco \n 
psyco_imported = 1 \n 
~~~ psyco_imported = 0 \n 
~~~ opts , pargs = getopt . getopt ( \n 
sys . argv [ 1 : ] , ) \n 
~~~ sys . stderr . write ( usage ) \n 
~~ usepytables = 0 \n 
usepostgres = 0 \n 
verbose = 0 \n 
doprofile = 0 \n 
dokprofile = 0 \n 
usepsyco = 0 \n 
userandom = 0 \n 
docreate = 0 \n 
optlevel = 0 \n 
kind = "medium" \n 
docompress = 0 \n 
complib = "zlib" \n 
doquery = False \n 
onlyidxquery = False \n 
onlynonidxquery = False \n 
inkernel = True \n 
avoidfscache = 0 \n 
rng = [ - 1000 , - 1000 ] \n 
repeatquery = 0 \n 
repeatvalue = 0 \n 
krows = \n 
niter = READ_TIMES \n 
dtype = "all" \n 
datadir = "data.nobackup" \n 
for option in opts : \n 
~~~ if option [ 0 ] == : \n 
~~~ usepytables = 1 \n 
~~ elif option [ 0 ] == : \n 
~~~ usepostgres = 1 \n 
~~~ verbose = 1 \n 
~~~ doprofile = 1 \n 
~~~ dokprofile = 1 \n 
~~~ usepsyco = 1 \n 
~~~ userandom = 1 \n 
~~~ docreate = 1 \n 
~~~ doquery = True \n 
onlyidxquery = True \n 
onlynonidxquery = True \n 
inkernel = False \n 
~~~ avoidfscache = 1 \n 
~~~ docompress = int ( option [ 1 ] ) \n 
~~~ complib = option [ 1 ] \n 
~~~ rng = [ int ( i ) for i in option [ 1 ] . split ( "," ) ] \n 
~~~ niter = int ( option [ 1 ] ) \n 
~~~ krows = option [ 1 ] \n 
~~~ datadir = option [ 1 ] \n 
~~~ optlevel = int ( option [ 1 ] ) \n 
~~~ if option [ 1 ] in ( , , , ) : \n 
~~~ kind = option [ 1 ] \n 
"\'ultralight\'" ) \n 
~~ ~~ elif option [ 0 ] == : \n 
~~~ if option [ 1 ] in ( , ) : \n 
~~~ dtype = option [ 1 ] \n 
~~~ repeatquery = 1 \n 
repeatvalue = int ( option [ 1 ] ) \n 
~~ ~~ if not usepytables and not usepostgres : \n 
~~ if usepytables : \n 
~~~ from pytables_backend import PyTables_DB \n 
db = PyTables_DB ( krows , rng , userandom , datadir , \n 
docompress , complib , kind , optlevel ) \n 
~~ elif usepostgres : \n 
~~~ from postgres_backend import Postgres_DB \n 
db = Postgres_DB ( krows , rng , userandom ) \n 
~~ if not avoidfscache : \n 
~~~ numpy . random . seed ( 20 ) \n 
~~~ if userandom : \n 
~~ if onlyidxquery : \n 
~~ ~~ if psyco_imported and usepsyco : \n 
~~~ psyco . bind ( db . create_db ) \n 
psyco . bind ( db . query_db ) \n 
~~ if docreate : \n 
~~~ if verbose : \n 
~~ db . create_db ( dtype , kind , optlevel , verbose ) \n 
~~ if doquery : \n 
if doprofile : \n 
~~~ import pstats \n 
import cProfile as prof \n 
prof . run ( \n 
stats = pstats . Stats ( ) \n 
stats . strip_dirs ( ) \n 
stats . sort_stats ( , ) \n 
~~~ stats . print_stats ( ) \n 
~~~ stats . print_stats ( 20 ) \n 
~~ ~~ elif dokprofile : \n 
~~~ from cProfile import Profile \n 
import lsprofcalltree \n 
prof = Profile ( ) \n 
kcg = lsprofcalltree . KCacheGrind ( prof ) \n 
ofile = open ( , ) \n 
kcg . output ( ofile ) \n 
ofile . close ( ) \n 
~~ elif doprofile : \n 
~~~ import hotshot \n 
import hotshot . stats \n 
prof = hotshot . Profile ( "indexed_search.prof" ) \n 
benchtime , stones = prof . run ( \n 
prof . close ( ) \n 
stats = hotshot . stats . load ( "indexed_search.prof" ) \n 
stats . print_stats ( 20 ) \n 
~~~ db . query_db ( niter , dtype , onlyidxquery , onlynonidxquery , \n 
avoidfscache , verbose , inkernel ) \n 
~~ ~~ if repeatquery : \n 
~~~ db . rng = [ 1 , 1 ] \n 
~~~ print ( "range:" , db . rng ) \n 
~~ db . query_db ( niter , dtype , onlyidxquery , onlynonidxquery , \n 
for i in range ( repeatvalue ) : \n 
~~~ for j in ( 1 , 2 , 5 ) : \n 
~~~ rng = j * 10 ** i \n 
db . rng = [ - rng / 2 , rng / 2 ] \n 
import matplotlib as mpl \n 
from pylab import * \n 
KB_ = 1024 \n 
MB_ = 1024 * KB_ \n 
GB_ = 1024 * MB_ \n 
linewidth = 2 \n 
markers = [ , , , , , , , , , ] \n 
markersize = 8 \n 
def get_values ( filename ) : \n 
~~~ f = open ( filename ) \n 
values = { "memcpyw" : [ ] , "memcpyr" : [ ] } \n 
for line in f : \n 
~~~ tmp = line . split ( ) [ 1 ] \n 
nthreads , size , elsize , sbits , codec , shuffle = [ i for i in tmp . split ( ) ] \n 
nthreads , size , elsize , sbits = map ( int , ( nthreads , size , elsize , sbits ) ) \n 
values [ "size" ] = size * NCHUNKS / MB_ ; \n 
values [ "elsize" ] = elsize ; \n 
values [ "sbits" ] = sbits ; \n 
values [ "codec" ] = codec \n 
values [ "shuffle" ] = shuffle \n 
( ratios , speedsw , speedsr ) = ( [ ] , [ ] , [ ] ) \n 
values [ nthreads ] = ( ratios , speedsw , speedsr ) \n 
~~ elif line . startswith ( ) : \n 
memcpyw = float ( tmp . split ( ) [ 1 ] ) \n 
values [ "memcpyw" ] . append ( memcpyw ) \n 
memcpyr = float ( tmp . split ( ) [ 1 ] ) \n 
values [ "memcpyr" ] . append ( memcpyr ) \n 
speedw = float ( tmp . split ( ) [ 1 ] ) \n 
ratio = float ( line . split ( ) [ - 1 ] ) \n 
speedsw . append ( speedw ) \n 
ratios . append ( ratio ) \n 
speedr = float ( tmp . split ( ) [ 1 ] ) \n 
speedsr . append ( speedr ) \n 
if "OK" not in line : \n 
~~ ~~ ~~ f . close ( ) \n 
return nthreads , values \n 
~~ def show_plot ( plots , yaxis , legends , gtitle , xmax = None ) : \n 
~~~ xlabel ( ) \n 
ylabel ( ) \n 
title ( gtitle ) \n 
xlim ( 0 , xmax ) \n 
ylim ( 0 , None ) \n 
grid ( True ) \n 
legend ( [ p [ 0 ] for p in plots \n 
if not isinstance ( p , mpl . lines . Line2D ) ] , \n 
legends , loc = "best" ) \n 
if outfile : \n 
savefig ( outfile , dpi = 64 ) \n 
~~~ show ( ) \n 
~~~ from optparse import OptionParser \n 
compress_title = \n 
decompress_title = \n 
yaxis = \n 
parser = OptionParser ( usage = usage ) \n 
parser . add_option ( , \n 
help = ( \n 
help = , ) \n 
default = None ) \n 
parser . add_option ( , , action = , \n 
default = False ) \n 
( options , args ) = parser . parse_args ( ) \n 
if len ( args ) == 0 : \n 
~~ elif len ( args ) > 1 : \n 
~~ if options . report and options . outfile : \n 
~~ if options . dspeed and options . cspeed : \n 
~~ elif options . cspeed : \n 
~~~ options . dspeed = False \n 
plot_title = compress_title \n 
~~~ options . dspeed = True \n 
plot_title = decompress_title \n 
~~ filename = args [ 0 ] \n 
cspeed = options . cspeed \n 
dspeed = options . dspeed \n 
if options . outfile : \n 
~~~ outfile = options . outfile \n 
~~ elif options . report : \n 
~~~ if cspeed : \n 
~~~ outfile = filename [ : filename . rindex ( ) ] + \n 
~~~ outfile = None \n 
~~ plots = [ ] \n 
legends = [ ] \n 
nthreads , values = get_values ( filename ) \n 
if options . limit : \n 
~~~ thread_range = eval ( options . limit ) \n 
~~~ thread_range = range ( 1 , nthreads + 1 ) \n 
~~ if options . title : \n 
~~~ plot_title = options . title \n 
~~ gtitle = plot_title \n 
for nt in thread_range : \n 
~~~ ( ratios , speedw , speedr ) = values [ nt ] \n 
if cspeed : \n 
~~~ speed = speedw \n 
~~~ speed = speedr \n 
~~ plot_ = plot ( ratios , speed , linewidth = 2 ) \n 
plots . append ( plot_ ) \n 
nmarker = nt \n 
if nt >= len ( markers ) : \n 
~~~ nmarker = nt % len ( markers ) \n 
~~ setp ( plot_ , marker = markers [ nmarker ] , markersize = markersize , \n 
linewidth = linewidth ) \n 
~~ if cspeed : \n 
~~~ mean = np . mean ( values [ "memcpyw" ] ) \n 
~~~ mean = np . mean ( values [ "memcpyr" ] ) \n 
~~ plot_ = axhline ( mean , linewidth = 3 , linestyle = , color = ) \n 
text ( 1.0 , mean + 50 , message ) \n 
show_plot ( plots , yaxis , legends , gtitle , xmax = int ( options . xmax ) if \n 
options . xmax else None ) \n 
import tables \n 
fileh = tables . open_file ( "attributes1.h5" , mode = "w" , \n 
root = fileh . root \n 
a = np . array ( [ 1 , 2 , 4 ] , np . int32 ) \n 
hdfarray . attrs . char = "1" \n 
hdfarray . attrs . int = 12 \n 
hdfarray . attrs . float = 12.32 \n 
hdfarray . attrs . object = { "a" : 32.1 , "b" : 1 , "c" : [ 1 , 2 ] } \n 
fileh . close ( ) \n 
def setUp ( filename ) : \n 
fileh . enable_undo ( ) \n 
return fileh \n 
~~ def tearDown ( fileh ) : \n 
~~~ fileh . disable_undo ( ) \n 
~~ def demo_6times3marks ( ) : \n 
fileh = setUp ( "undo-redo-6times3marks.h5" ) \n 
fileh . mark ( ) \n 
fileh . undo ( ) \n 
assert "/otherarray1" in fileh \n 
assert "/otherarray2" in fileh \n 
assert "/otherarray3" in fileh \n 
assert "/otherarray4" in fileh \n 
assert "/otherarray5" not in fileh \n 
assert "/otherarray6" not in fileh \n 
assert "/otherarray3" not in fileh \n 
assert "/otherarray4" not in fileh \n 
assert "/otherarray1" not in fileh \n 
assert "/otherarray2" not in fileh \n 
fileh . redo ( ) \n 
assert "/otherarray5" in fileh \n 
assert "/otherarray6" in fileh \n 
tearDown ( fileh ) \n 
~~ def demo_manyops ( ) : \n 
fileh = setUp ( "undo-redo-manyops.h5" ) \n 
new_node = fileh . copy_node ( , ) \n 
new_node = fileh . copy_children ( , , recursive = 1 ) \n 
fileh . rename_node ( , ) \n 
fileh . remove_node ( ) \n 
assert not in fileh \n 
assert in fileh \n 
assert fileh . root . agroup . anarray3 is new_node \n 
~~~ demo_6times3marks ( ) \n 
demo_manyops ( ) \n 
######################################################################## \n 
from . registry import class_name_dict , class_id_dict \n 
from . exceptions import ( ClosedNodeError , NodeError , UndoRedoWarning , \n 
PerformanceWarning ) \n 
from . path import join_path , split_path , isvisiblepath \n 
from . utils import lazyattr \n 
from . undoredo import move_to_shadow \n 
from . attributeset import AttributeSet , NotLoggedAttributeSet \n 
__docformat__ = \n 
def _closedrepr ( oldmethod ) : \n 
@ functools . wraps ( oldmethod ) \n 
def newmethod ( self ) : \n 
~~~ if not self . _v_isopen : \n 
~~~ cmod = self . __class__ . __module__ \n 
cname = self . __class__ . __name__ \n 
addr = hex ( id ( self ) ) \n 
return % ( cmod , cname , addr ) \n 
~~ return oldmethod ( self ) \n 
~~ return newmethod \n 
~~ class MetaNode ( type ) : \n 
def __new__ ( class_ , name , bases , dict_ ) : \n 
~~~ for mname in [ , ] : \n 
~~~ if mname in dict_ : \n 
~~~ dict_ [ mname ] = _closedrepr ( dict_ [ mname ] ) \n 
~~ ~~ return type . __new__ ( class_ , name , bases , dict_ ) \n 
~~ def __init__ ( class_ , name , bases , dict_ ) : \n 
~~~ super ( MetaNode , class_ ) . __init__ ( name , bases , dict_ ) \n 
class_name_dict [ class_ . __name__ ] = class_ \n 
cid = getattr ( class_ , , None ) \n 
if cid is not None : \n 
~~~ pcid = getattr ( base , , None ) \n 
if pcid == cid : \n 
~~~ class_id_dict [ cid ] = class_ \n 
~~ ~~ ~~ ~~ class Node ( six . with_metaclass ( MetaNode , object ) ) : \n 
_AttributeSet = AttributeSet \n 
def _g_getparent ( self ) : \n 
( parentpath , nodename ) = split_path ( self . _v_pathname ) \n 
return self . _v_file . _get_node ( parentpath ) \n 
~~ _v_parent = property ( _g_getparent ) \n 
@ lazyattr \n 
def _v_attrs ( self ) : \n 
return self . _AttributeSet ( self ) \n 
~~ def _g_gettitle ( self ) : \n 
if hasattr ( self . _v_attrs , ) : \n 
~~~ return self . _v_attrs . TITLE \n 
~~ ~~ def _g_settitle ( self , title ) : \n 
~~~ self . _v_attrs . TITLE = title \n 
~~ _v_title = property ( _g_gettitle , _g_settitle ) \n 
_v_isopen = False \n 
def __init__ ( self , parentnode , name , _log = True ) : \n 
~~~ if isinstance ( parentnode , class_name_dict [ ] ) : \n 
~~~ parentnode = parentnode . dereference ( ) \n 
~~ self . _v_file = None \n 
self . _v_isopen = False \n 
self . _v_pathname = None \n 
self . _v_name = None \n 
self . _v_depth = None \n 
self . _v_maxtreedepth = parentnode . _v_file . params [ ] \n 
self . _v__deleting = False \n 
self . _v_objectid = None \n 
self . _g_check_group ( parentnode ) \n 
parentnode . _g_check_open ( ) \n 
file_ = parentnode . _v_file \n 
if new : \n 
~~~ file_ . _check_writable ( ) \n 
~~ if new : \n 
~~~ parentnode . _g_refnode ( self , name , validate ) \n 
~~ self . _g_set_location ( parentnode , name ) \n 
~~~ self . _g_new ( parentnode , name , init = True ) \n 
~~~ self . _v_objectid = self . _g_create ( ) \n 
~~~ self . _v_objectid = self . _g_open ( ) \n 
~~ if new and _log and file_ . is_undo_enabled ( ) : \n 
~~~ self . _g_log_create ( ) \n 
~~ self . _g_post_init_hook ( ) \n 
~~~ self . _f_close ( ) \n 
~~ ~~ def _g_log_create ( self ) : \n 
~~~ self . _v_file . _log ( , self . _v_pathname ) \n 
~~ self . _v__deleting = True \n 
~~~ node_manager = self . _v_file . _node_manager \n 
node_manager . drop_node ( self , check_unregistered = False ) \n 
~~~ if self . _v_isopen : \n 
~~~ self . _v__deleting = True \n 
self . _f_close ( ) \n 
~~ ~~ ~~ def _g_pre_kill_hook ( self ) : \n 
~~ def _g_create ( self ) : \n 
~~ def _g_open ( self ) : \n 
~~ def _g_check_open ( self ) : \n 
if not self . _v_isopen : \n 
~~ def _g_set_location ( self , parentnode , name ) : \n 
parentdepth = parentnode . _v_depth \n 
self . _v_file = file_ \n 
self . _v_isopen = True \n 
root_uep = file_ . root_uep \n 
if name . startswith ( root_uep ) : \n 
~~~ assert parentdepth == 0 \n 
if root_uep == "/" : \n 
~~~ self . _v_pathname = name \n 
~~~ self . _v_pathname = name [ len ( root_uep ) : ] \n 
~~ _ , self . _v_name = split_path ( name ) \n 
self . _v_depth = name . count ( "/" ) - root_uep . count ( "/" ) + 1 \n 
~~~ self . _v_name = name \n 
self . _v_pathname = join_path ( parentnode . _v_pathname , name ) \n 
self . _v_depth = parentdepth + 1 \n 
~~ if parentdepth >= self . _v_maxtreedepth : \n 
% ( self . _v_pathname , self . _v_maxtreedepth ) , \n 
~~ if self . _v_pathname != : \n 
~~~ file_ . _node_manager . cache_node ( self , self . _v_pathname ) \n 
~~ ~~ def _g_update_location ( self , newparentpath ) : \n 
oldpath = self . _v_pathname \n 
newpath = join_path ( newparentpath , self . _v_name ) \n 
newdepth = newpath . count ( ) \n 
self . _v_pathname = newpath \n 
self . _v_depth = newdepth \n 
if newdepth > self . _v_maxtreedepth : \n 
% ( self . _v_maxtreedepth , ) , PerformanceWarning ) \n 
~~ node_manager = self . _v_file . _node_manager \n 
node_manager . rename_node ( oldpath , newpath ) \n 
self . _g_update_dependent ( ) \n 
~~ def _g_del_location ( self ) : \n 
node_manager = self . _v_file . _node_manager \n 
pathname = self . _v_pathname \n 
if not self . _v__deleting : \n 
~~~ node_manager . drop_from_cache ( pathname ) \n 
node_manager . registry . pop ( pathname , None ) \n 
~~ def _g_post_init_hook ( self ) : \n 
~~ def _g_update_dependent ( self ) : \n 
if in self . __dict__ : \n 
~~~ self . _v_attrs . _g_update_node_location ( self ) \n 
~~ ~~ def _f_close ( self ) : \n 
~~ myDict = self . __dict__ \n 
if in myDict : \n 
~~~ self . _v_attrs . _g_close ( ) \n 
~~ self . _g_del_location ( ) \n 
myDict . clear ( ) \n 
~~ def _g_remove ( self , recursive , force ) : \n 
parent = self . _v_parent \n 
parent . _g_unrefnode ( self . _v_name ) \n 
self . _g_delete ( parent ) \n 
~~ def _f_remove ( self , recursive = False , force = False ) : \n 
self . _g_check_open ( ) \n 
file_ = self . _v_file \n 
file_ . _check_writable ( ) \n 
if file_ . is_undo_enabled ( ) : \n 
~~~ self . _g_remove_and_log ( recursive , force ) \n 
~~~ self . _g_remove ( recursive , force ) \n 
~~ ~~ def _g_remove_and_log ( self , recursive , force ) : \n 
~~~ file_ = self . _v_file \n 
oldpathname = self . _v_pathname \n 
file_ . _log ( , oldpathname ) \n 
move_to_shadow ( file_ , oldpathname ) \n 
~~ def _g_move ( self , newparent , newname ) : \n 
oldparent = self . _v_parent \n 
oldname = self . _v_name \n 
newparent . _g_refnode ( self , newname ) \n 
oldparent . _g_unrefnode ( oldname ) \n 
self . _g_del_location ( ) \n 
self . _g_set_location ( newparent , newname ) \n 
self . _g_new ( newparent , self . _v_name , init = False ) \n 
self . _v_parent . _g_move_node ( oldparent . _v_objectid , oldname , \n 
newparent . _v_objectid , newname , \n 
oldpathname , self . _v_pathname ) \n 
~~ def _f_rename ( self , newname , overwrite = False ) : \n 
self . _f_move ( newname = newname , overwrite = overwrite ) \n 
~~ def _f_move ( self , newparent = None , newname = None , \n 
overwrite = False , createparents = False ) : \n 
if newparent is None and newname is None : \n 
~~ if newparent is None : \n 
~~~ newparent = oldparent \n 
~~ if newname is None : \n 
~~~ newname = oldname \n 
~~~ newfile = newparent . _v_file \n 
newpath = newparent . _v_pathname \n 
~~~ newfile = file_ \n 
newpath = newparent \n 
% ( newparent , ) ) \n 
~~ if newfile is not file_ : \n 
~~ file_ . _check_writable ( ) \n 
oldpath = oldparent . _v_pathname \n 
if newpath == oldpath and newname == oldname : \n 
~~ self . _g_check_not_contains ( newpath ) \n 
newparent = file_ . _get_or_create_path ( newparent , createparents ) \n 
self . _g_maybe_remove ( newparent , newname , overwrite ) \n 
self . _g_move ( newparent , newname ) \n 
~~~ self . _g_log_move ( oldpathname ) \n 
~~ ~~ def _g_log_move ( self , oldpathname ) : \n 
~~~ self . _v_file . _log ( , oldpathname , self . _v_pathname ) \n 
~~ def _g_copy ( self , newparent , newname , recursive , _log = True , ** kwargs ) : \n 
~~ def _g_copy_as_child ( self , newparent , ** kwargs ) : \n 
return self . _g_copy ( newparent , self . _v_name , \n 
recursive = False , _log = False , ** kwargs ) \n 
~~ def _f_copy ( self , newparent = None , newname = None , \n 
overwrite = False , recursive = False , createparents = False , \n 
srcfile = self . _v_file \n 
srcparent = self . _v_parent \n 
srcname = self . _v_name \n 
dstparent = newparent \n 
dstname = newname \n 
if dstparent is None and dstname is None : \n 
~~ if dstparent is None : \n 
~~~ dstparent = srcparent \n 
~~ if dstname is None : \n 
~~~ dstname = srcname \n 
~~~ dstfile = dstparent . _v_file \n 
dstpath = dstparent . _v_pathname \n 
~~~ dstfile = srcfile \n 
dstpath = dstparent \n 
% ( dstparent , ) ) \n 
~~ if dstfile is srcfile : \n 
~~~ srcpath = srcparent . _v_pathname \n 
if dstpath == srcpath and dstname == srcname : \n 
~~~ raise NodeError ( \n 
% self . _v_pathname ) \n 
~~ if recursive : \n 
~~~ self . _g_check_not_contains ( dstpath ) \n 
~~ ~~ dstparent = srcfile . _get_or_create_path ( dstparent , createparents ) \n 
if dstfile is not srcfile and srcfile . is_undo_enabled ( ) : \n 
UndoRedoWarning ) \n 
~~ self . _g_maybe_remove ( dstparent , dstname , overwrite ) \n 
return self . _g_copy ( dstparent , dstname , recursive , ** kwargs ) \n 
~~ def _f_isvisible ( self ) : \n 
return isvisiblepath ( self . _v_pathname ) \n 
~~ def _g_check_group ( self , node ) : \n 
~~~ if not isinstance ( node , class_name_dict [ ] ) : \n 
% node . _v_pathname ) \n 
~~ if not isinstance ( node , class_name_dict [ ] ) : \n 
~~ ~~ def _g_check_not_contains ( self , pathname ) : \n 
~~~ mypathname = self . _v_pathname \n 
or pathname == mypathname \n 
or pathname . startswith ( mypathname + ) ) : \n 
~~ ~~ def _g_maybe_remove ( self , parent , name , overwrite ) : \n 
~~~ if name in parent : \n 
~~~ if not overwrite : \n 
~~ parent . _f_get_child ( name ) . _f_remove ( True ) \n 
~~ ~~ def _g_check_name ( self , name ) : \n 
~~ ~~ def _f_getattr ( self , name ) : \n 
return getattr ( self . _v_attrs , name ) \n 
~~ def _f_setattr ( self , name , value ) : \n 
setattr ( self . _v_attrs , name , value ) \n 
~~ def _f_delattr ( self , name ) : \n 
delattr ( self . _v_attrs , name ) \n 
~~ ~~ class NotLoggedMixin : \n 
~~~ _AttributeSet = NotLoggedAttributeSet \n 
def _g_log_create ( self ) : \n 
~~ def _g_log_move ( self , oldpathname ) : \n 
~~ def _g_remove_and_log ( self , recursive , force ) : \n 
from tables import IsDescription , StringCol , BoolCol , IntCol , FloatCol \n 
from tables . node import NotLoggedMixin \n 
from tables . path import join_path \n 
from tables . tests import common \n 
from tables . tests . common import unittest \n 
from tables . tests . common import PyTablesTestCase as TestCase \n 
class BasicTestCase ( common . TempFileMixin , TestCase ) : \n 
_reopen_flag = False \n 
def _do_reopen ( self ) : \n 
~~~ if self . _reopen_flag : \n 
~~~ self . _reopen ( ) \n 
~~~ super ( BasicTestCase , self ) . setUp ( ) \n 
h5file = self . h5file \n 
root = h5file . root \n 
~~ def test00_simple ( self ) : \n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
~~ self . h5file . enable_undo ( ) \n 
self . h5file . undo ( ) \n 
self . assertTrue ( "/otherarray" not in self . h5file ) \n 
self . assertEqual ( self . h5file . _curaction , 0 ) \n 
self . assertEqual ( self . h5file . _curmark , 0 ) \n 
self . _do_reopen ( ) \n 
self . h5file . redo ( ) \n 
~~ self . assertTrue ( "/otherarray" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . _curaction , 1 ) \n 
~~ def test01_twice ( self ) : \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/otherarray" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 4 , 5 ] ) \n 
self . assertEqual ( self . h5file . _curaction , 2 ) \n 
~~ def test02_twice2 ( self ) : \n 
self . h5file . mark ( ) \n 
self . assertEqual ( self . h5file . _curaction , 3 ) \n 
self . assertEqual ( self . h5file . _curmark , 1 ) \n 
~~ def test03_6times3marks ( self ) : \n 
self . __class__ . __name__ ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" in self . h5file ) \n 
self . assertTrue ( "/otherarray4" in self . h5file ) \n 
self . assertTrue ( "/otherarray5" not in self . h5file ) \n 
self . assertTrue ( "/otherarray6" not in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
self . assertTrue ( "/otherarray1" not in self . h5file ) \n 
self . assertTrue ( "/otherarray5" in self . h5file ) \n 
self . assertTrue ( "/otherarray6" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ 5 , 6 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray4 . read ( ) , [ 6 , 7 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray5 . read ( ) , [ 7 , 8 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray6 . read ( ) , [ 8 , 9 ] ) \n 
~~ def test04_6times3marksro ( self ) : \n 
~~ self . h5file . mark ( ) \n 
~~ def test05_destructive ( self ) : \n 
~~ def test05b_destructive ( self ) : \n 
~~ def test05c_destructive ( self ) : \n 
~~ def test05d_destructive ( self ) : \n 
self . h5file . undo ( 0 ) \n 
~~ def test05e_destructive ( self ) : \n 
~~ def test05f_destructive ( self ) : \n 
self . h5file . create_array ( , , [ 1 ] ) \n 
self . assertTrue ( not in self . h5file ) \n 
newarr = self . h5file . create_array ( , , [ 1 ] ) \n 
self . assertTrue ( in self . h5file ) \n 
if not self . _reopen_flag : \n 
~~~ self . assertTrue ( self . h5file . root . newarray is newarr ) \n 
~~ ~~ def test06_totalunwind ( self ) : \n 
~~ def test07_totalrewind ( self ) : \n 
self . h5file . redo ( - 1 ) \n 
~~ def test08_marknames ( self ) : \n 
self . h5file . mark ( "first" ) \n 
self . h5file . mark ( "second" ) \n 
self . h5file . mark ( "third" ) \n 
self . h5file . undo ( "first" ) \n 
self . h5file . redo ( "third" ) \n 
self . h5file . undo ( "second" ) \n 
~~ def test08_initialmark ( self ) : \n 
initmid = self . h5file . get_current_mark ( ) \n 
self . h5file . undo ( initmid ) \n 
~~ def test09_marknames ( self ) : \n 
with self . assertRaises ( tables . UndoRedoError ) : \n 
~~~ self . h5file . undo ( "third" ) \n 
~~ self . h5file . redo ( "third" ) \n 
~~~ self . h5file . redo ( "second" ) \n 
~~ self . assertTrue ( "/otherarray1" in self . h5file ) \n 
~~ def test10_goto ( self ) : \n 
self . h5file . goto ( "first" ) \n 
self . h5file . goto ( "third" ) \n 
self . h5file . goto ( "second" ) \n 
self . h5file . goto ( - 1 ) \n 
~~ def test10_gotoint ( self ) : \n 
self . h5file . goto ( 1 ) \n 
self . h5file . goto ( 0 ) \n 
self . h5file . goto ( 3 ) \n 
self . h5file . goto ( 2 ) \n 
~~ def test11_contiguous ( self ) : \n 
m1 = self . h5file . mark ( ) \n 
m2 = self . h5file . mark ( ) \n 
self . assertNotEqual ( m1 , m2 ) \n 
self . h5file . undo ( m1 ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , m1 ) \n 
self . h5file . redo ( m2 ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , m2 ) \n 
self . h5file . goto ( m1 ) \n 
self . h5file . goto ( m2 ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , 0 ) \n 
~~ def test12_keepMark ( self ) : \n 
mid = self . h5file . mark ( ) \n 
self . assertTrue ( mid is not None ) \n 
~~ def test13_severalEnableDisable ( self ) : \n 
self . h5file . disable_undo ( ) \n 
self . h5file . enable_undo ( ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , mid ) \n 
~~ ~~ class PersistenceTestCase ( BasicTestCase ) : \n 
_reopen_flag = True \n 
~~ class CreateArrayTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( CreateArrayTestCase , self ) . setUp ( ) \n 
~~ def test00 ( self ) : \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 1 , 2 ] ) \n 
~~ def test01 ( self ) : \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 2 , 3 ] ) \n 
~~ def test02 ( self ) : \n 
self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ 3 , 4 ] ) \n 
~~ def test03 ( self ) : \n 
self . h5file . create_array ( , , \n 
self . assertTrue ( "/agroup/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/agroup/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3/otherarray3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup . otherarray2 . title , \n 
self . assertEqual ( self . h5file . root . agroup . agroup3 . otherarray3 . title , \n 
self . assertEqual ( self . h5file . root . agroup . otherarray2 . read ( ) , [ 2 , 3 ] ) \n 
self . assertEqual ( self . h5file . root . agroup . agroup3 . otherarray3 . read ( ) , \n 
[ 3 , 4 ] ) \n 
~~ ~~ class CreateGroupTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( CreateGroupTestCase , self ) . setUp ( ) \n 
self . assertTrue ( "/othergroup1" not in self . h5file ) \n 
self . assertTrue ( "/othergroup1" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup1 . _v_title , \n 
self . assertTrue ( "/othergroup2" not in self . h5file ) \n 
self . assertTrue ( "/othergroup2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup2 . _v_title , \n 
self . assertTrue ( "/othergroup3" not in self . h5file ) \n 
self . assertTrue ( "/othergroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup3 . _v_title , \n 
self . h5file . create_group ( \n 
self . assertTrue ( "/othergroup1/othergroup2" not in self . h5file ) \n 
self . assertTrue ( \n 
"/othergroup1/othergroup2/othergroup3" not in self . h5file ) \n 
self . assertTrue ( "/othergroup1/othergroup2" in self . h5file ) \n 
self . assertTrue ( "/othergroup1/othergroup2/othergroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup1 . othergroup2 . _v_title , \n 
self . h5file . root . othergroup1 . othergroup2 . othergroup3 . _v_title , \n 
~~ ~~ minRowIndex = 10 \n 
def populateTable ( where , name ) : \n 
class Indexed ( IsDescription ) : \n 
~~~ var1 = StringCol ( itemsize = 4 , dflt = b"" , pos = 1 ) \n 
var2 = BoolCol ( dflt = 0 , pos = 2 ) \n 
var3 = IntCol ( dflt = 0 , pos = 3 ) \n 
var4 = FloatCol ( dflt = 0 , pos = 4 ) \n 
~~ nrows = minRowIndex \n 
table = where . _v_file . create_table ( where , name , Indexed , "Indexed" , \n 
None , nrows ) \n 
for i in range ( nrows ) : \n 
~~~ table . row [ ] = str ( i ) \n 
table . row [ ] = i % 2 \n 
table . row [ ] = i \n 
table . row [ ] = float ( nrows - i - 1 ) \n 
table . row . append ( ) \n 
~~ table . flush ( ) \n 
indexrows = table . cols . var1 . create_index ( ) \n 
indexrows = table . cols . var2 . create_index ( ) \n 
indexrows = table . cols . var3 . create_index ( ) \n 
~~ ~~ class RenameNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( RenameNodeTestCase , self ) . setUp ( ) \n 
populateTable ( self . h5file . root , ) \n 
self . h5file . rename_node ( , ) \n 
self . assertTrue ( "/agroup2" in self . h5file ) \n 
self . assertTrue ( "/agroup3" not in self . h5file ) \n 
self . assertTrue ( "/agroup2" not in self . h5file ) \n 
self . assertTrue ( "/agroup3" in self . h5file ) \n 
self . assertTrue ( "/agroup" in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3" in self . h5file ) \n 
self . assertTrue ( "/agroup" not in self . h5file ) \n 
self . assertTrue ( "/agroup3/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup3/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup3/agroup3" in self . h5file ) \n 
~~ def test01b ( self ) : \n 
self . assertTrue ( "/agroup4" not in self . h5file ) \n 
self . assertTrue ( "/agroup4" in self . h5file ) \n 
self . assertTrue ( "/agroup4/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup4/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup4/agroup3" in self . h5file ) \n 
self . assertTrue ( "/anarray" in self . h5file ) \n 
self . assertTrue ( "/anarray2" not in self . h5file ) \n 
self . assertTrue ( "/anarray" not in self . h5file ) \n 
self . assertTrue ( "/anarray2" in self . h5file ) \n 
self . assertTrue ( "/table" in self . h5file ) \n 
table = self . h5file . root . table \n 
self . assertTrue ( table . cols . var1 . index is not None ) \n 
self . assertTrue ( table . cols . var2 . index is not None ) \n 
self . assertTrue ( table . cols . var3 . index is not None ) \n 
self . assertTrue ( table . cols . var4 . index is None ) \n 
self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n 
self . assertTrue ( "/table2" not in self . h5file ) \n 
self . assertEqual ( self . h5file . root . table . title , "Indexed" ) \n 
self . assertTrue ( "/table" not in self . h5file ) \n 
self . assertTrue ( "/table2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . table2 . title , "Indexed" ) \n 
table = self . h5file . root . table2 \n 
~~ ~~ class MoveNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( MoveNodeTestCase , self ) . setUp ( ) \n 
self . h5file . move_node ( , ) \n 
self . assertTrue ( "/agroup/agroup3/anarray" not in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3/anarray" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup . agroup3 . anarray . title , \n 
self . h5file . move_node ( , , ) \n 
self . assertTrue ( "/agroup2/agroup3" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup2 . agroup3 . _v_title , \n 
self . assertTrue ( "/agroup2/agroup3/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup3/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup3/agroup3" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup2 . agroup4 . _v_title , \n 
self . assertTrue ( "/agroup2/agroup4/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4/agroup3" in self . h5file ) \n 
self . assertTrue ( "/agroup2/anarray2" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup2/table2" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/table2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup2 . table2 . title , "Indexed" ) \n 
table = self . h5file . root . agroup2 . table2 \n 
~~ ~~ class RemoveNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( RemoveNodeTestCase , self ) . setUp ( ) \n 
self . h5file . remove_node ( ) \n 
~~ def test00b ( self ) : \n 
self . assertTrue ( "/agroup/anarray2" not in self . h5file ) \n 
~~ def test00c ( self ) : \n 
self . h5file . remove_node ( , recursive = 1 ) \n 
self . assertTrue ( "/agroup/anarray1" not in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3" not in self . h5file ) \n 
~~ ~~ class CopyNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( CopyNodeTestCase , self ) . setUp ( ) \n 
~~ def test00_copyLeaf ( self ) : \n 
new_node = self . h5file . copy_node ( , ) \n 
self . assertTrue ( self . h5file . root . agroup . agroup3 . anarray is new_node ) \n 
~~ def test00b_copyTable ( self ) : \n 
warnings . filterwarnings ( "ignore" , category = UserWarning ) \n 
table = self . h5file . copy_node ( \n 
, , propindexes = True ) \n 
warnings . filterwarnings ( "default" , category = UserWarning ) \n 
self . assertTrue ( "/agroup/agroup3/table" in self . h5file ) \n 
table = self . h5file . root . agroup . agroup3 . table \n 
self . assertEqual ( table . title , "Indexed" ) \n 
self . assertTrue ( "/agroup/agroup3/table" not in self . h5file ) \n 
~~ def test01_copyGroup ( self ) : \n 
new_node = self . h5file . copy_node ( \n 
, newname = , recursive = True ) \n 
self . assertTrue ( self . h5file . root . acopy is new_node ) \n 
~~ def test02_copyLeafOverwrite ( self ) : \n 
oldNode = self . h5file . root . agroup \n 
, newname = , overwrite = True ) \n 
self . assertTrue ( self . h5file . root . agroup is oldNode ) \n 
self . assertTrue ( self . h5file . root . agroup is new_node ) \n 
~~ def test03_copyChildren ( self ) : \n 
self . h5file . copy_children ( , , recursive = True ) \n 
~~ ~~ class ComplexTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( ComplexTestCase , self ) . setUp ( ) \n 
self . h5file . create_array ( self . h5file . root , , \n 
new_node = self . h5file . copy_children ( \n 
, , recursive = 1 ) \n 
self . assertTrue ( self . h5file . root . agroup . anarray3 is new_node ) \n 
self . assertEqual ( self . h5file . root . anarray [ : ] , [ 1 ] ) \n 
self . assertEqual ( self . h5file . root . anarray [ : ] , [ 4 ] ) \n 
self . h5file . create_group ( self . h5file . root . agroup2 , , \n 
self . assertEqual ( self . h5file . root . agroup2 . agroup5 . _v_title , \n 
self . h5file . create_group ( self . h5file . root . agroup , , \n 
~~ def test03b ( self ) : \n 
self . h5file . create_group ( self . h5file . root . agroup3 , , \n 
~~ ~~ class AttributesTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( AttributesTestCase , self ) . setUp ( ) \n 
array = self . h5file . create_array ( , , [ 1 , 2 ] ) \n 
attrs = array . attrs \n 
attrs . attr_1 = 10 \n 
attrs . attr_2 = 20 \n 
attrs . attr_3 = 30 \n 
~~ def test00_setAttr ( self ) : \n 
~~ array = self . h5file . root . array \n 
setattr ( attrs , , 0 ) \n 
self . assertTrue ( in attrs ) \n 
self . assertEqual ( attrs . attr_0 , 0 ) \n 
self . assertTrue ( not in attrs ) \n 
~~ def test01_setAttrExisting ( self ) : \n 
setattr ( attrs , , 11 ) \n 
self . assertEqual ( attrs . attr_1 , 11 ) \n 
self . assertEqual ( attrs . attr_1 , 10 ) \n 
~~ def test02_delAttr ( self ) : \n 
delattr ( attrs , ) \n 
~~ def test03_copyNodeAttrs ( self ) : \n 
~~ rattrs = self . h5file . root . _v_attrs \n 
rattrs . attr_0 = 0 \n 
rattrs . attr_1 = 100 \n 
array = self . h5file . root . array \n 
attrs . _f_copy ( self . h5file . root ) \n 
self . assertEqual ( rattrs . attr_0 , 0 ) \n 
self . assertEqual ( rattrs . attr_1 , 10 ) \n 
self . assertEqual ( rattrs . attr_2 , 20 ) \n 
self . assertEqual ( rattrs . attr_3 , 30 ) \n 
self . assertEqual ( rattrs . attr_1 , 100 ) \n 
self . assertTrue ( not in rattrs ) \n 
~~ def test04_replaceNode ( self ) : \n 
attrs . attr_1 = 11 \n 
arr = self . h5file . create_array ( , , [ 1 ] ) \n 
arr . attrs . attr_1 = 12 \n 
self . assertTrue ( in self . h5file . root . array . attrs ) \n 
self . assertEqual ( self . h5file . root . array . attrs . attr_1 , 10 ) \n 
self . assertEqual ( self . h5file . root . array . attrs . attr_1 , 12 ) \n 
~~ ~~ class NotLoggedTestCase ( common . TempFileMixin , TestCase ) : \n 
class NotLoggedArray ( NotLoggedMixin , tables . Array ) : \n 
~~ def test00_hierarchy ( self ) : \n 
self . h5file . create_group ( , ) \n 
arr = self . NotLoggedArray ( self . h5file . root , , \n 
[ 1 ] , self . _getMethodName ( ) ) \n 
arr . move ( ) \n 
arr . remove ( ) \n 
~~ def test01_attributes ( self ) : \n 
arr . _v_attrs . foo = \n 
self . assertEqual ( arr . _v_attrs . foo , ) \n 
del arr . _v_attrs . foo \n 
self . assertRaises ( AttributeError , getattr , arr . _v_attrs , ) \n 
~~ ~~ class CreateParentsTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( CreateParentsTestCase , self ) . setUp ( ) \n 
g1 = self . h5file . create_group ( , ) \n 
self . h5file . create_group ( g1 , ) \n 
~~ def existing ( self , paths ) : \n 
return frozenset ( path for path in paths if path in self . h5file ) \n 
~~ def basetest ( self , doit , pre , post ) : \n 
~~~ pre ( ) \n 
paths = [ , , , ] \n 
for newpath in paths : \n 
~~~ before = self . existing ( paths ) \n 
doit ( newpath ) \n 
after = self . existing ( paths ) \n 
self . assertTrue ( after . issuperset ( before ) ) \n 
post ( newpath ) \n 
self . assertEqual ( after , before ) \n 
~~ ~~ def test00_create ( self ) : \n 
def pre ( ) : \n 
~~ def doit ( newpath ) : \n 
~~~ self . h5file . create_array ( newpath , , [ 1 ] , createparents = True ) \n 
self . assertTrue ( join_path ( newpath , ) in self . h5file ) \n 
~~ def post ( newpath ) : \n 
~~~ self . assertTrue ( join_path ( newpath , ) not in self . h5file ) \n 
~~ self . basetest ( doit , pre , post ) \n 
~~ def test01_move ( self ) : \n 
~~~ self . h5file . create_array ( , , [ 1 ] ) \n 
~~~ self . h5file . move_node ( , newpath , createparents = True ) \n 
~~~ self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( join_path ( newpath , ) not in self . h5file ) \n 
~~ def test02_copy ( self ) : \n 
~~~ self . h5file . copy_node ( , newpath , createparents = True ) \n 
~~~ g = self . h5file . create_group ( , ) \n 
self . h5file . create_array ( g , , [ 1 ] ) \n 
~~~ self . h5file . copy_children ( , newpath , createparents = True ) \n 
~~ ~~ def suite ( ) : \n 
~~~ theSuite = unittest . TestSuite ( ) \n 
niter = 1 \n 
for n in range ( niter ) : \n 
~~~ theSuite . addTest ( unittest . makeSuite ( BasicTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( PersistenceTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CreateArrayTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CreateGroupTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( RenameNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( MoveNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( RemoveNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CopyNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( AttributesTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( ComplexTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( NotLoggedTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CreateParentsTestCase ) ) \n 
~~ if common . heavy : \n 
~~ return theSuite \n 
common . parse_argv ( sys . argv ) \n 
common . print_versions ( ) \n 
unittest . main ( defaultTest = ) \n 
import pylons \n 
from pylons . controllers import WSGIController \n 
def get_pylons ( decorator_args ) : \n 
if decorator_args : \n 
~~~ controller = decorator_args [ 0 ] \n 
if isinstance ( controller , WSGIController ) : \n 
~~~ return controller . _py_object \n 
~~ ~~ return pylons \n 
~~ import warnings \n 
from paste . fixture import TestApp \n 
from paste . registry import RegistryManager \n 
from __init__ import TestWSGIController \n 
def make_cache_controller_app ( ) : \n 
~~~ from pylons . testutil import ControllerWrap , SetupCacheGlobal \n 
from pylons . decorators import jsonify \n 
class CacheController ( WSGIController ) : \n 
~~~ @ jsonify \n 
def test_bad_json ( self ) : \n 
~~ @ jsonify \n 
def test_bad_json2 ( self ) : \n 
def test_good_json ( self ) : \n 
~~~ return dict ( fred = 42 ) \n 
~~ ~~ environ = { } \n 
app = ControllerWrap ( CacheController ) \n 
app = sap = SetupCacheGlobal ( app , environ ) \n 
app = RegistryManager ( app ) \n 
app = TestApp ( app ) \n 
return app , environ \n 
~~ class TestJsonifyDecorator ( TestWSGIController ) : \n 
~~~ self . app , environ = make_cache_controller_app ( ) \n 
TestWSGIController . setUp ( self ) \n 
environ . update ( self . environ ) \n 
warnings . simplefilter ( , Warning ) \n 
~~~ warnings . simplefilter ( , Warning ) \n 
~~ def test_bad_json ( self ) : \n 
~~~ for action in , : \n 
~~~ response = self . get_response ( action = action ) \n 
~~ except Warning , msg : \n 
~~~ assert in msg [ 0 ] \n 
~~ ~~ ~~ def test_good_json ( self ) : \n 
~~~ response = self . get_response ( action = ) \n 
assert response . header ( ) == \n 
############################################################################## \n 
if sys . version_info [ 0 ] > 2 : \n 
~~~ README = open ( os . path . join ( here , ) , encoding = "utf-8" ) . read ( ) \n 
CHANGES = open ( os . path . join ( here , ) , encoding = "utf-8" ) . read ( ) \n 
~~ requires = [ \n 
if ( 3 , ) < sys . version_info < ( 3 , 3 ) : \n 
~~~ requires . extend ( [ \n 
~~~ import wsgiref \n 
~~~ requires . append ( ) \n 
~~ testing_extras = [ , , ] \n 
docs_extras = [ , ] \n 
long_description = README + + CHANGES , \n 
author_email = "pylons-discuss@googlegroups.com" , \n 
maintainer_email = "domen@dev.si" , \n 
url = "https://github.com/Pylons/pyramid_jinja2" , \n 
install_requires = requires , \n 
: testing_extras , \n 
: docs_extras , \n 
tests_require = requires + [ ] , \n 
test_suite = "pyramid_jinja2.tests" , \n 
import cryptacular . bcrypt \n 
from sqlalchemy import ( \n 
Table , \n 
Column , \n 
ForeignKey , \n 
from sqlalchemy . orm import ( \n 
scoped_session , \n 
sessionmaker , \n 
relation , \n 
backref , \n 
column_property , \n 
synonym , \n 
joinedload , \n 
from sqlalchemy . types import ( \n 
Integer , \n 
Unicode , \n 
UnicodeText , \n 
from sqlalchemy . sql import func \n 
from sqlalchemy . ext . declarative import declarative_base \n 
from zope . sqlalchemy import ZopeTransactionExtension \n 
from pyramid . security import ( \n 
Everyone , \n 
Authenticated , \n 
Allow , \n 
DBSession = scoped_session ( sessionmaker ( extension = ZopeTransactionExtension ( ) ) ) \n 
Base = declarative_base ( ) \n 
crypt = cryptacular . bcrypt . BCRYPTPasswordManager ( ) \n 
def hash_password ( password ) : \n 
~~~ return unicode ( crypt . encode ( password ) ) \n 
~~ class User ( Base ) : \n 
user_id = Column ( Integer , primary_key = True ) \n 
username = Column ( Unicode ( 20 ) , unique = True ) \n 
name = Column ( Unicode ( 50 ) ) \n 
email = Column ( Unicode ( 50 ) ) \n 
hits = Column ( Integer , default = 0 ) \n 
misses = Column ( Integer , default = 0 ) \n 
delivered_hits = Column ( Integer , default = 0 ) \n 
delivered_misses = Column ( Integer , default = 0 ) \n 
_password = Column ( , Unicode ( 60 ) ) \n 
def _get_password ( self ) : \n 
~~~ return self . _password \n 
~~ def _set_password ( self , password ) : \n 
~~~ self . _password = hash_password ( password ) \n 
~~ password = property ( _get_password , _set_password ) \n 
password = synonym ( , descriptor = password ) \n 
def __init__ ( self , username , password , name , email ) : \n 
~~~ self . username = username \n 
def get_by_username ( cls , username ) : \n 
~~~ return DBSession . query ( cls ) . filter ( cls . username == username ) . first ( ) \n 
def check_password ( cls , username , password ) : \n 
~~~ user = cls . get_by_username ( username ) \n 
if not user : \n 
~~ return crypt . check ( user . password , password ) \n 
~~ ~~ ideas_tags = Table ( , Base . metadata , \n 
Column ( , Integer , ForeignKey ( ) ) , \n 
Column ( , Integer , ForeignKey ( ) ) \n 
class Tag ( Base ) : \n 
tag_id = Column ( Integer , primary_key = True ) \n 
name = Column ( Unicode ( 50 ) , unique = True , index = True ) \n 
def __init__ ( self , name ) : \n 
def extract_tags ( tags_string ) : \n 
~~~ tags = tags_string . replace ( , ) . replace ( , ) \n 
tags = [ tag . lower ( ) for tag in tags . split ( ) ] \n 
tags = set ( tags ) \n 
return tags \n 
def get_by_name ( cls , tag_name ) : \n 
~~~ tag = DBSession . query ( cls ) . filter ( cls . name == tag_name ) \n 
return tag . first ( ) \n 
def create_tags ( cls , tags_string ) : \n 
~~~ tags_list = cls . extract_tags ( tags_string ) \n 
for tag_name in tags_list : \n 
~~~ tag = cls . get_by_name ( tag_name ) \n 
if not tag : \n 
~~~ tag = Tag ( name = tag_name ) \n 
DBSession . add ( tag ) \n 
~~ tags . append ( tag ) \n 
~~ return tags \n 
def tag_counts ( cls ) : \n 
~~~ query = DBSession . query ( Tag . name , func . count ( ) ) \n 
return query . join ( ) . group_by ( Tag . name ) \n 
~~ ~~ voted_users = Table ( , Base . metadata , \n 
class Idea ( Base ) : \n 
idea_id = Column ( Integer , primary_key = True ) \n 
target_id = Column ( Integer , ForeignKey ( ) ) \n 
comments = relation ( , cascade = "delete" , \n 
backref = backref ( , remote_side = idea_id ) ) \n 
author_id = Column ( Integer , ForeignKey ( ) ) \n 
author = relation ( User , cascade = "delete" , backref = ) \n 
title = Column ( UnicodeText ) \n 
text = Column ( UnicodeText ) \n 
tags = relation ( Tag , secondary = ideas_tags , backref = ) \n 
voted_users = relation ( User , secondary = voted_users , lazy = , \n 
backref = ) \n 
hit_percentage = func . coalesce ( hits / ( hits + misses ) * 100 , 0 ) \n 
hit_percentage = column_property ( hit_percentage . label ( ) ) \n 
total_votes = column_property ( ( hits + misses ) . label ( ) ) \n 
vote_differential = column_property ( \n 
( hits - misses ) . label ( ) \n 
def get_query ( cls , with_joinedload = True ) : \n 
~~~ query = DBSession . query ( cls ) \n 
if with_joinedload : \n 
~~~ query = query . options ( joinedload ( ) , joinedload ( ) ) \n 
def get_by_id ( cls , idea_id , with_joinedload = True ) : \n 
~~~ query = cls . get_query ( with_joinedload ) \n 
return query . filter ( cls . idea_id == idea_id ) . first ( ) \n 
def get_by_tagname ( cls , tag_name , with_joinedload = True ) : \n 
return query . filter ( Idea . tags . any ( name = tag_name ) ) \n 
def ideas_bunch ( cls , order_by , how_many = 10 , with_joinedload = True ) : \n 
~~~ query = cls . get_query ( with_joinedload ) . join ( ) \n 
query = query . filter ( cls . target == None ) . order_by ( order_by ) \n 
return query . limit ( how_many ) . all ( ) \n 
~~ def user_voted ( self , username ) : \n 
~~~ return bool ( self . voted_users . filter_by ( username = username ) . first ( ) ) \n 
~~ def vote ( self , user , positive ) : \n 
~~~ if positive : \n 
~~~ self . hits += 1 \n 
self . author . hits += 1 \n 
user . delivered_hits += 1 \n 
~~~ self . misses += 1 \n 
self . author . misses += 1 \n 
user . delivered_misses += 1 \n 
~~ self . voted_users . append ( user ) \n 
~~ ~~ class RootFactory ( object ) : \n 
~~~ __acl__ = [ \n 
( Allow , Everyone , ) , \n 
( Allow , Authenticated , ) \n 
def __init__ ( self , request ) : \n 
from pyramid import testing \n 
import mock \n 
class Test_acl_modified ( unittest . TestCase ) : \n 
~~~ self . request = testing . DummyRequest ( ) \n 
self . config = testing . setUp ( request = self . request ) \n 
~~~ testing . tearDown ( ) \n 
~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import acl_modified \n 
return acl_modified ( event ) \n 
~~ @ mock . patch ( ) \n 
def test_it ( self , mock_get_auditlog ) : \n 
~~~ from substanced . audit import AuditLog \n 
self . request . user = Dummy ( { : 1 , : } ) \n 
event = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
auditlog = AuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
context . __oid__ = 5 \n 
event . registry = _makeRegistry ( ) \n 
event . object = context \n 
event . old_acl = \n 
event . new_acl = \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , 5 ) \n 
json . loads ( entry [ 2 ] . payload ) , \n 
: entry [ 2 ] . timestamp , \n 
: { : 1 , : } , \n 
def test_it_nolog ( self , mock_get_auditlog ) : \n 
~~~ mock_get_auditlog . side_effect = lambda c : None \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
~~ ~~ _marker = object ( ) \n 
class Test_content_added_moved_or_duplicated ( unittest . TestCase ) : \n 
~~~ from . . subscribers import content_added_moved_or_duplicated \n 
return content_added_moved_or_duplicated ( event ) \n 
def test_it_added ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
event = _makeEvent ( ) \n 
self . assertEqual ( entry [ 2 ] . oid , 10 ) \n 
: 5 \n 
def test_it_added_noscribe ( self , mock_get_auditlog ) : \n 
def test_it_moved ( self , mock_get_auditlog ) : \n 
event . moving = True \n 
event . duplicating = None \n 
def test_it_duplicated ( self , mock_get_auditlog ) : \n 
event . moving = None \n 
event . duplicating = True \n 
~~ ~~ class Test_content_removed ( unittest . TestCase ) : \n 
~~~ from . . subscribers import content_removed \n 
return content_removed ( event ) \n 
~~ def test_it_moving ( self ) : \n 
~~~ event = Dummy ( ) \n 
~~ ~~ class Test_content_modified ( unittest . TestCase ) : \n 
~~~ from . . subscribers import content_modified \n 
return content_modified ( event ) \n 
def test_it_noscribe ( self , mock_get_auditlog ) : \n 
~~ ~~ class Test_logged_in ( unittest . TestCase ) : \n 
~~~ from . . subscribers import logged_in \n 
return logged_in ( event ) \n 
event . request = Dummy ( ) \n 
event . request . context = context \n 
def test_it_user_has_oid ( self , mock_get_auditlog ) : \n 
user = Dummy ( ) \n 
user . __oid__ = 5 \n 
event . user = user \n 
event . login = \n 
self . assertEqual ( entry [ 2 ] . oid , None ) \n 
def test_it_user_has_no_oid ( self , mock_get_auditlog ) : \n 
~~ ~~ class Test_root_added ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import root_added \n 
return root_added ( event ) \n 
def test_it ( self , mock_set_auditlog ) : \n 
root = Dummy ( ) \n 
def is_set ( _root ) : \n 
~~~ self . assertEqual ( _root , root ) \n 
~~ mock_set_auditlog . side_effect = is_set \n 
event . object = root \n 
~~ ~~ class Dummy ( object ) : \n 
~~~ def __init__ ( self , kw = None ) : \n 
~~~ if kw : \n 
~~~ self . __dict__ . update ( kw ) \n 
~~ ~~ ~~ class DummyContentRegistry ( object ) : \n 
~~~ def typeof ( self , content ) : \n 
~~ ~~ def _makeAuditLog ( ) : \n 
return auditlog \n 
~~ def _makeRegistry ( ) : \n 
~~~ registry = Dummy ( ) \n 
registry . content = DummyContentRegistry ( ) \n 
return registry \n 
~~ def _makeEvent ( ) : \n 
event . parent = testing . DummyResource ( ) \n 
event . parent . __oid__ = 10 \n 
event . name = \n 
context . __parent__ = event . parent \n 
return event \n 
class Test_root_factory ( unittest . TestCase ) : \n 
~~~ self . config = testing . setUp ( ) \n 
~~ def _callFUT ( self , request , transaction , get_connection , evolve_packages ) : \n 
~~~ from . . import root_factory \n 
return root_factory ( request , transaction , get_connection , \n 
evolve_packages ) \n 
~~ def _makeRequest ( self , app_root = None ) : \n 
~~~ request = Dummy ( ) \n 
request . registry = DummyRegistry ( ) \n 
request . registry . content = Dummy ( ) \n 
request . registry . content . create = lambda * arg : app_root \n 
return request \n 
~~ def test_without_app_root ( self ) : \n 
~~~ txn = DummyTransaction ( ) \n 
root = { } \n 
gc = Dummy_get_connection ( root ) \n 
ep = DummyFunction ( True ) \n 
app_root = object ( ) \n 
request = self . _makeRequest ( app_root ) \n 
result = self . _callFUT ( request , txn , gc , ep ) \n 
self . assertEqual ( result , app_root ) \n 
self . assertTrue ( txn . committed ) \n 
self . assertTrue ( txn . savepointed ) \n 
self . assertTrue ( ep . called ) \n 
~~ def test_with_app_root ( self ) : \n 
root = { : app_root } \n 
request = testing . DummyRequest ( ) \n 
self . assertFalse ( txn . committed ) \n 
~~ ~~ class Test_includeme ( unittest . TestCase ) : \n 
~~~ def test_it ( self ) : \n 
~~~ from . . import ( \n 
includeme , \n 
connection_opened , \n 
connection_will_close , \n 
ZODBConnectionOpened , \n 
ZODBConnectionWillClose , \n 
config = DummyConfig ( ) \n 
includeme ( config ) \n 
config . subscriptions , \n 
[ ( connection_opened , ZODBConnectionOpened ) , \n 
( connection_will_close , ZODBConnectionWillClose ) , \n 
~~ ~~ class Test_connection_opened ( unittest . TestCase ) : \n 
~~~ from . . import connection_opened \n 
event = DummyEvent ( ) \n 
connection_opened ( event ) \n 
self . assertEqual ( event . request . _zodb_tx_counts , ( 0 , 0 ) ) \n 
~~ ~~ class Test_connection_will_close ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event , statsd_incr ) : \n 
~~~ from . . import connection_will_close \n 
return connection_will_close ( event , statsd_incr ) \n 
~~ def test_no_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( ) \n 
result = self . _callFUT ( event , None ) \n 
~~ def test_with_postitive_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( 5 , 5 ) \n 
event . request . _zodb_tx_counts = ( 1 , 1 ) \n 
L = [ ] \n 
def statsd_incr ( name , num , registry = None ) : \n 
~~~ L . append ( ( name , num ) ) \n 
~~ self . _callFUT ( event , statsd_incr ) \n 
L , \n 
[ ( , 4 ) , ( , 4 ) ] \n 
~~ def test_with_zero_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( 1 , 1 ) \n 
self . _callFUT ( event , None ) \n 
[ ] \n 
~~ ~~ class DummyTransaction ( object ) : \n 
~~~ committed = False \n 
savepointed = False \n 
def commit ( self ) : \n 
~~~ self . committed = True \n 
~~ def savepoint ( self ) : \n 
~~~ self . savepointed = True \n 
~~ ~~ class Dummy_get_connection ( object ) : \n 
~~~ def __init__ ( self , root ) : \n 
~~~ self . _root = root \n 
~~ def root ( self ) : \n 
~~~ return self . _root \n 
~~ def __call__ ( self , request ) : \n 
~~ ~~ class DummyFunction ( object ) : \n 
~~~ called = False \n 
def __init__ ( self , result ) : \n 
~~~ self . result = result \n 
~~ def __call__ ( self , * args , ** kw ) : \n 
~~~ self . called = True \n 
self . kw = kw \n 
return self . result \n 
~~ class DummyRegistry ( object ) : \n 
~~~ def notify ( self , event ) : \n 
~~~ self . event = event \n 
~~ ~~ class DummyConfig ( object ) : \n 
~~~ self . subscriptions = [ ] \n 
~~ def add_subscriber ( self , fn , event_type ) : \n 
~~~ self . subscriptions . append ( ( fn , event_type ) ) \n 
~~ ~~ class DummyConnection ( object ) : \n 
~~~ def __init__ ( self , loads , stores ) : \n 
~~~ self . loads = loads \n 
self . stores = stores \n 
~~ def getTransferCounts ( self ) : \n 
~~~ return ( self . loads , self . stores ) \n 
~~ ~~ class DummyEvent ( object ) : \n 
~~~ def __init__ ( self , loads = 0 , stores = 0 ) : \n 
self . conn = DummyConnection ( loads , stores ) \n 
~~ ~~ import pkg_resources \n 
import colander \n 
import deform . schema \n 
from pyramid . httpexceptions import HTTPFound \n 
from pyramid . response import Response \n 
from . . form import FormView \n 
from . . file import ( \n 
FilePropertiesSchema , \n 
FileUploadTempStore , \n 
file_upload_widget , \n 
file_name_node , \n 
USE_MAGIC , \n 
from . . interfaces import ( \n 
IFile , \n 
IFolder , \n 
from . . sdi import mgmt_view \n 
@ mgmt_view ( \n 
context = IFile , \n 
permission = , \n 
tab_condition = False , \n 
http_cache = 0 , \n 
def view_file ( context , request ) : \n 
~~~ return context . get_response ( request = request ) \n 
~~ @ mgmt_view ( \n 
tab_title = , \n 
permission = \n 
def view_tab ( context , request ) : \n 
~~~ return HTTPFound ( location = request . sdiapi . mgmt_path ( context ) ) \n 
~~ class AddFileSchema ( FilePropertiesSchema ) : \n 
~~~ file = colander . SchemaNode ( \n 
deform . schema . FileData ( ) , \n 
widget = file_upload_widget , \n 
missing = colander . null , \n 
~~ @ colander . deferred \n 
def name_or_file ( node , kw ) : \n 
~~~ def _name_or_file ( node , struct ) : \n 
~~~ if not struct [ ] and not struct [ ] : \n 
~~~ raise colander . Invalid ( node , ) \n 
~~ if not struct [ ] : \n 
~~~ filename = struct [ ] . get ( ) \n 
if filename : \n 
~~~ name_node = file_name_node . bind ( \n 
context = kw [ ] , request = kw [ ] \n 
name_node . validator ( node [ ] , filename ) \n 
~~~ raise colander . Invalid ( \n 
node , \n 
~~ ~~ ~~ return _name_or_file \n 
context = IFolder , \n 
renderer = , \n 
addable_content = , \n 
tab_condition = False \n 
class AddFileView ( FormView ) : \n 
~~~ title = \n 
schema = AddFileSchema ( validator = name_or_file ) . clone ( ) \n 
schema [ ] . missing = colander . null \n 
buttons = ( , ) \n 
def _makeob ( self , stream , title , mimetype ) : \n 
~~~ return self . request . registry . content . create ( \n 
stream = stream , \n 
mimetype = mimetype , \n 
title = title , \n 
~~ def add_success ( self , appstruct ) : \n 
~~~ name = appstruct [ ] \n 
title = appstruct [ ] or None \n 
filedata = appstruct [ ] \n 
mimetype = appstruct [ ] or USE_MAGIC \n 
stream = None \n 
filename = None \n 
if filedata : \n 
~~~ filename = filedata [ ] \n 
stream = filedata [ ] \n 
if stream : \n 
~~~ stream . seek ( 0 ) \n 
~~~ stream = None \n 
~~ ~~ name = name or filename \n 
fileob = self . _makeob ( stream , title , mimetype ) \n 
self . context [ name ] = fileob \n 
tmpstore = FileUploadTempStore ( self . request ) \n 
tmpstore . clear ( ) \n 
return HTTPFound ( self . request . sdiapi . mgmt_path ( self . context ) ) \n 
~~ ~~ onepixel = pkg_resources . resource_filename ( \n 
permission = NO_PERMISSION_REQUIRED \n 
def preview_image_upload ( request ) : \n 
~~~ uid = request . subpath [ 0 ] \n 
tempstore = FileUploadTempStore ( request ) \n 
filedata = tempstore . get ( uid , { } ) \n 
fp = filedata . get ( ) \n 
if fp is not None : \n 
~~~ fp . seek ( 0 ) \n 
filename = filedata [ ] \n 
~~ mimetype = mimetypes . guess_type ( filename , strict = False ) [ 0 ] \n 
if not mimetype or not mimetype . startswith ( ) : \n 
~~~ mimetype = \n 
fp = open ( onepixel , ) \n 
~~ response = Response ( content_type = mimetype , app_iter = fp ) \n 
class Test_principal_added ( unittest . TestCase ) : \n 
~~~ from . . subscribers import principal_added \n 
return principal_added ( event ) \n 
~~ def test_event_wo_loading_attr ( self ) : \n 
~~~ event = testing . DummyResource ( ) \n 
event . object = testing . DummyResource ( ) \n 
self . assertRaises ( AttributeError , self . _callFUT , event ) \n 
~~ def test_event_w_loading_True ( self ) : \n 
~~~ event = testing . DummyResource ( loading = True ) \n 
result = self . _callFUT ( event ) \n 
self . assertEqual ( result , None ) \n 
~~ def test_wo_principals_service ( self ) : \n 
~~~ from zope . interface import directlyProvides \n 
from ... interfaces import IFolder \n 
event = testing . DummyResource ( loading = False ) \n 
root = testing . DummyResource ( ) \n 
directlyProvides ( root , IFolder ) \n 
event . object = root [ ] = testing . DummyResource ( ) \n 
self . assertRaises ( ValueError , self . _callFUT , event ) \n 
~~ def test_user_not_in_groups ( self ) : \n 
~~~ from ... testing import make_site \n 
from ... interfaces import IUser \n 
site = make_site ( ) \n 
user = testing . DummyResource ( __provides__ = IUser ) \n 
site [ ] = user \n 
event = testing . DummyResource ( object = user , loading = False ) \n 
~~ def test_user_in_groups ( self ) : \n 
groups = site [ ] [ ] \n 
groups [ ] = testing . DummyResource ( ) \n 
~~ def test_group_not_in_users ( self ) : \n 
group = testing . DummyResource ( ) \n 
site [ ] = group \n 
event = testing . DummyResource ( object = group , loading = False ) \n 
~~ def test_group_in_users ( self ) : \n 
users = site [ ] [ ] \n 
users [ ] = testing . DummyResource ( ) \n 
~~ ~~ class Test_user_will_be_removed ( unittest . TestCase ) : \n 
~~~ from . . subscribers import user_will_be_removed \n 
return user_will_be_removed ( event ) \n 
~~ def test_loading ( self ) : \n 
~~~ event = testing . DummyResource ( loading = True , moving = None ) \n 
~~ def test_moving ( self ) : \n 
~~~ event = testing . DummyResource ( loading = False , moving = True ) \n 
~~ def test_it ( self ) : \n 
~~~ from ... interfaces import IFolder \n 
parent = testing . DummyResource ( __provides__ = IFolder ) \n 
user = testing . DummyResource ( ) \n 
reset = testing . DummyResource ( ) \n 
def commit_suicide ( ) : \n 
~~~ reset . committed = True \n 
~~ reset . commit_suicide = commit_suicide \n 
objectmap = DummyObjectMap ( ( reset , ) ) \n 
parent . __objectmap__ = objectmap \n 
parent [ ] = user \n 
event = testing . DummyResource ( object = user , loading = False , moving = None ) \n 
self . assertTrue ( reset . committed ) \n 
~~~ event = testing . DummyResource ( object = None , loading = False ) \n 
~~ ~~ class Test_user_added ( unittest . TestCase ) : \n 
~~~ from . . subscribers import user_added \n 
return user_added ( event ) \n 
~~ def test_it_user_has_no_oid ( self ) : \n 
~~~ user = testing . DummyResource ( ) \n 
event . registry = DummyRegistry ( ) \n 
~~~ from pyramid . security import Allow \n 
user . __oid__ = 1 \n 
user . __acl__ , \n 
[ ( Allow , 1 , ( , \n 
) ) ] ) \n 
~~ ~~ class Test_acl_maybe_added ( unittest . TestCase ) : \n 
~~~ from . . subscribers import acl_maybe_added \n 
return acl_maybe_added ( event ) \n 
~~~ event = DummyEvent ( moving = True , loading = False ) \n 
self . assertEqual ( self . _callFUT ( event ) , False ) \n 
~~~ event = DummyEvent ( moving = None , loading = True ) \n 
~~ def test_objectmap_is_None ( self ) : \n 
~~~ event = DummyEvent ( moving = None , object = None , loading = False ) \n 
~~ def test_no_acls ( self ) : \n 
~~~ from substanced . interfaces import IFolder \n 
resource1 = testing . DummyResource ( __provides__ = IFolder ) \n 
resource2 = testing . DummyResource ( ) \n 
resource1 [ ] = resource2 \n 
objectmap = DummyObjectMap ( ) \n 
resource1 . __objectmap__ = objectmap \n 
event = DummyEvent ( moving = None , object = resource1 , loading = False ) \n 
self . assertEqual ( objectmap . connections , [ ] ) \n 
~~ def test_with_acls ( self ) : \n 
~~~ from ... interfaces import PrincipalToACLBearing \n 
from substanced . interfaces import IFolder \n 
resource1 . __acl__ = [ ( None , , None ) , ( None , 1 , None ) ] \n 
resource2 . __acl__ = [ ( None , , None ) , ( None , 2 , None ) ] \n 
objectmap . connections , \n 
[ ( 2 , resource2 , PrincipalToACLBearing ) , \n 
( 1 , resource1 , PrincipalToACLBearing ) ] \n 
~~ ~~ class Test_acl_modified ( unittest . TestCase ) : \n 
~~~ event = DummyEvent ( object = None ) \n 
~~ def test_gardenpath ( self ) : \n 
resource = testing . DummyResource ( ) \n 
resource . __objectmap__ = objectmap \n 
event = DummyEvent ( \n 
object = resource , \n 
new_acl = [ ( None , , None ) , ( None , 1 , None ) ] , \n 
old_acl = [ ( None , , None ) , ( None , 2 , None ) ] , \n 
[ ( 1 , resource , PrincipalToACLBearing ) ] \n 
objectmap . disconnections , \n 
[ ( 2 , resource , PrincipalToACLBearing ) ] \n 
~~ ~~ class DummyObjectMap ( object ) : \n 
~~~ def __init__ ( self , result = ( ) ) : \n 
self . connections = [ ] \n 
self . disconnections = [ ] \n 
~~ def targets ( self , object , reftype ) : \n 
~~~ return self . result \n 
~~ def connect ( self , source , target , reftype ) : \n 
~~~ self . connections . append ( ( source , target , reftype ) ) \n 
~~ def disconnect ( self , source , target , reftype ) : \n 
~~~ self . disconnections . append ( ( source , target , reftype ) ) \n 
~~ ~~ class DummyRegistry ( object ) : \n 
~~~ def subscribers ( self , * arg ) : \n 
~~ ~~ from pyramid . httpexceptions import ( \n 
HTTPForbidden , \n 
HTTPFound \n 
from pyramid . renderers import get_renderer \n 
from pyramid . session import check_csrf_token \n 
remember , \n 
forget , \n 
NO_PERMISSION_REQUIRED , \n 
from ... util import get_oid \n 
from . . import mgmt_view \n 
from substanced . interfaces import IUserLocator \n 
from substanced . principal import DefaultUserLocator \n 
from substanced . event import LoggedIn \n 
context = HTTPForbidden , \n 
permission = NO_PERMISSION_REQUIRED , \n 
effective_principals = Authenticated , \n 
def login ( context , request ) : \n 
~~~ login_url = request . sdiapi . mgmt_path ( request . context , ) \n 
referrer = request . url \n 
if in referrer : \n 
~~~ return HTTPForbidden ( ) \n 
~~ if login_url in referrer : \n 
~~~ referrer = request . sdiapi . mgmt_path ( request . virtual_root ) \n 
~~ came_from = request . session . setdefault ( , referrer ) \n 
login = \n 
password = \n 
if in request . params : \n 
~~~ check_csrf_token ( request ) \n 
~~~ request . sdiapi . flash ( , ) \n 
~~~ login = request . params [ ] \n 
password = request . params [ ] \n 
adapter = request . registry . queryMultiAdapter ( \n 
( context , request ) , \n 
IUserLocator \n 
if adapter is None : \n 
~~~ adapter = DefaultUserLocator ( context , request ) \n 
~~ user = adapter . get_user_by_login ( login ) \n 
if user is not None and user . check_password ( password ) : \n 
~~~ request . session . pop ( , None ) \n 
headers = remember ( request , get_oid ( user ) ) \n 
request . registry . notify ( LoggedIn ( login , user , context , request ) ) \n 
return HTTPFound ( location = came_from , headers = headers ) \n 
~~ request . sdiapi . flash ( , ) \n 
~~ ~~ template = get_renderer ( \n 
) . implementation ( ) \n 
return dict ( \n 
url = request . sdiapi . mgmt_path ( request . virtual_root , ) , \n 
came_from = came_from , \n 
login = login , \n 
password = password , \n 
login_template = template , \n 
def logout ( request ) : \n 
~~~ headers = forget ( request ) \n 
return HTTPFound ( location = request . sdiapi . mgmt_path ( request . context ) , \n 
headers = headers ) \n 
~~ from venusian . tests . fixtures import categorydecorator \n 
from venusian . tests . fixtures import categorydecorator2 \n 
@ categorydecorator ( function = True ) \n 
~~~ return request \n 
~~ @ categorydecorator2 ( function = True ) \n 
mimetypes . add_type ( , ) \n 
from zope . structuredtext import stx2html \n 
from pyramid . view import render_view_to_response \n 
from pyramid . view import view_config \n 
from virginia . models import File \n 
from virginia . models import Directory \n 
@ view_config ( context = File ) \n 
def file_view ( context , request ) : \n 
~~~ dirname , filename = os . path . split ( context . path ) \n 
name , ext = os . path . splitext ( filename ) \n 
result = render_view_to_response ( context , request , ext ) \n 
~~ @ view_config ( context = Directory ) \n 
def directory_view ( context , request ) : \n 
~~~ path_info = request . environ [ ] \n 
if not path_info . endswith ( ) : \n 
~~~ response = HTTPFound ( location = path_info + ) \n 
~~ defaults = ( , , ) \n 
for name in defaults : \n 
~~~ index = context [ name ] \n 
~~ return file_view ( index , request ) \n 
~~ response = Response ( % context . path ) \n 
response . content_type = \n 
~~ @ view_config ( context = File , name = ) \n 
def structured_text_view ( context , request ) : \n 
result = stx2html ( context . source ) \n 
response = Response ( result ) \n 
@ view_config ( context = File , name = ) \n 
def raw_view ( context , request ) : \n 
response = Response ( context . source ) \n 
dirname , filename = os . path . split ( context . path ) \n 
mt , encoding = mimetypes . guess_type ( filename ) \n 
response . content_type = mt or \n 
if not hasattr ( unittest . defaultTestLoader , ) : \n 
~~~ raise ImportError ( ) \n 
~~ ~~ def additional_tests ( ) : \n 
~~~ setup_file = sys . modules [ ] . __file__ \n 
setup_dir = os . path . abspath ( os . path . dirname ( setup_file ) ) \n 
test_dir = os . path . join ( setup_dir , ) \n 
test_suite = unittest . defaultTestLoader . discover ( test_dir ) \n 
blacklist = [ ] \n 
if in __file__ : \n 
~~~ blacklist . append ( ) \n 
~~ return exclude_tests ( test_suite , blacklist ) \n 
~~ class SkipCase ( unittest . TestCase ) : \n 
~~~ def skeleton_run_test ( self ) : \n 
~~ ~~ def exclude_tests ( suite , blacklist ) : \n 
new_suite = unittest . TestSuite ( ) \n 
for test_group in suite . _tests : \n 
~~~ for test in test_group : \n 
~~~ if not hasattr ( test , ) : \n 
~~~ new_suite . addTest ( test ) \n 
~~ for subtest in test . _tests : \n 
~~~ method = subtest . _testMethodName \n 
if method in blacklist : \n 
~~~ setattr ( test , \n 
method , \n 
getattr ( SkipCase ( ) , ) ) \n 
~~ ~~ new_suite . addTest ( test ) \n 
~~ ~~ return new_suite \n 
from future . utils import surrogateescape \n 
surrogateescape . register_surrogateescape ( ) \n 
def message_from_string ( s , * args , ** kws ) : \n 
from future . backports . email . parser import Parser \n 
return Parser ( * args , ** kws ) . parsestr ( s ) \n 
~~ def message_from_bytes ( s , * args , ** kws ) : \n 
from future . backports . email . parser import BytesParser \n 
return BytesParser ( * args , ** kws ) . parsebytes ( s ) \n 
~~ def message_from_file ( fp , * args , ** kws ) : \n 
return Parser ( * args , ** kws ) . parse ( fp ) \n 
~~ def message_from_binary_file ( fp , * args , ** kws ) : \n 
return BytesParser ( * args , ** kws ) . parse ( fp ) \n 
_builtin_next = next \n 
_SENTINEL = object ( ) \n 
def newnext ( iterator , default = _SENTINEL ) : \n 
~~~ return iterator . __next__ ( ) \n 
~~~ return iterator . next ( ) \n 
iterator . __class__ . __name__ ) ) \n 
~~ ~~ ~~ except StopIteration as e : \n 
~~~ if default is _SENTINEL : \n 
~~~ raise e \n 
~~ ~~ ~~ __all__ = [ ] \n 
from future . utils import PY2 \n 
from sys import * \n 
~~~ from __builtin__ import intern \n 
from collections import Iterable \n 
from numbers import Integral \n 
from future . utils import istext , isbytes , PY3 , with_metaclass \n 
from future . types import no , issubset \n 
from future . types . newobject import newobject \n 
_builtin_bytes = bytes \n 
if PY3 : \n 
~~~ unicode = str \n 
~~ class BaseNewBytes ( type ) : \n 
~~~ def __instancecheck__ ( cls , instance ) : \n 
~~~ if cls == newbytes : \n 
~~~ return isinstance ( instance , _builtin_bytes ) \n 
~~~ return issubclass ( instance . __class__ , cls ) \n 
~~ ~~ ~~ class newbytes ( with_metaclass ( BaseNewBytes , _builtin_bytes ) ) : \n 
def __new__ ( cls , * args , ** kwargs ) : \n 
encoding = None \n 
errors = None \n 
~~~ return super ( newbytes , cls ) . __new__ ( cls ) \n 
~~ elif len ( args ) >= 2 : \n 
~~~ args = list ( args ) \n 
if len ( args ) == 3 : \n 
~~~ errors = args . pop ( ) \n 
~~ encoding = args . pop ( ) \n 
~~ if type ( args [ 0 ] ) == newbytes : \n 
~~~ return args [ 0 ] \n 
~~ elif isinstance ( args [ 0 ] , _builtin_bytes ) : \n 
~~~ value = args [ 0 ] \n 
~~ elif isinstance ( args [ 0 ] , unicode ) : \n 
~~~ assert encoding is None \n 
encoding = kwargs [ ] \n 
~~~ assert errors is None \n 
errors = kwargs [ ] \n 
~~ ~~ except AssertionError : \n 
~~ if encoding is None : \n 
~~ newargs = [ encoding ] \n 
if errors is not None : \n 
~~~ newargs . append ( errors ) \n 
~~ value = args [ 0 ] . encode ( * newargs ) \n 
~~ elif isinstance ( args [ 0 ] , Iterable ) : \n 
~~~ if len ( args [ 0 ] ) == 0 : \n 
~~~ value = \n 
~~~ values = [ chr ( x ) for x in args [ 0 ] ] \n 
value = . join ( values ) \n 
~~ ~~ ~~ elif isinstance ( args [ 0 ] , Integral ) : \n 
~~~ if args [ 0 ] < 0 : \n 
~~ value = * args [ 0 ] \n 
~~ return super ( newbytes , cls ) . __new__ ( cls , value ) \n 
~~~ return + super ( newbytes , self ) . __repr__ ( ) \n 
~~~ return + "\'{0}\'" . format ( super ( newbytes , self ) . __str__ ( ) ) \n 
~~ def __getitem__ ( self , y ) : \n 
~~~ value = super ( newbytes , self ) . __getitem__ ( y ) \n 
if isinstance ( y , Integral ) : \n 
~~~ return ord ( value ) \n 
~~~ return newbytes ( value ) \n 
~~ ~~ def __getslice__ ( self , * args ) : \n 
~~~ return self . __getitem__ ( slice ( * args ) ) \n 
~~~ if isinstance ( key , int ) : \n 
~~~ newbyteskey = newbytes ( [ key ] ) \n 
~~ elif type ( key ) == newbytes : \n 
~~~ newbyteskey = key \n 
~~~ newbyteskey = newbytes ( key ) \n 
~~ return issubset ( list ( newbyteskey ) , list ( self ) ) \n 
~~ @ no ( unicode ) \n 
def __add__ ( self , other ) : \n 
~~~ return newbytes ( super ( newbytes , self ) . __add__ ( other ) ) \n 
def __radd__ ( self , left ) : \n 
~~~ return newbytes ( left ) + self \n 
def __mul__ ( self , other ) : \n 
~~~ return newbytes ( super ( newbytes , self ) . __mul__ ( other ) ) \n 
def __rmul__ ( self , other ) : \n 
~~~ return newbytes ( super ( newbytes , self ) . __rmul__ ( other ) ) \n 
~~ def join ( self , iterable_of_bytes ) : \n 
~~~ errmsg = \n 
if isbytes ( iterable_of_bytes ) or istext ( iterable_of_bytes ) : \n 
~~~ raise TypeError ( errmsg . format ( 0 , type ( iterable_of_bytes ) ) ) \n 
~~ for i , item in enumerate ( iterable_of_bytes ) : \n 
~~~ if istext ( item ) : \n 
~~~ raise TypeError ( errmsg . format ( i , type ( item ) ) ) \n 
~~ ~~ return newbytes ( super ( newbytes , self ) . join ( iterable_of_bytes ) ) \n 
def fromhex ( cls , string ) : \n 
~~~ return cls ( string . replace ( , ) . decode ( ) ) \n 
def find ( self , sub , * args ) : \n 
~~~ return super ( newbytes , self ) . find ( sub , * args ) \n 
def rfind ( self , sub , * args ) : \n 
~~~ return super ( newbytes , self ) . rfind ( sub , * args ) \n 
~~ @ no ( unicode , ( 1 , 2 ) ) \n 
def replace ( self , old , new , * args ) : \n 
~~~ return newbytes ( super ( newbytes , self ) . replace ( old , new , * args ) ) \n 
~~ def encode ( self , * args ) : \n 
~~ def decode ( self , encoding = , errors = ) : \n 
from future . types . newstr import newstr \n 
if errors == : \n 
~~~ from future . utils . surrogateescape import register_surrogateescape \n 
register_surrogateescape ( ) \n 
~~ return newstr ( super ( newbytes , self ) . decode ( encoding , errors ) ) \n 
def startswith ( self , prefix , * args ) : \n 
~~~ return super ( newbytes , self ) . startswith ( prefix , * args ) \n 
def endswith ( self , prefix , * args ) : \n 
~~~ return super ( newbytes , self ) . endswith ( prefix , * args ) \n 
def split ( self , sep = None , maxsplit = - 1 ) : \n 
~~~ parts = super ( newbytes , self ) . split ( sep , maxsplit ) \n 
return [ newbytes ( part ) for part in parts ] \n 
~~ def splitlines ( self , keepends = False ) : \n 
parts = super ( newbytes , self ) . splitlines ( keepends ) \n 
def rsplit ( self , sep = None , maxsplit = - 1 ) : \n 
~~~ parts = super ( newbytes , self ) . rsplit ( sep , maxsplit ) \n 
def partition ( self , sep ) : \n 
~~~ parts = super ( newbytes , self ) . partition ( sep ) \n 
return tuple ( newbytes ( part ) for part in parts ) \n 
def rpartition ( self , sep ) : \n 
~~~ parts = super ( newbytes , self ) . rpartition ( sep ) \n 
~~ @ no ( unicode , ( 1 , ) ) \n 
def rindex ( self , sub , * args ) : \n 
pos = self . rfind ( sub , * args ) \n 
if pos == - 1 : \n 
~~ ~~ @ no ( unicode ) \n 
def index ( self , sub , * args ) : \n 
if isinstance ( sub , int ) : \n 
~~~ if len ( args ) == 0 : \n 
~~~ start , end = 0 , len ( self ) \n 
~~ elif len ( args ) == 1 : \n 
~~~ start = args [ 0 ] \n 
~~ elif len ( args ) == 2 : \n 
~~~ start , end = args \n 
~~ return list ( self ) [ start : end ] . index ( sub ) \n 
~~ if not isinstance ( sub , bytes ) : \n 
~~~ sub = self . __class__ ( sub ) \n 
~~~ return super ( newbytes , self ) . index ( sub , * args ) \n 
~~~ if isinstance ( other , ( _builtin_bytes , bytearray ) ) : \n 
~~~ return super ( newbytes , self ) . __eq__ ( other ) \n 
~~ ~~ def __ne__ ( self , other ) : \n 
~~~ if isinstance ( other , _builtin_bytes ) : \n 
~~~ return super ( newbytes , self ) . __ne__ ( other ) \n 
~~ ~~ unorderable_err = \n 
def __lt__ ( self , other ) : \n 
~~~ if not isbytes ( other ) : \n 
~~~ raise TypeError ( self . unorderable_err . format ( type ( other ) ) ) \n 
~~ return super ( newbytes , self ) . __lt__ ( other ) \n 
~~ def __le__ ( self , other ) : \n 
~~ return super ( newbytes , self ) . __le__ ( other ) \n 
~~ def __gt__ ( self , other ) : \n 
~~ return super ( newbytes , self ) . __gt__ ( other ) \n 
~~ def __ge__ ( self , other ) : \n 
~~ return super ( newbytes , self ) . __ge__ ( other ) \n 
~~ def __native__ ( self ) : \n 
~~~ return super ( newbytes , self ) . __str__ ( ) \n 
~~ def __getattribute__ ( self , name ) : \n 
if name in [ , ] : \n 
~~ return super ( newbytes , self ) . __getattribute__ ( name ) \n 
def rstrip ( self , bytes_to_strip = None ) : \n 
return newbytes ( super ( newbytes , self ) . rstrip ( bytes_to_strip ) ) \n 
def strip ( self , bytes_to_strip = None ) : \n 
return newbytes ( super ( newbytes , self ) . strip ( bytes_to_strip ) ) \n 
~~ def lower ( self ) : \n 
return newbytes ( super ( newbytes , self ) . lower ( ) ) \n 
def upper ( self ) : \n 
return newbytes ( super ( newbytes , self ) . upper ( ) ) \n 
@ no ( unicode ) \n 
def maketrans ( cls , frm , to ) : \n 
return newbytes ( string . maketrans ( frm , to ) ) \n 
~~ ~~ __all__ = [ ] \n 
from libpasteurize . fixes . fix_division import FixDivision \n 
from lib2to3 import fixer_base \n 
from lib2to3 . pygram import python_symbols as syms \n 
from lib2to3 . fixer_util import Name , Call , in_special_context \n 
from libfuturize . fixer_util import touch_import_top \n 
replaced_builtins = . split ( ) \n 
expression = . join ( [ "name=\'{0}\'" . format ( name ) for name in replaced_builtins ] ) \n 
class FixFutureBuiltins ( fixer_base . BaseFix ) : \n 
~~~ BM_compatible = True \n 
run_order = 9 \n 
def transform ( self , node , results ) : \n 
~~~ name = results [ "name" ] \n 
touch_import_top ( , name . value , node ) \n 
~~ ~~ from __future__ import absolute_import \n 
if sys . version_info [ 0 ] < 3 : \n 
~~~ from Tkinter import * \n 
~~~ raise ImportError ( \n 
from __future__ import print_function , absolute_import \n 
from subprocess import Popen , PIPE \n 
from future . tests . base import CodeHandler , unittest , skip26 \n 
class TestPasteurize ( CodeHandler ) : \n 
~~~ _ , self . textfilename = tempfile . mkstemp ( text = True ) \n 
super ( TestPasteurize , self ) . setUp ( ) \n 
~~~ os . unlink ( self . textfilename ) \n 
def test_range_slice ( self ) : \n 
code = \n 
self . unchanged ( code , from3 = True ) \n 
~~ def test_print ( self ) : \n 
~~ def test_division ( self ) : \n 
~~ @ unittest . expectedFailure \n 
def test_exception_indentation ( self ) : \n 
self . convert_check ( before , after , from3 = True ) \n 
def test_urllib_request ( self ) : \n 
~~ def test_urllib_refactor2 ( self ) : \n 
~~ def test_correct_exit_status ( self ) : \n 
from libpasteurize . main import main \n 
retcode = main ( [ self . textfilename ] ) \n 
~~ ~~ class TestFuturizeAnnotations ( CodeHandler ) : \n 
~~~ @ unittest . expectedFailure \n 
def test_return_annotations_alone ( self ) : \n 
self . convert_check ( b , a , from3 = True ) \n 
def test_single_param_annotations ( self ) : \n 
~~ def test_multiple_param_annotations ( self ) : \n 
~~ def test_mixed_annotations ( self ) : \n 
~~ def test_functions_unchanged ( self ) : \n 
self . unchanged ( s , from3 = True ) \n 
~~ import ast , copy \n 
from ast_utils import * \n 
class Inliner : \n 
~~~ def setup_inliner ( self , writer ) : \n 
~~~ self . writer = writer \n 
self . _with_inline = False \n 
self . _inline = [ ] \n 
self . _inline_ids = 0 \n 
self . _inline_breakout = False \n 
~~ def inline_helper_remap_names ( self , remap ) : \n 
~~ def inline_helper_return_id ( self , return_id ) : \n 
~~ def inline_function ( self , node ) : \n 
~~~ name = self . visit ( node . func ) \n 
fnode = self . _global_functions [ name ] \n 
fnode = copy . deepcopy ( fnode ) \n 
finfo = inspect_function ( fnode ) \n 
remap = { } \n 
for n in finfo [ ] : \n 
~~~ if n . id not in finfo [ ] : continue \n 
if isinstance ( n . id , ast . Name ) : \n 
~~~ raise RuntimeError \n 
~~ if n . id not in remap : \n 
~~~ new_name = n . id + % self . _inline_ids \n 
remap [ n . id ] = new_name \n 
self . _inline_ids += 1 \n 
~~ n . id = remap [ n . id ] \n 
~~ if remap : \n 
~~~ self . writer . write ( self . inline_helper_remap_names ( remap ) ) \n 
for n in remap : \n 
~~~ if n in finfo [ ] : \n 
~~~ self . _func_typedefs [ remap [ n ] ] = finfo [ ] [ n ] \n 
~~ ~~ ~~ offset = len ( fnode . args . args ) - len ( fnode . args . defaults ) \n 
for i , ad in enumerate ( fnode . args . args ) : \n 
~~~ if i < len ( node . args ) : \n 
~~~ ac = self . visit ( node . args [ i ] ) \n 
~~~ assert fnode . args . defaults \n 
dindex = i - offset \n 
ac = self . visit ( fnode . args . defaults [ dindex ] ) \n 
~~ ad = remap [ self . visit ( ad ) ] \n 
~~ return_id = name + str ( self . _inline_ids ) \n 
self . _inline . append ( return_id ) \n 
self . writer . write ( self . inline_helper_return_id ( return_id ) ) \n 
if True : \n 
~~~ self . _inline_breakout = True \n 
self . writer . write ( ) \n 
self . writer . push ( ) \n 
for b in fnode . body : \n 
~~~ self . visit ( b ) \n 
~~ if not len ( finfo [ ] ) : \n 
~~~ self . writer . write ( ) \n 
~~ self . writer . pull ( ) \n 
~~~ for b in fnode . body : \n 
~~ ~~ if self . _inline . pop ( ) != return_id : \n 
~~ for n in remap : \n 
~~~ gname = remap [ n ] \n 
~~~ if n . id == gname : \n 
~~~ n . id = n \n 
~~ ~~ ~~ return % return_id \n 
import ast \n 
import pythonjs \n 
class TransformSuperCalls ( ast . NodeVisitor ) : \n 
~~~ def __init__ ( self , node , class_names ) : \n 
~~~ self . _class_names = class_names \n 
self . visit ( node ) \n 
~~ def visit_Call ( self , node ) : \n 
~~~ if isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value ~~~ node . func . attr = + node . func . attr \n 
~~ ~~ ~~ class CollectNames ( ast . NodeVisitor ) : \n 
~~~ self . _names = [ ] \n 
~~ def visit_Name ( self , node ) : \n 
~~~ self . _names . append ( node ) \n 
~~ ~~ def collect_names ( node ) : \n 
~~~ a = CollectNames ( ) \n 
a . visit ( node ) \n 
return a . _names \n 
~~ class DartGenerator ( pythonjs . JSGenerator ) : \n 
~~~ def __init__ ( self , requirejs = False , insert_runtime = False ) : \n 
~~~ pythonjs . JSGenerator . __init__ ( self , requirejs = False , insert_runtime = False ) \n 
self . _classes = dict ( ) \n 
self . _class_props = dict ( ) \n 
self . _raw_dict = False \n 
~~ def visit_With ( self , node ) : \n 
~~~ s = [ ] \n 
for b in node . body : \n 
~~~ a = self . visit ( b ) \n 
a = a . replace ( , ) \n 
s . append ( a ) \n 
~~ return . join ( s ) \n 
~~ def _visit_subscript_ellipsis ( self , node ) : \n 
~~~ name = self . visit ( node . value ) \n 
return % name \n 
~~ def visit_List ( self , node ) : \n 
~~~ return % . join ( map ( self . visit , node . elts ) ) \n 
~~ def visit_Dict ( self , node ) : \n 
~~~ a = [ ] \n 
for i in range ( len ( node . keys ) ) : \n 
~~~ k = self . visit ( node . keys [ i ] ) \n 
v = self . visit ( node . values [ i ] ) \n 
a . append ( % ( k , v ) ) \n 
~~ b = . join ( a ) \n 
if self . _raw_dict : \n 
~~~ return % b \n 
~~ ~~ def visit_ClassDef ( self , node ) : \n 
~~~ node . _parents = set ( ) \n 
out = [ ] \n 
props = set ( [ ] ) \n 
bases = set ( ) \n 
base_classes = set ( ) \n 
self . _classes [ node . name ] = node \n 
self . _class_props [ node . name ] = props \n 
~~~ if isinstance ( decor , ast . Call ) : \n 
~~~ props . update ( [ self . visit ( a ) for a in decor . args ] ) \n 
~~ elif isinstance ( decor , ast . Attribute ) and isinstance ( decor . value , ast . Name ) and decor . value . id == ~~~ if decor . attr == : \n 
~~~ extends = True \n 
props . add ( ) \n 
for name_node in collect_names ( node ) : \n 
~~~ if name_node . id == : \n 
~~~ name_node . id = \n 
~~~ raise SyntaxError \n 
~~ ~~ for base in node . bases : \n 
~~~ n = self . visit ( base ) \n 
if n == : \n 
~~ node . _parents . add ( n ) \n 
bases . add ( n ) \n 
if n in self . _class_props : \n 
~~~ props . update ( self . _class_props [ n ] ) \n 
base_classes . add ( self . _classes [ n ] ) \n 
~~ for p in self . _classes [ n ] . _parents : \n 
~~~ bases . add ( p ) \n 
props . update ( self . _class_props [ p ] ) \n 
base_classes . add ( self . _classes [ p ] ) \n 
~~ ~~ if bases : \n 
~~~ if extends : \n 
~~~ assert len ( bases ) == 1 \n 
out . append ( % ( node . name , . join ( bases ) ) ) \n 
~~~ out . append ( % ( node . name , . join ( bases ) ) ) \n 
~~~ out . append ( % node . name ) \n 
~~ self . push ( ) \n 
for p in props : \n 
~~~ out . append ( self . indent ( ) + % p ) \n 
~~ method_names = set ( ) \n 
~~~ if isinstance ( b , ast . With ) : \n 
~~~ out . append ( self . visit ( b ) ) \n 
~~ elif isinstance ( b , ast . FunctionDef ) and len ( b . decorator_list ) : ##getter/setters \n 
~~~ for name_node in collect_names ( b ) : \n 
~~ ~~ b . args . args = b . args . args [ 1 : ] \n 
out . append ( self . visit ( b ) ) \n 
~~ elif extends : \n 
~~~ if isinstance ( b , ast . FunctionDef ) : \n 
~~~ b . args . args = b . args . args [ 1 : ] \n 
if b . name == node . name : \n 
~~~ args = [ self . visit ( a ) for a in b . args . args ] \n 
args = . join ( args ) \n 
out . append ( \n 
self . indent ( ) + % ( node . name , args , args ) \n 
b . name = \n 
~~ elif b . name == : \n 
~~~ b . name = \n 
b . _prefix = \n 
~~ ~~ line = self . visit ( b ) \n 
out . append ( line ) \n 
~~ elif isinstance ( b , ast . FunctionDef ) and b . name == node . name : \n 
~~~ args , kwargs = self . get_args_kwargs_from_funcdef ( b , skip_self = True ) \n 
kwargs_init = [ % ( x . split ( ) [ 0 ] , x . split ( ) [ 0 ] ) for x in kwargs ] \n 
if args : \n 
~~~ args = . join ( args ) \n 
if kwargs : \n 
~~~ out . append ( \n 
self . indent ( ) + % ( node . name , args , . join ( kwargs ) , node ) \n 
self . indent ( ) + % ( node . name , args , node . name , args ) \n 
~~ ~~ elif kwargs : \n 
self . indent ( ) + % ( node . name , . join ( kwargs ) , node . name , ) \n 
self . indent ( ) + % ( node . name , node . name ) \n 
~~ ~~ elif isinstance ( b , ast . FunctionDef ) : \n 
~~~ method_names . add ( b . name ) \n 
TransformSuperCalls ( b , bases ) \n 
operator = False \n 
if b . name == : \n 
~~~ operator = \n 
~~ args = [ self . visit ( a ) for a in b . args . args ] [ 1 : ] \n 
if operator and args : \n 
~~~ out . append ( self . indent ( ) + % ( operator , args , node . name , b . name \n 
~~ elif operator : \n 
~~~ out . append ( self . indent ( ) + % ( operator , node . name , b . name ) ) \n 
~~ elif args : \n 
~~~ out . append ( self . indent ( ) + % ( b . name , args , node . name , b . name ~~ else : \n 
~~~ out . append ( self . indent ( ) + % ( b . name , node . name , b . name ) ) \n 
~~ b . _prefix = \n 
name = b . name \n 
b . name = % name \n 
b . name = name \n 
~~~ line = self . visit ( b ) \n 
if line . startswith ( ) : \n 
~~~ out . append ( self . indent ( ) + line ) \n 
~~~ out . append ( line ) \n 
~~ ~~ ~~ if not extends and base_classes : \n 
~~~ for bnode in base_classes : \n 
~~~ for b in bnode . body : \n 
~~~ if b . name == : continue \n 
if b . name in method_names : continue \n 
args = [ self . visit ( a ) for a in b . args . args ] [ 1 : ] \n 
~~~ out . append ( self . indent ( ) + % ( b . name , args , bnode . name , b . ~~ else : \n 
~~~ out . append ( self . indent ( ) + % ( b . name , bnode . name , b . name ) ) \n 
~~ ~~ ~~ ~~ ~~ self . pull ( ) \n 
out . append ( ) \n 
return . join ( out ) \n 
~~ def get_args_kwargs_from_funcdef ( self , node , skip_self = False ) : \n 
~~~ args = [ ] \n 
kwargs = [ ] \n 
if skip_self : nargs = node . args . args [ 1 : ] \n 
else : nargs = node . args . args \n 
offset = len ( nargs ) - len ( node . args . defaults ) \n 
for i , arg in enumerate ( nargs ) : \n 
~~~ a = arg . id \n 
if dindex >= 0 and node . args . defaults : \n 
~~~ default_value = self . visit ( node . args . defaults [ dindex ] ) \n 
kwargs . append ( % ( a , default_value ) ) \n 
~~~ args . append ( a ) \n 
~~ ~~ return args , kwargs \n 
~~ def _visit_for_prep_iter_helper ( self , node , out , iter_name ) : \n 
self . indent ( ) + % ( iter_name , iter_name , iter_name ) \n 
~~ def visit_Expr ( self , node ) : \n 
~~~ s = self . visit ( node . value ) \n 
if isinstance ( node . value , ast . Call ) and isinstance ( node . value . func , ast . Name ) and node . value . func . ~~~ if s . endswith ( ) and in s . split ( ) : \n 
~~ elif not s . endswith ( ) : \n 
~~~ s += \n 
~~ ~~ elif not s . endswith ( ) : \n 
~~ return s \n 
~~ def visit_Print ( self , node ) : \n 
~~~ args = [ self . visit ( e ) for e in node . values ] \n 
if len ( args ) > 1 : \n 
~~~ s = % . join ( args ) \n 
~~ def visit_Assign ( self , node ) : \n 
~~~ assert len ( node . targets ) == 1 \n 
target = node . targets [ 0 ] \n 
if isinstance ( target , ast . Tuple ) : \n 
~~~ elts = [ self . visit ( e ) for e in target . elts ] \n 
if self . indent ( ) : \n 
~~~ return % ( . join ( elts ) , self . visit ( node . value ) ) \n 
~~~ target = self . visit ( target ) \n 
value = self . visit ( node . value ) \n 
~~~ code = % ( target , value ) \n 
~~ return code \n 
~~ ~~ def _visit_function ( self , node ) : \n 
~~~ getter = False \n 
setter = False \n 
args_typedefs = { } \n 
for decor in node . decorator_list : \n 
~~~ if isinstance ( decor , ast . Name ) and decor . id == : \n 
~~~ getter = True \n 
~~ elif isinstance ( decor , ast . Attribute ) and isinstance ( decor . value , ast . Name ) and decor . attr == ~~~ setter = True \n 
~~ elif isinstance ( decor , ast . Call ) and isinstance ( decor . func , ast . Name ) and decor . func . id == ~~~ for key in decor . keywords : \n 
~~~ args_typedefs [ key . arg ] = key . value . id \n 
~~ ~~ args = [ ] #self.visit(node.args) \n 
oargs = [ ] \n 
offset = len ( node . args . args ) - len ( node . args . defaults ) \n 
varargs = False \n 
varargs_name = None \n 
for i , arg in enumerate ( node . args . args ) : \n 
if a in args_typedefs : \n 
~~~ a = % ( args_typedefs [ a ] , a ) \n 
~~ dindex = i - offset \n 
if a . startswith ( ) : \n 
~~~ varargs_name = a . split ( ) [ - 1 ] \n 
varargs = [ % n for n in range ( 16 ) ] \n 
args . append ( % . join ( varargs ) ) \n 
~~ elif dindex >= 0 and node . args . defaults : \n 
oargs . append ( % ( a , default_value ) ) \n 
~~ ~~ if oargs : \n 
~~~ args . append ( % . join ( oargs ) ) \n 
~~ buffer = self . indent ( ) \n 
if hasattr ( node , ) : buffer += node . _prefix + \n 
if getter : \n 
~~~ buffer += % node . name \n 
~~ elif setter : \n 
~~~ buffer += % ( node . name , . join ( args ) ) \n 
if varargs : \n 
~~~ buffer += % varargs_name \n 
for i , n in enumerate ( varargs ) : \n 
~~~ buffer += % ( n , varargs_name , n ) \n 
~~ ~~ body = list ( ) \n 
for child in node . body : \n 
~~~ if isinstance ( child , ast . Str ) : \n 
~~~ body . append ( self . indent ( ) + self . visit ( child ) ) \n 
~~ ~~ buffer += . join ( body ) \n 
self . pull ( ) \n 
buffer += % self . indent ( ) \n 
return buffer \n 
~~ def visit_Is ( self , node ) : \n 
~~ def visit_IsNot ( self , node ) : \n 
~~ def visit_NotEq ( self , node ) : \n 
~~ def _visit_call_helper ( self , node ) : \n 
~~~ if node . args : \n 
~~~ args = [ self . visit ( e ) for e in node . args ] \n 
args = . join ( [ e for e in args if e ] ) \n 
~~~ args = \n 
~~ if isinstance ( node . func , ast . Name ) and node . func . id == and len ( node . args ) == 2 : \n 
~~~ func = \n 
~~~ func = self . visit ( node . func ) \n 
~~ if node . keywords : \n 
~~~ kwargs = . join ( [ % ( x . arg , self . visit ( x . value ) ) for x in node . keywords ] ) \n 
~~~ return % ( func , . join ( args ) , kwargs ) \n 
~~~ return % ( func , kwargs ) \n 
~~~ return % ( func , args ) \n 
~~ ~~ def _visit_call_helper_var ( self , node ) : \n 
~~~ args = [ self . visit ( a ) for a in node . args ] \n 
if self . _function_stack : \n 
~~~ fnode = self . _function_stack [ - 1 ] \n 
rem = [ ] \n 
~~~ if arg in fnode . _local_vars : \n 
~~~ rem . append ( arg ) \n 
~~~ fnode . _local_vars . add ( arg ) \n 
~~ ~~ for arg in rem : \n 
~~~ args . remove ( arg ) \n 
~~ ~~ out = [ ] \n 
~~~ out . append ( + . join ( args ) ) \n 
~~~ for key in node . keywords : \n 
~~~ out . append ( % ( key . value . id , key . arg ) ) \n 
~~ ~~ return . join ( out ) \n 
~~ def _visit_call_helper_list ( self , node ) : \n 
if node . args : \n 
~~ return % ( name , args ) \n 
~~ def _visit_call_helper_numpy_array ( self , node ) : \n 
~~~ simd = { \n 
arg_name = args = None \n 
direct = False \n 
if isinstance ( node . args [ 0 ] , ast . Name ) : \n 
~~~ arg_name = node . args [ 0 ] . id \n 
~~~ args = . join ( [ self . visit ( a ) for a in node . args [ 0 ] . elts ] ) \n 
~~~ direct = True \n 
~~ ~~ if node . keywords : \n 
~~~ if key . arg == : \n 
~~~ if isinstance ( key . value , ast . Attribute ) and key . value . attr in simd : \n 
~~~ if arg_name : \n 
~~~ return % arg_name \n 
~~ elif direct : \n 
~~~ return % ( simd [ key . value . attr ] , args ) \n 
~~~ return % args \n 
~~ ~~ def _visit_call_helper_instanceof ( self , node ) : \n 
~~~ args = map ( self . visit , node . args ) \n 
if len ( args ) == 2 : \n 
~~~ if args [ 1 ] == : \n 
~~~ args [ 1 ] = \n 
~~ return % tuple ( args ) \n 
~~~ raise SyntaxError ( args ) \n 
~~ ~~ def visit_ExceptHandler ( self , node ) : \n 
~~~ return . join ( [ self . visit ( n ) for n in node . body ] ) \n 
~~ def visit_Compare ( self , node ) : \n 
~~~ specials = { \n 
comp = [ ] \n 
if len ( node . ops ) == 0 : \n 
~~~ comp . append ( ) \n 
comp . append ( self . visit ( node . left ) ) \n 
comp . append ( ) \n 
~~~ if self . visit ( node . ops [ 0 ] ) in specials : \n 
~~ for i in range ( len ( node . ops ) ) : \n 
~~~ op = self . visit ( node . ops [ i ] ) \n 
if op in specials : \n 
~~~ comp . append ( specials [ op ] + % self . visit ( node . left ) ) \n 
~~~ comp . append ( op ) \n 
~~ if isinstance ( node . comparators [ i ] , ast . BinOp ) : \n 
comp . append ( self . visit ( node . comparators [ i ] ) ) \n 
~~~ comp . append ( self . visit ( node . comparators [ i ] ) ) \n 
~~ if op in specials : \n 
~~ ~~ ~~ return . join ( comp ) \n 
~~ ~~ def main ( script ) : \n 
~~~ tree = ast . parse ( script ) \n 
return DartGenerator ( ) . visit ( tree ) \n 
~~ def command ( ) : \n 
~~~ scripts = [ ] \n 
if len ( sys . argv ) > 1 : \n 
~~~ for arg in sys . argv [ 1 : ] : \n 
~~~ if arg . endswith ( ) : \n 
~~~ scripts . append ( arg ) \n 
~~ ~~ ~~ if len ( scripts ) : \n 
for script in scripts : \n 
~~~ a . append ( open ( script , ) . read ( ) ) \n 
~~ data = . join ( a ) \n 
~~~ data = sys . stdin . read ( ) \n 
~~ js = main ( data ) \n 
print ( js ) \n 
~~~ command ( ) \n 
~~~ if PYTHON == : \n 
~~~ pythonjs . configure ( direct_operator = ) \n 
~~ starttime = time ( ) \n 
n = 3000 \n 
seq = [ ] \n 
cache = [ ] \n 
w1 = threading . start_webworker ( worker , ( 0 , n , seq , cache ) ) \n 
sleep ( 1.0 ) \n 
testtime = time ( ) - starttime \n 
primes_per_sec = len ( seq ) * ( 1.0 / testtime ) \n 
print ( primes_per_sec ) \n 
print ( % testtime ) \n 
~~ with webworker : \n 
~~~ def worker ( start , end , seq , cache ) : \n 
for i in range ( start , end ) : \n 
~~~ if i in cache : \n 
~~~ cache . append ( i ) \n 
if is_prime ( i ) : \n 
~~~ seq . append ( i ) \n 
~~ ~~ ~~ print ( % i ) \n 
~~ def is_prime ( n ) : \n 
~~~ hits = 0 \n 
for x in range ( 2 , n ) : \n 
~~~ for y in range ( 2 , n ) : \n 
~~~ if x * y == n : \n 
~~~ hits += 1 \n 
if hits > 1 : \n 
~~ ~~ ~~ ~~ return True \n 
if d : \n 
~~~ err1 = 1 \n 
~~~ err1 = 0 \n 
~~ if { } : \n 
~~~ err2 = 1 \n 
~~~ err2 = 0 \n 
~~ d [ ] = \n 
~~~ err3 = 0 \n 
~~~ err3 = 1 \n 
~~ TestError ( err1 == 0 ) \n 
TestError ( err2 == 0 ) \n 
TestError ( err3 == 0 ) \n 
~~~ a = False \n 
b = False \n 
if not a : \n 
~~~ b = True \n 
~~ TestError ( b == True ) \n 
a = 0 \n 
a = 0.0 \n 
a = None \n 
~~~ a = range ( 10 ) \n 
TestError ( a [ 0 ] == 0 ) \n 
TestError ( a [ 1 ] == 1 ) \n 
TestError ( len ( a ) == 10 ) \n 
b = range ( 1 , 10 ) \n 
TestError ( b [ 0 ] == 1 ) \n 
TestError ( b [ 1 ] == 2 ) \n 
TestError ( len ( b ) == 9 ) \n 
c = 0 \n 
~~~ c += 1 \n 
~~ TestError ( c == 10 ) \n 
d = 0 \n 
for i in range ( 1 , 10 ) : \n 
~~~ d += 1 \n 
~~ TestError ( d == 9 ) \n 
for i in range ( 1 , 8 + 2 ) : \n 
~~~ e += 1 \n 
~~ TestError ( e == 9 ) \n 
~~~ def l ( f , a ) : threading . _start_new_thread ( f , a ) \n 
threading . start_webworker = l \n 
~~ seq = { } \n 
w1 = threading . start_webworker ( worker , ( seq , , ) ) \n 
w2 = threading . start_webworker ( worker , ( seq , , ) ) \n 
TestError ( in seq ) \n 
print ( seq ) \n 
~~ if PYTHON != : \n 
~~~ class webworker ( object ) : \n 
~~~ def __enter__ ( self , * args ) : pass \n 
def __exit__ ( self , * args ) : pass \n 
~~ webworker = webworker ( ) \n 
~~~ def worker ( seq , s , break_on ) : \n 
for char in s : \n 
~~~ seq [ char ] = True \n 
if break_on in seq : \n 
~~ ~~ from OpenGL . GL import * \n 
from OpenGL . GLU import * \n 
import pygame \n 
class Material ( object ) : \n 
~~~ self . name = "" \n 
self . texture_fname = None \n 
self . texture_id = None \n 
~~ ~~ class FaceGroup ( object ) : \n 
~~~ self . tri_indices = [ ] \n 
self . material_name = "" \n 
~~ ~~ class Model3D ( object ) : \n 
~~~ self . vertices = [ ] \n 
self . tex_coords = [ ] \n 
self . normals = [ ] \n 
self . materials = { } \n 
self . face_groups = [ ] \n 
self . display_list_id = None \n 
~~~ self . free_resources ( ) \n 
~~ def free_resources ( self ) : \n 
~~~ if self . display_list_id is not None : \n 
~~~ glDeleteLists ( self . display_list_id , 1 ) \n 
~~ for material in self . materials . values ( ) : \n 
~~~ if material . texture_id is not None : \n 
~~~ glDeleteTextures ( material . texture_id ) \n 
~~ ~~ self . materials . clear ( ) \n 
del self . vertices [ : ] \n 
del self . tex_coords [ : ] \n 
del self . normals [ : ] \n 
del self . face_groups [ : ] \n 
~~ def read_obj ( self , fname ) : \n 
~~~ current_face_group = None \n 
file_in = open ( fname ) \n 
for line in file_in : \n 
~~~ words = line . split ( ) \n 
command = words [ 0 ] \n 
data = words [ 1 : ] \n 
~~~ model_path = os . path . split ( fname ) [ 0 ] \n 
mtllib_path = os . path . join ( model_path , data [ 0 ] ) \n 
self . read_mtllib ( mtllib_path ) \n 
~~~ x , y , z = data \n 
vertex = ( float ( x ) , float ( y ) , float ( z ) ) \n 
self . vertices . append ( vertex ) \n 
~~~ s , t = data \n 
tex_coord = ( float ( s ) , float ( t ) ) \n 
self . tex_coords . append ( tex_coord ) \n 
normal = ( float ( x ) , float ( y ) , float ( z ) ) \n 
self . normals . append ( normal ) \n 
~~~ current_face_group = FaceGroup ( ) \n 
current_face_group . material_name = data [ 0 ] \n 
self . face_groups . append ( current_face_group ) \n 
~~ elif command == : \n 
for word in data : \n 
~~~ vi , ti , ni = word . split ( ) \n 
indices = ( int ( vi ) - 1 , int ( ti ) - 1 , int ( ni ) - 1 ) \n 
current_face_group . tri_indices . append ( indices ) \n 
~~ ~~ ~~ for material in self . materials . values ( ) : \n 
texture_path = os . path . join ( model_path , material . texture_fname ) \n 
texture_surface = pygame . image . load ( texture_path ) \n 
texture_data = pygame . image . tostring ( texture_surface , , True ) \n 
material . texture_id = glGenTextures ( 1 ) \n 
glBindTexture ( GL_TEXTURE_2D , material . texture_id ) \n 
glTexParameteri ( GL_TEXTURE_2D , \n 
GL_TEXTURE_MAG_FILTER , \n 
GL_LINEAR ) \n 
GL_TEXTURE_MIN_FILTER , \n 
GL_LINEAR_MIPMAP_LINEAR ) \n 
glPixelStorei ( GL_UNPACK_ALIGNMENT , 1 ) \n 
width , height = texture_surface . get_rect ( ) . size \n 
gluBuild2DMipmaps ( GL_TEXTURE_2D , \n 
3 , \n 
width , \n 
height , \n 
GL_RGB , \n 
GL_UNSIGNED_BYTE , \n 
texture_data ) \n 
~~ ~~ def read_mtllib ( self , mtl_fname ) : \n 
~~~ file_mtllib = open ( mtl_fname ) \n 
for line in file_mtllib : \n 
if command == : \n 
~~~ material = Material ( ) \n 
material . name = data [ 0 ] \n 
self . materials [ data [ 0 ] ] = material \n 
~~~ material . texture_fname = data [ 0 ] \n 
~~ ~~ ~~ def draw ( self ) : \n 
~~~ vertices = self . vertices \n 
tex_coords = self . tex_coords \n 
normals = self . normals \n 
for face_group in self . face_groups : \n 
~~~ material = self . materials [ face_group . material_name ] \n 
glBegin ( GL_TRIANGLES ) \n 
for vi , ti , ni in face_group . tri_indices : \n 
~~~ glTexCoord2fv ( tex_coords [ ti ] ) \n 
glNormal3fv ( normals [ ni ] ) \n 
glVertex3fv ( vertices [ vi ] ) \n 
~~ glEnd ( ) \n 
~~ ~~ def draw_quick ( self ) : \n 
~~~ if self . display_list_id is None : \n 
~~~ self . display_list_id = glGenLists ( 1 ) \n 
glNewList ( self . display_list_id , GL_COMPILE ) \n 
self . draw ( ) \n 
~~ glCallList ( self . display_list_id ) \n 
~~ ~~ def saturate_color ( color ) : \n 
~~~ red , green , blue = color \n 
red = min ( red , 255 ) \n 
green = min ( green , 255 ) \n 
blue = min ( blue , 255 ) \n 
return red , green , blue \n 
~~ import pygame \n 
from pygame . locals import * \n 
from sys import exit \n 
from gameobjects . vector2 import Vector2 \n 
picture_file = \n 
pygame . init ( ) \n 
screen = pygame . display . set_mode ( ( 640 , 480 ) , 0 , 32 ) \n 
picture = pygame . image . load ( picture_file ) . convert ( ) \n 
picture_pos = Vector2 ( 0 , 0 ) \n 
scroll_speed = 1000. \n 
clock = pygame . time . Clock ( ) \n 
joystick = None \n 
if pygame . joystick . get_count ( ) > 0 : \n 
~~~ joystick = pygame . joystick . Joystick ( 0 ) \n 
joystick . init ( ) \n 
~~ if joystick is None : \n 
pygame . quit ( ) \n 
exit ( ) \n 
~~~ for event in pygame . event . get ( ) : \n 
~~~ if event . type == QUIT : \n 
~~~ pygame . quit ( ) \n 
~~ ~~ scroll_direction = Vector2 ( * joystick . get_hat ( 0 ) ) \n 
scroll_direction . normalize ( ) \n 
screen . fill ( ( 255 , 255 , 255 ) ) \n 
screen . blit ( picture , ( - picture_pos . x , picture_pos . y ) ) \n 
time_passed = clock . tick ( ) \n 
time_passed_seconds = time_passed / 1000.0 \n 
picture_pos += scroll_direction * scroll_speed * time_passed_seconds \n 
pygame . display . update ( ) \n 
__version__ = "0.0.3" \n 
THISDIR = os . path . dirname ( os . path . abspath ( __file__ ) ) \n 
os . chdir ( THISDIR ) \n 
VERSION = open ( "version.txt" ) . readline ( ) . strip ( ) \n 
HOMEPAGE = "https://github.com/ofpay/dubbo-client-py" \n 
DOWNLOAD_BASEURL = "https://github.com/ofpay/dubbo-client-py/raw/master/dist/" \n 
DOWNLOAD_URL = DOWNLOAD_BASEURL + "dubbo-client-%s-py2.7.egg" % VERSION \n 
name = "dubbo-client" , \n 
long_description = open ( "README.md" ) . read ( ) , \n 
keywords = ( \n 
author_email = "chinalibra@gmail.com" , \n 
url = HOMEPAGE , \n 
download_url = DOWNLOAD_URL , \n 
install_requires = [ "kazoo>=2.0" , "python-jsonrpc>=0.7.3" ] , \n 
field = models . BooleanField ( default = False ) , \n 
import utils . models \n 
migrations . swappable_dependency ( settings . AUTH_USER_MODEL ) , \n 
( , models . AutoField ( verbose_name = , serialize = False , auto_created = True , primary_key ( , models . IntegerField ( default = 0 ) ) , \n 
( , models . IntegerField ( default = 0 ) ) , \n 
( , utils . models . JsonField ( default = { } ) ) , \n 
( , models . ForeignKey ( to = ) ) , \n 
( , models . ForeignKey ( to = settings . AUTH_USER_MODEL ) ) , \n 
import judger \n 
WA = 1 \n 
AC = 0 \n 
SPJ_ERROR = - 1 \n 
def file_exists ( path ) : \n 
~~~ return os . path . exists ( path ) \n 
~~ def spj ( path , max_cpu_time , max_memory , in_path , user_out_path ) : \n 
~~~ if file_exists ( in_path ) and file_exists ( user_out_path ) : \n 
~~~ result = judger . run ( path = path , in_file = in_path , out_file = "/tmp/spj.out" , \n 
max_cpu_time = max_cpu_time , max_memory = max_memory , \n 
args = [ in_path , user_out_path ] , env = [ "PATH=" + os . environ . get ( "PATH" , "" ) use_sandbox = True , use_nobody = True ) \n 
if result [ "signal" ] == 0 and result [ "exit_status" ] in [ AC , WA , SPJ_ERROR ] : \n 
~~~ result [ "spj_result" ] = result [ "exit_status" ] \n 
~~~ result [ "spj_result" ] = SPJ_ERROR \n 
migrations . RemoveField ( \n 
~~ from django . http import HttpResponse \n 
from utils . captcha import Captcha \n 
def show_captcha ( request ) : \n 
~~~ return HttpResponse ( Captcha ( request ) . display ( ) , content_type = "image/gif" ) \n 
def _print_after_skip ( skip , it = None , dist = None , etime = None ) : \n 
~~~ if it is None : \n 
~~~ msg = "{i:<13}{d:<15}{t:<17}" . format ( i = "Iteration" , \n 
d = "Distance" , \n 
print ( msg ) \n 
print ( "-" * len ( msg ) ) \n 
~~ if it % skip == 0 : \n 
~~~ if etime is None : \n 
~~~ msg = "{i:<13}{d:<15.3e}{t:<18.3e}" \n 
print ( msg . format ( i = it , d = dist , t = etime ) ) \n 
~~ def compute_fixed_point ( T , v , error_tol = 1e-3 , max_iter = 50 , verbose = 1 , \n 
print_skip = 5 , * args , ** kwargs ) : \n 
iterate = 0 \n 
error = error_tol + 1 \n 
~~~ start_time = time . time ( ) \n 
_print_after_skip ( print_skip , it = None ) \n 
~~ while iterate < max_iter and error > error_tol : \n 
~~~ new_v = T ( v , * args , ** kwargs ) \n 
iterate += 1 \n 
error = np . max ( np . abs ( new_v - v ) ) \n 
~~~ etime = time . time ( ) - start_time \n 
_print_after_skip ( print_skip , iterate , error , etime ) \n 
~~~ v [ : ] = new_v \n 
~~~ v = new_v \n 
~~ ~~ return v \n 
from scipy . linalg import LinAlgError \n 
from numpy . testing import assert_allclose \n 
from quantecon . lqcontrol import LQ \n 
from quantecon . robustlq import RBLQ \n 
class TestRBLQControl ( unittest . TestCase ) : \n 
~~~ a_0 = 100 \n 
a_1 = 0.5 \n 
rho = 0.9 \n 
sigma_d = 0.05 \n 
beta = 0.95 \n 
c = 2 \n 
gamma = 50.0 \n 
theta = 0.002 \n 
ac = ( a_0 - c ) / 2.0 \n 
R = np . array ( [ [ 0 , ac , 0 ] , \n 
[ ac , - a_1 , 0.5 ] , \n 
[ 0. , 0.5 , 0 ] ] ) \n 
R = - R \n 
Q = gamma / 2 \n 
A = np . array ( [ [ 1. , 0. , 0. ] , \n 
[ 0. , 1. , 0. ] , \n 
[ 0. , 0. , rho ] ] ) \n 
B = np . array ( [ [ 0. ] , \n 
[ 1. ] , \n 
[ 0. ] ] ) \n 
C = np . array ( [ [ 0. ] , \n 
[ 0. ] , \n 
[ sigma_d ] ] ) \n 
self . rblq_test = RBLQ ( Q , R , A , B , C , beta , theta ) \n 
self . lq_test = LQ ( Q , R , A , B , C , beta ) \n 
self . Fr , self . Kr , self . Pr = self . rblq_test . robust_rule ( ) \n 
~~~ del self . rblq_test \n 
~~ def test_robust_rule_vs_simple ( self ) : \n 
~~~ rblq = self . rblq_test \n 
Fr , Kr , Pr = self . Fr , self . Kr , self . Pr \n 
Fs , Ks , Ps = rblq . robust_rule_simple ( P_init = Pr , tol = 1e-12 ) \n 
assert_allclose ( Fr , Fs , rtol = 1e-4 ) \n 
assert_allclose ( Kr , Ks , rtol = 1e-4 ) \n 
assert_allclose ( Pr , Ps , rtol = 1e-4 ) \n 
~~ def test_f2k_and_k2f ( self ) : \n 
K_f2k , P_f2k = rblq . F_to_K ( Fr ) \n 
F_k2f , P_k2f = rblq . K_to_F ( Kr ) \n 
assert_allclose ( K_f2k , Kr , rtol = 1e-4 ) \n 
assert_allclose ( F_k2f , Fr , rtol = 1e-4 ) \n 
assert_allclose ( P_f2k , P_k2f , rtol = 1e-4 ) \n 
~~ def test_evaluate_F ( self ) : \n 
Kf , Pf , df , Of , of = rblq . evaluate_F ( Fr ) \n 
assert_allclose ( Pf , Pr ) \n 
assert_allclose ( Kf , Kr ) \n 
~~~ suite = unittest . TestLoader ( ) . loadTestsFromTestCase ( TestRBLQControl ) \n 
unittest . TextTestRunner ( verbosity = 2 , stream = sys . stderr ) . run ( suite ) \n 
import tables as pt \n 
fileName = "defaultAlphaFileName.h5" \n 
h5f = [ ] \n 
group = [ ] \n 
table = [ ] \n 
opened = False \n 
ctr = float ( 0.0 ) \n 
class AlphaDataModelClass ( pt . IsDescription ) : \n 
~~~ symbol = pt . StringCol ( 30 ) \n 
exchange = pt . StringCol ( 10 ) \n 
alphaValue = pt . Float32Col ( ) \n 
timestamp = pt . Time64Col ( ) \n 
~~ ~~ def openFile ( newFileName ) : \n 
global fileName , h5f , group , table , opened , ctr \n 
if newFileName is None : \n 
~~~ if ( len ( newFileName ) > 0 ) : \n 
~~~ fileName = str ( newFileName ) \n 
~~ ~~ if not opened : \n 
~~~ h5f = pt . openFile ( str ( fileName ) , mode = "w" ) \n 
group = h5f . createGroup ( "/" , ) \n 
table = h5f . createTable ( group , , AlphaDataModelClass ) \n 
opened = True \n 
~~ ~~ def addRow ( currSymbol , currExchange , currAlphaVal , currTS ) : \n 
global ctr \n 
if opened : \n 
~~~ ctr = ctr + 1 \n 
row = table . row \n 
row [ ] = currSymbol \n 
row [ ] = currExchange \n 
row [ ] = currAlphaVal \n 
row [ ] = currTS \n 
row . append ( ) \n 
~~~ ctr = 0 \n 
raise IOError \n 
~~ ~~ def closeFile ( ) : \n 
table . flush ( ) \n 
h5f . close ( ) \n 
import pickle as pkl \n 
import qstkutil . utils as utils \n 
import dircache \n 
~~~ print "Starting..." + str ( time . strftime ( "%H:%M:%S" ) ) \n 
~~~ rootdir = os . environ [ ] \n 
~~ fileExtensionToRemove = ".csv" \n 
listOfInputPaths = list ( ) \n 
listOfInputPaths . append ( rootdir + "/Raw/Yahoo/US/NASDAQ/" ) \n 
listOfInputPaths . append ( rootdir + "/Raw/Yahoo/US/NYSE/" ) \n 
listOfOutputPaths = list ( ) \n 
listOfOutputPaths . append ( rootdir + "/Processed/Yahoo/US/AMEX/" ) \n 
listOfOutputPaths . append ( rootdir + "/Processed/Yahoo/US/NASDAQ/" ) \n 
listOfOutputPaths . append ( rootdir + "/Processed/Yahoo/US/NYSE/" ) \n 
for path in listOfOutputPaths : \n 
~~~ if not ( os . access ( path , os . F_OK ) ) : \n 
if ( len ( listOfInputPaths ) != len ( listOfOutputPaths ) ) : \n 
sys . exit ( "FAILURE" ) \n 
~~ path_ctr = - 1 ; \n 
for path in listOfInputPaths : \n 
~~~ path_ctr = path_ctr + 1 ; \n 
stocks_at_this_path = dircache . listdir ( str ( path ) ) \n 
filtered_names = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , filtered_names ) \n 
stock_ctr = - 1 \n 
for stock in filtered_names : \n 
~~~ stock_ctr = stock_ctr + 1 \n 
stock_data = np . loadtxt ( path + stock + ".csv" , np . float , None , "," , None , 1 , use_cols ) \n 
stock_data_shape = stock_data . shape \n 
f = open ( listOfOutputPaths [ path_ctr ] + filtered_names [ stock_ctr ] + ".pkl" , "wb" ) \n 
pkl . dump ( stock_data , f , - 1 ) \n 
~~ ~~ print "Finished..." + str ( time . strftime ( "%H:%M:%S" ) ) \n 
~~ import cPickle \n 
from pandas import DataMatrix \n 
import qstkutil . DataAccess as da \n 
import qstkutil . qsdateutil as du \n 
symbols = list ( [ ] ) \n 
t = map ( int , sys . argv [ 1 ] . split ( ) ) \n 
startday = dt . datetime ( t [ 2 ] , t [ 0 ] , t [ 1 ] ) \n 
t = map ( int , sys . argv [ 2 ] . split ( ) ) \n 
endday = dt . datetime ( t [ 2 ] , t [ 0 ] , t [ 1 ] ) \n 
timeofday = dt . timedelta ( hours = 16 ) \n 
timestamps = du . getNYSEdays ( startday , endday , timeofday ) \n 
dataobj = da . DataAccess ( ) \n 
historic = dataobj . get_data ( timestamps , symbols , "close" ) \n 
alloc_val = random . random ( ) \n 
alloc = DataMatrix ( index = [ historic . index [ 0 ] ] , data = [ alloc_val ] , columns = symbols ) \n 
for date in range ( 1 , len ( historic . index ) ) : \n 
~~~ alloc_val = 1 #random.random() \n 
alloc = alloc . append ( DataMatrix ( index = [ historic . index [ date ] ] , data = [ alloc_val ] , columns = [ symbols ~~ alloc [ ] = 1 - alloc [ symbols [ 0 ] ] \n 
output = open ( sys . argv [ 3 ] , "wb" ) \n 
cPickle . dump ( alloc , output ) \n 
import QSTK . qstkutil . DataAccess as da \n 
from itertools import izip \n 
def getStocks ( listOfPaths ) : \n 
~~~ listOfStocks = list ( ) \n 
fileExtensionToRemove = ".h5" \n 
for path in listOfPaths : \n 
~~~ stocksAtThisPath = list ( ) \n 
stocksAtThisPath = dircache . listdir ( str ( path ) ) \n 
stocksAtThisPath = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , stocksAtThisPath \n 
for stock in stocksAtThisPath : \n 
~~~ listOfStocks . append ( stock ) \n 
~~ return listOfStocks \n 
~~~ print "Starting..." \n 
dataItemsList = [ ] \n 
dataItemsList . append ( ) \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_NASDAQ/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/Delisted_US_Recent/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/OTC/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_AMEX/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_Delisted/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_NYSE/") \n 
listOfStocks = list ( ) \n 
#listOfStocks.append("AAPL") \n 
#listOfStocks.append("YHOO") \n 
#listOfStocks.append("AMZN") \n 
listOfPaths = list ( ) \n 
listOfPaths . append ( "C:\\\\test\\\\temp\\\\" ) \n 
#listOfPaths.append("C:\\\\test\\\\hdf\\\\") \n 
listOfStocks = getStocks ( listOfPaths ) \n 
tslist = list ( alpha . getTimestampArray ( ) ) \n 
listOfTS = alpha . getTimestampArray ( ) \n 
for stock in [ "AAPL" ] : \n 
~~~ alphaList = alpha . getStockDataList ( stock , ) \n 
ctr = 0 \n 
for val in alphaList : \n 
ctr += 1 \n 
~~ ~~ print "DONE!" \n 
import collections_and_iterators \n 
class TestObjectMethods ( unittest . TestCase ) : \n 
~~~ self . singleLinkList = collections_and_iterators . SinglyLinkedList ( ) \n 
self . singleLinkListData = collections_and_iterators . SinglyLinkedList ( ) \n 
self . singleLinkListData . append ( "Cosmo" ) \n 
self . singleLinkListData . append ( "Allie" ) \n 
self . singleLinkListData . append ( "Watson" ) \n 
self . doubleLinkList = collections_and_iterators . DoublyLinkedList ( ) \n 
self . doubleLinkListData = collections_and_iterators . DoublyLinkedList ( ) \n 
~~ def test_empty_single_list ( self ) : \n 
~~~ self . assertEqual ( 0 , self . singleLinkList . size ) \n 
self . assertIsNone ( self . singleLinkList . head ) \n 
self . assertIsNone ( self . singleLinkList . cursor ) \n 
~~ def test_contains_success ( self ) : \n 
~~~ self . assertTrue ( "Cosmo" in self . singleLinkListData ) \n 
self . assertTrue ( "Allie" in self . singleLinkListData ) \n 
self . assertTrue ( "Watson" in self . singleLinkListData ) \n 
~~ def test_contains_failure ( self ) : \n 
~~~ self . assertFalse ( "Gabby" in self . singleLinkListData ) \n 
self . assertFalse ( "Thomas" in self . singleLinkListData ) \n 
~~ def test_append_success ( self ) : \n 
~~~ self . assertEqual ( "Cosmo" , self . singleLinkListData [ 0 ] ) \n 
self . assertEqual ( "Allie" , self . singleLinkListData [ 1 ] ) \n 
self . assertEqual ( "Watson" , self . singleLinkListData [ 2 ] ) \n 
~~ def test_append_failure ( self ) : \n 
~~~ with self . assertRaises ( IndexError ) : \n 
~~~ self . singleLinkListData [ 3 ] \n 
~~ self . singleLinkListData . append ( "Foley" ) \n 
self . assertEqual ( "Foley" , self . singleLinkListData [ 3 ] ) \n 
~~~ self . assertEqual ( "Cosmo" , self . singleLinkListData . __getitem__ ( 0 ) ) \n 
self . assertEqual ( "Allie" , self . singleLinkListData . __getitem__ ( 1 ) ) \n 
self . assertEqual ( "Watson" , self . singleLinkListData . __getitem__ ( 2 ) ) \n 
~~ def test_getitem_failure ( self ) : \n 
~~~ self . singleLinkListData . __getitem__ ( 3 ) \n 
self . singleLinkListData . __getitem__ ( - 3 ) \n 
~~ ~~ def test_setitem_success ( self ) : \n 
self . singleLinkListData [ 0 ] = "Smalls" \n 
self . assertEqual ( "Smalls" , self . singleLinkListData [ 0 ] ) \n 
~~ def test_setitem_failure ( self ) : \n 
~~~ self . singleLinkListData [ 5 ] = "Bruno" \n 
self . singleLinkListData [ - 1 ] = "Lucie" \n 
~~ ~~ def test_empty_double_list ( self ) : \n 
~~~ self . assertEqual ( 0 , self . doubleLinkList . size ) \n 
self . assertIsNone ( self . doubleLinkList . head ) \n 
self . assertIsNone ( self . doubleLinkList . cursor ) \n 
~~ def test_insert_success ( self ) : \n 
~~ def test_insert_fauilure ( self ) : \n 
~~~ unittest . main ( verbosity = 2 ) \n 
from textwrap import dedent \n 
from jedi . evaluate . cache import memoize_default \n 
from jedi . parser import Parser \n 
from jedi . common import indent_block \n 
DOCSTRING_PARAM_PATTERNS = [ \n 
DOCSTRING_RETURN_PATTERNS = [ \n 
REST_ROLE_PATTERN = re . compile ( ) \n 
@ memoize_default ( None , evaluator_is_first_arg = True ) \n 
def follow_param ( evaluator , param ) : \n 
~~~ func = param . parent_function \n 
param_str = _search_param_in_docstr ( func . raw_doc , str ( param . get_name ( ) ) ) \n 
return _evaluate_for_statement_string ( evaluator , param_str , param . get_parent_until ( ) ) \n 
~~ def _search_param_in_docstr ( docstr , param_str ) : \n 
patterns = [ re . compile ( p % re . escape ( param_str ) ) \n 
for p in DOCSTRING_PARAM_PATTERNS ] \n 
for pattern in patterns : \n 
~~~ match = pattern . search ( docstr ) \n 
~~~ return _strip_rst_role ( match . group ( 1 ) ) \n 
~~ def _strip_rst_role ( type_str ) : \n 
match = REST_ROLE_PATTERN . match ( type_str ) \n 
~~~ return match . group ( 1 ) \n 
~~~ return type_str \n 
~~ ~~ def _evaluate_for_statement_string ( evaluator , string , module ) : \n 
if string is None : \n 
~~ for element in re . findall ( , string ) : \n 
~~~ string = % element + string \n 
~~ p = Parser ( code % indent_block ( string ) , no_docstr = True ) \n 
pseudo_cls = p . module . subscopes [ 0 ] \n 
~~~ stmt = pseudo_cls . statements [ - 1 ] \n 
~~ pseudo_cls . parent = module \n 
definitions = evaluator . eval_statement ( stmt ) \n 
it = ( evaluator . execute ( d ) for d in definitions ) \n 
return list ( chain . from_iterable ( it ) ) or definitions \n 
~~ @ memoize_default ( None , evaluator_is_first_arg = True ) \n 
def find_return_types ( evaluator , func ) : \n 
~~~ def search_return_in_docstr ( code ) : \n 
~~~ for p in DOCSTRING_RETURN_PATTERNS : \n 
~~~ match = p . search ( code ) \n 
~~ ~~ ~~ type_str = search_return_in_docstr ( func . raw_doc ) \n 
return _evaluate_for_statement_string ( evaluator , type_str , func . get_parent_until ( ) ) \n 
class Assertions ( unittest . TestCase ) : \n 
~~ from jedi import parser \n 
from jedi . _compatibility import u \n 
~~ class TokenTest ( unittest . TestCase ) : \n 
~~~ def test_end_pos_one_line ( self ) : \n 
tok = parsed . module . subscopes [ 0 ] . statements [ 0 ] . _token_list [ 2 ] \n 
self . assertEqual ( tok . end_pos , ( 3 , 14 ) ) \n 
~~ def test_end_pos_multi_line ( self ) : \n 
self . assertEqual ( tok . end_pos , ( 4 , 11 ) ) \n 
~~ ~~ from types import FunctionType \n 
from rdflib . graph import ConjunctiveGraph \n 
from rdflib . graph import Graph \n 
from rdflib . term import BNode \n 
from rdflib . term import Literal \n 
from rdflib . term import URIRef \n 
from rdflib . term import Variable \n 
from rdflib . namespace import NamespaceManager \n 
from rdfextras . sparql import _questChar \n 
from rdfextras . sparql import SPARQLError \n 
from rdflib . util import check_object \n 
from rdflib . util import check_subject \n 
__all__ = [ , , ] \n 
class SPARQLGraph ( object ) : \n 
SPARQL_DATASET = 0 \n 
NAMED_GRAPH = 1 \n 
__slots__ = ( "graphVariable" , \n 
"DAWG_DATASET_COMPLIANCE" , \n 
"identifier" , \n 
"graphKind" , \n 
"graph" ) \n 
def __init__ ( self , graph , graphVariable = None , dSCompliance = False ) : \n 
~~~ assert not graphVariable or graphVariable [ 0 ] != , repr ( graphVariable ) \n 
self . graphVariable = graphVariable \n 
self . DAWG_DATASET_COMPLIANCE = dSCompliance \n 
self . graphKind = None \n 
if graph is not None : \n 
if isinstance ( graph , ConjunctiveGraph ) : \n 
~~~ self . graphKind = self . SPARQL_DATASET \n 
self . identifier = graph . default_context . identifier \n 
~~~ self . graphKind = self . NAMED_GRAPH \n 
self . identifier = graph . identifier \n 
~~ ~~ ~~ def setupGraph ( self , store , graphKind = None ) : \n 
~~~ gKind = graphKind and graphKind or self . graphKind \n 
self . graph = gKind ( store , self . identifier ) \n 
~~ def __reduce__ ( self ) : \n 
~~~ return ( SPARQLGraph , \n 
( None , \n 
self . graphVariable , \n 
self . DAWG_DATASET_COMPLIANCE ) , \n 
self . __getstate__ ( ) ) \n 
~~~ return ( self . graphVariable , \n 
self . DAWG_DATASET_COMPLIANCE , \n 
self . identifier ) #, \n 
~~ def __setstate__ ( self , arg ) : \n 
~~~ gVar , flag , identifier = arg \n 
self . graphVariable = gVar \n 
self . DAWG_DATASET_COMPLIANCE = flag \n 
self . identifier = identifier \n 
~~ def _clusterForward ( self , seed , Cluster ) : \n 
~~~ for ( p , o ) in self . graph . predicate_objects ( seed ) : \n 
~~~ if not ( seed , p , o ) in Cluster . graph : \n 
~~~ Cluster . add ( ( seed , p , o ) ) \n 
self . _clusterForward ( p , Cluster ) \n 
self . _clusterForward ( o , Cluster ) \n 
~~ ~~ def clusterForward ( self , seed , Cluster = None ) : \n 
if Cluster == None : \n 
~~~ Cluster = SPARQLGraph ( ) \n 
self . _clusterForward ( seed , Cluster ) \n 
return Cluster \n 
~~ def _clusterBackward ( self , seed , Cluster ) : \n 
~~~ for ( s , p ) in self . graph . subject_predicates ( seed ) : \n 
~~~ if not ( s , p , seed ) in Cluster . graph : \n 
~~~ Cluster . add ( ( s , p , seed ) ) \n 
self . _clusterBackward ( s , Cluster ) \n 
self . _clusterBackward ( p , Cluster ) \n 
~~ ~~ def clusterBackward ( self , seed , Cluster = None ) : \n 
self . _clusterBackward ( seed , Cluster ) \n 
~~ def cluster ( self , seed ) : \n 
return self . clusterBackward ( seed ) + self . clusterForward ( seed ) \n 
def _createResource ( v ) : \n 
if isinstance ( v , Literal ) or isinstance ( v , BNode ) or isinstance ( v , URIRef ) : \n 
~~ ~~ def _isResQuest ( r ) : \n 
if r and isinstance ( r , basestring ) and r [ 0 ] == _questChar : \n 
~~ class GraphPattern : \n 
def __init__ ( self , patterns = [ ] ) : \n 
self . patterns = [ ] \n 
self . constraints = [ ] \n 
self . unbounds = [ ] \n 
self . bnodes = { } \n 
if type ( patterns ) == list : \n 
~~~ self . addPatterns ( patterns ) \n 
~~ elif type ( patterns ) == tuple : \n 
~~~ self . addPattern ( patterns ) \n 
~~~ raise SPARQLError ( \n 
~~ ~~ def _generatePattern ( self , tupl ) : \n 
if type ( tupl ) != tuple : \n 
~~ if len ( tupl ) != 3 and len ( tupl ) != 4 : \n 
~~ if len ( tupl ) == 3 : \n 
~~~ ( s , p , o ) = tupl \n 
f = None \n 
~~~ ( s , p , o , f ) = tupl \n 
~~ final = [ ] \n 
for c in ( s , p , o ) : \n 
~~~ if _isResQuest ( c ) : \n 
~~~ if not c in self . unbounds : \n 
~~~ self . unbounds . append ( c ) \n 
~~ final . append ( c ) \n 
~~ elif isinstance ( c , BNode ) : \n 
~~~ final . append ( c ) \n 
~~~ final . append ( _createResource ( c ) ) \n 
~~ ~~ final . append ( f ) \n 
return tuple ( final ) \n 
~~ def addPattern ( self , tupl ) : \n 
self . patterns . append ( self . _generatePattern ( tupl ) ) \n 
~~ def insertPattern ( self , tupl ) : \n 
self . patterns . insert ( 0 , self . _generatePattern ( tupl ) ) \n 
~~ def addPatterns ( self , lst ) : \n 
for l in lst : \n 
~~~ self . addPattern ( l ) \n 
~~ ~~ def insertPatterns ( self , lst ) : \n 
for i in xrange ( len ( lst ) - 1 , - 1 , - 1 ) : \n 
~~~ self . insertPattern ( lst [ i ] ) \n 
~~ ~~ def addConstraint ( self , func ) : \n 
if type ( func ) == FunctionType : \n 
~~~ self . constraints . append ( func ) \n 
~~ ~~ def addConstraints ( self , lst ) : \n 
~~~ self . addConstraint ( l ) \n 
~~ ~~ def construct ( self , tripleStore , bindings ) : \n 
localBnodes = { } \n 
for c in self . bnodes : \n 
~~~ localBnodes [ c ] = BNode ( ) \n 
~~ def bind ( st ) : \n 
~~~ if _isResQuest ( st ) : \n 
~~~ if st in bindings : \n 
~~~ return bindings [ st ] \n 
~~~ if isinstance ( self , GraphPattern ) : \n 
~~~ return st \n 
~~ ~~ ~~ elif isinstance ( st , BNode ) : \n 
~~~ for c in self . bnodes : \n 
~~~ if self . bnodes [ c ] == st : \n 
~~~ return localBnodes [ c ] \n 
~~ ~~ return st \n 
~~ ~~ for pattern in self . patterns : \n 
~~~ ( s , p , o , f ) = pattern \n 
triplet = [ ] \n 
valid = True \n 
for res in ( s , p , o ) : \n 
~~~ val = bind ( res ) \n 
if val != None : \n 
~~~ triplet . append ( val ) \n 
~~~ valid = False \n 
~~ ~~ if valid : \n 
~~~ tripleStore . add ( tuple ( triplet ) ) \n 
~~ ~~ ~~ def __add__ ( self , other ) : \n 
retval = GraphPattern ( ) \n 
retval += self \n 
retval += other \n 
return retval \n 
self . patterns += other . patterns \n 
self . constraints += other . constraints \n 
for c in other . unbounds : \n 
~~ ~~ for c in other . bnodes : \n 
~~~ if not c in self . bnodes : \n 
~~~ self . bnodes [ c ] = other . bnodes [ c ] \n 
~~~ return self . __repr__ ( ) \n 
~~ def isEmpty ( self ) : \n 
return len ( self . patterns ) == 0 \n 
~~ ~~ class BasicGraphPattern ( GraphPattern ) : \n 
def __init__ ( self , patterns = [ ] , prolog = None ) : \n 
GraphPattern . __init__ ( self , patterns ) \n 
self . prolog = prolog \n 
~~ def canonicalTerm ( self , term ) : \n 
~~~ if isinstance ( term , URIRef ) : \n 
~~~ if self . prolog is not None : \n 
~~~ namespace_manager = NamespaceManager ( Graph ( ) ) \n 
for prefix , uri in self . prolog . prefixBindings . items ( ) : \n 
~~~ namespace_manager . bind ( prefix , uri , override = False ) \n 
~~~ prefix , uri , localName = namespace_manager . compute_qname ( term ) \n 
~~~ return term \n 
~~ if prefix not in self . prolog . prefixBindings : \n 
~~~ return . join ( [ prefix , localName ] ) \n 
~~ ~~ elif isinstance ( term , Literal ) : \n 
~~~ return term . n3 ( ) \n 
~~ elif isinstance ( term , BNode ) : \n 
~~~ assert isinstance ( term , Variable ) \n 
return term . n3 ( ) \n 
~~~ if self . constraints : \n 
. join ( [ . join ( [ \n 
self . canonicalTerm ( pat [ 0 ] ) , \n 
self . canonicalTerm ( pat [ 1 ] ) , \n 
self . canonicalTerm ( pat [ 2 ] ) ] \n 
for pat in self . patterns ] ) ) \n 
~~~ return "BGP(%s)" % ( \n 
. join ( [ + . join ( [ \n 
self . canonicalTerm ( s ) , \n 
self . canonicalTerm ( p ) , \n 
self . canonicalTerm ( o ) ] \n 
for s , p , o , f in self . patterns ] ) ) \n 
~~ def _generatePattern ( self , tupl ) : \n 
~~~ if isinstance ( c , Variable ) : \n 
~~ def fetchTerminalExpression ( self ) : \n 
~~~ yield self \n 
~~~ from rdfextras . sparql . evaluate import Unbound \n 
v1 = Variable ( "a" ) \n 
u1 = Unbound ( "a" ) \n 
g = BasicGraphPattern ( \n 
[ ( "a" , "?b" , 24 ) , ( "?r" , "?c" , 12345 ) , ( v1 , "?c" , 3333 ) , ( u1 , "?c" , 9999 ) ] ) \n 
print g \n 
~~ from rdflib import ConjunctiveGraph , plugin \n 
from rdflib . store import Store \n 
class JSON ( unittest . TestCase ) : \n 
~~~ self . graph = ConjunctiveGraph ( plugin . get ( , Store ) ( ) ) \n 
self . graph . parse ( StringIO ( test_data ) , format = "n3" ) \n 
~~ def testComma ( self ) : \n 
results = self . graph . query ( test_query ) \n 
result_json = results . serialize ( format = ) \n 
self . failUnless ( result_json . find ( correct ) > 0 ) \n 
~~ def testHeader ( self ) : \n 
results = self . graph . query ( test_header_query ) \n 
self . failUnless ( result_json . find ( \'"x",\' ) == - 1 ) \n 
from rdflib import plugin \n 
from rdflib . namespace import Namespace , RDF , RDFS \n 
from rdflib import Graph \n 
import rdflib \n 
class AdvancedTests ( unittest . TestCase ) : \n 
~~~ memStore = plugin . get ( , Store ) ( ) \n 
self . testGraph = Graph ( memStore ) \n 
self . testGraph . parse ( StringIO ( testGraph1N3 ) , format = ) \n 
~~ def testNamedGraph ( self ) : \n 
~~~ OWL_NS = Namespace ( "http://www.w3.org/2002/07/owl#" ) \n 
rt = self . testGraph . query ( sparqlQ4 ) \n 
self . assertEquals ( set ( rt ) , set ( ( x , ) for x in [ OWL_NS . DatatypeProperty , OWL_NS . ObjectProperty , \n 
~~ def testScopedBNodes ( self ) : \n 
~~~ rt = self . testGraph . query ( sparqlQ1 ) \n 
self . assertEquals ( list ( rt ) [ 0 ] [ 0 ] , URIRef ( "http://test/foo" ) ) \n 
~~ def testCollectionContentWithinAndWithout ( self ) : \n 
~~~ rt = self . testGraph . query ( sparqlQ3 ) \n 
self . assertEquals ( list ( rt ) [ 0 ] [ 0 ] , URIRef ( "http://test/bar" ) ) \n 
~~ def testCollectionAsObject ( self ) : \n 
~~~ rt = self . testGraph . query ( sparqlQ2 ) \n 
self . assertEquals ( 1 , len ( rt ) ) \n 
~~~ suite = unittest . makeSuite ( AdvancedTests ) \n 
unittest . TextTestRunner ( verbosity = 3 ) . run ( suite ) \n 
~~ from urllib2 import URLError \n 
~~~ from Ft . Lib import UriException \n 
~~~ from urllib2 import URLError as UriException \n 
from rdflib import ConjunctiveGraph , URIRef \n 
class SPARQLloadContextsTest ( unittest . TestCase ) : \n 
~~~ def test_dSet_parsed_as_URL_raises_Exception ( self ) : \n 
graph = ConjunctiveGraph ( ) \n 
graph . get_context ( URIRef ( "http://test/" ) \n 
) . parse ( "http://www.w3.org/People/Berners-Lee/card.rdf" ) \n 
self . assertRaises ( ( URLError , UriException ) , \n 
graph . query , ( querystr ) , loadContexts = False ) \n 
~~ def test_dSet_parsed_as_context_returns_results ( self ) : \n 
graph . get_context ( URIRef ( ) \n 
r = graph . query ( querystr , loadContexts = True ) \n 
self . assert_ ( len ( r . bindings ) is not 0 ) \n 
from rdflib import Graph , RDF , RDFS , Literal \n 
from rdflib . namespace import FOAF \n 
~~~ g = Graph ( ) \n 
bob = g . resource ( ) \n 
bob . set ( FOAF . name , Literal ( "Bob" ) ) \n 
bill = g . resource ( ) \n 
bill . add ( RDF . type , FOAF . Agent ) \n 
bill . set ( RDFS . label , Literal ( "Bill" ) ) \n 
bill . add ( FOAF . knows , bob ) \n 
for friend in bill [ FOAF . knows ] : \n 
for friend in bill [ FOAF . knows / FOAF . name ] : \n 
~~~ print friend \n 
~~ bill [ RDFS . label ] = Literal ( "William" ) \n 
print g . serialize ( format = ) \n 
from codecs import getreader \n 
from rdflib . py3compat import b \n 
from rdflib import ConjunctiveGraph \n 
from rdflib . plugins . parsers . ntriples import NTriplesParser \n 
from rdflib . plugins . parsers . ntriples import ParseError \n 
from rdflib . plugins . parsers . ntriples import r_tail \n 
from rdflib . plugins . parsers . ntriples import r_wspace \n 
from rdflib . plugins . parsers . ntriples import r_wspaces \n 
class NQuadsParser ( NTriplesParser ) : \n 
~~~ def parse ( self , inputsource , sink , ** kwargs ) : \n 
self . sink = ConjunctiveGraph ( store = sink . store , identifier = sink . identifier ) \n 
source = inputsource . getByteStream ( ) \n 
if not hasattr ( source , ) : \n 
~~ source = getreader ( ) ( source ) \n 
self . file = source \n 
self . buffer = \n 
~~~ self . line = __line = self . readline ( ) \n 
if self . line is None : \n 
~~~ self . parseline ( ) \n 
~~ except ParseError , msg : \n 
~~ ~~ return self . sink \n 
~~ def parseline ( self ) : \n 
~~~ self . eat ( r_wspace ) \n 
if ( not self . line ) or self . line . startswith ( ( ) ) : \n 
~~ subject = self . subject ( ) \n 
self . eat ( r_wspace ) \n 
predicate = self . predicate ( ) \n 
obj = self . object ( ) \n 
context = self . uriref ( ) or self . nodeid ( ) or self . sink . identifier \n 
self . eat ( r_tail ) \n 
if self . line : \n 
~~ self . sink . get_context ( context ) . add ( ( subject , predicate , obj ) ) \n 
from rdflib import Variable , BNode , URIRef , Literal , py3compat \n 
from rdflib . query import Result , ResultSerializer , ResultParser \n 
class CSVResultParser ( ResultParser ) : \n 
~~~ self . delim = "," \n 
~~ def parse ( self , source ) : \n 
~~~ r = Result ( ) \n 
if isinstance ( source . read ( 0 ) , py3compat . bytestype ) : \n 
~~~ source = codecs . getreader ( ) ( source ) \n 
~~ reader = csv . reader ( source , delimiter = self . delim ) \n 
r . vars = [ Variable ( x ) for x in reader . next ( ) ] \n 
r . bindings = [ ] \n 
for row in reader : \n 
~~~ r . bindings . append ( self . parseRow ( row , r . vars ) ) \n 
~~ return r \n 
~~ def parseRow ( self , row , v ) : \n 
~~~ return dict ( ( var , val ) \n 
for var , val in zip ( v , [ self . convertTerm ( t ) \n 
for t in row ] ) if val is not None ) \n 
~~ def convertTerm ( self , t ) : \n 
~~~ if t == "" : \n 
~~ if t . startswith ( "_:" ) : \n 
~~~ return URIRef ( t ) \n 
~~ return Literal ( t ) \n 
~~ ~~ class CSVResultSerializer ( ResultSerializer ) : \n 
~~~ def __init__ ( self , result ) : \n 
~~~ ResultSerializer . __init__ ( self , result ) \n 
self . delim = "," \n 
if result . type != "SELECT" : \n 
~~ ~~ def serialize ( self , stream , encoding = ) : \n 
~~~ if py3compat . PY3 : \n 
~~~ import codecs \n 
stream = codecs . getwriter ( encoding ) ( stream ) \n 
~~ out = csv . writer ( stream , delimiter = self . delim ) \n 
vs = [ self . serializeTerm ( v , encoding ) for v in self . result . vars ] \n 
out . writerow ( vs ) \n 
for row in self . result . bindings : \n 
~~~ out . writerow ( [ self . serializeTerm ( \n 
row . get ( v ) , encoding ) for v in self . result . vars ] ) \n 
~~ ~~ def serializeTerm ( self , term , encoding ) : \n 
~~~ if term is None : \n 
~~~ return "" \n 
~~ if not py3compat . PY3 : \n 
~~~ return term . encode ( encoding ) \n 
NOSE_ARGS = [ \n 
COVERAGE_EXTRA_ARGS = [ \n 
DEFAULT_ATTRS = [ ] \n 
DEFAULT_DIRS = [ , ] \n 
~~~ from sys import argv , exit , stderr \n 
try : import nose \n 
except ImportError : \n 
~~ if in argv : \n 
~~~ try : import coverage \n 
argv . remove ( ) \n 
~~~ NOSE_ARGS += COVERAGE_EXTRA_ARGS \n 
~~ ~~ if True not in [ a . startswith ( ) or a . startswith ( ) for a in argv ] : \n 
~~~ argv . append ( + . join ( DEFAULT_ATTRS ) ) \n 
~~ if not [ a for a in argv [ 1 : ] if not a . startswith ( ) ] : \n 
~~~ argv += DEFAULT_DIRS \n 
~~ finalArgs = argv + NOSE_ARGS \n 
nose . run_exit ( argv = finalArgs ) \n 
~~ 
